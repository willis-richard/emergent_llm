"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: Client
- Model: gemini-2.5-flash
- Game: collective_risk
"""

from dataclasses import dataclass
from enum import Enum
import math
import numpy as np
from numpy.typing import NDArray
import random

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState



class Strategy_COLLECTIVE_1(BaseStrategy):
    """
    Pro-Social Collective (PSC) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize the overall collective payoff by promoting and
    maintaining cooperation. It starts by cooperating, continues to cooperate
    as long as the collective meets the threshold, shows patience with a single
    failure, but defects after two or more consecutive failures to deter
    persistent free-riding. It defects in the final round due to the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            self.consecutive_failure_count = 0
            return Action.C
        previous_round_idx = current_round_0_indexed - 1
        my_prev_action = history.my_actions[previous_round_idx]
        opponent_prev_actions = history.opponent_actions[previous_round_idx, :]
        previous_round_cooperators = int(my_prev_action) + np.sum(opponent_prev_actions).item()
        if previous_round_cooperators >= min_cooperators:
            self.consecutive_failure_count = 0
            return Action.C
        else:
            self.consecutive_failure_count += 1
            if self.consecutive_failure_count >= 2:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_2(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    ACR aims to foster cooperation by adapting its behavior based on the collective outcome
    of the previous round. It starts by cooperating to signal willingness for collective good.
    In intermediate rounds, it cooperates if the collective threshold 'm' was met in the
    previous round, reinforcing success. If 'm' was not met, it defects to signal
    dissatisfaction and prompt re-evaluation. In the final round, it defects due to
    backward induction to avoid exploitation, minimizing individual loss.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            previous_round_cooperators = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if previous_round_cooperators >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_3(BaseStrategy):
    """
    Adaptive Threshold Enforcement (ATE) strategy for the Collective Risk Dilemma.

    This strategy is designed to promote collective cooperation and secure the 'k' bonus.
    It initiates cooperation in the first round. In subsequent intermediate rounds,
    it observes if the cooperation threshold 'm' was met in the previous round.
    If 'm' was met, it continues to cooperate to sustain the collective benefit.
    If 'm' was not met, it defects to signal disapproval and penalize collective failure,
    aiming to push the group towards renewed cooperation. In the final round, it defects
    to maximize individual payoff, consistent with backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_prev_action_was_C = history.my_actions[prev_round_idx]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev_round = int(my_prev_action_was_C) + num_opponent_cooperators_prev_round
        if total_cooperators_prev_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_4(BaseStrategy):
    """
    The Robust Collective Optimist (RCO) strategy aims to establish and maintain
    universal cooperation in the Collective Risk Dilemma. It starts by cooperating
    to signal intent. It rewards sustained collective success by continuing to
    cooperate. Upon collective failure (fewer than 'm' cooperators), it enters
    a punishment phase of defection. After a fixed number of punishment rounds,
    it enters a forgiveness phase, attempting to re-establish cooperation to
    avoid a permanent defection trap. It always defects in the final round
    due to backward induction.
    """
    PUNISHMENT_ROUNDS_PERIOD: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_rounds_below_m: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev = history.my_actions[state.round_number - 1]
        n_c_prev = int(my_action_prev) + opponent_cooperators_prev
        if n_c_prev >= m:
            self.consecutive_rounds_below_m = 0
        else:
            self.consecutive_rounds_below_m += 1
        if self.consecutive_rounds_below_m == 0:
            return Action.C
        elif self.consecutive_rounds_below_m <= self.PUNISHMENT_ROUNDS_PERIOD:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_5(BaseStrategy):
    """
    The Adaptive Threshold Maintainer (ATM) strategy for the Collective Risk Dilemma.
    It cooperates in the first round to initiate collective action.
    In intermediate rounds, it continues to cooperate if the collective threshold (m)
    was met in the previous round, otherwise it defects to signal failure and for self-protection.
    In the final round, it defects due to the absence of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_1_indexed_round = state.round_number + 1
        if current_1_indexed_round == 1:
            return Action.C
        elif current_1_indexed_round == total_rounds:
            return Action.D
        else:
            my_prev_action_was_cooperate = history.my_actions[-1]
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            N_c_prev = int(my_prev_action_was_cooperate) + num_opponent_cooperators_prev_round
            if N_c_prev >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_6(BaseStrategy):
    """
    Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    It initiates cooperation, rewards collective success, punishes collective failure,
    and offers conditional forgiveness for "near misses" in failed rounds.
    It defects in the final round as per rational end-game behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == total_rounds - 1:
            return Action.D
        my_previous_action_bool = history.my_actions[current_round_idx - 1]
        cooperators_prev_round_opponents = np.sum(history.opponent_actions[current_round_idx - 1, :])
        C_prev = int(my_previous_action_bool) + cooperators_prev_round_opponents
        if C_prev >= min_cooperators_m:
            return Action.C
        elif my_previous_action_bool == True:
            return Action.D
        elif C_prev == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_7(BaseStrategy):
    """
    Adaptive Collective Risk Averter (ACRA) strategy.

    This strategy aims to foster cooperation by initially contributing, then adapting
    its behavior based on whether the collective threshold 'm' was met in the
    previous round. It encourages cooperation by reciprocating success and attempting
    to bridge small gaps, while deterring widespread free-riding through temporary
    defection. It recognizes the unique incentives of the final round.

    Decision Rules (0-indexed rounds):
    1.  Round 0 (Initial Probe): Cooperate (C) to signal willingness.
    2.  Rounds 1 to n_rounds - 2 (Adaptive Play):
        *   If C_prev (total cooperators in previous round) >= m: Cooperate (C).
        *   If C_prev == m - 1: Cooperate (C) (threshold just missed, attempt to bridge).
        *   If C_prev < m - 1: Defect (D) (threshold missed significantly, deter free-riding).
    3.  Round n_rounds - 1 (Final Round): Defect (D) due to endgame rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        if current_round_idx == 0:
            return Action.C
        cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[current_round_idx - 1, :])
        my_action_prev_round_val = int(history.my_actions[current_round_idx - 1])
        C_prev = cooperators_from_opponents_prev_round + my_action_prev_round_val
        if C_prev >= self.m:
            return Action.C
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_8(BaseStrategy):
    """
    The "Patient Collective Enforcer" strategy for the Collective Risk Dilemma.
    It starts cooperatively, maintains cooperation as long as the collective succeeds,
    and tolerates a limited number of collective failures before defecting as a form
    of punishment or self-preservation.
    
    Core Philosophy: Prioritize achieving the collective reward 'k' by cooperating,
    but avoid being exploited by persistently non-cooperative groups. Show patience
    and willingness to re-establish cooperation after initial setbacks, but enforce
    a limit to collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PATIENCE_THRESHOLD = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        consecutive_failures = 0
        for r_idx in range(state.round_number - 1, -1, -1):
            my_action_in_round = history.my_actions[r_idx]
            opponent_cooperators_in_round = np.sum(history.opponent_actions[r_idx, :])
            total_cooperators_in_round = opponent_cooperators_in_round + int(my_action_in_round)
            if total_cooperators_in_round < self.game_description.m:
                consecutive_failures += 1
            else:
                break
        prev_round_idx = state.round_number - 1
        prev_my_action = history.my_actions[prev_round_idx]
        prev_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        prev_total_cooperators = prev_opponent_cooperators + int(prev_my_action)
        if prev_total_cooperators >= self.game_description.m:
            return Action.C
        elif consecutive_failures <= self.PATIENCE_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_9(BaseStrategy):
    """
    Adaptive Conditional Collective Play (ACCP) for the Collective Risk Dilemma.

    This strategy aims to promote collective success by initiating cooperation,
    sustaining it when critical, punishing collective failure or exploitation,
    and attempting to free-ride safely when there's a surplus of cooperation.
    It adopts a purely self-interested strategy in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_last_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        player_action_last_round_was_C = history.my_actions[-1]
        if num_cooperators_last_round < self.m:
            if player_action_last_round_was_C:
                return Action.D
            else:
                return Action.D
        elif player_action_last_round_was_C:
            if num_cooperators_last_round > self.m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_10(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR) strategy for the Collective Risk Dilemma.

    This strategy aims to achieve and sustain the collective reward `k` by promoting
    cooperation, sharing the burden of contribution, and strategically punishing
    non-cooperation when the collective goal is jeopardized. It balances the
    individual incentive to free-ride with the collective need to meet the threshold,
    using a responsive mechanism based on observed history and a memory of past failures.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.failure_streak: int = 0
        self.my_coop_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_last_action: Action = history.my_actions[state.round_number - 1]
        last_C_count: int = int(np.sum(history.opponent_actions[state.round_number - 1, :])) + int(my_last_action)
        if last_C_count >= self.m:
            self.failure_streak = 0
            self.my_coop_failures = 0
        else:
            self.failure_streak += 1
            if my_last_action == Action.C:
                self.my_coop_failures += 1
        if last_C_count >= self.m:
            if my_last_action == Action.C:
                return Action.C
            else:
                return Action.C
        elif self.failure_streak >= 2 and self.my_coop_failures >= 1:
            return Action.D
        elif my_last_action == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_11(BaseStrategy):
    """
    The "Adaptive Guardian" strategy for the Collective Risk Dilemma.
    This strategy prioritizes achieving and sustaining the collective reward 'k' by
    conditionally cooperating, but includes a pragmatic self-preservation measure
    against exploitation in the final round of a tournament.

    Key behaviors:
    - Initiates cooperation in the very first round to establish a collective effort.
    - Continues to cooperate in all intermediate rounds, irrespective of the previous
      round's outcome (whether the threshold 'm' was met or not). This unwavering
      cooperation aims to either maintain collective success or rally the group
      towards achieving the threshold.
    - Defects in the final round to avoid being exploited, recognizing the absence
      of future interactions and the shift in individual incentives.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_12(BaseStrategy):
    """
    Adaptive Collective Commitment (ACC) Strategy:

    This strategy aims to be a reliable cooperator, prioritizing the collective success
    of meeting the 'm' threshold in the Collective Risk Dilemma. It demonstrates patience
    and resilience by continuing to cooperate after minor setbacks, but strategically
    withdraws cooperation (defects) if the group consistently and significantly fails
    to contribute its fair share. This adaptive response protects the strategy from being
    a perpetual "sucker" while still pushing for collective success. It also rationally
    defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.my_last_action_was_C: bool = False
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        if state.round_number == 0:
            current_action = Action.C
            self.my_last_action_was_C = True
            self.consecutive_failures = 0
            return current_action
        N_c_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if state.round_number == self.r - 1:
            current_action = Action.D
            self.my_last_action_was_C = False
            return current_action
        if N_c_prev >= self.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if N_c_prev >= self.m:
            current_action = Action.C
            self.my_last_action_was_C = True
        elif self.consecutive_failures == 1 or N_c_prev >= self.m - 2:
            current_action = Action.C
            self.my_last_action_was_C = True
        else:
            current_action = Action.D
            self.my_last_action_was_C = False
        return current_action

class Strategy_COLLECTIVE_13(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Forgiveness (ACRF) strategy for the Collective Risk Dilemma.
    This strategy aims to foster and maintain collective cooperation by employing conditional
    cooperation, punishing collective failures, and forgiving past failures. It also accounts
    for the end-game effect common in finite repeated games.
    """
    PUNISHMENT_DURATION = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialises the strategy with game parameters and internal state variables.
        """
        self.game_description = game_description
        self.punishment_countdown = 0
        self.just_finished_punishment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (C for Cooperate, D for Defect) for the current round
        based on game history and its internal strategy state.
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        num_cooperators_in_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + (1 if history.my_actions[state.round_number - 1] else 0)
        current_action = Action.D
        if num_cooperators_in_prev_round >= m_threshold:
            self.punishment_countdown = 0
            self.just_finished_punishment = False
            current_action = Action.C
        elif self.punishment_countdown > 0:
            current_action = Action.D
            self.punishment_countdown -= 1
            self.just_finished_punishment = self.punishment_countdown == 0
        elif self.just_finished_punishment:
            current_action = Action.C
            self.just_finished_punishment = False
        else:
            current_action = Action.D
            self.punishment_countdown = self.PUNISHMENT_DURATION - 1
            self.just_finished_punishment = self.PUNISHMENT_DURATION == 1
        return current_action

class Strategy_COLLECTIVE_14(BaseStrategy):
    """
    Adaptive Reciprocity for Collective Good strategy.

    This strategy aims to establish and maintain a cooperative equilibrium
    in the Collective Risk Dilemma by initially cooperating, reinforcing
    cooperation upon success, tolerating minor failures, punishing persistent
    non-cooperation, and re-engaging after punishment. It also acts prudently
    in the final round to secure the best immediate outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_success_streak: int = 0
        self.cooperation_failure_streak: int = 0
        self.punishment_active: bool = False
        self.punishment_duration_remaining: int = 0
        self.FAILURE_TOLERANCE_THRESHOLD: int = 2
        self.PUNISHMENT_DURATION: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        n_players_cooperated_prev_round: int = -1
        if state.round_number > 0:
            n_players_cooperated_prev_round = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
            if n_players_cooperated_prev_round >= m:
                self.cooperation_success_streak += 1
                self.cooperation_failure_streak = 0
                self.punishment_active = False
                self.punishment_duration_remaining = 0
            else:
                self.cooperation_failure_streak += 1
                self.cooperation_success_streak = 0
        if state.round_number == n_rounds - 1:
            if not self.punishment_active and n_players_cooperated_prev_round >= m:
                return Action.C
            else:
                return Action.D
        if self.punishment_active:
            self.punishment_duration_remaining -= 1
            if self.punishment_duration_remaining <= 0:
                self.punishment_active = False
            return Action.D
        if state.round_number == 0:
            return Action.C
        elif n_players_cooperated_prev_round >= m:
            return Action.C
        elif self.cooperation_failure_streak >= self.FAILURE_TOLERANCE_THRESHOLD:
            self.punishment_active = True
            self.punishment_duration_remaining = self.PUNISHMENT_DURATION - 1
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_15(BaseStrategy):
    """
    The Adaptive Collective Threshold Enforcer (ACTE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize the collective payoff by consistently ensuring the 'm'
    cooperation threshold is met. It initiates cooperation, sustains it when successful,
    and adapts its behavior based on whether the collective goal was achieved in the
    previous round and its own prior action. It incorporates a rational defection in
    the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == r:
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_action_prev_round_bool = history.my_actions[-1]
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(my_action_prev_round_bool)
        if cooperators_prev_round >= m:
            if my_action_prev_round_bool:
                return Action.C
            else:
                return Action.C
        elif my_action_prev_round_bool:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_16(BaseStrategy):
    """
    Adaptive Collective Threshold strategy for the Collective Risk Dilemma.
    It starts with cooperation, continues to cooperate upon collective success,
    temporarily defects after collective failure (punishment phase), and
    re-attempts cooperation after a set number of consecutive defections
    (forgiveness phase). It defects in the final round for individual optimization.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_current_defection_streak: int = 0
        self.forgiveness_threshold_F: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.my_current_defection_streak = 0
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            self.my_current_defection_streak = 0
            return Action.D
        else:
            num_cooperators_previous_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            num_cooperators_previous_round += history.my_actions[state.round_number - 1]
            if num_cooperators_previous_round >= self.game_description.m:
                self.my_current_defection_streak = 0
                return Action.C
            elif self.my_current_defection_streak < self.forgiveness_threshold_F:
                self.my_current_defection_streak += 1
                return Action.D
            else:
                self.my_current_defection_streak = 0
                return Action.C

class Strategy_COLLECTIVE_17(BaseStrategy):
    """
    The Adaptive Collective Enforcement (ACE) strategy for the Collective Risk Dilemma.

    ACE aims to promote and sustain collective cooperation by initiating collective action,
    rewarding success with continued cooperation, and deterring free-riding or insufficient
    cooperation through temporary defection. It prioritizes the stability of the collective
    good (k reward) over individual opportunistic gains when the threshold is met,
    while also preventing being a "sucker" when the collective fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        my_action_prev_round = history.my_actions[current_round_0_indexed - 1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        total_cooperators_prev_round = int(my_action_prev_round) + opponent_cooperators_prev_round
        if total_cooperators_prev_round >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_18(BaseStrategy):
    """
    The Adaptive Trust and Response strategy dynamically adjusts its cooperation decision based on the
    observed number of cooperators in the previous round. It uses a 'current_trust_level' to quantify
    its belief in the collective's ability to meet the 'm' cooperator threshold. This trust level is
    updated dynamically and compared against a calculated 'TRUST_THRESHOLD' derived from the game parameters.
    The strategy aims to foster cooperation by initiating with it, continuing it upon collective success,
    and punishing collective failure by temporarily defecting. It defects in the final round to maximize
    individual payoff due to the absence of future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self._current_trust_level: float = 0.5
        self._trust_threshold: float = self.m / self.n_players
        self._trust_gain_factor: float = 0.1
        self._trust_loss_factor: float = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        n_c_previous_round = history.my_actions[state.round_number - 1] + np.sum(history.opponent_actions[state.round_number - 1, :])
        if n_c_previous_round >= self.m:
            self._current_trust_level = min(1.0, self._current_trust_level + self._trust_gain_factor)
        else:
            self._current_trust_level = max(0.0, self._current_trust_level - self._trust_loss_factor)
        if self._current_trust_level > self._trust_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_19(BaseStrategy):
    """
    Adaptive Collective Reciprocator with Forgiveness (ACRF) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize the collective payoff by establishing and maintaining cooperation
    to consistently meet the 'm' threshold, thereby securing the 'k' bonus for all players.
    It operates on the principles of:
    - Pro-Social Initiation: Always cooperates in the first round.
    - Collective Reciprocity: Continues cooperation if the collective threshold was met in the prior round;
      otherwise, defects to "punish" failure.
    - Forgiveness: After a single round of defection (punishment), it immediately attempts to re-establish
      cooperation, preventing endless cycles of defection.
    - Rational Edge: Defects in the final round to avoid exploitation, as there are no future interactions
      to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.is_punishing_mode: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
        if self.is_punishing_mode:
            self.is_punishing_mode = False
            return Action.C
        elif num_cooperators_prev_round >= min_cooperators_needed:
            return Action.C
        else:
            self.is_punishing_mode = True
            return Action.D

class Strategy_COLLECTIVE_20(BaseStrategy):
    """
    Adaptive Community Enforcer with Probing (ACE-P) for the Collective Risk Dilemma.

    This strategy aims to achieve collective success by promoting cooperation while
    remaining robust against exploitation and capable of adapting to various opponent
    behaviors. It prioritizes achieving the `m` threshold, where the collective
    reward `k` is realized.

    Core Principle:
    A conditional cooperator that starts by signalling willingness to cooperate.
    It then adapts its behavior based on whether the collective threshold `m` was
    met in the previous round. It actively punishes failure to meet the threshold
    but includes a "probing" mechanism to attempt to re-establish cooperation
    after prolonged collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.consecutive_failure_rounds = 0
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            num_opponent_cooperators_last_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_action_last_round_was_cooperate = history.my_actions[state.round_number - 1]
            num_total_cooperators_last_round = num_opponent_cooperators_last_round + my_action_last_round_was_cooperate
            if num_total_cooperators_last_round >= self.game_description.m:
                self.consecutive_failure_rounds = 0
                return Action.C
            else:
                self.consecutive_failure_rounds += 1
                if self.consecutive_failure_rounds >= 3:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_21(BaseStrategy):
    """
    The Committed Collective Enabler (CCE) strategy aims to establish and sustain a cooperative
    equilibrium where the minimum cooperation threshold (`m`) is met, thus ensuring the
    collective reward `k` for all players. It does so by committing to contribute when the
    collective goal is achieved, and by withdrawing contributions as a signal and punishment
    when it fails. It starts by cooperating and defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_my_action_cooperated_count = int(history.my_actions[-1])
        previous_round_opponent_cooperators_count = np.sum(history.opponent_actions[-1, :])
        total_cooperators_previous_round = previous_round_my_action_cooperated_count + previous_round_opponent_cooperators_count
        if total_cooperators_previous_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_22(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    It begins with cooperation, fosters it by continuing to cooperate if the collective
    threshold 'm' is met, and punishes collective failures by defecting for a fixed duration.
    It maintains a collective mindset by cooperating in the final round if the group has
    been successful, otherwise defects to avoid being a 'sucker'.
    """

    class MyState(Enum):
        COOPERATING = 1
        PUNISHING = 0

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_state = self.MyState.COOPERATING
        self.punishment_countdown = 0
        self.PUNISHMENT_DURATION = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_zero_indexed == 0:
            return Action.C
        c_count_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if self.my_state == self.MyState.PUNISHING:
            self.punishment_countdown -= 1
            if self.punishment_countdown <= 0:
                self.my_state = self.MyState.COOPERATING
        elif c_count_prev_round < min_cooperators_m:
            self.my_state = self.MyState.PUNISHING
            self.punishment_countdown = self.PUNISHMENT_DURATION
        if current_round_zero_indexed == total_rounds - 1:
            if self.my_state == self.MyState.PUNISHING:
                return Action.D
            else:
                return Action.C
        if self.my_state == self.MyState.PUNISHING:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_23(BaseStrategy):
    """
    The Collective Resilience and Reciprocity (CRR) strategy aims to foster and sustain cooperation,
    prioritize the collective good (achieving the 'k' reward), and be robust against various
    opponent behaviors, including exploitation. It leads by example, rewards collective success,
    encourages near misses, and punishes severe collective failure, while pragmatically defecting
    in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators_count: int | None = None
        self.punishment_phase_active: bool = False
        self.punishment_rounds_left: int = 0
        self.severe_failure_threshold: int = max(1, math.floor(self.game_description.m / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0 and history is not None:
            prev_my_action_cooperated = int(history.my_actions[state.round_number - 1])
            prev_opponent_cooperators_count = np.sum(history.opponent_actions[state.round_number - 1, :])
            self.prev_cooperators_count = prev_my_action_cooperated + prev_opponent_cooperators_count
        if state.round_number == 0:
            return Action.C
        if self.punishment_phase_active:
            self.punishment_rounds_left -= 1
            if self.punishment_rounds_left == 0:
                self.punishment_phase_active = False
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.prev_cooperators_count is not None:
            if self.prev_cooperators_count >= self.game_description.m:
                return Action.C
            elif self.prev_cooperators_count < self.severe_failure_threshold:
                self.punishment_phase_active = True
                self.punishment_rounds_left = 2
                return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_24(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.

    ATE prioritizes securing the collective reward 'k' by ensuring at least 'm'
    cooperators. It starts cooperatively, reinforces successful collective action,
    punishes collective failures with temporary defection, and actively probes
    for renewed cooperation after prolonged periods of failure. It also
    anticipates the rational end-game defection to prevent exploitation.
    """

    class Stance(Enum):
        """
        Internal states for the Adaptive Threshold Enforcer strategy.
        """
        COOPERATE_MAINTAIN = 0
        PUNISH = 1
        PROBE_COOPERATION = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATE strategy with game parameters and internal state.
        """
        self.game_description = game_description
        self.current_stance = self.Stance.COOPERATE_MAINTAIN
        self.rounds_in_current_stance = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (C or D) for the current round based on
        the Adaptive Threshold Enforcer strategy.

        Args:
            state (GameState): Current game state, including round_number (0-indexed).
            history (None | PlayerHistory): History of actions and payoffs from previous rounds.
                                            None for the very first round (round_number = 0).

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        if state.round_number == 0:
            self.current_stance = self.Stance.COOPERATE_MAINTAIN
            self.rounds_in_current_stance = 1
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        C_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        threshold_met = C_prev_round >= self.game_description.m
        action_for_this_round = Action.D
        if self.current_stance == self.Stance.COOPERATE_MAINTAIN:
            if threshold_met:
                action_for_this_round = Action.C
                self.rounds_in_current_stance += 1
            else:
                action_for_this_round = Action.D
                self.current_stance = self.Stance.PUNISH
                self.rounds_in_current_stance = 1
        elif self.current_stance == self.Stance.PUNISH:
            if threshold_met:
                action_for_this_round = Action.C
                self.current_stance = self.Stance.COOPERATE_MAINTAIN
                self.rounds_in_current_stance = 1
            else:
                self.rounds_in_current_stance += 1
                if self.rounds_in_current_stance <= 2:
                    action_for_this_round = Action.D
                else:
                    action_for_this_round = Action.C
                    self.current_stance = self.Stance.PROBE_COOPERATION
                    self.rounds_in_current_stance = 1
        elif self.current_stance == self.Stance.PROBE_COOPERATION:
            if threshold_met:
                action_for_this_round = Action.C
                self.current_stance = self.Stance.COOPERATE_MAINTAIN
                self.rounds_in_current_stance = 1
            else:
                action_for_this_round = Action.D
                self.current_stance = self.Stance.PUNISH
                self.rounds_in_current_stance = 1
        return action_for_this_round

class Strategy_COLLECTIVE_25(BaseStrategy):
    """
    Threshold-Conditional Reciprocity (TCR) strategy for the Collective Risk Dilemma.
    It initiates cooperation, sustains it when the collective threshold 'm' is met,
    defects when 'm' is not met to signal failure and protect itself,
    and defects in the final round for self-preservation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_as_int = int(history.my_actions[-1])
            num_cooperators_in_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_as_int
            if num_cooperators_in_prev_round >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_26(BaseStrategy):
    """
    Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    ACE prioritizes the achievement of the collective reward (k) by conditionally
    cooperating, punishing collective failures (especially if individual effort
    was wasted), and attempting to re-establish cooperation after failures.
    It explicitly accounts for the end-game effect in finitely repeated games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        num_cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        my_action_in_prev_round = history.my_actions[-1]
        if num_cooperators_in_prev_round >= self.m_threshold:
            return Action.C
        elif my_action_in_prev_round == True:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_27(BaseStrategy):
    """
    The "Adaptive Collective Builder" (PCB) strategy.
    It aims to foster and sustain collective cooperation by leveraging observable
    history to adapt its behavior. It starts with cooperation, shows resilience
    to one or two consecutive failures, but withdraws support when cooperation
    consistently fails (three or more consecutive failures). In the final round,
    it cooperates only if the collective was successful in the penultimate round,
    otherwise it defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._last_round_cooperators: None | int = None
        self._rounds_since_last_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number > 0:
            prev_round_opponent_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
            prev_round_my_action_was_C = history.my_actions[state.round_number - 1]
            self._last_round_cooperators = prev_round_opponent_cooperators + (1 if prev_round_my_action_was_C else 0)
            if self._last_round_cooperators >= m:
                self._rounds_since_last_success = 0
            else:
                self._rounds_since_last_success += 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            if self._last_round_cooperators >= m:
                return Action.C
            else:
                return Action.D
        elif self._last_round_cooperators >= m:
            return Action.C
        elif self._rounds_since_last_success <= 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_28(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" strategy aims to achieve robust collective outcomes
    (meeting the `m` cooperator threshold) while adapting to the group's behavior and
    optimizing its own payoff. It balances initial cooperation with cautious probing
    for defection and a mechanism to respond to persistent collective failure.
    """
    _PATIENCE_THRESHOLD = 2
    _DEFECTION_PROBE_THRESHOLD = 3

    class Stance(Enum):
        COOPERATE = 1
        PROBE_DEFECTION = 2
        PUNISH_DEFECTION = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_current_action_stance = self.Stance.COOPERATE
        self.cooperation_streak = 0
        self.defection_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t == 0:
            self.my_current_action_stance = self.Stance.COOPERATE
            return Action.C
        if current_round_t == total_rounds_r - 1:
            return Action.D
        my_action_prev_round_is_C = history.my_actions[-1]
        opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        C_prev = opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if C_prev >= min_cooperators_m:
            self.cooperation_streak += 1
            self.defection_streak = 0
        else:
            self.defection_streak += 1
            self.cooperation_streak = 0
        if self.defection_streak >= self._PATIENCE_THRESHOLD:
            self.my_current_action_stance = self.Stance.PUNISH_DEFECTION
        elif self.cooperation_streak >= self._DEFECTION_PROBE_THRESHOLD:
            if C_prev >= min_cooperators_m + 1:
                self.my_current_action_stance = self.Stance.PROBE_DEFECTION
            else:
                self.my_current_action_stance = self.Stance.COOPERATE
        elif C_prev < min_cooperators_m:
            self.my_current_action_stance = self.Stance.COOPERATE
        else:
            self.my_current_action_stance = self.Stance.COOPERATE
        if self.my_current_action_stance == self.Stance.COOPERATE:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_29(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for the Collective Risk Dilemma.

    ACR initiates cooperation, rewards collective success by maintaining individual roles,
    punishes collective failure by defecting, and defects in the final round for
    individual gain. It aims for robust and adaptive cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_t_1_indexed == 1:
            return Action.C
        if current_round_t_1_indexed == total_rounds:
            return Action.D
        my_last_action_was_cooperate = history.my_actions[-1]
        num_cooperators_prev_round = history.opponent_actions[-1, :].sum() + my_last_action_was_cooperate
        if num_cooperators_prev_round >= self.game_description.m:
            if my_last_action_was_cooperate:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_30(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain collective cooperation to secure the 'k' reward.
    It starts by signaling willingness to cooperate, then adapts its behavior based on whether the
    collective threshold 'm' was met in the previous round. It punishes collective failures by
    defecting and strategically defects in the final round due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACC strategy, storing the game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters such as n_players,
                                                          n_rounds, m (minimum cooperators needed),
                                                          and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on the
        Adaptive Collective Contributor (ACC) strategy rules.

        Args:
            state (GameState): The current state of the game, including the round number
                               (0-indexed).
            history (None | PlayerHistory): An object containing past actions and payoffs
                                            for this player and opponents. It is None for
                                            the very first round (state.round_number == 0).

        Returns:
            Action: The chosen action for the current round, either Action.C (Cooperate)
                    or Action.D (Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_previous_action = history.my_actions[state.round_number - 1]
            opponent_previous_actions = history.opponent_actions[state.round_number - 1, :]
            C_prev = int(my_previous_action) + np.sum(opponent_previous_actions)
            if C_prev >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_31(BaseStrategy):
    """
    The Adaptive Patience strategy for the Collective Risk Dilemma.

    This strategy aims to maximize collective payoff by initially cooperating and tolerating
    a calculated number of consecutive collective failures (where the 'm' threshold is not met).
    It will defect defensively if persistent collective failure indicates the group is
    unwilling or unable to cooperate, or if it's the final round of the game (backward induction).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failed_rounds_count: int = 0
        self.patience_limit: int = max(2, math.floor(self.game_description.n_rounds / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.failed_rounds_count = 0
            return Action.C
        prev_round_index = state.round_number - 1
        my_prev_action_cooperated = int(history.my_actions[prev_round_index])
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_index, :])
        C_prev = my_prev_action_cooperated + opponent_prev_cooperators
        if C_prev >= self.game_description.m:
            self.failed_rounds_count = 0
            return Action.C
        else:
            self.failed_rounds_count += 1
            if self.failed_rounds_count < self.patience_limit:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_32(BaseStrategy):
    """
    Adaptive Threshold-Based Cooperation (ATC) strategy.

    This strategy aims to foster and maintain collective cooperation by adapting
    its behavior based on the observed success or failure of meeting the
    collective cooperation threshold `m`. It aligns with a collective mindset
    by prioritizing the achievement of the shared reward `k` while also being
    robust to opportunistic defection.

    It starts by cooperating. If the collective cooperation threshold is met,
    it continues to cooperate. If the threshold is *not* met, it initiates
    a temporary "punishment" phase of defection. After this punishment,
    or if the threshold is met early during punishment, it attempts to
    re-establish cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_total_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.PUNISHMENT_DURATION = 2
        self.punishment_mode = False
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number + 1
        if current_game_round == 1:
            return Action.C
        elif current_game_round == self.n_total_rounds:
            return Action.D
        else:
            c_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if self.punishment_mode:
                if c_prev >= self.m_threshold:
                    self.punishment_mode = False
                    self.punishment_rounds_remaining = 0
                    return Action.C
                elif self.punishment_rounds_remaining > 0:
                    self.punishment_rounds_remaining -= 1
                    return Action.D
                else:
                    self.punishment_mode = False
                    return Action.C
            elif c_prev >= self.m_threshold:
                return Action.C
            else:
                self.punishment_mode = True
                self.punishment_rounds_remaining = self.PUNISHMENT_DURATION - 1
                return Action.D

class Strategy_COLLECTIVE_33(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) with Forgiveness.

    This strategy aims to maximize long-term payoff by promoting and sustaining
    collective cooperation to achieve the 'k' reward. It initiates cooperation,
    punishes non-cooperation for a fixed duration, and then attempts to re-establish
    cooperation, balancing collective benefit with individual rationality. It defects
    in the final round due to end-game effects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds: int = 0
        self.PUNISHMENT_DURATION: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_number == 0:
            return Action.C
        elif current_round_number == total_rounds - 1:
            return Action.D
        else:
            my_last_action_was_c = history.my_actions[-1]
            opponent_last_cooperators = np.sum(history.opponent_actions[-1, :])
            total_cooperators_last_round = opponent_last_cooperators + (1 if my_last_action_was_c else 0)
            if total_cooperators_last_round >= m_threshold:
                self.consecutive_failure_rounds = 0
                return Action.C
            else:
                self.consecutive_failure_rounds += 1
                if self.consecutive_failure_rounds <= self.PUNISHMENT_DURATION:
                    return Action.D
                else:
                    self.consecutive_failure_rounds = 0
                    return Action.C

class Strategy_COLLECTIVE_34(BaseStrategy):
    """
    The "Adaptive Threshold Guardian" (ATG) strategy for the Collective Risk Dilemma.
    It initiates cooperation, rewards collective success, punishes failure with temporary defection,
    and attempts to re-engage cooperation after punishment, while defecting in the final round.
    """
    punishment_duration: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.is_punishing: bool = False
        self.punishment_rounds_left: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        elif state.round_number == n_rounds - 1:
            return Action.D
        else:
            my_prev_action_cooperated = history.my_actions[state.round_number - 1]
            opponent_prev_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
            C_prev = my_prev_action_cooperated + opponent_prev_cooperators
            if self.is_punishing:
                if C_prev >= m:
                    self.is_punishing = False
                    self.punishment_rounds_left = 0
                    return Action.C
                elif self.punishment_rounds_left > 0:
                    self.punishment_rounds_left -= 1
                    return Action.D
                else:
                    self.is_punishing = False
                    return Action.C
            elif C_prev >= m:
                return Action.C
            else:
                self.is_punishing = True
                self.punishment_rounds_left = self.punishment_duration - 1
                return Action.D

class Strategy_COLLECTIVE_35(BaseStrategy):
    """
    Adaptive Threshold Response (ATR) strategy for the Collective Risk Dilemma.

    The ATR strategy aims to achieve the collective reward 'k' by promoting cooperation
    when the group demonstrates capacity to meet the 'm' threshold, and to protect
    individual players (and implicitly, the group from repeated losses) when the
    collective effort fails. It treats the success or failure of meeting the 'm'
    threshold as a signal of the group's collective intent and capability.

    - Round 1: Always Cooperate (C) to probe the group's cooperativeness.
    - Subsequent Rounds:
        - If the number of cooperators in the previous round was >= 'm': Continue to Cooperate (C).
        - If the number of cooperators in the previous round was < 'm': Defect (D) to protect endowment
          and signal that the current level of cooperation is insufficient.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            num_opponent_cooperators_previous_round = sum(history.opponent_actions[-1, :])
            num_my_cooperation_previous_round = history.my_actions[-1]
            total_cooperators_previous_round = num_opponent_cooperators_previous_round + num_my_cooperation_previous_round
            if total_cooperators_previous_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_36(BaseStrategy):
    """
    Dynamic Collective Reciprocator strategy for the Collective Risk Dilemma.
    This strategy aims to achieve and maintain collective cooperation by demonstrating
    an initial willingness to contribute, rewarding sustained collective success,
    and offering opportunities for the group to recover from minor setbacks.
    It is also designed to be robust against exploitation, opting to defect in
    the final round and when cooperation consistently fails or is significantly below
    the required threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.optimism_delta = 1
        self.forgiveness_memory = 2
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round == total_rounds - 1:
            return Action.D
        if current_round == 0:
            self.consecutive_failures = 0
            return Action.C
        cooperators_in_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if cooperators_in_prev_round >= m_threshold:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            is_near_miss = cooperators_in_prev_round >= m_threshold - self.optimism_delta
            is_within_forgiveness = self.consecutive_failures <= self.forgiveness_memory
            if is_near_miss and is_within_forgiveness:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_37(BaseStrategy):
    """
    Adaptive Cooperative Forgiveness (ACF) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain the collective benefit by being
    proactively cooperative, forgiving of minor setbacks, and firm against
    persistent exploitation. It prioritizes achieving the 'k' reward for everyone
    by focusing on consistently meeting the 'm' cooperators threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.max_failures_allowed_before_defecting: int = 2
        self.consecutive_failed_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.consecutive_failed_rounds = 0
            return Action.C
        previous_round_cooperators = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if previous_round_cooperators >= m_threshold:
            self.consecutive_failed_rounds = 0
        else:
            self.consecutive_failed_rounds += 1
        if previous_round_cooperators >= m_threshold:
            return Action.C
        elif self.consecutive_failed_rounds <= self.max_failures_allowed_before_defecting:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_38(BaseStrategy):
    """
    The Collective Guardian strategy for the Collective Risk Dilemma.

    This strategy aims to maximize collective welfare by ensuring the 'm'
    cooperator threshold is met consistently. It starts with cooperation
    to establish a positive baseline. In intermediate rounds, it reacts
    to the previous round's outcome: it cooperates if the 'm' threshold
    was met (maintaining collective success) and defects if it was missed
    (punishing collective failure to incentivize future cooperation).
    In the final round, it defects, adhering to rational end-game play.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_previous_action = history.my_actions[-1]
        opponent_previous_actions = history.opponent_actions[-1, :]
        N_C_prev = int(my_previous_action) + np.sum(opponent_previous_actions)
        if N_C_prev < self.m:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_39(BaseStrategy):
    """
    The Dynamic Collective Stabilizer (DCS) strategy aims to establish and maintain cooperation
    in the Collective Risk Dilemma. It uses adaptive rules to respond to the collective outcome
    of the previous round, attempting to stabilize successful cooperation and recover from
    collective failures. It includes a cautious mechanism for players to attempt free-riding
    when the collective is robustly over-cooperating, balanced by a strong pull back to
    cooperation to prevent exploitation.
    """
    COOPERATION_STABILITY_ROUNDS = 2
    DEFECTION_PENALTY_ROUNDS = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.consecutive_successes: int = 0
        self.rounds_to_cooperate_due_to_failure: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            current_action = Action.C
            self.my_last_action = current_action
            return current_action
        if current_round_t == self.game_description.n_rounds:
            current_action = Action.D
            self.my_last_action = current_action
            return current_action
        C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        my_action_prev_internal: Action = self.my_last_action
        if C_prev < self.game_description.m:
            self.consecutive_successes = 0
            if my_action_prev_internal == Action.D:
                self.rounds_to_cooperate_due_to_failure = self.DEFECTION_PENALTY_ROUNDS
            else:
                self.rounds_to_cooperate_due_to_failure = max(0, self.rounds_to_cooperate_due_to_failure - 1)
        else:
            self.consecutive_successes += 1
            self.rounds_to_cooperate_due_to_failure = max(0, self.rounds_to_cooperate_due_to_failure - 1)
        current_action = Action.C
        if self.rounds_to_cooperate_due_to_failure > 0:
            current_action = Action.C
        elif my_action_prev_internal == Action.C:
            if self.consecutive_successes >= self.COOPERATION_STABILITY_ROUNDS and C_prev > self.game_description.m:
                current_action = Action.D
            else:
                current_action = Action.C
        else:
            current_action = Action.C
        self.my_last_action = current_action
        return current_action

class Strategy_COLLECTIVE_40(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to secure the collective 'k' reward by proactively cooperating,
    reinforcing success, showing conditional forgiveness for minor collective failures,
    and defensively punishing severe collective breakdowns.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.tolerance_threshold_cooperation_failure = math.ceil(self.m / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round = history.my_actions[-1]
            c_prev = opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
            if c_prev >= self.m:
                return Action.C
            else:
                cooperation_deficit = self.m - c_prev
                if cooperation_deficit <= self.tolerance_threshold_cooperation_failure:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_41(BaseStrategy):
    """
    The Adaptive Collective Enforcer strategy aims to maximize collective welfare by achieving the 'k' bonus
    while protecting against persistent individual exploitation. It initiates cooperation to signal trust,
    sustains cooperation when the collective goal ('m') is met or nearly met, and strategically defects
    as an enforcement mechanism when the group consistently fails to reach the threshold. It concludes
    with an individually rational defection in the final round due to the absence of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev_round = history.my_actions[current_round_0_indexed - 1]
        opponent_actions_prev_round = history.opponent_actions[current_round_0_indexed - 1, :]
        num_cooperators_opponents_prev_round = np.sum(opponent_actions_prev_round)
        num_cooperators_total_prev_round = num_cooperators_opponents_prev_round
        if my_action_prev_round:
            num_cooperators_total_prev_round += 1
        if num_cooperators_total_prev_round >= self.m:
            return Action.C
        elif num_cooperators_total_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_42(BaseStrategy):
    """
    The Adaptive Collective Cooperator with Forgiveness (ACCF) strategy
    prioritizes achieving the collective reward by initially cooperating.
    It tolerates a single round of collective failure (not meeting the 'm'
    threshold) before defensively defecting. It resets its tolerance upon
    collective success. In the final round, it defects to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak_count = 0
        self.forgiveness_rounds = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.failure_streak_count = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_cooperators_in_previous_round = 0
        num_cooperators_in_previous_round += np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            num_cooperators_in_previous_round += 1
        if num_cooperators_in_previous_round >= self.game_description.m:
            self.failure_streak_count = 0
        else:
            self.failure_streak_count += 1
        if self.failure_streak_count <= self.forgiveness_rounds:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_43(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) Strategy for the Collective Risk Dilemma.

    This strategy aims to achieve collective reward by promoting cooperation while
    being robust against exploitation and able to adapt to varying levels of
    cooperation. It employs "tough love": starting with cooperation, punishing
    collective failures (threshold not met) with defection, and punishing
    persistent free-riding (exploitation) with temporary defection. It also
    implements an endgame defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACE strategy with game parameters and internal state variables.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.rounds_since_last_met_threshold: int = 0
        self.exploitation_monitor_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACE strategy logic.

        Args:
            state: GameState object containing the current round number.
            history: PlayerHistory object containing past actions and payoffs, or None for the first round.

        Returns:
            An Action (Action.C for Cooperate or Action.D for Defect).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            action = Action.C
            self.my_last_action = action
            self.rounds_since_last_met_threshold = 1
            self.exploitation_monitor_count = 0
            return action
        if current_round_1_indexed == r:
            action = Action.D
            self.my_last_action = action
            return action
        my_action_prev_bool = history.my_actions[-1]
        NC_prev = np.sum(history.opponent_actions[-1, :]) + (1 if my_action_prev_bool else 0)
        if NC_prev >= m:
            self.rounds_since_last_met_threshold = 0
        else:
            self.rounds_since_last_met_threshold += 1
        if self.my_last_action == Action.C:
            if NC_prev >= m and NC_prev < n:
                self.exploitation_monitor_count += 1
            else:
                self.exploitation_monitor_count = 0
        else:
            self.exploitation_monitor_count = 0
        if self.rounds_since_last_met_threshold > 0:
            action = Action.D
        else:
            exploitation_tolerance_limit = max(3, math.floor(r / 5))
            if self.exploitation_monitor_count >= exploitation_tolerance_limit:
                action = Action.D
                self.exploitation_monitor_count = 0
            else:
                action = Action.C
        self.my_last_action = action
        return action

class Strategy_COLLECTIVE_44(BaseStrategy):
    """
    Adaptive Collective Balancer: Aims to catalyze and sustain cooperation,
    adapt to observed behaviors, and strategically manage personal contributions
    to maximize both collective and individual long-term payoffs in a tournament setting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.my_cooperation_streak: int = 0
        self.my_defection_streak: int = 0
        self.system_failure_streak: int = 0
        self.opponent_actions_history: list[list[bool]] = []

    def _estimate_reliable_cooperators(self, reliability_threshold: float=0.6) -> int:
        """
        Helper function to estimate the number of opponents who are consistently cooperating.
        """
        num_past_rounds = len(self.opponent_actions_history)
        if num_past_rounds == 0:
            return 0
        reliable_count = 0
        num_opponents = self.game_description.n_players - 1
        for opponent_idx in range(num_opponents):
            opponent_cooperations = 0
            for round_data in self.opponent_actions_history:
                if round_data[opponent_idx]:
                    opponent_cooperations += 1
            cooperation_rate = opponent_cooperations / num_past_rounds
            if cooperation_rate >= reliability_threshold:
                reliable_count += 1
        return reliable_count

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            self.my_last_action = Action.C
            return Action.C
        my_action_prev_round_bool = history.my_actions[-1]
        my_action_prev_round = Action.C if my_action_prev_round_bool else Action.D
        opponent_actions_prev_round_bool = history.opponent_actions[-1, :].tolist()
        num_cooperators_opp_prev_round = sum(opponent_actions_prev_round_bool)
        num_cooperators_prev_round = num_cooperators_opp_prev_round + (1 if my_action_prev_round == Action.C else 0)
        self.opponent_actions_history.append(opponent_actions_prev_round_bool)
        if my_action_prev_round == Action.C:
            self.my_cooperation_streak += 1
            self.my_defection_streak = 0
        else:
            self.my_defection_streak += 1
            self.my_cooperation_streak = 0
        if num_cooperators_prev_round < m_threshold:
            self.system_failure_streak += 1
        else:
            self.system_failure_streak = 0
        if current_round_0_indexed == total_rounds - 1:
            self.my_last_action = Action.D
            return Action.D
        if num_cooperators_prev_round >= m_threshold:
            if my_action_prev_round == Action.C:
                reliable_others_count = self._estimate_reliable_cooperators()
                if num_cooperators_prev_round - m_threshold >= 2 and self.my_cooperation_streak >= 3 and (reliable_others_count >= m_threshold):
                    self.my_last_action = Action.D
                    return Action.D
                else:
                    self.my_last_action = Action.C
                    return Action.C
            elif num_cooperators_prev_round > m_threshold + 1:
                self.my_last_action = Action.D
                return Action.D
            else:
                self.my_last_action = Action.C
                return Action.C
        elif my_action_prev_round == Action.D:
            self.my_last_action = Action.C
            return Action.C
        elif self.system_failure_streak >= 2 and num_cooperators_prev_round < m_threshold / 2:
            self.my_last_action = Action.D
            return Action.D
        else:
            self.my_last_action = Action.C
            return Action.C

class Strategy_COLLECTIVE_45(BaseStrategy):
    """
    The Adaptive Collective Commitment with Forgiveness (ACCF) strategy
    for the Collective Risk Dilemma.

    This strategy aims to foster and sustain cooperation by:
    1. Initiating cooperation in the first round.
    2. Continuing to cooperate when the collective successfully meets the threshold (N_C_prev >= m).
    3. Offering 'forgiveness' by cooperating even when the collective narrowly misses the threshold (N_C_prev == m - 1).
    4. Punishing significant collective failures by defecting (N_C_prev < m - 1).
    5. Defecting in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if num_cooperators_prev_round >= self.m:
            return Action.C
        elif num_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_46(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    It aims to foster cooperation and achieve the collective good (meeting the 'm' threshold)
    while being robust against exploitation in a tournament setting. It balances a
    "collective mindset" with the need for self-protection against uncooperative or
    purely selfish opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        prev_round_idx = state.round_number - 1
        n_cooperators_opponents_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_round_was_C = history.my_actions[prev_round_idx]
        N_cooperators_prev_round = n_cooperators_opponents_prev_round + (1 if my_action_prev_round_was_C else 0)
        if current_round_t == self.game_description.n_rounds:
            if N_cooperators_prev_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        if N_cooperators_prev_round >= self.game_description.m:
            self._consecutive_failures = 0
            return Action.C
        else:
            self._consecutive_failures += 1
            if N_cooperators_prev_round <= 1 or self._consecutive_failures >= 2:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_47(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation by rewarding
    past success, punishing collective failure, and periodically re-testing the
    waters for renewed cooperation. It adapts its behavior based on the
    collective outcome of the previous round.

    Internal State:
    - current_mode: Tracks the strategy's current operational state:
        - 'Cooperating_Mode': Actively cooperates, expects others to meet the threshold.
        - 'Punishment_Mode': Defects for one round in response to collective failure.
        - 'Re_Evaluation_Mode': Attempts to cooperate again after a punishment round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_mode: str | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.current_mode = 'Cooperating_Mode'
            return Action.C
        my_prev_action_cooperated = history.my_actions[state.round_number - 1]
        opponents_prev_actions = history.opponent_actions[state.round_number - 1, :]
        C_prev = np.sum(opponents_prev_actions) + int(my_prev_action_cooperated)
        if self.current_mode == 'Cooperating_Mode':
            if C_prev >= self.game_description.m:
                return Action.C
            else:
                self.current_mode = 'Punishment_Mode'
                return Action.D
        elif self.current_mode == 'Punishment_Mode':
            self.current_mode = 'Re_Evaluation_Mode'
            return Action.D
        elif self.current_mode == 'Re_Evaluation_Mode':
            self.current_mode = 'Cooperating_Mode'
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_48(BaseStrategy):
    """
    The Adaptive Collective Pledger (ACP) strategy is designed to balance the collective goal of achieving
    the shared reward 'k' with the individual incentive to maximize payoff. It operates on a principle
    of conditional cooperation, prioritizing the establishment and maintenance of the collective benefit,
    and only attempting to free-ride when that benefit appears secure.

    Core principles:
    1. Prioritize Collective Reward 'k': Ensure the threshold 'm' is met.
    2. Conditional Cooperation: Cooperate when the collective reward is at risk or fragile.
    3. Opportunistic Free-Riding: Defect when cooperation is robustly strong.
    4. Edge Case Handling: Explicit rules for the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.FREE_RIDE_BUFFER = max(1, math.floor((self.n - self.m) / 2))
        self.num_cooperators_in_previous_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0:
            self.num_cooperators_in_previous_round = sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        elif self.num_cooperators_in_previous_round < self.m:
            return Action.C
        elif self.num_cooperators_in_previous_round >= self.m + self.FREE_RIDE_BUFFER:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_49(BaseStrategy):
    """
    Adaptive Threshold Enforcement (ATE) strategy for the Collective Risk Dilemma.

    This strategy aims to balance the collective good (ensuring the 'k' reward) with individual
    self-preservation in the face of varying opponent behaviors. It prioritizes the achievement
    of the cooperation threshold 'm' as the primary collective goal, only resorting to defection
    as a punishment for repeated collective failure or in the final round.

    Core Principles:
    1. Initiate Cooperation: Start with a pro-social action to establish trust and signal willingness
       to contribute to the collective good.
    2. Maintain Collective Success: When the cooperation threshold `m` is met, continue cooperating
       to consistently secure the collective reward `k`.
    3. Punish Collective Failure: If the cooperation threshold `m` is not met, signal disapproval
       and prevent exploitation. Be forgiving for a single failure, but defect if failures are persistent.
    4. Endgame Optimization: Defect in the final round due to the absence of future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == n_rounds - 1:
            return Action.D
        else:
            cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if cooperators_prev_round >= m:
                self.consecutive_failures = 0
                return Action.C
            else:
                self.consecutive_failures += 1
                if self.consecutive_failures == 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_50(BaseStrategy):
    """
    Adaptive Collective Strategist (ACS) aims to foster cooperation by initiating with C,
    sustaining C when the collective goal is met, punishing with D when the goal is missed,
    and re-engaging with C after punishment. It defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.initial_punishment_duration: int = 2
        self.rounds_of_punishment_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t: int = state.round_number + 1
        total_rounds: int = self.game_description.n_rounds
        m: int = self.game_description.m
        if current_round_t == 1:
            return Action.C
        if current_round_t == total_rounds:
            return Action.D
        prev_cooperators_opponents = np.sum(history.opponent_actions[-1, :])
        prev_my_action_was_C = history.my_actions[-1]
        prev_total_cooperators = prev_cooperators_opponents + (1 if prev_my_action_was_C else 0)
        threshold_met_prev_round = prev_total_cooperators >= m
        if self.rounds_of_punishment_remaining > 0:
            self.rounds_of_punishment_remaining -= 1
        elif not threshold_met_prev_round:
            self.rounds_of_punishment_remaining = self.initial_punishment_duration
        if self.rounds_of_punishment_remaining > 0:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_51(BaseStrategy):
    """
    Adaptive Commitment with Probabilistic Surplus Management strategy for the Collective Risk Dilemma.
    This strategy adapts to past collective outcomes and individual actions to balance cooperation
    for threshold achievement with individual payoff maximization through controlled defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_last_action: Action | None = None
        self.num_failures_since_last_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        current_action: Action
        if current_round_t == 1:
            current_action = Action.C
            self.my_last_action = current_action
            return current_action
        prev_round_index = state.round_number - 1
        my_action_prev = self.my_last_action
        Nc_prev = int(history.my_actions[prev_round_index]) + np.sum(history.opponent_actions[prev_round_index, :])
        if Nc_prev < self.m:
            self.num_failures_since_last_success += 1
        else:
            self.num_failures_since_last_success = 0
        if current_round_t == self.r:
            if Nc_prev < self.m:
                current_action = Action.D
            elif my_action_prev == Action.D:
                current_action = Action.D
            elif Nc_prev == self.m:
                current_action = Action.C
            else:
                current_action = Action.D
        elif Nc_prev < self.m:
            patience_threshold = math.floor(self.r / 3)
            if my_action_prev == Action.C and self.num_failures_since_last_success > patience_threshold:
                current_action = Action.D
            else:
                current_action = Action.C
        elif my_action_prev == Action.D:
            current_action = Action.D
        elif Nc_prev == self.m:
            current_action = Action.C
        else:
            P_defect = (Nc_prev - self.m) / (Nc_prev + (self.n - self.m))
            if random.random() < P_defect:
                current_action = Action.D
            else:
                current_action = Action.C
        self.my_last_action = current_action
        return current_action

class Strategy_COLLECTIVE_52(BaseStrategy):
    """
    Adaptive Collective Restoration (ACR) strategy for the Collective Risk Dilemma.
    This strategy prioritizes collective cooperation, adapting its mode based on
    whether the 'm' cooperator threshold was met in the previous round. It cycles
    between COOPERATE_MODE, PUNISH_MODE, and RETRY_MODE to maintain cooperation,
    punish failures, and offer chances for restoration. It defects in the final round.
    """

    class Mode(Enum):
        """Internal states for the Adaptive Collective Restoration strategy."""
        COOPERATE_MODE = 1
        PUNISH_MODE = 2
        RETRY_MODE = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self._current_mode = self.Mode.COOPERATE_MODE

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        last_round_my_action_was_C = history.my_actions[-1]
        last_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
        last_round_total_cooperators = last_round_opponent_cooperators + (1 if last_round_my_action_was_C else 0)
        action_for_current_round: Action
        if self._current_mode == self.Mode.COOPERATE_MODE:
            action_for_current_round = Action.C
        elif self._current_mode == self.Mode.PUNISH_MODE:
            action_for_current_round = Action.D
        elif self._current_mode == self.Mode.RETRY_MODE:
            action_for_current_round = Action.C
        else:
            action_for_current_round = Action.C
        next_mode_for_self = self._current_mode
        if self._current_mode == self.Mode.COOPERATE_MODE:
            if last_round_total_cooperators >= self.m:
                next_mode_for_self = self.Mode.COOPERATE_MODE
            else:
                next_mode_for_self = self.Mode.PUNISH_MODE
        elif self._current_mode == self.Mode.PUNISH_MODE:
            next_mode_for_self = self.Mode.RETRY_MODE
        elif self._current_mode == self.Mode.RETRY_MODE:
            if last_round_total_cooperators >= self.m:
                next_mode_for_self = self.Mode.COOPERATE_MODE
            else:
                next_mode_for_self = self.Mode.PUNISH_MODE
        self._current_mode = next_mode_for_self
        return action_for_current_round

class Strategy_COLLECTIVE_53(BaseStrategy):
    """
    The Adaptive Threshold Enforcer (ATE) strategy aims to navigate the Collective Risk Dilemma
    by proactively initiating cooperation, sustaining it when the collective goal is achieved,
    punishing collective failure by withdrawing contribution, and acknowledging end-game effects.
    It reacts directly to whether the crucial 'm' cooperator threshold was met in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        my_previous_action_was_cooperate = int(history.my_actions[previous_round_idx])
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        total_cooperators_prev_round = my_previous_action_was_cooperate + opponent_cooperators_prev_round
        if total_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_54(BaseStrategy):
    """
    The Adaptive Collective Prowess (ACP) strategy for the Collective Risk Dilemma.
    It initiates cooperation, sustains it when successful, shows patience by tolerating
    a single collective failure, but defects on repeated failures. It always defects
    in the final round, based on backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_t == total_rounds_r:
            return Action.D
        if current_round_t == 1:
            self.consecutive_failures = 0
            return Action.C
        my_prev_action_was_C_int = int(history.my_actions[-1])
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        C_prev = my_prev_action_was_C_int + opponent_prev_cooperators
        if C_prev >= m_threshold:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_55(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" (ACR) strategy aims to foster and maintain cooperation
    to achieve the collective reward (k) as frequently as possible, while also being robust against
    exploitation in a tournament setting. It balances initial trust with adaptive responses to the
    collective's past performance.

    This strategy operates based on the success or failure of meeting the cooperation threshold (m)
    in previous rounds, and incorporates a degree of forgiveness before resorting to defection.
    
    Strategy Parameter: FAILURE_TOLERANCE = 1
    This means the strategy will tolerate a single round where N_C < m and still cooperate in the
    subsequent round. If the collective fails for two consecutive rounds, the strategy will defect.
    This provides a balance of forgiveness and a swift response to persistent non-cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FAILURE_TOLERANCE = 1
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            self.consecutive_failures = 0
            return Action.C
        num_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_previous_round >= self.game_description.m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if current_round_t == self.game_description.n_rounds:
                return Action.D
            elif self.consecutive_failures > self.FAILURE_TOLERANCE:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_56(BaseStrategy):
    """
    The "Adaptive Collective Stabilizer" (ACS) strategy for the Collective Risk Dilemma.
    It promotes cooperation by starting with C, adapts based on collective success/failure,
    initiates temporary punishment for persistent failures, and quickly recovers
    cooperation upon collective success. It also handles end-game scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.THRESHOLD_FAIL_LIMIT: int = 2
        self.PUNISHMENT_DURATION: int = 2
        self.rounds_failed_consecutively: int = 0
        self.is_punishing: bool = False
        self.punishment_countdown: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_idx = state.round_number - 1
        my_prev_action_bool = history.my_actions[previous_round_idx]
        opponent_prev_actions_bools = history.opponent_actions[previous_round_idx, :]
        c_prev = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bools)
        if c_prev >= self.game_description.m:
            self.rounds_failed_consecutively = 0
            self.is_punishing = False
            self.punishment_countdown = 0
        else:
            self.rounds_failed_consecutively += 1
            if self.is_punishing:
                self.punishment_countdown -= 1
                if self.punishment_countdown <= 0:
                    self.is_punishing = False
        if state.round_number == self.game_description.n_rounds - 1:
            if c_prev >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        if self.is_punishing:
            return Action.D
        if self.rounds_failed_consecutively >= self.THRESHOLD_FAIL_LIMIT:
            self.is_punishing = True
            self.punishment_countdown = self.PUNISHMENT_DURATION - 1
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_57(BaseStrategy):
    """
    Adaptive Collective Thresholder strategy for the Collective Risk Dilemma.

    This strategy aims to maximize total payoff by initiating cooperation,
    sustaining it when collective action is successful, and withdrawing when
    it fails, finally defecting in the last round for individual rationality.

    Decision Rules:
    1. Round 1 (0-indexed `state.round_number == 0`): Always Cooperate (C).
       Rationale: An initial probe to establish willingness for collective good.
    2. Last Round (0-indexed `state.round_number == self.game_description.n_rounds - 1`): Always Defect (D).
       Rationale: Individually rational in the end game with no future interactions.
    3. Intermediate Rounds (0-indexed `state.round_number` from 1 to `n_rounds - 2`):
       - If the collective good was achieved in the previous round (total cooperators >= m): Cooperate (C).
         Rationale: Sustain the collective benefit by reinforcing commitment.
       - If the collective good was NOT achieved in the previous round (total cooperators < m): Defect (D).
         Rationale: Protect against self-sacrifice when collective action is insufficient, signaling need for more participation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_action_prev_round_bool = history.my_actions[-1]
            my_contribution_prev_round = 1 if my_action_prev_round_bool else 0
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_in_prev_round = my_contribution_prev_round + opponent_cooperators_prev_round
            if total_cooperators_in_prev_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_58(BaseStrategy):
    """
    Reciprocal Targeter with Sucker Protection strategy for the Collective Risk Dilemma.
    This strategy aims to balance collective success with individual self-preservation
    by adapting its behavior based on past outcomes. It cooperates initially,
    punishes consistent collective failures or personal over-contribution (being a "sucker"),
    and defects in the final round.
    """
    FAIL_TOLERANCE_THRESHOLD: int = 2
    SUCKER_TOLERANCE_THRESHOLD: int = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_failures: int = 0
        self._consecutive_excess_cooperation_as_sucker: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self._consecutive_failures = 0
            self._consecutive_excess_cooperation_as_sucker = 0
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_bool: bool = history.my_actions[prev_round_idx]
        my_action_prev_enum: Action = Action.C if my_action_prev_bool else Action.D
        num_opponent_cooperators_prev: int = np.sum(history.opponent_actions[prev_round_idx, :])
        C_prev: int = num_opponent_cooperators_prev + (1 if my_action_prev_bool else 0)
        if C_prev < m:
            self._consecutive_failures += 1
            self._consecutive_excess_cooperation_as_sucker = 0
        else:
            self._consecutive_failures = 0
            if my_action_prev_enum == Action.C and C_prev > m:
                self._consecutive_excess_cooperation_as_sucker += 1
            else:
                self._consecutive_excess_cooperation_as_sucker = 0
        if self._consecutive_failures >= self.FAIL_TOLERANCE_THRESHOLD:
            return Action.D
        elif self._consecutive_excess_cooperation_as_sucker >= self.SUCKER_TOLERANCE_THRESHOLD:
            return Action.D
        elif C_prev < m:
            return Action.C
        elif C_prev == m:
            return Action.C
        elif C_prev > m:
            return Action.D

class Strategy_COLLECTIVE_59(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy for the Collective Risk Dilemma.

    ARC aims to foster and sustain collective cooperation by adaptively responding
    to the observed collective success or failure in previous rounds. It balances a
    commitment to the collective good with a pragmatic approach to avoid exploitation.

    Decision Rules:
    1.  First Round (t = 1): Cooperate (C) to signal commitment and test cooperation.
    2.  Last Round (t = r): Defect (D) due to backward induction in a finitely repeated game.
    3.  Intermediate Rounds (1 < t < r):
        a.  If C_{t-1} >= m: Cooperate (C) to sustain established success.
        b.  If C_{t-1} = m - 1: Cooperate (C) as individual contribution is critical to meet 'm'.
        c.  If C_{t-1} < m - 1: Defect (D) to prevent unrewarded sacrifice and signal need for more collective effort.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (min_cooperators_needed), k (reward_factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing records of past actions and payoffs.
                     It is None for the first round (state.round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if cooperators_prev_round >= min_cooperators_m:
                return Action.C
            elif cooperators_prev_round == min_cooperators_m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_60(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    This strategy aims to promote and sustain collective cooperation by starting with
    trust, maintaining success, enforcing the cooperation threshold with punishment
    after failures, and attempting to rebuild cooperation. It adapts to repeated
    failures and accounts for individual rationality in the game's final round.
    """

    class Stance(Enum):
        COOPERATE = 'cooperate'
        PUNISH = 'punish'
        REBUILD = 'rebuild'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PUNISHMENT_DURATION: int = 2
        self.COOPERATE_MODE_FAILURE_TOLERANCE: int = 1
        self.REBUILD_MODE_HARD_LIMIT_FAILURES: int = self.game_description.m
        self._current_stance: self.Stance = self.Stance.COOPERATE
        self._punishment_rounds_remaining: int = 0
        self._consecutive_failures_in_cooperate_mode: int = 0
        self._consecutive_failures_in_rebuild_mode: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        prev_round_my_action_was_C: bool = history.my_actions[-1]
        prev_round_num_opponent_cooperators: int = np.sum(history.opponent_actions[-1, :])
        prev_round_num_cooperators: int = prev_round_num_opponent_cooperators + (1 if prev_round_my_action_was_C else 0)
        if self._current_stance == self.Stance.PUNISH:
            if self._punishment_rounds_remaining > 0:
                self._punishment_rounds_remaining -= 1
                return Action.D
            else:
                self._current_stance = self.Stance.REBUILD
                self._consecutive_failures_in_rebuild_mode = 0
                return Action.C
        elif self._current_stance == self.Stance.REBUILD:
            if prev_round_num_cooperators >= self.game_description.m:
                self._current_stance = self.Stance.COOPERATE
                self._consecutive_failures_in_cooperate_mode = 0
                self._consecutive_failures_in_rebuild_mode = 0
                return Action.C
            else:
                self._consecutive_failures_in_rebuild_mode += 1
                if self._consecutive_failures_in_rebuild_mode >= self.REBUILD_MODE_HARD_LIMIT_FAILURES:
                    self._current_stance = self.Stance.PUNISH
                    self._punishment_rounds_remaining = self.PUNISHMENT_DURATION - 1
                    self._consecutive_failures_in_rebuild_mode = 0
                    return Action.D
                else:
                    return Action.C
        elif self._current_stance == self.Stance.COOPERATE:
            if prev_round_num_cooperators >= self.game_description.m:
                self._consecutive_failures_in_cooperate_mode = 0
                return Action.C
            else:
                self._consecutive_failures_in_cooperate_mode += 1
                if self._consecutive_failures_in_cooperate_mode > self.COOPERATE_MODE_FAILURE_TOLERANCE:
                    self._current_stance = self.Stance.PUNISH
                    self._punishment_rounds_remaining = self.PUNISHMENT_DURATION - 1
                    self._consecutive_failures_in_cooperate_mode = 0
                    return Action.D
                else:
                    return Action.C

class Strategy_COLLECTIVE_61(BaseStrategy):
    """
    The Collective Tit-for-Tat with Self-Correction (CTSC) strategy is designed to foster
    cooperation in the Collective Risk Dilemma by being adaptive and robust. It prioritizes
    sustaining collective success when the cooperation threshold is met. When cooperation
    fails, it includes self-protective measures (defecting if exploited) and re-initiation
    mechanisms (cooperating again if there's minimal effort from others) to promote recovery.
    It begins by cooperating and defects in the final round due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as n_players,
                              n_rounds, m (minimum cooperators needed), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on
        the strategy's rules and past game history.

        Args:
            state: An object containing the current round number.
            history: An object containing records of past actions and payoffs for
                     this player and opponents. It is None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == n_rounds - 1:
            return Action.D
        my_action_last_round_bool = history.my_actions[current_round_zero_indexed - 1]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[current_round_zero_indexed - 1, :])
        C_last = (1 if my_action_last_round_bool else 0) + opponent_cooperators_last_round
        if C_last >= m_threshold:
            return Action.C
        elif my_action_last_round_bool:
            return Action.D
        elif C_last > 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_62(BaseStrategy):
    """
    Collective Reciprocity with Patience: This strategy is designed to promote collective benefit by
    initiating cooperation, rewarding sustained group success, exhibiting patience during minor
    setbacks, and ultimately defending against persistent exploitation. It incorporates end-game
    rationality to maximize individual payoff in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.TOLERANCE_THRESHOLD: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_prev_round >= self.game_description.m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures >= self.TOLERANCE_THRESHOLD:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_63(BaseStrategy):
    """
    Collective Enforcement & Trust with Forgiveness (CET-F) strategy.
    This strategy aims to foster collective cooperation in the Collective Risk Dilemma.
    It starts by cooperating, offers a 'second chance' after a single round of failure
    to meet the collective threshold, but defects if failures are persistent (two or more
    consecutive failures) to protect its endowment and signal unsustainable collective effort.
    It also defects in the final round due to lack of future incentives.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        my_action_prev_round = history.my_actions[-1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        cooperators_in_prev_round = my_action_prev_round + opponent_cooperators_prev_round
        k_achieved_last_round = cooperators_in_prev_round >= self.game_description.m
        if k_achieved_last_round:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures == 1:
            return Action.C
        elif self.consecutive_failures >= 2:
            return Action.D

class Strategy_COLLECTIVE_64(BaseStrategy):
    """
    Adaptive Cooperative Restoration (ACR) strategy for the Collective Risk Dilemma.
    This strategy aims to achieve and maintain the collective reward by ensuring the
    cooperation threshold 'm' is met. It adapts to the group's collective behavior,
    rewarding success with continued cooperation, and dynamically reacting to failure
    to try and restore cooperation without being consistently exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialises the strategy with game parameters and internal state variables.
        """
        self.game_description = game_description
        self.my_previous_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines this player's action for the current round based on ACR strategy rules.

        Args:
            state (GameState): The current state of the game (includes round_number, 0-indexed).
            history (None | PlayerHistory): Historical actions and payoffs.
                                            None for the very first round (round_number == 0).

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            action = Action.C
        elif current_round_0_indexed == total_rounds - 1:
            action = Action.D
        else:
            last_round_cooperators_count = np.sum(history.opponent_actions[-1]) + history.my_actions[-1]
            if last_round_cooperators_count >= min_cooperators_needed:
                action = Action.C
            elif self.my_previous_action == Action.C:
                action = Action.D
            else:
                action = Action.C
        self.my_previous_action = action
        return action

class Strategy_COLLECTIVE_65(BaseStrategy):
    """
    Collective Resilience with Forgiveness (CRF) strategy for the Collective Risk Dilemma.
    Aims to foster cooperation, punishes collective failures, and rapidly forgives
    upon collective success to maximize overall reward.
    """

    class Mode(Enum):
        COOPERATE = 1
        PUNISH = 2
    PUNISHMENT_DURATION = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_mode = self.Mode.COOPERATE
        self.punish_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        last_round_opponent_actions = history.opponent_actions[current_round - 1, :]
        last_round_my_action = history.my_actions[current_round - 1]
        num_cooperators_last_round = sum(last_round_opponent_actions) + int(last_round_my_action)
        action_for_current_round = Action.D
        next_mode = self.current_mode
        next_punish_countdown = self.punish_countdown
        if self.current_mode == self.Mode.COOPERATE:
            if num_cooperators_last_round >= min_cooperators:
                action_for_current_round = Action.C
            else:
                action_for_current_round = Action.D
                next_mode = self.Mode.PUNISH
                next_punish_countdown = self.PUNISHMENT_DURATION
        elif num_cooperators_last_round >= min_cooperators:
            action_for_current_round = Action.C
            next_mode = self.Mode.COOPERATE
            next_punish_countdown = 0
        else:
            action_for_current_round = Action.D
            next_punish_countdown -= 1
            if next_punish_countdown <= 0:
                next_punish_countdown = self.PUNISHMENT_DURATION
        self.current_mode = next_mode
        self.punish_countdown = next_punish_countdown
        return action_for_current_round

class Strategy_COLLECTIVE_66(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy aims to foster and maintain cooperation
    in the Collective Risk Dilemma. It begins by cooperating and adaptively responds to the
    collective outcome. It punishes sustained exploitation by defecting and free-rides on
    robust collective success, while also re-engaging in cooperation after a collective failure
    if it previously defected. In the final round, it defects for individual rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_rounds_failed_as_cooperator = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round_num == total_rounds:
            return Action.D
        if current_round_num == 1:
            return Action.C
        my_action_last_round = history.my_actions[-1]
        total_cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(my_action_last_round)
        if my_action_last_round is True and total_cooperators_last_round < min_cooperators:
            self.consecutive_rounds_failed_as_cooperator += 1
        else:
            self.consecutive_rounds_failed_as_cooperator = 0
        if self.consecutive_rounds_failed_as_cooperator >= 2:
            return Action.D
        if total_cooperators_last_round >= min_cooperators:
            if my_action_last_round is True:
                return Action.C
            else:
                return Action.D
        elif my_action_last_round is True:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_67(BaseStrategy):
    """
    Adaptive Collective Enforcement strategy for the Collective Risk Dilemma.
    It initiates cooperation, sustains it when the threshold is met, forgives
    single collective failures, but defects as a "shock" if persistent failures
    occur despite its cooperation. It also defects in the final round due to
    end-game rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_previous_action: Action | None = None
        self.failures_in_a_row: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == 1:
            self.failures_in_a_row = 0
            self.my_previous_action = Action.C
            return Action.C
        elif current_round_1_indexed == n_rounds:
            self.my_previous_action = Action.D
            return Action.D
        else:
            n_c_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if n_c_prev >= m:
                self.failures_in_a_row = 0
                self.my_previous_action = Action.C
                return Action.C
            elif self.my_previous_action == Action.D:
                self.failures_in_a_row = 0
                self.my_previous_action = Action.C
                return Action.C
            else:
                self.failures_in_a_row += 1
                if self.failures_in_a_row >= 2:
                    self.my_previous_action = Action.D
                    return Action.D
                else:
                    self.my_previous_action = Action.C
                    return Action.C

class Strategy_COLLECTIVE_68(BaseStrategy):
    """
    The Collective Reciprocity with Forgiveness (CRF) strategy aims to consistently
    achieve the collective reward 'k' by encouraging cooperation and providing
    a structured, adaptive response to collective failures. It operates on a
    shared understanding that sustained collective success (meeting the 'm'
    threshold) is paramount. The strategy is built around distinct "modes"
    that dictate a player's action, transitioning between these modes based
    on the previous round's collective outcome.

    Modes:
    - COOPERATE_ALL: The collective is cooperating successfully; continue to contribute.
    - PUNISH_DEFECTION: The collective failed; engage in a round of punishment by defecting.
    - REESTABLISH_COOPERATION: After punishment, attempt to re-establish cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_current_mode: str = 'COOPERATE_ALL'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self._my_current_mode = 'COOPERATE_ALL'
            return Action.C
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        success_last_round = num_cooperators_last_round >= self.game_description.m
        if self._my_current_mode == 'COOPERATE_ALL':
            if not success_last_round:
                self._my_current_mode = 'PUNISH_DEFECTION'
        elif self._my_current_mode == 'PUNISH_DEFECTION':
            self._my_current_mode = 'REESTABLISH_COOPERATION'
        elif self._my_current_mode == 'REESTABLISH_COOPERATION':
            if success_last_round:
                self._my_current_mode = 'COOPERATE_ALL'
            else:
                self._my_current_mode = 'PUNISH_DEFECTION'
        if self._my_current_mode == 'COOPERATE_ALL':
            return Action.C
        elif self._my_current_mode == 'PUNISH_DEFECTION':
            return Action.D
        elif self._my_current_mode == 'REESTABLISH_COOPERATION':
            return Action.C

class Strategy_COLLECTIVE_69(BaseStrategy):
    """
    The Collective Stability Maximizer (CSM) strategy for the Collective Risk Dilemma.
    It aims to foster and maintain collective cooperation by prioritizing the achievement
    of the 'k' reward while being adaptive to the group's performance. It is generally
    cooperative but also responsive to failures in collective action, promoting a stable
    cooperative equilibrium.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_t = state.round_number + 1
        if current_game_round_t == 1:
            return Action.C
        num_cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_in_prev_round < self.game_description.m:
            self.consecutive_failure_rounds += 1
        else:
            self.consecutive_failure_rounds = 0
        if current_game_round_t == self.game_description.n_rounds:
            return Action.D
        if self.consecutive_failure_rounds >= 1:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_70(BaseStrategy):
    """
    The Measured Collective strategy for the Collective Risk Dilemma.

    This strategy aims to achieve a stable collective optimum by prioritizing collective
    success, punishing collective failure, and strategically testing for exploitation
    to avoid being a perpetual "sucker" to free-riders. It adapts its behavior based on
    past collective outcomes and its own contribution history.

    Core Principles:
    1. Optimistic Start: Begin by cooperating.
    2. Sustain Success: If the collective project succeeded previously, continue cooperating.
    3. Punish Failure: If the collective project failed previously, withdraw cooperation.
    4. Measured Free-Riding (Exploitation Test): After sustained success, strategically
       defect for a single round to test robustness and gain a higher payoff. Revert
       to cooperation after a successful test.
    5. Endgame Rationality: Defect in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_action_prev: bool = True
        self._consecutive_coop_successes: int = 0
        self._rounds_since_last_defection_attempt: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        total_r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self._consecutive_coop_successes = 0
            self._rounds_since_last_defection_attempt = 0
            chosen_action = Action.C
            self._my_action_prev = True
            return chosen_action
        if current_t == total_r:
            chosen_action = Action.D
            self._my_action_prev = False
            return chosen_action
        prev_round_idx = state.round_number - 1
        C_prev = history.my_actions[prev_round_idx] + np.sum(history.opponent_actions[prev_round_idx, :])
        if not self._my_action_prev:
            self._rounds_since_last_defection_attempt = 0
        else:
            self._rounds_since_last_defection_attempt += 1
        current_round_action_bool: bool = True
        if C_prev >= m:
            if self._my_action_prev:
                self._consecutive_coop_successes += 1
                min_coop_successes_for_test = max(2, int(total_r / 10))
                min_rounds_since_test = max(2, int(total_r / 5))
                if self._consecutive_coop_successes >= min_coop_successes_for_test and self._rounds_since_last_defection_attempt >= min_rounds_since_test:
                    current_round_action_bool = False
                    self._consecutive_coop_successes = 0
                else:
                    current_round_action_bool = True
            else:
                current_round_action_bool = True
                self._consecutive_coop_successes = 0
        else:
            current_round_action_bool = False
            self._consecutive_coop_successes = 0
        self._my_action_prev = current_round_action_bool
        return Action.C if current_round_action_bool else Action.D

class Strategy_COLLECTIVE_71(BaseStrategy):
    """
    Adaptive Threshold Watcher with Exploitation Tolerance (ATW-ET) strategy.
    This strategy aims to achieve and maintain the collective good (ensuring 'm' cooperators)
    while dynamically responding to defection and free-riding. It prioritizes stability and
    recovery when the threshold is not met, and punishes persistent exploitation to deter
    opportunistic behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.my_punishment_mode_active: bool = False
        self.rounds_exploited_as_cooperator: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            self.my_punishment_mode_active = False
            self.rounds_exploited_as_cooperator = 0
            return Action.C
        elif current_round_t == self.r:
            self.my_punishment_mode_active = False
            self.rounds_exploited_as_cooperator = 0
            return Action.D
        else:
            my_last_action = history.my_actions[-1]
            C_prev = int(my_last_action) + np.sum(history.opponent_actions[-1, :])
            if self.my_punishment_mode_active:
                if C_prev < self.m:
                    return Action.D
                else:
                    self.my_punishment_mode_active = False
                    self.rounds_exploited_as_cooperator = 0
                    return Action.C
            elif C_prev < self.m:
                self.rounds_exploited_as_cooperator = 0
                return Action.C
            elif my_last_action == Action.C.value:
                if C_prev == self.m and self.n - C_prev > 0:
                    self.rounds_exploited_as_cooperator += 1
                    if self.rounds_exploited_as_cooperator >= 2:
                        self.my_punishment_mode_active = True
                        self.rounds_exploited_as_cooperator = 0
                        return Action.D
                    else:
                        return Action.C
                else:
                    self.rounds_exploited_as_cooperator = 0
                    return Action.C
            else:
                self.rounds_exploited_as_cooperator = 0
                return Action.C

class Strategy_COLLECTIVE_72(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain cooperation to consistently achieve
    the collective reward `k`. It balances the goal of achieving the collective
    good with the need to be robust against free-riders and avoid futile personal
    sacrifice. It uses a "trust score" system to track individual player behavior.
    """
    TRUST_INCREMENT = 1.0
    TRUST_DECREMENT = 2.0
    TRUSTED_THRESHOLD = 3.0
    UNTRUSTED_THRESHOLD = -3.0

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.trust_scores = np.zeros(self.n, dtype=np.float64)
        if not hasattr(self, 'player_id'):
            self._my_id = 0
        else:
            self._my_id = self.player_id
        self._opponent_global_ids = [i for i in range(self.n) if i != self._my_id]

    def _get_global_player_id(self, opponent_column_index: int) -> int:
        """
        Maps an index from `history.opponent_actions` (0 to n-2)
        to the actual global player ID (0 to n-1).
        """
        return self._opponent_global_ids[opponent_column_index]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == 0:
            return Action.C
        previous_round_actions_per_player = np.zeros(self.n, dtype=bool)
        previous_my_action = history.my_actions[-1]
        previous_round_actions_per_player[self._my_id] = previous_my_action
        previous_opponent_actions = history.opponent_actions[-1, :]
        for i, opp_action in enumerate(previous_opponent_actions):
            global_opp_id = self._get_global_player_id(i)
            previous_round_actions_per_player[global_opp_id] = opp_action
        previous_round_cooperators_count = np.sum(previous_round_actions_per_player)
        if current_round_t == self.r - 1:
            return Action.D
        for player_j_global_id in range(self.n):
            if player_j_global_id == self._my_id:
                continue
            if previous_round_actions_per_player[player_j_global_id] == Action.C.value:
                self.trust_scores[player_j_global_id] += self.TRUST_INCREMENT
            else:
                self.trust_scores[player_j_global_id] -= self.TRUST_DECREMENT
        if previous_round_cooperators_count >= self.m:
            return Action.C
        else:
            expected_other_cooperators = 0
            for player_j_global_id in range(self.n):
                if player_j_global_id == self._my_id:
                    continue
                if self.trust_scores[player_j_global_id] >= self.TRUSTED_THRESHOLD:
                    expected_other_cooperators += 1
                elif self.trust_scores[player_j_global_id] <= self.UNTRUSTED_THRESHOLD:
                    pass
                elif previous_round_actions_per_player[player_j_global_id] == Action.C.value:
                    expected_other_cooperators += 1
            if expected_other_cooperators + 1 >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_73(BaseStrategy):
    """
    Adaptive Collective Guardian (ACG) strategy for the Collective Risk Dilemma.

    This strategy aims to secure the collective reward (k) by promoting cooperation,
    adapting to group performance, and applying deterrence against persistent defection.
    It starts cooperatively, maintains cooperation on success, attempts to correct failures,
    and resorts to temporary defection only when sustained individual cooperation is
    exploited or insufficient. It defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.consecutive_failures_where_I_cooperated: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        action_for_this_round: Action
        if current_round_number == 0:
            action_for_this_round = Action.C
        elif current_round_number == n_rounds - 1:
            action_for_this_round = Action.D
        else:
            total_cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if total_cooperators_in_prev_round >= m_threshold:
                action_for_this_round = Action.C
                self.consecutive_failures_where_I_cooperated = 0
            else:
                my_action_prev_round_was_D = self.my_last_action == Action.D
                if my_action_prev_round_was_D:
                    action_for_this_round = Action.C
                    self.consecutive_failures_where_I_cooperated = 0
                else:
                    self.consecutive_failures_where_I_cooperated += 1
                    if self.consecutive_failures_where_I_cooperated >= 2:
                        action_for_this_round = Action.D
                        self.consecutive_failures_where_I_cooperated = 0
                    else:
                        action_for_this_round = Action.C
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_COLLECTIVE_74(BaseStrategy):
    """
    Adaptive Collective Momentum strategy for the Collective Risk Dilemma.

    This strategy initiates cooperation, rewards collective success, and punishes
    collective failure. It includes a "probe for recovery" mechanism to re-attempt
    cooperation after defection, preventing rigid defection spirals. Self-preservation
    is ensured by defecting if collective failures are too frequent or in the game's
    final rounds, anticipating rational defection by others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._max_consecutive_failures_to_probe: int = 3
        self._consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            self._consecutive_failures = 0
            return Action.C
        last_round_success = False
        my_action_last_round_int = int(history.my_actions[-1])
        opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        num_cooperators_last_round = my_action_last_round_int + opponent_cooperators_last_round
        if num_cooperators_last_round >= m_threshold:
            last_round_success = True
        if state.round_number == n_rounds - 1:
            return Action.D
        if state.round_number == n_rounds - 2:
            if last_round_success:
                return Action.C
            else:
                return Action.D
        elif last_round_success:
            self._consecutive_failures = 0
            return Action.C
        else:
            self._consecutive_failures += 1
            if self._consecutive_failures > self._max_consecutive_failures_to_probe:
                return Action.D
            elif self._consecutive_failures % 2 == 1:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_75(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain a state where the collective cooperation
    threshold (m) is consistently met. It starts by cooperating, then adapts its
    behavior based on whether the threshold was met in the previous round (rewarding
    success with cooperation, punishing failure with defection). In the final round,
    it defects to maximize individual payoff, following standard game-theoretic wisdom.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.min_cooperators_needed = game_description.m
        self.total_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.total_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_0_indexed - 1
            my_action_prev_round_is_cooperate = history.my_actions[previous_round_index]
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
            total_cooperators_prev_round = int(my_action_prev_round_is_cooperate) + opponent_cooperators_prev_round
            if total_cooperators_prev_round >= self.min_cooperators_needed:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_76(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    It prioritizes cooperation, especially in the initial round, when the collective
    threshold was met previously, or when its contribution is pivotal to reaching
    the threshold. It defects only when collective efforts appear futile or heavily
    exploited, signaling the need for greater overall participation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_t == 1:
            return Action.C
        my_prev_action_is_C = history.my_actions[-1]
        num_opponent_prev_cooperators = sum(history.opponent_actions[-1, :])
        N_C_prev = int(my_prev_action_is_C) + num_opponent_prev_cooperators
        if current_t == total_rounds:
            if N_C_prev >= min_cooperators_m:
                return Action.C
            else:
                return Action.D
        elif N_C_prev >= min_cooperators_m:
            return Action.C
        elif N_C_prev == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_77(BaseStrategy):
    """
    Adaptive Collective Nudge (ACN) strategy for the Collective Risk Dilemma.
    Prioritizes establishing and maintaining collective cooperation to ensure the 'm' threshold is met.
    It initiates trust by cooperating, continues cooperation when the threshold is met, and signals
    failure by temporarily defecting when the collective falls short. After a defined punishment
    period, it re-engages with cooperation, preventing permanent defection spirals.
    A rational end-game defection is employed in the final round of a finite game.
    """
    PUNISHMENT_DURATION = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.remaining_punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_num == total_rounds - 1:
            return Action.D
        if current_round_num == 0:
            self.remaining_punishment_rounds = 0
            return Action.C
        else:
            prev_round_my_action = history.my_actions[current_round_num - 1]
            prev_round_opponent_actions = history.opponent_actions[current_round_num - 1, :]
            prev_round_num_cooperators = sum(prev_round_opponent_actions) + (1 if prev_round_my_action else 0)
            if self.remaining_punishment_rounds > 0:
                self.remaining_punishment_rounds -= 1
            elif prev_round_num_cooperators < min_cooperators_m:
                self.remaining_punishment_rounds = self.PUNISHMENT_DURATION
            if self.remaining_punishment_rounds > 0:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_78(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain collective cooperation by:
    1. Cooperating in the first round to initiate collective success and signal intent.
    2. In subsequent rounds, observing the collective outcome of the previous round:
       - If the minimum cooperator threshold 'm' was met (C_t-1 >= m), continue to Cooperate (C)
         to reinforce the positive outcome and sustain collective benefit.
       - If the threshold 'm' was not met (C_t-1 < m), Defect (D) as a response to
         collective failure, signaling that cooperation cannot be taken for granted
         and preventing futile sacrifice.
    This approach balances collective good with self-preservation, adapting to
    opponent behavior and promoting sustainable cooperation. The strategy does not
    deviate its behavior in the last round, applying the same adaptive rule.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_previous_action_is_cooperate = history.my_actions[state.round_number - 1]
        opponent_previous_actions = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_previous_round = int(my_previous_action_is_cooperate) + np.sum(opponent_previous_actions)
        if num_cooperators_previous_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_79(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR) strategy aims to foster and maintain collective cooperation in the
    Collective Risk Dilemma by adapting to observed behavior. It balances the collective good with a
    pragmatic approach for a tournament setting where opponents are unknown.

    Core Principles:
    1.  Initial Commitment: Starts by cooperating.
    2.  Responsiveness to Success: Maintains cooperation if successful, but may defect if cooperation is excessive.
    3.  Resilience to Failure: Attempts to re-establish cooperation after failure, with a degree of patience.
    4.  Reciprocity: Adjusts actions based on past outcomes and own previous contribution.
    5.  Self-Preservation: Defects in the final round to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PATIENCE_THRESHOLD_C_FAIL = 2
        self.OPTIMAL_COOP_TOLERANCE = 0
        self.my_current_patience_for_C_fail = self.PATIENCE_THRESHOLD_C_FAIL

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_number_0_indexed == 0:
            return Action.C
        if current_round_number_0_indexed == n_rounds - 1:
            return Action.D
        my_last_round_action_bool = history.my_actions[-1]
        cooperators_in_last_round = np.sum(history.opponent_actions[-1, :]) + my_last_round_action_bool
        collective_succeeded_last_round = cooperators_in_last_round >= m
        if collective_succeeded_last_round:
            self.my_current_patience_for_C_fail = self.PATIENCE_THRESHOLD_C_FAIL
            if my_last_round_action_bool == Action.C.value:
                if cooperators_in_last_round > m + self.OPTIMAL_COOP_TOLERANCE:
                    return Action.D
                else:
                    return Action.C
            elif cooperators_in_last_round > m:
                return Action.D
            else:
                return Action.C
        elif my_last_round_action_bool == Action.C.value:
            self.my_current_patience_for_C_fail -= 1
            if self.my_current_patience_for_C_fail > 0:
                return Action.C
            else:
                self.my_current_patience_for_C_fail = self.PATIENCE_THRESHOLD_C_FAIL
                return Action.D
        else:
            self.my_current_patience_for_C_fail = self.PATIENCE_THRESHOLD_C_FAIL
            return Action.C

class Strategy_COLLECTIVE_80(BaseStrategy):
    """
    Adaptive Collective Maintainer (ACM) strategy for the Collective Risk Dilemma.
    Prioritizes achieving and sustaining the collective reward 'k', adapting to
    collective outcomes, forgiving single failures or near misses, and signaling
    persistent breakdown by temporarily defecting. Defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def _get_total_cooperators_in_round(self, history: PlayerHistory, round_idx: int) -> int:
        """
        Helper method to calculate the total number of cooperators (including self)
        for a specific round index from the provided history.
        """
        if round_idx < 0 or round_idx >= len(history.my_actions):
            return 0
        my_action_in_round_bool = history.my_actions[round_idx]
        my_contribution = 1 if my_action_in_round_bool else 0
        opponent_cooperators_in_round = np.sum(history.opponent_actions[round_idx, :])
        return int(opponent_cooperators_in_round) + my_contribution

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        observed_cooperators_in_prev_round = self._get_total_cooperators_in_round(history, state.round_number - 1)
        rounds_since_last_success_calculated = 0
        for round_idx in range(state.round_number - 1, -1, -1):
            round_cooperators = self._get_total_cooperators_in_round(history, round_idx)
            if round_cooperators >= m:
                rounds_since_last_success_calculated = 0
                break
            else:
                rounds_since_last_success_calculated += 1
        if observed_cooperators_in_prev_round >= m:
            return Action.C
        else:
            if rounds_since_last_success_calculated == 1:
                return Action.C
            min_cooperators_for_near_miss = max(1, m - 2)
            if rounds_since_last_success_calculated >= 2 and observed_cooperators_in_prev_round >= min_cooperators_for_near_miss:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_81(BaseStrategy):
    """
    Adaptive Reciprocity & Threshold Enforcement (ARTE) strategy for the Collective Risk Dilemma.

    ARTE aims to maximize collective payoff by consistently attempting to meet the cooperation threshold 'm'.
    It employs adaptive rules based on past outcomes to encourage cooperation, deter persistent defection,
    and maintain stability. The strategy prioritizes the collective good, only resorting to self-preservation
    (defection) when the collective effort repeatedly fails or when individual contribution appears futile.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.cooperators_last_round: int | None = None
        self.failures_in_a_row: int = 0
        self.Patience_Threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number > 0 and history is not None:
            prev_round_idx = state.round_number - 1
            my_action_prev_bool = history.my_actions[prev_round_idx]
            self.my_last_action = Action.C if my_action_prev_bool else Action.D
            opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
            self.cooperators_last_round = opponent_cooperators_prev + (1 if my_action_prev_bool else 0)
            if self.cooperators_last_round >= m:
                self.failures_in_a_row = 0
            else:
                self.failures_in_a_row += 1
        if current_t == 1:
            return Action.C
        elif current_t == n_rounds:
            if self.cooperators_last_round >= m and self.my_last_action == Action.C:
                return Action.C
            else:
                return Action.D
        elif self.cooperators_last_round >= m:
            if self.my_last_action == Action.C:
                return Action.C
            else:
                return Action.D
        elif self.failures_in_a_row > self.Patience_Threshold:
            return Action.D
        elif self.my_last_action == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_82(BaseStrategy):
    """
    Adaptive Trigger with Forgiveness: A collective strategy that aims to maximize collective payoff
    by leading cooperation, punishing defection strategically, and offering forgiveness to restart cooperation.
    It defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak: int = 0
        self.rounds_in_punishment_mode: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number > 0:
            previous_round_total_cooperators = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
            if previous_round_total_cooperators >= self.game_description.m:
                self.failure_streak = 0
                self.rounds_in_punishment_mode = 0
            else:
                self.failure_streak += 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        elif self.failure_streak == 0:
            return Action.C
        elif self.failure_streak <= 2:
            return Action.C
        else:
            n_players = self.game_description.n_players
            n_rounds = self.game_description.n_rounds
            punishment_duration_limit = max(2, min(n_players, math.ceil(n_rounds / 3)))
            if self.rounds_in_punishment_mode < punishment_duration_limit:
                self.rounds_in_punishment_mode += 1
                return Action.D
            else:
                self.rounds_in_punishment_mode = 0
                return Action.C

class Strategy_COLLECTIVE_83(BaseStrategy):
    """
    The Adaptive Collective Enabler (ACE) strategy for the Collective Risk Dilemma.
    This strategy aims to promote and sustain cooperation to consistently meet the
    collective threshold 'm' and secure the 'k' reward. It starts by cooperating
    and maintains cooperation when the threshold is met. When the collective fails,
    it attempts to re-establish cooperation, but has a patience limit to prevent
    indefinite exploitation.
    """
    PATIENCE_THRESHOLD = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_failed_threshold_consecutively: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.rounds_failed_threshold_consecutively = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_last_action_is_C = history.my_actions[state.round_number - 1]
        n_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + (1 if my_last_action_is_C else 0)
        if n_cooperators_prev_round >= self.game_description.m:
            self.rounds_failed_threshold_consecutively = 0
            return Action.C
        else:
            self.rounds_failed_threshold_consecutively += 1
            if not my_last_action_is_C:
                return Action.C
            elif self.rounds_failed_threshold_consecutively < self.PATIENCE_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_84(BaseStrategy):
    """
    Robust Collective Stabilizer strategy for the Collective Risk Dilemma.

    This strategy aims to consistently achieve the 'k' bonus by cooperating by default,
    but also to protect itself from being a "sucker" and to punish prolonged collective failure.
    It avoids aggressive free-riding to promote long-term stability and high overall payoffs
    for the group, which generally leads to better individual outcomes in repeated games
    with public goods.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.failure_streak: int = 0
        self.my_sucker_round_trigger: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round > 0:
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            my_cooperation_prev_int = 1 if self.my_last_action == Action.C else 0
            N_C_prev = num_opponent_cooperators_prev + my_cooperation_prev_int
            if N_C_prev < self.game_description.m:
                self.failure_streak += 1
                if self.my_last_action == Action.C:
                    self.my_sucker_round_trigger = True
                else:
                    self.my_sucker_round_trigger = False
            else:
                self.failure_streak = 0
                self.my_sucker_round_trigger = False
        action_for_this_round: Action
        if current_round == 0:
            action_for_this_round = Action.C
        elif current_round == self.game_description.n_rounds - 1:
            action_for_this_round = Action.D
        elif self.failure_streak >= 2:
            action_for_this_round = Action.D
        elif self.my_sucker_round_trigger:
            action_for_this_round = Action.D
        else:
            action_for_this_round = Action.C
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_COLLECTIVE_85(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma.

    Core Principle: Strive to meet the cooperation threshold (`m`) consistently.
    Initiate cooperation, maintain it when successful, offer forgiveness for minor
    setbacks, but disengage from persistent collective failure to avoid exploitation
    and signal dissatisfaction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        my_prev_action_is_C = history.my_actions[-1]
        opponent_prev_actions_are_C = history.opponent_actions[-1, :]
        cooperators_in_prev_round = int(my_prev_action_is_C) + np.sum(opponent_prev_actions_are_C)
        if cooperators_in_prev_round >= m:
            self.failure_streak = 0
        else:
            self.failure_streak += 1
        if cooperators_in_prev_round >= m:
            return Action.C
        elif self.failure_streak == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_86(BaseStrategy):
    """
    Adaptive Collective Risk Aversion (ACRA) strategy aims to promote and sustain cooperation
    towards the collective reward 'k', while being robust against exploitation and capable
    of recovering from breakdowns in cooperation. It balances individual incentives to defect
    with the collective benefit of cooperation through adaptive internal state management.
    """
    TOLERANCE_THRESHOLD = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_current_action_state: Action = Action.C
        self._consecutive_failure_count: int = 0
        self._consecutive_defection_streak: int = 0
        self._recooperation_probe_rounds: int = self.game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self._my_current_action_state = Action.C
            self._consecutive_failure_count = 0
            self._consecutive_defection_streak = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_C_opponents_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev = history.my_actions[state.round_number - 1]
        num_C_prev = num_C_opponents_prev + (1 if my_action_prev == Action.C.value else 0)
        threshold_met_prev = num_C_prev >= self.game_description.m
        if threshold_met_prev:
            self._consecutive_failure_count = 0
        else:
            self._consecutive_failure_count += 1
        if self._my_current_action_state == Action.C:
            if threshold_met_prev:
                self._consecutive_defection_streak = 0
                return Action.C
            elif self._consecutive_failure_count >= self.TOLERANCE_THRESHOLD:
                self._my_current_action_state = Action.D
                self._consecutive_defection_streak = 1
                return Action.D
            else:
                self._consecutive_defection_streak = 0
                return Action.C
        else:
            self._consecutive_defection_streak += 1
            if self._consecutive_defection_streak >= self._recooperation_probe_rounds:
                self._my_current_action_state = Action.C
                self._consecutive_defection_streak = 0
                self._consecutive_failure_count = 0
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_87(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Punishment Threshold.

    This strategy aims to secure collective benefits by defaulting to cooperation,
    but introduces an adaptive punishment mechanism if the collective repeatedly
    fails to meet the minimum cooperator threshold 'm'. It cooperates by default
    and resets its punitive state upon collective success. If the group fails
    to meet 'm' for a 'PUNISHMENT_THRESHOLD' number of consecutive rounds,
    the strategy temporarily defects to signal dissatisfaction and protect
    its own payoff, before reverting to cooperation once the threshold is met again.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m: int = game_description.m
        self.consecutive_failures: int = 0
        self.is_punishing: bool = False
        self.PUNISHMENT_THRESHOLD: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        N_C_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if N_C_previous_round >= self.m:
            self.consecutive_failures = 0
            self.is_punishing = False
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures >= self.PUNISHMENT_THRESHOLD:
                self.is_punishing = True
            else:
                self.is_punishing = False
        if self.is_punishing:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_88(BaseStrategy):
    """
    Threshold-Guarded Cooperation (TGC) strategy for the Collective Risk Dilemma.
    This strategy aims to foster and maintain collective cooperation to secure the 'k' reward.
    It adapts behavior based on the observed number of cooperators in the previous round
    and tracks persistent failures of the collective project. It initiates with cooperation,
    continues cooperation upon success, and only resorts to defection as a punishment
    signal when collective efforts consistently fail for a set number of rounds.
    It defects in the final round due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and internal state variables.

        Args:
            game_description: An object containing game parameters such as n_players,
                              n_rounds, m (minimum cooperators needed), and k (reward factor).
        """
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.num_rounds_project_failed: int = 0
        self.P_fail_threshold: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the Threshold-Guarded Cooperation rules.

        Args:
            state: GameState object containing the current round number (0-indexed).
            history: PlayerHistory object containing past actions and payoffs for all players.
                     Is None for the very first round (state.round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_zero_indexed = state.round_number
        n_rounds_total = self.game_description.n_rounds
        m_threshold = self.game_description.m
        my_current_action: Action
        if current_round_zero_indexed == 0:
            my_current_action = Action.C
            self.my_last_action = my_current_action
            return my_current_action
        if current_round_zero_indexed == n_rounds_total - 1:
            my_current_action = Action.D
            self.my_last_action = my_current_action
            return my_current_action
        cooperators_last_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if cooperators_last_round >= m_threshold:
            self.num_rounds_project_failed = 0
        else:
            self.num_rounds_project_failed += 1
        my_action_in_prev_round = self.my_last_action
        if cooperators_last_round >= m_threshold:
            my_current_action = Action.C
        elif my_action_in_prev_round == Action.C:
            if self.num_rounds_project_failed >= self.P_fail_threshold:
                my_current_action = Action.D
            else:
                my_current_action = Action.C
        else:
            my_current_action = Action.C
        self.my_last_action = my_current_action
        return my_current_action

class Strategy_COLLECTIVE_89(BaseStrategy):
    """
    The Cooperative Threshold Enforcer (CTE) strategy prioritizes achieving and
    sustaining the collective cooperation threshold (m) to secure the maximum
    collective reward (k). It adapts its behavior based on the success or
    failure of the collective in the previous round, balancing individual
    protection from exploitation with a willingness to contribute to the
    common good.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Cooperative Threshold Enforcer strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, m (min cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current player in the given round.

        Args:
            state: The current game state, including the round number.
            history: A record of all players' actions and payoffs for all previous rounds.
                     Is None for the very first round (round_number == 0).

        Returns:
            Action: Action.C for Cooperate or Action.D for Defect.
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        my_action_prev_round = history.my_actions[prev_round_index]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + my_action_prev_round
        if total_cooperators_prev_round >= m_threshold:
            return Action.C
        elif my_action_prev_round:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_90(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation to secure the 'k' bonus.
    It initiates cooperation in the first round to signal willingness and test the waters.
    In intermediate rounds, it continues to cooperate if the collective success threshold 'm'
    was met or nearly met in the previous round. It defects only if cooperation was far below
    the threshold, protecting itself from exploitation and signaling the necessity of broader
    participation. In the final round, it defects due to the end-game effect in finitely
    repeated games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.m:
            return Action.C
        elif num_cooperators_prev_round >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_91(BaseStrategy):
    """
    Forgiving Threshold Contributor (FTC) strategy for the Collective Risk Dilemma.
    Aims to foster and sustain collective cooperation by initially contributing,
    temporarily defecting when the collective threshold 'm' is not met,
    and then forgiving and re-engaging cooperation after a set punishment duration.
    """

    class FtcState(Enum):
        COOPERATE_INITIATIVE = 1
        PUNISHING_DEFECTION = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Forgiving Threshold Contributor (FTC) strategy.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, m, k.
        """
        self.game_description = game_description
        self.my_action_state = self.FtcState.COOPERATE_INITIATIVE
        self.rounds_in_punishment = 0
        self.DEFECTION_PUNISHMENT_DURATION = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on
        the strategy's internal state and the outcome of the previous round.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the very first round (state.round_number == 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        my_action_last_round_bool = history.my_actions[state.round_number - 1]
        opponent_cooperators_last_round = sum(history.opponent_actions[state.round_number - 1, :])
        num_cooperated_last_round = opponent_cooperators_last_round + (1 if my_action_last_round_bool else 0)
        threshold_met_last_round = num_cooperated_last_round >= self.game_description.m
        if self.my_action_state == self.FtcState.COOPERATE_INITIATIVE:
            if threshold_met_last_round:
                return Action.C
            else:
                self.my_action_state = self.FtcState.PUNISHING_DEFECTION
                self.rounds_in_punishment = 1
                return Action.D
        elif self.my_action_state == self.FtcState.PUNISHING_DEFECTION:
            if self.rounds_in_punishment < self.DEFECTION_PUNISHMENT_DURATION:
                self.rounds_in_punishment += 1
                return Action.D
            else:
                self.my_action_state = self.FtcState.COOPERATE_INITIATIVE
                self.rounds_in_punishment = 0
                return Action.C

class Strategy_COLLECTIVE_92(BaseStrategy):
    """
    The Adaptive Collective Success (ACS) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain a state where the collective threshold 'm' is met.
    It operates on a principle of conditional cooperation, rewarding past collective success
    and punishing collective failure, with a specific adjustment for the end-game effect.

    Decision Rules:
    1. Initial Round (t=1): Cooperate (C) to kickstart collective action.
    2. Intermediate Rounds (1 < t < r):
       - If the collective threshold 'm' was met in the previous round (C_t-1 >= m): Cooperate (C)
       - If the collective threshold 'm' was NOT met (C_t-1 < m): Defect (D)
    3. Final Round (t=r): Defect (D) due to the end-game effect (backward induction).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            my_cooperation_prev_round = history.my_actions[current_round_0_indexed - 1]
            opponent_cooperation_prev_round = sum(history.opponent_actions[current_round_0_indexed - 1, :])
            num_cooperators_prev_round = my_cooperation_prev_round + opponent_cooperation_prev_round
            if num_cooperators_prev_round >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_93(BaseStrategy):
    """
    Hopeful Forgiver: This strategy aims to consistently achieve the collective benefit (k) by
    promoting and sustaining cooperation. It starts cooperatively, reacts to collective failure
    with a brief signal of disapproval (punishment), but quickly attempts to re-establish
    cooperation, demonstrating a persistent commitment to the collective good.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._awaiting_punishment_completion: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_number == total_rounds - 1:
            return Action.D
        if current_round_number == 0:
            self._awaiting_punishment_completion = False
            return Action.C
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if self._awaiting_punishment_completion:
            self._awaiting_punishment_completion = False
            return Action.C
        elif cooperators_prev_round >= min_cooperators_needed:
            return Action.C
        else:
            self._awaiting_punishment_completion = True
            return Action.D

class Strategy_COLLECTIVE_94(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation by consistently
    contributing when the collective is successful, and punishing collective
    failure by withdrawing support. It embodies a strong collective mindset
    by prioritizing the achievement of the 'k' bonus for all.

    Core Principle: A form of "Collective Tit-for-Tat." It observes the
    collective outcome of the previous round and responds in kind: if the group
    met the cooperation threshold, it continues to cooperate; if the group failed,
    it defects. This behavior applies consistently across all rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACT strategy with the game parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Collective Threshold (ACT) strategy rules.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for
                     this player and opponents. This will be None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        my_action_prev_round_was_cooperate = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = np.sum(opponent_actions_prev_round) + (1 if my_action_prev_round_was_cooperate else 0)
        min_cooperators_m = self.game_description.m
        if num_cooperators_prev_round >= min_cooperators_m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_95(BaseStrategy):
    """
    Adaptive Threshold Enforcement with Persistent Failure Detection:

    This strategy dynamically adjusts its behavior based on the observed collective outcome of
    previous rounds. It aims to foster cooperation, punish defection that leads to collective
    failure, and protect itself from persistent exploitation, aligning with a collective mindset
    that values long-term stability and mutual benefit.

    Key principles:
    - Starts with cooperation to seed collective action.
    - Reinforces success by continuing to cooperate when the collective threshold is met.
    - Punishes collective failures where its own contribution was wasted (cooperated but failed).
    - Forgives collective failures and attempts to re-seed cooperation if it defected and thus
      did not personally suffer a loss in the previous failure round.
    - Enters a self-preservation (individual optimization) mode if the collective consistently
      fails to meet the threshold for a predefined number of consecutive rounds.
    - Defects in the final round to avoid exploitation, as there's no future to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.consecutive_collective_failures: int = 0
        self.persistent_failure_threshold: int = 2
        self.in_individual_optimization_mode: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self.consecutive_collective_failures = 0
            self.in_individual_optimization_mode = False
            return Action.C
        if current_round_number == self.r - 1:
            self.in_individual_optimization_mode = False
            self.consecutive_collective_failures = 0
            return Action.D
        C_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        my_action_prev = Action.C if history.my_actions[-1] else Action.D
        if C_prev >= self.m:
            self.consecutive_collective_failures = 0
            self.in_individual_optimization_mode = False
        else:
            self.consecutive_collective_failures += 1
            if self.consecutive_collective_failures >= self.persistent_failure_threshold:
                self.in_individual_optimization_mode = True
        action_to_take: Action
        if self.in_individual_optimization_mode:
            action_to_take = Action.D
        elif C_prev >= self.m:
            action_to_take = Action.C
        elif my_action_prev == Action.C:
            action_to_take = Action.D
        else:
            action_to_take = Action.C
        return action_to_take

class Strategy_COLLECTIVE_96(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain collective cooperation while being resilient to exploitation
    and capable of recovering from collective failures. It leverages observed history to inform its decisions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FREE_RIDE_SUCCESS_THRESHOLD: int = 3
        self.PUNISHMENT_FAILURE_THRESHOLD: int = 1
        self.my_previous_action: Action | None = None
        self.num_cooperators_prev_round: int | None = None
        self.consecutive_successes: int = 0
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_action_prev_round_bool = history.my_actions[-1]
        self.my_previous_action = Action.C if my_action_prev_round_bool else Action.D
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        self.num_cooperators_prev_round = int(opponent_cooperators_prev_round) + (1 if my_action_prev_round_bool else 0)
        if self.num_cooperators_prev_round >= self.game_description.m:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        current_action: Action
        if state.round_number == self.game_description.n_rounds - 1:
            current_action = Action.D
        elif self.num_cooperators_prev_round >= self.game_description.m:
            if self.my_previous_action == Action.C and self.num_cooperators_prev_round > self.game_description.m and (self.consecutive_successes >= self.FREE_RIDE_SUCCESS_THRESHOLD):
                current_action = Action.D
            else:
                current_action = Action.C
        elif self.consecutive_failures <= self.PUNISHMENT_FAILURE_THRESHOLD:
            current_action = Action.D
        else:
            current_action = Action.C
        return current_action

class Strategy_COLLECTIVE_97(BaseStrategy):
    """
    The "Adaptive Reciprocator with Collective Threshold" strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain the collective good by initially cooperating
    and then adapting based on the group's success in meeting the cooperation threshold (m).
    It cooperates when the threshold was met in the previous round, and defects when it was not,
    acting as a swift, short-term punishment for collective failure and protecting its endowment.
    In the final round, it defects due to end-game rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            my_prev_action_was_coop = history.my_actions[-1]
            opponents_prev_cooperators = np.sum(history.opponent_actions[-1, :])
            last_round_cooperators_count = int(my_prev_action_was_coop) + opponents_prev_cooperators
            if last_round_cooperators_count < m_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_98(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by ensuring the collective
    reward 'k' is secured whenever possible, while avoiding exploitation by
    free-riders and adapting to the observed cooperation levels of other players.

    It balances the need for collective success with self-preservation,
    initiating cooperation, adapting behavior based on past collective outcomes,
    punishing persistent non-cooperation, and strategically free-riding only
    when it won't jeopardize the collective bonus. It defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_threshold = 3
        self.consecutive_failures = 0
        self.consecutive_successes = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[state.round_number - 1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        C_prev = opponent_cooperators_prev + int(my_action_prev)
        if C_prev < self.game_description.m:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
            if self.consecutive_failures >= self.consecutive_failures_threshold:
                return Action.D
            else:
                return Action.C
        else:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
            if my_action_prev == True:
                if C_prev == self.game_description.m:
                    return Action.C
                elif C_prev - 1 >= self.game_description.m:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_99(BaseStrategy):
    """
    The Collective Adaptive Thresholding (CAT) strategy is a robust, adaptive
    conditional cooperator for the Collective Risk Dilemma. It prioritizes
    achieving the collective benefit (the 'k' reward) by maintaining cooperation
    as long as the group demonstrates a willingness to meet the minimum threshold 'm'.
    It incorporates a "patience" mechanism to avoid endless exploitation when
    collective action consistently fails, and makes a final effort for collective
    success in the last round if the penultimate round was successful.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.Patience_Threshold = 2
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_prev_action = history.my_actions[-1]
        opponent_prev_actions = history.opponent_actions[-1, :]
        C_prev = int(my_prev_action) + np.sum(opponent_prev_actions)
        if state.round_number == self.game_description.n_rounds - 1:
            if C_prev >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        if C_prev >= self.game_description.m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures < self.Patience_Threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_100(BaseStrategy):
    """
    The Collective Endeavor (CE) strategy aims to foster and maintain collective
    cooperation to secure the 'k' bonus for the group, while being robust against exploitation.
    It cooperates in the first round to signal goodwill. In subsequent rounds, it
    cooperates if the collective threshold 'm' was met in the previous round,
    and defects otherwise to signal dissatisfaction and protect itself.
    In the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        num_cooperators_prev_round = sum(history.opponent_actions[prev_round_index, :]) + history.my_actions[prev_round_index]
        m_threshold = self.game_description.m
        if num_cooperators_prev_round >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_101(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy aims to foster and maintain
    collective cooperation by rewarding success and punishing failure, with
    mechanisms to recover from persistent breakdowns. It is designed to be
    adaptive to various opponent behaviors while consistently striving for
    the collective good (achieving the 'k' reward).

    Key Principles:
    1. Optimistic Start: Begin by cooperating to signal willingness.
    2. Reciprocal Response: React directly to the collective outcome of the previous round.
       - Reward Success: If 'm' threshold met, continue or re-engage cooperation.
       - Punish Failure: If 'm' threshold missed, defect.
    3. Resilience and Forgiveness: Periodically re-test for cooperation after prolonged failures.
    4. End-Game Rationality: Adapt to the finite nature by defecting near the end.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.C
        self.consecutive_failures_count: int = 0
        self.consecutive_successes_count: int = 0
        self.END_GAME_HORIZON: int = 1
        self.FAILURE_TOLERANCE: int = 2
        self.RESET_FAILURE_THRESHOLD: int = max(5, math.floor(self.game_description.n_players / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        action_for_this_round: Action
        if state.round_number == 0:
            action_for_this_round = Action.C
        elif state.round_number >= self.game_description.n_rounds - self.END_GAME_HORIZON:
            action_for_this_round = Action.D
        else:
            previous_round_my_action_bool = history.my_actions[-1]
            previous_round_opponent_actions_bool = history.opponent_actions[-1, :]
            num_cooperators_prev_round = int(previous_round_my_action_bool) + np.sum(previous_round_opponent_actions_bool)
            if num_cooperators_prev_round >= self.game_description.m:
                self.consecutive_successes_count += 1
                self.consecutive_failures_count = 0
                if self.my_last_action == Action.D:
                    action_for_this_round = Action.C
                else:
                    action_for_this_round = Action.C
            else:
                self.consecutive_failures_count += 1
                self.consecutive_successes_count = 0
                if self.consecutive_failures_count <= self.FAILURE_TOLERANCE:
                    action_for_this_round = Action.D
                elif self.consecutive_failures_count % self.RESET_FAILURE_THRESHOLD == 0:
                    action_for_this_round = Action.C
                else:
                    action_for_this_round = Action.D
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_COLLECTIVE_102(BaseStrategy):
    """
    Adaptive Forgiving Collective Guardian (AFCG) strategy for the Collective Risk Dilemma.

    AFCG prioritizes achieving the collective reward by initially cooperating,
    maintaining cooperation as long as the threshold of cooperators (m) is met,
    and tolerating a limited number of collective failures. If failures persist
    beyond a defined tolerance, it switches to a permanent defection stance to
    protect itself from exploitation. It also defects in the final round as
    a rational adaptation to the finite game horizon.

    Strategy Parameter:
    - FAILURE_TOLERANCE_THRESHOLD: The maximum number of consecutive rounds of
      collective failure (N_c < m) that the strategy will tolerate before
      entering permanent defection mode. Recommended value is 1.
    """
    FAILURE_TOLERANCE_THRESHOLD: int = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.is_in_punishment_mode: bool = False
        self.n_rounds: int = game_description.n_rounds
        self.m: int = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        prev_round_my_action_val = 1 if history.my_actions[current_round_number - 1] else 0
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[current_round_number - 1, :])
        prev_round_total_cooperators = prev_round_opponent_cooperators + prev_round_my_action_val
        if not self.is_in_punishment_mode:
            if prev_round_total_cooperators < self.m:
                self.consecutive_failures += 1
                if self.consecutive_failures > self.FAILURE_TOLERANCE_THRESHOLD:
                    self.is_in_punishment_mode = True
            else:
                self.consecutive_failures = 0
        if self.is_in_punishment_mode:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_103(BaseStrategy):
    """
    The Adaptive Collective Enforcer strategy aims to establish and maintain a state where at least `m` players cooperate,
    thus securing the `k` reward for everyone. It acts as a "cooperation enforcer" by punishing insufficient cooperation
    and rewarding successful collective efforts. In scenarios where the collective goal is comfortably met, it allows
    for strategic free-riding if it previously successfully free-rode, maximizing individual gain without jeopardizing
    the collective outcome, as long as the system remains stable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        prev_round_index = current_round_0_indexed - 1
        cooperators_in_prev_round = int(history.my_actions[prev_round_index]) + np.sum(history.opponent_actions[prev_round_index, :])
        my_action_in_prev_round = history.my_actions[prev_round_index]
        if cooperators_in_prev_round < m_threshold:
            return Action.D
        elif my_action_in_prev_round == True:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_104(BaseStrategy):
    """
    The "Adaptive Forgiving Contributor" strategy for the Collective Risk Dilemma.

    This strategy aims to encourage and sustain cooperation by initiating cooperation,
    continuing it when the collective threshold is met, temporarily punishing
    collective failure, and periodically re-testing for cooperation to break
    defection spirals. It also accounts for end-game rationality by defecting
    in the final round.

    Core Principles:
    1. Initiate Cooperation: Always attempt to cooperate in the first round.
    2. Sustain Success: If the collective threshold (`m` cooperators) was met
       in the previous round, continue cooperating.
    3. Punish Failure (Temporarily): If the collective threshold was *not* met,
       withdraw cooperation (defect) in the next round.
    4. Forgive and Re-test: If collective failure persists for
       `RESET_COOPERATION_ATTEMPTS_AFTER` consecutive rounds, periodically
       re-attempt cooperation to break out of defection spirals.
    5. Acknowledge End-Game Rationality: In the final round, defect.
    """
    RESET_COOPERATION_ATTEMPTS_AFTER: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.consecutive_failure_rounds = 0
            return Action.C
        last_round_my_cooperation = int(history.my_actions[-1])
        last_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        last_round_total_cooperators = last_round_my_cooperation + last_round_opponent_cooperators
        if last_round_total_cooperators < self.game_description.m:
            self.consecutive_failure_rounds += 1
        else:
            self.consecutive_failure_rounds = 0
        if last_round_total_cooperators >= self.game_description.m:
            return Action.C
        elif self.consecutive_failure_rounds % self.RESET_COOPERATION_ATTEMPTS_AFTER == 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_105(BaseStrategy):
    """
    The Adaptive Threshold-Based Reciprocity (ATBR) strategy for the Collective Risk Dilemma.
    This strategy is conditionally cooperative, aiming to achieve and sustain the collective
    reward 'k'. It initiates cooperation in the first round, reinforces cooperation if the
    collective threshold 'm' was met in the previous round, and defects as a "punishment"
    if the threshold was not met. It accounts for the end-game effect by defecting in the
    final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        num_my_cooperated_prev_round = int(history.my_actions[previous_round_idx])
        num_opponent_cooperated_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        c_prev = num_my_cooperated_prev_round + num_opponent_cooperated_prev_round
        if c_prev >= self.m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_106(BaseStrategy):
    """
    Adaptive Threshold Reciprocity:
    This strategy starts by cooperating in the first round as an act of trust.
    In intermediate rounds, it observes the previous round's collective outcome.
    If the number of cooperators in the previous round met or exceeded the
    threshold 'm', the strategy continues to cooperate. If not, it defects
    to protect its endowment and signal conditional cooperation.
    In the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            prev_round_history_index = state.round_number - 1
            my_prev_action_is_C = history.my_actions[prev_round_history_index]
            opponent_prev_actions_are_C = history.opponent_actions[prev_round_history_index, :]
            num_C_prev_round = sum(opponent_prev_actions_are_C) + (1 if my_prev_action_is_C else 0)
            if num_C_prev_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_107(BaseStrategy):
    """
    The Adaptive Collective Nudger (ACN) strategy aims to foster and sustain collective cooperation
    in the Collective Risk Dilemma. It starts by cooperating, then adapts its behavior based on
    whether the group successfully meets the cooperation threshold (`m`) in previous rounds.
    It incorporates a forgiveness mechanism (`max_forgiveness`) to allow for initial or transient
    failures, but will eventually switch to defection if the collective consistently fails to
    uphold its side of the bargain. This strategy prioritizes the achievement of the collective
    reward `k` for the entire group, balancing patience with self-preservation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Nudger strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators needed), k (reward factor).
        """
        self.game_description = game_description
        self.failure_streak: int = 0
        self.max_forgiveness: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including round_number.
            history: An object containing records of past actions and payoffs for all players.
                     This will be None for the very first round (round_number = 0).

        Returns:
            An Action enum: Action.C for Cooperate, Action.D for Defect.
        """
        if state.round_number == 0:
            self.failure_streak = 0
            return Action.C
        previous_round_index = state.round_number - 1
        cooperators_prev_round = sum(history.opponent_actions[previous_round_index, :]) + int(history.my_actions[previous_round_index])
        m = self.game_description.m
        if cooperators_prev_round >= m:
            self.failure_streak = 0
            return Action.C
        else:
            self.failure_streak += 1
            if self.failure_streak <= self.max_forgiveness:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_108(BaseStrategy):
    """
    The Threshold-Conditional Cooperation (TCC) strategy aims to establish and maintain
    cooperation in the Collective Risk Dilemma. It starts by cooperating and then adapts
    its behavior based on whether the collective successfully met the minimum cooperation
    threshold (`m`) in the preceding round. If the threshold was met, it continues to
    cooperate to sustain the collective reward. If not, it defects as a self-protection
    mechanism and a signal to encourage greater cooperation from others in subsequent rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_action_prev_round = history.my_actions[-1]
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        num_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
        if num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_109(BaseStrategy):
    """
    The Adaptive Collective Effort (ACE) strategy for the Collective Risk Dilemma.

    ACE aims to achieve the collective reward 'k' as consistently as possible for all players,
    while being robust to opponent behaviors and protecting its own payoff. It embraces a
    "collective mindset" by proactively contributing and attempting to restore cooperation,
    but also intelligently defects when conditions suggest exploitation or persistent collective failure.

    Design Principles:
    1.  Initial Good Faith: Start by cooperating to signal willingness.
    2.  Adaptive Response to Success: If 'm' met, assess robustness. If abundant, strategically defect
        for individual gain. If just sufficient, continue to cooperate to reinforce stability.
    3.  Adaptive Response to Failure: If 'm' missed, differentiate between "near miss" and significant
        lack. In a near miss, re-cooperate. In significant failure, temporarily defect to protect payoff.
    4.  Rational End-Game: In the final round, defect for individual maximization.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_strategy_round = state.round_number + 1
        if current_strategy_round == 1:
            return Action.C
        if current_strategy_round == total_rounds:
            return Action.D
        else:
            my_action_prev_was_C = history.my_actions[state.round_number - 1]
            opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
            n_C_prev = (1 if my_action_prev_was_C else 0) + opponent_cooperators_prev
            if n_C_prev >= m_threshold:
                if n_C_prev < (n_players + m_threshold) / 2:
                    return Action.C
                else:
                    return Action.D
            elif n_C_prev < m_threshold / 2:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_110(BaseStrategy):
    """
    The Adaptive Threshold Enforcement (ATE) strategy promotes collective action by cooperating when the
    community project is successfully funded (m cooperators met) and defecting when it fails due to
    insufficient cooperation. It initiates with cooperation in the first round to probe for collective action,
    and defects in the final round due to the absence of future consequences. This strategy is adaptive
    to the group's behavior, aiming to establish and maintain a high-payoff cooperative equilibrium.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[previous_round_0_indexed, :])
        my_action_prev_round = history.my_actions[previous_round_0_indexed]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round)
        if total_cooperators_prev_round >= min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_111(BaseStrategy):
    """
    Adaptive Consensus Seeker (ACS) is a strategy for the Collective Risk Dilemma.
    It prioritizes achieving the collective reward by initially cooperating, then
    adapting its behavior based on past group performance. It employs phases of
    initial trust, sustained cooperation, and targeted punishment (defection)
    to establish and maintain a functional equilibrium, discouraging both
    collective failure and excessive free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.INITIAL_COOPERATION_ROUNDS = max(1, min(2, self.r - 1))
        self.PUNISHMENT_DURATION = max(2, min(5, self.r // 4))
        self.CONSECUTIVE_FAILURE_TRIGGER = 2
        self.CONSECUTIVE_FREE_RIDER_TRIGGER = 2
        self.FREE_RIDER_TOLERANCE_EXTRA = 1
        self.COOPERATION_RECOVERY_WINDOW = 3
        self.COOPERATION_RECOVERY_THRESHOLD_FACTOR = 0.8
        self.group_cooperation_history = []
        self.punishment_rounds_left = 0
        self.consecutive_failures = 0
        self.consecutive_excessive_free_riding = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_idx = state.round_number
        if current_round_0_idx > 0:
            prev_round_0_idx = current_round_0_idx - 1
            prev_N_C = np.sum(history.opponent_actions[prev_round_0_idx, :]) + int(history.my_actions[prev_round_0_idx])
            self.group_cooperation_history.append(prev_N_C)
            prev_threshold_met = prev_N_C >= self.m
            if prev_threshold_met:
                self.consecutive_failures = 0
                actual_free_riders = self.n - prev_N_C
                acceptable_free_riders_ideal = self.n - self.m
                if actual_free_riders > acceptable_free_riders_ideal + self.FREE_RIDER_TOLERANCE_EXTRA:
                    self.consecutive_excessive_free_riding += 1
                else:
                    self.consecutive_excessive_free_riding = 0
            else:
                self.consecutive_failures += 1
                self.consecutive_excessive_free_riding = 0
        if current_round_0_idx == self.r - 1:
            return Action.D
        if self.punishment_rounds_left > 0:
            self.punishment_rounds_left -= 1
            return Action.D
        if current_round_0_idx > 0:
            if self.consecutive_failures >= self.CONSECUTIVE_FAILURE_TRIGGER:
                self.punishment_rounds_left = self.PUNISHMENT_DURATION - 1
                self.consecutive_failures = 0
                return Action.D
            elif self.consecutive_excessive_free_riding >= self.CONSECUTIVE_FREE_RIDER_TRIGGER:
                self.punishment_rounds_left = self.PUNISHMENT_DURATION - 1
                self.consecutive_excessive_free_riding = 0
                return Action.D
        if current_round_0_idx < self.INITIAL_COOPERATION_ROUNDS:
            return Action.C
        if current_round_0_idx > 0:
            _prev_N_C_for_decision = self.group_cooperation_history[-1]
            _prev_threshold_met_for_decision = _prev_N_C_for_decision >= self.m
            if _prev_threshold_met_for_decision:
                return Action.C
            else:
                if _prev_N_C_for_decision == self.m - 1:
                    return Action.C
                start_idx = max(0, current_round_0_idx - self.COOPERATION_RECOVERY_WINDOW)
                recent_N_C_values = self.group_cooperation_history[start_idx:current_round_0_idx]
                if len(recent_N_C_values) > 0:
                    avg_N_C_recent = np.mean(recent_N_C_values)
                    if avg_N_C_recent / self.m >= self.COOPERATION_RECOVERY_THRESHOLD_FACTOR:
                        return Action.C
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_112(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    This strategy aims to foster and sustain cooperation, prioritizing the achievement
    and maintenance of the collective 'k' bonus. It starts cooperatively, reinforces
    successful cooperation, offers forgiveness for near-miss failures, and applies
    defection as a form of collective 'punishment' for significant, sustained failures
    to protect itself from exploitation and signal the cost of widespread non-cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          the game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate 'C' or Defect 'D') for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing the actions and payoffs
                                            from previous rounds. It is None for round 0.

        Returns:
            Action: The chosen action for the current round (Action.C or Action.D).
        """
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        else:
            prev_round_idx = state.round_number - 1
            num_cooperators_prev_round = int(history.my_actions[prev_round_idx]) + np.sum(history.opponent_actions[prev_round_idx, :])
            if num_cooperators_prev_round >= m:
                return Action.C
            elif num_cooperators_prev_round == m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_113(BaseStrategy):
    """
    The Adaptive Commitment to Threshold (ACT) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation by observing the collective
    outcome. It initiates with cooperation, continues to cooperate when the threshold `m`
    is met, and enters a temporary punishment phase (by defecting) if the threshold
    is consistently missed. After punishment, it reverts to cooperation, and makes a
    strategic decision in the final round based on recent collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._punishment_active: bool = False
        self._punishment_rounds_remaining: int = 0
        self._consecutive_threshold_failures: int = 0
        self._punishment_duration: int = 1
        self._failure_tolerance: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        c_prev = 0
        if state.round_number > 0:
            my_prev_action_cooperated = history.my_actions[state.round_number - 1]
            opponent_prev_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :]).item()
            c_prev = opponent_prev_cooperators
            if my_prev_action_cooperated:
                c_prev += 1
        if current_round_1_indexed == total_rounds:
            if c_prev >= m_threshold:
                return Action.C
            else:
                return Action.D
        if self._punishment_active:
            self._punishment_rounds_remaining -= 1
            if self._punishment_rounds_remaining > 0:
                return Action.D
            else:
                self._punishment_active = False
                self._consecutive_threshold_failures = 0
                return Action.C
        elif state.round_number == 0:
            return Action.C
        elif c_prev >= m_threshold:
            self._consecutive_threshold_failures = 0
            return Action.C
        else:
            self._consecutive_threshold_failures += 1
            if self._consecutive_threshold_failures >= self._failure_tolerance:
                self._punishment_active = True
                self._punishment_rounds_remaining = self._punishment_duration
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_115(BaseStrategy):
    """
    Adaptive Collective Commitment (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to balance fostering collective success with self-preservation
    and robustness in a tournament setting. It prioritizes the consistent achievement
    of the collective `k` bonus. It actively supports cooperation when the group
    demonstrates it can meet the threshold `m`. When cooperation fails, it retreats
    to a defensive defection stance but periodically "probes" for a return to collective
    action, preventing a permanent breakdown. It also acknowledges the individually
    rational "endgame" defection in finite repeated games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.is_cooperating_default: bool = True
        self.consecutive_failure_rounds: int = 0
        self.probe_interval: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_idx]
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        C_prev = int(my_action_prev) + int(opponent_cooperators_prev)
        if C_prev >= self.game_description.m:
            self.consecutive_failure_rounds = 0
            self.is_cooperating_default = True
            return Action.C
        else:
            self.consecutive_failure_rounds += 1
            if self.is_cooperating_default:
                self.is_cooperating_default = False
                return Action.D
            elif self.consecutive_failure_rounds % self.probe_interval == 0:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_116(BaseStrategy):
    """
    The Collective Resilience (CR) strategy aims to foster cooperation by initiating with a "leap of faith" 
    and then adapting its behavior based on the collective outcome of the previous round. 
    It punishes collective failures with a single round of defection but quickly attempts to re-establish 
    cooperation, preventing prolonged spirals of non-cooperation. In the final round, it defects due to 
    the absence of future consequences, aligning with individual rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.is_punishing_next_round: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.is_punishing_next_round = False
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_round = history.my_actions[prev_round_idx]
        opponent_actions_prev_round = history.opponent_actions[prev_round_idx, :]
        n_c_prev = int(my_action_prev_round) + sum(opponent_actions_prev_round)
        if n_c_prev >= self.game_description.m:
            self.is_punishing_next_round = False
            return Action.C
        elif self.is_punishing_next_round:
            self.is_punishing_next_round = False
            return Action.C
        else:
            self.is_punishing_next_round = True
            return Action.D

class Strategy_COLLECTIVE_117(BaseStrategy):
    """
    Adaptive Collective Thresholder (ACT) strategy for the Collective Risk Dilemma.

    This strategy aims to promote and sustain cooperation by:
    1. Initiating cooperative efforts in the first round.
    2. Rewarding collective success (meeting the 'm' threshold) with continued cooperation.
    3. Punishing consistent collective failure (below 'm' for F_tol consecutive rounds) by
       withdrawing cooperation.
    4. Acknowledging the "end game effect" by defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak: int = 0
        self.F_tol: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round == 0:
            self.failure_streak = 0
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        cooperators_opponent_prev_round = np.sum(history.opponent_actions[-1, :])
        cooperators_my_prev_round = int(history.my_actions[-1])
        C_count_t_minus_1 = cooperators_opponent_prev_round + cooperators_my_prev_round
        if C_count_t_minus_1 >= m:
            self.failure_streak = 0
            return Action.C
        else:
            self.failure_streak += 1
            if self.failure_streak >= self.F_tol:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_118(BaseStrategy):
    """
    The "Forgiving Adaptive Collective Strategy" (FACS) is designed to maximize collective welfare
    in the Collective Risk Dilemma. It promotes stable cooperation by cooperating proactively,
    forgiving single instances of collective failure, but punishing persistent non-cooperation
    to prevent exploitation. It always provides a path for re-establishing cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._prev_round_threshold_met: bool = True
        self._prev_prev_round_threshold_met: bool = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            self._prev_round_threshold_met = True
            self._prev_prev_round_threshold_met = True
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        num_cooperators_in_round_t_minus_1 = np.sum(history.opponent_actions[current_round_0_indexed - 1, :]) + history.my_actions[current_round_0_indexed - 1]
        status_round_t_minus_1_met: bool = num_cooperators_in_round_t_minus_1 >= m_threshold
        old_prev_round_status_met = self._prev_round_threshold_met
        self._prev_round_threshold_met = status_round_t_minus_1_met
        self._prev_prev_round_threshold_met = old_prev_round_status_met
        if self._prev_round_threshold_met:
            return Action.C
        elif self._prev_prev_round_threshold_met:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_119(BaseStrategy):
    """
    Adaptive Collective Trust (ACT) strategy for the Collective Risk Dilemma.
    This strategy adapts its behavior based on past collective performance,
    using a 'trust_level' and 'punish_rounds_remaining' to balance
    collective good with self-protection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.trust_level: float = 0.5
        self.punish_rounds_remaining: int = 0
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            chosen_action = Action.C
            self.my_last_action = chosen_action
            return chosen_action
        if state.round_number == self.game_description.n_rounds - 1:
            chosen_action = Action.D
            self.my_last_action = chosen_action
            return chosen_action
        num_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + (1 if self.my_last_action == Action.C else 0)
        if num_cooperators_prev >= self.game_description.m:
            self.trust_level = min(1.0, self.trust_level + 0.1)
            self.punish_rounds_remaining = max(0, self.punish_rounds_remaining - 1)
            if self.my_last_action == Action.C and num_cooperators_prev == self.game_description.m and (self.trust_level > 0.7):
                if self.punish_rounds_remaining == 0:
                    self.punish_rounds_remaining = 1
        else:
            self.trust_level = max(0.0, self.trust_level - 0.2)
            if self.my_last_action == Action.C:
                self.punish_rounds_remaining = 2
            else:
                self.punish_rounds_remaining = max(1, self.punish_rounds_remaining)
        chosen_action: Action
        if self.punish_rounds_remaining > 0:
            chosen_action = Action.D
            self.punish_rounds_remaining -= 1
        elif self.trust_level >= 0.5:
            chosen_action = Action.C
        else:
            chosen_action = Action.D
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_COLLECTIVE_120(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for the Collective Risk Dilemma.
    This strategy aims to foster and sustain cooperation, ensuring the collective reward 'k' is obtained.
    It starts with a cooperative stance, forgives initial failures, but adapts to protect itself
    against persistent exploitation or collective collapse. It also includes mechanisms to
    prevent being a "sucker" when cooperation is abundant.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description: CollectiveRiskDescription = game_description
        self.my_last_action: Action | None = None
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        action_for_this_round: Action
        if state.round_number == 0:
            action_for_this_round = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            action_for_this_round = Action.D
        else:
            n_c_prev_my_action_bool: bool = history.my_actions[-1]
            n_c_prev_opponents_count: int = np.sum(history.opponent_actions[-1, :])
            n_c_prev: int = n_c_prev_opponents_count + (1 if n_c_prev_my_action_bool else 0)
            current_my_last_action: Action = self.my_last_action
            if n_c_prev >= self.game_description.m:
                self.consecutive_failures = 0
                if current_my_last_action == Action.C:
                    if n_c_prev > self.game_description.m:
                        action_for_this_round = Action.D
                    else:
                        action_for_this_round = Action.C
                else:
                    action_for_this_round = Action.D
            else:
                self.consecutive_failures += 1
                if self.consecutive_failures <= 2:
                    action_for_this_round = Action.C
                else:
                    action_for_this_round = Action.D
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_COLLECTIVE_121(BaseStrategy):
    """
    The "Adaptive Collective Risk Averter (ACRA)" strategy promotes collective cooperation in the
    Collective Risk Dilemma by starting cooperatively, conditionally continuing cooperation based
    on collective success, and defensively defecting in response to repeated collective failures,
    before attempting cooperation again. It balances the collective good with individual self-protection.

    The strategy operates in two modes:
    1. "Attempting_Cooperation": The player tries to cooperate. If the collective fails to meet
       the minimum cooperators threshold (m) for `FAILURE_TOLERANCE` consecutive rounds,
       the player switches to "Protective_Defection".
    2. "Protective_Defection": The player defects. This phase lasts for `DEFECT_PHASE_DURATION`
       rounds, after which the player reverts to "Attempting_Cooperation".

    It also includes an "end game" effect, defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_state: str = 'Attempting_Cooperation'
        self.consecutive_failures_as_cooperator: int = 0
        self.defection_phase_counter: int = 0
        self.failure_tolerance: int = 1
        self.defect_phase_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        prev_round_idx = state.round_number - 1
        n_c_prev = int(history.my_actions[prev_round_idx]) + np.sum(history.opponent_actions[prev_round_idx, :])
        if self.my_state == 'Attempting_Cooperation':
            if n_c_prev >= self.game_description.m:
                self.consecutive_failures_as_cooperator = 0
            else:
                self.consecutive_failures_as_cooperator += 1
                if self.consecutive_failures_as_cooperator > self.failure_tolerance:
                    self.my_state = 'Protective_Defection'
                    self.defection_phase_counter = 1
                    self.consecutive_failures_as_cooperator = 0
        else:
            self.defection_phase_counter += 1
            if self.defection_phase_counter > self.defect_phase_duration:
                self.my_state = 'Attempting_Cooperation'
                self.defection_phase_counter = 0
        if self.my_state == 'Attempting_Cooperation':
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_122(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma.

    ACT aims to promote and sustain cooperation by being initially trusting,
    reinforcing success, forgiving minor failures, and withdrawing support
    when collective efforts consistently fall short. It balances individual
    self-preservation with the collective goal of achieving the 'k' reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_success = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        n_rounds = self.game_description.n_rounds
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == n_rounds - 1:
            return Action.D
        previous_round_my_action_bool = history.my_actions[current_round_zero_indexed - 1]
        previous_round_opponent_actions_bool_array = history.opponent_actions[current_round_zero_indexed - 1, :]
        C_count_t_minus_1 = np.sum(previous_round_opponent_actions_bool_array) + previous_round_my_action_bool
        if C_count_t_minus_1 >= m:
            self.rounds_since_success = 0
            return Action.C
        else:
            self.rounds_since_success += 1
            if self.rounds_since_success == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_123(BaseStrategy):
    """
    Adaptive Collective Maintenance (ACM) strategy for the Collective Risk Dilemma.
    This strategy prioritizes achieving the collective reward (`k`) for all players by
    maintaining cooperation. It is forgiving of minor setbacks in cooperation but
    retaliates against persistent failure to meet the collective threshold (`m`),
    aiming to incentivize a return to a cooperative equilibrium. It acknowledges the
    end-game effect in repeated games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.forgiveness_threshold = 1
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_total_cooperators = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_total_cooperators >= self.game_description.m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures <= self.forgiveness_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_124(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the Collective Risk Dilemma.

    This strategy aims to achieve the collective reward 'k' for the group by
    initiating cooperation, rewarding sustained collective success, and cautiously
    punishing persistent collective failures. It also includes a self-preservation
    mechanism for the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveCollectiveReciprocity strategy.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters like n_players,
                                                          n_rounds, m, and k.
        """
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.consecutive_successes: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing records of past actions
                                            and payoffs for the player and opponents.
                                            None for the very first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate,
                    Action.D for Defect).
        """
        current_round_1_indexed: int = state.round_number + 1
        r: int = self.game_description.n_rounds
        m: int = self.game_description.m
        num_cooperators_last_round: int = 0
        if current_round_1_indexed > 1:
            previous_round_index = state.round_number - 1
            my_action_prev_round_bool = history.my_actions[previous_round_index]
            opponent_actions_prev_round_bools = history.opponent_actions[previous_round_index, :]
            num_cooperators_last_round = sum(opponent_actions_prev_round_bools) + int(my_action_prev_round_bool)
            if num_cooperators_last_round >= m:
                self.consecutive_failures = 0
                self.consecutive_successes += 1
            else:
                self.consecutive_successes = 0
                self.consecutive_failures += 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == r:
            return Action.D
        elif num_cooperators_last_round >= m:
            return Action.C
        elif self.consecutive_failures <= 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_125(BaseStrategy):
    """
    Collective Adaptive Reciprocity (CAR) strategy for the Collective Risk Dilemma.
    This strategy aims to foster and maintain collective cooperation while being
    robust against exploitation. It adapts its behavior based on the group's
    performance in the immediately preceding round and accounts for the game's
    finite nature.

    Decision Rules:
    1. First Round: Cooperate (C) to signal intent.
    2. Last Round: Defect (D) to avoid exploitation in a finite game.
    3. Intermediate Rounds:
       - If the minimum cooperators threshold (m) was met in the previous round,
         continue to Cooperate (C).
       - If the threshold was NOT met in the previous round, switch to Defect (D)
         to avoid being exploited and to signal for more cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        my_contribution_prev_round = 1 if my_action_prev_round_was_C else 0
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_contribution_prev_round
        if total_cooperators_prev_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_126(BaseStrategy):
    """
    The Threshold-Conditional Adaptive Reciprocator (TCAR) strategy aims to foster and sustain collective
    cooperation in the Collective Risk Dilemma. It initiates cooperation, maintains it when the collective
    threshold 'm' is met, and withdraws cooperation if the group consistently fails to meet the threshold,
    acting as a self-protective mechanism against exploitation and futile efforts. In the final round,
    it defects due to the absence of future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_to_meet_m = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            self.consecutive_failures_to_meet_m = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_cooperators_prev_round_opponents = sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_round = history.my_actions[state.round_number - 1]
        num_C_prev = num_cooperators_prev_round_opponents + (1 if my_action_prev_round else 0)
        if num_C_prev >= m_threshold:
            self.consecutive_failures_to_meet_m = 0
            return Action.C
        else:
            self.consecutive_failures_to_meet_m += 1
            if self.consecutive_failures_to_meet_m < 2:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_127(BaseStrategy):
    """
    Adaptive Threshold Enforcement (ATE) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize the collective reward 'k' by balancing initial
    cooperation, monitoring group success, and adapting behavior based on
    recent outcomes and individual contribution fairness. It includes specific
    logic for the first and last rounds of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.L = min(self.r - 1, 5) if self.r > 1 else 0
        self.recent_success_history = []
        self.my_action_history = []
        self.COOPERATION_THRESHOLD_HIGH = 0.7
        self.MY_CONTRIBUTION_FAIRNESS_THRESHOLD = float(self.m) / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.r:
            return Action.D
        n_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        my_prev_action_bool = history.my_actions[-1]
        was_successful_prev_round = n_cooperators_prev_round >= self.m
        self.recent_success_history.append(was_successful_prev_round)
        if len(self.recent_success_history) > self.L:
            self.recent_success_history.pop(0)
        self.my_action_history.append(my_prev_action_bool)
        if len(self.my_action_history) > self.L:
            self.my_action_history.pop(0)
        num_successful_rounds = sum((1 for flag in self.recent_success_history if flag is True))
        effective_success_rate = float(num_successful_rounds) / len(self.recent_success_history)
        num_my_cooperations = sum((1 for action_bool in self.my_action_history if action_bool is True))
        my_contribution_ratio = float(num_my_cooperations) / len(self.my_action_history)
        if effective_success_rate >= self.COOPERATION_THRESHOLD_HIGH:
            return Action.C
        elif my_contribution_ratio > self.MY_CONTRIBUTION_FAIRNESS_THRESHOLD:
            return Action.D
        elif n_cooperators_prev_round < self.m:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_128(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation by adapting its behavior
    based on whether the community project's threshold (m) was met in the previous round.
    It initiates cooperation, punishes observed free-riding by defecting, and offers
    forgiveness to re-establish collective action.

    Internal state:
    - current_phase: "Cooperation_Seeking" or "Punishment"
    """
    COOPERATION_SEEKING = 'Cooperation_Seeking'
    PUNISHMENT = 'Punishment'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_phase = self.COOPERATION_SEEKING

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if state.round_number == 0:
            return Action.C
        elif state.round_number == total_rounds - 1:
            return Action.D
        else:
            my_prev_action_was_cooperation = history.my_actions[-1]
            opponent_prev_cooperators_count = sum(history.opponent_actions[-1, :])
            C_prev = (1 if my_prev_action_was_cooperation else 0) + opponent_prev_cooperators_count
            if self.current_phase == self.COOPERATION_SEEKING:
                if C_prev >= min_cooperators_needed:
                    return Action.C
                else:
                    self.current_phase = self.PUNISHMENT
                    return Action.D
            elif self.current_phase == self.PUNISHMENT:
                if C_prev >= min_cooperators_needed:
                    self.current_phase = self.COOPERATION_SEEKING
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_129(BaseStrategy):
    """
    Adaptive Threshold Reciprocator strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain a cooperative equilibrium by
    initiating cooperation, rewarding collective success, and punishing collective
    failure. It balances individual incentive to defect with the collective benefit
    of meeting the cooperation threshold.

    Decision Rules:
    1.  First Round (round_number == 0): Cooperate (C) to initiate goodwill.
    2.  Intermediate Rounds (0 < round_number < n_rounds - 1):
        - If the number of cooperators in the previous round was less than 'm',
          Defect (D) to punish collective failure.
        - If the number of cooperators in the previous round was 'm' or more,
          Cooperate (C) to reinforce collective success.
    3.  Final Round (round_number == n_rounds - 1): Defect (D) due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        opponent_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_in_prev_round = history.my_actions[-1]
        cooperators_in_previous_round = opponent_cooperators_in_prev_round + my_action_in_prev_round
        if cooperators_in_previous_round < self.m:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_130(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.
    It adapts its behavior based on the group's performance in previous rounds, balancing
    the need for punishment with a mechanism for forgiveness to prevent permanent breakdowns
    in cooperation.

    Core Principles:
    1. Initiate Trust: Start with cooperation.
    2. Reward Success: Maintain cooperation after group success.
    3. Punish Failure: Defect immediately following a collective failure.
    4. Forgive and Re-initiate: After persistent collective failure, attempt to re-establish cooperation.
    5. Strategic End-Game: Adapt behavior in the final round based on recent collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.last_round_was_successful: bool = False
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        prev_round_idx = current_round_idx - 1
        num_cooperators_in_prev_round = int(history.my_actions[prev_round_idx]) + np.sum(history.opponent_actions[prev_round_idx, :])
        if num_cooperators_in_prev_round >= self.m:
            self.last_round_was_successful = True
            self.consecutive_failures = 0
        else:
            self.last_round_was_successful = False
            self.consecutive_failures += 1
        if current_round_idx == self.n_rounds - 1:
            if self.last_round_was_successful:
                return Action.C
            else:
                return Action.D
        elif self.last_round_was_successful:
            return Action.C
        elif self.consecutive_failures == 1:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_131(BaseStrategy):
    """
    Adaptive Collective Assurance (ACA) strategy for the Collective Risk Dilemma.

    This strategy is designed to balance individual short-term rationality with
    collective long-term benefit. It aims to foster cooperation by initiating
    with a cooperative stance, sustaining it when the collective goal is met,
    and strategically defecting to avoid exploitation or to efficiently free-ride
    when the collective good is robustly secured. A key feature is forcing
    free-riders to contribute when collective success is fragile (exactly at 'm').
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        else:
            prev_round_my_action = history.my_actions[state.round_number - 1]
            prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            total_coop_prev = np.sum(prev_round_opponent_actions) + int(prev_round_my_action)
            if total_coop_prev >= m_threshold:
                if prev_round_my_action == Action.C:
                    return Action.C
                elif total_coop_prev == m_threshold:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_COLLECTIVE_132(BaseStrategy):
    """
    The "Collective Threshold Guard" strategy aims to establish and maintain collective success
    (meeting the `m` cooperator threshold) through conditional cooperation. It adapts to
    collective performance by either free-riding (when success is robust), sustaining
    cooperation (when success is fragile), or punishing persistent failure through defection.
    It handles the first and last rounds as edge cases.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_threshold: int = 2
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round == 0:
            self.consecutive_failures = 0
            return Action.C
        my_last_action_was_C = history.my_actions[-1]
        opponents_cooperating_last_round = np.sum(history.opponent_actions[-1, :])
        observed_cooperators_last_round = opponents_cooperating_last_round + (1 if my_last_action_was_C else 0)
        if observed_cooperators_last_round >= min_cooperators_m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if current_round == total_rounds - 1:
            return Action.D
        elif self.consecutive_failures > 0:
            if self.consecutive_failures >= self.punishment_threshold:
                return Action.D
            else:
                return Action.C
        elif observed_cooperators_last_round > min_cooperators_m:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_133(BaseStrategy):
    """
    Sentinel Threshold strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain a state where the minimum
    cooperation threshold (`m`) is met, thus securing the `k` reward for all
    players. It starts cooperatively, reinforces successful collective action,
    and provides opportunities for recovery from minor failures. However, it
    will defect in the face of persistent or significant collective defection
    to avoid being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.coop_push_range = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == 1:
            return Action.C
        if current_round == self.r:
            return Action.D
        prev_round_idx = state.round_number - 1
        N_C_prev = sum(history.opponent_actions[prev_round_idx, :]) + (1 if history.my_actions[prev_round_idx] else 0)
        my_action_prev_is_C = history.my_actions[prev_round_idx]
        if N_C_prev >= self.m:
            return Action.C
        elif N_C_prev >= self.m - self.coop_push_range and my_action_prev_is_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_134(BaseStrategy):
    """
    The Adaptive Collective Stabilizer (ACS) strategy is designed for the Collective Risk Dilemma.
    It initiates with cooperation, maintains full cooperation when achieved, punishes free-riding
    when the collective threshold 'm' is met but not all contribute, and attempts to re-establish
    cooperation after a collective failure to meet the 'm' threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_prev_action_was_cooperate = history.my_actions[state.round_number - 1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
        n_c_prev = int(my_prev_action_was_cooperate) + opponent_prev_cooperators
        if n_c_prev == self.n_players:
            return Action.C
        elif n_c_prev >= self.m_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_135(BaseStrategy):
    """
    Forgiving Collective Tit-for-Tat (FCTfT) strategy for the Collective Risk Dilemma.

    This strategy prioritizes collective success by aiming to consistently meet the
    'm' cooperator threshold. It defaults to cooperation. If collective cooperation
    fails (fewer than 'm' cooperators), it employs a measured, short-term punishment
    mechanism: it defects for one round if it cooperated in the failed round. If it
    defected in the failed round, it quickly returns to cooperation to re-establish
    the desired state (the "forgiving" aspect). It defects in the final round to
    protect its own payoff against rational self-interested agents, acknowledging
    the "last round problem."
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_action_last_round: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        if state.round_number == 0:
            current_action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            current_action = Action.D
        else:
            prev_round_idx = state.round_number - 1
            num_cooperators_opponents_prev = np.sum(history.opponent_actions[prev_round_idx, :])
            my_action_prev_is_cooperate = history.my_actions[prev_round_idx]
            num_cooperators_prev_round = num_cooperators_opponents_prev + (1 if my_action_prev_is_cooperate else 0)
            if num_cooperators_prev_round >= self.game_description.m:
                current_action = Action.C
            elif self.my_action_last_round == Action.C:
                current_action = Action.D
            else:
                current_action = Action.C
        self.my_action_last_round = current_action
        return current_action

class Strategy_COLLECTIVE_136(BaseStrategy):
    """
    Adaptive Collective Threshold Strategy (ACTS) for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation by:
    1. Cooperating in the first round to initiate collective success.
    2. Continuing to cooperate in subsequent rounds if the collective threshold 'm' was met in the previous round.
    3. Defecting in subsequent rounds if the collective threshold 'm' was NOT met in the previous round,
       to punish failure and protect individual endowment.
    4. Always defecting in the final round due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_137(BaseStrategy):
    """
    Adaptive Collective Assurance (ACA) strategy for the Collective Risk Dilemma.
    It defaults to cooperation, and if the collective project consistently fails
    (fewer than 'm' cooperators for X consecutive rounds), it temporarily defects
    for Y rounds to signal dissatisfaction and encourage cooperation, before
    returning to its cooperative stance. It does not attempt to free-ride.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FAILURE_THRESHOLD_X: int = 2
        self.PUNISHMENT_DURATION_Y: int = 2
        self.rounds_failed_consecutively: int = 0
        self.in_punishment_mode: bool = False
        self.punishment_end_round: int = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_C_count = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if prev_C_count < self.game_description.m:
            self.rounds_failed_consecutively += 1
        else:
            self.rounds_failed_consecutively = 0
        if self.in_punishment_mode:
            if state.round_number > self.punishment_end_round:
                self.in_punishment_mode = False
                self.punishment_end_round = -1
        elif self.rounds_failed_consecutively >= self.FAILURE_THRESHOLD_X:
            self.in_punishment_mode = True
            self.punishment_end_round = state.round_number + self.PUNISHMENT_DURATION_Y - 1
        if self.in_punishment_mode:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_139(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy promotes collective success by starting with cooperation,
    tolerating minor failures, but temporarily defecting as a "protest" if collective efforts consistently
    fail. After a protest, it attempts to re-establish cooperation. It defects in the final round to
    maximize payoff in a finitely repeated game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._F_THRESHOLD: int = 2
        self._current_state: str = 'Cooperating'
        self._consecutive_failures_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        c_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self._current_state == 'Cooperating':
            if c_prev >= m_threshold:
                self._consecutive_failures_count = 0
                return Action.C
            else:
                self._consecutive_failures_count += 1
                if self._consecutive_failures_count >= self._F_THRESHOLD:
                    self._current_state = 'Protesting'
                    self._consecutive_failures_count = 0
                    return Action.D
                else:
                    return Action.C
        elif self._current_state == 'Protesting':
            self._current_state = 'Cooperating'
            self._consecutive_failures_count = 0
            return Action.C

class Strategy_COLLECTIVE_140(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy balances collective cooperation
    with self-preservation. It primarily cooperates but tracks consecutive failures
    to meet the 'm' threshold. It tolerates a specific number of consecutive failures
    before reverting to defection to prevent exploitation. Special rules apply
    to the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.max_failure_tolerance: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        observed_cooperators_previous_round = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
        if observed_cooperators_previous_round >= min_cooperators_needed:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures < self.max_failure_tolerance:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_141(BaseStrategy):
    """
    The Collective Conditional Contributor (CCC) strategy for the Collective Risk Dilemma.
    It initiates with cooperation and subsequently adapts its behavior based on the
    collective outcome of the previous round. If the collective met the cooperation
    threshold (m) in the previous round, it continues to cooperate. If the collective
    failed to meet the threshold, it defects to avoid exploitation and signal the
    need for increased cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player and opponents.
                     It is None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        else:
            last_round_total_cooperators = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if last_round_total_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_142(BaseStrategy):
    """
    The Collective Enforcer (CE) strategy aims to foster and sustain collective
    cooperation to achieve the shared reward 'k'. It prioritizes the collective
    outcome but incorporates adaptive rules to react to insufficient cooperation
    and protect itself from consistent exploitation, balancing collective good
    with individual robustness.

    Decision rules per round (t):
    - Round 1 (t=0): Always Cooperate (C).
    - Subsequent Rounds (t > 0):
        - If previous round's total cooperators >= m (threshold met): Cooperate (C).
        - If previous round's total cooperators < m (threshold NOT met):
            - If this player cooperated in the previous round: Defect (D) (punitive).
            - If this player defected in the previous round: Cooperate (C) (rectifying).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        if state.round_number == 0:
            current_action = Action.C
            self._my_last_action = current_action
            return current_action
        num_cooperators_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        my_action_in_prev_round = self._my_last_action
        if num_cooperators_prev_round >= self.game_description.m:
            current_action = Action.C
        elif my_action_in_prev_round == Action.C:
            current_action = Action.D
        else:
            current_action = Action.C
        self._my_last_action = current_action
        return current_action

class Strategy_COLLECTIVE_143(BaseStrategy):
    """
    The Dynamic Collective Effort (DCE) strategy for the Collective Risk Dilemma.
    It initiates with cooperation, maintains it when the collective threshold is met,
    shows patience for a few failures, but defects if collective effort consistently
    falls short to protect against exploitation. In the final round, it defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_misses_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators_in_previous_round = history.my_actions[-1] + sum(history.opponent_actions[-1, :])
        if cooperators_in_previous_round >= self.game_description.m:
            self.consecutive_misses_count = 0
        else:
            self.consecutive_misses_count += 1
        if cooperators_in_previous_round >= self.game_description.m:
            return Action.C
        elif self.consecutive_misses_count <= 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_144(BaseStrategy):
    """
    Adaptive Threshold Enforcement (ATE) strategy for the Collective Risk Dilemma.
    It aims to foster cooperation by starting cooperatively, then adapting its behavior
    based on the observed collective outcome of previous rounds. It employs a two-pronged
    punitive approach:
    1. Punish Collective Failure: If the 'm' threshold is not met, the strategy defects.
    2. Deter Persistent Free-Riding: If the 'm' threshold is met but a significant number
       of players consistently free-ride, the strategy temporarily defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_failed_threshold: int = 0
        self.rounds_excessive_free_riding: int = 0
        self.FREE_RIDER_TOLERANCE: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        previous_round_cooperators = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
        if previous_round_cooperators < m:
            self.rounds_failed_threshold += 1
            self.rounds_excessive_free_riding = 0
        else:
            self.rounds_failed_threshold = 0
            if previous_round_cooperators < n:
                self.rounds_excessive_free_riding += 1
            else:
                self.rounds_excessive_free_riding = 0
        if self.rounds_failed_threshold >= 1:
            return Action.D
        elif self.rounds_excessive_free_riding >= self.FREE_RIDER_TOLERANCE:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_145(BaseStrategy):
    """
    The Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.
    This strategy is a conditional cooperator that learns from the collective outcome
    of the previous round and its own role in that outcome. It balances initial
    willingness to cooperate with self-protection and a persistent drive to
    re-establish cooperation after failure, ultimately defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        c_prev = num_opponent_cooperators_prev + int(my_action_prev)
        if c_prev >= self.m_threshold:
            return Action.C
        elif my_action_prev == True:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_146(BaseStrategy):
    """
    The Adaptive Collective Steward (ACS) strategy for the Collective Risk Dilemma.
    It aims to achieve the collective good (meeting the 'm' cooperator threshold
    to secure the 'k' reward) while being adaptive and robust enough to prevent
    consistent exploitation by purely self-interested opponents.

    Core principles:
    1.  Initial Trust: Starts by cooperating to signal willingness and establish
        a cooperative baseline.
    2.  Collective Goal First: Prioritizes meeting the 'm' threshold above all else,
        as this benefits everyone.
    3.  Patient Recovery: If the collective goal is missed, attempts to recover
        cooperation.
    4.  Strategic Defection: Only defects in two specific scenarios:
        a.  Despair: If cooperation has completely collapsed despite persistent efforts.
        b.  Persistent Exploitation: If this strategy is consistently contributing
            (meaning 'm' is met), but others are persistently free-riding. This
            defection serves as a signal to encourage contribution from others.
    5.  Last Round Consideration: Adjusts behavior for the final round based on
        recent collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action = Action.C
        self._consecutive_rounds_m_failed_despite_my_cooperation: int = 0
        self._consecutive_rounds_free_riding_observed_by_me: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_1_indexed = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self._my_last_action = Action.C
            return Action.C
        my_action_prev_round = Action.C if history.my_actions[-1] else Action.D
        C_prev = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        observed_defectors_prev = n_players - C_prev
        if my_action_prev_round == Action.C:
            if C_prev >= m:
                self._consecutive_rounds_m_failed_despite_my_cooperation = 0
                if observed_defectors_prev > 0:
                    self._consecutive_rounds_free_riding_observed_by_me += 1
                else:
                    self._consecutive_rounds_free_riding_observed_by_me = 0
            else:
                self._consecutive_rounds_m_failed_despite_my_cooperation += 1
                self._consecutive_rounds_free_riding_observed_by_me = 0
        else:
            self._consecutive_rounds_m_failed_despite_my_cooperation = 0
            self._consecutive_rounds_free_riding_observed_by_me = 0
        if current_game_round_1_indexed == n_rounds:
            if C_prev >= m:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
            self._my_last_action = chosen_action
            return chosen_action
        else:
            if self._consecutive_rounds_m_failed_despite_my_cooperation >= 2 and C_prev <= 1:
                chosen_action = Action.D
                self._my_last_action = chosen_action
                return chosen_action
            if self._consecutive_rounds_free_riding_observed_by_me >= 3:
                chosen_action = Action.D
                self._my_last_action = chosen_action
                self._consecutive_rounds_free_riding_observed_by_me = 0
                return chosen_action
            chosen_action = Action.C
            self._my_last_action = chosen_action
            return chosen_action

class Strategy_COLLECTIVE_147(BaseStrategy):
    """
    The Adaptive Collective Consensus (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by starting with a cooperative move,
    and then adapting based on the collective's success in meeting the 'm' threshold.
    It provides a second chance after a single failure but defects if failures
    persist, protecting individual payoff and signaling the need for collective effort.
    It also accounts for end-game dynamics by defecting in the last round and
    making a strategic decision in the second to last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.consecutive_failures = 0
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == n_rounds - 2:
            round_idx_for_C_r_minus_2 = n_rounds - 3
            cooperators_in_r_minus_2 = sum(history.opponent_actions[round_idx_for_C_r_minus_2, :]) + int(history.my_actions[round_idx_for_C_r_minus_2])
            if cooperators_in_r_minus_2 >= m:
                return Action.C
            else:
                return Action.D
        prev_round_idx = current_round_0_indexed - 1
        cooperators_in_prev_round = sum(history.opponent_actions[prev_round_idx, :]) + int(history.my_actions[prev_round_idx])
        if cooperators_in_prev_round >= m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_148(BaseStrategy):
    """
    The Adaptive Threshold Contributor (ATC) strategy aims to consistently achieve the collective reward 'k'
    in the Collective Risk Dilemma by adapting its behavior based on the group's past actions.
    It initiates cooperation, responds to collective failure by increasing cooperation, and optimizes
    individual contribution (rotating the burden) when the threshold is met. In the final round,
    it acts selfishly due to end-game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_cooperators_prev = int(my_action_prev) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev >= m_threshold:
            if my_action_prev:
                if num_cooperators_prev == m_threshold:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_149(BaseStrategy):
    """
    Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.

    ACE aims to establish and maintain a cooperative equilibrium by demonstrating
    initial willingness to contribute, reinforcing collective success, and
    punishing collective failure to meet the cooperation threshold. It adapts
    its behavior based on whether the collective met the minimum cooperation
    threshold (`m`) in the previous round, while acknowledging the unique
    dynamics of the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_t == 1:
            return Action.C
        if current_t == total_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        num_cooperators_in_previous_round = int(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx, :])
        if num_cooperators_in_previous_round >= min_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_150(BaseStrategy):
    """
    Dynamic Thresholding & Reciprocity (DTR) strategy for the Collective Risk Dilemma.
    This strategy aims to consistently achieve the collective reward `k` by promoting and
    maintaining the `m` cooperators threshold. It starts with an initial act of cooperation
    to test the environment. It responds to collective success by cautiously probing for
    opportunities to maximize individual payoff, while ensuring the threshold is not jeopardized.
    It responds to collective failure by giving the group a second chance, but will resort to
    self-preservation if cooperation consistently falters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met_buffer: int = 1
        self.failure_leniency_rounds: int = 1
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_0_indexed == 0:
            self.consecutive_failures = 0
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        last_round_my_action_is_C = history.my_actions[-1]
        last_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        last_round_NC = (1 if last_round_my_action_is_C else 0) + last_round_opponent_cooperators
        if last_round_NC >= min_cooperators_m:
            self.consecutive_failures = 0
            if last_round_NC > min_cooperators_m + self.threshold_met_buffer:
                return Action.D
            else:
                return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures <= self.failure_leniency_rounds:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_151(BaseStrategy):
    """
    Adaptive Threshold Enforcement Strategy for Collective Risk Dilemma.

    This strategy aims to facilitate the collective success of meeting the cooperation
    threshold (m) to secure the reward k. It starts with an initial attempt at
    cooperation and then adaptively responds to the group's success or failure in
    meeting the threshold in previous rounds, minimizing individual losses when the
    collective goal is not met. It defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        elif current_round_t == self.game_description.n_rounds:
            return Action.D
        else:
            N_C_previous_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            if N_C_previous_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_152(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and sustain a cooperative equilibrium where
    the minimum cooperation threshold (`m`) is met, thus securing the `k` bonus
    for all players. It does this by starting cooperatively, rewarding collective
    success, forgiving single lapses in cooperation, and protecting against
    sustained exploitation by temporarily withdrawing cooperation. It acknowledges
    the end-game effect in the final round.

    State Variable:
    - self.failure_streak: An integer counter, initialized to 0. Tracks
      consecutive rounds where the collective threshold (`m`) was NOT met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[state.round_number - 1]
        opponent_prev_actions_were_cooperate = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(my_prev_action_was_cooperate) + np.sum(opponent_prev_actions_were_cooperate)
        if num_cooperators_prev_round >= min_cooperators_needed:
            self.failure_streak = 0
            return Action.C
        else:
            self.failure_streak += 1
            if self.failure_streak == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_153(BaseStrategy):
    """
    Adaptive Threshold Cooperator with End-Game Realism (ATCER) strategy.

    This strategy aims to foster cooperation to achieve the collective benefit 'k',
    while being resilient against exploitation and adapting to the observed behavior
    of other players. It operates on principles of initial trust, reciprocal cooperation,
    conditional punishment, and rational end-game play.

    Key components:
    - Initial Cooperation: Always cooperates in the first round to signal willingness.
    - Rewarding Success: Continues to cooperate if the 'm' cooperator threshold was met
      in the previous round.
    - Conditional Punishment: Switches to defection if the 'm' threshold fails to be met
      for 'm' consecutive rounds, preventing exploitation.
    - Forgiveness: Tolerates a limited number of consecutive failures (less than 'm')
      before initiating punishment, giving others a chance to re-cooperate.
    - End-Game Realism: Defects in the absolute last round and applies cautious logic
      in the penultimate round, recognizing the reduced incentive for cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._rounds_since_last_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round == 1:
            self._rounds_since_last_success = 0
            return Action.C
        n_cooperated_last_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if n_cooperated_last_round >= m_threshold:
            self._rounds_since_last_success = 0
        else:
            self._rounds_since_last_success += 1
        if current_round == total_rounds:
            return Action.D
        elif current_round == total_rounds - 1:
            if n_cooperated_last_round >= m_threshold:
                return Action.C
            else:
                return Action.D
        elif n_cooperated_last_round >= m_threshold:
            return Action.C
        elif self._rounds_since_last_success >= m_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_154(BaseStrategy):
    """
    Adaptive Collective Trigger (ACT) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain the collective good (meeting the 'm'
    cooperator threshold) by conditionally cooperating. It signals willingness to
    contribute to collective success and punishes collective failure, also accounting
    for the end-game effect in finitely repeated games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveCollectiveTrigger strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing historical actions and payoffs for the player and opponents,
                     or None if it's the very first round (state.round_number == 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        num_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_index]) + history.my_actions[previous_round_index]
        if num_cooperators_previous_round >= min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_155(BaseStrategy):
    """
    Resilient Collective Cooperation strategy aims to foster and maintain cooperation by defaulting to cooperation,
    tolerating minor collective failures, but punishing persistent non-cooperation. It incorporates backward
    induction for the end-game.
    """

    class PlayerStatus(Enum):
        COOPERATING = 1
        PUNISHING = 0
    FAILURE_TOLERANCE: int = 2
    PUNISHMENT_DURATION: int = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_status: Strategy.PlayerStatus = self.PlayerStatus.COOPERATING
        self.rounds_since_last_success: int = 0
        self.punishment_countdown: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0:
            num_cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if num_cooperators_in_prev_round >= self.game_description.m:
                self.rounds_since_last_success = 0
                if self.current_status == self.PlayerStatus.PUNISHING:
                    self.current_status = self.PlayerStatus.COOPERATING
                    self.punishment_countdown = 0
            else:
                self.rounds_since_last_success += 1
                if self.current_status == self.PlayerStatus.COOPERATING:
                    if self.rounds_since_last_success >= self.FAILURE_TOLERANCE:
                        self.current_status = self.PlayerStatus.PUNISHING
                        self.punishment_countdown = self.PUNISHMENT_DURATION
                else:
                    self.punishment_countdown -= 1
                    if self.punishment_countdown <= 0:
                        self.current_status = self.PlayerStatus.COOPERATING
                        self.rounds_since_last_success = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.current_status == self.PlayerStatus.COOPERATING:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_156(BaseStrategy):
    """
    Collective Trust Builder with Conditional Enforcement (CTBCE) strategy.

    This strategy aims to foster collective cooperation in the Collective Risk Dilemma
    by starting cooperatively, prioritizing the meeting of the 'm' cooperator threshold,
    and conditionally punishing free-riders or withdrawing cooperation in the face of
    persistent collective failure, while accounting for end-game effects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.player_cooperation_counts = np.zeros(self.n, dtype=int)
        self.player_defection_counts = np.zeros(self.n, dtype=int)
        self.my_last_action: Action | None = None
        self.last_round_threshold_met: bool = False
        self.last_round_cooperator_count: int = 0
        self.failure_to_meet_threshold_streak: int = 0
        self.player_id: int = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        current_round_one_indexed = current_round_zero_indexed + 1
        if current_round_zero_indexed == 0:
            self.my_last_action = Action.C
            return Action.C
        my_action_prev_round_bool = history.my_actions[-1]
        my_action_prev_round_enum = Action.C if my_action_prev_round_bool else Action.D
        opponent_actions_prev_round_bool = history.opponent_actions[-1, :]
        if self.player_id == -1:
            pass
        global_player_indices = [i for i in range(self.n) if i != self.player_id]
        if my_action_prev_round_enum == Action.C:
            self.player_cooperation_counts[self.player_id] += 1
        else:
            self.player_defection_counts[self.player_id] += 1
        for opp_history_idx, is_cooperating in enumerate(opponent_actions_prev_round_bool):
            global_idx = global_player_indices[opp_history_idx]
            if is_cooperating:
                self.player_cooperation_counts[global_idx] += 1
            else:
                self.player_defection_counts[global_idx] += 1
        self.my_last_action = my_action_prev_round_enum
        self.last_round_cooperator_count = np.sum(opponent_actions_prev_round_bool) + (1 if my_action_prev_round_bool else 0)
        self.last_round_threshold_met = self.last_round_cooperator_count >= self.m
        if self.last_round_threshold_met:
            self.failure_to_meet_threshold_streak = 0
        else:
            self.failure_to_meet_threshold_streak += 1
        if current_round_one_indexed == self.r:
            return Action.D
        if current_round_one_indexed == self.r - 1:
            if self.last_round_threshold_met and self.last_round_cooperator_count > self.m:
                return Action.D
            else:
                return Action.C
        if self.last_round_threshold_met:
            if self.my_last_action == Action.D:
                return Action.C
            else:
                num_persistent_defectors = 0
                rounds_played_for_stats = current_round_zero_indexed
                for player_idx in range(self.n):
                    if player_idx == self.player_id:
                        continue
                    if self.player_defection_counts[player_idx] > self.player_cooperation_counts[player_idx] and self.player_defection_counts[player_idx] > rounds_played_for_stats / 3:
                        num_persistent_defectors += 1
                if num_persistent_defectors >= 1 and self.last_round_cooperator_count > self.m:
                    return Action.D
                else:
                    return Action.C
        elif self.my_last_action == Action.D:
            return Action.C
        elif self.failure_to_meet_threshold_streak <= 1 or self.last_round_cooperator_count >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_157(BaseStrategy):
    """
    Adaptive Collective Enforcement strategy for the Collective Risk Dilemma.
    It prioritizes collective reward by proactive cooperation, offering forgiveness for initial failures
    to meet the cooperation threshold, and eventually sanctioning prolonged non-cooperation.
    It actively contributes to the collective burden when success is achieved and recognizes
    the altered incentives in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_threshold_rounds = 2
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        if history is not None:
            n_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
            if n_cooperators_prev >= m:
                self.consecutive_failures = 0
                return Action.C
            else:
                self.consecutive_failures += 1
                if self.consecutive_failures <= self.failure_threshold_rounds:
                    return Action.C
                else:
                    return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_158(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Forgiveness (ACR-F) for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation to secure the 'k' reward.
    It initiates with cooperation, rewards collective success, forgives a single round of
    collective failure, and punishes persistent failure by defecting. It anticipates
    end-game effects by defecting in the final round.

    State variables:
    - consecutive_failures_count: Tracks the number of consecutive rounds where the
                                  collective threshold 'm' was not met. This counter
                                  resets to 0 upon a round of collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_my_action_is_C = history.my_actions[current_round_0_indexed - 1]
        prev_round_opponent_actions_are_C = history.opponent_actions[current_round_0_indexed - 1, :]
        prev_round_num_cooperators = int(prev_round_my_action_is_C) + np.sum(prev_round_opponent_actions_are_C)
        if prev_round_num_cooperators >= m_threshold:
            self.consecutive_failures_count = 0
        else:
            self.consecutive_failures_count += 1
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        elif self.consecutive_failures_count == 0:
            return Action.C
        elif self.consecutive_failures_count == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_159(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to promote and sustain cooperation by defaulting to cooperation.
    It monitors the group's performance and adapts by withdrawing cooperation only if
    the group consistently fails to meet the required threshold, signaling that cooperation
    is not being reciprocated or is unsustainable. It also incorporates robustness for
    the final round of a finite game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.F_threshold = max(3, math.ceil(self.game_description.m / 2))
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_in_prev_round < self.game_description.m:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.consecutive_failures >= self.F_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_160(BaseStrategy):
    """
    The "Adaptive Collective Stabilizer with Restart" (ACSR) strategy for the Collective Risk Dilemma.
    It aims to balance individual rationality with the collective good by initially cooperating,
    sustaining cooperation during success, punishing collective failure by defecting, and
    periodically attempting to re-establish cooperation after prolonged failure.
    It defects in the final round due to endgame rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.restart_interval: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number > 0:
            previous_round_index = current_round_number - 1
            my_previous_action_was_C = history.my_actions[previous_round_index] == Action.C
            opponents_previous_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
            total_cooperators_previous_round = opponents_previous_cooperators + (1 if my_previous_action_was_C else 0)
            if total_cooperators_previous_round >= self.game_description.m:
                self.consecutive_failures = 0
            else:
                self.consecutive_failures += 1
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures % self.restart_interval == 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_161(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    It initiates cooperation, sustains it when the collective goal (m) is met,
    retracts cooperation upon collective failure, and strategically defects
    in the final round for self-preservation. This strategy aims to foster
    and maintain cooperation while being adaptive and robust against exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including the round number.
            history: An object containing past actions and payoffs for this player and opponents.
                     It is None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_0_indexed - 1
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
            my_action_prev_round_was_C = history.my_actions[previous_round_index]
            num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
            if num_cooperators_prev_round >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_162(BaseStrategy):
    """
    The Adaptive Collective Risk Averter (ACRA) strategy is designed to foster and maintain collective cooperation
    in the Collective Risk Dilemma. It prioritizes the collective good by initially cooperating and aiming to sustain it.
    The strategy understands that while individual defection might offer a marginal gain when the collective threshold is met,
    sustained collective cooperation leads to higher overall payoffs for everyone. To achieve this, it employs a system
    of forgiveness for minor group lapses and temporary punishment for significant failures, combined with a willingness
    to re-establish cooperation after punishment. The strategy also incorporates standard game theory for the final round
    to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PUNISHMENT_DURATION = 2
        self.FORGIVENESS_THRESHOLD = 1
        self.punishment_active = False
        self.punishment_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == total_rounds - 1:
            return Action.D
        prev_round_index = current_round_zero_indexed - 1
        cooperators_observed_last_round = np.sum(history.opponent_actions[prev_round_index, :]) + history.my_actions[prev_round_index]
        if cooperators_observed_last_round >= min_cooperators_needed:
            self.punishment_active = False
            self.punishment_countdown = 0
            return Action.C
        elif self.punishment_active:
            if self.punishment_countdown > 0:
                self.punishment_countdown -= 1
                return Action.D
            else:
                self.punishment_active = False
                return Action.C
        elif min_cooperators_needed - cooperators_observed_last_round <= self.FORGIVENESS_THRESHOLD:
            return Action.C
        else:
            self.punishment_active = True
            self.punishment_countdown = self.PUNISHMENT_DURATION - 1
            return Action.D

class Strategy_COLLECTIVE_163(BaseStrategy):
    """
    The Adaptive Collective Maintenance (ACM) strategy promotes and sustains cooperation
    in the Collective Risk Dilemma by reacting to the observed collective outcome of the
    previous round. It aims to achieve the collective benefit (k reward) and is robust
    against exploitation, particularly in the endgame.

    - Round 1: Always Cooperate (C) to initiate cooperation.
    - Intermediate Rounds (2 to r-1):
        - If the previous round met the 'm' cooperator threshold: Cooperate (C).
        - If the previous round failed to meet the 'm' cooperator threshold: Defect (D)
          as a signal/punishment.
    - Final Round (r): Always Defect (D) due to the end-game effect, protecting against exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            my_previous_action_cooperated = int(history.my_actions[current_round_0_indexed - 1])
            opponent_previous_cooperators = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
            total_cooperators_previous_round = my_previous_action_cooperated + opponent_previous_cooperators
            if total_cooperators_previous_round >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_164(BaseStrategy):
    """
    The Adaptive Collective Averter (ACA) strategy aims to establish and maintain
    collective cooperation to consistently achieve the 'k' reward. It starts by
    signaling willingness to cooperate. It then adapts its behavior based on
    past collective outcomes: reinforcing success, and implementing a temporary
    "punishment" phase of defection when cooperation consistently fails to meet
    the required threshold 'm'. A key aspect is its forgiveness mechanism to allow
    re-establishment of cooperation after a breakdown.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_mode: bool = False
        self.punishment_rounds_left: int = 0
        self.rounds_to_punish: int = 3
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.punishment_mode = False
            self.punishment_rounds_left = 0
            self.consecutive_failures = 0
            return Action.C
        if self.punishment_mode:
            if self.punishment_rounds_left > 0:
                self.punishment_rounds_left -= 1
                return Action.D
            else:
                self.punishment_mode = False
                self.consecutive_failures = 0
                return Action.C
        my_action_prev = history.my_actions[-1]
        opponent_actions_prev = history.opponent_actions[-1, :]
        C_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        if C_prev >= self.game_description.m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures >= 2:
                self.punishment_mode = True
                self.punishment_rounds_left = self.rounds_to_punish
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_165(BaseStrategy):
    """
    Adaptive Collective Security with Retaliation (ACSR) strategy.

    This strategy aims to maximize the collective benefit (securing the 'k' reward)
    while being resilient to free-riding and adaptive to varying levels of cooperation
    among opponents in a tournament setting. It prioritizes meeting the cooperation
    threshold 'm', adapts cooperation based on past success or failure, and retaliates
    if the collective repeatedly fails despite the player's efforts.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_action_i: Action | None = None
        self.consecutive_failures_count: int = 0
        self.max_tolerated_failures: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            chosen_action = Action.C
            self.prev_action_i = chosen_action
            self.consecutive_failures_count = 0
            return chosen_action
        n_c_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        threshold_met_prev = n_c_prev >= self.game_description.m
        if threshold_met_prev:
            self.consecutive_failures_count = 0
        else:
            self.consecutive_failures_count += 1
        chosen_action: Action
        if current_round_idx == self.game_description.n_rounds - 1:
            if threshold_met_prev:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        elif self.consecutive_failures_count > self.max_tolerated_failures:
            chosen_action = Action.D
        elif threshold_met_prev:
            chosen_action = self.prev_action_i
        else:
            chosen_action = Action.C
        self.prev_action_i = chosen_action
        return chosen_action

class Strategy_COLLECTIVE_166(BaseStrategy):
    """
    Adaptive Collective Steward strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation by adapting its behavior
    based on the group's past success in meeting the cooperation threshold. It cooperates
    in the first round to signal willingness, continues cooperating if the threshold was
    met in the previous round (reinforcing success), defects if the threshold was not met
    (as a collective punishment and self-preservation), and defects in the final round
    due to the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Steward strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, m (threshold), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing records of past actions and payoffs.
                     Will be None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds_1_indexed = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds_1_indexed - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        C_prev = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_cooperate else 0)
        if C_prev >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_167(BaseStrategy):
    """
    Implements the "Adaptive Collective Maintainer" strategy for the Collective Risk Dilemma.

    This strategy prioritizes the achievement and maintenance of the collective reward 'k'.
    It starts by cooperating in the first round to initiate collective action.
    In subsequent middle rounds, it observes the previous round's outcome:
    - If the minimum number of cooperators 'm' was met, it continues to cooperate to sustain
      the collective success.
    - If 'm' was not met, it defects to protect itself from wasted effort and to signal
      the need for greater collective contribution.
    In the final round, it defects to prevent exploitation, acknowledging the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Maintainer strategy.

        Args:
            game_description: An object containing the game's parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for this player
                     and opponents, or None if it's the very first round.

        Returns:
            An Action (Action.C for Cooperate or Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        n_cooperators_last_round = np.sum(history.opponent_actions[previous_round_idx, :]) + history.my_actions[previous_round_idx]
        if n_cooperators_last_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_168(BaseStrategy):
    """
    Adaptive Collective Responder (ACR) strategy for the Collective Risk Dilemma.
    It initiates cooperation, rewards collective success, shows resilience to near-failures,
    and self-protects against exploitation and widespread failure. Defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_action: Action | None = None
        self.consecutive_failures_as_c: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number > 0:
            my_last_action_was_c = self.last_action == Action.C
            N_C_prev = np.sum(history.opponent_actions[-1, :]) + (1 if my_last_action_was_c else 0)
            if my_last_action_was_c and N_C_prev < m:
                self.consecutive_failures_as_c += 1
            else:
                self.consecutive_failures_as_c = 0
        else:
            self.consecutive_failures_as_c = 0
            self.last_action = None
        my_current_action: Action
        if current_round_1_indexed == total_rounds:
            my_current_action = Action.D
        elif current_round_1_indexed == 1:
            my_current_action = Action.C
        elif N_C_prev >= m:
            my_current_action = Action.C
        else:
            MAX_CONSECUTIVE_FAILURES_AS_C = 1
            if self.last_action == Action.C and self.consecutive_failures_as_c >= MAX_CONSECUTIVE_FAILURES_AS_C:
                my_current_action = Action.D
            else:
                HOPE_THRESHOLD = m - 1
                if N_C_prev >= HOPE_THRESHOLD:
                    my_current_action = Action.C
                else:
                    my_current_action = Action.D
        self.last_action = my_current_action
        return my_current_action

class Strategy_COLLECTIVE_169(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for the Collective Risk Dilemma.
    This strategy is designed to foster and maintain collective cooperation by initiating
    cooperation, reinforcing it upon success, offering forgiveness for minor shortfalls,
    and enacting defensive punishment for significant collective failures. It aims to steer
    the group towards a stable cooperative equilibrium.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                         n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Contains the current round number (0-indexed).
            history (PlayerHistory | None): Contains records of previous rounds' actions
                                            and payoffs for all players. None for the first round.

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        prev_round_index = state.round_number - 1
        opponents_cooperated_prev_round = np.sum(history.opponent_actions[prev_round_index, :])
        my_action_prev_round_as_int = 1 if history.my_actions[prev_round_index] else 0
        total_cooperators_prev_round = opponents_cooperated_prev_round + my_action_prev_round_as_int
        if total_cooperators_prev_round >= min_cooperators_needed - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_170(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) Strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by:
    - Initiating with cooperation to establish a collective base.
    - Sustaining cooperation when the group achieves its threshold.
    - Taking responsibility by cooperating in the current round if one defected
      in a previous round that saw collective failure.
    - Identifying and responding to persistent free-riders (players who repeatedly
      defect when the group fails) by conditionally withdrawing cooperation
      only when their number exceeds the group's tolerance for defectors.
    - Employing a standard end-game defection strategy for the final round of
      the finite repeated game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.R_tolerance = 2
        self.failure_defection_count: NDArray[np.int_] = np.zeros(self.n - 1, dtype=int)
        self.my_last_action: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.my_last_action = Action.C
            return Action.C
        if current_round == self.r - 1:
            self.my_last_action = Action.D
            return Action.D
        previous_round_index = current_round - 1
        my_action_prev: bool = history.my_actions[previous_round_index]
        opponent_actions_prev: NDArray[np.bool_] = history.opponent_actions[previous_round_index, :]
        cooperators_prev: int = int(my_action_prev) + np.sum(opponent_actions_prev)
        for j_idx in range(self.n - 1):
            player_j_action_prev: bool = opponent_actions_prev[j_idx]
            if cooperators_prev < self.m:
                if not player_j_action_prev:
                    self.failure_defection_count[j_idx] += 1
                else:
                    self.failure_defection_count[j_idx] = max(0, self.failure_defection_count[j_idx] - 1)
            elif player_j_action_prev:
                self.failure_defection_count[j_idx] = max(0, self.failure_defection_count[j_idx] - 1)
        chosen_action: Action = Action.C
        if cooperators_prev >= self.m:
            chosen_action = Action.C
        elif self.my_last_action == Action.D:
            chosen_action = Action.C
        else:
            num_persistent_free_riders: int = 0
            for j_idx in range(self.n - 1):
                if self.failure_defection_count[j_idx] >= self.R_tolerance:
                    num_persistent_free_riders += 1
            if num_persistent_free_riders > self.n - self.m:
                chosen_action = Action.D
            else:
                chosen_action = Action.C
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_COLLECTIVE_171(BaseStrategy):
    """
    The Adaptive Threshold Enforcer strategy for the Collective Risk Dilemma.
    This strategy aims to foster and maintain collective action by exhibiting
    conditional cooperation, forgiveness for minor setbacks, and strategic
    punishment/self-protection against persistent free-riding or insufficient group effort.
    It cooperates initially, defects in the final round, and in intermediate rounds,
    it tracks consecutive failures to meet the 'm' threshold. If failures exceed
    F_threshold (2), it defects; otherwise, it cooperates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.F_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        all_players_prev_actions = np.concatenate((history.opponent_actions[previous_round_idx], np.array([history.my_actions[previous_round_idx]])))
        num_cooperators_prev = np.sum(all_players_prev_actions)
        if num_cooperators_prev >= self.game_description.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.consecutive_failures >= self.F_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_172(BaseStrategy):
    """
    Adaptive Reciprocity for Collective Benefit (ARCB) strategy for the Collective Risk Dilemma.

    This strategy aims to consistently meet the collective cooperation threshold (`m`) to secure
    the `k` reward. It primarily cooperates but incorporates mechanisms to deter free-riding
    and respond to collective failures or sustained exploitation.

    It will temporarily defect under two main conditions:
    1. Persistent Collective Failure: If the group repeatedly fails to meet the `m` threshold.
    2. Redundant Cooperation / Sustained Exploitation: If the threshold is consistently met,
       and this player's cooperation is demonstrably not critical (`N_C > m`), it will
       temporarily defect to test the system's stability and signal against perpetual free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_trigger_rounds = 2
        self.testing_defection_trigger = 2
        self.defection_phase_duration = 1
        self.consecutive_failures = 0
        self.consecutive_redundant_cooperations = 0
        self.current_defection_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_action: Action
        if state.round_number == 0:
            current_round_action = Action.C
        else:
            my_action_prev_round_bool = history.my_actions[-1]
            opponents_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
            n_c_prev = opponents_cooperated_prev_round + (1 if my_action_prev_round_bool else 0)
            if self.current_defection_countdown > 0:
                current_round_action = Action.D
                self.current_defection_countdown -= 1
            elif n_c_prev < self.game_description.m:
                self.consecutive_failures += 1
                if self.consecutive_failures >= self.punishment_trigger_rounds:
                    current_round_action = Action.D
                    self.current_defection_countdown = self.defection_phase_duration
                    self.consecutive_failures = 0
                else:
                    current_round_action = Action.C
                self.consecutive_redundant_cooperations = 0
            else:
                self.consecutive_failures = 0
                if my_action_prev_round_bool:
                    if n_c_prev > self.game_description.m:
                        self.consecutive_redundant_cooperations += 1
                        if self.consecutive_redundant_cooperations >= self.testing_defection_trigger:
                            current_round_action = Action.D
                            self.current_defection_countdown = self.defection_phase_duration
                            self.consecutive_redundant_cooperations = 0
                        else:
                            current_round_action = Action.C
                    else:
                        current_round_action = Action.C
                        self.consecutive_redundant_cooperations = 0
                else:
                    current_round_action = Action.D
                    self.consecutive_redundant_cooperations = 0
            if state.round_number == self.game_description.n_rounds - 1:
                if current_round_action == Action.C:
                    cooperators_if_i_defect_this_round = n_c_prev - (1 if my_action_prev_round_bool else 0)
                    if cooperators_if_i_defect_this_round >= self.game_description.m:
                        current_round_action = Action.D
        return current_round_action

class Strategy_COLLECTIVE_173(BaseStrategy):
    """
    The Collective Consensus Builder (CCB) is designed for the Collective Risk Dilemma game,
    aiming to secure the collective reward `k` as reliably as possible, while also being
    adaptive and robust to various player behaviors. It prioritizes establishing and
    maintaining a cooperative equilibrium but introduces calculated adjustments to avoid
    exploitation and test the collective's resilience.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_streak_success: int = 0
        self.current_streak_failure: int = 0
        self.MIN_SUCCESS_STREAK_TO_TEST: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            self.current_streak_success = 0
            self.current_streak_failure = 0
            return Action.C
        prev_round_index = state.round_number - 1
        my_action_prev_bool = history.my_actions[prev_round_index]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_index, :])
        num_cooperators_prev = num_opponent_cooperators_prev + (1 if my_action_prev_bool else 0)
        if num_cooperators_prev >= m_threshold:
            self.current_streak_success += 1
            self.current_streak_failure = 0
        else:
            self.current_streak_failure += 1
            self.current_streak_success = 0
        if current_round_1_indexed == total_rounds:
            if num_cooperators_prev >= m_threshold:
                return Action.C
            else:
                return Action.D
        elif num_cooperators_prev >= m_threshold:
            if self.current_streak_success >= self.MIN_SUCCESS_STREAK_TO_TEST and my_action_prev_bool:
                return Action.D
            else:
                return Action.C
        elif self.current_streak_failure >= 2:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_174(BaseStrategy):
    """
    Adaptive Collective Commitment (ACC) strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain a high level of cooperation by
    starting optimistically and then conditionally committing to cooperation.
    It rewards collective success (meeting the 'm' threshold) by continuing
    to contribute and "punishes" collective failure by withdrawing its
    contribution, thereby creating an incentive for all players to ensure
    the cooperation threshold is met. It defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed == 0:
            return Action.C
        if current_round_number_0_indexed == self.n_rounds - 1:
            return Action.D
        my_previous_action_bool = history.my_actions[-1]
        opponent_previous_actions_bool = history.opponent_actions[-1, :]
        num_cooperators_in_previous_round = int(my_previous_action_bool) + np.sum(opponent_previous_actions_bool)
        if num_cooperators_in_previous_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_175(BaseStrategy):
    """
    Resilient Threshold Follower (RTF) strategy for the Collective Risk Dilemma.
    Prioritizes achieving the collective reward (k) by cooperating by default.
    Initiates a 1-round defection punishment phase if the collective failed
    to meet the 'm' cooperator threshold in the previous round.
    Always cooperates if the threshold was met, and defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_to_defect: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_idx == 0:
            self.rounds_to_defect = 0
            return Action.C
        elif current_round_idx == n_rounds - 1:
            self.rounds_to_defect = 0
            return Action.D
        else:
            prev_round_opponent_cooperators = sum(history.opponent_actions[current_round_idx - 1, :])
            my_prev_round_action = history.my_actions[current_round_idx - 1]
            prev_round_total_cooperators = prev_round_opponent_cooperators + int(my_prev_round_action)
            if self.rounds_to_defect > 0:
                self.rounds_to_defect -= 1
                return Action.D
            elif prev_round_total_cooperators < m_threshold:
                self.rounds_to_defect = 1
                return Action.D
            else:
                self.rounds_to_defect = 0
                return Action.C

class Strategy_COLLECTIVE_176(BaseStrategy):
    """
    Adaptive Collective Stability (ACS) strategy for the Collective Risk Dilemma.
    This strategy aims to promote collective benefit by initiating cooperation,
    punishing collective failures, sustaining success, and preventing exploitation
    by free-riders, with a rational defection in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        prev_round_index = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_index]
        opponent_actions_prev = history.opponent_actions[prev_round_index, :]
        num_cooperators_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        threshold_met_prev = num_cooperators_prev >= m
        if not threshold_met_prev:
            return Action.D
        else:
            num_defectors_prev = n - num_cooperators_prev
            if my_action_prev and num_cooperators_prev > m and (num_defectors_prev > 0):
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_177(BaseStrategy):
    """
    The Adaptive Collective Pacer (ACP) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation by
    starting with cooperation and dynamically adjusting its behavior based on
    the previous round's collective success or failure. It employs an
    internal "cooperation meter" to track historical performance,
    incrementing it upon success and decrementing it upon failure (with a
    magnitude proportional to the deficit). The meter is bounded to ensure
    responsiveness. In the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._cooperation_meter: int = 0
        self.min_meter_bound: int = -2 * self.game_description.n_rounds
        self.max_meter_bound: int = 2 * self.game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_last_round: int = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if num_cooperators_last_round >= self.game_description.m:
            self._cooperation_meter += 1
        else:
            self._cooperation_meter -= self.game_description.m - num_cooperators_last_round
        self._cooperation_meter = max(self.min_meter_bound, min(self.max_meter_bound, self._cooperation_meter))
        cooperation_threshold: int = 0
        if self._cooperation_meter >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_178(BaseStrategy):
    """
    ORBS: Optimistic Reciprocity and Burden Sharing strategy for the Collective Risk Dilemma.
    This strategy aims to maximize collective welfare by fostering cooperation, providing mechanisms
    for recovery from failures, signaling dissatisfaction when cooperation consistently breaks down,
    and adapting to the end of the game.
    """

    class Mode(Enum):
        """
        Enum to track the current behavioral stance of the player.
        """
        COOPERATING_MODE = 1
        RECOVERY_MODE = 2
        PUNISH_MODE = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.my_mode: Strategy.Mode = self.Mode.COOPERATING_MODE
        self.rounds_since_last_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.C
        my_action_prev_bool = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        N_C_prev = opponent_cooperators_prev + (1 if my_action_prev_bool else 0)
        success_prev = N_C_prev >= self.m
        if success_prev:
            self.my_mode = self.Mode.COOPERATING_MODE
            self.rounds_since_last_success = 0
        else:
            self.rounds_since_last_success += 1
            if self.rounds_since_last_success == 1:
                self.my_mode = self.Mode.RECOVERY_MODE
            else:
                self.my_mode = self.Mode.PUNISH_MODE
        if self.my_mode == self.Mode.COOPERATING_MODE:
            return Action.C
        elif self.my_mode == self.Mode.RECOVERY_MODE:
            if my_action_prev_bool is True:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_179(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    It combines an initial period of trust-building, an adaptive response to past outcomes,
    and a pragmatic approach to the end-game. It aims to achieve and sustain collective
    cooperation by ensuring the cooperation threshold (`m`) is met as often as possible,
    while also protecting against consistent exploitation when collective efforts fail.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.initial_cooperate_rounds = 1
        self.failure_patience_threshold = 2
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed_m = self.game_description.m
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed <= self.initial_cooperate_rounds:
            return Action.C
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= min_cooperators_needed_m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures < self.failure_patience_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_180(BaseStrategy):
    """
    Adaptive Collective Persistence (ACP) strategy for the Collective Risk Dilemma.

    The ACP strategy champions collective success. It consistently strives to achieve the 'k' reward
    by cooperating, tolerating initial failures, and using minimal, focused defection as a signal
    of unmet cooperation thresholds. After a single round of "punishment," it quickly re-attempts
    to establish a cooperative equilibrium. Its goal is to guide the collective towards a stable,
    high-payoff outcome for all players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._FAILURE_TOLERANCE_THRESHOLD = 2
        self._consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self._consecutive_failures = 0
            return Action.C
        my_prev_action_was_cooperation = history.my_actions[-1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = (1 if my_prev_action_was_cooperation else 0) + num_opponent_cooperators_prev_round
        success_in_prev_round = total_cooperators_prev_round >= self.game_description.m
        if success_in_prev_round:
            self._consecutive_failures = 0
        else:
            self._consecutive_failures += 1
        action_for_this_round = Action.C
        if self._consecutive_failures >= self._FAILURE_TOLERANCE_THRESHOLD:
            action_for_this_round = Action.D
            self._consecutive_failures = 0
        if current_round_1_indexed == self.game_description.n_rounds:
            if success_in_prev_round:
                return Action.C
            else:
                return Action.D
        return action_for_this_round

class Strategy_COLLECTIVE_181(BaseStrategy):
    """
    This strategy, named "Adaptive Reciprocity and Risk Sharing," is designed to foster cooperation for the
    collective good while remaining robust and adaptive to various opponent behaviors in a tournament setting.
    It balances the individual incentive to defect with the collective need to meet the cooperation threshold 'm'.

    Core Principles:
    1. Proactive Cooperation: Initiate cooperation to signal willingness and attempt to establish a cooperative equilibrium.
    2. Conditional Cooperation: Maintain cooperation when the collective threshold is successfully met.
    3. Adaptive Probing: When cooperation fails, adjust behavior by either defending against exploitation or re-initiating
       a cooperative effort.
    4. Rational Endgame: Recognize the unique dynamics of the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_t == 1:
            return Action.C
        elif current_t == total_rounds:
            return Action.D
        else:
            my_action_prev_bool = history.my_actions[-1]
            opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            c_count_prev = (1 if my_action_prev_bool else 0) + opponent_cooperators_prev
            if c_count_prev >= min_cooperators_m:
                return Action.C
            elif my_action_prev_bool:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_182(BaseStrategy):
    """
    Adaptive Collective Steward (ACS) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by leading by example and sustaining
    success. It also incorporates a forgiveness mechanism, giving the collective
    a limited number of chances to recover from failures before temporarily
    defecting to protect against exploitation. In the final round, it defects
    for rational self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Steward strategy.

        Args:
            game_description (CollectiveRiskDescription): Description of the game
                                                         parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.failure_tolerance: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Current game state, including the 0-indexed round number.
            history (None | PlayerHistory): History of actions and payoffs from
                                            previous rounds. None for the very first round (round 0).

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round >= self.game_description.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures <= self.failure_tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_183(BaseStrategy):
    """
    The Robust Collective Contributor (RCC) strategy aims to foster and maintain
    collective cooperation to achieve the shared reward (`k`) while also protecting
    itself from persistent exploitation by non-cooperative players. It balances
    the collective good with individual robustness in a competitive tournament setting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.recovery_patience_limit = 3
        self.my_current_recovery_attempts = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.my_current_recovery_attempts = 0
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= m:
            self.my_current_recovery_attempts = 0
            return Action.C
        else:
            severe_failure_threshold = max(1, math.floor(m / 2))
            if num_cooperators_prev_round < severe_failure_threshold:
                self.my_current_recovery_attempts = 0
                return Action.D
            elif self.my_current_recovery_attempts < self.recovery_patience_limit:
                self.my_current_recovery_attempts += 1
                return Action.C
            else:
                self.my_current_recovery_attempts = 0
                return Action.D

class Strategy_COLLECTIVE_184(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy aims to establish and maintain collective
    cooperation to secure the 'k' reward, while also protecting itself from exploitation.
    It prioritizes the collective good by initially cooperating and sustaining successful
    cooperation, but acts as an enforcer by defecting when collective goals are not met,
    signaling the need for greater commitment from the group. It recognizes the "end-game"
    effect in finite repeated games to avoid last-round exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.min_cooperators_needed = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.total_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round >= self.min_cooperators_needed:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_185(BaseStrategy):
    """
    Adaptive Collective Stabilizer (ACS) for the Collective Risk Dilemma.

    This strategy aims to balance collective good with self-protection.
    It initiates cooperation, reinforces successful collective action,
    signals dissatisfaction by defecting when the collective goal is not met,
    and reverts to self-interest in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_prev_action_was_C = history.my_actions[prev_round_idx]
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev_round = int(my_prev_action_was_C) + opponent_prev_cooperators
        if num_cooperators_prev_round >= min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_186(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.

    ACE aims to foster and sustain collective cooperation by initiating with trust,
    conditionally cooperating when the collective goal (m cooperators) is met,
    and imposing a temporary punishment when the collective fails to meet this goal.
    It prioritizes the achievement of the collective reward (k) for all, while remaining
    pragmatic and robust against exploitation, especially in the game's final stages.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_of_punishment_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == n_rounds:
            return Action.D
        if self.rounds_of_punishment_remaining > 0:
            self.rounds_of_punishment_remaining -= 1
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_prev_action_cooperated = history.my_actions[state.round_number - 1]
        opponents_prev_cooperated_count = np.sum(history.opponent_actions[state.round_number - 1, :])
        cooperators_in_prev_round = (1 if my_prev_action_cooperated else 0) + opponents_prev_cooperated_count
        if cooperators_in_prev_round >= m_threshold:
            return Action.C
        else:
            self.rounds_of_punishment_remaining = 1
            return Action.D

class Strategy_COLLECTIVE_187(BaseStrategy):
    """
    The Adaptive Collective Pledger (ACP) strategy dynamically adjusts its cooperation
    based on past collective success in meeting the cooperation threshold.
    It initiates cooperation, defects in the final round, and for intermediate rounds,
    it reinforces successful cooperation, free-rides when possible, and attempts to
    correct collective failures by switching behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        action_for_this_round: Action
        if current_round == 0:
            action_for_this_round = Action.C
        elif current_round == total_rounds - 1:
            action_for_this_round = Action.D
        else:
            my_action_prev_round = self.my_last_action
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
            total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round == Action.C else 0)
            threshold_met_prev = total_cooperators_prev_round >= min_cooperators_m
            if threshold_met_prev:
                if my_action_prev_round == Action.C:
                    action_for_this_round = Action.C
                else:
                    action_for_this_round = Action.D
            elif my_action_prev_round == Action.C:
                action_for_this_round = Action.D
            else:
                action_for_this_round = Action.C
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_COLLECTIVE_188(BaseStrategy):
    """
    Strategy: Threshold-Triggered Continuous Cooperation with End-Game Prudence (TTCC-EP)

    Prioritizes the collective achievement of the reward `k` by largely maintaining cooperation,
    adapting to past collective performance, and being robust to the realities of finite repeated games.

    Decision Rules:
    - Round 1: Cooperates to establish cooperative intent and maximize initial chance of success.
    - Intermediate Rounds (1 < t < r): Evaluates the previous round's outcome:
        - If N_C(t-1) < m (collective failure): Cooperates to re-commit and prevent collapse.
        - If N_C(t-1) >= m (collective success): Cooperates to maintain stability and reinforce success.
        (Note: Both scenarios lead to cooperation in intermediate rounds as per strategy rationale).
    - Last Round (t = r): Defects due to the end-game effect to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_number == 0:
            return Action.C
        elif current_round_number == total_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if num_cooperators_prev_round < min_cooperators_needed:
                return Action.C
            else:
                return Action.C

class Strategy_COLLECTIVE_189(BaseStrategy):
    """
    Smart Collective Contributor (SCC) strategy for the Collective Risk Dilemma.
    This strategy aims to foster sustained cooperation by initiating cooperation,
    conditionally cooperating based on collective success, self-correcting after
    successful free-riding, and collectively punishing widespread failure. It
    opts for individual rationality by defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_countdown = 0
        self.PUNISHMENT_LENGTH = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_in_prev_round = history.my_actions[prev_round_idx]
        opponent_actions_in_prev_round = history.opponent_actions[prev_round_idx, :]
        cooperators_in_prev_round = sum(opponent_actions_in_prev_round) + int(my_action_in_prev_round)
        if cooperators_in_prev_round < m:
            self.punishment_countdown = self.PUNISHMENT_LENGTH
            return Action.D
        elif my_action_in_prev_round == Action.C:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_190(BaseStrategy):
    """
    Adaptive Collective Endeavor (ACE) strategy for the Collective Risk Dilemma.

    This strategy balances individual payoff maximization with the need for collective
    success. It initiates with cooperation, reinforces optimal cooperation (N_C = m),
    nudges the group towards efficiency when cooperation is in surplus (N_C > m),
    persists in cooperation after minor failures, and withdraws to protect
    individual interests during persistent collective failure. It also prioritizes
    individual rationality in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_rounds_failed_threshold_met: int = 0
        self.failure_tolerance_limit: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number > 0:
            num_opponent_cooperators_previous_round = history.opponent_actions[-1].sum()
            my_action_previous_round = history.my_actions[-1]
            total_cooperators_previous_round = num_opponent_cooperators_previous_round + int(my_action_previous_round)
            if total_cooperators_previous_round < m:
                self.consecutive_rounds_failed_threshold_met += 1
            else:
                self.consecutive_rounds_failed_threshold_met = 0
        if state.round_number == 0:
            return Action.C
        elif state.round_number == r - 1:
            return Action.D
        elif total_cooperators_previous_round >= m:
            if total_cooperators_previous_round == m:
                return Action.C
            else:
                return Action.D
        elif self.consecutive_rounds_failed_threshold_met < self.failure_tolerance_limit:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_191(BaseStrategy):
    """
    Adaptive Collective Threshold Pacer (ACTP) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain the collective reward 'k' by promoting
    cooperation. It starts with cooperation, punishes collective failures (fewer than 'm'
    cooperators) by defecting for one round, and protects against exploitation in the
    final round by defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACTP strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the ACTP strategy's rules.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing records of past actions and payoffs.
                     It is None for the very first round (state.round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = int(my_prev_action_is_cooperate) + int(opponent_prev_cooperators)
        if total_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_193(BaseStrategy):
    """
    Adaptive Collective Threshold Keeper (ACTK) strategy for the Collective Risk Dilemma.

    ACTK aims to foster and maintain collective cooperation by initiating with 'C',
    rewarding sustained collective success with continued 'C', and implementing
    temporary 'D' punishments for collective failures (fewer than 'm' cooperators).
    It is designed to be robust in a tournament setting by not relying on assumed
    altruism, but instead using conditional cooperation and deterrence to
    encourage behavior that benefits the collective in the long run.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._punishment_mode: bool = False
        self._punishment_rounds_remaining: int = 0
        self.PUNISHMENT_DURATION: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self._punishment_mode:
            self._punishment_rounds_remaining -= 1
            if self._punishment_rounds_remaining == 0:
                self._punishment_mode = False
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            num_cooperators_last_round += 1
        if num_cooperators_last_round >= self.game_description.m:
            return Action.C
        else:
            self._punishment_mode = True
            self._punishment_rounds_remaining = self.PUNISHMENT_DURATION
            return Action.D

class Strategy_COLLECTIVE_194(BaseStrategy):
    """
    Adaptive Collective Maintainer (ACM) strategy for the Collective Risk Dilemma.
    This strategy adapts its cooperation based on the collective's recent success in meeting the
    cooperation threshold `m`. It starts cooperatively, maintains cooperation when the threshold
    is met, shows patience for a few consecutive failures, but eventually defects if the collective
    consistently fails to cooperate, to protect itself from exploitation.
    """
    PATIENCE_THRESHOLD = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Maintainer strategy.

        Args:
            game_description: A CollectiveRiskDescription object containing game parameters
                              (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.consecutive_failures = 0
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs, or None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        num_cooperators_last_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_last_round >= self.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if current_round_0_indexed == self.n_rounds - 1:
            if num_cooperators_last_round >= self.m:
                return Action.C
            else:
                return Action.D
        elif self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures <= self.PATIENCE_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_195(BaseStrategy):
    """
    The Adaptive Collective Maintainer (ACM) strategy.
    This strategy aims to foster and maintain collective cooperation in the Collective Risk Dilemma
    by prioritizing securing the collective reward ('k') for all players.
    It contributes to the threshold ('m') consistently, while also being adaptive to failures
    and resistant to sustained exploitation.

    The strategy operates by monitoring the collective's success in meeting the 'm' threshold
    and adjusting its own behavior accordingly. It starts by cooperating,
    continues cooperating if the collective succeeds or for limited failures.
    It defects in the last round to avoid being a 'sucker' and defects after
    persistent failures if it has been consistently cooperating without reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_failures: int = 0
        self._my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self._my_last_action = Action.C
            return Action.C
        N_C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        m_threshold = self.game_description.m
        if N_C_prev >= m_threshold:
            self._consecutive_failures = 0
        else:
            self._consecutive_failures += 1
        if state.round_number == self.game_description.n_rounds - 1:
            self._my_last_action = Action.D
            return Action.D
        if N_C_prev >= m_threshold:
            self._my_last_action = Action.C
            return Action.C
        elif self._consecutive_failures < 3 or self._my_last_action == Action.D:
            self._my_last_action = Action.C
            return Action.C
        else:
            self._my_last_action = Action.D
            return Action.D

class Strategy_COLLECTIVE_196(BaseStrategy):
    """
    The Forgiving and Persistent Collector (FPC) strategy for the Collective Risk Dilemma.

    This strategy aims to achieve sustainable cooperation by combining an initial commitment
    to cooperation, a temporary collective "strike" when the threshold is missed, and a
    persistent willingness to re-engage in cooperation to prevent indefinite collective failure.

    Internal State:
    - failure_streak: An integer counter, initialized to 0. It tracks the number of consecutive
      rounds where the collective cooperation threshold (m) was not met. It resets to 0
      whenever the threshold is met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == n_rounds:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_actions_prev = history.opponent_actions[-1, :]
        n_c_prev = int(my_action_prev) + np.sum(opponent_actions_prev).item()
        if n_c_prev >= m_threshold:
            self.failure_streak = 0
        else:
            self.failure_streak += 1
        if self.failure_streak == 0:
            return Action.C
        elif self.failure_streak == 1:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_197(BaseStrategy):
    """
    The Conditional Collective Cooperator (CCC) strategy aims to foster and maintain collective
    action by initiating cooperation and being conditionally cooperative, while also being
    robust against exploitation and failures in collective effort. It balances the desire for
    collective good with the need for self-preservation in a competitive tournament environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        opponent_prev_cooperators = sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = int(my_prev_action_was_C) + opponent_prev_cooperators
        if num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            severe_failure_threshold = math.ceil(self.game_description.m / 2.0)
            if num_cooperators_prev_round < severe_failure_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_198(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize collective payoff by promoting cooperation,
    punishing collective failures, and incorporating a forgiveness mechanism
    to break defection spirals. It also includes a rational defection in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_state: str = 'Cooperating'
        self.rounds_in_punishment: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round_t == 1:
            self.current_state = 'Cooperating'
            self.rounds_in_punishment = 0
            return Action.C
        if current_round_t == total_rounds:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= min_cooperators:
            self.current_state = 'Cooperating'
            self.rounds_in_punishment = 0
            return Action.C
        elif self.current_state == 'Cooperating':
            self.current_state = 'Punishing'
            self.rounds_in_punishment = 1
            return Action.D
        else:
            self.rounds_in_punishment += 1
            if self.rounds_in_punishment < 3:
                return Action.D
            else:
                self.current_state = 'Cooperating'
                self.rounds_in_punishment = 0
                return Action.C

class Strategy_COLLECTIVE_199(BaseStrategy):
    """
    Dynamic Trust and Reciprocity (DTR) strategy for the Collective Risk Dilemma.
    This strategy adapts its behavior between 'Trusting' and 'Cautious' modes
    based on the collective cooperation in previous rounds. It aims to achieve
    the collective reward 'k' by cooperating when the group meets the threshold 'm',
    but switches to defecting ('Cautious' mode) if the threshold is missed.
    It includes a forgiveness mechanism to periodically re-test cooperation
    even after repeated failures.
    """

    class Mode(Enum):
        """Enum to represent the strategy's internal mode."""
        Trusting = 'Trusting'
        Cautious = 'Cautious'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.mode: Strategy.Mode = self.Mode.Trusting
        self.round_of_last_cooperation_attempt_in_cautious_mode: int = 0
        self.PUNISHMENT_PERIOD_IN_CAUTIOUS_MODE: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        num_cooperators_in_prev_round = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
        met_threshold_last_round = num_cooperators_in_prev_round >= m_threshold
        if self.mode == self.Mode.Trusting:
            if met_threshold_last_round:
                return Action.C
            else:
                self.mode = self.Mode.Cautious
                self.round_of_last_cooperation_attempt_in_cautious_mode = state.round_number
                return Action.D
        elif met_threshold_last_round:
            self.mode = self.Mode.Trusting
            self.round_of_last_cooperation_attempt_in_cautious_mode = 0
            return Action.C
        else:
            rounds_since_my_last_cautious_cooperation_attempt = state.round_number - self.round_of_last_cooperation_attempt_in_cautious_mode
            if rounds_since_my_last_cautious_cooperation_attempt >= self.PUNISHMENT_PERIOD_IN_CAUTIOUS_MODE and total_rounds - current_round_1_indexed > 1:
                self.round_of_last_cooperation_attempt_in_cautious_mode = state.round_number
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_200(BaseStrategy):
    """
    The "Adaptive Threshold Enforcer" strategy for the Collective Risk Dilemma.

    This strategy aims to initiate and sustain cooperation by consistently
    contributing to the common good when the collective has demonstrated its
    ability to meet the minimum cooperation threshold (`m`). If the collective
    fails to meet this threshold, the strategy switches to defection to avoid
    being exploited and to signal that greater collective effort is required.
    In the final round, it reverts to purely self-interested defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_index = state.round_number
        if current_round_index == 0:
            return Action.C
        elif current_round_index == self.n_rounds - 1:
            return Action.D
        else:
            prev_round_index = current_round_index - 1
            prev_round_opponent_cooperators = np.sum(history.opponent_actions[prev_round_index, :])
            prev_round_my_action_val = 1 if history.my_actions[prev_round_index] else 0
            prev_round_total_cooperators = prev_round_opponent_cooperators + prev_round_my_action_val
            if prev_round_total_cooperators >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_201(BaseStrategy):
    """
    The Adaptive Collective Contributor strategy aims to establish and maintain a cooperative equilibrium
    where the minimum threshold 'm' of cooperators is consistently met. It prioritizes the collective
    good, but adapts to situations where the collective fails or when there's an excessive surplus of
    cooperation, attempting to maximize individual payoff while ensuring collective success.

    Key behaviors:
    - Initiates cooperation in the first round.
    - If the threshold was met:
        - And the player cooperated: May defect if there's a significant buffer of excess cooperators.
        - And the player defected: Continues defecting (successful individual optimization).
    - If the threshold was NOT met: Defects as a form of punishment/self-preservation.
    - Defects in the last round due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_my_action_is_c = history.my_actions[-1]
        prev_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_total_cooperators = prev_opponent_cooperators + int(prev_my_action_is_c)
        if prev_total_cooperators >= m:
            if prev_my_action_is_c:
                excess_cooperators = prev_total_cooperators - m
                max_potential_safe_defectors_if_all_cooperated = n_players - m
                if excess_cooperators >= math.ceil(max_potential_safe_defectors_if_all_cooperated / 2):
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_202(BaseStrategy):
    """
    The Adaptive Collective Trust (ACT) strategy aims to foster and maintain a cooperative environment
    to consistently achieve the collective reward 'k'. It prioritizes the collective good by
    initiating cooperation and rewarding successful collective action, while simultaneously
    protecting against exploitation by withdrawing cooperation when the collective fails or
    in the game's final stage. The strategy adapts its behavior based on the observed
    collective outcome of the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing game parameters
                                                          such as n_players, n_rounds, m, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate 'C' or Defect 'D') for the current round
        based on the Adaptive Collective Trust strategy rules.

        Args:
            state (GameState): The current state of the game, including the current round number.
            history (None | PlayerHistory): A record of past rounds' actions and payoffs.
                                            None for the very first round (round_number == 0).

        Returns:
            Action: The chosen action for the current round, either Action.C or Action.D.
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[-1]
        num_opponent_cooperators_in_prev_round = sum(history.opponent_actions[-1, :])
        num_cooperators_in_prev_round = my_prev_action_is_cooperate + num_opponent_cooperators_in_prev_round
        if num_cooperators_in_prev_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_203(BaseStrategy):
    """
    Dynamic Retaliation & Re-engagement (DRR) Strategy for the Collective Risk Dilemma.

    This strategy aims to achieve and maintain collective cooperation by being
    initially pro-social, selectively punishing collective failures, and offering
    opportunities for re-engagement, all while protecting itself from persistent
    exploitation. It balances the "collective mindset" with the realities of a
    competitive tournament.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_threshold_met = False
        self.cooperation_attempts_since_last_success = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        my_action_prev = history.my_actions[previous_round_idx]
        opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :])
        C_prev = int(my_action_prev) + opponent_cooperators_prev
        threshold_met_prev_round = C_prev >= self.game_description.m
        if threshold_met_prev_round:
            self.cooperation_attempts_since_last_success = 0
            self.last_round_threshold_met = True
            return Action.C
        elif self.last_round_threshold_met:
            self.last_round_threshold_met = False
            return Action.D
        else:
            patience_limit = self.game_description.n_players - self.game_description.m + 1
            if self.cooperation_attempts_since_last_success < patience_limit:
                self.cooperation_attempts_since_last_success += 1
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_204(BaseStrategy):
    """
    The Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.

    ATE aims to achieve and maintain the collective reward 'k'. It starts
    cooperatively and then observes if the group successfully met the 'm'
    cooperator threshold in the previous round. If successful, it continues
    to cooperate. If the threshold is missed, it defects as a collective
    punishment, signaling the need for more cooperation from others. This
    mechanism seeks to enforce a norm of collective contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.collective_success_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_idx = state.round_number - 1
        num_cooperators_prev_round = int(history.my_actions[previous_round_idx])
        num_cooperators_prev_round += sum(history.opponent_actions[previous_round_idx])
        if num_cooperators_prev_round >= self.game_description.m:
            self.collective_success_streak += 1
        else:
            self.collective_success_streak = 0
        is_last_round = state.round_number == self.game_description.n_rounds - 1
        if is_last_round:
            if self.collective_success_streak > 0:
                return Action.C
            else:
                return Action.D
        elif self.collective_success_streak > 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_205(BaseStrategy):
    """
    The Adaptive Conditional Cooperation (ACC) strategy for the Collective Risk Dilemma.
    This strategy aims to foster cooperation by initiating with a cooperative move and then
    adapting based on whether the collective threshold 'm' was met in the previous round.
    It prioritizes achieving the 'k' bonus for the group, only resorting to defection
    to signal collective failure or prevent exploitation when cooperation is not sustained.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: An instance of CollectiveRiskDescription containing game parameters
                              like n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the strategy rules.

        Args:
            state: An instance of GameState containing the current round number.
            history: An instance of PlayerHistory containing records of past actions and payoffs.
                     It is None for the very first round (round_number == 0).

        Returns:
            An Action (C for Cooperate, D for Defect) for the current round.
        """
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        is_last_round = state.round_number == self.game_description.n_rounds - 1
        if is_last_round:
            if num_cooperators_prev >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        elif num_cooperators_prev >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_206(BaseStrategy):
    """
    The Adaptive Community Steward (ACS) strategy for the Collective Risk Dilemma.
    This strategy aims to foster cooperation by leading with C, sustaining it when
    successful, forgiving a single collective failure, but ultimately reverting
    to self-preservation (D) if collective failures persist. It defects in the
    final round for end-game rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            self.consecutive_failures = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        num_cooperators_in_prev_round = num_opponent_cooperators_prev_round + int(my_prev_action_was_C)
        if num_cooperators_in_prev_round >= m_threshold:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_207(BaseStrategy):
    """
    The "Adaptive Trust & Reciprocity" strategy aims to foster and maintain collective cooperation
    in the Collective Risk Dilemma. It adapts its behavior based on the observed collective outcome
    of previous rounds, specifically whether the cooperation threshold 'm' was met. It prioritizes
    the collective good by striving to ensure the 'k' reward is secured for everyone, while also
    protecting itself against persistent non-cooperation.

    Strategic Principles:
    1.  Initial Good Faith: Start cooperatively to signal willingness.
    2.  Adaptive Trust: Maintain an internal 'collective_trust_score' reflecting collective reliability.
    3.  Conditional Cooperation: Cooperate when trust is sufficiently high.
    4.  Reciprocal Punishment: Defect when trust is low to signal unsustainable cooperation.
    5.  Endgame Rationality: Revert to defection in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.collective_trust_score = float(self.game_description.m) / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        n_players = self.game_description.n_players
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if cooperators_previous_round >= m_threshold:
            self.collective_trust_score = min(1.0, self.collective_trust_score + 0.1 * (1.0 - self.collective_trust_score))
        else:
            self.collective_trust_score = max(0.0, self.collective_trust_score - 0.2 * self.collective_trust_score)
        if self.collective_trust_score >= float(m_threshold) / n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_208(BaseStrategy):
    """
    Adaptive Collective Effort (ACE) strategy for the Collective Risk Dilemma.
    This strategy aims to balance collective welfare with individual protection and adaptability,
    based on game parameters and observed history. It starts with cooperation, sustains success,
    punishes exploitation, attempts to initiate recovery from collective failure, and defects
    in the final round for individual rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        elif current_round_idx == self.n_rounds - 1:
            return Action.D
        else:
            prev_round_data_idx = current_round_idx - 1
            my_prev_action = history.my_actions[prev_round_data_idx]
            prev_Nc = np.sum(history.opponent_actions[prev_round_data_idx, :]) + my_prev_action
            if prev_Nc >= self.m_threshold:
                return Action.C
            elif my_prev_action is True:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_209(BaseStrategy):
    """
    Adaptive Cooperative Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy embodies a collective mindset by actively promoting and sustaining cooperation
    to meet the collective threshold `m`. It is adaptive, learning from past rounds, and robust,
    responding to both success and failure with a balance of generosity, self-protection, and forgiveness.

    Core Principles:
    1. Optimistic Start: Begin by cooperating to establish a baseline.
    2. Sustain Success: If the collective threshold 'm' was met in the previous round, continue cooperating.
    3. Punish Exploitation: If the collective failed and this strategy cooperated in the previous round, defect.
    4. Forgive and Re-engage: If the collective failed and this strategy defected in the previous round, cooperate.
    5. Rational Endgame: Defect in the final round to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round == 0:
            return Action.C
        elif current_round == total_rounds - 1:
            return Action.D
        else:
            my_action_prev = history.my_actions[current_round - 1]
            opponent_actions_prev_round = history.opponent_actions[current_round - 1, :]
            num_cooperators_prev_round = np.sum(opponent_actions_prev_round) + my_action_prev
            if num_cooperators_prev_round >= min_cooperators_needed:
                return Action.C
            elif my_action_prev == True:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_210(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) Strategy for the Collective Risk Dilemma.

    ACE initiates cooperation in the first round to establish a pro-social tone.
    In subsequent rounds, it adaptively responds to the collective outcome of the
    immediately preceding round. If the minimum cooperation threshold 'm' was met
    in the previous round (project succeeded), ACE continues to cooperate to sustain
    the collective benefit. If the threshold was not met (project failed), ACE defects
    to signal disapproval, minimize personal loss, and encourage others to increase
    cooperation. This logic applies consistently until the very last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        min_cooperators_m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        else:
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_cooperate = int(history.my_actions[-1])
            total_cooperators_prev_round = opponent_cooperators_prev_round + my_action_prev_round_is_cooperate
            project_succeeded_in_prev_round = total_cooperators_prev_round >= min_cooperators_m
            if project_succeeded_in_prev_round:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_211(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) with Re-evaluation strategy for the Collective Risk Dilemma.
    This strategy starts by cooperating, tolerates a limited number of consecutive group failures,
    then switches to defection for a fixed duration to punish non-cooperation. After punishment,
    it re-evaluates by attempting cooperation again. It always defects in the final round.
    """
    COOPERATION_TOLERANCE_THRESHOLD = 2
    PUNISHMENT_DURATION_ROUNDS = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.status: str = 'TRY_COOPERATE'
        self.consecutive_failures: int = 0
        self.rounds_in_punishment: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_number == 0:
            self.status = 'TRY_COOPERATE'
            self.consecutive_failures = 0
            self.rounds_in_punishment = 0
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        last_round_idx = current_round_number - 1
        num_cooperators_last_round = int(history.my_actions[last_round_idx]) + np.sum(history.opponent_actions[last_round_idx, :])
        group_succeeded_last_round = num_cooperators_last_round >= m
        if self.status == 'TRY_COOPERATE':
            if group_succeeded_last_round:
                self.consecutive_failures = 0
                return Action.C
            else:
                self.consecutive_failures += 1
                if self.consecutive_failures <= self.COOPERATION_TOLERANCE_THRESHOLD:
                    return Action.C
                else:
                    self.status = 'PUNISH_DEFECTION'
                    self.rounds_in_punishment = 1
                    return Action.D
        elif self.status == 'PUNISH_DEFECTION':
            if self.rounds_in_punishment < self.PUNISHMENT_DURATION_ROUNDS:
                self.rounds_in_punishment += 1
                return Action.D
            else:
                self.status = 'TRY_COOPERATE'
                self.consecutive_failures = 0
                self.rounds_in_punishment = 0
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_212(BaseStrategy):
    """
    The Adaptive Threshold-Maintaining (ATM) strategy aims to achieve and sustain 
    the collective benefit in the Collective Risk Dilemma. It prioritizes stable 
    cooperation, attempts to recover from minor setbacks, and protects itself 
    from persistent exploitation. It starts cooperatively, maintains cooperation 
    when the threshold is met, tries to re-establish cooperation after minor failures, 
    and defects only when the collective efforts persistently fail or are severely lacking. 
    It also defects in the final round due to individual rationality in a finitely repeated game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_fail_rounds = 0
        self.consecutive_success_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_is_C = history.my_actions[state.round_number - 1]
        opponent_prev_actions_are_C = history.opponent_actions[state.round_number - 1, :]
        C_prev = int(my_prev_action_is_C) + np.sum(opponent_prev_actions_are_C)
        min_cooperators_m = self.game_description.m
        if C_prev >= min_cooperators_m:
            self.consecutive_fail_rounds = 0
            self.consecutive_success_rounds += 1
            return Action.C
        else:
            self.consecutive_success_rounds = 0
            self.consecutive_fail_rounds += 1
            if self.consecutive_fail_rounds >= 3 or C_prev < min_cooperators_m / 2.0:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_213(BaseStrategy):
    """
    The Adaptive Collective Trigger (ACT) strategy aims to maximize its total payoff
    by actively promoting and sustaining cooperation to meet the collective threshold `m`.
    It does so by initially showing good faith, maintaining cooperation during periods
    of collective success, and strategically withdrawing cooperation to penalize
    collective failures, thereby incentivizing a return to cooperation. The strategy
    also incorporates defensive measures against end-game exploitation.

    Decision Rules:
    1. Round 1: Cooperate (C)
    2. Last Round: Defect (D)
    3. Intermediate Rounds:
        - If the collective threshold 'm' was met in the previous round, Cooperate (C).
        - If the collective threshold 'm' was NOT met in the previous round, Defect (D).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number + 1
        total_game_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_game_round == 1:
            return Action.C
        if current_game_round == total_game_rounds:
            return Action.D
        my_prev_action_cooperated = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        N_C_prev = my_prev_action_cooperated + opponent_prev_cooperators
        if N_C_prev >= min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_214(BaseStrategy):
    """
    The Collective Tit-for-Tat (CTFT) strategy for the Collective Risk Dilemma.
    It starts by cooperating and subsequently mirrors the collective outcome of the
    previous round: cooperate if the collective threshold 'm' was met, defect otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_index = state.round_number - 1
        my_action_prev_round_is_cooperate = history.my_actions[previous_round_index]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + my_action_prev_round_is_cooperate
        if total_cooperators_prev_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_215(BaseStrategy):
    """
    The Adaptive Threshold Enforcer (ATE) strategy aims to establish and maintain cooperation in the
    Collective Risk Dilemma. It leads by cooperating initially, punishes collective failure to meet
    the 'm' cooperator threshold, and offers a chance for recovery after prolonged failures.
    It adopts a rational defection in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_0_indexed > 0:
            prev_round_my_action = history.my_actions[current_round_0_indexed - 1]
            prev_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            N_c_prev = sum(prev_round_opponent_actions) + prev_round_my_action
            if N_c_prev >= min_cooperators_m:
                self.consecutive_failures = 0
            else:
                self.consecutive_failures += 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        elif N_c_prev >= min_cooperators_m:
            return Action.C
        elif self.consecutive_failures <= 2:
            return Action.D
        elif random.random() < 0.3:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_216(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation by adapting its behavior
    based on the collective outcome (whether the cooperation threshold 'm' was met)
    and its own previous action. It starts cooperatively and tries to reinforce
    cooperation after success, re-commits after free-riding, punishes collective
    failure where it cooperated, and attempts to re-ignite cooperation after
    collective failure where it defected. It maintains its logic in the final round
    to avoid backward induction issues.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_action_prev_bool = history.my_actions[-1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        n_c_prev = int(my_action_prev_bool) + num_opponent_cooperators_prev
        m_threshold = self.game_description.m
        if n_c_prev >= m_threshold:
            if my_action_prev_bool == True:
                return Action.C
            else:
                return Action.C
        elif my_action_prev_bool == True:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_217(BaseStrategy):
    """
    The Adaptive Threshold Enforcer (ATE) strategy is a conditional cooperator for the
    Collective Risk Dilemma. It aims to sustain collective cooperation by establishing
    an initial cooperative stance, rewarding collective success, and punishing collective
    failure to meet the critical cooperation threshold 'm'.

    It operates in three phases:
    1. Initial Phase (Round 1): Always Cooperate (C) to signal willingness and test the waters.
    2. Mid-Game Phase (Round 2 to r-1): Adapts based on the previous round's collective outcome.
       - If total cooperators >= m in the previous round, Cooperate (C).
       - If total cooperators < m in the previous round, Defect (D).
    3. End-Game Phase (Round r): Always Defect (D) to avoid last-round exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_cooperation_prev = 1 if history.my_actions[-1] else 0
        opponents_cooperation_prev = sum(history.opponent_actions[-1, :])
        C_prev = my_cooperation_prev + opponents_cooperation_prev
        min_cooperators_m = self.game_description.m
        if C_prev >= min_cooperators_m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_218(BaseStrategy):
    """
    The Adaptive Collective Consensus (ACC) strategy for the Collective Risk Dilemma.
    It monitors collective success and individual cooperation tendencies, adapting its
    behavior to foster cooperation when successful, pull back when exploited, and
    re-establish cooperation when viable, while defecting in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._coop_scores: NDArray[np.float64] = np.full(self.game_description.n_players - 1, 1.0, dtype=np.float64)
        self._learning_rate: float = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        my_decision: Action
        if state.round_number == 0:
            my_decision = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            my_decision = Action.D
        else:
            prev_round_idx = state.round_number - 1
            prev_round_my_action_bool = history.my_actions[prev_round_idx]
            prev_round_opponent_actions_bool = history.opponent_actions[prev_round_idx, :]
            c_prev = int(prev_round_my_action_bool) + np.sum(prev_round_opponent_actions_bool)
            for opponent_idx in range(self.game_description.n_players - 1):
                if prev_round_opponent_actions_bool[opponent_idx]:
                    self._coop_scores[opponent_idx] = min(1.0, self._coop_scores[opponent_idx] + self._learning_rate)
                else:
                    self._coop_scores[opponent_idx] = max(0.0, self._coop_scores[opponent_idx] - self._learning_rate)
            if c_prev >= self.game_description.m:
                my_decision = Action.C
            elif self._my_last_action == Action.C:
                my_decision = Action.D
            else:
                sum_of_other_coop_scores = np.sum(self._coop_scores)
                if sum_of_other_coop_scores >= self.game_description.m - 1:
                    my_decision = Action.C
                else:
                    my_decision = Action.D
        self._my_last_action = my_decision
        return my_decision

class Strategy_COLLECTIVE_219(BaseStrategy):
    """
    The Adaptive Threshold Enforcement (ATE) strategy is designed for the Collective Risk Dilemma.
    It fosters cooperation by initiating with trust, rewarding collective success (achieving 'm' cooperators),
    and punishing collective failure to meet the threshold. This adaptive approach aims to guide
    the collective towards sustainable cooperation, balancing individual protection with group welfare.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= 2:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        N_C_prev = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if N_C_prev >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_220(BaseStrategy):
    """
    Strategy: Robust Collective Threshold (RCT)

    The Robust Collective Threshold (RCT) strategy aims to consistently secure
    the collective 'k' bonus in the Collective Risk Dilemma.
    It prioritizes collective success by cooperating reliably and only attempts
    to free-ride under very specific, low-risk conditions, preventing self-induced
    oscillations or system collapse. It adapts its commitment to cooperation
    immediately following collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds_total = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        my_action_prev_bool = history.my_actions[-1]
        opponents_cooperated_prev = np.sum(history.opponent_actions[-1, :])
        N_C_prev = int(my_action_prev_bool) + int(opponents_cooperated_prev)
        if current_round_1_indexed == n_rounds_total:
            if N_C_prev >= m_threshold:
                return Action.C
            else:
                return Action.D
        if N_C_prev < m_threshold:
            return Action.C
        elif my_action_prev_bool == True:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_221(BaseStrategy):
    """
    The Adaptive Collective Risk Strategy (ACRS) is designed to balance
    collective cooperation with individual protection against exploitation.
    It initiates cooperation, then adapts its behavior based on whether
    the collective cooperation threshold (m) was met in previous rounds.
    It tolerates a certain number of consecutive failures before defecting,
    but always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_threshold = game_description.m
        self.n_rounds = game_description.n_rounds
        self._strikes_against_cooperation: int = 0
        self._max_strikes_before_defection: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_last_round >= self.m_threshold:
            self._strikes_against_cooperation = 0
        else:
            self._strikes_against_cooperation += 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        elif self._strikes_against_cooperation < self._max_strikes_before_defection:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_222(BaseStrategy):
    """
    Adaptive Collective Maintainer (ACM) strategy for the Collective Risk Dilemma.

    This strategy aims to consistently achieve the collective reward `k` by fostering cooperation
    within the group, while also being robust against exploitation and adapting to varying
    opponent behaviors. It prioritizes the collective good but acknowledges individual
    rationality in specific contexts.

    It starts with cooperation, reinforces success, temporarily "punishes" collective failure,
    and includes a "forgiveness" mechanism to re-attempt cooperation after prolonged failure,
    preventing permanent defection spirals.
    """
    _RESET_FAILURE_THRESHOLD_DEFAULT = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_failures: int = 0
        self._reset_failure_threshold: int = self._RESET_FAILURE_THRESHOLD_DEFAULT

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if state.round_number == 0:
            self._consecutive_failures = 0
            return Action.C
        elif current_round_number_1_indexed == total_rounds:
            return Action.D
        else:
            num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
            my_action_last_round = history.my_actions[-1]
            num_cooperators_last_round = num_opponent_cooperators_last_round + (1 if my_action_last_round else 0)
            if num_cooperators_last_round >= min_cooperators:
                self._consecutive_failures = 0
                return Action.C
            else:
                self._consecutive_failures += 1
                if self._consecutive_failures >= self._reset_failure_threshold:
                    self._consecutive_failures = 0
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_223(BaseStrategy):
    """
    Forgiving Trigger with Dropout (FTD) strategy for the Collective Risk Dilemma.

    This strategy embodies a collective mindset by initiating cooperation and sustaining it
    when the group is successful. It protects itself from exploitation and persistent
    failure through conditional defection and a mechanism to permanently withdraw from
    the collective effort if failures are too frequent.

    Strategy Parameters:
    - PUNISHMENT_DURATION: The number of rounds the strategy will defect immediately
                           following a single round where the collective threshold 'm'
                           was *not* met. A value of 1 means a quick "punishment" or
                           protective defection.
    - DROPOUT_THRESHOLD: The number of *consecutive* rounds the collective threshold 'm'
                         must fail to be met before this strategy gives up on cooperation permanently.
                         Calculated as `max(3, floor(r / 5))`, ensuring a minimum level of patience.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PUNISHMENT_DURATION = 1
        self.DROPOUT_THRESHOLD = max(3, math.floor(self.game_description.n_rounds / 5))
        self.consecutive_failed_rounds = 0
        self.is_in_punishment_mode = False
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number > 0:
            previous_round_0_indexed = state.round_number - 1
            num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_0_indexed, :]) + int(history.my_actions[previous_round_0_indexed])
            if num_cooperators_prev_round < self.game_description.m:
                self.consecutive_failed_rounds += 1
                if not self.is_in_punishment_mode and self.consecutive_failed_rounds < self.DROPOUT_THRESHOLD:
                    self.is_in_punishment_mode = True
                    self.punishment_rounds_remaining = self.PUNISHMENT_DURATION
            else:
                self.consecutive_failed_rounds = 0
                self.is_in_punishment_mode = False
                self.punishment_rounds_remaining = 0
            if self.is_in_punishment_mode:
                self.punishment_rounds_remaining -= 1
                if self.punishment_rounds_remaining <= 0:
                    self.is_in_punishment_mode = False
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        if self.consecutive_failed_rounds >= self.DROPOUT_THRESHOLD:
            return Action.D
        if self.is_in_punishment_mode:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_224(BaseStrategy):
    """
    Adaptive Collective Endeavor (ACE) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation towards
    achieving the 'm' threshold. It starts by offering cooperation, sustains it
    when successful, and adapts to failures by offering a grace period before
    withdrawing support in the face of persistent defection. It acknowledges
    the inevitable breakdown of cooperation in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failed_rounds_in_a_row: int = 0
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        total_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            self.my_last_action = Action.C
            return Action.C
        if current_round_number == total_rounds - 1:
            self.my_last_action = Action.D
            return Action.D
        n_c_prev_opponents = np.sum(history.opponent_actions[-1, :])
        my_prev_action_was_C = history.my_actions[-1]
        n_c_prev = n_c_prev_opponents + (1 if my_prev_action_was_C else 0)
        if n_c_prev >= m:
            self.failed_rounds_in_a_row = 0
            self.my_last_action = Action.C
            return Action.C
        else:
            self.failed_rounds_in_a_row += 1
            if n_c_prev >= m - 1:
                self.my_last_action = Action.C
                return Action.C
            elif self.failed_rounds_in_a_row <= 2:
                self.my_last_action = Action.C
                return Action.C
            else:
                self.my_last_action = Action.D
                return Action.D

class Strategy_COLLECTIVE_225(BaseStrategy):
    """
    Adaptive Collective Restoration (ACR) strategy for the Collective Risk Dilemma.
    This strategy aims to achieve sustained cooperation by initiating cooperation,
    maintaining it when the collective target (m) is met, punishing collective failure
    with defection, and then attempting to restore cooperation after a persistent failure
    to prevent a permanent collapse. It adapts to end-game dynamics by defecting in the
    final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        prev_round_idx = state.round_number - 1
        opp_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev = history.my_actions[prev_round_idx]
        C_count_prev = int(opp_cooperators_prev + (1 if my_action_prev else 0))
        was_prev_round_successful = C_count_prev >= self.m
        if was_prev_round_successful:
            return Action.C
        else:
            was_round_t_minus_2_failed = False
            if current_round_1_indexed >= 3:
                t_minus_2_round_idx = state.round_number - 2
                opp_cooperators_t_minus_2 = np.sum(history.opponent_actions[t_minus_2_round_idx, :])
                my_action_t_minus_2 = history.my_actions[t_minus_2_round_idx]
                C_count_t_minus_2 = int(opp_cooperators_t_minus_2 + (1 if my_action_t_minus_2 else 0))
                was_round_t_minus_2_failed = C_count_t_minus_2 < self.m
            if not was_round_t_minus_2_failed:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_226(BaseStrategy):
    """
    Cooperative Enforcement with Free-Rider Deterrence (CE-FRD) strategy.

    This strategy aims to secure the collective reward 'k' by prioritizing cooperation,
    but it is not a persistent "sucker". It tolerates some free-riding to achieve
    the collective good. However, repeated exploitation will trigger a defensive,
    albeit risky, punishment (defection) to encourage broader contribution. Punishments
    are short-lived to allow for quick recovery of cooperation. It also adapts to
    game-end incentives by defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.last_punishment_trigger_round: int = 0
        self.consecutive_free_rider_rounds: int = 0
        self.FR_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t_1_indexed = state.round_number + 1
        if current_round_t_1_indexed == self.n_rounds:
            return Action.D
        if current_round_t_1_indexed == 1:
            self.last_punishment_trigger_round = 0
            self.consecutive_free_rider_rounds = 0
            return Action.C
        my_action_prev = history.my_actions[-1]
        N_c_prev = np.sum(history.opponent_actions[-1, :]) + int(my_action_prev)
        if current_round_t_1_indexed == self.last_punishment_trigger_round + 1:
            return Action.D
        if my_action_prev == Action.C.value and N_c_prev >= self.m_threshold and (N_c_prev < self.n_players):
            self.consecutive_free_rider_rounds += 1
        else:
            self.consecutive_free_rider_rounds = 0
        if N_c_prev < self.m_threshold:
            self.last_punishment_trigger_round = current_round_t_1_indexed
            return Action.D
        if self.consecutive_free_rider_rounds >= self.FR_threshold:
            self.last_punishment_trigger_round = current_round_t_1_indexed
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_227(BaseStrategy):
    """
    Dynamic Threshold Contributor (DTC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster conditional cooperation, react adaptively to
    collective outcomes, and balance individual payoffs with the achievement
    of the collective reward.

    Decision Logic:
    1.  **First Round (Round 1):** Always Cooperate (C). This initiates a
        cooperative signal and tests the waters for collective success.
    2.  **Last Round (Round r):** Always Defect (D). Standard backward
        induction dictates this as the individually rational choice in the
        absence of future interactions.
    3.  **Intermediate Rounds (1 < t < r):** The strategy dynamically flips
        its own action from the immediately preceding round. That is, if the
        player cooperated in the last round, they defect in the current round;
        if they defected, they cooperate. This consistent flipping occurs
        regardless of whether the collective cooperation threshold (`m`) was
        met or not. The underlying rationale, as described in the strategy,
        is to encourage a dynamic equilibrium of alternating contributions
        and free-riding, adapting to both collective success and failure.

    The strategy maintains an internal state `_my_last_action` to remember
    its choice from the immediately preceding round, ensuring persistent,
    instance-specific behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        action_for_this_round: Action
        if current_round_number == 0:
            action_for_this_round = Action.C
        elif current_round_number == total_rounds - 1:
            action_for_this_round = Action.D
        else:
            action_for_this_round = Action(not self._my_last_action.value)
        self._my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_COLLECTIVE_228(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain collective cooperation by initiating
    cooperative behavior, rewarding successful group outcomes with continued cooperation,
    and signaling displeasure (by withdrawing cooperation) when the group fails to
    achieve the collective good. It prioritizes the sustained achievement of the
    collective reward 'k' for all players over short-term individual exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round = history.my_actions[-1]
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round
        if my_action_prev_round:
            num_total_cooperators_prev_round += 1
        if num_total_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_229(BaseStrategy):
    """
    Adaptive Cooperation with Punish & Forgive (ACPF) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation to meet the collective threshold (`m`) and secure
    the `k` reward, while being adaptive to group behavior and protecting the individual player
    from consistent exploitation. It relies solely on game parameters and the observable history
    of actions from previous rounds.

    Decision Rules:
    - First Round (t=1): Always Cooperate (C) to initiate cooperation.
    - Last Round (t=r): Always Defect (D) based on backward induction.
    - Intermediate Rounds (1 < t < r):
        - If collective threshold `m` was met in the previous round: Continue to Cooperate (C).
        - If collective threshold `m` was NOT met AND this player Cooperated in the previous round: Defect (D) as a "punishment" signal.
        - If collective threshold `m` was NOT met AND this player Defected in the previous round: Cooperate (C) to "forgive" and re-attempt cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_C_prev = int(my_action_prev) + sum(history.opponent_actions[-1, :])
        if num_C_prev >= self.m:
            return Action.C
        elif num_C_prev < self.m and my_action_prev == True:
            return Action.D
        elif num_C_prev < self.m and my_action_prev == False:
            return Action.C

class Strategy_COLLECTIVE_230(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy aims to establish and maintain
    a state where the minimum cooperation threshold (`m`) is met. It starts cooperatively,
    rewards collective success with continued cooperation, and signals dissatisfaction
    with temporary defection when collective efforts fall short, specifically when
    the player itself has contributed. It incorporates a forgiveness mechanism to
    prevent permanent defection spirals by using a short punishment duration.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_to_defect: int = 0
        self.PUNISHMENT_DURATION: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_index = state.round_number - 1
        my_action_prev: np.bool_ = history.my_actions[previous_round_index]
        num_opponent_cooperators_prev: int = np.sum(history.opponent_actions[previous_round_index, :])
        N_C_prev: int = num_opponent_cooperators_prev + (1 if my_action_prev else 0)
        m: int = self.game_description.m
        if self.rounds_to_defect > 0:
            self.rounds_to_defect -= 1
        elif my_action_prev == True and N_C_prev < m:
            self.rounds_to_defect = self.PUNISHMENT_DURATION
        if self.rounds_to_defect > 0:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_231(BaseStrategy):
    """
    The Adaptive Threshold Cooperator (ATC) strategy for the Collective Risk Dilemma.

    This strategy aims to encourage and sustain collective cooperation by adaptively
    responding to the group's performance relative to the critical 'm' cooperator threshold.
    It acts as a conditional cooperator, initially testing the waters, adapting based on
    observed collective cooperation levels, and escalating punishment if the collective
    goal (m cooperators) is not met. It prioritizes achieving the collective benefit
    ('k' bonus) while being robust against exploitation, particularly in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_duration = 1
        self.my_current_state = 'INITIAL'
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == 1:
            self.my_current_state = 'COOPERATING'
            self.punishment_rounds_remaining = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        my_action_last_round = history.my_actions[-1]
        cooperators_in_last_round = num_opponent_cooperators_last_round + (1 if my_action_last_round else 0)
        if self.my_current_state == 'PUNISHING':
            if self.punishment_rounds_remaining > 0:
                self.punishment_rounds_remaining -= 1
                return Action.D
            else:
                self.my_current_state = 'COOPERATING'
                return Action.C
        if self.my_current_state == 'COOPERATING':
            if cooperators_in_last_round >= min_cooperators_needed:
                return Action.C
            else:
                self.my_current_state = 'PUNISHING'
                self.punishment_rounds_remaining = self.punishment_duration - 1
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_232(BaseStrategy):
    """
    Adaptive Collective Effort strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain collective cooperation by
    responding adaptively to the group's performance in meeting the minimum
    cooperation threshold 'm'. It starts with cooperation, sustains it during
    collective success, initiates a short collective punishment (defection)
    after failure to meet the threshold, and defects in the final round to
    avoid exploitation.
    """
    PUNISHMENT_DURATION: int = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_countdown: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.punishment_countdown = 0
            return Action.C
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            return Action.D
        if num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            self.punishment_countdown = self.PUNISHMENT_DURATION
            return Action.D

class Strategy_COLLECTIVE_233(BaseStrategy):
    """
    Adaptive Collective Response (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize the collective payoff by promoting cooperation
    when the collective goal (m cooperators) is met and withdrawing support
    when it isn't, thus acting as a signal and a protective measure.

    Decision Rules:
    1. Round 1 (Initial Play): Cooperate (C).
    2. Rounds 2 to (r-1) (Adaptive Play):
       - If the collective threshold (m) was met in the immediately preceding round, Cooperate (C).
       - Otherwise (threshold not met), Defect (D).
    3. Round r (Final Play): Defect (D).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_action_in_prev_round = int(history.my_actions[-1])
        opponent_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
        previous_round_total_cooperators = my_action_in_prev_round + opponent_cooperators_in_prev_round
        if previous_round_total_cooperators >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_234(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation (ARC) strategy for the Collective Risk Dilemma.

    This strategy aims to achieve collective benefits by fostering cooperation,
    adapting to the group's performance, and ensuring individual actions
    contribute to the desired outcome without being consistently exploited.

    Decision Rules:
    - In Round 1 (state.round_number == 0), the player always Cooperates (C) to
      establish a cooperative intent.
    - In subsequent rounds (state.round_number > 0), the player observes the
      total number of cooperators from the previous round (C_t-1).
      - If C_t-1 was greater than or equal to the minimum cooperation threshold (m),
        the player Cooperates (C) again to maintain collective success.
      - If C_t-1 was less than m, the player Defects (D) to signal collective
        failure, avoid unrewarded sacrifice, and prompt other players to re-evaluate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ARC strategy with the game's parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators needed),
                              and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing records of past actions and payoffs
                     for this player and opponents, or None for the first round.

        Returns:
            An Action enum value (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        else:
            previous_round_cooperators_count = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            m_threshold = self.game_description.m
            if previous_round_cooperators_count >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_235(BaseStrategy):
    """
    Implements the 'Resilient Collective Contributor' strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain collective cooperation by showing initial willingness,
    rewarding collective success, and demonstrating patience in the face of temporary setbacks before
    resorting to punishment for persistent failure. It is designed to maximize overall game payoff
    by securing the 'k' reward for the group, while protecting individual payoff against persistent free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and internal state.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, m, k.
        """
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player and opponents,
                     or None if it's the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_t: int = state.round_number + 1
        total_rounds_r: int = self.game_description.n_rounds
        min_cooperators_m: int = self.game_description.m
        if state.round_number > 0:
            C_prev: int = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if C_prev >= min_cooperators_m:
                self.consecutive_failures = 0
            else:
                self.consecutive_failures += 1
        if current_round_t == total_rounds_r:
            return Action.D
        if self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures <= 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_236(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for the Collective Risk Dilemma.

    The ATC strategy aims to ensure the collective reward 'k' is secured by meeting
    the 'm' cooperator threshold as consistently as possible. It is a conditional
    cooperator, starting with a commitment to cooperation, then adapting its behavior
    based on the group's performance in previous rounds. It includes a limited
    self-protection mechanism to avoid exploitation when cooperation levels are
    excessively high or when the collective repeatedly fails. End-game rationality
    is applied in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        prev_cooperators = np.sum(history.opponent_actions[prev_round_idx]) + history.my_actions[prev_round_idx]
        my_prev_action_is_C = history.my_actions[prev_round_idx]
        if prev_cooperators >= m:
            if prev_cooperators == m:
                return Action.C
            elif my_prev_action_is_C and n_rounds - state.round_number > 2:
                return Action.D
            else:
                return Action.C
        elif prev_cooperators >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_237(BaseStrategy):
    """
    Adaptive Collective Vigilance fosters and maintains cooperation by being
    initially trustful, conditionally cooperative, and resilient to exploitation,
    all while prioritizing the collective outcome of achieving the 'k' reward.
    It adapts its behavior based on observed outcomes and aims to robustly
    encourage sufficient cooperation (m players) without being a perpetual
    "sucker" to persistent free-riders.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.max_failures_tolerated = 2
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == 1:
            self.consecutive_failures = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_is_cooperate = history.my_actions[-1]
        N_C_prev = num_opponent_cooperators_prev + (1 if my_action_prev_is_cooperate else 0)
        if N_C_prev >= min_cooperators_needed:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures <= self.max_failures_tolerated:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_238(BaseStrategy):
    """
    This strategy promotes and sustains collective cooperation in the Collective Risk Dilemma.
    It cooperates initially, maintains cooperation after success, punishes defection after failure,
    and attempts to re-initiate cooperation after prolonged periods of collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._rounds_since_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        if state.round_number == 0:
            current_action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            current_action = Action.D
        else:
            prev_round_idx = state.round_number - 1
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
            my_action_prev_bool = history.my_actions[prev_round_idx]
            C_prev_round = num_opponent_cooperators_prev + (1 if my_action_prev_bool else 0)
            if C_prev_round >= self.game_description.m:
                self._rounds_since_success = 0
                current_action = Action.C
            else:
                self._rounds_since_success += 1
                if self._my_last_action == Action.C:
                    current_action = Action.D
                elif self._rounds_since_success > 3:
                    current_action = Action.C
                else:
                    current_action = Action.D
        self._my_last_action = current_action
        return current_action

class Strategy_COLLECTIVE_239(BaseStrategy):
    """
    The Adaptive Collective Steward strategy aims to foster cooperation and achieve the collective
    reward (`k`) for the group while being robust against various opponent behaviors, including
    free-riding and persistent defection. It starts cooperatively, attempts to maintain cooperation,
    punishes failures to meet the cooperation threshold (`m`), and offers opportunities for recovery.
    It only resorts to permanent defection after repeated breakdowns of collective effort.
    Crucially, it avoids opportunistic defection (free-riding) when the collective goal is met.
    """
    PUNISHMENT_DURATION_BASE = 2
    MAX_RECOVERY_ATTEMPTS = 2

    class Status(Enum):
        """Internal states for the Adaptive Collective Steward strategy."""
        COOPERATING = 0
        PUNISHING = 1
        RECOVERING = 2
        ABANDONED = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_status: self.Status = self.Status.COOPERATING
        self.punishment_rounds_left: int = 0
        self.recovery_attempts_made: int = 0
        self.has_abandoned_collective_effort: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on internal state
        and observed history.
        """
        if state.round_number == 0:
            return Action.C
        if self.has_abandoned_collective_effort:
            return Action.D
        my_action_prev = history.my_actions[state.round_number - 1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        num_cooperators_prev = int(my_action_prev) + opponent_cooperators_prev
        if self.current_status == self.Status.PUNISHING:
            self.punishment_rounds_left -= 1
            if self.punishment_rounds_left == 0:
                self.current_status = self.Status.RECOVERING
        elif self.current_status == self.Status.COOPERATING or self.current_status == self.Status.RECOVERING:
            if num_cooperators_prev < self.game_description.m:
                self.recovery_attempts_made += 1
                if self.recovery_attempts_made >= self.MAX_RECOVERY_ATTEMPTS and state.round_number < self.game_description.n_rounds - 1:
                    self.has_abandoned_collective_effort = True
                    self.current_status = self.Status.ABANDONED
                    return Action.D
                else:
                    self.current_status = self.Status.PUNISHING
                    self.punishment_rounds_left = self.PUNISHMENT_DURATION_BASE
            else:
                if self.current_status == self.Status.RECOVERING:
                    self.recovery_attempts_made = 0
                self.current_status = self.Status.COOPERATING
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators_prev < self.game_description.m:
                return Action.D
            else:
                return Action.C
        if self.current_status == self.Status.PUNISHING:
            return Action.D
        elif self.current_status == self.Status.RECOVERING:
            return Action.C
        elif self.current_status == self.Status.COOPERATING:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_240(BaseStrategy):
    """
    Phased Collective Resilience (PCR) strategy for the Collective Risk Dilemma.

    This strategy aims to achieve and maintain collective cooperation by adapting its
    behavior based on the group's success in meeting the cooperation threshold in
    previous rounds. It operates with two cyclical phases: COOPERATE_ATTEMPT, where
    players persist in cooperating to achieve the threshold, and COLLECTIVE_PUNISH,
    where players defect to signal dissatisfaction and reset expectations if cooperation
    persistently fails. The strategy also includes an end-game defection for the last round.
    """

    class PhasedCollectiveResiliencePhase(Enum):
        """
        Internal states for the Phased Collective Resilience strategy.
        - COOPERATE_ATTEMPT: The strategy is currently trying to achieve cooperation.
        - COLLECTIVE_PUNISH: The strategy is currently punishing collective failure.
        """
        COOPERATE_ATTEMPT = 1
        COLLECTIVE_PUNISH = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_C: int = 2
        self.W_P: int = 1
        self.phase_state: self.PhasedCollectiveResiliencePhase = self.PhasedCollectiveResiliencePhase.COOPERATE_ATTEMPT
        self.phase_round_counter: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t_indexed: int = state.round_number + 1
        if state.round_number == 0:
            self.phase_state = self.PhasedCollectiveResiliencePhase.COOPERATE_ATTEMPT
            self.phase_round_counter = 1
            return Action.C
        if current_round_t_indexed == self.game_description.n_rounds:
            return Action.D
        n_cooperators_prev_round: int = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        m_threshold: int = self.game_description.m
        if n_cooperators_prev_round >= m_threshold:
            self.phase_state = self.PhasedCollectiveResiliencePhase.COOPERATE_ATTEMPT
            self.phase_round_counter = 1
            return Action.C
        elif self.phase_state == self.PhasedCollectiveResiliencePhase.COOPERATE_ATTEMPT:
            self.phase_round_counter += 1
            if self.phase_round_counter <= self.W_C:
                return Action.C
            else:
                self.phase_state = self.PhasedCollectiveResiliencePhase.COLLECTIVE_PUNISH
                self.phase_round_counter = 1
                return Action.D
        elif self.phase_state == self.PhasedCollectiveResiliencePhase.COLLECTIVE_PUNISH:
            self.phase_round_counter += 1
            if self.phase_round_counter <= self.W_P:
                return Action.D
            else:
                self.phase_state = self.PhasedCollectiveResiliencePhase.COOPERATE_ATTEMPT
                self.phase_round_counter = 1
                return Action.C

class Strategy_COLLECTIVE_241(BaseStrategy):
    """
    Adaptive Conditional Cooperator strategy for the Collective Risk Dilemma.
    This strategy aims to maximize collective payoff by starting with cooperation
    and maintaining it as long as the threshold `m` is met. It tolerates a single
    failure but defects on persistent collective failure to protect itself from
    exploitation and signal the unsustainability of the current cooperation level.
    In the final round, it prioritizes individual rationality by defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        elif state.round_number == n_rounds - 1:
            return Action.D
        else:
            cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if cooperators_prev_round >= m_threshold:
                self.consecutive_failures = 0
                return Action.C
            else:
                self.consecutive_failures += 1
                if self.consecutive_failures == 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_242(BaseStrategy):
    """
    Adaptive Collective Commitment (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to balance collective good with individual robustness.
    It cooperates in the first round to proactively establish a cooperative
    equilibrium. In intermediate rounds, it continues to cooperate if the
    collective met the cooperation threshold in the previous round, thereby
    sustaining the collective reward. If the threshold was not met, it defects
    as a signal of collective failure, potentially incentivizing a return to
    cooperation. In the final round, it defects due to end-game rationality,
    maximizing individual payoff without future consequence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        n_c_prev_opponents = sum(history.opponent_actions[-1, :])
        n_c_prev_me = int(history.my_actions[-1])
        n_c_prev_total = n_c_prev_opponents + n_c_prev_me
        if n_c_prev_total >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_243(BaseStrategy):
    """
    Adaptive Collective Maximizer (ACM) strategy for the Collective Risk Dilemma.
    This strategy aims to achieve a stable cooperative state where the 'm' threshold
    is met consistently, while also allowing for strategic individual benefit from
    defection without collapsing the system. It balances initial trust, conditional
    reciprocity, and self-preservation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._consecutive_failures: int = 0
        self._consecutive_successes: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if current_round == 1:
            self._my_last_action = Action.C
            return Action.C
        prev_round_idx = state.round_number - 1
        my_prev_action_bool = history.my_actions[prev_round_idx]
        opponent_cooperators_prev = sum(history.opponent_actions[prev_round_idx, :])
        N_C_prev = (1 if my_prev_action_bool else 0) + opponent_cooperators_prev
        if N_C_prev >= m:
            self._consecutive_successes += 1
            self._consecutive_failures = 0
        else:
            self._consecutive_failures += 1
            self._consecutive_successes = 0
        current_action: Action = Action.D
        if N_C_prev >= m:
            if self._my_last_action == Action.C:
                if N_C_prev == m:
                    current_action = Action.C
                else:
                    current_action = Action.D
            elif N_C_prev == m:
                current_action = Action.C
            else:
                current_action = Action.D
        elif current_round == r:
            current_action = Action.D
        elif self._consecutive_failures <= 1:
            current_action = Action.C
        else:
            current_action = Action.D
        self._my_last_action = current_action
        return current_action

class Strategy_COLLECTIVE_244(BaseStrategy):
    """
    Implements the "Robust Collective Reciprocity" strategy for the Collective Risk Dilemma.
    This strategy is designed to balance the collective goal of achieving the shared reward 'k'
    with the need for self-preservation. It is adaptive, reactive, and incorporates a degree
    of patience to facilitate stable cooperation, while defecting in the last round or
    in response to persistent collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds: int = 0
        self.Patience_Limit: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t == 1:
            self.consecutive_failure_rounds = 0
            return Action.C
        if current_round_t == total_rounds_r:
            return Action.D
        previous_round_index = state.round_number - 1
        my_action_prev = history.my_actions[previous_round_index]
        opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_index, :])
        total_observed_cooperators_prev = my_action_prev + opponent_cooperators_prev
        if total_observed_cooperators_prev >= min_cooperators_m:
            self.consecutive_failure_rounds = 0
        else:
            self.consecutive_failure_rounds += 1
        if self.consecutive_failure_rounds == 0:
            return Action.C
        elif self.consecutive_failure_rounds <= self.Patience_Limit:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_245(BaseStrategy):
    """
    The Collective Stability Seeker (CSS) strategy for the Collective Risk Dilemma.
    It aims to maximize total payoff by fostering and maintaining cooperation within the group,
    being adaptive to opponent behaviors. It starts by cooperating, enforces cooperation
    through conditional defection, and offers periodic opportunities for the collective
    to return to a cooperative state via a forgiveness mechanism. It handles the last
    round by prioritizing self-protection, except in cases of exceptionally stable
    and perfect cooperation throughout the entire game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.forgive_period = 2
        self.consecutive_fail_punish_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number + 1
        if current_game_round == 1:
            self.consecutive_fail_punish_rounds = 0
            return Action.C
        prev_round_idx = state.round_number - 1
        num_cooperators_prev_round = int(history.my_actions[prev_round_idx]) + np.sum(history.opponent_actions[prev_round_idx, :])
        if current_game_round == self.game_description.n_rounds:
            if num_cooperators_prev_round >= self.game_description.m:
                all_prev_rounds_successful = True
                for t_idx in range(state.round_number):
                    total_cooperators_in_t = int(history.my_actions[t_idx]) + np.sum(history.opponent_actions[t_idx, :])
                    if total_cooperators_in_t < self.game_description.m:
                        all_prev_rounds_successful = False
                        break
                if all_prev_rounds_successful:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        if num_cooperators_prev_round >= self.game_description.m:
            self.consecutive_fail_punish_rounds = 0
            return Action.C
        else:
            self.consecutive_fail_punish_rounds += 1
            if self.consecutive_fail_punish_rounds >= self.forgive_period:
                self.consecutive_fail_punish_rounds = 0
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_246(BaseStrategy):
    """
    Adaptive Collective Threshold Strategy (ACTS) aims to foster and maintain collective cooperation by responding
    to the group's ability to meet the cooperation threshold `m`. It starts with an optimistic cooperative stance,
    punishes collective failures with a brief period of defection, and then attempts to re-establish cooperation.
    It explicitly handles the first and last rounds as critical edge cases.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_status: bool = True
        self.punishment_rounds_left: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_count = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_0_indexed == total_rounds_count - 1:
            return Action.D
        if current_round_0_indexed == 0:
            self.cooperation_status = True
            self.punishment_rounds_left = 0
            return Action.C
        my_prev_action_was_C = history.my_actions[current_round_0_indexed - 1]
        opponents_prev_actions_were_C = history.opponent_actions[current_round_0_indexed - 1, :]
        n_C_prev = int(my_prev_action_was_C) + np.sum(opponents_prev_actions_were_C)
        if n_C_prev >= min_cooperators_m:
            self.cooperation_status = True
            self.punishment_rounds_left = 0
        elif self.cooperation_status == True:
            self.cooperation_status = False
            self.punishment_rounds_left = 1
        if self.cooperation_status == True:
            return Action.C
        elif self.punishment_rounds_left > 0:
            self.punishment_rounds_left -= 1
            return Action.D
        else:
            self.cooperation_status = True
            return Action.C

class Strategy_COLLECTIVE_247(BaseStrategy):
    """
    Adaptive Collective Enforcer: This strategy aims to foster and maintain collective cooperation by
    leading with contribution, sustaining it when successful, punishing collective failure (especially
    when exploited), and attempting to re-initiate cooperation when it failed to contribute to a
    successful outcome. It balances the collective goal of meeting the cooperation threshold with
    the need for individual self-preservation against exploitative behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.total_rounds:
            return Action.D
        my_action_t_minus_1 = history.my_actions[-1]
        num_cooperators_t_minus_1 = int(my_action_t_minus_1) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_t_minus_1 >= self.m_threshold:
            return Action.C
        elif my_action_t_minus_1 is True:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_248(BaseStrategy):
    """
    The Adaptive Collective Endeavour (ACE) strategy for the Collective Risk Dilemma.

    ACE balances initial good faith cooperation with pragmatic self-protection.
    It starts by cooperating unconditionally in the first round. In subsequent
    rounds, it observes the group's performance against the cooperation threshold (m).
    It continues to cooperate if the threshold was met in the previous round, or if
    it was the first consecutive failure to meet the threshold (a "second chance").
    However, if the group fails to meet the threshold for two or more consecutive
    rounds, ACE defects as a signal of dissatisfaction and to avoid persistent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_failed_threshold_consecutively = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.rounds_failed_threshold_consecutively = 0
            return Action.C
        my_prev_action_is_cooperate = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_total_cooperators = int(my_prev_action_is_cooperate) + opponent_prev_cooperators
        if prev_total_cooperators >= self.game_description.m:
            self.rounds_failed_threshold_consecutively = 0
        else:
            self.rounds_failed_threshold_consecutively += 1
        if prev_total_cooperators >= self.game_description.m:
            return Action.C
        elif self.rounds_failed_threshold_consecutively == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_249(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy aims to foster and maintain
    collective cooperation in the Collective Risk Dilemma. It starts with a cooperative
    gesture, then adapts its behavior based on whether the collective cooperation
    threshold 'm' was met in the previous round. It punishes collective failures
    with a short defection period but quickly reverts to cooperation to avoid
    prolonged conflict. In the final round, it defects to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._current_state: str = 'Cooperating'
        self._punishment_rounds_left: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            self._current_state = 'Cooperating'
            self._punishment_rounds_left = 0
            return Action.C
        my_prev_action_val = int(history.my_actions[-1])
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = my_prev_action_val + opponent_cooperators_prev_round
        if self._current_state == 'Punishing':
            self._punishment_rounds_left -= 1
            if self._punishment_rounds_left > 0:
                return Action.D
            else:
                self._current_state = 'Cooperating'
                return Action.C
        elif total_cooperators_prev_round >= m_threshold:
            return Action.C
        else:
            self._current_state = 'Punishing'
            self._punishment_rounds_left = 1
            return Action.D

class Strategy_COLLECTIVE_250(BaseStrategy):
    """
    The "Collective Endeavor" strategy for the Collective Risk Dilemma.

    This strategy aims to foster and sustain collective cooperation by adaptively
    responding to the group's past performance. It prioritizes the achievement
    of the collective good while also protecting against persistent exploitation.

    It initiates cooperation, consistently contributes when the collective goal is met,
    and offers second chances after moderate failures. It only withdraws its contribution
    when the collective effort is severely lacking or when continued cooperation offers
    no future benefit (e.g., the last round after a failure). This aligns with a
    collective mindset by actively working towards the common reward (k) for all players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        my_prev_action_int = int(history.my_actions[current_round - 1])
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
        C_prev = my_prev_action_int + num_opponent_cooperators_prev_round
        if current_round == self.n_rounds - 1:
            if C_prev >= self.m:
                return Action.C
            else:
                return Action.D
        if C_prev >= self.m:
            return Action.C
        elif C_prev < math.floor(self.m / 2):
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_251(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy prioritizes the collective good by aiming to
    consistently meet the 'm' cooperators threshold. It starts by cooperating and then makes
    decisions in subsequent rounds based on whether the collective met the 'm' threshold in the
    previous round. If the threshold was met, it cooperates; otherwise, it defects as a collective
    punishment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_prev_action_cooperated = history.my_actions[state.round_number - 1]
        opponent_prev_actions_cooperated = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(my_prev_action_cooperated) + np.sum(opponent_prev_actions_cooperated)
        if num_cooperators_prev_round < self.game_description.m:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_252(BaseStrategy):
    """
    Adaptive Collective Assurance (ACA) strategy for the Collective Risk Dilemma.
    ACA aims to foster and maintain collective success by adapting its behavior
    based on the collective outcome of the previous round. It cooperates when
    the collective threshold `m` is met or as an initial trust-building move,
    and defects (punishes) when the threshold is not met, to signal disapproval
    and avoid being a "sucker." It forgives and re-cooperates if the collective
    recovers, but always defects in the final round due to terminal effects.
    """

    class AgentState(Enum):
        """
        Internal states for the ACA strategy.
        - COOPERATE_MODE: Agent's default stance, aiming to contribute to collective success.
        - PUNISH_MODE: Agent defects to signal disapproval and avoid being a "sucker"
                       following a collective failure.
        """
        COOPERATE_MODE = 1
        PUNISH_MODE = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._agent_state = self.AgentState.COOPERATE_MODE

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            self._agent_state = self.AgentState.COOPERATE_MODE
            return Action.C
        c_count_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self._agent_state == self.AgentState.COOPERATE_MODE:
            if c_count_prev >= m:
                return Action.C
            else:
                self._agent_state = self.AgentState.PUNISH_MODE
                return Action.D
        elif c_count_prev >= m:
            self._agent_state = self.AgentState.COOPERATE_MODE
            return Action.C
        else:
            self._agent_state = self.AgentState.PUNISH_MODE
            return Action.D

class Strategy_COLLECTIVE_253(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    It initiates cooperation, rewards sustained collective success by continuing to cooperate,
    and signals disapproval of collective failure by defecting. It's adaptive,
    reacting to the collective outcome of the previous round to nudge the system
    towards meeting the 'm' cooperator threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACE strategy.

        Args:
            state: The current game state, including the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player and opponents.
                     It is None for the very first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        my_prev_action_coop_val = 1 if history.my_actions[-1] else 0
        opponent_prev_cooperators_count = np.sum(history.opponent_actions[-1, :])
        c_count_prev = my_prev_action_coop_val + opponent_prev_cooperators_count
        if c_count_prev >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_254(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation through initial optimism and conditional
    reciprocity, balanced with a mechanism to deter consistent free-riding or failure.
    It starts by cooperating, forgives a limited number of collective failures to meet
    the 'm' threshold, but switches to defection if failures are persistent. In the
    final round, it always defects due to backward induction.
    """
    FAILURE_TOLERANCE_THRESHOLD: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failed_collective_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if state.round_number == 0:
            return Action.C
        else:
            my_prev_action_bool = history.my_actions[-1]
            opponent_prev_actions_bool = history.opponent_actions[-1, :]
            cooperators_in_prev_round = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bool)
            if cooperators_in_prev_round >= min_cooperators_needed:
                self.failed_collective_rounds = 0
            else:
                self.failed_collective_rounds += 1
            if state.round_number == total_rounds - 1:
                return Action.D
            elif self.failed_collective_rounds >= self.FAILURE_TOLERANCE_THRESHOLD:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_255(BaseStrategy):
    """
    Adaptive Collective Consensus (ACC) strategy for the Collective Risk Dilemma.

    This strategy prioritizes collective success through initial cooperation,
    sustains successful states, punishes collective failures with temporary defection,
    and then attempts to re-initiate cooperation to break persistent cycles of failure.
    It incorporates a rational defection in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Consensus strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, m (min cooperators needed), k (reward factor).
        """
        self.game_description = game_description
        self.has_failed_since_last_success: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player
                     and opponents. It is None for the very first round (round 0).

        Returns:
            An Action enum member (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.has_failed_since_last_success = False
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            n_c_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if n_c_prev >= self.game_description.m:
                self.has_failed_since_last_success = False
                return Action.C
            elif not self.has_failed_since_last_success:
                self.has_failed_since_last_success = True
                return Action.D
            else:
                self.has_failed_since_last_success = False
                return Action.C

class Strategy_COLLECTIVE_256(BaseStrategy):
    """
    Enhanced Adaptive Collective Strategy (EACS) for the Collective Risk Dilemma.

    This strategy aims to foster and sustain collective cooperation while protecting
    itself from being consistently exploited. It initiates cooperation, adapts its
    behavior based on the previous round's outcome and its own action, incorporates
    a mechanism to retaliate against sustained individual exploitation (being a "sucker"),
    and adopts a rational defection in the final round.

    Internal State Variables:
    - _consecutive_sucker_rounds: An integer counter. This increments when the player
                                  cooperates, the collective threshold `m` is *exactly* met,
                                  and the player is thus potentially exploited. It resets otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_sucker_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if current_round_num == 0:
            self._consecutive_sucker_rounds = 0
            return Action.C
        if current_round_num == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_index = current_round_num - 1
        my_action_prev_round_is_C = history.my_actions[prev_round_index]
        cooperators_prev_round_count = np.sum(history.opponent_actions[prev_round_index]) + int(my_action_prev_round_is_C)
        if my_action_prev_round_is_C and cooperators_prev_round_count == self.game_description.m:
            self._consecutive_sucker_rounds += 1
        else:
            self._consecutive_sucker_rounds = 0
        action_for_this_round: Action
        if self._consecutive_sucker_rounds >= 3:
            action_for_this_round = Action.D
            self._consecutive_sucker_rounds = 0
        elif cooperators_prev_round_count >= self.game_description.m:
            if my_action_prev_round_is_C:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        elif my_action_prev_round_is_C:
            action_for_this_round = Action.D
        else:
            action_for_this_round = Action.C
        return action_for_this_round

class Strategy_COLLECTIVE_257(BaseStrategy):
    """
    Adaptive Collective Consensus (ACC) strategy for the Collective Risk Dilemma.
    This strategy aims to achieve and sustain collective cooperation by balancing
    initial trust, dynamic reciprocity, punishment for exploitation or failure,
    and self-protection against persistent group non-cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACC strategy with game parameters and internal state variables.
        """
        self.game_description = game_description
        self.my_previous_action: Action = Action.C
        self.consecutive_failure_rounds: int = 0
        self.permanent_defection_mode: bool = False
        self.MAX_CONSECUTIVE_FAILURES: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACC strategy rules.
        """
        current_action: Action
        if state.round_number == 0:
            current_action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            current_action = Action.D
        else:
            prev_round_index = state.round_number - 1
            my_action_in_prev_round_history = history.my_actions[prev_round_index]
            opponent_actions_in_prev_round = history.opponent_actions[prev_round_index, :]
            num_cooperators_prev = np.sum(opponent_actions_in_prev_round) + (1 if my_action_in_prev_round_history == Action.C else 0)
            num_defectors_prev = self.game_description.n_players - num_cooperators_prev
            if num_cooperators_prev < self.game_description.m:
                self.consecutive_failure_rounds += 1
            else:
                self.consecutive_failure_rounds = 0
            if self.consecutive_failure_rounds >= self.MAX_CONSECUTIVE_FAILURES:
                self.permanent_defection_mode = True
            if self.permanent_defection_mode:
                current_action = Action.D
            elif num_cooperators_prev >= self.game_description.m:
                if self.my_previous_action == Action.C:
                    if num_defectors_prev > self.game_description.n_players - self.game_description.m:
                        current_action = Action.D
                    else:
                        current_action = Action.C
                else:
                    current_action = Action.C
            elif self.my_previous_action == Action.C:
                current_action = Action.D
            else:
                current_action = Action.C
        self.my_previous_action = current_action
        return current_action

class Strategy_COLLECTIVE_258(BaseStrategy):
    """
    The Adaptive Trust & Punishment (ATP) strategy for the Collective Risk Dilemma.
    It starts with cooperation. If the collective threshold 'm' is met, it continues
    to cooperate. If the threshold is not met, it defects for a calculated period
    to signal dissatisfaction and protect individual payoff, then re-attempts
    cooperation. It always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_rounds_left: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        elif state.round_number == n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if self.punishment_rounds_left > 0:
                self.punishment_rounds_left -= 1
                return Action.D
            elif num_cooperators_prev_round >= m:
                return Action.C
            else:
                P_total_duration = max(1, min(n_rounds - state.round_number, (m + 1) // 2))
                self.punishment_rounds_left = P_total_duration - 1
                return Action.D

class Strategy_COLLECTIVE_259(BaseStrategy):
    """
    The Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.

    This strategy maintains a strong bias towards cooperation to ensure the collective
    threshold (m) is met. It tolerates single failures but responds to repeated
    collective failures by defecting, acting as a "punishment" mechanism to signal
    that persistent non-cooperation is unsustainable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_index = state.round_number - 1
        my_prev_action_was_cooperate = history.my_actions[previous_round_index]
        opponents_prev_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
        n_cooperators_prev_round = (1 if my_prev_action_was_cooperate else 0) + opponents_prev_cooperators
        threshold_met = n_cooperators_prev_round >= self.game_description.m
        if threshold_met:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.consecutive_failures >= 2:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_260(BaseStrategy):
    """
    The "Adaptive Collective Reciprocator" (ACR) strategy aims to foster collective
    cooperation to achieve the communal reward 'k' while remaining robust against
    exploitation and adapting to the observed behavior of the group. It balances
    the collective good with self-preservation, recognizing the competitive
    nature of a tournament where explicit coordination is not possible.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Reciprocator strategy.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters like
                                                          n_players, n_rounds, m, k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing the history of
                                            actions and payoffs for this player
                                            and opponents. None for round 0.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            cooperators_in_previous_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if cooperators_in_previous_round >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_261(BaseStrategy):
    """
    Adaptive Collective Consensus (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation by consistently
    contributing to the collective good while being ready to punish collective
    failures. It prioritizes the achievement of the collective reward 'k' for
    all players, adapting its behavior based on past outcomes.

    It operates in two main modes:
    1. Cooperative mode: Contributes (C) when the collective goal ('m' cooperators)
       was met in the previous round, or in the very first round.
    2. Punishing mode: Defects (D) for a predefined duration (`P_duration`)
       after the collective goal was NOT met in the previous round. This signals
       disapproval and attempts to enforce compliance.

    In the last round, it either completes a punishment, defects if the collective
    goal failed in the prior round (to prevent further loss when recovery is impossible),
    or cooperates to ensure a final collective reward if cooperation was successful.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_to_defect_for_punishment: int = 0
        self.P_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == 0:
            return Action.C
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        m = self.game_description.m
        r = self.game_description.n_rounds
        if current_round_t == r - 1:
            if self.rounds_to_defect_for_punishment > 0:
                self.rounds_to_defect_for_punishment -= 1
                return Action.D
            elif num_cooperators_prev_round < m:
                return Action.D
            else:
                return Action.C
        if self.rounds_to_defect_for_punishment > 0:
            self.rounds_to_defect_for_punishment -= 1
            return Action.D
        elif num_cooperators_prev_round >= m:
            return Action.C
        else:
            self.rounds_to_defect_for_punishment = self.P_duration - 1
            return Action.D

class Strategy_COLLECTIVE_262(BaseStrategy):
    """
    The "Adaptive Collective Reciprocator" strategy.

    This strategy aims to balance collective good and individual rationality by
    initiating cooperation, sustaining it when the collective threshold (m) is met,
    punishing collective failures, and offering forgiveness to re-establish
    cooperation. It defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and internal state.

        Args:
            game_description: An object containing game parameters such as n_players,
                              n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description
        self._punishment_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing arrays of past actions and payoffs for
                     this player and opponents. None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_1_indexed == 1:
            self._punishment_mode = False
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            prev_round_idx = state.round_number - 1
            prev_round_my_action_was_cooperate = history.my_actions[prev_round_idx]
            prev_round_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
            prev_round_total_cooperators = int(prev_round_my_action_was_cooperate) + prev_round_opponent_cooperators
            if self._punishment_mode:
                self._punishment_mode = False
                return Action.C
            elif prev_round_total_cooperators >= min_cooperators_m:
                return Action.C
            else:
                self._punishment_mode = True
                return Action.D

class Strategy_COLLECTIVE_263(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    It prioritizes collective cooperation to meet the 'm' threshold and unlock the 'k' reward.
    It starts cooperatively, sustains cooperation when successful, signals dissatisfaction
    with defection after its own cooperation fails, and takes responsibility by cooperating
    if its own defection contributed to group failure. It also defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_countdown = 0
        self.PUNISHMENT_DURATION = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_number > 0:
            if self.punishment_countdown > 0:
                self.punishment_countdown -= 1
            elif current_round_number < total_rounds - 1:
                my_action_prev = history.my_actions[-1]
                n_c_prev = np.sum(history.opponent_actions[-1]) + int(my_action_prev)
                if n_c_prev < m_threshold:
                    if my_action_prev == True:
                        self.punishment_countdown = self.PUNISHMENT_DURATION
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        if self.punishment_countdown > 0:
            return Action.D
        else:
            my_action_prev = history.my_actions[-1]
            n_c_prev = np.sum(history.opponent_actions[-1]) + int(my_action_prev)
            if n_c_prev >= m_threshold:
                return Action.C
            else:
                return Action.C

class Strategy_COLLECTIVE_264(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    The ACC strategy prioritizes achieving the collective 'k' bonus by contributing
    to the 'm' cooperator threshold. It adapts its behavior based on the outcome
    of the previous round:
    - In Round 1 (initialization), it always Cooperates to set a positive tone.
    - In intermediate rounds, it observes if the 'm' cooperator threshold was met
      in the previous round. If met, it continues to Cooperate, reinforcing success.
      If not met, it Defects, signaling dissatisfaction and protecting its endowment.
      This ensures it contributes even if it previously defected and benefited from
      others' cooperation, preventing consistent free-riding and distributing the burden.
    - In the final round (end game), it always Defects, a pragmatic choice to
      avoid exploitation due to the disappearance of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        elif state.round_number == n_rounds - 1:
            return Action.D
        else:
            my_cooperation_last_round = int(history.my_actions[-1])
            opponent_cooperation_last_round = np.sum(history.opponent_actions[-1, :])
            total_cooperated_last_round = my_cooperation_last_round + opponent_cooperation_last_round
            if total_cooperated_last_round >= m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_265(BaseStrategy):
    """
    The Adaptive Collective Response (ACR) strategy aims to foster cooperation and achieve the collective reward 'k'
    for all players by initiating cooperation and sustaining it conditionally. It punishes collective failure
    by withdrawing cooperation and protects itself from exploitation, especially in the game's final stages.
    This balances a "collective mindset" with "robustness to a wide range of opponent behaviours" in a tournament.

    Decision Rules:
    1. First Round (t=1, state.round_number == 0): Cooperate (C) to initiate collective effort.
    2. Last Round (t=r, state.round_number == n_rounds - 1): Defect (D) to prevent exploitation.
    3. Intermediate Rounds (1 < t < r):
       - If the minimum cooperation threshold 'm' was met in the immediately preceding round (t-1), then Cooperate (C).
       - If the threshold 'm' was NOT met in the immediately preceding round (t-1), then Defect (D).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        my_action_last_round = history.my_actions[current_round_number - 1]
        opponent_actions_last_round = history.opponent_actions[current_round_number - 1, :]
        num_cooperators_last_round = int(my_action_last_round) + np.sum(opponent_actions_last_round)
        if num_cooperators_last_round >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_266(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the Collective Risk Dilemma.
    It initiates cooperation, offers forgiveness for up to two consecutive
    failures to meet the cooperation threshold, and defects if failures persist
    or in the final round to protect its endowment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            self.failure_counter = 0
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        opponent_cooperators_prev_round = sum(history.opponent_actions[current_round_0_indexed - 1, :])
        my_action_prev_round_bool = history.my_actions[current_round_0_indexed - 1]
        my_action_prev_round_int = 1 if my_action_prev_round_bool else 0
        total_cooperators_prev_round = opponent_cooperators_prev_round + my_action_prev_round_int
        if total_cooperators_prev_round >= min_cooperators_needed:
            self.failure_counter = 0
            return Action.C
        else:
            self.failure_counter += 1
            forgiveness_threshold = 2
            if self.failure_counter <= forgiveness_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_267(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to ensure the group consistently meets the minimum cooperation
    threshold (`m`) to unlock the `k` reward. Once collective success is established,
    it becomes more opportunistic, aiming to maximize its individual payoff by defecting
    when possible, while simultaneously nudging the group towards the most efficient
    collective outcome (exactly `m` cooperators). It incorporates self-preservation
    mechanisms to avoid being indefinitely exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.COOPERATE_INIT_ROUNDS = 3
        self.SUCCESS_WINDOW = 5
        self.FAILURE_THRESHOLD_LOW = 0.4
        self.FAILURE_THRESHOLD_MID = 0.7
        self.EXPLOITATION_TOLERANCE_ROUNDS = 5
        self.OVER_COOPERATION_THRESHOLD_FACTOR = 1.2
        self.history_Nc = []
        self.my_cooperation_streak_in_failing_group = 0
        self.last_was_failing_group_state = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.history_Nc = []
            self.my_cooperation_streak_in_failing_group = 0
            self.last_was_failing_group_state = False
            return Action.C
        Nc_prev_round = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
        self.history_Nc.append(Nc_prev_round)
        if len(self.history_Nc) > self.SUCCESS_WINDOW:
            self.history_Nc = self.history_Nc[-self.SUCCESS_WINDOW:]
        successful_rounds_in_window = 0
        for nc_val in self.history_Nc:
            if nc_val >= self.m:
                successful_rounds_in_window += 1
        recent_success_rate = successful_rounds_in_window / len(self.history_Nc)
        current_failing_state = recent_success_rate < self.FAILURE_THRESHOLD_LOW
        if current_failing_state:
            if self.last_was_failing_group_state:
                self.my_cooperation_streak_in_failing_group += 1
            else:
                self.my_cooperation_streak_in_failing_group = 1
        else:
            self.my_cooperation_streak_in_failing_group = 0
        self.last_was_failing_group_state = current_failing_state
        if current_round_0_indexed < self.COOPERATE_INIT_ROUNDS:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        Nc_last_round = self.history_Nc[-1]
        if recent_success_rate < self.FAILURE_THRESHOLD_LOW:
            if self.my_cooperation_streak_in_failing_group > self.EXPLOITATION_TOLERANCE_ROUNDS:
                return Action.D
            else:
                return Action.C
        elif recent_success_rate < self.FAILURE_THRESHOLD_MID:
            if Nc_last_round < self.m:
                return Action.C
            else:
                return Action.D
        elif Nc_last_round < self.m:
            return Action.C
        elif Nc_last_round > self.m * self.OVER_COOPERATION_THRESHOLD_FACTOR:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_268(BaseStrategy):
    """
    The Adaptive Collective-Conditional (ACC) strategy aims to foster and sustain collective
    cooperation by responding to the group's overall success or failure in meeting the
    cooperation threshold. It balances a commitment to the collective good with a pragmatic
    need to protect itself from persistent exploitation, all while maintaining a path back
    to cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.just_punished: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_failures = 0
            self.just_punished = False
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_last_action_was_C = history.my_actions[state.round_number - 1]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        total_cooperators_last_round = opponent_cooperators_last_round + (1 if my_last_action_was_C else 0)
        if total_cooperators_last_round >= self.game_description.m:
            self.consecutive_failures = 0
            self.just_punished = False
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.just_punished:
                self.just_punished = False
                return Action.C
            elif self.consecutive_failures <= 2:
                return Action.C
            else:
                self.just_punished = True
                return Action.D

class Strategy_COLLECTIVE_269(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy promotes and sustains cooperation
    in the Collective Risk Dilemma. It cooperates initially, monitors for free-riding,
    and initiates a temporary punishment phase (defection) if others consistently
    free-ride while the collective threshold is met. After punishment, it attempts
    to re-establish cooperation. It defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_to_trigger_punishment: int = 3
        self.punishment_duration: int = 2
        self.my_last_action: Action | None = None
        self.cooperation_streak_success: int = 0
        self.punishment_active: bool = False
        self.punishment_rounds_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        n_players = self.game_description.n_players
        if current_round_number == 0:
            self.my_last_action = Action.C
            return Action.C
        if current_round_number == n_rounds - 1:
            self.my_last_action = Action.D
            return Action.D
        num_opponent_cooperators_prev = sum(history.opponent_actions[-1, :])
        C_prev = num_opponent_cooperators_prev
        if self.my_last_action == Action.C:
            C_prev += 1
        total_defectors_prev = n_players - C_prev
        if self.punishment_active:
            self.punishment_rounds_remaining -= 1
            if self.punishment_rounds_remaining > 0:
                self.my_last_action = Action.D
                return Action.D
            else:
                self.punishment_active = False
                self.cooperation_streak_success = 0
                self.my_last_action = Action.C
                return Action.C
        elif C_prev < m_threshold:
            self.my_last_action = Action.C
            self.cooperation_streak_success = 0
            return Action.C
        elif self.my_last_action == Action.D:
            self.cooperation_streak_success = 0
            self.my_last_action = Action.D
            return Action.D
        else:
            self.cooperation_streak_success += 1
            if total_defectors_prev > 0:
                if self.cooperation_streak_success >= self.rounds_to_trigger_punishment:
                    self.punishment_active = True
                    self.punishment_rounds_remaining = self.punishment_duration
                    self.cooperation_streak_success = 0
                    self.my_last_action = Action.D
                    return Action.D
                else:
                    self.my_last_action = Action.C
                    return Action.C
            else:
                self.my_last_action = Action.C
                return Action.C

class Strategy_COLLECTIVE_270(BaseStrategy):
    """
    Collective Steward: A strategy for the Collective Risk Dilemma that initiates cooperation,
    reinforces collective success, and withdraws support upon collective failure,
    defecting in the final round to prevent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            my_cooperation_in_prev_round = int(history.my_actions[-1])
            opponent_cooperations_in_prev_round = np.sum(history.opponent_actions[-1, :])
            C_previous_round = my_cooperation_in_prev_round + opponent_cooperations_in_prev_round
            if C_previous_round >= self.m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_271(BaseStrategy):
    """
    Adaptive Collective Threshold Strategy (ACTS) for the Collective Risk Dilemma.

    This strategy aims to foster and sustain collective cooperation by initiating with a
    cooperative stance and then conditionally cooperating based on the group's ability
    to meet the minimum cooperation threshold (`m`) in the previous round. It leverages
    the repeated nature of the game to establish a feedback mechanism for collective
    success or failure. In the final round, it defects to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == self.n_rounds - 1:
            return Action.D
        last_round_opponent_cooperators = sum(history.opponent_actions[current_round_zero_indexed - 1, :])
        last_round_my_action_was_C = int(history.my_actions[current_round_zero_indexed - 1])
        total_cooperators_last_round = last_round_opponent_cooperators + last_round_my_action_was_C
        if total_cooperators_last_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_272(BaseStrategy):
    """
    The Adaptive Trigger Cooperator (ATC) strategy prioritizes collective success in the Collective Risk Dilemma.
    It initiates cooperation in the first round to establish a positive precedent.
    In subsequent rounds, it maintains cooperation if the collective threshold 'm' was met in the previous round,
    fostering stability and sustained collective gain. If the threshold was missed, it defects as a collective
    punishment to signal dissatisfaction and re-incentivize cooperation.
    In the final round, it defects due to the disappearance of future consequences, aligning with rational end-game play.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        num_cooperated_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperated_last_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_273(BaseStrategy):
    """
    Adaptive Threshold Enforcer with Forgiveness (ATE-F) strategy.

    This strategy aims to establish and maintain a collective state where the
    cooperation threshold `m` is consistently met. It does this by:
    1. Proactively cooperating in the first round to initiate a positive dynamic.
    2. Conditionally maintaining cooperation based on observed collective behavior,
       specifically whether the `m` threshold was met in the previous round.
    3. Offering forgiveness by continuing to cooperate for a specified number of
       consecutive collective failures (where `C_prev < m`).
    4. Robustly responding to persistent collective failure by switching to defection
       to protect individual payoff.
    5. Defecting in the final round due to the absence of future consequences (endgame effect).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.tolerance_level = 1
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_opponent_cooperators_count = sum(history.opponent_actions[-1, :])
        prev_round_my_action_was_C = history.my_actions[-1]
        prev_round_total_cooperators = prev_round_opponent_cooperators_count + prev_round_my_action_was_C
        if prev_round_total_cooperators >= self.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures <= self.tolerance_level:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_274(BaseStrategy):
    """
    Grudging Cooperative Reciprocity (GCR) strategy aims to balance collective good
    with individual rationality. It starts by cooperating, sustains cooperation
    when critical, free-rides when cooperation is redundant, forgives initial
    failures, punishes sustained non-cooperation, and defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the GCR strategy with game parameters and internal state variables.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.consecutive_successes: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the GCR strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for the player and opponents,
                     or None if it's the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        previous_round_idx = state.round_number - 1
        my_previous_action_was_C: bool = history.my_actions[previous_round_idx]
        num_opponent_C = np.sum(history.opponent_actions[previous_round_idx, :])
        last_round_num_C = num_opponent_C + (1 if my_previous_action_was_C else 0)
        if last_round_num_C >= self.game_description.m:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if last_round_num_C >= self.game_description.m:
            if my_previous_action_was_C and last_round_num_C - 1 < self.game_description.m:
                return Action.C
            else:
                return Action.D
        elif self.consecutive_failures <= 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_275(BaseStrategy):
    """
    The "Adaptive Collective Forgiveness" strategy for the Collective Risk Dilemma.
    This strategy starts optimistically by cooperating. It tolerates
    a specified number of consecutive rounds where the collective cooperation
    threshold (m) is not met. If failures persist beyond this tolerance,
    it temporarily defects for a short duration to signal dissatisfaction
    and encourage a return to cooperation, before reverting to cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.min_cooperators = game_description.m
        self.cooperation_willingness = True
        self.rounds_remaining_to_defect = 0
        self.consecutive_failed_rounds = 0
        self.FAILURE_TOLERANCE_ROUNDS = 2
        self.DEFECT_DURATION_ROUNDS = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0:
            prev_round_my_action_val = int(history.my_actions[-1])
            prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            prev_round_N_c = prev_round_my_action_val + prev_round_opponent_cooperators
            if self.rounds_remaining_to_defect > 0:
                self.rounds_remaining_to_defect -= 1
                if self.rounds_remaining_to_defect == 0:
                    self.cooperation_willingness = True
            if prev_round_N_c < self.min_cooperators:
                self.consecutive_failed_rounds += 1
                if self.cooperation_willingness and self.consecutive_failed_rounds >= self.FAILURE_TOLERANCE_ROUNDS:
                    self.cooperation_willingness = False
                    self.rounds_remaining_to_defect = self.DEFECT_DURATION_ROUNDS
            else:
                self.consecutive_failed_rounds = 0
        if self.cooperation_willingness:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_276(BaseStrategy):
    """
    Adaptive Altruism with Corrective Defection (AACD) strategy for the Collective Risk Dilemma.

    This strategy balances collective welfare by prioritizing securing the 'k' bonus
    with the need for self-preservation and deterrence of persistent free-riders.

    It cooperates by default and when the collective threshold 'm' was missed.
    It defects strategically when the 'k' bonus was met with an "excess" of cooperators
    AND there are identified persistent exploiters, aiming to punish and deter.
    In the final round, it makes a pragmatic decision based on the likelihood of collective
    success given the number of persistent exploiters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.forgiveness_threshold: int = 3
        self.exploit_streak_map: dict[int, int] = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            num_opponents = n - 1
            for opponent_idx in range(num_opponents):
                self.exploit_streak_map[opponent_idx] = 0
            return Action.C
        my_action_prev = history.my_actions[-1]
        opponent_actions_prev = history.opponent_actions[-1, :]
        N_C_prev = int(my_action_prev) + opponent_actions_prev.sum()
        num_opponents = n - 1
        for opponent_idx in range(num_opponents):
            opponent_action = opponent_actions_prev[opponent_idx]
            if not opponent_action and N_C_prev >= m:
                self.exploit_streak_map[opponent_idx] += 1
            else:
                self.exploit_streak_map[opponent_idx] = 0
        if state.round_number == r - 1:
            if N_C_prev < m:
                return Action.C
            else:
                N_persistent_exploiters = sum((1 for streak in self.exploit_streak_map.values() if streak >= self.forgiveness_threshold))
                if N_persistent_exploiters > n - m:
                    return Action.D
                else:
                    return Action.C
        if N_C_prev < m:
            return Action.C
        else:
            N_persistent_exploiters = sum((1 for streak in self.exploit_streak_map.values() if streak >= self.forgiveness_threshold))
            if N_C_prev > m and N_persistent_exploiters > 0:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_277(BaseStrategy):
    """
    The Adaptive Threshold Contributor (ATC) strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain cooperation by initiating cooperatively,
    monitoring collective success, encouraging cooperation when the group struggles,
    punishing persistent failure, and strategically defecting when the group
    significantly over-cooperates. It also exhibits "goodwill" cooperation in the
    final round if the group has a recent history of success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.consecutive_successes: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        n_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        my_action_last_round = history.my_actions[-1]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_last_round = opponent_cooperators_last_round + int(my_action_last_round)
        group_succeeded_last_round = total_cooperators_last_round >= m
        if group_succeeded_last_round:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        action_for_this_round: Action
        if state.round_number == n_rounds - 1:
            if group_succeeded_last_round:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        else:
            failure_threshold = max(3, m)
            if self.consecutive_failures >= failure_threshold:
                action_for_this_round = Action.D
            elif group_succeeded_last_round:
                if total_cooperators_last_round == m:
                    action_for_this_round = Action.C
                elif total_cooperators_last_round > m:
                    if total_cooperators_last_round >= m + 2:
                        action_for_this_round = Action.D
                    else:
                        action_for_this_round = Action.C
            else:
                action_for_this_round = Action.C
        return action_for_this_round

class Strategy_COLLECTIVE_278(BaseStrategy):
    """
    Robust Collective Enforcer (RCE) strategy for the Collective Risk Dilemma.

    The RCE strategy prioritizes consistently meeting the cooperation threshold (m) to secure the
    collective reward k for all players. It adapts to past outcomes, pushing for cooperation
    when the collective goal is at risk and maintaining stability when it's achieved.
    Individual defection for private gain is only considered in the very last round, where
    no future interactions for reciprocity exist.

    Core principles:
    1.  **First Round:** Always cooperate to initiate collective action and signal willingness.
    2.  **Last Round:** Always defect due to the lack of future interactions (endgame effect).
    3.  **Intermediate Rounds (Collective Failure):** If the cooperation threshold 'm' was NOT met
        in the previous round, the strategy responds by *always cooperating*. This is a strong
        commitment to rectify collective failure, prioritizing the group's ability to achieve 'k'.
    4.  **Intermediate Rounds (Collective Success):** If the cooperation threshold 'm' WAS met
        in the previous round:
        *   **Fragile Success (exactly 'm' cooperators):** A player continues their previous action.
            If they cooperated, they continue to ensure the threshold isn't broken. If they defected,
            they continue to free-ride without harming the collective.
        *   **Robust Success (more than 'm' cooperators):** A player also continues their previous action.
            If they cooperated, they continue to do so to maintain the buffer and ensure collective
            success, avoiding risky attempts to reduce cooperation. If they defected, they continue
            to free-ride within the existing buffer.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        my_action_prev_round = Action.C if history.my_actions[-1] else Action.D
        if num_cooperators_prev_round < m:
            return Action.C
        elif num_cooperators_prev_round == m:
            if my_action_prev_round == Action.C:
                return Action.C
            else:
                return Action.D
        elif my_action_prev_round == Action.C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_279(BaseStrategy):
    """
    Adaptive Collective Steward (ATS) strategy for the Collective Risk Dilemma.
    This strategy prioritizes achieving and maintaining the collective cooperation threshold (m).
    It starts by cooperating, and continues to cooperate if the threshold was met in the previous round.
    If the threshold is not met, it shows resilience by continuing to cooperate for a configurable number
    of rounds (F_THRESHOLD) before temporarily withdrawing cooperation (defecting). This defection acts
    as a signal of protest, aiming to encourage greater participation from other players.
    In the final round, it defects due to end-game rationality, anticipating similar behavior from others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failures_in_a_row: int = 0
        self.F_THRESHOLD: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.failures_in_a_row = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        c_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if c_prev >= self.game_description.m:
            self.failures_in_a_row = 0
            return Action.C
        else:
            self.failures_in_a_row += 1
            if self.failures_in_a_row >= self.F_THRESHOLD:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_280(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma.
    This strategy aims to ensure the collective threshold 'm' is met by initiating
    cooperation, adapting based on the previous round's outcome, and defecting
    in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_idx - 1
        my_prev_action_is_cooperate = history.my_actions[prev_round_idx]
        opponent_prev_actions_are_cooperate = history.opponent_actions[prev_round_idx, :]
        num_cooperators_prev_round = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions_are_cooperate)
        if num_cooperators_prev_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_281(BaseStrategy):
    """
    Robust Adaptive Threshold Enforcer (RATE) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize collective success by initiating cooperation,
    sustaining it when the threshold is met, punishing collective failures,
    and probing for recovery after prolonged failure, while defecting in the
    final round for individual rationality.
    """

    class _InternalState(Enum):
        COOPERATE = 1
        PUNISH = 2
        PROBE = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._current_state = self._InternalState.COOPERATE
        self._rounds_in_current_state = 0
        self._MAX_PUNISHMENT_DURATION = self.game_description.n_players
        self._PROBE_FREQUENCY = max(2, math.floor(self.game_description.n_players / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self._current_state = self._InternalState.COOPERATE
            self._rounds_in_current_state = 1
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self._rounds_in_current_state += 1
        prev_round_idx = state.round_number - 1
        n_c_prev_opponents = np.sum(history.opponent_actions[prev_round_idx, :])
        my_prev_action_was_c = history.my_actions[prev_round_idx]
        n_c_prev = n_c_prev_opponents + (1 if my_prev_action_was_c else 0)
        if n_c_prev >= self.game_description.m:
            if self._current_state in [self._InternalState.PUNISH, self._InternalState.PROBE]:
                self._current_state = self._InternalState.COOPERATE
                self._rounds_in_current_state = 1
        elif self._current_state == self._InternalState.COOPERATE:
            self._current_state = self._InternalState.PUNISH
            self._rounds_in_current_state = 1
        elif self._current_state == self._InternalState.PUNISH:
            if self._rounds_in_current_state > self._MAX_PUNISHMENT_DURATION:
                self._current_state = self._InternalState.PROBE
                self._rounds_in_current_state = 1
        elif self._current_state == self._InternalState.PROBE:
            pass
        if self._current_state == self._InternalState.COOPERATE:
            return Action.C
        elif self._current_state == self._InternalState.PUNISH:
            return Action.D
        elif self._current_state == self._InternalState.PROBE:
            if self._rounds_in_current_state == 1 or self._rounds_in_current_state % self._PROBE_FREQUENCY == 0:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_282(BaseStrategy):
    """
    The Collective Trust Builder (CTB) strategy for the Collective Risk Dilemma.
    It aims to foster and maintain cooperation by responding to the collective
    outcome of the previous round. It initiates cooperation in Round 1, defects
    in the final round, and in intermediate rounds, it cooperates if the
    collective threshold 'm' was met in the previous round, otherwise it defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == n_rounds:
            return Action.D
        else:
            my_prev_action_is_cooperate = history.my_actions[-1]
            opponent_prev_cooperators_count = sum(history.opponent_actions[-1, :])
            N_c_prev = int(my_prev_action_is_cooperate) + opponent_prev_cooperators_count
            if N_c_prev >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_283(BaseStrategy):
    """
    The Robust Collective Enforcer (RCE) strategy aims to foster and sustain cooperation
    by prioritizing the collective success of meeting the 'm' cooperator threshold.
    It starts with cooperation, enforces collective accountability by withdrawing support
    if the threshold is not met, and is forgiving, allowing cooperation to resume
    once the collective goal is achieved again. It defects in the final round,
    acknowledging the finite nature of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_previous_action_bool = history.my_actions[current_round_0_indexed - 1]
        my_previous_action_int = int(my_previous_action_bool)
        previous_opponent_cooperators_count = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        previous_total_cooperators_count = my_previous_action_int + previous_opponent_cooperators_count
        if previous_total_cooperators_count < self.m:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_284(BaseStrategy):
    """
    The Collective Prober strategy aims to achieve collective good by proactively cooperating,
    adapting to collective outcomes, and employing defensive measures against exploitation,
    while periodically attempting to re-establish cooperation after failures.
    It defects in the final round due to end-game rationality.
    """
    _COOPERATE_MAINTAIN = 'COOPERATE_MAINTAIN'
    _DEFENSIVE_PUNISH = 'DEFENSIVE_PUNISH'
    _PROBE_FORGIVE = 'PROBE_FORGIVE'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_rounds_failed_threshold: int = 0
        self.current_play_mode: str = self._COOPERATE_MAINTAIN
        self.reprobe_interval: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t == total_rounds_r:
            return Action.D
        if current_round_t == 1:
            self.consecutive_rounds_failed_threshold = 0
            self.current_play_mode = self._COOPERATE_MAINTAIN
            return Action.C
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1])
        my_action_prev_round_is_C = history.my_actions[state.round_number - 1]
        N_C_prev = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if N_C_prev >= min_cooperators_m:
            self.consecutive_rounds_failed_threshold = 0
            self.current_play_mode = self._COOPERATE_MAINTAIN
            return Action.C
        else:
            self.consecutive_rounds_failed_threshold += 1
            if self.consecutive_rounds_failed_threshold >= self.reprobe_interval:
                self.consecutive_rounds_failed_threshold = 0
                self.current_play_mode = self._PROBE_FORGIVE
                return Action.C
            else:
                self.current_play_mode = self._DEFENSIVE_PUNISH
                return Action.D

class Strategy_COLLECTIVE_285(BaseStrategy):
    """
    Adaptive Collective Assurance (ACA) strategy for the Collective Risk Dilemma.
    This strategy aims to foster and sustain cooperation by initially cooperating,
    maintaining cooperation when the group successfully meets the threshold,
    punishing when the group fails despite its own cooperation, and offering
    a chance for recovery by re-cooperating after a punishment phase or when
    already defecting and the group still fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishing = False
        self.punishment_rounds_to_go = 0
        self.P_value = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action_for_this_round = None
        if state.round_number == 0:
            current_action_for_this_round = Action.C
        else:
            my_action_last_round_was_cooperate = history.my_actions[-1]
            num_cooperators_in_t_minus_1 = sum(history.opponent_actions[-1, :])
            if my_action_last_round_was_cooperate:
                num_cooperators_in_t_minus_1 += 1
            threshold_met_last_round = num_cooperators_in_t_minus_1 >= self.game_description.m
            if self.punishing:
                self.punishment_rounds_to_go -= 1
                if self.punishment_rounds_to_go == 0:
                    self.punishing = False
            if self.punishing:
                current_action_for_this_round = Action.D
            elif threshold_met_last_round:
                current_action_for_this_round = Action.C
            elif my_action_last_round_was_cooperate:
                self.punishing = True
                self.punishment_rounds_to_go = self.P_value
                current_action_for_this_round = Action.D
            else:
                current_action_for_this_round = Action.C
        return current_action_for_this_round

class Strategy_COLLECTIVE_286(BaseStrategy):
    """
    The "Adaptive Group Cooperator" (AGC) strategy aims to promote and sustain
    collective cooperation in the Collective Risk Dilemma. It prioritizes
    establishing the collective good (k) and adapting its behavior based on
    the group's success in meeting the cooperation threshold (m). The strategy
    emphasizes a collective mindset, even in the face of individual free-riding
    incentives.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_action_last_round = history.my_actions[-1]
        num_cooperators_last_round = my_action_last_round + sum(history.opponent_actions[-1, :])
        if num_cooperators_last_round >= self.game_description.m:
            return Action.C
        elif my_action_last_round == True:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_287(BaseStrategy):
    """
    The Adaptive Collective Sustainer (ACS) strategy for the Collective Risk Dilemma.
    It aims to achieve and maintain the collective reward 'k' by proactively cooperating,
    dynamically responding to the group's collective action, and strategically adapting
    to maintain robustness against exploitation. It balances the collective good with
    the need for individual self-preservation in a competitive environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_status: str = 'COOPERATING'
        self.rounds_remaining_in_punishment: int = 0
        self.PUNISHMENT_DURATION: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_game_round == 1:
            return Action.C
        if current_game_round == total_rounds:
            return Action.D
        prev_round_index_history = state.round_number - 1
        cooperators_in_prev_round = 0
        if history.my_actions[prev_round_index_history] == Action.C:
            cooperators_in_prev_round += 1
        cooperators_in_prev_round += np.sum(history.opponent_actions[prev_round_index_history, :])
        was_threshold_met_prev = cooperators_in_prev_round >= self.game_description.m
        if self.current_status == 'PUNISHING':
            self.rounds_remaining_in_punishment -= 1
            if self.rounds_remaining_in_punishment == 0:
                self.current_status = 'RECOVERING'
            return Action.D
        elif self.current_status == 'RECOVERING':
            if was_threshold_met_prev:
                self.current_status = 'COOPERATING'
                return Action.C
            else:
                self.current_status = 'PUNISHING'
                self.rounds_remaining_in_punishment = self.PUNISHMENT_DURATION
                return Action.D
        elif self.current_status == 'COOPERATING':
            if was_threshold_met_prev:
                return Action.C
            else:
                self.current_status = 'PUNISHING'
                self.rounds_remaining_in_punishment = self.PUNISHMENT_DURATION
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_288(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Stability Check strategy for the Collective Risk Dilemma.
    This strategy aims to foster and sustain collective cooperation while protecting against exploitation.
    It balances individual rationality with the collective goal of achieving the reward 'k'.
    The strategy starts cooperatively, punishes insufficient cooperation, rewards consistent cooperation,
    and makes a calculated sacrifice for collective stability when cooperation is fragile.
    It also acknowledges the end-game rationality of defection in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing records of past actions
                                            and payoffs for this player and opponents.
                                            It is None for the first round (state.round_number == 0).

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        else:
            my_action_prev = history.my_actions[-1]
            opponent_actions_prev = history.opponent_actions[-1, :]
            c_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
            if c_prev < m_threshold:
                return Action.D
            elif my_action_prev == Action.C.value:
                return Action.C
            elif c_prev == m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_289(BaseStrategy):
    """
    Adaptive Collective Reciprocity with End-Game Defection strategy for the Collective Risk Dilemma.
    This strategy aims to foster cooperation by adapting its behavior based on the observed
    number of cooperators in the previous round. It punishes collective failures,
    offers forgiveness to re-establish cooperation, and pragmatically defects in the final round.
    """

    class PlayerState(Enum):
        COOPERATE_MODE = 1
        PUNISH_MODE = 2
        FORGIVE_MODE = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and sets up internal state variables.
        """
        self.game_description = game_description
        self.PUNISHMENT_DURATION = 2
        self.FORGIVENESS_DURATION = 1
        self.current_player_state: self.PlayerState = self.PlayerState.COOPERATE_MODE
        self.rounds_in_current_state: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on the
        strategy's internal state and the history of previous rounds.
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            self.current_player_state = self.PlayerState.COOPERATE_MODE
            self.rounds_in_current_state = 1
            return Action.C
        prev_round_idx = current_round_0_indexed - 1
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_round_is_C = history.my_actions[prev_round_idx]
        C_prev = int(opponent_cooperators_prev_round) + (1 if my_action_prev_round_is_C else 0)
        action_to_play = Action.D
        if self.current_player_state == self.PlayerState.COOPERATE_MODE:
            if C_prev >= m_threshold:
                action_to_play = Action.C
                self.rounds_in_current_state += 1
            else:
                action_to_play = Action.D
                self.current_player_state = self.PlayerState.PUNISH_MODE
                self.rounds_in_current_state = 1
        elif self.current_player_state == self.PlayerState.PUNISH_MODE:
            action_to_play = Action.D
            self.rounds_in_current_state += 1
            if self.rounds_in_current_state > self.PUNISHMENT_DURATION:
                self.current_player_state = self.PlayerState.FORGIVE_MODE
                self.rounds_in_current_state = 1
        elif self.current_player_state == self.PlayerState.FORGIVE_MODE:
            action_to_play = Action.C
            self.rounds_in_current_state += 1
            if self.rounds_in_current_state > self.FORGIVENESS_DURATION:
                self.current_player_state = self.PlayerState.COOPERATE_MODE
                self.rounds_in_current_state = 1
        return action_to_play

class Strategy_COLLECTIVE_290(BaseStrategy):
    """
    Adaptive Collective Equilibrium (ACE) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation to achieve the
    public good reward `k`, while remaining adaptive and robust against various
    opponent behaviors, including exploitation.

    Core Principles:
    - Initiate Cooperation: Begin by contributing to signal willingness for collective success.
    - Maintain Collective Good: If the community successfully meets the cooperation
      threshold, continue to contribute to sustain the benefit.
    - Adaptive Response to Failure: If the threshold is not met:
        - If you contributed to a failed effort, withdraw your contribution (defect)
          to signal dissatisfaction and avoid repeated exploitation.
        - If you did *not* contribute to a failed effort, try to re-initiate
          cooperation (contribute) to break cycles of defection and promote recovery.
    - Rational Terminal Play: Acknowledge the finite nature of the game by defecting
      in the final round, as is strategically optimal in repeated games with a known end.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_action_last_round: np.bool_ = history.my_actions[-1]
        opponent_actions_last_round: NDArray[np.bool_] = history.opponent_actions[-1, :]
        num_cooperators_last_round = int(my_action_last_round) + np.sum(opponent_actions_last_round)
        if num_cooperators_last_round >= m_threshold:
            return Action.C
        elif my_action_last_round:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_291(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation by:
    1. Cooperating in the first round to probe the environment and signal willingness.
    2. In adaptive rounds (2 to r-1), conditionally cooperating:
       - If the collective threshold 'm' was met in the previous round, it cooperates to sustain success.
       - If the threshold was not met, it defects to signal dissatisfaction and protect its endowment.
    3. Defecting in the final round ('r') due to the end-game effect, maximizing individual payoff when there's no future interaction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_cooperators_in_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_in_previous_round >= min_cooperators_m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_292(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy prioritizes achieving the collective benefit (securing the 'k' reward)
    by adapting its behavior based on the observed collective outcome of previous rounds.
    It starts cooperatively, sustains cooperation when the collective goal ('m' cooperators)
    is met, and signals dissatisfaction (by defecting) when the collective goal is missed.
    In the final round, it defects to maximize immediate personal payoff, adhering to
    backward induction principles for finite repeated games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_opponents = np.sum(history.opponent_actions[-1, :])
        num_cooperators_last_round = int(history.my_actions[-1]) + num_cooperators_opponents
        if num_cooperators_last_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_293(BaseStrategy):
    """
    Stable Threshold Cooperator (STC) strategy for the Collective Risk Dilemma.
    This strategy prioritizes achieving and maintaining the collective reward (k)
    by being conditionally cooperative. It starts by cooperating, attempts to
    re-establish cooperation if the threshold is missed, and maintains its
    previous action (cooperate or defect) if the threshold is met to promote stability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_last_action_sent: Action | None = None
        self.consecutive_failed_cooperations: int = 0
        self.patience_threshold: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        action_for_current_round: Action
        if current_round_1_indexed == self.r:
            action_for_current_round = Action.D
            self.my_last_action_sent = action_for_current_round
            return action_for_current_round
        if current_round_1_indexed == 1:
            action_for_current_round = Action.C
            self.my_last_action_sent = action_for_current_round
            return action_for_current_round
        my_action_in_prev_round = self.my_last_action_sent
        opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_cooperation_in_prev_round_count = int(my_action_in_prev_round.value)
        C_prev = opponent_cooperators_prev + my_cooperation_in_prev_round_count
        if my_action_in_prev_round == Action.C:
            if C_prev < self.m:
                self.consecutive_failed_cooperations += 1
            else:
                self.consecutive_failed_cooperations = 0
        else:
            self.consecutive_failed_cooperations = 0
        if C_prev < self.m:
            if self.consecutive_failed_cooperations < self.patience_threshold:
                action_for_current_round = Action.C
            else:
                action_for_current_round = Action.D
        else:
            action_for_current_round = my_action_in_prev_round
        self.my_last_action_sent = action_for_current_round
        return action_for_current_round

class Strategy_COLLECTIVE_294(BaseStrategy):
    """
    Collective Pulse with Adaptive Reset strategy for the Collective Risk Dilemma.
    This strategy aims to maximize collective welfare by ensuring the 'k' reward is consistently met,
    while distributing the cost of cooperation fairly among players. It reacts to both success
    and failure in meeting the cooperation threshold 'm'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.player_id: int = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        i = self.player_id
        n = self.n_players
        m = self.m
        r = self.n_rounds
        if t == 1:
            return Action.C
        if t == r:
            return Action.D
        n_c_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if n_c_prev < m:
            return Action.C
        else:
            duty_index = (t - 1 + (i - 1)) % n
            if duty_index < m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_295(BaseStrategy):
    """
    Adaptive Threshold Enforcement Strategy for Collective Risk Dilemma.

    This strategy aims to consistently ensure that the number of cooperators (`N_C`)
    meets or exceeds the minimum threshold `m` to unlock the collective reward `k` for all players.
    It employs a phased approach: initial cooperation, adaptive monitoring and response,
    and a final-round adjustment.

    It operates by maintaining a "stance" (Cooperate, Punish, or Recover) that shifts
    based on the observed collective behavior in previous rounds.
    """

    class StrategyMode(Enum):
        COOPERATE_MODE = 'cooperate'
        PUNISH_MODE = 'punish'
        RECOVER_MODE = 'recover'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.current_mode = self.StrategyMode.COOPERATE_MODE
        self.consecutive_failures = 0
        self.rounds_since_punishment_started = 0
        self.OPTIMISTIC_COOPERATION_ROUNDS = min(self.r, 2)
        self.PUNISHMENT_THRESHOLD = 2
        self.RECOVERY_THRESHOLD = 2
        self.END_GAME_ROUNDS = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed >= self.r - self.END_GAME_ROUNDS + 1:
            return Action.D
        if current_round_1_indexed <= self.OPTIMISTIC_COOPERATION_ROUNDS:
            return Action.C
        n_c_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        threshold_met_previous_round = n_c_previous_round >= self.m
        if self.current_mode == self.StrategyMode.COOPERATE_MODE:
            if not threshold_met_previous_round:
                self.consecutive_failures += 1
                if self.consecutive_failures >= self.PUNISHMENT_THRESHOLD:
                    self.current_mode = self.StrategyMode.PUNISH_MODE
                    self.rounds_since_punishment_started = 0
            else:
                self.consecutive_failures = 0
        elif self.current_mode == self.StrategyMode.PUNISH_MODE:
            self.rounds_since_punishment_started += 1
            if self.rounds_since_punishment_started >= self.RECOVERY_THRESHOLD:
                self.current_mode = self.StrategyMode.RECOVER_MODE
                self.consecutive_failures = 0
        elif self.current_mode == self.StrategyMode.RECOVER_MODE:
            if threshold_met_previous_round:
                self.current_mode = self.StrategyMode.COOPERATE_MODE
                self.consecutive_failures = 0
            else:
                self.current_mode = self.StrategyMode.PUNISH_MODE
                self.rounds_since_punishment_started = 0
        if self.current_mode == self.StrategyMode.COOPERATE_MODE or self.current_mode == self.StrategyMode.RECOVER_MODE:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_296(BaseStrategy):
    """
    Threshold-Adaptive Reciprocity (TAR) strategy for the Collective Risk Dilemma.

    This strategy aims to foster collective cooperation by:
    1.  Starting with cooperation (Initial Trust).
    2.  Maintaining cooperation when the collective threshold 'm' is met in the
        previous round (Collective Success Reciprocity).
    3.  Offering a single "forgiveness" round by cooperating after a first
        collective failure (Adaptive Forgiveness).
    4.  Punishing persistent collective failure (two or more consecutive rounds
        below 'm') by defecting (Strategic Punishment).
    5.  Defecting in the final round to maximize individual payoff, as no future
        interactions exist (Rational End-Game).

    The strategy balances encouraging group benefit with protecting against exploitation
    in the face of persistent collective inaction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            self._consecutive_failures = 0
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        num_cooperators_prev_round = sum(history.opponent_actions[prev_round_idx, :]) + history.my_actions[prev_round_idx]
        if num_cooperators_prev_round >= min_cooperators_needed:
            self._consecutive_failures = 0
            return Action.C
        else:
            self._consecutive_failures += 1
            if self._consecutive_failures == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_297(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.
    This strategy adapts its behavior based on the recent history of collective
    success or failure in meeting the cooperation threshold. It aims to foster
    cooperation, tolerate minor setbacks, and strategically defect to prevent
    exploitation or optimize collective efficiency when robust cooperation is observed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_successes: int = 0
        self.consecutive_failures: int = 0
        self.MAX_FAILURES_TOLERATED: int = 2
        self.MIN_SUCCESSES_TO_TEST_DEFECTION: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_last_round >= self.game_description.m:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        if self.consecutive_failures > self.MAX_FAILURES_TOLERATED:
            return Action.D
        elif self.consecutive_successes >= self.MIN_SUCCESSES_TO_TEST_DEFECTION:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_298(BaseStrategy):
    """
    The Adaptive Forgiving Trigger (AFT) strategy balances collective success with
    individual robustness in the Collective Risk Dilemma. It prioritizes cooperation
    to achieve the collective reward 'k', but adaptively switches to defection
    if the collective repeatedly fails to meet the 'm' cooperators threshold.
    It includes a 'forgiveness' mechanism to tolerate a certain number of
    consecutive failures before defecting, allowing for recovery from minor
    setbacks. The strategy also incorporates backward induction for the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.forgiveness_rounds = 1
        self.consecutive_failures_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_failures_count = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_cooperators_count = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_round_cooperators_count >= self.game_description.m:
            self.consecutive_failures_count = 0
            return Action.C
        else:
            self.consecutive_failures_count += 1
            if self.consecutive_failures_count <= self.forgiveness_rounds:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_299(BaseStrategy):
    """
    The Adaptive Restoration Collective (ARC) strategy aims to achieve and maintain the collective benefit
    by initiating cooperation, punishing immediate failures to meet the cooperation threshold, and critically,
    attempting to restore cooperation after prolonged periods of collective failure. This approach prevents a
    total collapse into defection while still deterring persistent free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed > 0:
            num_cooperators_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            if num_cooperators_prev_round >= min_cooperators_needed:
                self.consecutive_failures = 0
            else:
                self.consecutive_failures += 1
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        elif current_round_0_indexed == 0:
            return Action.C
        elif self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures == 1:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_300(BaseStrategy):
    """
    Adaptive Collective Enforcement with Forgiveness strategy for the Collective Risk Dilemma.
    This strategy aims to secure the collective bonus 'k' for all players by promoting and sustaining
    cooperation. It responds to collective success (meeting the cooperation threshold) with
    continued cooperation. In cases of collective failure (threshold not met), it initiates a
    brief, temporary period of collective punishment (defection) to signal the importance of
    meeting the threshold and to protect individual payoffs. The punishment is designed to be
    temporary and forgiving, allowing for re-engagement and recovery of cooperation, maximizing
    long-term collective welfare. It also incorporates a pragmatic defection in the final round
    of the game, recognizing the finite nature of interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == total_rounds:
            return Action.D
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            return Action.D
        if state.round_number == 0:
            return Action.C
        prev_round_my_action = history.my_actions[-1]
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_round_total_cooperators = prev_round_opponent_cooperators + (1 if prev_round_my_action else 0)
        if prev_round_total_cooperators >= m_threshold:
            return Action.C
        else:
            remaining_rounds_including_current = total_rounds - state.round_number
            punishment_rounds_total = min(2, remaining_rounds_including_current)
            if punishment_rounds_total > 0:
                self.punishment_countdown = punishment_rounds_total - 1
                return Action.D
            else:
                return Action.D

class Strategy_COLLECTIVE_301(BaseStrategy):
    """
    Adaptive Collective Trust (ACT) strategy for the Collective Risk Dilemma.
    This strategy starts by cooperating, then adaptively responds to whether
    the collective cooperation threshold 'm' was met in the previous round.
    It enacts temporary punitive defection phases after failures, followed by
    re-attempts at cooperation (forgiveness), and defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACT strategy with game parameters and internal state.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.defection_streak: int = 0
        self.forgiveness_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the ACT strategy rules.

        Args:
            state: Current game state, including the current round number (0-indexed).
            history: Historical data of this player's and opponents' actions/payoffs
                     from previous rounds. None for the first round.

        Returns:
            An Action enum: Action.C for Cooperate, Action.D for Defect.
        """
        if state.round_number == 0:
            self.defection_streak = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        my_prev_action_cooperated = history.my_actions[previous_round_index]
        opponent_prev_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
        C_prev = int(my_prev_action_cooperated) + opponent_prev_cooperators
        if C_prev >= self.game_description.m:
            self.defection_streak = 0
            return Action.C
        else:
            self.defection_streak += 1
            if self.defection_streak <= self.forgiveness_threshold:
                return Action.D
            else:
                self.defection_streak = 0
                return Action.C

class Strategy_COLLECTIVE_302(BaseStrategy):
    """
    Adaptive Collective Aspiration (ACA) strategy for the Collective Risk Dilemma.

    This strategy balances collective goals with individual rationality. It starts with a
    strong cooperative stance (warm-up phase), then adapts its behavior based on
    the collective's success or failure in meeting the cooperation threshold (adaptive phase),
    and concludes with a collective-oriented or individually rational decision in the
    final round (endgame phase).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n_rounds = self.game_description.n_rounds
        self.WARMUP_ROUNDS_THRESHOLD = max(1, min(3, n_rounds // 4)) if n_rounds > 1 else 1
        self.FAILURE_TOLERANCE_THRESHOLD = 2
        self._failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_0_indexed == 0:
            self._failure_streak = 0
            return Action.C
        prev_round_cooperators_count = sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if history.my_actions[current_round_0_indexed - 1]:
            prev_round_cooperators_count += 1
        if current_round_0_indexed == n_rounds - 1:
            if prev_round_cooperators_count >= m:
                return Action.C
            else:
                return Action.D
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed <= self.WARMUP_ROUNDS_THRESHOLD:
            return Action.C
        if prev_round_cooperators_count >= m:
            self._failure_streak = 0
            return Action.C
        else:
            self._failure_streak += 1
            if self._failure_streak <= self.FAILURE_TOLERANCE_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_303(BaseStrategy):
    """
    Adaptive Collective Threshold Maintainer (ACTM) Strategy.

    This strategy prioritizes achieving and maintaining the collective reward 'k' by
    consistently cooperating in most rounds. It adapts to the group's previous performance
    to decide on cooperation or defection, with a special consideration for the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        my_action_prev_round = history.my_actions[current_round_0_indexed - 1]
        opponent_actions_prev_round = history.opponent_actions[current_round_0_indexed - 1, :]
        num_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
        if current_round_0_indexed == total_rounds - 1:
            if num_cooperators_prev_round >= min_cooperators_m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_304(BaseStrategy):
    """
    Adaptive Forgiveness and Collective Enforcement (AFCE) strategy for the Collective Risk Dilemma.

    This strategy balances cooperation and self-preservation. It starts by cooperating (initial trust),
    continues cooperating as long as the collective threshold `m` is met (reinforcement).
    It forgives a single instance of collective failure (threshold not met) by continuing to cooperate.
    However, if collective failures persist for two or more consecutive rounds, the strategy defects
    to protect its endowment and signal dissatisfaction. In the final round, it defects due to
    end-game rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        n_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if n_cooperators_prev_round >= min_cooperators_needed:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures <= 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_305(BaseStrategy):
    """
    The Adaptive Collective Support (ACS) strategy for the Collective Risk Dilemma.

    ACS aims to balance optimistic initiation of cooperation, positive reinforcement
    for collective success, adaptive responses to partial failures, and self-protective
    measures against severe exploitation.

    Key behaviors:
    - Starts with cooperation to signal willingness.
    - Defects in the final round to maximize individual payoff (end-game effect).
    - If the collective threshold was met in the previous round, continues to cooperate.
    - If no one cooperated in the previous round, defects to avoid futile self-sacrifice.
    - If some but not enough cooperated:
        - If this player cooperated last, defects to protect against exploitation.
        - If this player defected last, cooperates to re-attempt collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        my_last_action_is_C = history.my_actions[-1]
        num_cooperators_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = num_cooperators_opponents_prev_round + (1 if my_last_action_is_C else 0)
        if total_cooperators_prev_round >= self.m_threshold:
            return Action.C
        elif total_cooperators_prev_round == 0:
            return Action.D
        elif my_last_action_is_C:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_306(BaseStrategy):
    """
    The Adaptive Collective Balancer (ACB) strategy for the Collective Risk Dilemma.
    This strategy aims to achieve and sustain the collective benefit (reward 'k') 
    by cooperating, while simultaneously protecting itself from exploitation if 
    other players fail to contribute. It is designed to be adaptive and robust, 
    learning from the group's past performance without assuming prior coordination.

    Core Principle: Start cooperatively, sustain cooperation when the collective 
    goal is met, punish sustained failure, and provide opportunities for 
    re-establishing cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PATIENCE_THRESHOLD = 2
        self.consecutive_failures_observed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        n_rounds_total = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_number_1_indexed == n_rounds_total:
            return Action.D
        if current_round_number_1_indexed == 1:
            self.consecutive_failures_observed = 0
            return Action.C
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_in_prev_round_is_C = history.my_actions[-1]
        prev_round_num_cooperators = num_opponent_cooperators_prev_round + (1 if my_action_in_prev_round_is_C else 0)
        my_action_in_prev_round_enum = Action.C if my_action_in_prev_round_is_C else Action.D
        if prev_round_num_cooperators >= min_cooperators_needed:
            self.consecutive_failures_observed = 0
            return Action.C
        else:
            self.consecutive_failures_observed += 1
            if my_action_in_prev_round_enum == Action.C:
                return Action.D
            elif self.consecutive_failures_observed >= self.PATIENCE_THRESHOLD:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_307(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) strategy for the Collective Risk Dilemma.
    ACE aims to establish and maintain a high level of cooperation among players
    to consistently meet the 'm' cooperators threshold and maximize the 'k' reward.
    It starts cooperatively, rewards sustained success, and punishes sustained
    failure, while exhibiting forgiveness to encourage re-engagement.
    """
    _PUNISHMENT_THRESHOLD: int = 2
    _PUNISHMENT_DURATION: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_successes: int = 0
        self.consecutive_failures: int = 0
        self.punishment_active: bool = False
        self.punishment_timer: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            previous_round_idx = current_round_0_indexed - 1
            n_c_prev = int(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx, :])
            if n_c_prev >= m_threshold:
                self.consecutive_successes += 1
                self.consecutive_failures = 0
            else:
                self.consecutive_failures += 1
                self.consecutive_successes = 0
            if self.punishment_active:
                self.punishment_timer -= 1
                if self.punishment_timer <= 0:
                    self.punishment_active = False
                    self.consecutive_failures = 0
            elif self.consecutive_failures >= self._PUNISHMENT_THRESHOLD:
                self.punishment_active = True
                self.punishment_timer = self._PUNISHMENT_DURATION
            if self.punishment_active:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_308(BaseStrategy):
    """
    The Adaptive Group Enforcer (AGE) strategy for the Collective Risk Dilemma.
    This strategy initiates cooperation in the first round to foster trust and collective action.
    In subsequent rounds, it adapts its behavior based on whether the group successfully
    met the cooperation threshold ('m') in the previous round. If the threshold was met,
    it continues to cooperate; otherwise, it defects as a signal and for self-preservation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            my_action_prev = history.my_actions[-1]
            opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            num_cooperators_prev_round = my_action_prev + opponent_cooperators_prev
            if num_cooperators_prev_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_309(BaseStrategy):
    """
    Adaptive Collective Re-engagement (ACR) strategy for the Collective Risk Dilemma.

    This strategy defaults to cooperation to achieve the collective reward `k`. It dynamically
    adapts to observed collective outcomes. If the community fails to meet the cooperation
    threshold (`m`), the player temporarily withdraws its contribution (defects for one round)
    to signal dissatisfaction and protect its endowment, then attempts to re-engage cooperation.
    Special handling for the final round exists to either secure a final collective reward
    or protect against late-game defection.
    """

    class PlayerState(Enum):
        COOPERATING = 'COOPERATING'
        PUNISHING = 'PUNISHING'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_state = self.PlayerState.COOPERATING

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        action_for_current_round = Action.C
        prev_C_total = -1
        if state.round_number > 0:
            prev_round_history_index = state.round_number - 1
            prev_C_total = np.sum(history.opponent_actions[prev_round_history_index, :]) + int(history.my_actions[prev_round_history_index])
            if self.my_state == self.PlayerState.COOPERATING:
                if prev_C_total < m_threshold:
                    self.my_state = self.PlayerState.PUNISHING
            elif self.my_state == self.PlayerState.PUNISHING:
                self.my_state = self.PlayerState.COOPERATING
        if current_round_1_indexed == n_rounds:
            if prev_C_total >= m_threshold:
                action_for_current_round = Action.C
            else:
                action_for_current_round = Action.D
        elif self.my_state == self.PlayerState.COOPERATING:
            action_for_current_round = Action.C
        elif self.my_state == self.PlayerState.PUNISHING:
            action_for_current_round = Action.D
        return action_for_current_round

class Strategy_COLLECTIVE_310(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    ACR aims to foster and sustain collective cooperation to achieve the 'k' reward for all players,
    while also being robust against exploitation. It cooperates by default, acts as a collective
    enforcer by punishing widespread failure, and employs measured reciprocity and self-preservation
    to prevent exploitation when the collective is stable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.p_explore_defection = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        N_C_prev = num_opponent_cooperators_last_round + int(my_prev_action_was_C)
        if N_C_prev < self.m:
            return Action.D
        elif my_prev_action_was_C:
            if N_C_prev == self.m:
                return Action.C
            elif random.random() < self.p_explore_defection and N_C_prev - 1 >= self.m:
                return Action.D
            else:
                return Action.C
        elif N_C_prev == self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_311(BaseStrategy):
    """
    The Adaptive Threshold Enforcer (ATE) strategy aims to maximize collective payoff
    by striving to meet the cooperation threshold 'm'. It initiates cooperation,
    sustains it when the threshold is met, and enforces cooperation by punishing
    (defecting) when the threshold is consistently missed. It defects in the final
    round to prevent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k
        self.punishment_mode: bool = False
        self.punishment_rounds_left: int = 0
        self.failed_rounds_count: int = 0
        self.MAX_FAILED_ROUNDS_BEFORE_PUNISHMENT: int = 1
        self.PUNISHMENT_DURATION_ROUNDS: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        prev_round_index = current_round - 1
        my_prev_action_was_cooperate = history.my_actions[prev_round_index]
        opponent_prev_actions = history.opponent_actions[prev_round_index, :]
        prev_round_cooperators = int(my_prev_action_was_cooperate) + np.sum(opponent_prev_actions)
        if prev_round_cooperators < self.m_threshold:
            self.failed_rounds_count += 1
        else:
            self.failed_rounds_count = 0
        if self.punishment_mode:
            action = Action.D
            self.punishment_rounds_left -= 1
            if self.punishment_rounds_left == 0:
                self.punishment_mode = False
        elif self.failed_rounds_count >= self.MAX_FAILED_ROUNDS_BEFORE_PUNISHMENT:
            self.punishment_mode = True
            self.punishment_rounds_left = self.PUNISHMENT_DURATION_ROUNDS
            action = Action.D
        else:
            action = Action.C
        return action

class Strategy_COLLECTIVE_312(BaseStrategy):
    """
    The Adaptive Collective Re-Cooperator (ACR) strategy for the Collective Risk Dilemma.
    It initiates with cooperation, tolerates a single round of collective failure
    (where 'm' is not met), and responds to persistent failure by defecting.
    It defects in the final round due to end-game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_count: int = 0
        self.F_tolerance: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_failure_count = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[-1]
        opponent_prev_actions_were_cooperate = history.opponent_actions[-1, :]
        num_cooperators_prev_round = int(my_prev_action_was_cooperate) + np.sum(opponent_prev_actions_were_cooperate)
        if num_cooperators_prev_round >= self.game_description.m:
            self.consecutive_failure_count = 0
        else:
            self.consecutive_failure_count += 1
        if self.consecutive_failure_count <= self.F_tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_313(BaseStrategy):
    """
    The Forgiving Collective Initiator (FCI) strategy is designed to promote and sustain
    cooperation in the Collective Risk Dilemma. It initiates cooperation, patiently attempts
    to re-establish it after minor setbacks (forgiving a single collective failure), but
    firmly withdraws support if collective effort consistently fails (more than one
    consecutive failure). It also strategically defects in the final round of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Forgiving Collective Initiator strategy.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, m, k.
        """
        self.game_description = game_description
        self.failed_rounds_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the FCI strategy logic.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for all players,
                     or None if it's the very first round (round_number == 0).

        Returns:
            Action: Either Action.C (Cooperate) or Action.D (Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        num_opponent_C_prev_round = np.sum(history.opponent_actions[-1, :])
        num_C_prev_round = int(my_prev_action_was_C) + num_opponent_C_prev_round
        if num_C_prev_round >= self.game_description.m:
            self.failed_rounds_count = 0
        else:
            self.failed_rounds_count += 1
        if self.failed_rounds_count == 0:
            return Action.C
        elif self.failed_rounds_count == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_314(BaseStrategy):
    """
    Collective Enforcer with Forgiveness Strategy for the Collective Risk Dilemma.

    This strategy aims to promote cooperation by starting cooperatively, sustaining
    cooperation when the group meets the threshold, punishing when the group fails,
    and forgiving when cooperation is re-established. It also protects itself in
    the final round by defecting.
    """

    class StrategyState(Enum):
        """Internal states for the Collective Enforcer with Forgiveness strategy."""
        COOPERATING = 1
        PUNISHING = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and sets the initial state.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.current_strategy_state = self.StrategyState.COOPERATING

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on game state and history.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs of this player and opponents.
                     Is None for the very first round (round_number == 0).

        Returns:
            An Action (C or D) for the current round.
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == 1:
            self.current_strategy_state = self.StrategyState.COOPERATING
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_cooperators_previous_round = int(sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
        if self.current_strategy_state == self.StrategyState.COOPERATING:
            if num_cooperators_previous_round >= min_cooperators_needed:
                return Action.C
            else:
                self.current_strategy_state = self.StrategyState.PUNISHING
                return Action.D
        elif self.current_strategy_state == self.StrategyState.PUNISHING:
            if num_cooperators_previous_round >= min_cooperators_needed:
                self.current_strategy_state = self.StrategyState.COOPERATING
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_315(BaseStrategy):
    """
    Adaptive Collective Sustainer with Forgiveness (ACS-F) strategy for the Collective Risk Dilemma.

    This strategy aims to achieve collective success by defaulting to cooperation,
    forgiving minor deviations, and strategically punishing consistent failures
    or exploitation to re-establish cooperation. It balances the collective mindset
    with necessary robustness.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.consecutive_exploitations: int = 0
        self.punishment_rounds_left: int = 0
        self.failed_cooperation_threshold: int = 2
        self.exploitation_threshold: int = 2
        self.punishment_duration: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_action_prev_round = history.my_actions[-1]
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        c_prev = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
        outcome_prev_round_met_threshold = c_prev >= self.game_description.m
        if self.punishment_rounds_left > 0:
            self.punishment_rounds_left -= 1
        if outcome_prev_round_met_threshold:
            self.consecutive_failures = 0
            if c_prev == self.game_description.m:
                self.consecutive_exploitations += 1
            else:
                self.consecutive_exploitations = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_exploitations = 0
        if self.punishment_rounds_left == 0:
            if self.consecutive_failures >= self.failed_cooperation_threshold:
                self.punishment_rounds_left = self.punishment_duration
            elif self.consecutive_exploitations >= self.exploitation_threshold:
                self.punishment_rounds_left = self.punishment_duration
        if self.punishment_rounds_left > 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.consecutive_failures == 0 and self.consecutive_exploitations == 0:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_316(BaseStrategy):
    """
    The "Adaptive Collective Enforcer (ACE)" strategy for the Collective Risk Dilemma.
    It prioritizes achieving the collective benefit (meeting the 'm' cooperator threshold)
    by being proactively cooperative, enforcing the collective threshold, and
    protecting itself from consistent exploitation. It balances the desire for
    collective good with the realities of a competitive tournament environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_countdown: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed_m = self.game_description.m
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        else:
            my_prev_action_was_cooperate = history.my_actions[current_round_0_indexed - 1]
            opponents_prev_cooperators_count = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
            total_cooperators_in_prev_round = int(my_prev_action_was_cooperate) + opponents_prev_cooperators_count
            if total_cooperators_in_prev_round >= min_cooperators_needed_m:
                return Action.C
            else:
                self.punishment_countdown = 1
                return Action.D

class Strategy_COLLECTIVE_317(BaseStrategy):
    """
    Adaptive Patience and Reciprocity (APR) strategy for the Collective Risk Dilemma.

    This strategy is a conditional cooperator that prioritizes the sustained achievement
    of the collective 'k' bonus. It is patient with initial collective failures and
    actively tries to restore cooperation, but it will eventually defect if the group
    consistently fails to meet the threshold. Conversely, if the group is consistently
    successful, it cautiously and reciprocally explores individual payoff optimization
    (free-riding) but always cycles back to cooperation to maintain stability and avoid
    being a persistent exploiter.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.consecutive_failures: int = 0
        self.cooperation_streak_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        chosen_action: Action
        if state.round_number == 0:
            chosen_action = Action.C
            self.my_last_action = chosen_action
            return chosen_action
        prev_round_idx = state.round_number - 1
        prev_my_action_was_C = history.my_actions[prev_round_idx]
        C_count_prev = int(prev_my_action_was_C) + np.sum(history.opponent_actions[prev_round_idx, :])
        prev_was_collective_success = C_count_prev >= m_threshold
        if not prev_was_collective_success:
            self.consecutive_failures += 1
            self.cooperation_streak_success = 0
        else:
            self.consecutive_failures = 0
            if prev_my_action_was_C:
                self.cooperation_streak_success += 1
            else:
                self.cooperation_streak_success = 0
        min_consecutive_failures_threshold = min(n_rounds - 1, 2)
        if self.consecutive_failures >= min_consecutive_failures_threshold:
            chosen_action = Action.D
        elif not prev_was_collective_success:
            chosen_action = Action.C
        elif prev_my_action_was_C:
            if C_count_prev > m_threshold and self.cooperation_streak_success >= 3:
                chosen_action = Action.D
            else:
                chosen_action = Action.C
        else:
            chosen_action = Action.C
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_COLLECTIVE_318(BaseStrategy):
    """
    The Adaptive Collective Maintainer (ACM) strategy is built on principles of conditional cooperation,
    aiming to initiate and sustain collective effort. It exhibits a proactive stance towards cooperation
    but adapts to persistent collective failure by temporarily withdrawing support to protect its own
    interests and signal the unsustainability of current collective behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak: int = 0
        self.forgiveness_rounds: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_my_action = history.my_actions[-1]
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        c_prev = int(previous_round_my_action) + np.sum(previous_round_opponent_actions)
        m = self.game_description.m
        if c_prev >= m:
            self.failure_streak = 0
            return Action.C
        else:
            self.failure_streak += 1
            if self.failure_streak <= self.forgiveness_rounds:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_319(BaseStrategy):
    """
    The "Adaptive Forgiver" strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain cooperation. It starts by cooperating
    and continues to cooperate as long as the collective goal (m cooperators) is met.
    It provides a "second chance" by cooperating after a single round where the
    threshold was not met. However, if the collective fails to meet the threshold
    for two or more consecutive rounds, it switches to defection as a protective
    measure and a signal for persistent non-cooperation. In the final round, it
    always defects, following the individually rational choice in finite games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        previous_round_index = state.round_number - 1
        cooperators_in_previous_round = int(history.my_actions[previous_round_index]) + np.sum(history.opponent_actions[previous_round_index, :])
        if cooperators_in_previous_round >= m_threshold:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if cooperators_in_previous_round >= m_threshold:
            return Action.C
        elif self.consecutive_failures == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_320(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by leading with a cooperative action in the first round.
    It continues to cooperate when the collective threshold 'm' is met in the previous round,
    reinforcing success. If the threshold is missed, it shows patience, tolerating a
    configurable number of consecutive failures (`defection_tolerance_limit`) before
    temporarily defecting. This defection serves as a signal or 'punishment' to encourage
    others to cooperate more. In the final round, it defects due to the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.rounds_failed_threshold_consec: int = 0
        self.defection_tolerance_limit: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == 0:
            return Action.C
        if current_round_t == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_t - 1
        opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :])
        my_action_prev = history.my_actions[previous_round_idx]
        C_prev = int(my_action_prev) + opponent_cooperators_prev
        if C_prev >= self.m:
            self.rounds_failed_threshold_consec = 0
        else:
            self.rounds_failed_threshold_consec += 1
        if C_prev >= self.m:
            return Action.C
        elif self.rounds_failed_threshold_consec <= self.defection_tolerance_limit:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_321(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain a cooperative equilibrium by
    reciprocating cooperation when successful, punishing exploitation or collective
    failure, and attempting to revive cooperation when it falters. It explicitly
    considers its own past actions in the context of collective outcomes to avoid
    being a perpetual "sucker" while still promoting the group's welfare.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators needed), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing the player's and opponents' actions and payoffs
                     from previous rounds. It is None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed == 0:
            return Action.C
        if current_round_number_0_indexed == r - 1:
            return Action.D
        my_last_action_was_C = history.my_actions[-1]
        C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if C_prev >= m:
            if C_prev == n:
                return Action.C
            elif my_last_action_was_C:
                return Action.D
            else:
                return Action.C
        elif C_prev == 0:
            return Action.D
        elif my_last_action_was_C:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_322(BaseStrategy):
    """
    The Adaptive Collective Risk Strategist (ACRS) is designed to navigate the Collective Risk Dilemma.
    It aims to achieve the collective reward 'k' consistently while adapting to the group's behavior
    and protecting individual payoffs when collective efforts fail. It incorporates principles of
    initial cooperation, stability in success, targeted persistence in near-misses, and
    self-preservation in significant failures, with rational defection in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        my_cooperation_prev_count = 1 if my_action_prev_bool else 0
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        c_prev = opponent_cooperators_prev + my_cooperation_prev_count
        if c_prev >= min_cooperators_m:
            if my_action_prev_bool:
                return Action.C
            else:
                return Action.D
        elif c_prev == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_323(BaseStrategy):
    """
    Implements the "Resilient Collective Cooperator (RCC)" strategy for the Collective Risk Dilemma.
    This strategy starts by cooperating and attempts to maintain cooperation by forgiving
    a certain number of consecutive collective failures (m cooperators not met).
    After the forgiveness buffer is exceeded, it temporarily defects to signal dissatisfaction
    and protect individual payoff, hoping to re-establish cooperation. It always defects
    in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0
        self.forgiveness_buffer = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_prev_action_is_cooperate = history.my_actions[-1]
        opponent_prev_actions = history.opponent_actions[-1, :]
        num_cooperators_prev_round = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions)
        if num_cooperators_prev_round >= self.game_description.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.consecutive_failures <= self.forgiveness_buffer:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_324(BaseStrategy):
    """
    Adaptive Collective Nudge (ACN) strategy for the Collective Risk Dilemma.
    This strategy aims to foster cooperation, deter exploitation, and adapt
    pragmatically to the group's performance. It prioritizes collective
    outcomes while ensuring robustness against diverse opponent behaviors
    in a tournament setting.

    The core idea is to establish initial cooperation, reward successful
    collective action, strategically nudge the group towards cooperation
    after failures, and resort to a defensive stance only when failures
    become persistent.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.failure_streak: int = 0
        self._persistent_failure_threshold: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_action: Action
        if state.round_number == 0:
            current_round_action = Action.C
            self.failure_streak = 0
        else:
            my_prev_action_bool = history.my_actions[-1]
            prev_round_num_cooperators = sum(history.opponent_actions[-1, :]) + (1 if my_prev_action_bool else 0)
            threshold_met_prev_round = prev_round_num_cooperators >= self.game_description.m
            if threshold_met_prev_round:
                self.failure_streak = 0
            else:
                self.failure_streak += 1
            if threshold_met_prev_round:
                if self.my_last_action == Action.C:
                    current_round_action = Action.C
                else:
                    current_round_action = Action.C
            elif self.failure_streak >= self._persistent_failure_threshold:
                current_round_action = Action.D
            elif self.my_last_action == Action.C:
                current_round_action = Action.D
            else:
                current_round_action = Action.C
        self.my_last_action = current_round_action
        return current_round_action

class Strategy_COLLECTIVE_325(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    Prioritizes the collective good by striving to achieve the 'm' cooperator threshold
    to secure the 'k' reward for all players. It exhibits an initial willingness to
    cooperate and maintains cooperation when the group is successful. However, it is
    pragmatic and will temporarily withdraw cooperation if the group consistently
    fails to meet the common goal, thus preventing exploitation and signaling
    dissatisfaction to the collective. It also defects in the final round
    due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Contributor strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including the round number.
            history: An object containing the player's and opponents' past actions and payoffs.
                     It is None for the very first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.consecutive_failures = 0
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            cooperators_count_prev_round = np.sum(history.opponent_actions[-1, :])
            if history.my_actions[-1]:
                cooperators_count_prev_round += 1
            if cooperators_count_prev_round >= self.game_description.m:
                self.consecutive_failures = 0
                return Action.C
            else:
                self.consecutive_failures += 1
                return Action.D

class Strategy_COLLECTIVE_326(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.

    This strategy aims to promote and sustain collective cooperation by prioritizing the consistent
    achievement of the collective threshold 'm'. It balances self-preservation with collective
    reward using conditional cooperation, targeted punishment, and strategic forgiveness.
    It responds to observed collective outcomes rather than individual player histories.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATE strategy with game parameters and internal state variables.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.max_punishment_rounds = 2
        self.rounds_since_success = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action for the current round based on the ATE strategy rules.

        Args:
            state: An object containing the current round number (0-indexed).
                   round_number = 0 for the first round of the game.
            history: An object containing past actions and payoffs. None for the first round.

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        else:
            opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round = history.my_actions[-1]
            total_cooperators_prev_round = opponent_cooperators_prev_round + my_action_prev_round
            if total_cooperators_prev_round >= self.game_description.m:
                self.rounds_since_success = 0
                return Action.C
            else:
                self.rounds_since_success += 1
                if self.rounds_since_success <= self.max_punishment_rounds:
                    return Action.D
                else:
                    return Action.C

class Strategy_COLLECTIVE_327(BaseStrategy):
    """
    The Robust Collective Sustainer strategy aims to achieve and maintain collective benefit
    in the Collective Risk Dilemma. It initiates cooperation, sustains it when the threshold
    is met, tolerates a single round of collective failure, but punishes persistent failures
    by defecting. It also defects in the final round due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_strikes: int = 0
        self.my_last_action: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_t == total_rounds:
            action_to_take = Action.D
            self.my_last_action = action_to_take
            return action_to_take
        if current_round_t == 1:
            action_to_take = Action.C
            self.my_last_action = action_to_take
            return action_to_take
        previous_round_Nc = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            previous_round_Nc += 1
        if previous_round_Nc >= min_cooperators_needed:
            self.failure_strikes = 0
            action_to_take = Action.C
        else:
            self.failure_strikes += 1
            if self.failure_strikes == 1:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_COLLECTIVE_328(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.
    It starts cooperatively, maintains cooperation if the threshold 'm' is met,
    initiates a temporary punishment (defecting) phase if the threshold is missed,
    and then "forgives" by re-cooperating after a set number of consecutive failures
    during the punishment phase, to re-attempt collective success.
    The strategy incorporates rational defection in the final round of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self._in_punishment_phase: bool = False
        self._punishment_rounds_since_last_cooperation_attempt: int = 0
        self._forgiveness_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self._in_punishment_phase = False
            self._punishment_rounds_since_last_cooperation_attempt = 0
            return Action.C
        if current_round_number_1_indexed == self.n_rounds:
            return Action.D
        previous_round_index = state.round_number - 1
        num_cooperators_prev_round = int(history.my_actions[previous_round_index]) + np.sum(history.opponent_actions[previous_round_index, :])
        threshold_met_last_round = num_cooperators_prev_round >= self.m
        if threshold_met_last_round:
            self._in_punishment_phase = False
            self._punishment_rounds_since_last_cooperation_attempt = 0
            return Action.C
        elif not self._in_punishment_phase:
            self._in_punishment_phase = True
            return Action.D
        else:
            self._punishment_rounds_since_last_cooperation_attempt += 1
            if self._punishment_rounds_since_last_cooperation_attempt >= self._forgiveness_threshold:
                self._in_punishment_phase = False
                self._punishment_rounds_since_last_cooperation_attempt = 0
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_329(BaseStrategy):
    """
    Adaptive Collective Threshold Responder (ACTR) strategy for the Collective Risk Dilemma.

    ACTR aims to establish and sustain a cooperative equilibrium where the minimum threshold 'm'
    is consistently met. It initiates with cooperation in the first round to signal willingness
    to contribute to the common good.

    In intermediate rounds (not the first or last), ACTR adapts its behavior based on the
    collective outcome of the previous round:
    - If the minimum cooperation threshold 'm' was met or exceeded in the previous round
      (collective success), ACTR continues to Cooperate (C) to reinforce the positive state
      and sustain the beneficial equilibrium.
    - If 'm' was not met in the previous round (collective failure), ACTR chooses to Defect (D).
      This serves as a signal to the group that insufficient cooperation will result in a
      withdrawal of contribution, and also prevents repeated exploitation, acting as a form
      of "collective tit-for-tat."

    In the final round, ACTR pragmatically chooses to Defect (D). This is a robust decision
    in finite repeated games, guarding against being exploited in the anticipated breakdown
    of cooperation that often occurs at the end of such games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
        previous_round_my_action_cooperated = history.my_actions[state.round_number - 1]
        previous_round_total_cooperators = previous_round_opponent_cooperators + previous_round_my_action_cooperated
        if previous_round_total_cooperators >= min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_330(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    Aims to foster and maintain collective cooperation to secure the 'k' reward,
    while being adaptive and robust enough to survive in a tournament against
    diverse opponent behaviors. It prioritizes collective success but includes
    mechanisms for self-preservation if the collective effort consistently fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_rounds_missed_threshold_consecutively = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        n_c_opponents_prev_round = int(np.sum(history.opponent_actions[-1, :]))
        my_action_prev_round_is_c = history.my_actions[-1]
        n_c_prev_round = n_c_opponents_prev_round + (1 if my_action_prev_round_is_c else 0)
        if n_c_prev_round >= min_cooperators_needed:
            self.num_rounds_missed_threshold_consecutively = 0
        else:
            self.num_rounds_missed_threshold_consecutively += 1
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if self.num_rounds_missed_threshold_consecutively == 0:
            return Action.C
        elif self.num_rounds_missed_threshold_consecutively == 1:
            return Action.C
        else:
            lower_cooperation_bound = max(1, math.floor(min_cooperators_needed / 2))
            if n_c_prev_round < lower_cooperation_bound:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_331(BaseStrategy):
    """
    Adaptive Collective Response (ACR) strategy for the Collective Risk Dilemma.

    ACR is a conditionally cooperative strategy designed to secure the collective
    reward 'k'. It begins by cooperating and dynamically monitors whether the 'm'
    cooperation threshold is met in the previous round. If the threshold is not met,
    it initiates a temporary defection (punishment) phase for a predefined duration.
    Once the punishment concludes, it reverts to cooperation. In the final round
    of the game, ACR adopts a purely self-interested defection strategy to prevent
    exploitation, as there are no future interactions to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_countdown: int = 0
        self.PUNISHMENT_LENGTH: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_index = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_index == total_rounds - 1:
            return Action.D
        if current_round_index == 0:
            return Action.C
        my_action_prev_round_is_C = history.my_actions[-1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = int(my_action_prev_round_is_C) + num_opponent_cooperators_prev_round
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
        elif num_cooperators_prev_round < m_threshold:
            self.punishment_countdown = self.PUNISHMENT_LENGTH
        if self.punishment_countdown > 0:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_332(BaseStrategy):
    """
    Adaptive Collective Pledger (ACP) strategy for the Collective Risk Dilemma game.
    It cooperates initially, maintains cooperation if the project succeeds,
    defects once if the project fails (and player previously cooperated),
    then re-cooperates if the project continues to fail (and player previously defected).
    It defects in the final round due to the endgame effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        action_to_take: Action
        if current_round == 0:
            action_to_take = Action.C
            self.my_last_action = action_to_take
            return action_to_take
        elif current_round == total_rounds - 1:
            action_to_take = Action.D
            return action_to_take
        else:
            my_action_prev_round_bool = history.my_actions[-1]
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1])
            total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_bool else 0)
            project_succeeded_last_round = total_cooperators_prev_round >= min_cooperators_m
            my_last_action_for_decision = Action.C if my_action_prev_round_bool else Action.D
            if project_succeeded_last_round:
                action_to_take = Action.C
            elif my_last_action_for_decision == Action.C:
                action_to_take = Action.D
            else:
                action_to_take = Action.C
            self.my_last_action = action_to_take
            return action_to_take

class Strategy_COLLECTIVE_333(BaseStrategy):
    """
    Adaptive Threshold Maintenance (ATM) strategy for Collective Risk Dilemma.
    Prioritizes achieving the collective reward 'k' by ensuring at least 'm' cooperators.
    It begins by signaling cooperation and then adapts its behavior based on the success
    or failure of the collective in previous rounds, and its own previous action. It aims
    to be robust by leading by example when cooperation is low, sustaining cooperation
    when it's precarious, and prudently testing defection when there's a clear surplus
    of cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_number_0_indexed == 0:
            return Action.C
        if current_round_number_0_indexed == total_rounds - 1:
            return Action.D
        previous_round_idx = current_round_number_0_indexed - 1
        my_previous_action = history.my_actions[previous_round_idx]
        opponent_cooperators_previous_round = sum(history.opponent_actions[previous_round_idx, :])
        num_cooperators_previous_round = opponent_cooperators_previous_round + int(my_previous_action)
        if num_cooperators_previous_round < min_cooperators_m:
            return Action.C
        elif num_cooperators_previous_round == min_cooperators_m:
            return Action.C
        elif my_previous_action == Action.C:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_334(BaseStrategy):
    """
    The Dynamic Threshold Adapter (DTA) strategy balances collective cooperation
    with individual robustness. It adapts its behavior based on the collective
    outcome of the previous round and its own prior action, cycling through
    states of COOPERATE, RECALIBRATE, and PUNISH to incentivize cooperation,
    respond to free-riding, and enforce collective responsibility. It starts
    by cooperating, defects in the last round (finite game rationality), and
    dynamically adjusts its stance in intermediate rounds.
    """

    class DTAState(Enum):
        """
        Internal states for the Dynamic Threshold Adapter strategy.
        - COOPERATE: Actively trying to contribute to the collective good.
        - RECALIBRATE: Temporarily defecting in response to free-riding
                      while the threshold was met.
        - PUNISH: Defecting due to sustained collective failure (threshold not met).
        """
        COOPERATE = 1
        RECALIBRATE = 2
        PUNISH = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_current_stance: Strategy.DTAState = self.DTAState.COOPERATE
        self.my_last_action: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        chosen_action: Action
        if state.round_number == 0:
            chosen_action = Action.C
            self.my_current_stance = self.DTAState.COOPERATE
            self.my_last_action = chosen_action
            return chosen_action
        elif state.round_number == self.game_description.n_rounds - 1:
            chosen_action = Action.D
            self.my_last_action = chosen_action
            return chosen_action
        else:
            N_C_prev = sum(history.opponent_actions[-1, :]) + (1 if self.my_last_action == Action.C else 0)
            my_action_prev = self.my_last_action
            if self.my_current_stance == self.DTAState.COOPERATE:
                if N_C_prev >= self.game_description.m:
                    D_prev = self.game_description.n_players - N_C_prev
                    if D_prev > 0 and my_action_prev == Action.C:
                        chosen_action = Action.D
                        self.my_current_stance = self.DTAState.RECALIBRATE
                    else:
                        chosen_action = Action.C
                else:
                    chosen_action = Action.D
                    self.my_current_stance = self.DTAState.PUNISH
            elif self.my_current_stance == self.DTAState.RECALIBRATE:
                if N_C_prev >= self.game_description.m:
                    chosen_action = Action.C
                    self.my_current_stance = self.DTAState.COOPERATE
                else:
                    chosen_action = Action.D
                    self.my_current_stance = self.DTAState.PUNISH
            elif self.my_current_stance == self.DTAState.PUNISH:
                if N_C_prev >= self.game_description.m:
                    chosen_action = Action.C
                    self.my_current_stance = self.DTAState.COOPERATE
                else:
                    chosen_action = Action.D
            else:
                chosen_action = Action.D
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_COLLECTIVE_335(BaseStrategy):
    """
    The Adaptive Collective Consensus strategy is designed to balance the collective goal of meeting the cooperation
    threshold with the need for individual robustness and competitive performance. It initiates and sustains
    cooperation, but adapts to opponent behavior using conditional defection for self-preservation and signaling.
    It incorporates mechanisms for optimistic pushes, controlled free-riding, and temporary punishment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.rounds_since_last_success: int = 0
        self.rounds_since_last_failure: int = 0
        self.consecutive_free_rides: int = 0
        self.is_in_punishment_mode: bool = False
        self.FREE_RIDE_GRACE_BUFFER: int = 1
        self.FREE_RIDE_LIMIT_ROUNDS: int = 1
        self.PUNISHMENT_THRESHOLD_LOW_FACTOR: float = 0.5
        self.PUNISHMENT_MIN_THRESHOLD: int = 1
        self.PUNISHMENT_DEFECTION_STREAK: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        m = self.game_description.m
        r = self.game_description.n_rounds
        if current_round_t == 1:
            self.my_last_action = Action.C
            return Action.C
        my_action_prev_round_was_C = history.my_actions[-1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        N_C_prev = int(my_action_prev_round_was_C) + opponent_cooperators_prev_round
        if N_C_prev >= m:
            self.rounds_since_last_failure = 0
            self.rounds_since_last_success += 1
            if self.is_in_punishment_mode:
                self.is_in_punishment_mode = False
        else:
            self.rounds_since_last_success = 0
            self.rounds_since_last_failure += 1
        if self.is_in_punishment_mode or self.rounds_since_last_failure >= self.PUNISHMENT_DEFECTION_STREAK:
            self.is_in_punishment_mode = True
            self.my_last_action = Action.D
            return Action.D
        if current_round_t == r:
            if N_C_prev >= m:
                self.my_last_action = Action.C
                return Action.C
            else:
                self.my_last_action = Action.D
                return Action.D
        min_N_C_for_severe_failure = max(self.PUNISHMENT_MIN_THRESHOLD, math.floor(m * self.PUNISHMENT_THRESHOLD_LOW_FACTOR))
        if N_C_prev >= m:
            if N_C_prev >= m + self.FREE_RIDE_GRACE_BUFFER and self.consecutive_free_rides < self.FREE_RIDE_LIMIT_ROUNDS:
                self.consecutive_free_rides += 1
                self.my_last_action = Action.D
                return Action.D
            else:
                self.consecutive_free_rides = 0
                self.my_last_action = Action.C
                return Action.C
        else:
            self.consecutive_free_rides = 0
            if N_C_prev < min_N_C_for_severe_failure:
                self.my_last_action = Action.D
                return Action.D
            else:
                self.my_last_action = Action.C
                return Action.C

class Strategy_COLLECTIVE_336(BaseStrategy):
    """
    Adaptive Threshold-Based Cooperation (ATBC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and sustain cooperation to achieve the collective reward 'k'.
    It initiates cooperation in the first round to establish a collective effort.
    In intermediate rounds, it adapts its behavior based on whether the 'm' cooperator
    threshold was met in the previous round: it continues to cooperate if the threshold
    was met, and defects if it was not. In the final round, it always defects to maximize
    individual payoff, a standard game theory outcome for known-end games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_previous_action_was_cooperate = history.my_actions[state.round_number - 1]
        opponent_previous_actions = history.opponent_actions[state.round_number - 1, :]
        previous_round_cooperators = int(my_previous_action_was_cooperate) + np.sum(opponent_previous_actions)
        if previous_round_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_337(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.
    This strategy aims to promote and sustain cooperation by
    cooperating initially, punishing collective failures with temporary defection,
    and then "forgiving" by attempting to re-establish cooperation.
    It also allows for strategic free-riding once the collective threshold is met
    if it previously defected without jeopardizing the collective good.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.max_punishment_rounds = 2
        self.failure_streak: int = 0
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        my_current_action: Action
        if current_round_t == 0:
            my_current_action = Action.C
        elif current_round_t == total_rounds_r - 1:
            my_current_action = Action.D
        else:
            C_count_prev = sum(history.opponent_actions[-1, :])
            C_count_prev += self.my_last_action.value
            if C_count_prev >= min_cooperators_m:
                self.failure_streak = 0
                if self.my_last_action == Action.C:
                    my_current_action = Action.C
                else:
                    my_current_action = Action.D
            else:
                self.failure_streak += 1
                if self.failure_streak <= self.max_punishment_rounds:
                    my_current_action = Action.D
                else:
                    my_current_action = Action.C
        self.my_last_action = my_current_action
        return my_current_action

class Strategy_COLLECTIVE_338(BaseStrategy):
    """
    Adaptive Collective Sustainer (ACS) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and sustain collective cooperation while being robust
    against various opponent behaviors. It prioritizes achieving the collective reward `k`
    through conditional cooperation, incorporating a built-in cycle of punishment and
    forgiveness. It starts by cooperating, continues to do so if the collective threshold
    `m` is met. If `m` is not met, it defects for one round to signal disapproval
    (`punishment_mode = True`). In the subsequent round, if the threshold is still not met
    and it was in punishment mode, it attempts to "forgive" by cooperating again,
    preventing a permanent defection spiral. In the last round of the game, it always
    defects due to the endgame effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Sustainer (ACS) strategy.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                         game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.punishment_mode: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing records of past rounds'
                                            actions and payoffs. None for the first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            self.punishment_mode = False
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_bool = history.my_actions[state.round_number - 1]
        opponent_prev_actions_bool = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_in_prev_round = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bool)
        prev_threshold_met = num_cooperators_in_prev_round >= self.game_description.m
        if prev_threshold_met:
            self.punishment_mode = False
            return Action.C
        elif self.punishment_mode:
            self.punishment_mode = False
            return Action.C
        else:
            self.punishment_mode = True
            return Action.D

class Strategy_COLLECTIVE_339(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize the collective payoff by striving for consistent
    cooperation. It starts cooperatively and continues to cooperate as long as the
    collective effort succeeds. It is adaptive to opponent behavior and robust
    against exploitation:
    - It never attempts to free-ride if the collective threshold is met.
    - It maintains a 'collective_defection_streak' to track consecutive failures
      to meet the cooperation threshold 'm'.
    - After three consecutive failures, it permanently 'gives up' on cooperation
      and defects for the remainder of the game.
    - For minor, single failures, it "forgives" and attempts to re-establish
      cooperation if the previous cooperation count was at least m/2.
    - Otherwise (more severe single failure or two consecutive failures), it
      temporarily defects as a signal/punishment, reverting to cooperation only
      if the threshold is met again.
    - In the last round, it prioritizes individual payoff if cooperation failed
      in the prior round and the 'given up' flag is not set, but cooperates
      if cooperation was successful or if already given up.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.collective_defection_streak: int = 0
        self.has_given_up_on_cooperation: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        C_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if C_prev >= m:
            self.collective_defection_streak = 0
        else:
            self.collective_defection_streak += 1
        if self.collective_defection_streak >= 3:
            self.has_given_up_on_cooperation = True
        if state.round_number == n_rounds - 1:
            if self.has_given_up_on_cooperation:
                return Action.D
            elif C_prev >= m:
                return Action.C
            else:
                return Action.D
        if self.has_given_up_on_cooperation:
            return Action.D
        if C_prev >= m:
            return Action.C
        elif self.collective_defection_streak == 1 and C_prev >= m / 2.0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_340(BaseStrategy):
    """
    The "Adaptive Collective Contributor (ACC)" strategy for the Collective Risk Dilemma.
    This strategy aims to foster and sustain cooperation, balancing individual self-preservation
    with collective success. It's adaptive, learning from past rounds, and robust to varying
    levels of opponent cooperation or defection.

    It operates on the principle of "conditional cooperation with structured forgiveness and
    limited exploitation tolerance." It proactively cooperates but will withdraw support if
    collective efforts consistently fail. If cooperation is successful, it ensures it's not
    perpetually exploited by free-riders while also taking its turn to contribute.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.consecutive_failures: int = 0
        self.my_coop_streak_successful: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            final_decision_for_this_round = Action.C
            self.my_last_action = final_decision_for_this_round
            self.consecutive_failures = 0
            self.my_coop_streak_successful = 0
            return final_decision_for_this_round
        my_action_was_c_prev = history.my_actions[-1]
        n_cooperators_among_opponents_prev = np.sum(history.opponent_actions[-1, :])
        N_C_prev = n_cooperators_among_opponents_prev + (1 if my_action_was_c_prev else 0)
        N_D_prev = n - N_C_prev
        planned_action_this_round = Action.C
        if N_C_prev < m:
            self.consecutive_failures += 1
            self.my_coop_streak_successful = 0
            if self.consecutive_failures >= 2:
                planned_action_this_round = Action.D
            else:
                planned_action_this_round = Action.C
        else:
            self.consecutive_failures = 0
            if my_action_was_c_prev:
                self.my_coop_streak_successful += 1
                if self.my_coop_streak_successful >= m and N_C_prev == m:
                    planned_action_this_round = Action.D
                else:
                    planned_action_this_round = Action.C
            else:
                self.my_coop_streak_successful = 0
                planned_action_this_round = Action.C
        if current_round_1_indexed == r and N_C_prev >= m and (planned_action_this_round == Action.D):
            planned_action_this_round = Action.C
        self.my_last_action = planned_action_this_round
        return planned_action_this_round

class Strategy_COLLECTIVE_341(BaseStrategy):
    """
    Robust Collective Maintainer (RCM) strategy for the Collective Risk Dilemma.
    Prioritizes stable collective success by consistently cooperating when the collective threshold is at risk or merely met.
    It avoids attempts at individual free-riding that could destabilize the collective outcome.
    RCM's primary goal is to ensure the collective 'k' bonus is achieved in as many rounds as possible,
    adapting only to conditions of collective failure or the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_cooperation_last_round = int(history.my_actions[-1])
        opponent_cooperation_last_round = np.sum(history.opponent_actions[-1, :])
        cooperators_last_round = my_cooperation_last_round + opponent_cooperation_last_round
        return Action.C

class Strategy_COLLECTIVE_342(BaseStrategy):
    """
    Adaptive Threshold Enforcement (ATE) Strategy for Collective Risk Dilemma.

    This strategy aims to foster and maintain a cooperative environment where
    the minimum cooperator threshold (m) is consistently met. It starts
    cooperatively, punishes collective failures with a single round of
    defection, and then attempts to re-establish cooperation. It explicitly
    accounts for the first and last rounds as special cases.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.is_punishing_due_to_failure = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.is_punishing_due_to_failure = False
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            self.is_punishing_due_to_failure = False
            return Action.D
        my_prev_action_cooperated = history.my_actions[state.round_number - 1]
        opponents_prev_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
        num_cooperators_prev_round = int(my_prev_action_cooperated) + int(opponents_prev_cooperators)
        if num_cooperators_prev_round >= self.game_description.m:
            self.is_punishing_due_to_failure = False
            return Action.C
        elif self.is_punishing_due_to_failure:
            self.is_punishing_due_to_failure = False
            return Action.C
        else:
            self.is_punishing_due_to_failure = True
            return Action.D

class Strategy_COLLECTIVE_343(BaseStrategy):
    """
    The Adaptive Collective Sustainer (ACS) strategy aims to establish and sustain
    collective cooperation to consistently achieve the shared reward 'k'. It operates
    on an adaptive, round-by-round basis, rewarding successful group cooperation
    and punishing collective failure, while accounting for the finite nature of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            my_prev_action_cooperated = history.my_actions[state.round_number - 1]
            opponents_prev_cooperated_count = np.sum(history.opponent_actions[state.round_number - 1, :])
            C_prev = my_prev_action_cooperated + opponents_prev_cooperated_count
            if C_prev >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_344(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to promote and sustain
    collective cooperation in the Collective Risk Dilemma by adapting its behavior
    based on past outcomes and its own contribution. It incorporates an initial
    signaling phase, a dynamic response to collective success or failure, and a
    rational end-game play.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        else:
            my_action_prev = history.my_actions[state.round_number - 1]
            opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
            N_C_prev = int(my_action_prev) + opponent_cooperators_prev
            if N_C_prev >= self.m:
                if my_action_prev == True:
                    if N_C_prev == self.m:
                        return Action.C
                    else:
                        return Action.D
                else:
                    return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_345(BaseStrategy):
    """
    The Adaptive Collective Stabilizer (ACS) strategy aims to foster and sustain collective
    cooperation for the majority of the game, while being adaptive to fluctuating levels
    of cooperation and robust against various opponent behaviors, including potential
    exploitation in the final round. It initiates cooperation, tolerates minor setbacks,
    signals significant collective failure through temporary defection, and then re-engages
    in cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.D_threshold = 2
        self.P_rounds = 2
        self.defection_streak = 0
        self.punish_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_0_indexed_round == 0:
            return Action.C
        if current_0_indexed_round == n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if num_cooperators_prev_round >= m_threshold:
            self.defection_streak = 0
        else:
            self.defection_streak += 1
        if self.punish_rounds_left > 0:
            self.punish_rounds_left -= 1
            return Action.D
        elif self.defection_streak >= self.D_threshold:
            remaining_intermediate_rounds = n_rounds - current_0_indexed_round - 2
            self.punish_rounds_left = max(0, min(self.P_rounds - 1, remaining_intermediate_rounds))
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_346(BaseStrategy):
    """
    The Adaptive Collective Steward strategy for the Collective Risk Dilemma.
    It initiates cooperation, encourages its maintenance, and takes corrective
    action when the collective goal (m cooperators) is not met, always
    prioritizing the achievement of the collective reward (k).

    This strategy operates under a "collective mindset":
    - It prioritizes achieving the 'k' reward for all players.
    - Its contribution is conditional and adapts based on the previous round's outcome.
    - It reinforces successful cooperation by contributing to maintain the threshold.
    - If it defected and the collective failed, it attempts to re-initiate cooperation.
    - If it cooperated and the collective failed, it switches to defecting to protect itself (punishment).
    - It does not persistently free-ride; if it defected and the collective succeeded, it switches to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        prev_round_index = state.round_number - 1
        my_action_prev_bool = history.my_actions[prev_round_index]
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_index, :])
        N_C_prev = int(my_action_prev_bool) + opponent_cooperators_prev
        if N_C_prev < m:
            if my_action_prev_bool:
                return Action.D
            else:
                return Action.C
        elif my_action_prev_bool:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_347(BaseStrategy):
    """
    The Adaptive Collective Thresholder (ACT) strategy for the Collective Risk Dilemma.
    It aims to foster collective cooperation by starting cooperatively, maintaining cooperation
    when the collective threshold 'm' is met, and conditionally punishing with defection
    when the threshold is missed. It also periodically attempts to re-establish cooperation
    after failures and defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACT strategy with game parameters and internal state variables.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.consecutive_failure_rounds: int = 0
        self.retest_period: int = max(2, math.floor(self.game_description.n_rounds / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the strategy's rules and historical outcomes.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player and opponents,
                     or None if it's the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            n_cooperators_prev_round = int(history.my_actions[-1]) + int(np.sum(history.opponent_actions[-1, :]))
            if n_cooperators_prev_round >= min_cooperators:
                self.consecutive_failure_rounds = 0
                return Action.C
            else:
                self.consecutive_failure_rounds += 1
                if self.consecutive_failure_rounds % self.retest_period == 0:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_348(BaseStrategy):
    """
    Conditional Contributor with Forgiveness (CCF) strategy for the Collective Risk Dilemma.

    This strategy aims to achieve collective reward 'k' by initially cooperating. It monitors
    whether the group meets the minimum cooperator threshold 'm' in previous rounds.
    If the threshold is met, it continues to cooperate. If the threshold is not met,
    it increments a `consecutive_failures` counter. It tolerates one round of failure
    (playing C even after a setback), but if two or more consecutive failures occur,
    it defects as a "punishment" and for self-preservation, then resets the failure counter
    to allow for a new cycle of cooperation attempts.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number + 1
        if current_round_number == 1:
            return Action.C
        prev_round_index = state.round_number - 1
        my_prev_action_was_C = history.my_actions[prev_round_index]
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_index, :])
        num_cooperators_prev_round = int(my_prev_action_was_C) + opponent_prev_cooperators
        m = self.game_description.m
        if num_cooperators_prev_round >= m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures >= 2:
                self.consecutive_failures = 0
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_349(BaseStrategy):
    """
    Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    ACE aims to achieve the collective reward `k` as consistently as possible.
    It starts with cooperation and then adaptively responds to whether the
    cooperation threshold `m` was met in the previous round. Collective failures
    are punished with a temporary defection to signal detriment, followed by
    quick forgiveness to re-establish cooperation. In the final round, it defects
    to maximize private payoff and avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action_t_minus_1: Action | None = None
        self.rounds_in_punishment_mode: int = 0
        self.punishment_duration: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        action: Action
        if current_round_0_indexed == total_rounds - 1:
            action = Action.D
            return action
        if current_round_0_indexed == 0:
            action = Action.C
            self.my_last_action_t_minus_1 = action
            self.rounds_in_punishment_mode = 0
            return action
        previous_round_index = current_round_0_indexed - 1
        num_cooperators_in_previous_round = np.sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        if num_cooperators_in_previous_round >= min_cooperators_needed:
            action = Action.C
            self.my_last_action_t_minus_1 = action
            self.rounds_in_punishment_mode = 0
        elif self.rounds_in_punishment_mode < self.punishment_duration:
            action = Action.D
            self.my_last_action_t_minus_1 = action
            self.rounds_in_punishment_mode += 1
        else:
            action = Action.C
            self.my_last_action_t_minus_1 = action
            self.rounds_in_punishment_mode = 0
        return action

class Strategy_COLLECTIVE_350(BaseStrategy):
    """
    The Progressive Collective Reinforcement strategy aims to foster and maintain
    collective cooperation in the Collective Risk Dilemma by adapting its behavior
    based on past group performance. It prioritizes the collective good (achieving
    the 'k' bonus) while being robust against persistent defection or inability
    to cooperate.

    It begins by cooperating to signal willingness, reinforces cooperation upon
    collective success, offers a "second chance" for near-misses (first consecutive
    failure by being one cooperator short), and withdraws cooperation only when
    failures are significant or repeated. In the final round, it defects to avoid
    exploitation due to the absence of future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        elif state.round_number == n_rounds - 1:
            return Action.D
        else:
            C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if C_prev >= m:
                self.consecutive_failures = 0
                return Action.C
            else:
                self.consecutive_failures += 1
                if C_prev == m - 1 and self.consecutive_failures == 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_351(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation by:
    1. Initiating cooperation (always in the first round).
    2. Tolerating minor failures (up to `TOLERANCE_ROUNDS` consecutive misses of 'm' cooperators).
    3. Punishing persistent non-cooperation by temporarily defecting (`PUNISHMENT_DURATION`).
    4. Re-establishing cooperation after punishment.
    5. Protecting against end-game exploitation by defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.TOLERANCE_ROUNDS = 2
        self.PUNISHMENT_DURATION = 1
        self.rounds_failed_to_meet_m = 0
        self.punishment_mode = False
        self.current_punishment_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.rounds_failed_to_meet_m = 0
            self.punishment_mode = False
            self.current_punishment_round = 0
            return Action.C
        C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if C_prev >= self.m:
            self.rounds_failed_to_meet_m = 0
            self.punishment_mode = False
            self.current_punishment_round = 0
        else:
            self.rounds_failed_to_meet_m += 1
        if self.punishment_mode:
            if self.current_punishment_round < self.PUNISHMENT_DURATION:
                self.current_punishment_round += 1
                return Action.D
            else:
                self.punishment_mode = False
                self.current_punishment_round = 0
                return Action.C
        elif self.rounds_failed_to_meet_m >= self.TOLERANCE_ROUNDS:
            self.punishment_mode = True
            self.current_punishment_round = 1
            self.rounds_failed_to_meet_m = 0
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_352(BaseStrategy):
    """
    Adaptive Threshold Maintainer (ATM) strategy for the Collective Risk Dilemma.
    This strategy is a finite state machine that learns from the collective outcome
    of the previous round. It attempts to establish and maintain cooperation by
    contributing (playing C) but punishes sustained collective failure by defecting (playing D),
    then cautiously re-engages with cooperation. It defects in the final round.
    """

    class State(Enum):
        COOPERATING = 'COOPERATING'
        PUNISHING = 'PUNISHING'
        RETESTING = 'RETESTING'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_tolerance_rounds = 1
        self.punish_duration = 1
        self.retest_duration = 1
        self.current_state = self.State.COOPERATING
        self.consecutive_failures = 0
        self.punish_countdown = 0
        self.retest_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if C_prev < self.game_description.m:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        if self.current_state == self.State.COOPERATING:
            if self.consecutive_failures > self.failure_tolerance_rounds:
                self.current_state = self.State.PUNISHING
                self.punish_countdown = self.punish_duration
        elif self.current_state == self.State.PUNISHING:
            self.punish_countdown -= 1
            if self.punish_countdown == 0:
                self.current_state = self.State.RETESTING
                self.retest_countdown = self.retest_duration
                self.consecutive_failures = 0
        elif self.current_state == self.State.RETESTING:
            self.retest_countdown -= 1
            if self.retest_countdown == 0:
                self.current_state = self.State.COOPERATING
                self.consecutive_failures = 0
        if self.current_state == self.State.COOPERATING:
            return Action.C
        elif self.current_state == self.State.PUNISHING:
            return Action.D
        elif self.current_state == self.State.RETESTING:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_353(BaseStrategy):
    """
    The Adaptive Collective Consensus strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain collective cooperation to consistently meet
    the 'm' threshold and secure the 'k' reward for all players.
    
    It operates on the principle of starting with cooperation, granting a "grace period"
    for a single collective failure, and only resorting to defection (punishment) if
    collective failures become persistent. It also incorporates backward induction by
    defecting in the final round.
    
    Internal state:
    - rounds_since_last_success: Tracks consecutive rounds where the 'm' cooperation
                                 threshold was NOT met. Reset to 0 upon success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_last_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            self.rounds_since_last_success = 0
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            num_opponent_cooperators_last_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_action_last_round_int = int(history.my_actions[state.round_number - 1])
            num_total_cooperators_last_round = num_opponent_cooperators_last_round + my_action_last_round_int
            threshold_met_last_round = num_total_cooperators_last_round >= m_threshold
            if threshold_met_last_round:
                self.rounds_since_last_success = 0
                return Action.C
            else:
                self.rounds_since_last_success += 1
                if self.rounds_since_last_success <= 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_354(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    It cooperates initially, adapts by cooperating when the collective threshold 'm' is met
    in the previous round, and defects as a 'punishment' when the threshold is missed.
    It also defects in the final round due to the known finite horizon (endgame effect).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_last_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_action_prev_round = history.my_actions[state.round_number - 1]
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
            if total_cooperators_prev_round >= self.game_description.m:
                self.rounds_since_last_success = 0
                return Action.C
            else:
                self.rounds_since_last_success += 1
                return Action.D

class Strategy_COLLECTIVE_355(BaseStrategy):
    """
    Collective Adaptive Threshold Strategy (CATS) for the Collective Risk Dilemma.

    This strategy aims to establish and maintain collective cooperation to secure
    the 'k' reward. It adapts based on the previous round's collective outcome:
    - In the first round, it cooperates to initiate collective action.
    - In the last round, it defects due to the end-game effect.
    - In intermediate rounds, if the cooperation threshold 'm' was met in the
      previous round, it continues to cooperate.
    - If the threshold was NOT met, it defects for one round as a punishment,
      then attempts to cooperate again in the subsequent round to re-establish
      cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and internal state variables.
        """
        self.game_description = game_description
        self.self_punished_last_round: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the CATS strategy.

        Args:
            state: GameState object containing the current round number.
            history: PlayerHistory object containing past actions and payoffs,
                     or None if it's the very first round.

        Returns:
            An Action (C or D) for the current round.
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round_0_indexed == 0:
            self.self_punished_last_round = False
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            self.self_punished_last_round = False
            return Action.D
        previous_round_my_action_was_C = history.my_actions[-1]
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        previous_round_total_cooperators = previous_round_opponent_cooperators + previous_round_my_action_was_C
        previous_round_met_threshold = previous_round_total_cooperators >= min_cooperators
        if previous_round_met_threshold:
            self.self_punished_last_round = False
            return Action.C
        elif self.self_punished_last_round:
            self.self_punished_last_round = False
            return Action.C
        else:
            self.self_punished_last_round = True
            return Action.D

class Strategy_COLLECTIVE_356(BaseStrategy):
    """
    The "Robust Collective Sustainer" strategy prioritizes the consistent achievement of the collective
    reward (k bonus) for all players. It operates on the principle that sustained collective action yields
    the highest long-term payoffs. This strategy actively contributes to the collective good when it is
    deemed beneficial or necessary to achieve the threshold 'm'. It adapts by punishing insufficient
    cooperation and by initiating renewed cooperation after collective failures, thereby aiming to
    stabilize the cooperative outcome.

    Decision Rules:
    - Round 0 (First Round): Always Cooperate (C) to initiate trust.
    - Last Round (round_number == n_rounds - 1): Always Defect (D) due to the "end-game effect."
    - Intermediate Rounds (Adaptive Phase):
        - If the collective threshold 'm' was met in the previous round: Continue Cooperating (C).
        - If the collective threshold 'm' was NOT met in the previous round:
            - If I cooperated in the previous round: Defect (D) to signal dissatisfaction and as a punishment.
            - If I defected in the previous round: Cooperate (C) to re-initiate collective effort and rectify my part in the failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_prev_action_was_C = history.my_actions[prev_round_idx]
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev = int(my_prev_action_was_C) + opponent_cooperators_prev
        if total_cooperators_prev >= m:
            return Action.C
        elif my_prev_action_was_C:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_357(BaseStrategy):
    """
    The Adaptive Collective Consensus (ACC) strategy aims to achieve and sustain the collective
    benefit (securing the 'k' reward) by promoting cooperation. It starts cooperatively, adapts
    its behavior based on the collective outcome of the previous round, and provides mechanisms
    for both rewarding success and penalizing failure, ultimately striving to re-establish
    cooperation after breakdowns. It also accounts for the endgame effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        my_action_prev_is_C = history.my_actions[-1]
        if cooperators_prev_round >= m_threshold:
            return Action.C
        elif my_action_prev_is_C:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_358(BaseStrategy):
    """
    The Adaptive Reciprocity with Forgiveness (ARF) strategy for the Collective Risk Dilemma.
    It initiates cooperation, sustains it when the collective threshold 'm' is met,
    and is forgiving of a single collective failure. However, it will defect
    if the group fails to meet the threshold for a second consecutive round,
    acting as a punishment, and immediately switches back to cooperation
    as soon as the threshold is met again.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds = 0
        self.forgiveness_threshold = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_prev_action_is_cooperate = history.my_actions[-1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        c_prev = int(my_prev_action_is_cooperate) + int(num_opponent_cooperators_prev_round)
        if c_prev >= self.game_description.m:
            self.consecutive_failure_rounds = 0
            return Action.C
        else:
            self.consecutive_failure_rounds += 1
            if self.consecutive_failure_rounds <= self.forgiveness_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_359(BaseStrategy):
    """
    The Adaptive Threshold Enforcer strategy for the Collective Risk Dilemma.

    This strategy aims to promote collective success by starting cooperatively and
    sustaining cooperation when the group successfully meets the cooperation threshold
    in the previous round. It withdraws cooperation (defects) to punish or protect
    itself when the collective goal fails, signaling a need for change. In the final
    round, it prioritizes self-preservation by always defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Threshold Enforcer strategy.

        Args:
            game_description: An object containing the game parameters,
                              including n_players, n_rounds, m (threshold), and k (reward factor).
        """
        self.game_description = game_description
        self.m_threshold = game_description.m
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Threshold Enforcer strategy rules.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object containing past actions and payoffs
                     for this player and opponents. None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_in_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_in_previous_round >= self.m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_360(BaseStrategy):
    """
    Adaptive Commitment to Threshold (ACT) strategy for the Collective Risk Dilemma.

    ACT aims to consistently secure the collective reward 'k' by promoting stable
    cooperation, adapting to collective failures, and preventing exploitation,
    while remaining robust in a tournament setting.

    It starts with cooperation to establish trust, then conditionally cooperates
    if the collective threshold was met in the previous round. If the threshold
    was not met, it adapts: defecting if it cooperated previously (to signal
    conditional cooperation), or cooperating if it defected previously (to
    rectify its own contribution to failure). In the last round, it defects
    to prevent exploitation (end-game effect).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        action_to_take: Action
        if state.round_number == 0:
            action_to_take = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            action_to_take = Action.D
        else:
            c_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
            m_threshold = self.game_description.m
            if c_prev >= m_threshold:
                action_to_take = Action.C
            elif self.my_last_action == Action.C:
                action_to_take = Action.D
            elif self.my_last_action == Action.D:
                action_to_take = Action.C
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_COLLECTIVE_361(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy aims to foster and sustain collective cooperation
    by initially contributing, being forgiving of occasional failures, but also protecting itself
    from persistent exploitation or chronic group underperformance. It embodies a collective mindset
    by prioritizing the achievement of the threshold `m` but with a self-preservation mechanism.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.consecutive_group_failures: int = 0
        self.my_cooperations_in_successful_rounds: int = 0
        self.FORGIVENESS_THRESHOLD: int = 2
        self.MAX_EXPLOITATION_TOLERANCE: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        if state.round_number == 0:
            current_action = Action.C
        else:
            my_prev_action_actual: np.bool_ = history.my_actions[-1]
            C_prev: int = int(my_prev_action_actual) + np.sum(history.opponent_actions[-1, :])
            if C_prev >= self.game_description.m:
                self.consecutive_group_failures = 0
            else:
                self.consecutive_group_failures += 1
            if C_prev >= self.game_description.m:
                if my_prev_action_actual:
                    self.my_cooperations_in_successful_rounds += 1
                else:
                    self.my_cooperations_in_successful_rounds = 0
            else:
                self.my_cooperations_in_successful_rounds = 0
            if self.consecutive_group_failures >= self.FORGIVENESS_THRESHOLD:
                current_action = Action.D
            elif self.my_cooperations_in_successful_rounds >= self.MAX_EXPLOITATION_TOLERANCE:
                current_action = Action.D
            else:
                current_action = Action.C
        self.my_last_action = current_action
        return current_action

class Strategy_COLLECTIVE_138(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain a cooperative equilibrium by exhibiting
    conditional cooperation, forgiveness, and measured retaliation. It prioritizes securing
    the collective reward `k` but also protects its own endowment against persistent
    exploitation.

    Core Principles:
    1. Initiation: Start cooperatively to signal willingness.
    2. Reinforcement: Continue cooperating when the collective successfully meets the threshold.
    3. Forgiveness: Allow for a single collective failure before resorting to punishment.
    4. Protection: Defect if the collective consistently fails to protect individual endowment.
    5. Endgame Rationality: Revert to individual payoff maximization in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_round_my_action_is_C = history.my_actions[-1]
        c_t_minus_1 = prev_round_opponent_cooperators + (1 if prev_round_my_action_is_C else 0)
        if c_t_minus_1 >= self.game_description.m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_192(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    The ACC strategy aims to establish and maintain the collective good (achieving the 'k' bonus for all players)
    by starting cooperatively, rewarding successful cooperation, punishing collective failure, and deterring
    persistent free-riding. It balances individual protection with collective benefit.

    Key principles:
    - Initial cooperation to signal willingness for collective good.
    - Defection in the final round to maximize individual payoff when no future interactions exist.
    - Continue cooperating if all players cooperate, reinforcing ideal collective outcome.
    - Tolerate a fixed amount of exploitation (my cooperation while others free-ride and threshold is met)
      before defecting to punish persistent free-riders.
    - Return to cooperation after a round of free-riding to stabilize collective equilibrium.
    - Defect to punish collective failure (threshold not met), signaling dissatisfaction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_consecutive_cooperations_under_exploitation: int = 0
        self._exploitation_tolerance_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self._my_consecutive_cooperations_under_exploitation = 0
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        my_action_prev_was_C = history.my_actions[-1]
        C_count_prev = prev_round_opponent_cooperators + (1 if my_action_prev_was_C else 0)
        if C_count_prev >= m:
            if C_count_prev == n:
                self._my_consecutive_cooperations_under_exploitation = 0
                return Action.C
            elif my_action_prev_was_C:
                self._my_consecutive_cooperations_under_exploitation += 1
                if self._my_consecutive_cooperations_under_exploitation > self._exploitation_tolerance_threshold:
                    self._my_consecutive_cooperations_under_exploitation = 0
                    return Action.D
                else:
                    return Action.C
            else:
                self._my_consecutive_cooperations_under_exploitation = 0
                return Action.C
        else:
            self._my_consecutive_cooperations_under_exploitation = 0
            return Action.D

class Strategy_COLLECTIVE_362(BaseStrategy):
    """
    Threshold Enforcement with Forgiveness (TEF) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by initiating with cooperation,
    rewarding collective success, forgiving a single collective failure,
    and punishing persistent failures to meet the cooperation threshold.
    It defects in the final round due to the standard endgame effect in game theory.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            self.failure_streak = 0
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= m_threshold:
            self.failure_streak = 0
        else:
            self.failure_streak += 1
        if self.failure_streak == 0:
            return Action.C
        elif self.failure_streak == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_363(BaseStrategy):
    """
    The Adaptive Collective Effort (ACE) strategy for the Collective Risk Dilemma.
    It starts cooperatively, maintains cooperation if the group meets or nearly meets the
    cooperation threshold, and temporarily defects if the collective effort significantly fails,
    aiming to signal the need for greater commitment. In the final round, it defects to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_rounds = game_description.n_rounds
        self.min_cooperators_m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.total_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_action_prev_round_is_cooperate = history.my_actions[prev_round_idx]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_last_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_is_cooperate else 0)
        if num_cooperators_last_round >= self.min_cooperators_m:
            return Action.C
        elif num_cooperators_last_round >= self.min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_364(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy aims to foster collective cooperation
    to achieve a threshold reward, while being adaptive to group performance and robust
    against exploitation. It starts by cooperating, reinforces cooperation if the collective
    goal is met, punishes if exploited, and proactively attempts to re-initiate cooperation
    if it free-rode during a collective failure. In the last round, it defects for self-preservation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_round = history.my_actions[prev_round_idx]
        cooperators_in_prev_round = int(my_action_prev_round) + np.sum(history.opponent_actions[prev_round_idx, :])
        if cooperators_in_prev_round >= self.m:
            return Action.C
        elif my_action_prev_round:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_365(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Forgiveness (ACRF) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize collective payoff by promoting cooperation,
    punishing collective failures, and offering mechanisms for recovery.

    Key Principles:
    1.  Initiate Cooperation: Starts by cooperating to establish a cooperative baseline.
    2.  Maintain Success: Continues to cooperate when the collective goal (m cooperators) is met.
    3.  Punish Failure: Defects when the collective goal is not met, signaling disapproval.
    4.  Forgive and Probe: Periodically attempts to re-initiate cooperation after failures
        to prevent getting stuck in a perpetual defection cycle.
    5.  Last Round Prudence: Defects in the final round due to backward induction.

    Strategy Parameter:
    FORGIVENESS_INTERVAL (F): An integer representing how many consecutive collective
                              failures must occur before the strategy attempts to
                              cooperate again (probes for re-cooperation).
                              A value of F=2 means it punishes once, then tries to cooperate.
    """
    FORGIVENESS_INTERVAL = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACRF strategy with game parameters and internal state.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs, or None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        prev_round_my_action_was_C = history.my_actions[prev_round_index]
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[prev_round_index, :])
        total_cooperators_prev_round = int(prev_round_my_action_was_C) + prev_round_opponent_cooperators
        if total_cooperators_prev_round >= self.game_description.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures % self.FORGIVENESS_INTERVAL == 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_366(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation by:
    1.  Starting cooperatively to initiate collective benefit.
    2.  Punishing collective failure to meet the cooperation threshold by defecting.
    3.  Maintaining collective success by self-correcting (cooperating if previously
        free-rode successfully) and consistently cooperating if it contributed to success.
    4.  Defecting in the final round due to the absence of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_prev_action = history.my_actions[prev_round_idx]
        prev_round_cooperators = np.sum(history.opponent_actions[prev_round_idx, :]) + my_prev_action
        if prev_round_cooperators < min_cooperators_needed:
            return Action.D
        elif not my_prev_action:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_367(BaseStrategy):
    """
    Adaptive Collective Contribution with Peer Monitoring (ACC-PM) strategy.
    This strategy prioritizes the consistent achievement of the collective reward 'k'.
    It starts by signaling willingness to cooperate, then dynamically adjusts its cooperation propensity
    based on past collective success or failure. It also identifies and responds to the general
    cooperativeness of other players, particularly when collective action is uncertain,
    while incorporating elements of strategic defection to avoid exploitation and to "shock"
    the system back towards cooperation when needed, especially considering end-game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.P_my_cooperate_propensity: float = 1.0
        self.player_coop_scores: list[float] = [0.5] * self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        num_cooperators_prev_round = 0
        if history.my_actions[-1]:
            self.player_coop_scores[0] = self.player_coop_scores[0] * 0.9 + 0.1
            num_cooperators_prev_round += 1
        else:
            self.player_coop_scores[0] = self.player_coop_scores[0] * 0.9 + 0.0
        for opp_idx, opp_action_bool in enumerate(history.opponent_actions[-1, :]):
            internal_player_score_idx = opp_idx + 1
            if opp_action_bool:
                self.player_coop_scores[internal_player_score_idx] = self.player_coop_scores[internal_player_score_idx] * 0.9 + 0.1
                num_cooperators_prev_round += 1
            else:
                self.player_coop_scores[internal_player_score_idx] = self.player_coop_scores[internal_player_score_idx] * 0.9 + 0.0
        if num_cooperators_prev_round >= self.m:
            self.P_my_cooperate_propensity = min(1.0, self.P_my_cooperate_propensity + 0.1)
        else:
            self.P_my_cooperate_propensity = max(0.0, self.P_my_cooperate_propensity - 0.2)
        if current_round_t == self.r:
            return Action.D
        my_action = Action.D
        if self.P_my_cooperate_propensity >= 0.75:
            my_action = Action.C
        elif self.P_my_cooperate_propensity < 0.25:
            my_action = Action.D
        else:
            expected_cooperators_from_others = 0.0
            for i in range(1, self.n):
                expected_cooperators_from_others += self.player_coop_scores[i]
            if expected_cooperators_from_others + 1 >= self.m:
                my_action = Action.C
            else:
                my_action = Action.D
        return my_action

class Strategy_COLLECTIVE_368(BaseStrategy):
    """
    Dynamic Burden-Sharing with Forgiveness strategy for the Collective Risk Dilemma.

    This strategy aims to maximize collective payoff by adaptively adjusting cooperation
    based on the group's past success/failure in meeting the 'm' threshold and the
    prevalence of free-riding. It punishes persistent failures and tests free-riders
    to encourage more equitable burden sharing, with a focus on returning to cooperation
    for long-term collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.consecutive_successes_as_cooperator_with_free_riders: int = 0
        self.failure_punish_threshold: int = 2
        self.free_rider_test_threshold: int = 2

    def _get_total_cooperators_in_round(self, round_index_in_history: int, history: PlayerHistory) -> int:
        """
        Calculates the total number of cooperators in a given historical round,
        including this player's action.
        """
        my_action_bool = history.my_actions[round_index_in_history]
        opponent_cooperators = np.sum(history.opponent_actions[round_index_in_history, :])
        return int(opponent_cooperators + (1 if my_action_bool else 0))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        prev_round_index_in_history = state.round_number - 1
        prev_round_cooperators = self._get_total_cooperators_in_round(prev_round_index_in_history, history)
        my_action_prev_round_bool = history.my_actions[prev_round_index_in_history]
        my_action_prev_round = Action.C if my_action_prev_round_bool else Action.D
        if current_round_1_indexed == total_rounds:
            if prev_round_cooperators < m_threshold:
                return Action.D
            else:
                return Action.C
        if prev_round_cooperators < m_threshold:
            self.consecutive_failures += 1
            self.consecutive_successes_as_cooperator_with_free_riders = 0
            if self.consecutive_failures > self.failure_punish_threshold:
                return Action.D
            else:
                return Action.C
        else:
            self.consecutive_failures = 0
            if my_action_prev_round == Action.C:
                num_defectors = n_players - prev_round_cooperators
                if num_defectors >= 1:
                    self.consecutive_successes_as_cooperator_with_free_riders += 1
                    if self.consecutive_successes_as_cooperator_with_free_riders > self.free_rider_test_threshold:
                        return Action.D
                    else:
                        return Action.C
                else:
                    self.consecutive_successes_as_cooperator_with_free_riders = 0
                    return Action.C
            elif my_action_prev_round == Action.D:
                self.consecutive_successes_as_cooperator_with_free_riders = 0
                return Action.C

class Strategy_COLLECTIVE_369(BaseStrategy):
    """
    The Adaptive Community Contributor (ACC) strategy for the Collective Risk Dilemma.
    It prioritizes collective cooperation by being initially cooperative, tolerating
    some collective failures, punishing persistent non-cooperation with temporary
    defection, and quickly returning to cooperation when the collective shows signs
    of improvement. It aims to pull the group towards optimal outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.TOLERANCE_LEVEL = 2
        self.RECONCILIATION_ROUNDS = 1
        self.consecutive_failures = 0
        self.my_state_category = 'COOPERATING'
        self.punish_rounds_elapsed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        threshold_met_prev_round = cooperators_in_prev_round >= self.game_description.m
        if threshold_met_prev_round:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.my_state_category == 'COOPERATING':
            if self.consecutive_failures >= self.TOLERANCE_LEVEL and state.round_number < self.game_description.n_rounds - 1:
                self.my_state_category = 'PUNISHING'
                self.punish_rounds_elapsed = 0
                return Action.D
            else:
                return Action.C
        elif self.my_state_category == 'PUNISHING':
            if threshold_met_prev_round:
                self.my_state_category = 'COOPERATING'
                self.punish_rounds_elapsed = 0
                return Action.C
            elif self.punish_rounds_elapsed >= self.RECONCILIATION_ROUNDS:
                self.my_state_category = 'COOPERATING'
                self.punish_rounds_elapsed = 0
                return Action.C
            else:
                self.punish_rounds_elapsed += 1
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_370(BaseStrategy):
    """
    Adaptive Trigger for Collective Benefit (ATCB) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize collective well-being by ensuring the 'm' threshold
    for cooperation is met. It uses a clear punishment mechanism (defecting) when the
    collective threshold is missed and an adaptive 'switch if successful' mechanism
    to manage individual contributions when the threshold is met. This promotes
    distributed costs of cooperation and prevents sustained free-riding or exploitation.
    Special handling is provided for the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_cooperators_prev = int(my_action_prev) + np.sum(history.opponent_actions[-1, :])
        active_punishment_mode = num_cooperators_prev < m
        if active_punishment_mode:
            return Action.D
        elif my_action_prev == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_371(BaseStrategy):
    """
    The Community Builder strategy for the Collective Risk Dilemma.
    This strategy aims to foster and maintain cooperation by adapting to the group's
    performance in meeting the cooperation threshold. It balances initiating cooperation,
    rewarding success, punishing collective failure, and quickly forgiving to re-establish
    a cooperative equilibrium. It also accounts for the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._current_stance: str = 'MAINTAIN_COOP'
        self._punish_rounds_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == total_rounds:
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_action_prev_was_c = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev = int(opponent_cooperators_prev) + (1 if my_action_prev_was_c else 0)
        threshold_met_prev = num_cooperators_prev >= self.game_description.m
        action_to_take: Action = Action.D
        if self._current_stance == 'MAINTAIN_COOP':
            if threshold_met_prev:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
                self._current_stance = 'PUNISH_GROUP'
                self._punish_rounds_remaining = 1
        elif self._current_stance == 'PUNISH_GROUP':
            self._punish_rounds_remaining -= 1
            if self._punish_rounds_remaining > 0:
                action_to_take = Action.D
            else:
                action_to_take = Action.C
                self._current_stance = 'REBUILD_COOP'
        elif self._current_stance == 'REBUILD_COOP':
            if threshold_met_prev:
                action_to_take = Action.C
                self._current_stance = 'MAINTAIN_COOP'
            else:
                action_to_take = Action.D
                self._current_stance = 'PUNISH_GROUP'
                self._punish_rounds_remaining = 1
        return action_to_take

class Strategy_COLLECTIVE_372(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation with Buffer Management (ARC-BM) strategy for the Collective Risk Dilemma.
    This strategy aims to secure the collective bonus 'k' by ensuring the 'm' cooperator
    threshold is met. It starts cooperatively, adapts behavior based on past success or failure,
    and selectively defects when a robust buffer of cooperators exists to prevent
    sustained exploitation and encourage efficiency.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.
        No explicit internal state for 'my_last_action' or 'observed_cooperators_prev_round'
        is maintained as instance variables, because this information is consistently
        provided via the `history` object in each call to `__call__`.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Reciprocal Cooperation with Buffer Management (ARC-BM) strategy.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): The history of actions and payoffs from
                                            previous rounds. None for the first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        my_current_action: Action
        if current_round_1_indexed == 1:
            my_current_action = Action.C
        elif current_round_1_indexed == total_rounds:
            my_current_action = Action.D
        else:
            my_action_prev_round_bool = history.my_actions[-1]
            observed_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (1 if my_action_prev_round_bool else 0)
            if observed_cooperators_prev_round < min_cooperators_m:
                my_current_action = Action.C
            elif observed_cooperators_prev_round == min_cooperators_m:
                my_current_action = Action.C
            else:
                my_current_action = Action.D
        return my_current_action

class Strategy_COLLECTIVE_373(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Forgiveness strategy for the Collective Risk Dilemma.
    This strategy aims to maximize collective welfare by promoting cooperation while being
    robust against exploitation. It balances the need for individual protection with a collective
    mindset by adapting its behavior based on the group's past actions and periodically
    attempting to re-establish cooperation after failures.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_last_success: int = 0
        self.cooperation_test_interval: int = 4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds_total_1_indexed = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed > 1:
            cooperators_in_prev_round = sum(history.opponent_actions[-1, :])
            if history.my_actions[-1]:
                cooperators_in_prev_round += 1
            if cooperators_in_prev_round >= m_threshold:
                self.rounds_since_last_success = 0
            else:
                self.rounds_since_last_success += 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == n_rounds_total_1_indexed:
            return Action.D
        elif self.rounds_since_last_success == 0:
            return Action.C
        elif self.rounds_since_last_success % self.cooperation_test_interval == 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_374(BaseStrategy):
    """
    The Adaptive Collective Threshold Cooperator (ACTC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by starting pro-socially and adapting to the group's
    performance. It tolerates a limited number of collective failures before temporarily defecting
    to protect itself or signal non-sustainability. It is always ready to re-engage in cooperation
    upon collective success and defects in the final round due to the absence of future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_group_failure_count: int = 0
        self.F_max: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        N_C_prev = int(my_prev_action_is_cooperate) + opponent_prev_cooperators
        if N_C_prev >= self.game_description.m:
            self.consecutive_group_failure_count = 0
            return Action.C
        else:
            self.consecutive_group_failure_count += 1
            if self.consecutive_group_failure_count >= self.F_max:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_375(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain a cooperative environment.
    It begins by cooperating to signal good faith. In subsequent rounds,
    it observes if the collective goal (minimum cooperators 'm' met) was achieved
    in the previous round. If met, it continues to cooperate; if not met, it defects
    to signal dissatisfaction and protect its individual endowment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_action_prev_round = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
        if num_cooperators_prev_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_376(BaseStrategy):
    """
    Adaptive Group Steward (AGS) strategy.

    AGS prioritizes achieving the collective benefit by striving to meet the 'm' cooperator threshold.
    It adopts a "start cooperative, reward cooperation, punish persistent defection" approach,
    with a built-in mechanism for forgiveness to allow for collective recovery from minor lapses.
    Its "collective" nature is reflected in its willingness to initiate cooperation and maintain
    it when successful, and its final-round behavior which acknowledges past collective effort.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.MAX_FORGIVING_FAILURES = 1
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        previous_round_N_C = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_round_N_C < m_threshold:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        if state.round_number == n_rounds - 1:
            if previous_round_N_C >= m_threshold:
                return Action.C
            else:
                return Action.D
        if previous_round_N_C >= m_threshold:
            return Action.C
        elif self.consecutive_failures <= self.MAX_FORGIVING_FAILURES:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_377(BaseStrategy):
    """
    Adaptive Reciprocity for Collective Good (ARCG) strategy.

    This strategy aims to establish and maintain collective success by
    optimistically initiating cooperation, sustaining it when successful,
    and using a "punish then re-attempt" mechanism for collective failures.
    It adapts its behavior based on the outcome of the previous round and
    the history of collective success or failure, with specific handling
    for the first and last rounds of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._rounds_failed_threshold_consecutively: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self._rounds_failed_threshold_consecutively = 0
            return Action.C
        cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if current_round_1_indexed == n_rounds:
            if cooperators_prev_round >= m:
                return Action.C
            else:
                return Action.D
        if cooperators_prev_round >= m:
            self._rounds_failed_threshold_consecutively = 0
            return Action.C
        else:
            self._rounds_failed_threshold_consecutively += 1
            if self._rounds_failed_threshold_consecutively == 1:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_378(BaseStrategy):
    """
    Stable Collective Achievement (SCA) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize the collective payoff by consistently achieving the 'k' bonus.
    It employs a reactive, conditional cooperation mechanism coupled with a patience threshold
    for collective failure. It starts with cooperation, maintains it when the collective
    threshold is met, and shows patience for a single round of failure. However,
    it defects to cut individual losses and signal punishment if collective failure
    is sustained for more than one consecutive round. In the final round, it defects
    due to the absence of future incentives.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round == 0:
            self.consecutive_failures = 0
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        n_c_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if n_c_prev >= min_cooperators:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures <= 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_379(BaseStrategy):
    """
    Dynamic Contribution and Reciprocity (DCR) strategy for the Collective Risk Dilemma.
    This strategy aims for consistent collective success by balancing cooperation
    with self-preservation, adapting based on historical outcomes and individual
    contribution patterns.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: None | Action = None
        self.consecutive_failures: int = 0
        self.consecutive_successes_as_C: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_action: Action
        if state.round_number == self.game_description.n_rounds - 1:
            current_round_action = Action.D
            return current_round_action
        if state.round_number == 0:
            current_round_action = Action.C
            self.my_last_action = current_round_action
            return current_round_action
        prev_round_idx = state.round_number - 1
        my_action_prev_round_bool = self.my_last_action == Action.C
        opponent_actions_prev_round = history.opponent_actions[prev_round_idx, :]
        num_cooperators_prev_round = int(my_action_prev_round_bool) + np.sum(opponent_actions_prev_round)
        threshold_met_prev_round = num_cooperators_prev_round >= self.game_description.m
        if threshold_met_prev_round:
            self.consecutive_failures = 0
            if my_action_prev_round_bool:
                self.consecutive_successes_as_C += 1
            else:
                self.consecutive_successes_as_C = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes_as_C = 0
        if threshold_met_prev_round:
            if self.my_last_action == Action.D:
                current_round_action = Action.D
            elif self.my_last_action == Action.C:
                if num_cooperators_prev_round - self.game_description.m >= 1 and self.consecutive_successes_as_C >= 2:
                    current_round_action = Action.D
                else:
                    current_round_action = Action.C
        elif self.my_last_action == Action.D:
            current_round_action = Action.C
        elif self.my_last_action == Action.C:
            if self.consecutive_failures >= 2:
                current_round_action = Action.D
            else:
                current_round_action = Action.C
        self.my_last_action = current_round_action
        return current_round_action

class Strategy_COLLECTIVE_380(BaseStrategy):
    """
    Adaptive Threshold Enforcement (ATE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize collective welfare by striving to achieve the community reward 'k' consistently.
    It starts by cooperating to signal willingness and establish a cooperative equilibrium.
    It then monitors the collective's ability to meet the minimum cooperation threshold 'm'.
    It continues to cooperate if the threshold is met or if failures are infrequent, but withdraws cooperation
    (defects) if the collective repeatedly fails to meet the threshold, acting as a form of self-protection
    and a signal for others to increase their contributions.
    The strategy accounts for the unique dynamics of the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0
        self.PATIENCE_THRESHOLD = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_t > 1:
            prev_round_opponent_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
            prev_round_my_action_is_cooperate = history.my_actions[state.round_number - 1]
            prev_round_total_cooperators = prev_round_opponent_cooperators + int(prev_round_my_action_is_cooperate)
            if prev_round_total_cooperators < m_threshold:
                self.consecutive_failures += 1
            else:
                self.consecutive_failures = 0
        if current_round_t == total_rounds:
            return Action.D
        if current_round_t == 1:
            return Action.C
        if self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures <= self.PATIENCE_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_381(BaseStrategy):
    """
    Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    ACE prioritizes achieving the collective reward 'k' by initially cooperating.
    It initiates a temporary defection phase (punishment) if the collective
    fails to meet the cooperation threshold 'm' in the previous round.
    If the threshold is met, it continues to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.P = 1
        self.private_punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_prev_action_cooperated = 1 if history.my_actions[-1] else 0
        opponent_prev_cooperators = sum(history.opponent_actions[-1, :])
        C_prev = my_prev_action_cooperated + opponent_prev_cooperators
        if self.private_punishment_rounds_remaining > 0:
            self.private_punishment_rounds_remaining -= 1
            return Action.D
        elif C_prev >= m_threshold:
            return Action.C
        elif C_prev < m_threshold:
            self.private_punishment_rounds_remaining = self.P
            return Action.D

class Strategy_COLLECTIVE_382(BaseStrategy):
    """
    Adaptive Threshold Cooperation: This strategy aims to foster collective cooperation by
    conditioning its own contribution on the group's success in meeting the required
    threshold for collective benefit (m). It cooperates in the first round to signal
    willingness, and in subsequent rounds, cooperates if the collective threshold (m)
    was met in the previous round, otherwise defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            previous_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
            my_action_in_previous_round_as_int = int(history.my_actions[-1])
            total_cooperators_previous_round = previous_round_opponent_cooperators + my_action_in_previous_round_as_int
            if total_cooperators_previous_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_383(BaseStrategy):
    """
    The Adaptive Reciprocal Coordination (ARC) strategy aims to promote collective action
    in the Collective Risk Dilemma by balancing proactive cooperation, responsiveness to
    collective outcomes, and self-preservation. It maximizes individual payoff by
    securing the collective reward (`k`) through coordinated cooperation, while being
    robust against exploitation and adapting to the observed behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            prev_round_idx = state.round_number - 1
            my_action_in_prev_round_is_C = history.my_actions[prev_round_idx]
            num_cooperators_in_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            if my_action_in_prev_round_is_C:
                num_cooperators_in_prev_round += 1
            prev_round_threshold_met = num_cooperators_in_prev_round >= self.m
            successful_rounds_count = 0
            total_past_rounds = state.round_number
            for r_idx in range(total_past_rounds):
                round_cooperators = np.sum(history.opponent_actions[r_idx, :])
                if history.my_actions[r_idx]:
                    round_cooperators += 1
                if round_cooperators >= self.m:
                    successful_rounds_count += 1
            success_ratio = 0.0
            if total_past_rounds > 0:
                success_ratio = successful_rounds_count / total_past_rounds
            if prev_round_threshold_met:
                return Action.C
            elif my_action_in_prev_round_is_C:
                return Action.D
            elif num_cooperators_in_prev_round >= self.m - 1:
                return Action.C
            elif success_ratio >= 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_384(BaseStrategy):
    """
    The Adaptive Collective Guardian (ACG) strategy for the Collective Risk Dilemma.

    This strategy balances individual incentives with the collective benefit by
    prioritizing meeting the cooperation threshold 'm'. It employs conditional
    cooperation, historical analysis, and a mechanism for collective punishment
    and forgiveness to adapt to opponent behaviors and encourage sustained cooperation.

    It strives to secure the collective reward (k) by cooperating, but becomes
    defensive if the collective threshold is consistently missed, initiating
    a temporary punishment phase before attempting to re-establish cooperation.
    """
    PATIENCE_THRESHOLD_MISS: int = 2
    FORGIVENESS_COOLDOWN: int = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_threshold_misses: int = 0
        self._in_punishment_mode: bool = False
        self._punishment_round_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self._consecutive_threshold_misses = 0
            self._in_punishment_mode = False
            self._punishment_round_count = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self._in_punishment_mode:
            self._punishment_round_count += 1
            if self._punishment_round_count < self.FORGIVENESS_COOLDOWN:
                return Action.D
            else:
                self._in_punishment_mode = False
                self._punishment_round_count = 0
        C_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if C_prev >= self.game_description.m:
            self._consecutive_threshold_misses = 0
            return Action.C
        else:
            self._consecutive_threshold_misses += 1
            if self._consecutive_threshold_misses >= self.PATIENCE_THRESHOLD_MISS:
                self._in_punishment_mode = True
                self._punishment_round_count = 0
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_385(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to proactively initiate cooperation, sustain it when successful,
    and adaptively respond to failures while signaling the need for collective effort.
    It balances collective benefit with self-preservation by maintaining a 'persistent
    but not naive cooperation' philosophy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.X_failure_tolerance: int = 2
        self.Z_persistence_threshold: int = max(1, math.floor(self.game_description.m / 3))
        self.my_consecutive_failures_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
            if num_cooperators_prev_round >= self.game_description.m:
                self.my_consecutive_failures_count = 0
                return Action.C
            else:
                self.my_consecutive_failures_count += 1
                if self.my_consecutive_failures_count >= self.X_failure_tolerance or num_cooperators_prev_round < self.Z_persistence_threshold:
                    return Action.D
                else:
                    return Action.C

class Strategy_COLLECTIVE_386(BaseStrategy):
    """
    Adaptive Trigger with Proportional Response (ATPR) strategy for Collective Risk Dilemma.
    This strategy aims to navigate the tension between individual rationality and collective
    welfare by promoting collective success while protecting against consistent exploitation
    or failure. It is adaptive and robust, capable of playing effectively against a wide
    range of opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_consecutive_cooperations_when_exploited: int = 0
        self.consecutive_failures_observed: int = 0
        self.MAX_EXPLOITATION_TOLERANCE: int = 2
        self.MAX_FAILURE_TOLERANCE: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev = num_opponent_cooperators_prev + (1 if my_action_prev else 0)
        threshold_met_prev = num_cooperators_prev >= m_threshold
        if threshold_met_prev:
            self.consecutive_failures_observed = 0
            if my_action_prev:
                if num_cooperators_prev > m_threshold:
                    self.my_consecutive_cooperations_when_exploited += 1
                else:
                    self.my_consecutive_cooperations_when_exploited = 0
            else:
                self.my_consecutive_cooperations_when_exploited = 0
        else:
            self.consecutive_failures_observed += 1
            self.my_consecutive_cooperations_when_exploited = 0
        if self.consecutive_failures_observed > self.MAX_FAILURE_TOLERANCE:
            return Action.D
        if my_action_prev and threshold_met_prev and (num_cooperators_prev > m_threshold) and (self.my_consecutive_cooperations_when_exploited > self.MAX_EXPLOITATION_TOLERANCE):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_387(BaseStrategy):
    """
    The Adaptive Threshold Cooperator (ATC) strategy for the Collective Risk Dilemma.
    ATC leads with cooperation, sustains successful collective action, tolerates
    minor failures, and punishes persistent non-cooperation by temporarily defecting.
    It defects in the final round to avoid exploitation and prevent futile sacrifice
    when no one else cooperates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k
        self.consecutive_failure_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1] == True:
            cooperators_last_round += 1
        if cooperators_last_round >= self.m_threshold:
            self.consecutive_failure_rounds = 0
            return Action.C
        else:
            self.consecutive_failure_rounds += 1
            if self.consecutive_failure_rounds >= 3:
                return Action.D
            elif self.consecutive_failure_rounds == 1:
                if cooperators_last_round == 0:
                    return Action.D
                else:
                    return Action.C
            elif self.consecutive_failure_rounds == 2:
                return Action.C
        return Action.C

class Strategy_COLLECTIVE_388(BaseStrategy):
    """
    The Adaptive Collective Assurance (ACA) strategy promotes sustained collective success
    in the Collective Risk Dilemma. It prioritizes meeting the 'm' cooperator threshold
    to secure the 'k' bonus. It uses conditional defection to discourage persistent
    free-riding and maintain fairness, but critically prioritizes re-establishing
    cooperation if the collective fails to meet the threshold.

    Key features:
    - Starts with cooperation.
    - Updates a floating-point 'defection score' for each opponent based on their
      defections specifically when the collective threshold was met. This score decays
      with cooperation.
    - Resets all defection scores to zero upon collective failure, emphasizing
      re-establishment of cooperation.
    - Triggers defection if too many opponents are identified as persistent free-riders.
    - Triggers defection if the player was a critical cooperator (threshold 'm' barely met)
      while others free-rode.
    - In the final round, adopts a rational defection strategy if the threshold was met,
      or a pivotal cooperation strategy if its cooperation can secure the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.FRT = 3.0
        self.FREE_RIDER_RATIO_TOLERANCE = 0.5
        self.SCORE_DECAY_FACTOR = 0.5
        self.my_cooperation_count = 0
        self.others_defection_score = [0.0] * (self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        my_action = Action.C
        if current_round_0_indexed == 0:
            my_action = Action.C
        else:
            prev_round_0_indexed = current_round_0_indexed - 1
            my_action_prev_round_bool = history.my_actions[prev_round_0_indexed]
            opponent_actions_prev_round_bool = history.opponent_actions[prev_round_0_indexed, :]
            num_cooperators_prev_round = np.sum(opponent_actions_prev_round_bool).item()
            if my_action_prev_round_bool:
                num_cooperators_prev_round += 1
            if num_cooperators_prev_round >= self.m:
                for j_idx, action_j_bool in enumerate(opponent_actions_prev_round_bool):
                    if not action_j_bool:
                        self.others_defection_score[j_idx] += 1
                    else:
                        self.others_defection_score[j_idx] = max(0.0, self.others_defection_score[j_idx] - self.SCORE_DECAY_FACTOR)
            else:
                for j_idx in range(len(self.others_defection_score)):
                    self.others_defection_score[j_idx] = 0.0
            if current_round_1_indexed == self.n_rounds:
                if num_cooperators_prev_round >= self.m:
                    my_action = Action.D
                elif num_cooperators_prev_round == self.m - 1:
                    my_action = Action.C
                else:
                    my_action = Action.D
            elif num_cooperators_prev_round < self.m:
                my_action = Action.C
            else:
                my_action = Action.C
                trigger_defection = False
                num_high_score_defectors = 0
                for score in self.others_defection_score:
                    if score >= self.FRT:
                        num_high_score_defectors += 1
                if num_high_score_defectors > (self.n_players - self.m) * self.FREE_RIDER_RATIO_TOLERANCE:
                    trigger_defection = True
                if num_cooperators_prev_round == self.m and my_action_prev_round_bool:
                    trigger_defection = True
                if trigger_defection:
                    my_action = Action.D
        if my_action == Action.C:
            self.my_cooperation_count += 1
        return my_action

class Strategy_COLLECTIVE_389(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) Strategy for the Collective Risk Dilemma.
    This strategy conditionally cooperates, aiming to foster collective success while
    protecting individual interests within a tournament setting.

    - **First Round (t=1 / round_number=0):** Initiates cooperation (C) as an exploratory move
      to signal willingness to contribute and gather initial data on the group's propensity
      to cooperate.
    - **Intermediate Rounds (1 < t < r / 0 < round_number < r-1):** Observes the outcome
      of the immediately preceding round.
      - If the minimum cooperation threshold (m) was met (`N_C_t-1 >= m`), it cooperates (C)
        to reinforce this positive collective outcome.
      - If the threshold was NOT met (`N_C_t-1 < m`), it defects (D) for self-preservation
        and to signal dissatisfaction, potentially incentivizing higher cooperation in future rounds.
    - **Last Round (t=r / round_number=r-1):** Defects (D) based on the backward induction
      principle, as there are no future interactions to influence, thereby securing the
      highest possible individual payoff for that round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        total_cooperators_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if total_cooperators_prev_round >= min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_390(BaseStrategy):
    """
    The "Dynamic Collective Enforcer" (DCE) strategy is designed to promote and
    sustain cooperation in the Collective Risk Dilemma by prioritizing the
    collective good while being robust against exploitation and learning from
    past outcomes. It aims to achieve the `k` reward for all players as
    consistently as possible.

    It begins by cooperating and continues to cooperate after successful rounds.
    It forgives a single consecutive failure to meet the cooperation threshold,
    giving the group a second chance. However, if cooperation consistently fails
    for two or more consecutive rounds, it defects as a strong signal (or
    punishment) and to protect its individual payoff. In the final round, it
    acts purely out of self-interest and defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number + 1
        if current_game_round > 1:
            prev_round_my_action_bool = history.my_actions[state.round_number - 1]
            prev_round_opponent_actions_bool = history.opponent_actions[state.round_number - 1, :]
            num_cooperators_prev_round = np.sum(prev_round_opponent_actions_bool) + int(prev_round_my_action_bool)
            if num_cooperators_prev_round >= self.m:
                self.consecutive_failures = 0
            else:
                self.consecutive_failures += 1
        if current_game_round == self.n_rounds:
            return Action.D
        if current_game_round == 1:
            return Action.C
        if self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_391(BaseStrategy):
    """
    Adaptive Forgiving Threshold (AFT) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and sustain collective cooperation to consistently
    meet the `m` threshold and secure the `k` reward for all players. It starts by
    signaling cooperation, maintains cooperation if successful, and offers a limited
    number of "forgiving" attempts after a collective failure before resorting to
    self-preservation through defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Forgiving Threshold strategy.

        Args:
            game_description: An object containing game parameters such as n_players,
                              n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description
        self.F = 2
        self.consecutive_failures_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on
        the Adaptive Forgiving Threshold strategy rules.

        Args:
            state: The current state of the game, including the current round number (0-indexed).
            history: A record of actions and payoffs from previous rounds for all players.
                     This will be None for the very first round (state.round_number == 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            self.consecutive_failures_count = 0
            return Action.D
        else:
            my_prev_action_value = int(history.my_actions[-1])
            opponents_prev_cooperators = np.sum(history.opponent_actions[-1, :])
            N_prev_C = my_prev_action_value + int(opponents_prev_cooperators)
            if N_prev_C >= m_threshold:
                self.consecutive_failures_count = 0
                return Action.C
            else:
                self.consecutive_failures_count += 1
                if self.consecutive_failures_count >= self.F:
                    return Action.D
                else:
                    return Action.C

class Strategy_COLLECTIVE_392(BaseStrategy):
    """
    Adaptive Threshold Enforcer strategy for the Collective Risk Dilemma.
    This strategy aims to promote cooperation by proactively cooperating,
    conditionally supporting collective success, and temporarily punishing
    failures to meet the cooperation threshold. It resets punishment quickly
    and defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_duration = 1
        self.current_punishment_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed > 1:
            observed_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]).item() + history.my_actions[-1].item()
            if observed_cooperators_previous_round < m:
                if self.current_punishment_countdown == 0:
                    self.current_punishment_countdown = self.punishment_duration
            else:
                self.current_punishment_countdown = 0
        if current_round_1_indexed == total_rounds:
            return Action.D
        elif self.current_punishment_countdown > 0:
            self.current_punishment_countdown -= 1
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_393(BaseStrategy):
    """
    Forgiving Success Maintainer (FSM) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and sustain collective cooperation to consistently meet
    the 'm' threshold, thus securing the 'k' reward for all players. It is designed to
    be robust by tolerating minor setbacks, adapting to persistent failures, and
    acknowledging individual incentives in the endgame, all while prioritizing the
    collective good.

    Core Principles:
    1. Optimistic Start: Begin by cooperating.
    2. Success Maintenance: Continue cooperating if the threshold was met previously.
    3. Forgiving Adaptation: Tolerate a few consecutive failures while cooperating.
    4. Responsive Defection: Defect if failures persist despite cooperative attempts.
    5. Re-initiation: After a period of reactive defection, attempt cooperation again.
    6. Endgame Rationality: Defect in the final round to maximize private payoff.
    """
    FORGIVENESS_THRESHOLD: int = 2
    "\n    The number of consecutive rounds the strategy will tolerate collective failure\n    (i.e., fewer than 'm' cooperators) while this player continues to cooperate,\n    before switching to defect.\n    "
    RETRY_COOPERATE_AFTER_DEFECTION_ROUNDS: int = 1
    '\n    The number of rounds the strategy will defect after entering a defection state\n    (due to prolonged collective failure) before attempting to cooperate again.\n    '

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_last_collective_success: int = 0
        '\n        An integer, counting consecutive rounds where `num_cooperators_prev_round < m`.\n        Resets to 0 if `num_cooperators_prev_round >= m`. Initialized to 0.\n        '
        self.my_current_defection_streak: int = 0
        '\n        An integer, counting consecutive rounds this player has chosen to Defect (D).\n        Resets to 0 if this player chooses to Cooperate (C). Initialized to 0.\n        '

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        prev_round_index = state.round_number - 1
        my_last_action_was_cooperate = history.my_actions[prev_round_index]
        num_opponent_cooperators_prev_round = int(sum(history.opponent_actions[prev_round_index, :]))
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_last_action_was_cooperate)
        if num_total_cooperators_prev_round >= m_threshold:
            self.rounds_since_last_collective_success = 0
        else:
            self.rounds_since_last_collective_success += 1
        if not my_last_action_was_cooperate:
            self.my_current_defection_streak += 1
        else:
            self.my_current_defection_streak = 0
        if state.round_number == total_rounds - 1:
            return Action.D
        if num_total_cooperators_prev_round >= m_threshold:
            return Action.C
        if self.my_current_defection_streak > 0:
            if self.my_current_defection_streak < self.RETRY_COOPERATE_AFTER_DEFECTION_ROUNDS:
                return Action.D
            else:
                return Action.C
        if self.rounds_since_last_collective_success <= self.FORGIVENESS_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_394(BaseStrategy):
    """
    The Adaptive Collective Trust (ACT) strategy for the Collective Risk Dilemma.
    It initiates with trust, sustains cooperation when successful, punishes exploitation,
    and attempts proactive leadership by cooperating if its action would have met the threshold.
    It employs end-game pragmatism based on recent collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        my_previous_action_was_C = history.my_actions[-1]
        cooperators_among_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        C_prev = cooperators_among_opponents_prev_round + (1 if my_previous_action_was_C else 0)
        if 1 < current_round_t < self.n_rounds:
            if C_prev >= self.m:
                return Action.C
            elif my_previous_action_was_C:
                return Action.D
            elif C_prev + 1 >= self.m:
                return Action.C
            else:
                return Action.D
        elif current_round_t == self.n_rounds:
            if C_prev >= self.m and my_previous_action_was_C:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_395(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by consistently achieving the collective
    reward 'k' (by ensuring at least 'm' cooperators) while adapting to opponent behavior
    and optimizing its contribution. It prioritizes collective success but seeks efficiency
    in individual contributions.

    Decision rules are based on the immediately preceding round's outcome:
    - First round: Always Cooperate (C) to establish a baseline.
    - Last round: Always Defect (D) due to terminal rationality.
    - Intermediate rounds:
        - If the collective project succeeded (C_prev >= m):
            - If there was excess cooperation (C_prev > m): Defect (D) to test for efficiency.
            - If cooperation was exact (C_prev == m): Cooperate (C) to maintain success.
        - If the collective project failed (C_prev < m): Cooperate (C) to contribute to success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        No additional internal state beyond game_description is strictly needed
        as decisions are based on the latest round's history provided in __call__.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action ('C' or 'D') for the current round based on game state and history.

        Args:
            state (GameState): The current round number (0-indexed).
            history (None | PlayerHistory): Past actions and payoffs, or None for the first round.

        Returns:
            Action: Action.C for Cooperate, or Action.D for Defect.
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_action_prev_round = history.my_actions[state.round_number - 1]
        opponent_cooperation_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        C_prev = int(my_action_prev_round) + int(opponent_cooperation_prev_round)
        m = self.game_description.m
        if C_prev >= m:
            if C_prev > m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_396(BaseStrategy):
    """
    The "Adaptive Threshold Enforcer (ATE)" strategy aims to achieve and maintain the
    collective reward 'k' by promoting cooperation, adapting to observed collective
    outcomes, and deterring persistent free-riding. It balances a collective mindset
    with robustness against various opponent behaviors, operating entirely on game
    parameters and observed history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_round_my_action_was_C = history.my_actions[-1]
        prev_total_cooperators = prev_round_opponent_cooperators + (1 if prev_round_my_action_was_C else 0)
        if prev_total_cooperators >= self.game_description.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_total_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        if prev_total_cooperators >= self.game_description.m:
            return Action.C
        elif self.consecutive_failures <= 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_397(BaseStrategy):
    """
    Adaptive Collective Guardian (ACG) strategy.

    This strategy aims to promote and sustain collective cooperation while being robust
    against various opponent behaviors, including exploitation. It balances the collective
    good with self-preservation, adapting its stance based on the observed success or
    failure of the collective effort to meet the cooperation threshold.

    Decision Rules:
    1.  First Round (t=1): Always Cooperate (C) to initiate collective effort.
    2.  Last Round (t=r): Always Defect (D) to prevent exploitation when the "shadow of the future" disappears.
    3.  Intermediate Rounds (1 < t < r):
        *   If the collective met the threshold (m) in the previous round: Cooperate (C).
            Reset `consecutive_failures` to 0.
        *   If the collective failed to meet the threshold (m) in the previous round:
            *   Increment `consecutive_failures`.
            *   If `consecutive_failures` is 1 (first consecutive failure): Cooperate (C)
                (forgiveness, giving the collective another chance).
            *   If `consecutive_failures` is > 1 (two or more consecutive failures): Defect (D)
                (punishment, to avoid exploitation and signal unsustainable non-cooperation).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        cooperators_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if cooperators_prev_round >= m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_399(BaseStrategy):
    """
    Collective Guardian with Punishment and Forgiveness (CGPF) strategy for the Collective Risk Dilemma.

    This strategy aims to consistently ensure the collective threshold 'm' for reward 'k' is met.
    It's forgiving of initial collective failure, but punishes persistent non-cooperation.
    It also strategically free-rides to share the burden when cooperation is abundant.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.collective_failure_streak = 0
        self.round_count_since_i_last_defected = 0
        self.free_ride_coop_threshold = math.ceil(self.game_description.n_players / self.game_description.m)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_action_in_prev_round_bool = history.my_actions[-1]
        num_opponent_cooperators_in_prev_round = sum(history.opponent_actions[-1, :])
        N_C_in_prev_round = num_opponent_cooperators_in_prev_round + (1 if my_action_in_prev_round_bool else 0)
        if N_C_in_prev_round < self.game_description.m:
            self.collective_failure_streak += 1
        else:
            self.collective_failure_streak = 0
        if my_action_in_prev_round_bool:
            self.round_count_since_i_last_defected += 1
        else:
            self.round_count_since_i_last_defected = 0
        if self.collective_failure_streak > 0:
            if self.collective_failure_streak >= 2:
                return Action.D
            else:
                return Action.C
        elif N_C_in_prev_round == self.game_description.m:
            return Action.C
        elif my_action_in_prev_round_bool and self.round_count_since_i_last_defected >= self.free_ride_coop_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_400(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims for collective success by initiating cooperation,
    monitoring the group's performance, and adapting its behavior. It defaults
    to cooperation but introduces a "punishment" phase (defection) if the group
    repeatedly fails to meet the minimum cooperation threshold. It handles the
    first and last rounds with specific game-theoretic considerations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.defection_streak_count = 0
        self.cooperation_streak_count = 0
        self.defection_penalty_threshold = max(2, math.floor(self.game_description.n_rounds / 5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[state.round_number - 1]
        opponents_prev_actions_were_cooperate = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = (1 if my_prev_action_was_cooperate else 0) + np.sum(opponents_prev_actions_were_cooperate)
        m = self.game_description.m
        if num_cooperators_prev_round < m:
            self.defection_streak_count += 1
            self.cooperation_streak_count = 0
        else:
            self.cooperation_streak_count += 1
            self.defection_streak_count = 0
        if self.defection_streak_count > self.defection_penalty_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_401(BaseStrategy):
    """
    Adaptive Collective Support (ACS) strategy for the Collective Risk Dilemma.

    This strategy aims to consistently meet the 'm' cooperator threshold by adapting
    its behavior based on the previous round's collective outcome and its own prior action.
    It prioritizes collective success but also protects against exploitation or repeated futile cooperation.

    The strategy initializes by cooperating in the first round to signal willingness for collective
    benefit. In intermediate rounds, it assesses whether the collective goal (m cooperators)
    was met previously. If met, it continues to cooperate to sustain success. If not met,
    it adjusts its behavior based on whether its own previous action was cooperative or
    defective, and how close the group was to the threshold. In the final round, it defects
    to maximize immediate payoff, recognizing the end of the game's influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_bool = history.my_actions[prev_round_idx]
        opp_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        N_C_prev = int(my_action_prev_bool) + opp_cooperators_prev
        if N_C_prev >= m_threshold:
            return Action.C
        elif my_action_prev_bool == True:
            if N_C_prev == m_threshold - 1:
                return Action.C
            else:
                return Action.D
        elif N_C_prev + 1 >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_402(BaseStrategy):
    """
    The Adaptive Threshold Monitor (ATM) strategy for the Collective Risk Dilemma.
    It fosters and maintains collective cooperation by enforcing the `m` cooperator
    threshold. It operates on principles of initial good faith, conditional cooperation,
    and group-level punishment for collective failure, aiming to establish a stable
    equilibrium where the minimum cooperation threshold is consistently met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.punishment_rounds_remaining: int = 0
        self.PUNISHMENT_DURATION_P: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if num_cooperators_prev_round >= self.m:
                return Action.C
            else:
                self.punishment_rounds_remaining = self.PUNISHMENT_DURATION_P - 1
                return Action.D

class Strategy_COLLECTIVE_403(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy aims to maximize total collective payoff
    by consistently achieving the cooperation threshold `m`. It initiates cooperation,
    maintains it when successful, and adaptively responds to failures by entering
    a temporary "punishment" phase of defection, followed by attempts to re-initiate
    cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.rounds_since_last_success: int = 0
        self.T_recoop_attempts: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if cooperators_prev_round < self.m:
            self.rounds_since_last_success += 1
        else:
            self.rounds_since_last_success = 0
        if self.rounds_since_last_success == 0:
            return Action.C
        elif self.rounds_since_last_success <= self.T_recoop_attempts:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_404(BaseStrategy):
    """
    The Robust Collective Cooperator (RCC) strategy for the Collective Risk Dilemma.
    This strategy prioritizes achieving and maintaining the collective cooperation threshold `m`.
    It cooperates by default, punishes collective failures with a short defection period,
    and attempts to re-establish cooperation after prolonged periods of failure to avoid
    getting stuck in low-payoff equilibria.
    In the final round, it defects to maximize individual payoff due to the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.punishment_duration = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        n_c_prev = sum(history.opponent_actions[prev_round_idx, :]) + int(history.my_actions[prev_round_idx])
        failure_streak = 0
        for round_idx in range(prev_round_idx, -1, -1):
            n_c_in_this_round = sum(history.opponent_actions[round_idx, :]) + int(history.my_actions[round_idx])
            if n_c_in_this_round < self.m:
                failure_streak += 1
            else:
                break
        if n_c_prev >= self.m:
            return Action.C
        elif failure_streak <= self.punishment_duration:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_405(BaseStrategy):
    """
    Adaptive Collective Assurance (ACA) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation to consistently meet the 'm'
    threshold, thereby maximizing the collective 'k' reward. It initiates cooperation,
    rewards collective success by continuing to cooperate, punishes collective failure
    (below 'm' cooperators) by defecting, and defects in the final round for individual
    gain due to no future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev = sum(history.opponent_actions[state.round_number - 1, :])
            my_action_prev = history.my_actions[state.round_number - 1]
            num_total_cooperators_prev = num_opponent_cooperators_prev + my_action_prev
            if num_total_cooperators_prev >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_406(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) strategy for the Collective Risk Dilemma.
    This strategy aims to achieve and sustain collective cooperation by initially
    cooperating, continuing cooperation when the threshold is met, and implementing
    a temporary punishment (defection) phase when the collective fails to meet
    the threshold. It includes an end-game defection to prevent exploitation.
    """
    _PUNISHMENT_PERIOD: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._num_cooperators_last_round: int | None = None
        self._in_punishment_mode: bool = False
        self._punishment_rounds_counter: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_pseudocode = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if state.round_number > 0:
            self._my_last_action = Action.C if history.my_actions[-1] else Action.D
            cooperators_from_opponents_last_round = np.sum(history.opponent_actions[-1, :])
            cooperators_from_my_last_action = 1 if self._my_last_action == Action.C else 0
            self._num_cooperators_last_round = cooperators_from_opponents_last_round + cooperators_from_my_last_action
        action_for_current_round: Action
        if current_round_pseudocode == total_rounds_r:
            action_for_current_round = Action.D
        elif current_round_pseudocode == 1:
            action_for_current_round = Action.C
        elif self._in_punishment_mode:
            self._punishment_rounds_counter += 1
            if self._punishment_rounds_counter <= self._PUNISHMENT_PERIOD:
                action_for_current_round = Action.D
            else:
                self._in_punishment_mode = False
                self._punishment_rounds_counter = 0
                action_for_current_round = Action.C
        elif self._num_cooperators_last_round is not None and self._num_cooperators_last_round >= min_cooperators_m:
            self._punishment_rounds_counter = 0
            action_for_current_round = Action.C
        else:
            self._in_punishment_mode = True
            self._punishment_rounds_counter = 1
            action_for_current_round = Action.D
        return action_for_current_round

class Strategy_COLLECTIVE_407(BaseStrategy):
    """
    Adaptive Threshold Follower with Forgiveness:
    This strategy aims to foster collective cooperation by cooperating initially,
    then adaptively responding to whether the collective cooperation threshold (m)
    was met in the previous round. It sustains cooperation if the threshold was met,
    otherwise defects as a punishment/disengagement. It is forgiving, immediately
    reverting to cooperation as soon as the threshold is met again. It defects
    in the last round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        else:
            my_action_prev = history.my_actions[-1]
            opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            num_cooperators_prev = my_action_prev + opponent_cooperators_prev
            if num_cooperators_prev >= m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_408(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and sustain collective cooperation by adaptively
    responding to the group's ability to consistently meet the cooperation threshold `m`.
    It starts with an optimistic cooperative stance, punishes sustained collective
    failure by withdrawing cooperation, and rewards collective success by re-engaging.

    Key principles:
    - Round 1: Always Cooperate (C) to signal goodwill.
    - Intermediate Rounds:
        - Track `consecutive_successes` and `consecutive_failures` based on whether
          the 'm' threshold was met in the previous round.
        - If in 'cooperate_mode' and 2 consecutive failures occur, switch to 'defect_mode'.
        - If in 'defect_mode' and 1 consecutive success occurs, switch back to 'cooperate_mode'.
        - Act according to `my_current_stance`.
    - Last Round: Always Defect (D) to avoid exploitation in the absence of future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_successes: int = 0
        self.consecutive_failures: int = 0
        self.my_current_stance: str = 'cooperate_mode'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if state.round_number == 0:
            self.my_current_stance = 'cooperate_mode'
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        my_prev_action: bool = history.my_actions[-1]
        opponent_prev_actions: NDArray[np.bool_] = history.opponent_actions[-1, :]
        num_cooperators_prev_round = int(my_prev_action) + np.sum(opponent_prev_actions)
        if num_cooperators_prev_round >= min_cooperators_needed:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        if self.my_current_stance == 'cooperate_mode':
            if self.consecutive_failures >= 2:
                self.my_current_stance = 'defect_mode'
        elif self.consecutive_successes >= 1:
            self.my_current_stance = 'cooperate_mode'
        if self.my_current_stance == 'cooperate_mode':
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_409(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.
    This strategy aims to achieve and sustain the collective good (meeting the `m` cooperator
    threshold for the `k` bonus) while being adaptive to opponent behavior and robust against
    exploitation. It balances initial trust with conditional cooperation and self-preservation.
    """
    PATIENCE_LIMIT = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.rounds_I_cooperated_and_failed: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.my_last_action = Action.C
            self.rounds_I_cooperated_and_failed = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            self.my_last_action = Action.D
            return Action.D
        prev_my_action: Action = self.my_last_action
        prev_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        C_count_t_minus_1 = (1 if prev_my_action == Action.C else 0) + prev_opponent_cooperators
        threshold_met_t_minus_1 = C_count_t_minus_1 >= self.game_description.m
        if threshold_met_t_minus_1:
            self.rounds_I_cooperated_and_failed = 0
        elif prev_my_action == Action.C:
            self.rounds_I_cooperated_and_failed += 1
        my_current_action: Action = Action.C
        if threshold_met_t_minus_1:
            my_current_action = Action.C
        elif self.rounds_I_cooperated_and_failed >= self.PATIENCE_LIMIT:
            my_current_action = Action.D
        else:
            my_current_action = Action.C
        self.my_last_action = my_current_action
        return my_current_action

class Strategy_COLLECTIVE_410(BaseStrategy):
    """
    The Collective Guardian strategy for the Collective Risk Dilemma.

    This strategy prioritizes the consistent achievement of the collective reward 'k'.
    It establishes initial trust by cooperating in the first round. In subsequent rounds,
    it sustains cooperation if the collective successfully met the minimum cooperation
    threshold 'm' in the previous round, thereby reinforcing successful collective
    behavior and ensuring stability. If the collective failed to meet the threshold,
    it defects as a temporary "collective punishment" or protest signal, aiming to
    make the cost of collective failure evident and spur more cooperation in future rounds.
    In the final round, it defects, consistent with the endgame effect in repeated games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        num_cooperators_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_411(BaseStrategy):
    """
    Adaptive Threshold Contributor (ATC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation when beneficial to the collective,
    adapt to the group's performance, and protect itself against sustained exploitation.

    Core Principles:
    1. Initiate Cooperation: Start by cooperating to signal willingness.
    2. Reward Success: Continue cooperating when the collective threshold is met or nearly met.
    3. Punish Failure (and self-protect): Withdraw cooperation when the group consistently fails
       to meet the threshold.
    4. Pragmatic Exit: Defect in the final round to prevent exploitation due to end-game incentives.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_prev_action = history.my_actions[current_round_0_indexed - 1]
        opponents_prev_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        C_prev_round = my_prev_action + np.sum(opponents_prev_actions)
        if C_prev_round >= self.m_threshold:
            return Action.C
        elif C_prev_round == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_412(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy aims to establish and maintain collective
    cooperation in the Collective Risk Dilemma by being conditionally cooperative, self-correcting
    when free-riding, and punishing collective failures.

    It starts optimistically by cooperating. In intermediate rounds, it bases decisions on the
    previous round's outcome:
    - If the collective threshold was met, it cooperates to maintain stability, even if it free-rode previously.
    - If the collective threshold was not met, and the player cooperated, it defects for one round as punishment.
    - If the collective threshold was not met, and the player defected, it continues to defect to protect itself.
    The strategy defects in the final round due to end-game rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._punishment_rounds_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            self._punishment_rounds_remaining = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        if self._punishment_rounds_remaining > 0:
            self._punishment_rounds_remaining -= 1
            return Action.D
        prev_round_history_idx = state.round_number - 1
        my_prev_action_was_cooperate = history.my_actions[prev_round_history_idx]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_history_idx, :])
        total_cooperators_prev = num_opponent_cooperators_prev + my_prev_action_was_cooperate
        prev_round_threshold_met = total_cooperators_prev >= m_threshold
        if prev_round_threshold_met:
            return Action.C
        elif my_prev_action_was_cooperate:
            self._punishment_rounds_remaining = 1
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_413(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.
    Aims to ensure the collective threshold `m` is met, initiating cooperation,
    reciprocating collective success, discouraging own free-riding, punishing
    collective failure, and forgiving upon success. Reverts to individualistic
    behavior in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_collective_failed_consecutively: int = 0
        self.am_i_punishing: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        last_round_idx = state.round_number - 1
        my_last_action_was_C = history.my_actions[last_round_idx]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[last_round_idx, :])
        total_cooperators_last_round = (1 if my_last_action_was_C else 0) + opponent_cooperators_last_round
        threshold_met_last_round = total_cooperators_last_round >= self.game_description.m
        if threshold_met_last_round:
            self.rounds_collective_failed_consecutively = 0
            if self.am_i_punishing:
                self.am_i_punishing = False
        else:
            self.rounds_collective_failed_consecutively += 1
        if self.am_i_punishing:
            return Action.D
        elif self.rounds_collective_failed_consecutively >= 1:
            self.am_i_punishing = True
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_414(BaseStrategy):
    """
    "Adaptive Guardian of the Collective" strategy for the Collective Risk Dilemma.
    This strategy prioritizes the collective good by reliably cooperating to meet the
    'm' threshold, thus activating the 'k' bonus for all. It adapts by entering a
    punishment mode if the collective consistently fails to cooperate, protecting
    its own endowment and signaling dissatisfaction. In the final round, it defects
    due to end-game rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.initial_cooperate_rounds = 1
        self.failure_threshold_for_punishment = 2
        self.punishment_duration = 2
        self.failure_streak = 0
        self.punish_mode = False
        self.punishment_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round == 0:
            return Action.C
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round < m:
            self.failure_streak += 1
        else:
            self.failure_streak = 0
        if self.punish_mode:
            self.punishment_countdown -= 1
            if self.punishment_countdown == 0:
                self.punish_mode = False
        elif self.failure_streak >= self.failure_threshold_for_punishment:
            self.punish_mode = True
            self.punishment_countdown = self.punishment_duration
            self.failure_streak = 0
        if current_round == n_rounds - 1:
            return Action.D
        if self.punish_mode:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_415(BaseStrategy):
    """
    The Collective Enforcer with Sucker Prevention (CESP) strategy is designed to promote and sustain cooperation
    in the Collective Risk Dilemma by actively aiming for the collective threshold `m` to be met.
    It is adaptive, learning from past outcomes, and robust against exploitation by
    incorporating a "sucker prevention" mechanism. The strategy prioritizes the collective
    good but avoids becoming a perpetual free-rider.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.consecutive_marginal_failures_as_cooperator: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m_threshold = self.game_description.m
        if state.round_number == 0:
            self.my_last_action = Action.C
            self.consecutive_marginal_failures_as_cooperator = 0
            return Action.C
        my_previous_action_bool = history.my_actions[-1]
        opponent_previous_actions_bool = history.opponent_actions[-1, :]
        C_obs = int(my_previous_action_bool) + np.sum(opponent_previous_actions_bool)
        if C_obs >= m_threshold:
            self.consecutive_marginal_failures_as_cooperator = 0
            self.my_last_action = Action.C
            return Action.C
        else:
            is_marginal_miss = C_obs == m_threshold - 1
            if is_marginal_miss:
                if my_previous_action_bool is True:
                    if self.consecutive_marginal_failures_as_cooperator >= 1:
                        self.consecutive_marginal_failures_as_cooperator = 0
                        self.my_last_action = Action.D
                        return Action.D
                    else:
                        self.consecutive_marginal_failures_as_cooperator += 1
                        self.my_last_action = Action.C
                        return Action.C
                else:
                    self.consecutive_marginal_failures_as_cooperator = 0
                    self.my_last_action = Action.C
                    return Action.C
            else:
                self.consecutive_marginal_failures_as_cooperator = 0
                self.my_last_action = Action.D
                return Action.D

class Strategy_COLLECTIVE_416(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    It initiates cooperation, sustains it during collective success, punishes collective
    failure for a set duration, and retreats to permanent defection if failures are
    persistent. It also defects in the final round.
    """
    FAILURE_PUNISHMENT_DURATION: int
    FORGIVENESS_THRESHOLD: int
    my_last_action: Action | None
    consecutive_failures: int
    punishment_mode: bool
    punishment_rounds_remaining: int

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FAILURE_PUNISHMENT_DURATION = 2
        self.FORGIVENESS_THRESHOLD = self.game_description.m
        self.my_last_action = None
        self.consecutive_failures = 0
        self.punishment_mode = False
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        chosen_action: Action
        if current_round == total_rounds - 1:
            chosen_action = Action.D
        elif current_round == 0:
            chosen_action = Action.C
        else:
            my_cooperation_prev = 1 if self.my_last_action == Action.C else 0
            opponent_cooperators_prev = sum(history.opponent_actions[-1, :])
            N_C_prev = opponent_cooperators_prev + my_cooperation_prev
            threshold_met_prev = N_C_prev >= min_cooperators_needed
            if self.punishment_mode:
                self.punishment_rounds_remaining -= 1
                if self.punishment_rounds_remaining <= 0:
                    self.punishment_mode = False
            if not threshold_met_prev:
                self.consecutive_failures += 1
            else:
                self.consecutive_failures = 0
            if self.consecutive_failures >= self.FORGIVENESS_THRESHOLD:
                chosen_action = Action.D
            elif self.punishment_mode:
                chosen_action = Action.D
            elif threshold_met_prev:
                chosen_action = Action.C
            else:
                self.punishment_mode = True
                self.punishment_rounds_remaining = self.FAILURE_PUNISHMENT_DURATION
                chosen_action = Action.D
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_COLLECTIVE_417(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff over `r` rounds by:
    - Initiating Cooperation: Provides an initial impetus for the collective good in the first round.
    - Sustaining Successful Cooperation: Rewards and reinforces collective success by continuing to contribute
      when the threshold `m` is met in the previous round.
    - Protecting Against Exploitation: Withdraws cooperation when the collective fails (threshold `m` not met)
      in the previous round, preventing individual sacrifice.
    - Rational Self-Interest in the End: Maximizes individual gain by defecting in the final round,
      as future interactions cease.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Contributor strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACC strategy rules.

        Args:
            state: The current game state, including the round number.
            history: A record of actions and payoffs from previous rounds for this player and opponents.
                     Will be None for the very first round (round_number == 0).

        Returns:
            An Action (C for Cooperate, D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        my_prev_action_was_c = int(history.my_actions[-1])
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = my_prev_action_was_c + num_opponent_cooperators_prev_round
        if total_cooperators_prev_round >= min_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_418(BaseStrategy):
    """
    The Adaptive Collective Maintainer (ACM) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and sustain collective cooperation.
    It initiates cooperation in the first round to signal willingness.
    In intermediate rounds, it cooperates if the collective met the minimum
    cooperation threshold 'm' in the previous round, thereby helping to maintain
    the collective benefit. If the threshold was not met, it defects to avoid
    exploitation and signal dissatisfaction, pushing the collective towards
    a default defection state rather than repeated loss for cooperators.
    In the final round, it defects, following standard end-game game-theoretic logic.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (min_cooperators_needed), k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for this player
                     and opponents. It is None for the first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        my_prev_action_was_cooperation = history.my_actions[previous_round_idx]
        num_cooperators_in_previous_round = int(my_prev_action_was_cooperation) + np.sum(history.opponent_actions[previous_round_idx, :])
        if num_cooperators_in_previous_round >= min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_419(BaseStrategy):
    """
    The Adaptive Collective Maintainer (ACM) strategy for the Collective Risk Dilemma.

    This strategy balances collective good with individual robustness. It aims to foster
    and maintain cooperation by starting with 'C' (Cooperate), continuing cooperation if
    the collective threshold 'm' is met, and strategically defecting if the group
    consistently fails to meet 'm'. It also 'forgives' and re-engages cooperation if
    the group demonstrates sustained recovery. A key feature is defection in the final
    round, reflecting the lack of future incentives.
    """
    PATIENCE_THRESHOLD = 2
    FORGIVENESS_THRESHOLD = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and its internal state.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.state = 'COOPERATE_MODE'
        self.consecutive_failures = 0
        self.consecutive_successes = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on game state and history.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing records of past actions and payoffs for
                     this player and opponents. None for the first round (round_number 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        action_to_play = None
        if self.state == 'COOPERATE_MODE':
            if cooperators_in_prev_round >= self.game_description.m:
                self.consecutive_failures = 0
                action_to_play = Action.C
            else:
                self.consecutive_failures += 1
                if self.consecutive_failures >= self.PATIENCE_THRESHOLD:
                    self.state = 'DEFECT_MODE'
                    self.consecutive_successes = 0
                    action_to_play = Action.D
                else:
                    action_to_play = Action.C
        elif self.state == 'DEFECT_MODE':
            if cooperators_in_prev_round >= self.game_description.m:
                self.consecutive_successes += 1
                if self.consecutive_successes >= self.FORGIVENESS_THRESHOLD:
                    self.state = 'COOPERATE_MODE'
                    self.consecutive_failures = 0
                    action_to_play = Action.C
                else:
                    action_to_play = Action.D
            else:
                self.consecutive_successes = 0
                action_to_play = Action.D
        return action_to_play

class Strategy_COLLECTIVE_420(BaseStrategy):
    """
    The Adaptive Threshold Enforcer (ATE) strategy is designed to navigate the Collective Risk Dilemma
    by prioritizing the collective good of consistently achieving the cooperation threshold.
    It leverages historical group performance to guide individual decisions.

    - In the first round, it always Cooperates to signal willingness to contribute.
    - In intermediate rounds, if the collective threshold (m) was met in the previous round,
      it continues to Cooperate, reinforcing success.
    - If the collective threshold was NOT met in the previous round, it Defects, signaling
      a breakdown in cooperation and aiming to make the consequences of collective failure clear.
    - In the final round, it always Defects, as the "shadow of the future" disappears,
      preventing exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        my_previous_action_bool = history.my_actions[state.round_number - 1]
        opponents_previous_actions_bool = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_last_round = int(my_previous_action_bool) + np.sum(opponents_previous_actions_bool)
        if num_cooperators_last_round >= self.m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_421(BaseStrategy):
    """
    Dynamic Collective Responder (DCR) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation by being
    initially cooperative, then adaptive, responsive, and moderately forgiving.
    It prioritizes the collective achievement of the reward 'k' while also
    protecting itself against persistent exploitation.

    State Variable:
    - consecutive_failures: An integer counter tracking the number of
      consecutive rounds where the total number of cooperators was less than
      the required minimum 'm'. Initialized to 0.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m: int = game_description.m
        self.n_rounds: int = game_description.n_rounds
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.consecutive_failures = 0
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if current_round_0_indexed == self.n_rounds - 1:
            if num_cooperators_prev_round >= self.m:
                return Action.C
            else:
                return Action.D
        if self.consecutive_failures == 0:
            return Action.C
        elif self.consecutive_failures == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_422(BaseStrategy):
    """
    The Adaptive Forgiving Collective Strategy (AFCS) aims to achieve and sustain
    the collective reward 'k' by prioritizing conditional cooperation, strategic
    punishment, and timely forgiveness in the Collective Risk Dilemma. It balances
    collective success with individual robustness in a tournament setting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.forgiveness_rounds = 1
        self.failure_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_previous_round >= self.game_description.m:
            self.failure_counter = 0
            return Action.C
        else:
            self.failure_counter += 1
            if self.failure_counter <= self.forgiveness_rounds:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_423(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.
    This strategy aims to promote and sustain cooperation by leading with cooperation,
    tolerating a single failure, and only resorting to defection as a punishment
    for repeated collective failures, or in the final round. It balances the
    collective good with a necessary level of self-preservation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.rounds_since_success = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        if current_t == 1:
            return Action.C
        if current_t == self.n_rounds:
            return Action.D
        my_cooperated_prev = int(history.my_actions[state.round_number - 1])
        opponents_cooperated_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        c_prev = my_cooperated_prev + opponents_cooperated_prev
        if c_prev >= self.m:
            self.rounds_since_success = 0
            return Action.C
        else:
            self.rounds_since_success += 1
            if self.rounds_since_success >= 2:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_424(BaseStrategy):
    """
    Adaptive Threshold Responder with Forgiveness strategy for the Collective Risk Dilemma.

    This strategy aims to achieve the collective reward 'k' by promoting cooperation.
    It starts cooperatively, attempts to maintain cooperation as long as the collective goal
    ('m' cooperators) is met, and temporarily defects when the collective goal is missed
    to "punish" the group's failure. It then attempts to re-establish cooperation,
    showing forgiveness. The final round behavior balances collective benefit with strategic rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_to_defect_after_failure = 2
        self.current_defection_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_game_round_0_indexed == 0:
            return Action.C
        prev_round_index = current_game_round_0_indexed - 1
        my_prev_action_cooperated = history.my_actions[prev_round_index]
        opponent_prev_cooperators = sum(history.opponent_actions[prev_round_index, :])
        total_cooperators_prev_round = int(my_prev_action_cooperated) + opponent_prev_cooperators
        prev_threshold_met = total_cooperators_prev_round >= min_cooperators
        if current_game_round_0_indexed == total_rounds - 1:
            if prev_threshold_met:
                return Action.C
            else:
                return Action.D
        if prev_threshold_met:
            self.current_defection_countdown = 0
        elif self.current_defection_countdown == 0:
            self.current_defection_countdown = self.rounds_to_defect_after_failure
        else:
            self.current_defection_countdown -= 1
        if self.current_defection_countdown > 0:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_425(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.
    It promotes sustained collective cooperation by starting cooperatively,
    punishing failures to meet the cooperation threshold, and quickly forgiving.

    Guiding Principles:
    - Proactive Cooperation: Begin by cooperating.
    - Threshold Enforcement: Respond to whether the collective cooperation threshold (`m`) was met.
    - Proportional Punishment: Implement a measured, temporary defection phase when `m` is not met.
    - Swift Forgiveness: Reset punishment upon observing collective success.
    - Collective Stability: Prioritize actions that sustain conditions for the collective reward `k`.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PUNISHMENT_DURATION_ROUNDS: int = 1
        self.FORGIVENESS_THRESHOLD: int = 0
        self.consecutive_failures: int = 0
        self.punishment_active: bool = False
        self.punishment_rounds_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_in_prev_round >= self.game_description.m:
            self.consecutive_failures = 0
            self.punishment_active = False
            self.punishment_rounds_remaining = 0
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures > self.FORGIVENESS_THRESHOLD and (not self.punishment_active):
                self.punishment_active = True
                self.punishment_rounds_remaining = self.PUNISHMENT_DURATION_ROUNDS
        if self.punishment_active:
            action_for_current_round = Action.D
            self.punishment_rounds_remaining -= 1
            if self.punishment_rounds_remaining <= 0:
                self.punishment_active = False
        else:
            action_for_current_round = Action.C
        return action_for_current_round

class Strategy_COLLECTIVE_426(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation by prioritizing
    the achievement of the minimum cooperation threshold (m). It starts cooperatively,
    punishes collective failure by defecting for a specified duration, and then
    attempts to re-establish cooperation. It also accounts for the terminal nature
    of the game by defecting in the last round.
    """

    class State(Enum):
        COOPERATE = 1
        PUNISH = 2
    PUNISHMENT_DURATION = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_state = self.State.COOPERATE
        self.rounds_in_current_state = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.rounds_in_current_state = 1
            return Action.C
        my_prev_action_is_C = history.my_actions[state.round_number - 1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
        C_count_prev = opponent_prev_cooperators + (1 if my_prev_action_is_C else 0)
        action_to_play: Action = Action.C
        if self.current_state == self.State.COOPERATE:
            if C_count_prev >= self.game_description.m:
                action_to_play = Action.C
                self.rounds_in_current_state += 1
            else:
                action_to_play = Action.D
                self.current_state = self.State.PUNISH
                self.rounds_in_current_state = 1
        elif self.current_state == self.State.PUNISH:
            if self.rounds_in_current_state < self.PUNISHMENT_DURATION:
                action_to_play = Action.D
                self.rounds_in_current_state += 1
            else:
                action_to_play = Action.C
                self.current_state = self.State.COOPERATE
                self.rounds_in_current_state = 1
        return action_to_play

class Strategy_COLLECTIVE_427(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" strategy aims to maximize collective welfare in the Collective Risk Dilemma.
    It initiates with cooperation, rewards collective success by continuing to cooperate, and forgives minor failures
    to meet the cooperation threshold. However, it punishes persistent collective failure by defecting to cut losses
    and signal the unsustainability of non-cooperation. In the final round, it defects as the rational end-game
    move due to no future consequences.

    Internal State Variables:
    - self.last_round_cooperators: Number of players who cooperated in the previous round.
    - self.failure_streak: Counter for consecutive rounds where the 'm' threshold was not met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_cooperators: int = self.game_description.n_players
        self.failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed > 0:
            previous_round_idx = current_round_0_indexed - 1
            num_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :]) + history.my_actions[previous_round_idx]
            self.last_round_cooperators = num_cooperators_prev
            if self.last_round_cooperators < m_threshold:
                self.failure_streak += 1
            else:
                self.failure_streak = 0
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == n_rounds - 1:
            return Action.D
        elif self.last_round_cooperators >= m_threshold:
            return Action.C
        elif self.failure_streak < 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_428(BaseStrategy):
    """
    The Adaptive Collective Pledger (ACP) strategy is an adaptive, state-based approach
    that seeks to establish and maintain collective cooperation while being robust
    to varying opponent behaviors. It prioritizes collective success, adapts to
    group behavior through forgiveness and punishment, and strategically attempts
    to free-ride after sustained success without undermining the collective.
    It applies endgame rationality in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._failed_rounds_count: int = 0
        self._successful_rounds_count: int = 0
        self._is_probing_defection: bool = False
        self._n_players: int = game_description.n_players
        self._total_rounds: int = game_description.n_rounds
        self._min_cooperators: int = game_description.m
        self._reward_factor: float = game_description.k
        self._forgiveness_limit: int = 2
        self._success_streak_for_probe: int = self._min_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self._failed_rounds_count = 0
            self._successful_rounds_count = 0
            self._is_probing_defection = False
            return Action.C
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self._min_cooperators:
            self._failed_rounds_count = 0
            self._successful_rounds_count += 1
        else:
            self._successful_rounds_count = 0
            self._failed_rounds_count += 1
            self._is_probing_defection = False
        if state.round_number == self._total_rounds - 1:
            self._is_probing_defection = False
            return Action.D
        if self._failed_rounds_count >= self._forgiveness_limit:
            self._is_probing_defection = False
            return Action.D
        if self._successful_rounds_count >= self._success_streak_for_probe:
            if self._is_probing_defection:
                self._is_probing_defection = False
                return Action.C
            else:
                self._is_probing_defection = True
                return Action.D
        self._is_probing_defection = False
        return Action.C

class Strategy_COLLECTIVE_429(BaseStrategy):
    """
    Implements the Collective Forgiveness & Punisher (CFP) strategy for the
    Collective Risk Dilemma. This strategy aims to establish and maintain a
    cooperative equilibrium by starting cooperatively, punishing collective
    failure temporarily, forgiving success quickly, and adapting for end-game effects.
    """
    FAIL_TRIGGER_THRESHOLD = 2
    PUNISH_DURATION = 2
    ENDGAME_THRESHOLD = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_action_history: list[Action] = []
        self.collective_cooperators_history: list[int] = []
        self.consecutive_failures: int = 0
        self.punishment_rounds_active: int = 0
        self.punishment_mode_on: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            my_action = Action.C
            self.my_action_history.append(my_action)
            return my_action
        my_prev_action_was_C = self.my_action_history[-1] == Action.C
        C_prev = np.sum(history.opponent_actions[-1, :]) + (1 if my_prev_action_was_C else 0)
        self.collective_cooperators_history.append(C_prev)
        if C_prev < m_threshold:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        if self.punishment_mode_on:
            if C_prev >= m_threshold:
                self.punishment_mode_on = False
                self.punishment_rounds_active = 0
            elif self.punishment_rounds_active >= self.PUNISH_DURATION:
                self.punishment_mode_on = False
                self.punishment_rounds_active = 0
        elif self.consecutive_failures >= self.FAIL_TRIGGER_THRESHOLD:
            self.punishment_mode_on = True
            self.punishment_rounds_active = 0
        my_action = Action.C
        if n_rounds - current_round_0_indexed <= self.ENDGAME_THRESHOLD:
            my_action = Action.D
        elif self.punishment_mode_on:
            my_action = Action.D
        self.my_action_history.append(my_action)
        if my_action == Action.D and self.punishment_mode_on:
            self.punishment_rounds_active += 1
        return my_action

class Strategy_COLLECTIVE_430(BaseStrategy):
    """
    Adaptive Forgiving Responder (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain collective cooperation while being resilient to failures and
    robust against exploitative behaviors. It balances initial trust, conditional punishment, and forgiveness
    to maximize the number of rounds where the collective reward 'k' is achieved.

    It operates based on the collective's historical performance, specifically whether the minimum cooperation
    threshold 'm' was met in previous rounds. It incorporates initial cooperation, adaptive responses to
    collective success or failure, and a rational adjustment for the game's end.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t == 1:
            self.consecutive_failure_rounds = 0
            return Action.C
        if current_round_t == total_rounds_r:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= min_cooperators_m:
            self.consecutive_failure_rounds = 0
        else:
            self.consecutive_failure_rounds += 1
        if self.consecutive_failure_rounds == 0:
            return Action.C
        elif self.consecutive_failure_rounds % 2 == 1:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_431(BaseStrategy):
    """
    The "Resilient Contributor" strategy is designed to promote and sustain cooperation
    in the Collective Risk Dilemma. It balances proactive collective action with adaptive
    responses to the group's observed behavior, aiming to consistently achieve the
    collective reward 'k'. It starts with an initial period of cooperation,
    maintains cooperation when the threshold is met or nearly met, and defects
    when cooperation is severely lacking or in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.initial_rounds_count = max(1, min(5, math.floor(self.n_rounds / 5)))
        self.deficit_tolerance = max(1, math.floor(self.m / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.n_rounds:
            return Action.D
        if current_round_t <= self.initial_rounds_count:
            return Action.C
        num_C_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        T_met_prev = num_C_prev >= self.m
        if T_met_prev:
            return Action.C
        elif num_C_prev < self.m - self.deficit_tolerance:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_432(BaseStrategy):
    """
    Adaptive Forgiving Threshold (AFT) strategy for the Collective Risk Dilemma.

    This strategy aims to encourage and sustain collective cooperation by:
    - Cooperating initially to establish trust and signal willingness.
    - Sustaining cooperation when the group meets or nearly meets the threshold 'm'
      in the previous round (forgiving a near-miss by continuing to cooperate if C_prev = m-1).
    - Defecting as a signal and self-protection when the group significantly fails
      to meet the threshold (C_prev < m-1).
    - Defecting in the final round to maximize individual payoff, as there are no
      future interactions to consider.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_prev_action_is_C = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        c_prev = int(my_prev_action_is_C) + opponent_prev_cooperators
        if c_prev < m_threshold - 1:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_433(BaseStrategy):
    """
    The Adaptive Forgiveness and Enforcement (AFE) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by starting cooperatively, rewarding
    collective success, and implementing a short, adaptive punishment mechanism
    for collective failures. It quickly forgives past failures to prevent
    indefinite defection spirals and adopts a self-interested defection
    in the final round.
    """
    _PUNISHMENT_DURATION_ON_FAILURE = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.currently_in_punishment_mode: bool = False
        self.rounds_remaining_in_punishment: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_t == n_rounds:
            return Action.D
        if current_round_t == 1:
            self.currently_in_punishment_mode = False
            self.rounds_remaining_in_punishment = 0
            return Action.C
        previous_round_idx = state.round_number - 1
        my_action_prev = history.my_actions[previous_round_idx]
        opponent_actions_prev = history.opponent_actions[previous_round_idx, :]
        cooperators_in_previous_round = int(my_action_prev) + np.sum(opponent_actions_prev)
        if cooperators_in_previous_round >= m_threshold:
            self.currently_in_punishment_mode = False
            self.rounds_remaining_in_punishment = 0
        elif not self.currently_in_punishment_mode:
            self.currently_in_punishment_mode = True
            self.rounds_remaining_in_punishment = self._PUNISHMENT_DURATION_ON_FAILURE
        else:
            self.rounds_remaining_in_punishment -= 1
            if self.rounds_remaining_in_punishment <= 0:
                self.currently_in_punishment_mode = False
                self.rounds_remaining_in_punishment = 0
        if self.currently_in_punishment_mode:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_434(BaseStrategy):
    """
    Cooperative Enforcer with Recalibration (CER) strategy for the Collective Risk Dilemma.

    The CER strategy prioritizes the establishment and maintenance of the collective 'k' bonus.
    It starts by attempting cooperation, then conditions its future actions on the group's
    ability to meet the cooperation threshold 'm'. It punishes collective failure briefly
    to signal dissatisfaction and encourage compliance, but quickly attempts to re-establish
    cooperation. It accounts for the endgame effect by defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_counter: int = 0
        self.P_DURATION: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            self.punishment_counter = 0
            return Action.C
        previous_round_history_index = state.round_number - 1
        num_cooperators_prev = np.sum(history.opponent_actions[previous_round_history_index, :]) + history.my_actions[previous_round_history_index]
        if num_cooperators_prev >= min_cooperators_needed:
            self.punishment_counter = 0
        elif self.punishment_counter == 0:
            self.punishment_counter = self.P_DURATION
        else:
            self.punishment_counter -= 1
        if self.punishment_counter > 0:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_435(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize collective payoff by promoting cooperation,
    rewarding success, attempting recovery from near misses, and protecting
    against exploitation by defecting on significant failures. It also accounts
    for the end-game effect by defecting in the final round.

    Decision Rules:
    1.  Round 1: Cooperate to initiate a positive dynamic.
    2.  Last Round: Defect due to the end-game effect (no future interactions).
    3.  Intermediate Rounds:
        -   If the cooperation threshold 'm' was met in the previous round: Cooperate to reinforce success.
        -   If cooperation was a "near miss" (m-1 cooperators) in the previous round: Cooperate to attempt recovery.
        -   If cooperation was a "significant failure" (fewer than m-1 cooperators): Defect to protect against exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        my_prev_action_was_C = history.my_actions[state.round_number - 1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        n_c_previous_round = int(my_prev_action_was_C) + int(num_opponent_cooperators_prev_round)
        if n_c_previous_round >= m_threshold:
            return Action.C
        elif n_c_previous_round >= m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_436(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy aims to foster and sustain collective cooperation
    by committing to contribution, encouraging others through consistent efforts, and only resorting
    to defection as a corrective measure against persistent collective failure or exploitation.

    It prioritizes achieving the collective bonus `k`. It starts by cooperating, sustains cooperation
    as long as the threshold is met, and gives the group a chance to recover from failures before
    withdrawing its contribution as a signal for necessary change.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.patience_threshold = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        consecutive_failures = 0
        if state.round_number > 0:
            for i in range(state.round_number - 1, -1, -1):
                my_action_in_round_i = history.my_actions[i]
                opponent_cooperators_in_round_i = sum(history.opponent_actions[i, :])
                num_cooperators_in_round_i = opponent_cooperators_in_round_i
                if my_action_in_round_i:
                    num_cooperators_in_round_i += 1
                if num_cooperators_in_round_i < self.game_description.m:
                    consecutive_failures += 1
                else:
                    break
        num_cooperators_prev_round = -1
        my_action_prev_round = False
        if state.round_number > 0:
            prev_round_idx = state.round_number - 1
            my_action_prev_round = history.my_actions[prev_round_idx]
            opponent_cooperators_prev_round = sum(history.opponent_actions[prev_round_idx, :])
            num_cooperators_prev_round = opponent_cooperators_prev_round
            if my_action_prev_round:
                num_cooperators_prev_round += 1
        if current_round == 1:
            return Action.C
        if current_round == self.game_description.n_rounds:
            if consecutive_failures > self.patience_threshold or (num_cooperators_prev_round < self.game_description.m and my_action_prev_round == Action.C.value):
                return Action.D
            else:
                return Action.C
        elif num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        elif consecutive_failures <= self.patience_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_437(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for the Collective Risk Dilemma.
    This strategy fosters cooperation, punishes non-cooperation, and offers forgiveness,
    all while adapting to observed collective behavior. It aims for the highest possible
    collective payoff, which in turn generally maximizes individual long-term payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._state = 'INITIAL'
        self._rounds_in_current_state = 0
        self.PUNISHMENT_DURATION = 1
        self.FORGIVENESS_THRESHOLD = self.PUNISHMENT_DURATION + 1
        self._initial_coop_rounds_param = max(1, min(self.game_description.n_rounds // 4, 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number > 0:
            my_action_last_round = history.my_actions[state.round_number - 1]
            opponent_actions_last_round = history.opponent_actions[state.round_number - 1, :]
            num_cooperators_last_round = int(my_action_last_round) + np.sum(opponent_actions_last_round)
            threshold_met_last_round = num_cooperators_last_round >= m_threshold
            self._rounds_in_current_state += 1
            if self._state == 'INITIAL':
                if self._rounds_in_current_state >= self._initial_coop_rounds_param:
                    if threshold_met_last_round:
                        self._state = 'COOPERATING'
                    else:
                        self._state = 'PUNISHING'
                    self._rounds_in_current_state = 0
            elif self._state == 'COOPERATING':
                if not threshold_met_last_round:
                    self._state = 'PUNISHING'
                    self._rounds_in_current_state = 0
            elif self._state == 'PUNISHING':
                if threshold_met_last_round:
                    self._state = 'COOPERATING'
                    self._rounds_in_current_state = 0
                elif self._rounds_in_current_state >= self.FORGIVENESS_THRESHOLD:
                    self._state = 'FORGIVING'
                    self._rounds_in_current_state = 0
            elif self._state == 'FORGIVING':
                if threshold_met_last_round:
                    self._state = 'COOPERATING'
                else:
                    self._state = 'PUNISHING'
                self._rounds_in_in_current_state = 0
        if current_round_number_1_indexed == total_rounds:
            return Action.D
        if self._state == 'INITIAL':
            return Action.C
        elif self._state == 'COOPERATING':
            return Action.C
        elif self._state == 'PUNISHING':
            return Action.D
        elif self._state == 'FORGIVING':
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_438(BaseStrategy):
    """
    The "Adaptive Collective Contributor" strategy is designed to foster cooperation
    and achieve the collective good, while remaining robust against exploitation
    and adapting to the observed behavior of other players. It prioritizes the
    collective success of meeting the threshold 'm', but acknowledges the
    individual incentives inherent in the game, particularly in the final rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self.failure_streak = 0
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        prev_round_index = current_round_number - 1
        c_count_prev = int(history.my_actions[prev_round_index]) + np.sum(history.opponent_actions[prev_round_index, :])
        if c_count_prev >= self.m:
            self.failure_streak = 0
            return Action.C
        else:
            self.failure_streak += 1
            if self.failure_streak == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_439(BaseStrategy):
    """
    The Adaptive Collective Maintainer (ACM) strategy promotes and sustains cooperation in the
    Collective Risk Dilemma by prioritizing the achievement of the collective bonus 'k'.
    It cooperates in the first round and then adapts its behavior based on whether the
    minimum number of cooperators ('m') was met in the previous round. It explicitly
    resists the typical last-round defection if the collective has been successful.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_threshold = self.game_description.m
        my_prev_action_cooperated = history.my_actions[state.round_number - 1]
        opponent_prev_cooperators_count = np.sum(history.opponent_actions[state.round_number - 1, :])
        total_prev_cooperators = my_prev_action_cooperated + opponent_prev_cooperators_count
        if state.round_number == self.game_description.n_rounds - 1:
            if total_prev_cooperators >= m_threshold:
                return Action.C
            else:
                return Action.D
        elif total_prev_cooperators >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_440(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Forgiveness strategy for the Collective Risk Dilemma.
    This strategy is designed to promote and sustain cooperation towards the collective good,
    while being adaptive to the observed behavior of other players and robust against exploitation.
    It starts by cooperating, maintains cooperation if the collective threshold (m) is met,
    shows forgiveness for a 'near miss' (m-1 cooperators), punishes severe shortfalls by defecting,
    and defects in the final round due to end-game rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialises the strategy with game parameters extracted from the game_description.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the strategy's rules and the game history.
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_number - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_round_cooperated = history.my_actions[prev_round_idx]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_cooperated else 0)
        if total_cooperators_prev_round >= self.m:
            return Action.C
        elif total_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_441(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.
    This strategy aims to initiate and sustain cooperation for the collective good,
    while protecting itself from persistent exploitation and adapting to the
    observed behavior of other players.

    Core Principles:
    - Proactive Cooperation: Initiate cooperation in Round 1.
    - Reciprocal Cooperation: Continue cooperating when the collective successfully meets the threshold.
    - Forgiving Adaptation: Tolerate a single round of collective failure, giving the group a chance.
    - Self-Protective Defection: Withdraw cooperation if the collective consistently fails.
    - Rational Endgame: Defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_idx = state.round_number - 1
        cooperators_from_opponents = np.sum(history.opponent_actions[prev_round_idx, :])
        my_previous_action_was_cooperation = history.my_actions[prev_round_idx]
        total_cooperators_prev_round = cooperators_from_opponents + my_previous_action_was_cooperation
        threshold_met_in_prev_round = total_cooperators_prev_round >= self.game_description.m
        if threshold_met_in_prev_round:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.consecutive_failures >= 2:
            return Action.D
        elif threshold_met_in_prev_round:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_442(BaseStrategy):
    """
    Adaptive Collective Resilience Strategy for the Collective Risk Dilemma.
    This strategy prioritizes establishing and maintaining collective success (meeting the
    cooperation threshold `m`) by demonstrating patience, signaling displeasure at
    persistent failure, and always being willing to return to a cooperative stance. It
    acknowledges rational incentives for individual players, particularly in the endgame,
    to balance collective good with self-preservation in a competitive tournament environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.PATIENCE_THRESHOLD = 2
        self.PUNISHMENT_DURATION = 1
        self.consecutive_failures = 0
        self.is_punishing = False
        self.punishment_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_idx = state.round_number
        total_rounds = self.r
        if current_round_0_idx == 0:
            self.consecutive_failures = 0
            self.is_punishing = False
            self.punishment_rounds_left = 0
            return Action.C
        if current_round_0_idx == total_rounds - 1:
            return Action.D
        my_action_prev_bool = history.my_actions[current_round_0_idx - 1]
        opponent_actions_prev_bool_array = history.opponent_actions[current_round_0_idx - 1, :]
        num_cooperators_last_round = int(my_action_prev_bool) + np.sum(opponent_actions_prev_bool_array)
        if num_cooperators_last_round >= self.m:
            self.consecutive_failures = 0
            self.is_punishing = False
            self.punishment_rounds_left = 0
        else:
            self.consecutive_failures += 1
        current_action_to_return = Action.C
        if self.is_punishing:
            self.punishment_rounds_left -= 1
            if self.punishment_rounds_left == 0:
                self.is_punishing = False
                self.consecutive_failures = 0
                current_action_to_return = Action.C
            else:
                current_action_to_return = Action.D
        elif self.consecutive_failures >= self.PATIENCE_THRESHOLD:
            self.is_punishing = True
            self.punishment_rounds_left = self.PUNISHMENT_DURATION
            current_action_to_return = Action.D
        else:
            current_action_to_return = Action.C
        return current_action_to_return

class Strategy_COLLECTIVE_443(BaseStrategy):
    """
    The Adaptive Collective Trust (ACT) strategy for the Collective Risk Dilemma.

    This strategy aims to maintain collective cooperation by adapting its 'trust_level'
    based on whether the minimum cooperation threshold 'm' was met in the previous round.
    It starts with initial cooperation, reinforces success by increasing trust, and
    punishes failure by decreasing trust (scaled by the magnitude of failure).
    Cooperation decisions are then based on this adaptive trust level, with special
    consideration for the first and final rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.initial_trust = 1.0
        self.alpha_inc = 0.15
        self.alpha_dec_base = 0.3
        self.trust_threshold_cooperate = 0.6
        self.trust_threshold_last_round = 0.7
        self.trust_level = self.initial_trust

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number > 0:
            previous_round_history_index = current_round_number - 1
            cooperators_previous_round = np.sum(history.opponent_actions[previous_round_history_index]) + (1 if history.my_actions[previous_round_history_index] else 0)
            if cooperators_previous_round >= self.m_threshold:
                self.trust_level = min(1.0, self.trust_level + self.alpha_inc * (1.0 - self.trust_level))
            else:
                distance_to_m = self.m_threshold - cooperators_previous_round
                normalized_distance = distance_to_m / self.m_threshold
                self.trust_level = max(0.0, self.trust_level - self.alpha_dec_base * normalized_distance)
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            if self.trust_level >= self.trust_threshold_last_round:
                return Action.C
            else:
                return Action.D
        elif self.trust_level >= self.trust_threshold_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_444(BaseStrategy):
    """
    The Adaptive Collective Enforcement (ACE) strategy for the Collective Risk Dilemma.
    It aims to secure the collective benefit 'k' by prioritizing cooperation,
    but adapts by withdrawing cooperation (defecting) if the collective fails
    to meet the 'm' cooperator threshold for two consecutive rounds, and this player
    has consistently cooperated during those failures. It defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_failed_threshold_consecutive: int = 0
        self.my_consecutive_cooperations_while_failing: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0:
            my_action_last_round = history.my_actions[-1]
            opponent_actions_last_round = history.opponent_actions[-1, :]
            C_last = int(my_action_last_round) + np.sum(opponent_actions_last_round)
            if C_last < self.game_description.m:
                self.rounds_failed_threshold_consecutive += 1
            else:
                self.rounds_failed_threshold_consecutive = 0
            if my_action_last_round is True and C_last < self.game_description.m:
                self.my_consecutive_cooperations_while_failing += 1
            else:
                self.my_consecutive_cooperations_while_failing = 0
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        action_for_this_round: Action
        if C_last >= self.game_description.m:
            action_for_this_round = Action.C
        elif self.rounds_failed_threshold_consecutive >= 2 and self.my_consecutive_cooperations_while_failing >= 1:
            action_for_this_round = Action.D
        else:
            action_for_this_round = Action.C
        return action_for_this_round

class Strategy_COLLECTIVE_445(BaseStrategy):
    """
    The "Resilient Threshold Enforcer" (RTE) strategy for the Collective Risk Dilemma.
    It prioritizes achieving the collective threshold 'm' to secure the group reward 'k'.
    It starts by cooperating and continues to cooperate when the threshold was met in the
    previous round. After a failure to meet the threshold, it attempts to re-establish
    cooperation. Only after 3 or more consecutive failures does it defect for
    self-preservation, recognizing a persistent lack of collective will.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        m = self.game_description.m
        current_consecutive_failures = 0
        for i in range(state.round_number - 1, -1, -1):
            my_action_in_round_i = history.my_actions[i]
            opponent_actions_in_round_i = history.opponent_actions[i, :]
            total_cooperators_in_round_i = int(my_action_in_round_i) + np.sum(opponent_actions_in_round_i)
            if total_cooperators_in_round_i < m:
                current_consecutive_failures += 1
            else:
                break
        self.consecutive_failures = current_consecutive_failures
        my_action_prev = history.my_actions[state.round_number - 1]
        opponent_actions_prev = history.opponent_actions[state.round_number - 1, :]
        n_c_last_round = int(my_action_prev) + np.sum(opponent_actions_prev)
        if n_c_last_round >= m:
            return Action.C
        elif self.consecutive_failures >= 3:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_446(BaseStrategy):
    """
    The Adaptive Collective Stabilizer (ACS) strategy for the Collective Risk Dilemma.

    ACS aims to achieve and maintain the collective benefit of the 'k' reward by
    fostering a cooperative environment. It dynamically adjusts its behavior
    based on the observed collective outcome of the previous round and its own
    prior action. The strategy prioritizes securing the 'm'-cooperator threshold,
    responds to both success and failure, and implements a differential response
    to collective failure that encourages recovery.

    Key principles:
    - Initial cooperation to signal willingness.
    - Defection in the final round (rational self-interest in a finitely repeated game).
    - Sustain cooperation if the collective project succeeded in the previous round.
    - If the project failed:
        - Defect if player i cooperated (punish non-cooperators, prevent exploitation).
        - Cooperate if player i defected (attempt to recover collective effort).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.total_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.total_rounds - 1:
            return Action.D
        my_last_action_bool = history.my_actions[-1]
        my_last_action = Action.C if my_last_action_bool else Action.D
        last_round_num_C = np.sum(history.opponent_actions[-1, :]) + my_last_action_bool
        if last_round_num_C >= self.m:
            return Action.C
        elif my_last_action == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_447(BaseStrategy):
    """
    Adaptive Collective Commitment (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain cooperation for the collective benefit
    by starting cooperatively, rewarding successful collective action, and strategically
    probing after failures to re-initiate cooperation. It is designed to be adaptive
    to various opponent behaviors and robust in a tournament setting where specific
    coordination cannot be assumed.

    Core Principles:
    1. Initial Good Faith: Start by cooperating.
    2. Conditional Cooperation: Maintain cooperation if the threshold was met.
    3. Punishment for Exploitation: Defect if cooperating leads to failure.
    4. Strategic Probing: After a period of collective failure, attempt to re-initiate.
    5. Endgame Rationality: Defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.probe_interval = 2
        self._my_last_action = Action.C
        self._last_round_cooperators = 0
        self._rounds_since_last_success = 0
        self._rounds_since_last_probe = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        my_current_action = None
        if current_round_0_indexed > 0:
            self._my_last_action = Action.C if history.my_actions[-1] else Action.D
            self._last_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if self._last_round_cooperators >= m_threshold:
                self._rounds_since_last_success = 0
                self._rounds_since_last_probe = 0
            else:
                self._rounds_since_last_success += 1
                self._rounds_since_last_probe += 1
        if current_round_0_indexed == 0:
            my_current_action = Action.C
        elif current_round_0_indexed == n_rounds - 1:
            my_current_action = Action.D
        elif self._last_round_cooperators >= m_threshold:
            my_current_action = Action.C
        elif self._my_last_action == Action.C:
            my_current_action = Action.D
        elif self._rounds_since_last_probe >= self.probe_interval:
            my_current_action = Action.C
            self._rounds_since_last_probe = 0
        else:
            my_current_action = Action.D
        return my_current_action

class Strategy_COLLECTIVE_448(BaseStrategy):
    """
    The Collective Steward strategy aims to establish and sustain collective 
    cooperation by proactively cooperating, maintaining cooperation when successful,
    and adaptively responding to failures through re-probing or retraction
    to protect itself from exploitation in a finite tournament environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._num_cooperators_last_round: int | None = None
        self._cooperation_streak: int = 0
        self._defection_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_game_round_1_indexed > 1:
            my_prev_action_bool = history.my_actions[-1]
            opponents_prev_actions_bool = history.opponent_actions[-1, :]
            prev_round_num_cooperators_observed = int(my_prev_action_bool) + np.sum(opponents_prev_actions_bool)
            if prev_round_num_cooperators_observed >= min_cooperators:
                self._cooperation_streak += 1
                self._defection_streak = 0
            else:
                self._defection_streak += 1
                self._cooperation_streak = 0
            self._num_cooperators_last_round = prev_round_num_cooperators_observed
            self._my_last_action = Action.C if my_prev_action_bool else Action.D
        my_action_for_this_round: Action
        if current_game_round_1_indexed == 1:
            my_action_for_this_round = Action.C
        elif current_game_round_1_indexed == total_rounds:
            my_action_for_this_round = Action.D
        elif self._num_cooperators_last_round >= min_cooperators:
            my_action_for_this_round = Action.C
        elif self._defection_streak == 1:
            if self._num_cooperators_last_round == min_cooperators - 1:
                my_action_for_this_round = Action.C
            else:
                my_action_for_this_round = Action.D
        else:
            my_action_for_this_round = Action.D
        return my_action_for_this_round

class Strategy_COLLECTIVE_450(BaseStrategy):
    """
    Adaptive Collective Consensus (ACC) strategy for the Repeated Collective Risk Dilemma.
    This strategy balances initial trust, sustained cooperation, resilience to minor
    failures, robustness against exploitation, and end-game prudence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.max_failure_tolerance = 2
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        if current_round_t == 1:
            return Action.C
        c_count_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if c_count_prev_round >= self.game_description.m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures < self.max_failure_tolerance:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_451(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation and consistently meet the collective
    threshold 'm' to ensure all players receive the 'k' reward. It adapts by
    remembering past outcomes, punishing collective failure after cooperating
    and excessive free-riding, and self-correcting its own free-riding when
    the collective threshold is met but free-riding isn't excessive.
    It cooperates in the first and last rounds as a signal of collective intent.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FAILURE_PATIENCE_THRESHOLD: int = 1
        self.FREE_RIDER_TOLERANCE_THRESHOLD: int = self.game_description.n_players - self.game_description.m
        self.my_action_last_round: Action | None = None
        self.num_cooperators_last_round: int | None = None
        self.num_failures_in_a_row: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed > 0:
            prev_round_index = current_round_0_indexed - 1
            self.my_action_last_round = Action.C if history.my_actions[prev_round_index] else Action.D
            my_cooperation_count = int(history.my_actions[prev_round_index])
            opponent_cooperation_count = np.sum(history.opponent_actions[prev_round_index, :])
            self.num_cooperators_last_round = my_cooperation_count + opponent_cooperation_count
            num_defectors_last_round = n - self.num_cooperators_last_round
        else:
            self.my_action_last_round = None
            self.num_cooperators_last_round = None
            num_defectors_last_round = None
            self.num_failures_in_a_row = 0
        current_action: Action
        if current_round_0_indexed == 0:
            current_action = Action.C
        elif current_round_0_indexed == r - 1:
            current_action = Action.C
        elif self.num_cooperators_last_round < m:
            self.num_failures_in_a_row += 1
            if self.my_action_last_round == Action.C:
                if self.num_failures_in_a_row > self.FAILURE_PATIENCE_THRESHOLD or self.num_cooperators_last_round <= 1:
                    current_action = Action.D
                else:
                    current_action = Action.C
            else:
                current_action = Action.C
        else:
            self.num_failures_in_a_row = 0
            if self.my_action_last_round == Action.C:
                current_action = Action.C
            elif num_defectors_last_round > self.FREE_RIDER_TOLERANCE_THRESHOLD:
                current_action = Action.D
            else:
                current_action = Action.C
        return current_action

class Strategy_COLLECTIVE_452(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) strategy for the Collective Risk Dilemma.
    This strategy initiates cooperation in the first round. In intermediate rounds,
    it cooperates if the collective goal (m cooperators) was met in the previous
    round, and defects otherwise. In the final round, it defects. This aims to
    foster conditional cooperation and prevent exploitation in the end game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_prev_cooperation = 1 if history.my_actions[-1] else 0
        opponent_prev_cooperation = np.sum(history.opponent_actions[-1, :])
        num_cooperators_previous_round = my_prev_cooperation + opponent_prev_cooperation
        if num_cooperators_previous_round >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_453(BaseStrategy):
    """
    Adaptive Collective Guardian (ACG) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and sustain collective cooperation by initiating collaborative
    efforts, maintaining commitment when the collective goal is met, and defensively
    withdrawing support when collective efforts fail. It adapts its behavior based on
    the group's success in meeting the cooperation threshold in the previous round,
    recognizing the unique dynamics of the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_t = state.round_number + 1
        if current_t == 1:
            return Action.C
        elif current_t == n_rounds:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            num_opponent_cooperators = np.sum(history.opponent_actions[previous_round_idx, :])
            my_previous_action_was_cooperation = history.my_actions[previous_round_idx]
            num_total_cooperators_prev_round = num_opponent_cooperators + int(my_previous_action_was_cooperation)
            if num_total_cooperators_prev_round >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_454(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) strategy for the Collective Risk Dilemma.
    This strategy aims to foster and maintain cooperation by initiating cooperation,
    sustaining it when the group demonstrates sufficient cooperation, and
    selectively defecting to signal dissatisfaction and avoid exploitation
    when cooperation repeatedly fails or in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.consecutive_failure_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        if current_0_indexed_round == 0:
            return Action.C
        if current_0_indexed_round == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_0_indexed_round - 1
        cooperators_in_previous_round = np.sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        if cooperators_in_previous_round >= self.m:
            self.consecutive_failure_count = 0
        else:
            self.consecutive_failure_count += 1
        PatienceThreshold = 2
        if self.consecutive_failure_count < PatienceThreshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_455(BaseStrategy):
    """
    The Adaptive Equity Seeker (AES) strategy for the Collective Risk Dilemma.

    AES aims for stable collective cooperation by tracking individual and collective
    performance streaks. It promotes an equitable distribution of free-riding
    opportunities when there's an excess of cooperators, and punishes collective
    failures by defecting to minimize individual losses and signal for re-cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_coop_streak_in_success: int = 0
        self._my_defect_streak_in_success: int = 0
        self._collective_success_streak: int = 0
        self._collective_failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        prev_round_idx = state.round_number - 1
        self._update_streaks(history, prev_round_idx, m)
        if state.round_number == r - 1:
            return Action.D
        if self._collective_failure_streak > 0:
            return Action.D
        else:
            my_action_prev_round_bool = history.my_actions[prev_round_idx]
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            num_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_bool else 0)
            excess_cooperators = num_cooperators_prev_round - m
            if my_action_prev_round_bool == True:
                if excess_cooperators > 0:
                    cooperation_turns_before_defect = max(1, math.ceil(n * 1.0 / (n - m + 1)))
                    if self._my_coop_streak_in_success >= cooperation_turns_before_defect:
                        return Action.D
                    else:
                        return Action.C
                else:
                    return Action.C
            else:
                return Action.C

    def _update_streaks(self, history: PlayerHistory, prev_round_idx: int, m: int):
        """
        Helper method to update all streak variables based on the outcomes
        of the specified previous round.
        """
        my_action_prev_round_bool = history.my_actions[prev_round_idx]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_bool else 0)
        if num_cooperators_prev_round >= m:
            self._collective_success_streak += 1
            self._collective_failure_streak = 0
            if my_action_prev_round_bool:
                self._my_coop_streak_in_success += 1
                self._my_defect_streak_in_success = 0
            else:
                self._my_defect_streak_in_success += 1
                self._my_coop_streak_in_success = 0
        else:
            self._collective_failure_streak += 1
            self._collective_success_streak = 0
            self._my_coop_streak_in_success = 0
            self._my_defect_streak_in_success = 0

class Strategy_COLLECTIVE_456(BaseStrategy):
    """
    The Adaptive Collective Sustainer (ACS) strategy for the Collective Risk Dilemma.
    It aims to foster and maintain collective cooperation by adapting to the previous round's
    collective outcome. The strategy initiates with cooperation, sustains it when the
    collective threshold is met, punishes collective failure by defecting, and defects
    in the final round due to the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_rounds = game_description.n_rounds
        self.min_cooperators_needed = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        cooperators_in_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_in_previous_round >= self.min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_457(BaseStrategy):
    """
    The Adaptive Resilient Contributor strategy aims to foster and maintain collective
    cooperation to meet the threshold 'm' and secure the 'k' bonus for all. It starts
    by signaling cooperation, reinforces successful cooperation, attempts to re-establish
    cooperation after failures, and critically, protects itself from being exploited
    if its cooperative efforts are consistently unrewarded due to widespread defection.
    In the final round, it prioritizes individual payoff as there are no future
    interactions to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_since_my_C_was_ineffective = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == total_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_action_prev = history.my_actions[-1]
        N_C_prev = int(my_action_prev) + sum(history.opponent_actions[-1, :])
        if N_C_prev >= m_threshold:
            self.consecutive_failures_since_my_C_was_ineffective = 0
            if my_action_prev == Action.D.value:
                return Action.D
            else:
                return Action.C
        elif my_action_prev == Action.D.value:
            self.consecutive_failures_since_my_C_was_ineffective = 0
            return Action.C
        else:
            if m_threshold - N_C_prev >= 2:
                self.consecutive_failures_since_my_C_was_ineffective += 1
            else:
                self.consecutive_failures_since_my_C_was_ineffective = 0
            if self.consecutive_failures_since_my_C_was_ineffective >= 3:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_458(BaseStrategy):
    """
    The "Collective First, Punish Failure" strategy for the Collective Risk Dilemma.
    It prioritizes achieving the collective reward 'k' by cooperating initially and
    continuing cooperation if the 'm' cooperator threshold was met in the previous round.
    If the threshold was not met (collective failure), it defects to punish non-cooperation.
    In the final round, it defects due to the endgame effect, maximizing its final payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        previous_round_actions_idx = current_round_0_indexed - 1
        num_cooperators_prev_round = sum(history.opponent_actions[previous_round_actions_idx, :])
        if history.my_actions[previous_round_actions_idx]:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_459(BaseStrategy):
    """
    The Adaptive Collective Sustainer (ACS) strategy for the Collective Risk Dilemma.

    ACS aims to establish and maintain a stable cooperative equilibrium by:
    1.  Starting with cooperation to signal willingness to contribute.
    2.  Sustaining cooperation when the collective threshold 'm' is met.
    3.  Initiating a "retaliation" (defection) phase of fixed duration
        if the collective threshold is not met, especially after this player cooperated.
    4.  Attempting to re-establish cooperation after the retaliation phase,
        demonstrating resilience and a commitment to long-term collective benefit.
    5.  Adjusting behavior in the final round based on the penultimate round's outcome,
        balancing collective good with self-preservation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_intent: bool = True
        self.rounds_in_retaliation: int = 0
        self.max_retaliation_rounds: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            self.cooperation_intent = True
            self.rounds_in_retaliation = 0
            return Action.C
        n_coop_prev = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if current_round_idx == self.game_description.n_rounds - 1:
            if n_coop_prev >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        if n_coop_prev >= self.game_description.m:
            self.cooperation_intent = True
            self.rounds_in_retaliation = 0
            return Action.C
        elif self.cooperation_intent:
            self.cooperation_intent = False
            self.rounds_in_retaliation = 1
            return Action.D
        elif self.rounds_in_retaliation < self.max_retaliation_rounds:
            self.rounds_in_retaliation += 1
            return Action.D
        else:
            self.cooperation_intent = True
            self.rounds_in_retaliation = 0
            return Action.C

class Strategy_COLLECTIVE_460(BaseStrategy):
    """
    Adaptive Collective Punisher (ACP) for the Collective Risk Dilemma.

    This strategy aims to achieve and maintain the collective good (meeting the 'm'
    cooperator threshold) by promoting cooperation, deterring free-riding, and
    providing mechanisms for recovery. It adapts based on observed group behavior,
    acknowledging both collective success and failure, and individual exploitation.

    Core Principles:
    1. Initiate Cooperation: Start by contributing to establish trust.
    2. Sustain Success: Continue cooperating when the collective goal is met.
    3. Punish Free-Riding: React to situations where the collective goal is met,
       but a substantial number of players defected.
    4. Punish Collective Failure: Respond to repeated failures to meet the
       collective goal.
    5. Forgive and Re-engage: Allow for recovery from individual defection or
       limited collective failure.
    6. Endgame Rationality: Defect in the final round due to lack of future interaction.
    """
    FAILURE_PATIENCE = 2
    FREE_RIDER_PATIENCE = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak = 0
        self.free_rider_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed > 0:
            last_round_idx = current_round_number_0_indexed - 1
            my_action_last_round = history.my_actions[last_round_idx]
            opponent_actions_last_round = history.opponent_actions[last_round_idx, :]
            last_round_cooperators_count = np.sum(opponent_actions_last_round).item() + (1 if my_action_last_round else 0)
            if last_round_cooperators_count < m:
                self.failure_streak += 1
                self.free_rider_streak = 0
            else:
                self.failure_streak = 0
                if last_round_cooperators_count < n:
                    self.free_rider_streak += 1
                else:
                    self.free_rider_streak = 0
        if current_round_number_0_indexed == 0:
            return Action.C
        if current_round_number_0_indexed == r - 1:
            return Action.D
        if last_round_cooperators_count < m:
            if my_action_last_round == False:
                return Action.C
            elif self.failure_streak > self.FAILURE_PATIENCE:
                return Action.D
            else:
                return Action.C
        elif self.free_rider_streak > self.FREE_RIDER_PATIENCE:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_461(BaseStrategy):
    """
    Adaptive Collective Nudger (ACN) strategy for the Collective Risk Dilemma.
    This strategy aims to balance collective good (meeting the 'm' threshold) with
    self-preservation by adapting its cooperation based on past collective behavior
    and its own previous action.

    Decision Rules Summary:
    1.  **First Round:** Cooperate (C) to establish a positive baseline.
    2.  **Last Round:** Defect (D) due to end-game effects (no future interactions for punishment/reward).
    3.  **Intermediate Rounds:**
        *   If the collective threshold (`m`) was met in the previous round: Continue to Cooperate (C) to maintain stability.
        *   If the threshold was NOT met:
            *   If the group was only one cooperator short (`C_prev == m - 1`): Cooperate (C) to make a strong push for the threshold.
            *   If the group failed significantly (`C_prev < m - 1`):
                *   If this strategy cooperated in the previous round: Defect (D) to signal dissatisfaction and avoid exploitation.
                *   If this strategy defected in the previous round: Cooperate (C) to try contributing to the collective goal again.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        previous_round_0_indexed = current_round_0_indexed - 1
        my_action_prev_bool = history.my_actions[previous_round_0_indexed]
        opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        C_prev = (1 if my_action_prev_bool else 0) + opponent_cooperators_prev
        if C_prev >= m:
            return Action.C
        elif C_prev >= m - 1:
            return Action.C
        elif my_action_prev_bool:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_462(BaseStrategy):
    """
    Adaptive Collective Stabilizer (ACS) Strategy for the Collective Risk Dilemma.

    This strategy aims to secure the collective reward 'k' by prioritizing cooperation
    to meet the 'm' threshold. It starts cooperatively, attempts to stabilize optimal
    cooperation, and uses a limited "punishment" mechanism (temporary defection) to
    signal distress and encourage more cooperation if the collective repeatedly fails.
    It also prudently free-rides when there's an excess of cooperation, contributing
    to overall efficiency without jeopardizing the 'k' reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_rounds_failed: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_action_prev_round_is_C = history.my_actions[state.round_number - 1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if total_cooperators_prev_round >= self.game_description.m:
            self.num_rounds_failed = 0
        else:
            self.num_rounds_failed += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.num_rounds_failed == 0:
            if total_cooperators_prev_round > self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif self.num_rounds_failed == 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_463(BaseStrategy):
    """
    Adaptive Threshold Maintenance (ATM) strategy for the Collective Risk Dilemma.
    It prioritizes achieving the collective reward by cooperating, but defends against
    persistent exploitation by defecting after consecutive failures to meet the threshold.
    It also defects in the last round to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak = 0
        self.failure_tolerance_limit = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :]) + int(history.my_actions[previous_round_index])
        m_threshold = self.game_description.m
        if num_cooperators_prev_round >= m_threshold:
            self.failure_streak = 0
            return Action.C
        else:
            self.failure_streak += 1
            if self.failure_streak <= self.failure_tolerance_limit:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_464(BaseStrategy):
    """
    Adaptive Collective with Forgiveness strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain collective cooperation by starting with
    good faith cooperation. It provides 'forgiveness' for a limited number of consecutive
    failures to meet the cooperation threshold before resorting to defection as a signal
    of dissatisfaction and to avoid exploitation. Cooperation is resumed if the collective
    goal is met again. In the final round, it defects due to endgame rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds: int = 0
        self.consecutive_failure_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round >= self.game_description.m:
            self.consecutive_failure_rounds = 0
            return Action.C
        else:
            self.consecutive_failure_rounds += 1
            if self.consecutive_failure_rounds >= self.consecutive_failure_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_465(BaseStrategy):
    """
    Dynamic Threshold Enforcer (DTE) strategy for the Collective Risk Dilemma.
    Starts cooperative, tolerates single collective failures, but punishes repeated failures
    by defecting to protect against exploitation. Maintains cooperation when the collective
    good is achieved.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_previous_round = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
        is_last_round = state.round_number == self.game_description.n_rounds - 1
        if is_last_round:
            if num_cooperators_previous_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        elif num_cooperators_previous_round >= self.game_description.m:
            self.consecutive_failures_count = 0
            return Action.C
        else:
            self.consecutive_failures_count += 1
            if self.consecutive_failures_count < 2:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_466(BaseStrategy):
    """
    The Forgiving Collective Assurance (FCA) strategy for the Collective Risk Dilemma.

    FCA prioritizes establishing and sustaining the collective reward (k bonus).
    It initiates by cooperating (Round 1).
    If the collective threshold was met in the previous round, it reinforces this by continuing to cooperate.
    If the collective threshold was missed:
        - If FCA itself cooperated in the previous round, it temporarily defects as a 'punishment' for collective failure.
        - If FCA itself defected in the previous round, it 'forgives' and attempts to re-engage by cooperating.
    In the final round, it defects due to the absence of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        elif state.round_number == n_rounds - 1:
            return Action.D
        else:
            my_action_prev = history.my_actions[-1]
            opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            c_count_prev = int(my_action_prev) + opponent_cooperators_prev
            if c_count_prev >= m:
                return Action.C
            elif my_action_prev == True:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_467(BaseStrategy):
    """
    Adaptive Collective Maximizer (ACM) Strategy:
    Aims to promote and sustain cooperation to meet the collective reward 'm' threshold.
    It initiates cooperation, punishes collective failures with a "warning shot" defection,
    attempts to re-initiate cooperation, and resorts to sustained defection if failures persist.
    It also accounts for end-game effects by defecting in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.fail_recovery_attempts: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self.fail_recovery_attempts = 0
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round >= self.m:
            self.fail_recovery_attempts = 0
            return Action.C
        elif self.fail_recovery_attempts == 0:
            self.fail_recovery_attempts = 1
            return Action.D
        elif self.fail_recovery_attempts == 1:
            self.fail_recovery_attempts = 2
            return Action.C
        else:
            self.fail_recovery_attempts = 3
            return Action.D

class Strategy_COLLECTIVE_468(BaseStrategy):
    """
    The Adaptive Forgiving Enforcer strategy aims to foster and maintain cooperation
    in the Collective Risk Dilemma. It starts with cooperation, sustains it when
    the collective goal (m cooperators) is met, punishes a single failure with
    defection, then forgives by re-cooperating. In the final round, it defects
    to maximize individual payoff due to no future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and internal state variables.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          n_players, n_rounds, m, k.
        """
        self.game_description = game_description
        self._rounds_since_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Forgiving Enforcer strategy rules.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing past actions and
                                            payoffs, or None for the first round.

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_t = state.round_number
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t == 0:
            return Action.C
        if current_round_t == total_rounds_r - 1:
            return Action.D
        if history is not None:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_C = history.my_actions[-1]
            num_my_cooperators_prev_round = 1 if my_action_prev_round_is_C else 0
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + num_my_cooperators_prev_round
            if total_cooperators_prev_round >= min_cooperators_m:
                self._rounds_since_success = 0
            else:
                self._rounds_since_success += 1
        if self._rounds_since_success == 0:
            return Action.C
        elif self._rounds_since_success == 1:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_469(BaseStrategy):
    """
    The Adaptive m-Cooperation (AmC) strategy balances collective success with
    robustness against exploitation. It cooperates in the first round and
    continues to cooperate if the collective threshold 'm' was met in the
    previous round. If the threshold was not met, it defects to signal
    the need for greater group effort. In the final round, it defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the AmC strategy.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for the player
                     and opponents. It is None for the very first round.

        Returns:
            An Action (C for Cooperate, D for Defect).
        """
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        previous_round_index = current_round_number - 1
        my_prev_action_was_cooperate = history.my_actions[previous_round_index]
        opponents_prev_actions_cooperated = np.sum(history.opponent_actions[previous_round_index, :])
        num_cooperators_prev_round = int(my_prev_action_was_cooperate) + int(opponents_prev_actions_cooperated)
        if num_cooperators_prev_round >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_470(BaseStrategy):
    """
    Collective Vigilance strategy for the Collective Risk Dilemma.
    It initiates cooperation, rewards sustained collective success,
    and conditionally punishes collective failures. The strategy aims
    to establish a high-payoff cooperative equilibrium by being
    adaptive and robust to various opponent behaviors in a tournament setting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failed_rounds_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self.failed_rounds_count = 0
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if C_prev >= m:
            self.failed_rounds_count = 0
            return Action.C
        else:
            self.failed_rounds_count += 1
            if self.failed_rounds_count == 1:
                return Action.D
            elif self.failed_rounds_count > 1:
                if self.failed_rounds_count % 2 == 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_471(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) Strategy:

    This strategy aims to foster and maintain cooperation in the Collective Risk Dilemma
    by conditionally contributing to the collective good while protecting itself from
    sustained exploitation. It operates on the principle of collective reciprocation:
    if the collective met its required contribution 'm' in the previous round, it
    contributes (Cooperate); if it failed, it withholds contribution (Defect) to avoid
    individual loss and signal the need for others to adjust their behavior. The last
    round is handled by backward induction (Defect).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        total_cooperators_prev_round = history.my_actions[previous_round_index] + np.sum(history.opponent_actions[previous_round_index, :])
        if total_cooperators_prev_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_472(BaseStrategy):
    """
    Dynamic Collective Enforcement (DCE) strategy for the Collective Risk Dilemma.
    It adapts its behavior based on collective outcomes and individual free-riding,
    transitioning between COOPERATE, PUNISH, and PROBE states to enforce cooperation.
    """
    STATE_COOPERATE = 'COOPERATE'
    STATE_PUNISH = 'PUNISH'
    STATE_PROBE = 'PROBE'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PUNISH_DURATION = self.game_description.m
        self.PROBE_DURATION = 2
        self.FREE_RIDER_TOLERANCE = 1
        self._current_state = self.STATE_COOPERATE
        self._punish_counter = 0
        self._probe_counter = 0
        self._other_player_defection_counts = [0] * (self.game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_idx = state.round_number - 1
        my_prev_action = history.my_actions[prev_round_idx]
        opponent_prev_actions = history.opponent_actions[prev_round_idx, :]
        N_C_prev = int(my_prev_action) + np.sum(opponent_prev_actions)
        if self._current_state == self.STATE_COOPERATE:
            if N_C_prev < self.game_description.m:
                self._current_state = self.STATE_PUNISH
                self._punish_counter = 1
                self._other_player_defection_counts = [0] * (self.game_description.n_players - 1)
            else:
                free_rider_detected = False
                for opponent_idx, action_is_cooperate in enumerate(opponent_prev_actions):
                    if action_is_cooperate == Action.D.value:
                        self._other_player_defection_counts[opponent_idx] += 1
                    else:
                        self._other_player_defection_counts[opponent_idx] = 0
                    if self._other_player_defection_counts[opponent_idx] > self.FREE_RIDER_TOLERANCE:
                        free_rider_detected = True
                        break
                if free_rider_detected:
                    self._current_state = self.STATE_PUNISH
                    self._punish_counter = 1
                    self._other_player_defection_counts = [0] * (self.game_description.n_players - 1)
        elif self._current_state == self.STATE_PUNISH:
            self._punish_counter += 1
            if self._punish_counter >= self.PUNISH_DURATION:
                self._current_state = self.STATE_PROBE
                self._probe_counter = 1
                self._punish_counter = 0
        elif self._current_state == self.STATE_PROBE:
            if N_C_prev >= self.game_description.m:
                self._probe_counter += 1
                if self._probe_counter >= self.PROBE_DURATION:
                    self._current_state = self.STATE_COOPERATE
                    self._probe_counter = 0
            else:
                self._current_state = self.STATE_PUNISH
                self._punish_counter = 1
                self._probe_counter = 0
        if self._current_state == self.STATE_COOPERATE or self._current_state == self.STATE_PROBE:
            return Action.C
        elif self._current_state == self.STATE_PUNISH:
            return Action.D

class Strategy_COLLECTIVE_473(BaseStrategy):
    """
    Collective Enforcement with Forgiveness (CEF) strategy for the Collective Risk Dilemma.
    Aims to maximize collective payoff by establishing cooperation, enforcing it
    through temporary punishment of collective failures, and offering forgiveness
    to prevent irreversible breakdowns, while adapting to the endgame effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.ENDGAME_ROUNDS_TO_DEFECT = 2
        self.PUNISHMENT_DURATION_ROUNDS = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if state.round_number == 0:
            return Action.C
        endgame_start_round_0_indexed = n_rounds - self.ENDGAME_ROUNDS_TO_DEFECT
        if state.round_number >= endgame_start_round_0_indexed:
            return Action.D
        cooperators_per_round_history = []
        for i in range(state.round_number):
            my_action_in_round_i = history.my_actions[i]
            opponent_cooperators_in_round_i = np.sum(history.opponent_actions[i, :])
            total_cooperators_in_round_i = int(my_action_in_round_i) + opponent_cooperators_in_round_i
            cooperators_per_round_history.append(total_cooperators_in_round_i)
        nc_prev = cooperators_per_round_history[-1]
        consecutive_failure_rounds = 0
        for nc_val in reversed(cooperators_per_round_history):
            if nc_val < min_cooperators_needed:
                consecutive_failure_rounds += 1
            else:
                break
        if nc_prev >= min_cooperators_needed:
            return Action.C
        elif consecutive_failure_rounds <= self.PUNISHMENT_DURATION_ROUNDS:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_474(BaseStrategy):
    """
    The "Robust Conditional Cooperator" strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective good by rewarding successful
    cooperation and penalizing collective failure. It starts with an optimistic cooperative
    gesture in the first round, adapts its behavior in intermediate rounds based on the
    outcome of the previous round, and defects in the final round based on backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing the game's parameters:
                              n_players, n_rounds, m (minimum cooperators needed), k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for this player and opponents.
                     It is None for the very first round (state.round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        elif current_round_t == self.game_description.n_rounds:
            return Action.D
        else:
            cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if cooperators_prev_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_475(BaseStrategy):
    """
    Adaptive Threshold Cooperate (ATC) strategy for the Collective Risk Dilemma.
    ATC seeks to establish and maintain a cooperative equilibrium by starting cooperatively,
    rewarding collective success, and offering a limited window of forgiveness for failures
    before resorting to self-protection through defection. It also defects in the last
    round to align with game-theoretic rationality for finite games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.T_fail: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            prev_round_cooperators = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if prev_round_cooperators >= self.game_description.m:
                self.consecutive_failures = 0
                return Action.C
            else:
                self.consecutive_failures += 1
                if self.consecutive_failures <= self.T_fail:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_476(BaseStrategy):
    """
    Collective Tit-for-Tat (CTT) strategy for the Collective Risk Dilemma.

    This strategy aims to foster collective cooperation while protecting individual
    payoffs from collective failure or end-game exploitation.
    - In the first round, it always cooperates to initiate a positive collective outcome.
    - In intermediate rounds, it cooperates if the collective met the minimum cooperation
      threshold (m) in the previous round; otherwise, it defects.
    - In the last round, it always defects to prevent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_rounds = game_description.n_rounds
        self.min_cooperators_needed = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_t = state.round_number + 1
        if current_game_round_t == 1:
            return Action.C
        if current_game_round_t == self.total_rounds:
            return Action.D
        cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_last_round >= self.min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_477(BaseStrategy):
    """
    Adaptive Collective Risk Averter with Forgiveness (ACRA-F) strategy for the Collective Risk Dilemma.

    This strategy aims to foster and sustain collective cooperation, ensuring the 'm' threshold
    for the collective reward 'k' is met. It starts cooperatively, adapts based on past
    collective performance, forgives minor failures, punishes persistent exploitation, and
    periodically attempts to reconcile to re-establish cooperation. It defects in the final
    round for rational self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_count: int = 0
        self.rounds_in_punishment_state: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_t == 0:
            return Action.C
        if current_round_t == n_rounds - 1:
            return Action.D
        my_action_last_round = history.my_actions[-1]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        observed_cooperators_last_round = my_action_last_round + opponent_cooperators_last_round
        if observed_cooperators_last_round >= m:
            self.consecutive_failures_count = 0
            self.rounds_in_punishment_state = 0
        else:
            self.consecutive_failures_count += 1
        if self.consecutive_failures_count == 0:
            return Action.C
        elif observed_cooperators_last_round >= m - 1 or self.consecutive_failures_count <= 2:
            self.rounds_in_punishment_state = 0
            return Action.C
        else:
            self.rounds_in_punishment_state += 1
            if self.rounds_in_punishment_state > 3 and current_round_t < n_rounds - 2 and (observed_cooperators_last_round >= m / 2):
                self.rounds_in_punishment_state = 0
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_478(BaseStrategy):
    """
    The Adaptive Collective Equilibrium Seeker (ACES) strategy for the Collective Risk Dilemma.

    ACES is an optimistic but conditionally cooperative strategy. It aims to establish
    and maintain collective cooperation to achieve the 'k' reward. It starts by cooperating
    and continues to do so as long as the group meets the 'm' cooperation threshold.
    It tolerates a single instance of collective failure (not meeting 'm') by giving
    the group another chance. However, if the group fails to meet the threshold for
    two or more *consecutive* rounds, ACES switches to defecting as a self-preservation
    mechanism and a signal of dissatisfaction. It can return to cooperation if the
    collective effort succeeds again.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.consecutive_failures: int = 0
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        my_previous_action_was_C = history.my_actions[-1]
        num_opponent_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        n_c_previous_round = int(my_previous_action_was_C) + num_opponent_cooperators_previous_round
        m_threshold = self.game_description.m
        if n_c_previous_round >= m_threshold:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures > 1:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_479(BaseStrategy):
    """
    Implements a conditional cooperation strategy. Cooperates in the first round to initiate collective action.
    In intermediate rounds, it cooperates if the collective threshold `m` was met in the previous round,
    otherwise it defects to punish collective failure. Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        previous_round_my_action = history.my_actions[-1]
        previous_round_total_cooperators = previous_round_opponent_cooperators + previous_round_my_action
        if previous_round_total_cooperators >= self.m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_480(BaseStrategy):
    """
    Adaptive Collective Commitment (ACC) strategy for the Collective Risk Dilemma.
    This strategy aims to achieve and sustain collective benefit by being
    initially cooperative, adaptively responding to group performance, and
    robustly deterring exploitation.

    Decision Rules:
    - Round 1 (0-indexed round 0): Cooperate (initial optimistic commitment).
    - Intermediate Rounds (0-indexed rounds 1 to n_rounds - 2):
        Decision is based on the number of cooperators (N_C_prev) in the immediately preceding round:
        - If N_C_prev < m (Collective Failure): Defect. (Signals that cooperation levels are insufficient.)
        - If N_C_prev == m (Bare Minimum Success): Defect. (Signals that free-riding is not acceptable and pushes for more robust cooperation.)
        - If N_C_prev > m (Robust Success): Cooperate. (Sustains the healthy, robust collective effort.)
    - Last Round (0-indexed round n_rounds - 1): Defect (Standard game theory final round rationality).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_index = state.round_number
        if current_round_index == 0:
            return Action.C
        if current_round_index == self.n_rounds - 1:
            return Action.D
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_int = 1 if history.my_actions[-1] else 0
        N_C_prev = opponent_cooperators_prev + my_action_prev_int
        if N_C_prev < self.m:
            return Action.D
        elif N_C_prev == self.m:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_481(BaseStrategy):
    """
    Adaptive Collective Risk Enforcer (ACRE) strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain cooperation by starting cooperatively,
    monitoring the collective outcome, and initiating a temporary "strike" (defection)
    when the collective goal (m cooperators) is not met. It also incorporates a
    forgiveness mechanism to prevent perpetual defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n_rounds = game_description.n_rounds
        self.strike_active: bool = False
        self.strike_rounds_left: int = 0
        self.default_strike_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_N_C = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
        threshold_met_last_round = prev_round_N_C >= self.m
        if threshold_met_last_round:
            self.strike_active = False
            self.strike_rounds_left = 0
        elif not self.strike_active:
            self.strike_active = True
            self.strike_rounds_left = self.default_strike_duration
        else:
            self.strike_rounds_left = max(0, self.strike_rounds_left - 1)
        if self.strike_active and self.strike_rounds_left > 0:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_482(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.

    ACR aims to foster and sustain collective cooperation by:
    1.  Proactively cooperating in the first round to signal willingness.
    2.  Maintaining cooperation as long as the collective threshold 'm' is met
        consistently, or only after a single failure.
    3.  Punishing sustained collective failure (two or more consecutive rounds
        where 'm' was not met) by defecting.
    4.  Forgiving past failures and re-engaging in cooperation once the
        threshold 'm' is met again.
    5.  Defecting in the final round due to endgame rationality.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0
        self.failure_tolerance = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_cooperation_last_round = int(history.my_actions[-1])
        opponent_cooperation_last_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_last_round = my_cooperation_last_round + opponent_cooperation_last_round
        if total_cooperators_last_round < self.game_description.m:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        if self.consecutive_failures >= self.failure_tolerance:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_483(BaseStrategy):
    """
    The Adaptive Reciprocal Contributor (ARC) strategy aims to achieve and sustain the collective reward 'k'
    by promoting cooperation, but it also adapts to collective failures to avoid being exploited.
    It is based on observing the outcome of the previous round (specifically, whether the cooperation
    threshold 'm' was met) and the player's own action in that round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        previous_round_idx = state.round_number - 1
        my_action_prev_bool = history.my_actions[previous_round_idx]
        my_cooperation_count_prev = 1 if my_action_prev_bool else 0
        opponent_cooperation_count_prev = np.sum(history.opponent_actions[previous_round_idx, :])
        C_prev = my_cooperation_count_prev + opponent_cooperation_count_prev
        if C_prev >= m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_484(BaseStrategy):
    """
    The Conditional Collective Reciprocity (CCR) strategy for the Collective Risk Dilemma.
    This strategy starts by cooperating to signal willingness. In intermediate rounds,
    it reciprocates collective success (m or more cooperators in the previous round)
    with continued cooperation, and punishes collective failure with defection.
    In the final round, it defects due to end-game effects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        num_cooperators_opponents_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_was_cooperate = history.my_actions[-1]
        num_cooperators_prev = num_cooperators_opponents_prev + (1 if my_action_prev_was_cooperate else 0)
        if num_cooperators_prev >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_485(BaseStrategy):
    """
    Adaptive Threshold Manager (ATM) strategy for the Collective Risk Dilemma.
    This strategy aims to promote and maintain collective cooperation by adapting
    its behavior based on the group's success in meeting the cooperation threshold,
    while also protecting itself against persistent exploitation.

    It starts by cooperating, gives the group one chance after a failure to meet
    the threshold, and only defects after multiple consecutive failures. It
    accounts for the end-game effect by defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_prev_action_is_c = history.my_actions[prev_round_idx]
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev_round = int(my_prev_action_is_c) + int(opponent_prev_cooperators)
        if num_cooperators_prev_round >= self.m:
            return Action.C
        else:
            consecutive_failures = 0
            for hist_idx in range(prev_round_idx, -1, -1):
                my_hist_action_is_c = history.my_actions[hist_idx]
                opponent_hist_cooperators = np.sum(history.opponent_actions[hist_idx, :])
                cooperators_in_hist_round = int(my_hist_action_is_c) + int(opponent_hist_cooperators)
                if cooperators_in_hist_round < self.m:
                    consecutive_failures += 1
                else:
                    break
            if consecutive_failures == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_486(BaseStrategy):
    """
    Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize the collective benefit (the 'k' reward) by
    promoting cooperation, while being robust to free-riding and strategic defection.
    It operates on the principle of conditional cooperation: cooperate if the collective
    goal (minimum cooperators 'm') was met in the previous round, and defect otherwise.
    It cooperates in the first round to establish a baseline and defects in the last
    round based on backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_C = history.my_actions[-1]
            last_round_cooperators = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
            if last_round_cooperators >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_487(BaseStrategy):
    """
    The Resilient Collective Contributor strategy for the Collective Risk Dilemma.
    This strategy aims to establish and maintain a cooperative equilibrium by consistently
    contributing to the collective good when it is successful, while adaptively reacting
    to collective failures to avoid exploitation. It embodies a "collective mindset" by
    prioritizing the overall success of meeting the threshold, but is also robust enough
    to disengage from costly cooperation if the group consistently fails to achieve the
    common goal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_mode: bool = True
        self.failed_rounds_in_a_row: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_my_cooperation_prev_round = 1 if history.my_actions[state.round_number - 1] else 0
        num_opponent_cooperation_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        N_C_prev_round = num_my_cooperation_prev_round + num_opponent_cooperation_prev_round
        threshold_met_prev_round = N_C_prev_round >= self.game_description.m
        if threshold_met_prev_round:
            self.failed_rounds_in_a_row = 0
            self.cooperation_mode = True
        else:
            self.failed_rounds_in_a_row += 1
            if self.failed_rounds_in_a_row == 1:
                self.cooperation_mode = True
            elif self.failed_rounds_in_a_row >= 2:
                self.cooperation_mode = False
        if state.round_number == self.game_description.n_rounds - 1:
            if self.cooperation_mode:
                return Action.C
            else:
                return Action.D
        elif self.cooperation_mode:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_488(BaseStrategy):
    """
    The Collective Guardian strategy for the Collective Risk Dilemma.

    This strategy aims to foster and maintain collective cooperation to consistently
    meet the 'm' cooperator threshold and unlock the collective reward 'k'.
    It starts by cooperating, and in intermediate rounds, it mimics the collective
    outcome of the previous round: cooperating if the threshold 'm' was met, and
    defecting if it was not, effectively punishing collective failure.
    In the final round, it defects as there are no future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Collective Guardian strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the current round number.
            history: An object containing the history of actions and payoffs for the
                     current player and opponents, or None if it's the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        my_previous_action_was_C = history.my_actions[current_round_number - 1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_number - 1, :])
        total_cooperators_last_round = int(my_previous_action_was_C) + opponent_cooperators_prev_round
        if total_cooperators_last_round < m_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_489(BaseStrategy):
    """
    Stable Collective Maximizer with Punishment: Prioritizes stable cooperation to meet the
    collective threshold (m) and secure the k bonus. It starts cooperatively,
    maintains cooperation during success, and punishes persistent collective failure
    when it has been a consistent cooperator, by defecting. Defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_consecutive_failure_I_cooperated = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_number == 0:
            self.rounds_consecutive_failure_I_cooperated = 0
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        previous_round_index = current_round_number - 1
        my_action_t_prev = history.my_actions[previous_round_index]
        C_t_prev = int(my_action_t_prev) + np.sum(history.opponent_actions[previous_round_index, :])
        if C_t_prev < m:
            if my_action_t_prev == True and self.rounds_consecutive_failure_I_cooperated >= 1:
                self.rounds_consecutive_failure_I_cooperated = 0
                return Action.D
            else:
                if my_action_t_prev == True:
                    self.rounds_consecutive_failure_I_cooperated += 1
                else:
                    self.rounds_consecutive_failure_I_cooperated = 0
                return Action.C
        else:
            self.rounds_consecutive_failure_I_cooperated = 0
            return Action.C

class Strategy_COLLECTIVE_490(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Punishment and Forgiveness (ACRPF)
    
    This strategy aims to foster and maintain collective cooperation to consistently meet the 'm' cooperator threshold,
    thereby securing the 'k' reward for all players.
    
    It initiates cooperation in the first round, sustains it when the collective target ('k' bonus) is met,
    and defects (punishes) when the target is not met to signal consequences and reset cooperation attempts.
    In the final round, it defects to prevent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        cooperating_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_cooperate = int(history.my_actions[-1])
        C_prev = cooperating_opponents_prev_round + my_action_prev_round_is_cooperate
        if C_prev >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_491(BaseStrategy):
    """
    The "Collective Steward" strategy aims to foster and maintain collective cooperation
    to achieve the threshold `m` and secure the collective reward `k`. It does so by
    exhibiting conditional cooperation, rewarding collective success, and punishing
    collective failure, while also accounting for the specific dynamics of the first
    and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_idx - 1
        my_prev_action_is_cooperate = history.my_actions[previous_round_idx]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_prev_action_is_cooperate else 0)
        if num_total_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_492(BaseStrategy):
    """
    Loyal Contributor with Self-Preservation strategy for the Collective Risk Dilemma.
    Aims to secure the collective reward 'k' by prioritizing cooperation,
    while cautiously attempting to free-ride when there's a surplus of cooperators,
    and protecting itself from exploitation by defecting if cooperation consistently fails.
    Defects in the final round due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failed_cooperation_streak: int = 0
        self.successful_cooperation_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_action_prev: bool = history.my_actions[-1]
        C_prev: int = int(my_action_prev) + np.sum(history.opponent_actions[-1, :])
        current_failed_streak: int = self.failed_cooperation_streak
        current_successful_streak: int = self.successful_cooperation_streak
        new_action: Action
        if C_prev < m:
            if my_action_prev:
                current_failed_streak += 1
            else:
                current_failed_streak = 0
            current_successful_streak = 0
            if current_failed_streak >= 2:
                new_action = Action.D
            else:
                new_action = Action.C
        else:
            current_failed_streak = 0
            if my_action_prev:
                current_successful_streak += 1
            else:
                current_successful_streak = 0
            if C_prev == m:
                new_action = Action.C if my_action_prev else Action.D
            elif my_action_prev and current_successful_streak >= 2 and (C_prev > m + 1):
                new_action = Action.D
            else:
                new_action = Action.C if my_action_prev else Action.D
        self.failed_cooperation_streak = current_failed_streak
        self.successful_cooperation_streak = current_successful_streak
        return new_action

class Strategy_COLLECTIVE_493(BaseStrategy):
    """
    Adaptive Collective Maintenance (ACM) strategy for the Collective Risk Dilemma.
    It initiates cooperation, tolerates a fixed number of consecutive collective failures
    (MAX_PATIENCE_LEVEL), and defects in the final round based on backward induction.
    """
    MAX_PATIENCE_LEVEL = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.patience_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m_threshold = self.game_description.m
        r_total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.patience_counter = 0
            return Action.C
        if state.round_number == r_total_rounds - 1:
            return Action.D
        num_cooperators_last_round = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
        if num_cooperators_last_round >= m_threshold:
            self.patience_counter = 0
            return Action.C
        else:
            self.patience_counter += 1
            if self.patience_counter <= self.MAX_PATIENCE_LEVEL:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_494(BaseStrategy):
    """
    Adaptive Threshold Enforcement (ATE) strategy for the Collective Risk Dilemma.

    This strategy aims to establish and maintain a state where the collective reward 'k' is
    consistently achieved. It does so by actively cooperating when the collective goal
    (minimum 'm' cooperators) was met in the previous round, and by signaling dissatisfaction
    (defecting) when the collective goal was missed. This creates a conditional cooperation
    mechanism that punishes collective failure and reinforces collective success.

    Specific rules:
    - Round 1: Cooperates to initiate collective cooperation.
    - Intermediate Rounds (2 to r-1):
        - If the collective goal ('m' cooperators) was met in the previous round, it cooperates
          to sustain the collective benefit.
        - If the collective goal was NOT met in the previous round, it defects to avoid exploitation
          and signal dissatisfaction.
    - Final Round (r): Defects due to no future consequences (backward induction).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as n_players,
                              n_rounds, m (minimum cooperators needed), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Threshold Enforcement strategy.

        Args:
            state: An object containing the current round number.
            history: An object containing the history of actions and payoffs for
                     this player and opponents in previous rounds. It is None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_495(BaseStrategy):
    """
    The Dynamic Collective Reciprocator (DCR) strategy for the Collective Risk Dilemma.
    It initiates cooperation, sustains it when the collective threshold 'm' is met,
    punishes collective failures by temporarily defecting, and proactively attempts
    to re-establish cooperation after prolonged breakdowns. It also defects in the
    final round for individual payoff maximization.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_success = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        threshold_met_prev_round = num_cooperators_prev_round >= min_cooperators_m
        if threshold_met_prev_round:
            self.rounds_since_success = 0
        else:
            self.rounds_since_success += 1
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if threshold_met_prev_round:
            return Action.C
        elif self.rounds_since_success <= 2:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_496(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by always cooperating in the first round
    and continuing to cooperate as long as the collective goal (m cooperators) is met.
    It demonstrates forgiveness by tolerating a single consecutive failure to meet the
    threshold, still cooperating in the subsequent round. However, if two or more
    consecutive rounds fail to meet the threshold, it switches to defecting to protect
    its individual payoff, re-engaging in cooperation if the collective goal is met
    in a future round. It concludes by defecting in the final round based on
    backward induction.
    """
    FAILURE_TOLERANCE = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n_rounds = game_description.n_rounds
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        previous_round_my_action_is_C = history.my_actions[-1]
        previous_round_opponent_cooperators_count = sum(history.opponent_actions[-1, :])
        total_cooperators_previous_round = previous_round_opponent_cooperators_count + int(previous_round_my_action_is_C)
        if total_cooperators_previous_round >= self.m:
            self.consecutive_failures = 0
            return Action.C
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures <= self.FAILURE_TOLERANCE:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_497(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by initiating with a 'Cooperate' move.
    In intermediate rounds, it reinforces cooperation if the collective threshold 'm' was met
    in the previous round, even if it free-rode. If the threshold was not met, it defects
    to protect itself, regardless of its own previous action, signaling dissatisfaction or
    self-preservation in the face of collective failure.
    In the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_action_in_prev_round = history.my_actions[-1]
        opponent_actions_in_prev_round = history.opponent_actions[-1, :]
        num_cooperators_prev_round = sum(opponent_actions_in_prev_round) + int(my_action_in_prev_round)
        if num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_498(BaseStrategy):
    """
    The Adaptive Collective Sustenance (ACS) strategy aims to foster and sustain collective
    cooperation in the Collective Risk Dilemma. It is initially generous, cooperates when
    the collective threshold is met to reinforce success, and defects only when the group
    fails to meet the cooperation threshold, acting as a collective punishment signal.
    This strategy prioritizes the shared benefit of meeting the cooperation threshold
    over individual free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Collective Sustenance strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Collective Sustenance strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for the player and opponents.
                     It is None for the very first round (round_number = 0).

        Returns:
            An Action enum member (Action.C for Cooperate or Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        else:
            my_action_last_round = history.my_actions[state.round_number - 1]
            opponent_actions_last_round = history.opponent_actions[state.round_number - 1, :]
            num_cooperators_last_round = np.sum(opponent_actions_last_round) + int(my_action_last_round)
            m = self.game_description.m
            if num_cooperators_last_round < m:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_499(BaseStrategy):
    """
    Adaptive Forgiving Contributor (AFC) strategy for the Collective Risk Dilemma.

    This strategy aims to consistently achieve the shared reward 'k' by ensuring
    at least 'm' players cooperate each round. It balances an initial willingness
    to contribute with a pragmatic response to persistent collective failure.

    Core Philosophy:
    Starts by signaling cooperation, then adapts its behavior based on the group's
    success in meeting the cooperation threshold 'm'. It is "forgiving" by giving
    the group multiple chances to cooperate after a failure, but eventually becomes
    "less forgiving" if failures persist, to minimize individual losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_count: int = 0
        self.PatienceThreshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            c_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            m_threshold = self.game_description.m
            if c_prev_round >= m_threshold:
                self.consecutive_failures_count = 0
                return Action.C
            else:
                self.consecutive_failures_count += 1
                if state.round_number == self.game_description.n_rounds - 1:
                    return Action.D
                elif self.consecutive_failures_count <= self.PatienceThreshold:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_500(BaseStrategy):
    """
    Adaptive Collective Consensus strategy for the Collective Risk Dilemma.
    This strategy aims to foster and maintain cooperation by being conditionally
    cooperative, and by proactively addressing both collective failures and
    individual free-riding through adaptive punishment mechanisms.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.Patience_Failures = self.m - 1
        self.Patience_FreeRiding = 2
        self.Punish_Rounds = 1
        self.rounds_failed_cooperating_as_C = 0
        self.rounds_tolerated_free_riding_as_C = 0
        self.in_punishment_phase = False
        self.punishment_countdown = 0
        self.my_actual_action_in_prev_round = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self.my_actual_action_in_prev_round = Action.C
            return Action.C
        if current_round_number == self.r - 1:
            return Action.D
        prev_round_idx = current_round_number - 1
        prev_opponent_cooperators_count = sum(history.opponent_actions[prev_round_idx, :])
        my_action_in_prev_round = self.my_actual_action_in_prev_round
        prev_total_cooperators_count = prev_opponent_cooperators_count + (1 if my_action_in_prev_round == Action.C else 0)
        if self.in_punishment_phase:
            self.punishment_countdown -= 1
            if self.punishment_countdown <= 0:
                self.in_punishment_phase = False
                self.rounds_failed_cooperating_as_C = 0
                self.rounds_tolerated_free_riding_as_C = 0
        elif my_action_in_prev_round == Action.C:
            if prev_total_cooperators_count < self.m:
                self.rounds_failed_cooperating_as_C += 1
                self.rounds_tolerated_free_riding_as_C = 0
            else:
                self.rounds_failed_cooperating_as_C = 0
                if prev_total_cooperators_count < self.n:
                    self.rounds_tolerated_free_riding_as_C += 1
                else:
                    self.rounds_tolerated_free_riding_as_C = 0
        else:
            self.rounds_failed_cooperating_as_C = 0
            self.rounds_tolerated_free_riding_as_C = 0
        current_action = Action.C
        if self.in_punishment_phase:
            current_action = Action.D
        elif self.rounds_failed_cooperating_as_C >= self.Patience_Failures:
            self.in_punishment_phase = True
            self.punishment_countdown = self.Punish_Rounds
            current_action = Action.D
        elif self.rounds_tolerated_free_riding_as_C >= self.Patience_FreeRiding:
            self.in_punishment_phase = True
            self.punishment_countdown = self.Punish_Rounds
            current_action = Action.D
        self.my_actual_action_in_prev_round = current_action
        return current_action

class Strategy_COLLECTIVE_501(BaseStrategy):
    """
    The Dynamic Collective Responsibility (DCR) strategy for the Collective Risk Dilemma.

    This strategy aims to promote cooperation by adaptively responding to collective outcomes.
    It starts with cooperation, protests collective failure by defecting (if it cooperated previously),
    takes responsibility for failure by cooperating (if it defected previously), and reinforces
    collective success by maintaining cooperation (or switching to cooperation if it free-rode).
    It accounts for end-game rationality by defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the DCR strategy with game parameters.

        Args:
            game_description: An object containing the game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Dynamic Collective Responsibility strategy rules.

        Args:
            state: Current game state, including the round number. `state.round_number` is 0-indexed.
            history: Player's history of actions and payoffs, and opponents' actions and payoffs.
                     This will be `None` for the very first round (`state.round_number == 0`).

        Returns:
            An `Action` enum member (`Action.C` for Cooperate, `Action.D` for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        r_current = state.round_number + 1
        if r_current == 1:
            return Action.C
        if r_current == n_rounds:
            return Action.D
        my_prev_action_is_C = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        C_prev = int(my_prev_action_is_C) + opponent_cooperators_prev
        if C_prev < m_threshold:
            if my_prev_action_is_C:
                return Action.D
            else:
                return Action.C
        elif my_prev_action_is_C:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_502(BaseStrategy):
    """
    The Threshold-Focused Adaptive Cooperator (TFAC) strategy for the Collective Risk Dilemma.
    This strategy prioritizes achieving the collective reward (k) by initiating cooperation
    and maintaining it when the collective threshold (m) is met. It allows for a single
    collective failure while the player cooperates before temporarily defecting to avoid
    persistent exploitation. It also attempts to restore cooperation if the player defected
    during a collective failure. The strategy employs backward induction, defecting in the
    final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_tolerance_count = 1
        self.consecutive_failures_while_cooperating: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            self.consecutive_failures_while_cooperating = 0
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_action_prev_is_C: bool = history.my_actions[prev_round_idx]
        n_c_prev: int = int(my_action_prev_is_C) + np.sum(history.opponent_actions[prev_round_idx, :])
        if n_c_prev >= m:
            self.consecutive_failures_while_cooperating = 0
            return Action.C
        elif not my_action_prev_is_C:
            self.consecutive_failures_while_cooperating = 0
            return Action.C
        else:
            self.consecutive_failures_while_cooperating += 1
            if self.consecutive_failures_while_cooperating <= self.failure_tolerance_count:
                return Action.C
            else:
                self.consecutive_failures_while_cooperating = 0
                return Action.D

class Strategy_COLLECTIVE_503(BaseStrategy):
    """
    The "Dynamic Cooperative Enforcer" strategy for the Collective Risk Dilemma.

    This strategy aims to balance individual incentives with collective welfare
    by being primarily cooperative, adaptively punishing collective failures,
    and strategically deterring persistent free-riders without jeopardizing
    the collective good. It operates as a finite state machine.

    States:
    - COOPERATE_STEADY: The default state, where the player cooperates.
    - PUNISH_FAILURE: The player defects for a short period to punish a
                      collective failure (threshold `m` not met).
    - DETER_FREE_RIDERS: The player defects for a short period to deter
                         free-riders when the collective threshold is met
                         but cooperation is abundant enough to absorb a
                         temporary defection without missing the threshold.
    """
    COOPERATE_STEADY = 'COOPERATE_STEADY'
    PUNISH_FAILURE = 'PUNISH_FAILURE'
    DETER_FREE_RIDERS = 'DETER_FREE_RIDERS'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_current_state: str = self.COOPERATE_STEADY
        self.rounds_in_current_state: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.my_current_state = self.COOPERATE_STEADY
            self.rounds_in_current_state = 1
            return Action.C
        old_state = self.my_current_state
        old_rounds_in_current_state = self.rounds_in_current_state
        my_prev_action_was_c = history.my_actions[-1]
        opponent_cooperators_prev = sum(history.opponent_actions[-1, :])
        N_c_prev = opponent_cooperators_prev + (1 if my_prev_action_was_c else 0)
        new_state = old_state
        action_to_play = Action.C
        if N_c_prev < self.game_description.m:
            new_state = self.PUNISH_FAILURE
            action_to_play = Action.D
        elif old_state == self.PUNISH_FAILURE:
            if old_rounds_in_current_state == 1:
                new_state = self.PUNISH_FAILURE
                action_to_play = Action.D
            else:
                new_state = self.COOPERATE_STEADY
                action_to_play = Action.C
        elif old_state == self.DETER_FREE_RIDERS:
            if old_rounds_in_current_state == 1:
                new_state = self.DETER_FREE_RIDERS
                action_to_play = Action.D
            else:
                new_state = self.COOPERATE_STEADY
                action_to_play = Action.C
        elif old_state == self.COOPERATE_STEADY and N_c_prev >= self.game_description.m:
            num_defectors_prev_round = self.game_description.n_players - N_c_prev
            if N_c_prev > self.game_description.m and num_defectors_prev_round > 0:
                new_state = self.DETER_FREE_RIDERS
                action_to_play = Action.D
            else:
                new_state = self.COOPERATE_STEADY
                action_to_play = Action.C
        self.my_current_state = new_state
        if self.my_current_state == old_state:
            self.rounds_in_current_state = old_rounds_in_current_state + 1
        else:
            self.rounds_in_current_state = 1
        return action_to_play

class Strategy_COLLECTIVE_504(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) strategy for the Collective Risk Dilemma.
    Aims to establish and maintain cooperation by starting cooperatively,
    rewarding collective success, and deterring consistent underperformance
    through conditional defection, with end-game defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FailureTolerance: int = 1
        self.ConsecutiveFailures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.ConsecutiveFailures = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        n_cooperators_prev_total = np.sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
        if n_cooperators_prev_total >= self.game_description.m:
            self.ConsecutiveFailures = 0
            return Action.C
        else:
            self.ConsecutiveFailures += 1
            if self.ConsecutiveFailures > self.FailureTolerance:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_505(BaseStrategy):
    """
    Adaptive Collective Consensus (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by proactively contributing to the
    collective good when needed, while responsibly attempting to free-ride when it's
    safe and sustainable. It is designed to be robust, adaptive to various opponent
    behaviors, and aligns with a collective mindset initially, becoming more self-protective
    if cooperation consistently fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._round_history_C_counts: list[int] = []
        self._failure_streak: int = 0
        self._my_previous_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        chosen_action: Action
        if current_round_0_indexed > 0:
            my_prev_cooperated = int(history.my_actions[-1])
            opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
            C_prev = my_prev_cooperated + opponent_prev_cooperators
            self._round_history_C_counts.append(int(C_prev))
            self._my_previous_action = Action.C if history.my_actions[-1] else Action.D
        if current_round_0_indexed == 0:
            chosen_action = Action.C
        elif current_round_0_indexed == n_rounds - 1:
            chosen_action = Action.D
        else:
            C_prev = self._round_history_C_counts[current_round_0_indexed - 1]
            if C_prev < m:
                self._failure_streak += 1
                remaining_rounds = n_rounds - (current_round_0_indexed + 1)
                max_failure_tolerance = max(3, min(remaining_rounds, math.floor(n_rounds / 4)))
                if self._failure_streak > max_failure_tolerance:
                    chosen_action = Action.D
                else:
                    chosen_action = Action.C
            else:
                self._failure_streak = 0
                defection_margin = C_prev - m
                if defection_margin == 0:
                    chosen_action = Action.C
                elif self._my_previous_action == Action.C:
                    if defection_margin >= 2:
                        chosen_action = Action.D
                    else:
                        chosen_action = Action.C
                else:
                    chosen_action = Action.D
        self._my_previous_action = chosen_action
        return chosen_action

class Strategy_COLLECTIVE_506(BaseStrategy):
    """
    Adaptive Collective Enforcer (ACE) strategy for the Collective Risk Dilemma.

    The ACE strategy prioritizes achieving and maintaining the collective reward 'k'
    by ensuring 'm' or more cooperators. It starts with cooperation, reinforces
    success, and implements a nuanced punishment/re-try mechanism when cooperation
    falters. It balances individual protection against exploitation with a
    persistent drive towards collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACE strategy with game parameters and internal state.

        Args:
            game_description: An object containing game parameters:
                              n_players (total players), n_rounds (total rounds),
                              m (minimum cooperators needed), k (reward factor).
        """
        self.game_description = game_description
        self.rounds_since_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: GameState object containing the current round number (0-indexed).
            history: PlayerHistory object containing actions and payoffs from previous rounds.
                     It is None for the very first round (state.round_number == 0).

        Returns:
            Action.C for Cooperate or Action.D for Defect.
        """
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        N_C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        my_action_prev_bool = history.my_actions[-1]
        my_action_prev_enum = Action.C if my_action_prev_bool else Action.D
        if N_C_prev >= m:
            self.rounds_since_success = 0
        else:
            self.rounds_since_success += 1
        if self.rounds_since_success == 0:
            return Action.C
        elif self.rounds_since_success == 1:
            if my_action_prev_enum == Action.C:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_507(BaseStrategy):
    """
    Adaptive Trigger with Forgiveness (ATF) strategy for the Collective Risk Dilemma.
    This strategy prioritizes the consistent achievement of the collective reward by
    defaulting to cooperation. It is designed to be highly resilient, forgiving
    individual defections or single instances of collective failure. It only defects
    as a last resort, when the collective has demonstrated persistent failure to meet
    the threshold, signaling a breakdown that necessitates a stronger response.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Trigger with Forgiveness strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators needed), k (reward factor).
        """
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing historical actions and payoffs for
                     this player and opponents up to the previous round.
                     Will be None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        my_action_prev_round = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
        if num_cooperators_prev_round < self.game_description.m:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        punishment_threshold = 2
        if self.consecutive_failures >= punishment_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_508(BaseStrategy):
    """
    The Adaptive Threshold Cooperator (ATC) strategy for the Collective Risk Dilemma.
    It prioritizes achieving collective benefit by initiating cooperation and maintaining it
    as long as the group demonstrates reasonable commitment to the 'm' threshold.
    It adapts by withdrawing cooperation if the threshold is not met for a sustained
    period, defined by `failure_tolerance_rounds`, and defects in the final round
    due to the end-game problem.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_count = 0
        self.failure_tolerance_rounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[state.round_number - 1]
        opponent_prev_actions_are_cooperate = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_in_prev_round = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions_are_cooperate)
        if num_cooperators_in_prev_round < self.game_description.m:
            self.consecutive_failures_count += 1
        else:
            self.consecutive_failures_count = 0
        if self.consecutive_failures_count >= self.failure_tolerance_rounds:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_509(BaseStrategy):
    """
    Adaptive Collective Guardian (ACG) strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation to consistently meet the collective
    threshold 'm' and secure the reward 'k'. It initiates cooperation in the
    first round and defects in the last round. In intermediate rounds, it
    adapts its behavior based on whether the collective threshold was met in
    the immediately preceding round. It cooperates if the threshold was met,
    prioritizing collective stability, and defects if it was not met,
    to avoid exploitation and signal the need for greater cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        previous_round_history_idx = current_round_idx - 1
        my_previous_action_cooperated = history.my_actions[previous_round_history_idx]
        opponents_previous_cooperators = np.sum(history.opponent_actions[previous_round_history_idx, :])
        total_cooperators_in_previous_round = int(my_previous_action_cooperated) + opponents_previous_cooperators
        if total_cooperators_in_previous_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_510(BaseStrategy):
    """
    The Threshold-Seeking Enforcer (TSE) strategy aims to foster and maintain 
    collective cooperation to meet the critical threshold 'm'. It initiates 
    cooperation, consistently contributes when the collective is successful, 
    and withholds cooperation (defects) when the collective fails to meet its goal, 
    thereby incentivizing greater collective effort in subsequent rounds. 
    It also defects in the final round based on backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1])
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_is_cooperate)
        if total_cooperators_prev_round >= min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_511(BaseStrategy):
    """
    The Adaptive Collective Balancer (ACB) strategy for the Collective Risk Dilemma.
    It aims to establish and maintain cooperation by starting cooperatively,
    reacting to observed cooperation levels, incorporating bounded patience for
    collective failures, and a mechanism to prevent excessive free-riding while
    still contributing to the collective.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.forgiveness_threshold = 1
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.r:
            return Action.D
        my_previous_action = history.my_actions[-1]
        num_opponent_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        num_cooperators_previous_round = num_opponent_cooperators_previous_round + (1 if my_previous_action else 0)
        if num_cooperators_previous_round < self.m:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        if self.consecutive_failures > self.forgiveness_threshold:
            return Action.D
        elif my_previous_action == Action.C:
            if num_cooperators_previous_round == self.m:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_512(BaseStrategy):
    """
    Adaptive Forgiving Cooperator (AFC) strategy for the Collective Risk Dilemma.
    This strategy aims to foster and maintain collective cooperation, adapting
    to observed group behavior by conditionally cooperating, punishing insufficient
    cooperation, and offering forgiveness to restart cooperation cycles.
    It takes a "leadership" role by initiating cooperation and a "stewardship" role
    by maintaining it when successful. It also applies backward induction in the
    final round to prevent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.D_streak: int = 0
        self.max_punish_rounds: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        my_prev_action_val = int(history.my_actions[-1])
        opponent_prev_cooperators_sum = sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = my_prev_action_val + opponent_prev_cooperators_sum
        if total_cooperators_prev_round >= min_cooperators_m:
            self.D_streak = 0
            return Action.C
        else:
            self.D_streak += 1
            if self.D_streak >= self.max_punish_rounds:
                self.D_streak = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_1(BaseStrategy):
    """
    The Self-Preserving Opportunist strategy for the Collective Risk Dilemma.
    This strategy prioritizes individual payoff, free-rides when possible,
    and only cooperates when its action is pivotal to achieving the collective reward 'k',
    making the cooperation individually profitable. It exploits the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.r - 1:
            return Action.D
        C_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        met_prev = C_prev >= self.m
        if met_prev:
            return Action.D
        else:
            my_previous_action_was_C = history.my_actions[-1]
            if my_previous_action_was_C:
                return Action.D
            else:
                cooperators_needed = self.m - C_prev
                if cooperators_needed <= 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_2(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    It defaults to defection, aiming to free-ride. It conditionally cooperates
    for a limited duration after collective failures to meet the cooperation
    threshold, to "kick-start" the reward, and then reverts to defection.
    It protects itself from being continuously exploited via a "sucker tolerance" mechanism.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.sucker_rounds_consecutive_failures: int = 0
        self.max_sucker_tolerance: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        my_action_prev_round = history.my_actions[prev_round_index]
        num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :]) + (1 if my_action_prev_round else 0)
        if num_cooperators_prev_round >= self.game_description.m:
            self.sucker_rounds_consecutive_failures = 0
        elif my_action_prev_round:
            self.sucker_rounds_consecutive_failures += 1
        else:
            self.sucker_rounds_consecutive_failures = 0
        if num_cooperators_prev_round >= self.game_description.m:
            return Action.D
        elif self.sucker_rounds_consecutive_failures >= self.max_sucker_tolerance:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_3(BaseStrategy):
    """
    The Opportunistic Exploiter strategy is designed to maximize individual payoff
    by strategically free-riding on others' cooperation when possible, and only
    contributing when its contribution is essential to unlock the collective reward 'k'.
    It also minimizes losses by defecting when collective cooperation is insufficient.

    Decision Rules:
    1.  First Round (state.round_number == 0): Defect (D) to test for free-riding opportunities
        and guarantee a payoff of 1 if the project fails.
    2.  Last Round (state.round_number == total_rounds - 1): Defect (D) as there's no
        future to influence, maximizing immediate personal gain (1 or 1+k).
    3.  Intermediate Rounds (0 < state.round_number < total_rounds - 1):
        a.  If the project succeeded in the previous round (total_cooperators_prev >= m):
            i.  If others alone met or exceeded 'm' (cooperators_among_others_prev >= m):
                Defect (D) to free-ride and achieve 1+k.
            ii. If others were exactly 'm-1' (cooperators_among_others_prev == m - 1):
                Cooperate (C) because own cooperation is critical to achieve 'm',
                yielding 'k' which is greater than '1'.
            iii. If others were less than 'm-1' (cooperators_among_others_prev < m - 1):
                Defect (D) because own cooperation won't save the project,
                guaranteeing '1' instead of '0'.
        b.  If the project failed in the previous round (total_cooperators_prev < m):
            Defect (D) to guarantee '1' instead of '0', as others' cooperation
            is insufficient.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_opponent_cooperated_prev = np.sum(history.opponent_actions[-1, :])
        prev_num_cooperated = num_opponent_cooperated_prev + (1 if my_action_prev else 0)
        if prev_num_cooperated >= self.m:
            cooperators_among_others_prev = num_opponent_cooperated_prev
            if cooperators_among_others_prev >= self.m:
                return Action.D
            elif cooperators_among_others_prev == self.m - 1:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_4(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly exploitative, maximizing its own payoff
    by free-riding whenever possible, and strategically cooperating only when its
    individual contribution is pivotal for securing the collective reward (k),
    which directly benefits the player. It adapts its behavior based on the
    observed number of cooperators in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        Nc_prev_opponents = np.sum(history.opponent_actions[-1, :])
        Nc_prev_my_action = int(history.my_actions[-1])
        Nc_prev = Nc_prev_opponents + Nc_prev_my_action
        if Nc_prev >= self.m:
            return Action.D
        elif Nc_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_5(BaseStrategy):
    """
    The "Adaptive Free-Rider with Calculated Intervention" strategy for the Collective Risk Dilemma.

    This strategy aims to primarily free-ride on the cooperation of others, securing the highest
    individual payoff (1+k), but will strategically cooperate when the collective good (k)
    is at risk and its cooperation can be decisive in meeting the threshold. It seeks to
    exploit others' willingness to cooperate while preventing a total collapse into low payoffs.

    Decision Rules (based on 0-indexed state.round_number and 1-indexed total rounds 'r'):
    1.  First Round (state.round_number == 0): Defect (D) to probe initial inclinations and avoid
        being the first-mover cooperator.
    2.  Last Round (state.round_number == r - 1): Defect (D) as there are no future consequences
        or reputation effects.
    3.  Intermediate Rounds (0 < state.round_number < r - 1):
        a. If the cooperation threshold was met in the previous round (C_prev >= m):
           Action: Defect (D) to free-ride and maximize payoff (1+k) on others' contributions.
        b. If the cooperation threshold was NOT met in the previous round (C_prev < m):
           i. If my cooperation is critical and sufficient to meet the threshold (C_prev + 1 >= m):
              Action: Cooperate (C). This is a calculated intervention to restore the collective
              benefit, as receiving 'k' is better than '0' or '1'.
           ii. Otherwise (C_prev + 1 < m):
               Action: Defect (D) to avoid being a "sucker" whose contribution is wasted if
               it cannot single-handedly meet the threshold. In this case, I receive a payoff of '1'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == total_rounds - 1:
            return Action.D
        cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
        if cooperators_in_prev_round >= m_threshold:
            return Action.D
        elif cooperators_in_prev_round + 1 >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_6(BaseStrategy):
    """
    The Opportunistic Collective Maintainer strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal gain by attempting to free-ride on others'
    cooperation. It dynamically adapts its behavior based on the collective outcome
    of previous rounds. It includes mechanisms to initiate cooperation when its
    contribution is pivotal or to "reset" a completely failed cooperative effort,
    while also safeguarding against persistent exploitation of its own cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.max_reset_attempts: int = 2
        self.my_action_last_round: Action = Action.D
        self.last_action_reason: str = ''
        self.failed_resets_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        num_rounds = self.game_description.n_rounds
        m = self.game_description.m
        action_for_this_round: Action = Action.D
        reason_for_this_round: str = ''
        N_c_last_round: int = 0
        N_c_others_last_round: int = 0
        if current_round > 0:
            N_c_last_round = np.sum(history.opponent_actions[-1, :]).item() + (1 if history.my_actions[-1] else 0)
            if self.my_action_last_round == Action.C and self.last_action_reason == 'RESET_INITIATED' and (N_c_last_round < m):
                self.failed_resets_count += 1
            else:
                self.failed_resets_count = 0
            N_c_others_last_round = np.sum(history.opponent_actions[-1, :]).item()
        if current_round == num_rounds - 1:
            action_for_this_round = Action.D
            reason_for_this_round = 'LAST_ROUND_DEFECTION'
        elif current_round == 0:
            action_for_this_round = Action.D
            reason_for_this_round = 'INITIAL_PROBE'
        elif N_c_last_round >= m:
            action_for_this_round = Action.D
            reason_for_this_round = 'FREE_RIDE_SUCCESS'
        elif N_c_others_last_round == m - 1:
            action_for_this_round = Action.C
            reason_for_this_round = 'PIVOTAL_COOPERATION'
        elif N_c_last_round == 0:
            if self.failed_resets_count < self.max_reset_attempts:
                action_for_this_round = Action.C
                reason_for_this_round = 'RESET_INITIATED'
            else:
                action_for_this_round = Action.D
                reason_for_this_round = 'RESET_ATTEMPTS_EXCEEDED'
        else:
            action_for_this_round = Action.D
            reason_for_this_round = 'INSUFFICIENT_COOPERATION'
        self.my_action_last_round = action_for_this_round
        self.last_action_reason = reason_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_7(BaseStrategy):
    """
    Adaptive Exploiter: This strategy implements an exploitative approach for the Collective Risk Dilemma.
    It primarily defaults to defection to free-ride on others' cooperation. If the collective project
    repeatedly fails to meet the 'm' cooperator threshold, causing everyone to miss the 'k' bonus,
    this strategy temporarily cooperates (initiates a "probe") to try and re-establish collective success.
    As soon as the 'k' bonus is re-established (or the probe duration ends), it reverts to defecting.
    It always defects in the final round as there are no future consequences to consider.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0
        self.rounds_in_current_probe = 0
        self.COOPERATION_PROBE_THRESHOLD = 2
        self.COOPERATION_PROBE_DURATION = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r_total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed > 0:
            prev_round_index = current_round_0_indexed - 1
            prev_num_cooperators = int(np.sum(history.opponent_actions[prev_round_index, :])) + int(history.my_actions[prev_round_index])
            if prev_num_cooperators >= m_threshold:
                self.consecutive_failures = 0
                self.rounds_in_current_probe = 0
            else:
                self.consecutive_failures += 1
                if self.rounds_in_current_probe > 0:
                    self.rounds_in_current_probe += 1
        if current_round_0_indexed == r_total_rounds - 1:
            return Action.D
        if 0 < self.rounds_in_current_probe <= self.COOPERATION_PROBE_DURATION:
            return Action.C
        if self.consecutive_failures >= self.COOPERATION_PROBE_THRESHOLD:
            self.rounds_in_current_probe = 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_8(BaseStrategy):
    """
    The Calculated Free-Rider (CFR) strategy is an adaptive, self-interested approach
    for the Repeated Collective Risk Dilemma. It prioritizes free-riding by defecting
    whenever possible, but strategically cooperates when its contribution is pivotal
    to meet the collective threshold and secure a personal payoff (k) that is greater
    than the payoff from defecting (1) if the project fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        N_expected_others_C = 0
        num_opponents = history.opponent_actions.shape[1]
        for opponent_idx in range(num_opponents):
            opponent_past_actions = history.opponent_actions[:, opponent_idx]
            coop_rate_j = np.mean(opponent_past_actions)
            if coop_rate_j >= 0.5:
                N_expected_others_C += 1
        if N_expected_others_C >= m:
            return Action.D
        elif N_expected_others_C == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_9(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own total payoff by ensuring the collective 
    threshold 'm' is met as often as possible, then immediately leveraging that success to free-ride. 
    It acts as a conditional cooperator, willing to contribute when needed to "kick-start" cooperation 
    or re-establish it after failure, but quick to switch to defection to capture the '1+k' payoff. 
    It also incorporates a "punishment" mechanism to avoid being a perpetual "sucker" if others 
    consistently fail to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.MAX_SUCKER_ROUNDS = 2
        self.PUNISHMENT_ROUNDS = 2
        self.count_C_no_threshold = 0
        self.count_D_punishment = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            self.count_C_no_threshold = 1
            self.count_D_punishment = 0
            return Action.C
        prev_C_count = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1] == Action.C.value:
            prev_C_count += 1
        if prev_C_count >= self.m:
            self.count_C_no_threshold = 0
            self.count_D_punishment = 0
            return Action.D
        elif self.count_D_punishment > 0:
            if self.count_D_punishment < self.PUNISHMENT_ROUNDS:
                self.count_D_punishment += 1
                return Action.D
            else:
                self.count_D_punishment = 0
                self.count_C_no_threshold = 1
                return Action.C
        elif self.count_C_no_threshold >= self.MAX_SUCKER_ROUNDS:
            self.count_D_punishment = 1
            self.count_C_no_threshold = 0
            return Action.D
        else:
            self.count_C_no_threshold += 1
            return Action.C

class Strategy_EXPLOITATIVE_10(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily free-riding
    (defecting) when conditions allow. It strategically cooperates only when
    necessary to prevent prolonged project failure, thereby attempting to
    re-establish a state where free-riding becomes profitable again. The strategy
    monitors the collective behavior of other players and adapts its stance
    according to a "consecutive failures" threshold.

    - **Initialization:** Sets a threshold for tolerating consecutive project failures.
    - **First Round:** Always cooperates as a probe to encourage initial collective action.
    - **Last Round:** Always defects to maximize immediate payoff without future consequences.
    - **Intermediate Rounds:**
        - Monitors the number of cooperators from the previous round.
        - Updates an internal counter for consecutive project failures.
        - **If persistent failure (above threshold):** Switches to permanent defection to cut losses and secure a baseline payoff.
        - **If project succeeded last round:** Defects to free-ride on others' contributions.
        - **If project failed last round (but not persistently):** Cooperates to try and "kickstart" or "sustain" the project, making a strategic investment for future free-riding opportunities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_threshold: int = 2
        self.num_consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_idx = current_round_0_indexed - 1
        opp_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_round_is_C = history.my_actions[prev_round_idx]
        total_cooperators_prev_round = opp_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if total_cooperators_prev_round < m:
            self.num_consecutive_failures += 1
        else:
            self.num_consecutive_failures = 0
        if self.num_consecutive_failures >= self.consecutive_failures_threshold:
            return Action.D
        elif total_cooperators_prev_round >= m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_11(BaseStrategy):
    """
    Adaptive Free-Rider with Failure Avoidance Strategy.

    This strategy aims to maximize individual payoff by defaulting to defection (free-riding).
    However, it includes a mechanism to temporarily cooperate if the collective project
    consistently fails to meet the success threshold ('m' cooperators). This prevents
    prolonged periods of low payoffs for all players, including itself, and then
    reverts to defecting once success is achieved.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._T_FAILURE_THRESHOLD = 2
        self._consecutive_failures_observed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t > 1:
            prev_round_my_action_int = 1 if history.my_actions[-1] else 0
            prev_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
            prev_round_total_cooperators = prev_round_opponent_cooperators + prev_round_my_action_int
            if prev_round_total_cooperators < min_cooperators_m:
                self._consecutive_failures_observed += 1
            else:
                self._consecutive_failures_observed = 0
        if current_round_t == 1:
            my_action = Action.D
        elif current_round_t == total_rounds_r:
            my_action = Action.D
        elif self._consecutive_failures_observed >= self._T_FAILURE_THRESHOLD:
            my_action = Action.C
        else:
            my_action = Action.D
        return my_action

class Strategy_EXPLOITATIVE_12(BaseStrategy):
    """
    The Opportunistic Collective Stabilizer (OCS) strategy aims for the highest possible payoff
    (1+k) by free-riding (defecting when the threshold is met). It cooperates only when its
    single contribution is critical to achieving the threshold 'm', and only if there are
    sufficient future rounds to justify this temporary sacrifice for long-term free-riding.
    It transitions to an exploitative endgame mentality earlier than the last round to avoid
    being a "sucker" in late-game cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_opponent_cooperators = sum(history.opponent_actions[state.round_number - 1, :])
        previous_round_my_action = history.my_actions[state.round_number - 1]
        last_round_cooperators_count = previous_round_opponent_cooperators + previous_round_my_action
        if last_round_cooperators_count >= self.m_threshold:
            return Action.D
        elif last_round_cooperators_count == self.m_threshold - 1 and current_round_1_indexed <= self.n_rounds - 3:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_13(BaseStrategy):
    """
    Adaptive Opportunist with Deterrence strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by largely defecting, but
    will strategically cooperate to ensure the collective k reward is maintained.
    It features a "testing" phase to minimize personal cooperation and a
    "deterrence" phase to prevent endless self-sacrifice if cooperation
    consistently fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action = Action.D
        self._consecutive_failures: int = 0
        self._consecutive_successes: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            my_action_for_this_round = Action.D
            self._my_last_action = my_action_for_this_round
            return my_action_for_this_round
        cooperators_opponents_prev = sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        C_prev = cooperators_opponents_prev + (1 if my_action_prev_round_is_C else 0)
        k_was_achieved_prev_round = C_prev >= m
        if k_was_achieved_prev_round:
            self._consecutive_successes += 1
            self._consecutive_failures = 0
        else:
            self._consecutive_failures += 1
            self._consecutive_successes = 0
        my_action_for_this_round = Action.D
        if current_round_1_indexed == n_rounds:
            my_action_for_this_round = Action.D
        elif self._consecutive_failures >= m:
            my_action_for_this_round = Action.D
        elif k_was_achieved_prev_round:
            if C_prev >= m + 1:
                my_action_for_this_round = Action.D
            elif C_prev == m:
                if self._my_last_action == Action.D:
                    my_action_for_this_round = Action.C
                else:
                    my_action_for_this_round = Action.D
        else:
            my_action_for_this_round = Action.C
        self._my_last_action = my_action_for_this_round
        return my_action_for_this_round

class Strategy_EXPLOITATIVE_14(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal gain by primarily defecting,
    but it uses past rounds' outcomes to make calculated decisions: cooperating
    when its contribution is critical to achieving the threshold (thus enabling
    future free-riding), and defecting when the threshold is either clearly met
    or significantly missed, signaling to others to bear more of the cost.

    Decision Rules (mapping to 0-indexed game rounds):
    1.  Round 0 (initial round): Cooperate (C)
        - Rationale: Initial investment to observe others and establish potential for collective benefit.
    2.  Last Round (state.round_number == self.game_description.n_rounds - 1): Defect (D)
        - Rationale: No future consequences. Maximize immediate payoff by free-riding.
    3.  Intermediate Rounds (0 < state.round_number < self.game_description.n_rounds - 1):
        a.  If the cooperation threshold (`m`) was met in the previous round: Defect (D)
            - Rationale: Free-ride and maximize immediate payoff, exploiting the cooperation of others.
        b.  If the cooperation threshold (`m`) was *not* met in the previous round:
            i.  If the group was close to the threshold (only 1 more cooperator needed, i.e., `m - C_count_prev <= 1`): Cooperate (C)
                - Rationale: Strategic investment to bridge the gap and ensure the collective benefit `k` is achieved, enabling future free-riding.
            ii. If the group was far from the threshold (more than 1 cooperator needed, i.e., `m - C_count_prev > 1`): Defect (D)
                - Rationale: Minimize personal loss (receive 1 instead of 0 if threshold fails again). This also acts as a "punishment" or signal, pressuring others to contribute more.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description (CollectiveRiskDescription): Description of the game.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Current game state, including the round number (0-indexed).
            history (None | PlayerHistory): History of past rounds' actions and payoffs.
                                           None for the very first round (round_number == 0).

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]).item()
        my_action_prev_round = int(history.my_actions[prev_round_idx])
        total_cooperators_prev_round = opponent_cooperators_prev_round + my_action_prev_round
        m = self.game_description.m
        if total_cooperators_prev_round >= m:
            return Action.D
        else:
            gap_to_threshold = m - total_cooperators_prev_round
            if gap_to_threshold <= 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_15(BaseStrategy):
    """
    The Adaptive Free-Rider strategy is designed for the Collective Risk Dilemma.
    It prioritizes maximizing individual payoff by free-riding whenever possible,
    but pragmatically contributes when its cooperation is critical to achieve the
    collective reward 'k' and doing so is individually beneficial (k > 1).

    Decision Rules:
    1.  Round 1 (state.round_number == 0): Cooperate. This initiates with a constructive
        signal, contributing to the possibility of meeting 'm' and securing 'k'.
    2.  Subsequent Rounds (state.round_number > 0):
        The strategy observes the number of cooperators among other players in the
        immediately preceding round (`other_prev_cooperators`).
        - If `other_prev_cooperators >= m`: Defect. Other players already met or
          exceeded the threshold, allowing for free-riding.
        - If `other_prev_cooperators == m - 1`: Cooperate. My contribution is
          precisely what is needed to reach 'm'. Since k > 1, my payoff of 'k'
          (from cooperating) is better than '1' (if I defected and 'k' was missed).
          This is strategic cooperation.
        - If `other_prev_cooperators < m - 1`: Defect. Even with my cooperation,
          the threshold 'm' would not be met, making my contribution futile.
          I defect to keep my endowment.
    This logic applies identically to intermediate and the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_index = state.round_number - 1
        other_prev_cooperators = np.sum(history.opponent_actions[prev_round_index, :])
        if other_prev_cooperators >= self.m:
            return Action.D
        elif other_prev_cooperators == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_16(BaseStrategy):
    """
    Adaptive Free-Rider with Grudging Cooperation (AFRGC) strategy.
    This strategy aims to free-ride by defecting when enough cooperation
    is expected from others to meet the 'm' threshold. When cooperation
    is at risk, it grudgingly cooperates to ensure the collective 'k' bonus
    is achieved, if its own cooperation can make a difference.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.WINDOW_SIZE = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round == 1:
            return Action.C
        if current_round == n_rounds:
            return Action.D
        recent_other_cooperators = []
        num_past_rounds_for_history = state.round_number
        start_index_for_window = max(0, num_past_rounds_for_history - self.WINDOW_SIZE)
        for t in range(start_index_for_window, num_past_rounds_for_history):
            my_action_t_is_cooperate = history.my_actions[t]
            total_opponent_cooperators_t = np.sum(history.opponent_actions[t, :])
            total_cooperators_t = total_opponent_cooperators_t + (1 if my_action_t_is_cooperate else 0)
            cooperators_excluding_self_t = total_cooperators_t - (1 if my_action_t_is_cooperate else 0)
            recent_other_cooperators.append(cooperators_excluding_self_t)
        E_coop_others = np.mean(recent_other_cooperators)
        if E_coop_others >= m:
            return Action.D
        elif E_coop_others + 1 >= m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_17(BaseStrategy):
    """
    Robust Exploiter strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual total payoff by being highly exploitative.
    It defaults to defection, minimizing personal contribution. Cooperation is
    considered only under very specific conditions where it's individually beneficial
    and critically necessary to secure the collective bonus, leveraging the
    cooperation of others while avoiding unnecessary costs.

    Decision Rules:
    1.  Initial Round (t=1, state.round_number=0): Defect (D). No history, safest to free-ride.
    2.  Last Round (t=r, state.round_number=game_description.n_rounds-1): Defect (D).
        No future consequences, standard backward induction.
    3.  Intermediate Rounds (0 < state.round_number < game_description.n_rounds-1):
        Examine previous round's outcome (t-1, state.round_number-1).
        a. If cooperation threshold was NOT met (N_C_prev < m): Defect (D). Avoid contributing to a failing effort.
        b. If cooperation threshold WAS met (N_C_prev >= m):
            i.  If there was "excess" cooperation (N_C_prev > m): Defect (D). Free-ride on buffer.
            ii. If threshold was *barely* met (N_C_prev == m):
                - If I defected last round (my_action_prev == D): Defect (D). Continue free-riding.
                - If I cooperated last round (my_action_prev == C):
                  My cooperation was critical. If I defect now, threshold will fail.
                  - If k > 2: Cooperate (C). My payoff (k-1) is strictly better than 1 (from defecting and causing failure).
                  - If 1 < k <= 2: Defect (D). My payoff 1 (from defecting and causing failure) is equal to or better than (k-1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_idx]
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        n_c_prev = opponent_cooperators_prev + int(my_action_prev)
        if n_c_prev < self.game_description.m:
            return Action.D
        elif n_c_prev > self.game_description.m:
            return Action.D
        elif my_action_prev == Action.D.value:
            return Action.D
        elif self.game_description.k > 2.0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_18(BaseStrategy):
    """
    The Greedy Conditional Cooperator strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly exploitative, adaptive, and robust.
    It aims to maximize its own payoff by free-riding whenever possible, avoiding
    "sucker's payoffs," and only contributing when its contribution is critically
    beneficial for its own returns, specifically leveraging the k > 1 condition.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        r_current = state.round_number + 1
        if r_current == n_rounds:
            return Action.D
        if r_current == 1:
            return Action.D
        N_C_others_last_round = np.sum(history.opponent_actions[-1, :])
        if N_C_others_last_round >= m_threshold:
            return Action.D
        elif N_C_others_last_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_19(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed for the Collective Risk Dilemma.
    It aims to maximize its own payoff by selectively cooperating when its contribution
    is critical to securing the collective good ('k' bonus) and free-riding whenever
    possible, especially when the collective good is already secured by others.
    It adapts its behavior based on the outcome of the previous round, avoiding futile
    cooperation and strategically contributing to ensure the 'k' bonus is met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_round = history.my_actions[prev_round_idx]
        num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + int(my_action_prev_round)
        if num_cooperators_prev_round >= self.m:
            return Action.D
        elif my_action_prev_round == Action.C:
            return Action.D
        else:
            num_cooperators_others_prev_round = num_cooperators_prev_round
            if num_cooperators_others_prev_round == self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_20(BaseStrategy):
    """
    Exploitative Adaptive Pivot Strategy (EAPS) for the Collective Risk Dilemma.

    This strategy aims to maximize personal gain by primarily free-riding on the cooperation
    of others. It only contributes (cooperates) when its contribution is likely to be pivotal
    in achieving the collective reward 'k', or when previous attempts to free-ride have led
    to a collective failure that hurts its own payoff. It strategically defects in the final
    round to avoid any future repercussions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          n_players, n_rounds, m, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) for the current round.

        Args:
            state (GameState): The current game state, including the round number (0-indexed).
            history (None | PlayerHistory): Past actions and outcomes. None for the first round.
        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_round_bool = history.my_actions[prev_round_idx]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_bool else 0)
        success_prev_round = num_cooperators_prev_round >= m
        if success_prev_round:
            return Action.D
        elif my_action_prev_round_bool:
            return Action.D
        elif num_cooperators_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_21(BaseStrategy):
    """
    The Adaptive Minimal Contributor strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its individual payoff by strategically balancing
    cooperation and defection. It initiates cooperation in the first round to
    establish the potential for the collective reward 'k'. In subsequent rounds,
    it free-rides (defects) if the cooperation threshold 'm' was met in the
    previous round, thereby gaining an additional 1 unit of payoff. If the
    threshold was not met, it cooperates to help re-establish the collective
    reward 'k', which is a prerequisite for future free-riding opportunities.
    In the final round, it always defects to maximize its terminal payoff, as
    there are no future interactions to influence.

    This strategy is adaptive as it responds directly to the outcome of the
    previous round and is robust because it actively works to prevent a
    complete collapse of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters like
                                                          n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number.
            history (PlayerHistory | None): An object containing the player's
                                            and opponents' actions and payoffs
                                            from previous rounds. None for round 0.

        Returns:
            Action: The chosen action, Action.C for Cooperate or Action.D for Defect.
        """
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        num_opponent_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_index, :])
        my_action_previous_round_is_C = history.my_actions[previous_round_index]
        num_total_cooperators_previous_round = num_opponent_cooperators_previous_round + int(my_action_previous_round_is_C)
        if num_total_cooperators_previous_round >= min_cooperators_needed:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_22(BaseStrategy):
    """
    The "Adaptive Threshold Investor" strategy for the Collective Risk Dilemma.
    It prioritizes defection to free-ride but strategically cooperates in Round 1,
    and also when it can be a pivotal cooperator to meet the 'm' threshold,
    learning from the previous round's outcome. It always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_previous_action_was_C = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        chosen_action = Action.D
        if current_round_number == 0:
            chosen_action = Action.C
        elif current_round_number == self.n_rounds - 1:
            chosen_action = Action.D
        else:
            my_prev_action_was_C_from_history = history.my_actions[current_round_number - 1]
            opponent_cooperators_prev = np.sum(history.opponent_actions[current_round_number - 1, :])
            N_C_prev = int(my_prev_action_was_C_from_history) + int(opponent_cooperators_prev)
            if N_C_prev >= self.m:
                chosen_action = Action.D
            elif N_C_prev == self.m - 1 and (not self.my_previous_action_was_C):
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        self.my_previous_action_was_C = chosen_action == Action.C
        return chosen_action

class Strategy_EXPLOITATIVE_23(BaseStrategy):
    """
    "The Adaptive Instigator with Calculated Free-Riding" strategy for the Collective Risk Dilemma.
    This strategy is designed to maximize its own total payoff by benefiting from the collective reward 'k'
    while minimizing personal contribution. It strategically initiates cooperation, aggressively free-rides,
    and punishes collective failures to re-establish the conditions for free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_grace_period: int = 1
        self.defection_punishment_rounds: int = 2
        self.my_current_state: str | None = None
        self.rounds_in_current_state: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self.my_current_state = 'INSTIGATE_COOPERATE'
            self.rounds_in_current_state = 1
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        N_C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.my_current_state == 'INSTIGATE_COOPERATE':
            if N_C_prev >= self.m:
                self.my_current_state = 'FREE_RIDE'
                self.rounds_in_current_state = 1
                return Action.D
            else:
                self.rounds_in_current_state += 1
                if self.rounds_in_current_state <= self.cooperation_grace_period + 1:
                    return Action.C
                else:
                    self.my_current_state = 'PUNISH_DEFECTION'
                    self.rounds_in_current_state = 1
                    return Action.D
        elif self.my_current_state == 'FREE_RIDE':
            if N_C_prev >= self.m:
                self.rounds_in_current_state += 1
                return Action.D
            else:
                self.my_current_state = 'PUNISH_DEFECTION'
                self.rounds_in_current_state = 1
                return Action.D
        elif self.my_current_state == 'PUNISH_DEFECTION':
            self.rounds_in_current_state += 1
            if self.rounds_in_current_state <= self.defection_punishment_rounds:
                return Action.D
            else:
                self.my_current_state = 'INSTIGATE_COOPERATE'
                self.rounds_in_current_state = 1
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_24(BaseStrategy):
    """
    The Adaptive Piranha strategy is an opportunistic free-rider. It observes the
    level of cooperation in the group and adjusts its behavior to either exploit
    sufficient cooperation, strategically initiate cooperation when pivotal, or
    retreat to defection when cooperation is futile or too costly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_cooperation_attempts_failed_streak: int = 0
        self.PATIENCE_LIMIT: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_number > 0:
            my_action_prev_round = history.my_actions[current_round_number - 1]
            opponent_cooperators_prev_round = sum(history.opponent_actions[current_round_number - 1, :])
            N_C_prev_total = opponent_cooperators_prev_round + (1 if my_action_prev_round == Action.C else 0)
            if my_action_prev_round == Action.C and N_C_prev_total < min_cooperators_m:
                self.my_cooperation_attempts_failed_streak += 1
            else:
                self.my_cooperation_attempts_failed_streak = 0
        action_for_current_round: Action
        if current_round_number == 0:
            action_for_current_round = Action.C
        elif current_round_number == total_rounds - 1:
            action_for_current_round = Action.D
        elif self.my_cooperation_attempts_failed_streak >= self.PATIENCE_LIMIT:
            action_for_current_round = Action.D
        elif N_C_prev_total >= min_cooperators_m:
            action_for_current_round = Action.D
        elif N_C_prev_total == min_cooperators_m - 1:
            action_for_current_round = Action.C
        else:
            action_for_current_round = Action.D
        return action_for_current_round

class Strategy_EXPLOITATIVE_25(BaseStrategy):
    """
    The Strategic Free-Rider strategy aims to maximize its own total payoff by free-riding on others' cooperation.
    It contributes strategically: only in the first round to initiate collective action, in intermediate rounds
    when its cooperation is pivotal to meet the threshold (since k > 1 makes this individually rational),
    and defects in all other scenarios, including the final round, to exploit others or avoid being a 'sucker'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        previous_other_cooperators = np.sum(history.opponent_actions[-1, :])
        if previous_other_cooperators >= m_threshold:
            return Action.D
        elif previous_other_cooperators == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_26(BaseStrategy):
    """
    Critical Contributor (if beneficial) strategy for the Collective Risk Dilemma.

    This strategy implements an exploitative, adaptive, and robust approach. It aims to
    maximize individual payoff by primarily free-riding, only cooperating under
    specific conditions where its contribution is critical to meeting the 'm' cooperator
    threshold AND securing the collective reward 'k' provides a strictly better individual
    payoff than defecting (i.e., if k > 2).

    Decision Rules:
    1.  **Always Defect if `k <= 2`:** If `k` is not high enough (k-1 <= 1), there's no
        individual incentive to cooperate, even if critical. Defecting always yields 1,
        while cooperating yields at most k-1 (if threshold met) or 0 (if threshold missed).
    2.  **Always Defect in Round 1:** Start by defecting to gather information on opponents'
        behavior and to free-ride initially.
    3.  **Always Defect in the Last Round `r`:** In the final round, there are no future
        interactions. The game becomes a one-shot game, where defecting is the individually
        dominant strategy.
    4.  **For Intermediate Rounds (`1 < t < r`) when `k > 2`:**
        *   **If `C_{t-1} >= m`:** If the threshold was met or exceeded in the previous round,
            it implies others are contributing enough. The strategy free-rides by defecting.
        *   **If `C_{t-1} == m-1`:** If the previous round was exactly one cooperator short of
            the threshold, this player's cooperation alone would make the difference. Since
            `k > 2`, cooperating (yielding k-1) is strictly better than defecting (yielding 1
            because the threshold would be missed). The strategy cooperates.
        *   **If `C_{t-1} < m-1`:** If the previous round was more than one cooperator short
            of the threshold, this player's single cooperation would not be enough to meet 'm'.
            Cooperating would yield 0, while defecting yields 1. The strategy defects to avoid
            futile cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if self.k <= 2:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if state.round_number == 0:
            return Action.D
        num_cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_in_prev_round >= self.m:
            return Action.D
        elif num_cooperators_in_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_27(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy is designed to maximize individual payoff in the Collective Risk Dilemma by
    strategically free-riding while ensuring the collective reward 'k' is achieved often enough to be exploited.
    It balances initial investment, opportunistic defection, and calculated re-engagement to adapt to opponent
    behaviors and exploit cooperative tendencies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if round_t == 1:
            return Action.C
        if round_t == total_rounds:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        C_prev = opponent_cooperators_prev + (1 if my_action_prev_bool else 0)
        if C_prev >= m_threshold:
            return Action.D
        elif not my_action_prev_bool:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_28(BaseStrategy):
    """
    The Greedy Trigger strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by free-riding when
    the collective risk threshold ('m') was met in the previous round.
    It acts as a safety net by cooperating when the threshold was missed,
    thus contributing to restore the potential for the 'k' reward.
    It cooperates in the first round to establish a baseline and defects
    in the final round due to lack of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = int(np.sum(history.opponent_actions[current_round_0_indexed - 1, :]))
        my_action_prev_round_bool = history.my_actions[current_round_0_indexed - 1]
        my_action_prev_round_int = 1 if my_action_prev_round_bool else 0
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_int
        if total_cooperators_prev_round >= m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_29(BaseStrategy):
    """
    The Maverick Free-Rider strategy prioritizes individual payoff. It starts by defecting,
    free-rides when the collective project succeeds, and only cooperates when its contribution
    is critical to meet the threshold (m) and secure a personal payoff of 'k' (since k > 1).
    It also implements a loss-cutting mechanism by switching to permanent defection
    after a sustained period of collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.failure_streak: int = 0
        self.success_streak: int = 0
        self.FAILURE_TOLERANCE_THRESHOLD: int = max(1, math.ceil(self.game_description.n_rounds / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        self.my_last_action = Action.C if history.my_actions[-1] else Action.D
        others_cooperated_prev_round_count = np.sum(history.opponent_actions[-1, :])
        N_C_total_prev = others_cooperated_prev_round_count + (1 if self.my_last_action == Action.C else 0)
        others_last_cooperations = others_cooperated_prev_round_count
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        if self.failure_streak >= self.FAILURE_TOLERANCE_THRESHOLD:
            return Action.D
        if N_C_total_prev >= self.game_description.m:
            self.failure_streak = 0
            self.success_streak += 1
            return Action.D
        else:
            self.success_streak = 0
            self.failure_streak += 1
            if others_last_cooperations == self.game_description.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_30(BaseStrategy):
    """
    An exploitative and adaptive strategy for the Collective Risk Dilemma.
    It defaults to defecting in the first round to observe group behavior.
    In subsequent rounds, it free-rides (defects) if the cooperation threshold
    'm' was met by others in the previous round. It strategically cooperates
    only when its contribution is critical to reach 'm' (and k > 1 makes it
    individually profitable). Otherwise, it defects to avoid futile costs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        cooperators_excluding_me_last_round = history.opponent_actions[-1, :].sum()
        if cooperators_excluding_me_last_round >= self.m:
            return Action.D
        elif cooperators_excluding_me_last_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_31(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma game.
    This strategy aims to maximize individual payoff by primarily free-riding
    on the cooperation of others. It only contributes when its single contribution
    is critically needed to meet the 'm' threshold, and only when doing so
    is demonstrably beneficial (i.e., secures the collective reward 'k' which is > 1)
    compared to continuous defection without 'k'. The strategy explicitly handles
    first and last rounds by defecting due to game-theoretic principles for
    one-shot interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        num_cooperators_prev_round_opponents = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        my_action_prev_round_was_cooperation = history.my_actions[previous_round_0_indexed]
        num_cooperators_prev_round = num_cooperators_prev_round_opponents + int(my_action_prev_round_was_cooperation)
        if num_cooperators_prev_round >= m:
            return Action.D
        else:
            needed_cooperators = m - num_cooperators_prev_round
            if needed_cooperators > 1:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_32(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily defecting (free-riding).
    It adapts to observed collective outcomes:
    - Always defects in the first and last rounds.
    - If the collective project succeeded in the previous round (N_C_prev >= m), it continues to defect,
      exploiting the other players' cooperation to secure the maximum individual payoff (k+1).
    - If the collective project failed in the previous round (N_C_prev < m):
        - If 'k' (reward factor) is high enough (k > 2), it attempts to "bootstrap" cooperation by
          cooperating itself. However, if its previous attempt to cooperate also resulted in failure,
          it reverts to defecting, acting as a "punishment" or withdrawal from futile effort.
        - If 'k' is not high enough (k <= 2), the individual incentive for this strategy to cooperate is low,
          as even successful cooperation (k-1) yields no more than universal defection (1).
          Thus, it stubbornly continues to defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_previous_action: Action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        chosen_action: Action
        if state.round_number == 0:
            chosen_action = Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            chosen_action = Action.D
        else:
            previous_round_my_action_was_C = self.my_previous_action == Action.C
            previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            N_C_prev = previous_round_opponent_cooperators + (1 if previous_round_my_action_was_C else 0)
            if N_C_prev >= self.game_description.m:
                chosen_action = Action.D
            elif self.game_description.k > 2:
                if previous_round_my_action_was_C and N_C_prev < self.game_description.m:
                    chosen_action = Action.D
                else:
                    chosen_action = Action.C
            else:
                chosen_action = Action.D
        self.my_previous_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_33(BaseStrategy):
    """
    The "Balanced Exploiter" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by initiating cooperation to unlock the 'k' reward,
    then free-riding when a sufficient buffer of cooperators exists, or maintaining cooperation when
    essential. It punishes insufficient cooperation and defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if current_round_number_1_indexed == 1:
            return Action.C
        if current_round_number_1_indexed == r:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        C_prev = opponent_cooperators_prev + int(my_action_prev)
        if C_prev >= m:
            if C_prev > m:
                return Action.D
            elif my_action_prev == Action.C:
                return Action.C
            else:
                return Action.D
        elif my_action_prev == Action.C:
            return Action.D
        elif C_prev >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_34(BaseStrategy):
    """
    The Adaptive Exploiter (AEx) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own total payoff by strategically oscillating
    between phases of cooperation and defection. It adapts to the observed collective
    behavior, attempting to foster cooperation to meet the 'm' threshold, then exploiting
    that environment by defecting. If cooperation breaks down, it seeks to re-establish it.
    If consistent cooperation from the group is not observed, it defaults to a self-preserving
    defection state.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n_rounds = game_description.n_rounds
        self.Patience_Limit_Exploring: int = self.m
        self.Reboot_Coop_Rounds: int = 2
        self.my_state: str = 'EXPLORING'
        self.rounds_in_exploring_without_success: int = 0
        self.consecutive_coop_in_punishing: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed: int = state.round_number
        total_rounds_zero_indexed: int = self.n_rounds - 1
        if current_round_zero_indexed == total_rounds_zero_indexed:
            return Action.D
        if current_round_zero_indexed == 0:
            self.my_state = 'EXPLORING'
            self.rounds_in_exploring_without_success = 1
            return Action.C
        prev_round_idx: int = current_round_zero_indexed - 1
        my_prev_action_is_C: np.bool_ = history.my_actions[prev_round_idx]
        num_my_cooperators_prev_round: int = 1 if my_prev_action_is_C else 0
        num_opponent_cooperators_prev_round: int = np.sum(history.opponent_actions[prev_round_idx, :])
        N_C_t_minus_1: int = num_my_cooperators_prev_round + num_opponent_cooperators_prev_round
        if self.my_state == 'EXPLORING':
            if N_C_t_minus_1 >= self.m:
                self.my_state = 'EXPLOITING'
                self.rounds_in_exploring_without_success = 0
                return Action.D
            elif self.rounds_in_exploring_without_success < self.Patience_Limit_Exploring:
                self.rounds_in_exploring_without_success += 1
                return Action.C
            else:
                self.my_state = 'PUNISHING'
                self.rounds_in_exploring_without_success = 0
                return Action.D
        elif self.my_state == 'EXPLOITING':
            if N_C_t_minus_1 >= self.m:
                return Action.D
            else:
                self.my_state = 'EXPLORING'
                self.rounds_in_exploring_without_success = 1
                return Action.C
        elif self.my_state == 'PUNISHING':
            if N_C_t_minus_1 >= self.m:
                self.consecutive_coop_in_punishing += 1
                if self.consecutive_coop_in_punishing >= self.Reboot_Coop_Rounds:
                    self.my_state = 'EXPLOITING'
                    self.consecutive_coop_in_punishing = 0
                    return Action.D
                else:
                    return Action.D
            else:
                self.consecutive_coop_in_punishing = 0
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_35(BaseStrategy):
    """
    The Pragmatic Exploiter (PE) strategy aims to maximize personal payoff by primarily free-riding.
    It defects in the first and last rounds. In intermediate rounds, it defects if the cooperation
    threshold was met in the previous round, or if its cooperation is not critical or not
    sufficiently rewarded (when k <= 2). It cooperates only when k > 2 and its single
    contribution is critical to meet the threshold, as this yields a strictly higher payoff
    than defecting in that specific scenario.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        n_c_observed_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if n_c_observed_prev >= m_threshold:
            return Action.D
        elif k_reward_factor <= 2:
            return Action.D
        else:
            needed_cooperators = m_threshold - n_c_observed_prev
            if needed_cooperators == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_36(BaseStrategy):
    """
    The "Opportunistic Pumper-Dumper" strategy aims to maximize individual payoff
    by strategically alternating between cooperating ("PUMP") to ensure the collective
    reward threshold is met, and defecting ("DUMP") to free-ride once the threshold
    is established. It adapts to observed cooperation levels, re-engaging "PUMP"
    if free-riding causes the collective reward to be missed, and reverting to "DUMP"
    when the collective good is stable. In the final round, it always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_current_strategy_phase = 'PUMP'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        if current_round_number_1_indexed == self.n_rounds:
            return Action.D
        if current_round_number_1_indexed == 1:
            self.my_current_strategy_phase = 'PUMP'
            return Action.C
        my_prev_action_was_cooperate = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        C_prev = int(my_prev_action_was_cooperate) + opponent_prev_cooperators
        if self.my_current_strategy_phase == 'PUMP':
            if C_prev >= self.m:
                self.my_current_strategy_phase = 'DUMP'
                return Action.D
            else:
                return Action.C
        elif self.my_current_strategy_phase == 'DUMP':
            if C_prev >= self.m:
                return Action.D
            else:
                self.my_current_strategy_phase = 'PUMP'
                return Action.C

class Strategy_EXPLOITATIVE_37(BaseStrategy):
    """
    The Calculated Free-Rider strategy prioritizes individual payoff maximization through calculated
    free-riding and strategic, minimal cooperation when individually advantageous. It leverages
    observed history to predict opponent behavior and make optimal decisions.

    Core Principles:
    1. Prioritize Self-Interest: Always choose the action that maximizes the player's own payoff
       for the current round, given the likely collective outcome.
    2. Free-Ride on Success: If the collective threshold is met by others, always defect to gain
       the private endowment bonus in addition to the collective reward.
    3. Strategic Contribution: Only cooperate if the player's individual contribution is *critical*
       to reaching the collective threshold AND results in a higher personal payoff than defecting.
       This is a self-interested investment.
    4. Avoid Waste: Do not cooperate if the contribution is unlikely to meet the threshold, as this
       would result in a pure loss of the endowment.
    5. Exploit Endgame: In the final round, with no future interactions to influence, revert to
       immediate self-interest by defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        prev_round_0_indexed = state.round_number - 1
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_0_indexed, :])
        my_action_prev_was_C = history.my_actions[prev_round_0_indexed]
        C_prev = num_opponent_cooperators_prev + (1 if my_action_prev_was_C else 0)
        if C_prev >= self.m:
            return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_38(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy aims to maximize its own total payoff in the Collective Risk Dilemma
    by strategically navigating the tension between individual incentive to defect and the collective need
    for cooperation. It prioritizes defection, free-rides when possible, contributes only when critical
    to secure the collective benefit, and attempts a strategic restart after failure only if its contribution
    can realistically make a difference.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_round = history.my_actions[prev_round_idx]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round
        if my_action_prev_round == Action.C:
            total_cooperators_prev_round += 1
        if total_cooperators_prev_round >= m_threshold:
            if num_opponent_cooperators_prev_round >= m_threshold:
                return Action.D
            elif num_opponent_cooperators_prev_round == m_threshold - 1:
                return Action.C
        elif my_action_prev_round == Action.D:
            if total_cooperators_prev_round + 1 >= m_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_39(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma.
    It prioritizes free-riding on the cooperation of others,
    contributes only when that contribution is pivotal to
    reaching the collective reward threshold, and strictly
    defects in the first and final rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        C_others_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        if C_others_prev >= self.m_threshold:
            return Action.D
        elif C_others_prev == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_40(BaseStrategy):
    """
    The "Calculated Free-Rider" (CFR) strategy aims to maximize its own cumulative payoff
    by free-riding on the cooperation of others whenever the collective reward is active.
    It strategically cooperates only when absolutely necessary to re-establish the
    collective reward, acting as an investment, and defects otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds = self.n_rounds
        if current_round_zero_indexed == total_rounds - 1:
            return Action.D
        if current_round_zero_indexed == 0:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round >= self.m:
            return Action.D
        elif num_cooperators_last_round + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_41(BaseStrategy):
    """
    Greedy Contributor strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by defaulting to defection
    and free-riding. It only cooperates if its contribution is pivotal to
    reaching the collective reward threshold 'm' AND the individual payoff
    from cooperation (k) is greater than defecting (1) in that specific scenario.
    It avoids futile cooperation that would result in a 0 payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        N_C_others_prev = np.sum(history.opponent_actions[-1, :]).item()
        N_C_others_expected = N_C_others_prev
        if N_C_others_expected >= self.m:
            return Action.D
        elif N_C_others_expected == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_42(BaseStrategy):
    """
    Greedy Adaptive Free Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by dynamically assessing
    when its cooperation is critical to meet the threshold 'm' (and thus secure
    the 'k' reward) and when it can safely defect and free-ride.

    Decision Rules:
    1.  Last Round: Always Defect (exploits end-game effect).
    2.  First Round: Always Defect (probes for cooperators without initial cost).
    3.  Intermediate Rounds:
        a. If enough other players cooperated in the previous round (>= m), free-ride by Defecting.
        b. If exactly m-1 other players cooperated in the previous round, Cooperate to secure the 'k' reward.
        c. If too few other players cooperated in the previous round (< m-1), Defect to avoid futile contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        num_others_cooperated_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        if num_others_cooperated_prev_round >= self.m:
            return Action.D
        elif num_others_cooperated_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_43(BaseStrategy):
    """
    The 'Adaptive Opportunist' strategy in the Collective Risk Dilemma.
    It cooperates in the first round to probe the environment, defects in the last round
    to free-ride, and in middle rounds, it adapts its behavior based on
    whether its cooperation was essential or futile in the previous round, or
    if others are cooperating enough for it to free-ride.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveOpportunist strategy with game parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, m (minimum cooperators), k (reward factor).
        """
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for
                     this player and opponents, or None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        c_self_prev = int(history.my_actions[-1])
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        C_count_prev = C_others_prev + c_self_prev
        m_threshold = self.m
        if C_others_prev >= m_threshold:
            return Action.D
        elif C_count_prev >= m_threshold:
            return Action.C
        elif C_count_prev + 1 >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_44(BaseStrategy):
    """
    The Conditional Exploiter (CE) strategy aims to maximize individual payoff
    by strategically defecting whenever possible. It prioritizes free-riding
    but will conditionally cooperate when its contribution is pivotal and
    the collective reward 'k' makes cooperation individually rational, preventing
    a total collapse that would be worse. It defects in the first and final rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        c_prev = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if c_prev >= self.m:
            return Action.D
        if c_prev == self.m - 1:
            if self.k > 2:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_45(BaseStrategy):
    """
    Adaptive Opportunistic Free-Rider (AOFR) strategy for the Collective Risk Dilemma.

    This strategy is designed to be highly self-interested, adaptive to the collective behavior
    of other players, and robust across a range of game parameters. It prioritizes defecting
    to gain the maximum individual payoff ($1+k$) whenever the collective reward ($k$) is achieved,
    and only cooperates under very specific, individually advantageous conditions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if self.k <= 2:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        my_previous_action_is_cooperate = history.my_actions[previous_round_0_indexed]
        num_cooperators_prev_opponents = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        num_cooperators_previous_round = num_cooperators_prev_opponents + (1 if my_previous_action_is_cooperate else 0)
        threshold_met_prev = num_cooperators_previous_round >= self.m
        if threshold_met_prev:
            return Action.D
        else:
            cooperators_needed = self.m - num_cooperators_previous_round
            if cooperators_needed == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_46(BaseStrategy):
    """
    "Adaptive Free-Rider with Endgame Exploitation" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily defecting. It strategically
    cooperates only when necessary to 'rescue' the collective effort from failing to meet
    the cooperation threshold 'm', allowing for a return to free-riding in subsequent rounds.
    It aggressively exploits the game's final rounds by always defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed >= self.n_rounds - 2:
            return Action.D
        last_round_idx = current_round_0_indexed - 1
        my_last_action_cooperated = history.my_actions[last_round_idx]
        num_cooperators_last_round = np.sum(history.opponent_actions[last_round_idx, :])
        if my_last_action_cooperated:
            num_cooperators_last_round += 1
        last_round_threshold_met = num_cooperators_last_round >= self.m
        if last_round_threshold_met:
            return Action.D
        elif not my_last_action_cooperated:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_47(BaseStrategy):
    """
    The Pivotal Free-Rider strategy defaults to defection, aiming to free-ride on others' cooperation.
    It only cooperates when its single action is pivotal to reaching the collective cooperation
    threshold 'm' in intermediate rounds, thereby securing the collective reward 'k' and achieving
    a net positive personal gain. It always defects in the first and final rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        elif current_round_1_indexed == n_rounds:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            c_prev = sum(previous_round_opponent_actions)
            if c_prev == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_48(BaseStrategy):
    """
    The CalculatedFreeRider strategy is designed to be exploitative by maximizing its own payoff.
    It prioritizes free-riding whenever possible, only cooperating when its contribution is
    individually optimal, typically when it is critical to achieving the collective reward 'k'.
    It adapts based on the most recent observed outcome, assuming persistence in others' behavior
    for the immediate next round.

    Decision rules:
    - Round 1 (initial round): Always Defect ('D') to probe for free-riding opportunities
      and observe opponent behavior without personal cost.
    - Last Round (round 'r'): Always Defect ('D') as there are no future rounds to influence,
      making defection the individually dominant strategy in this final one-shot interaction.
    - Intermediate Rounds (1 < current_round < r):
        - Predicts others' cooperation based on the total cooperators in the previous round.
        - Calculates expected personal payoff if playing Cooperate ('C') this round.
        - Calculates expected personal payoff if playing Defect ('D') this round.
        - Chooses 'C' only if it leads to a strictly higher expected personal payoff.
        - Otherwise (including ties), defaults to 'D' (preferring the less costly option).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D
        last_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        last_round_my_action_was_C = history.my_actions[-1]
        total_cooperators_t_minus_1 = last_round_opponent_cooperators + int(last_round_my_action_was_C)
        expected_payoff_if_C = 0.0
        if total_cooperators_t_minus_1 + 1 >= self.m_threshold:
            expected_payoff_if_C = self.k_reward
        else:
            expected_payoff_if_C = 0.0
        expected_payoff_if_D = 0.0
        if total_cooperators_t_minus_1 >= self.m_threshold:
            expected_payoff_if_D = 1.0 + self.k_reward
        else:
            expected_payoff_if_D = 1.0
        if expected_payoff_if_C > expected_payoff_if_D:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_49(BaseStrategy):
    """
    Adaptive Free-Rider with Futility Check (AFR-F) Strategy for Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding whenever possible.
    It defects in the first and last rounds. In intermediate rounds, it observes
    the previous round's outcome. If the collective reward (k) was achieved, it defects
    to free-ride. If the reward was not achieved, it will cooperate to try and restore
    the collective good, unless its own cooperation in the previous round proved futile
    and the failure has been sustained for at least two consecutive rounds, in which
    case it also defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_m_not_met_count: int = 0
        self.my_cooperation_was_futile_prev: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        my_action_prev_round_bool = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        C_count_prev = opponent_cooperators_prev + int(my_action_prev_round_bool)
        m_met_prev_round = C_count_prev >= m
        if m_met_prev_round:
            self.consecutive_m_not_met_count = 0
            self.my_cooperation_was_futile_prev = False
        else:
            self.consecutive_m_not_met_count += 1
            self.my_cooperation_was_futile_prev = my_action_prev_round_bool
        if state.round_number == n_rounds - 1:
            return Action.D
        if m_met_prev_round:
            return Action.D
        elif self.my_cooperation_was_futile_prev and self.consecutive_m_not_met_count >= 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_50(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.
    This strategy prioritizes defecting to free-ride on others' cooperation.
    It only cooperates under specific conditions: if its contribution is
    the marginal one needed to meet the 'm' threshold AND if that
    cooperation is individually profitable (k > 2). It defects in the
    first and last rounds, and whenever the threshold was met previously
    or its cooperation wouldn't make a difference/isn't profitable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_number - 1, :])
            my_action_prev_round_was_cooperate = history.my_actions[current_round_number - 1]
            num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_cooperate else 0)
            if num_cooperators_prev_round >= self.m:
                return Action.D
            elif self.k > 2:
                if num_cooperators_prev_round == self.m - 1:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_EXPLOITATIVE_51(BaseStrategy):
    """
    Adaptive Opportunist (A-Ops) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by exploiting cooperation
    when it's established, initiating it when necessary for future exploitation,
    and punishing when our contributions are taken for granted. It always
    defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0
        self.consecutive_successes = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_number == 0:
            return Action.C
        prev_round_idx = current_round_number - 1
        my_action_prev_bool = history.my_actions[prev_round_idx]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        N_C_prev = int(my_action_prev_bool) + num_opponent_cooperators_prev
        if N_C_prev >= m:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        if current_round_number == n_rounds - 1:
            return Action.D
        if N_C_prev >= m:
            return Action.D
        else:
            threshold_failures = max(3, math.ceil(n_rounds / 4))
            if self.consecutive_failures >= threshold_failures:
                return Action.D
            elif not my_action_prev_bool:
                if N_C_prev >= m - 1:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_EXPLOITATIVE_52(BaseStrategy):
    """
    The "Adaptive Free-rider with Learned Contribution Threshold" strategy aims
    to maximize individual payoff by opportunistically free-riding, strategically
    contributing only when critically effective and individually profitable,
    and quickly reverting to defection when contributions are futile.
    It balances the desire for the collective reward 'k' with an exploitative
    aversion to personal sacrifice.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_num_1_indexed = state.round_number + 1
        if current_round_num_1_indexed == n_rounds:
            return Action.D
        if current_round_num_1_indexed == 1:
            return Action.C
        prev_round_index = state.round_number - 1
        n_cooperators_prev = np.sum(history.opponent_actions[prev_round_index, :]) + (1 if history.my_actions[prev_round_index] else 0)
        my_action_prev = history.my_actions[prev_round_index]
        if n_cooperators_prev >= m:
            return Action.D
        else:
            needed_for_m = m - n_cooperators_prev
            if needed_for_m == 1:
                if my_action_prev == True:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_53(BaseStrategy):
    """
    Adaptive Opportunistic Defector (AOD) Strategy for Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily defecting.
    It strategically cooperates only when the collective reward 'k' has been
    consistently missed for a predefined number of consecutive rounds (S_threshold),
    indicating a potential need for its contribution.
    However, if its cooperation proves ineffective (i.e., the 'm' threshold is still
    not met), it immediately reverts to defecting to avoid a "sucker's payoff."
    The strategy always defects in the first and final rounds to exploit lack of
    history or future consequences, respectively.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.S_threshold = 2
        self.rounds_since_last_success = 0
        self.my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_0_indexed == 0:
            my_action = Action.D
            self.my_last_action = my_action
            return my_action
        if current_round_0_indexed == total_rounds - 1:
            my_action = Action.D
            self.my_last_action = my_action
            return my_action
        cooperators_in_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_in_previous_round >= m:
            self.rounds_since_last_success = 0
        else:
            self.rounds_since_last_success += 1
        my_tentative_action = Action.D
        if self.rounds_since_last_success >= self.S_threshold:
            my_tentative_action = Action.C
        my_action = my_tentative_action
        if self.my_last_action == Action.C and cooperators_in_previous_round < m:
            my_action = Action.D
        self.my_last_action = my_action
        return my_action

class Strategy_EXPLOITATIVE_54(BaseStrategy):
    """
    Adaptive Exploiter with Learning Threshold: Prioritizes defection to maximize individual gain.
    Temporarily cooperates only when the collective good (k bonus) is jeopardized and my cooperation
    might help restore it, paving the way for future exploitation. This temporary cooperation
    is bounded by a "patience" threshold to avoid being consistently exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.max_futile_coop_rounds: int = 2
        self._my_last_action: Action = Action.D
        self._consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number: int = state.round_number
        n_rounds: int = self.game_description.n_rounds
        m: int = self.game_description.m
        action_to_take: Action = Action.D
        if current_round_number == 0:
            action_to_take = Action.D
        elif current_round_number == n_rounds - 1:
            action_to_take = Action.D
        else:
            prev_round_my_action_bool: bool = history.my_actions[-1]
            prev_round_opponent_actions_bool: NDArray[np.bool_] = history.opponent_actions[-1, :]
            prev_round_total_cooperators: int = np.sum(prev_round_opponent_actions_bool) + (1 if prev_round_my_action_bool else 0)
            if prev_round_my_action_bool == True and prev_round_total_cooperators < m:
                self._consecutive_failures += 1
            else:
                self._consecutive_failures = 0
            if self._consecutive_failures >= self.max_futile_coop_rounds:
                action_to_take = Action.D
            elif prev_round_total_cooperators >= m:
                action_to_take = Action.D
            else:
                action_to_take = Action.C
        self._my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_55(BaseStrategy):
    """
    The Conditional Contributor & Free-Rider (CCFR) strategy is designed to maximize
    individual payoff in the Collective Risk Dilemma. It strategically balances cooperation
    (to ensure the collective reward k is often met) with defection (to free-ride on others'
    contributions). It adapts to the observed level of cooperation in the preceding round,
    leveraging its own action to either secure the public good when critical or free-ride
    when safe.

    Core Principles:
    1. Initial Probe: Cooperate in the first round to gauge collective willingness.
    2. Backward Induction: Defect in the final round.
    3. Conditional Free-Riding: Defect when the threshold 'm' is sufficiently exceeded.
    4. Critical Contribution: Cooperate when the strategy's contribution is essential to
       meet or maintain the threshold 'm'.
    5. Punish Widespread Defection: Defect when cooperation is too low to reach 'm',
       to preserve private payoff and signal loss of 'k'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m_threshold = self.game_description.m
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        my_prev_action_cooperated = int(history.my_actions[state.round_number - 1])
        opponent_prev_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
        C_prev = my_prev_action_cooperated + opponent_prev_cooperators
        if C_prev >= m_threshold + 1:
            return Action.D
        if C_prev == m_threshold or C_prev == m_threshold - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_56(BaseStrategy):
    """
    Calculated Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by balancing strategic cooperation
    to secure the collective 'k' bonus with free-riding when the bonus is secured.
    It cooperates in the first round and defects in the last. In intermediate rounds,
    it defects if the threshold was met in the previous round, or if it cooperated
    previously and the threshold was still missed. It cooperates if it defected
    previously and the threshold was missed, to try and re-establish cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_action_last_round: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_pseudocode = state.round_number + 1
        total_rounds_pseudocode = self.game_description.n_rounds
        action_to_take: Action
        if current_round_num_pseudocode == total_rounds_pseudocode:
            action_to_take = Action.D
        elif current_round_num_pseudocode == 1:
            action_to_take = Action.C
        else:
            previous_round_idx = state.round_number - 1
            my_prev_action_contribution = 1 if history.my_actions[previous_round_idx] else 0
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :])
            previous_round_cooperators = num_opponent_cooperators_prev + my_prev_action_contribution
            if previous_round_cooperators >= self.game_description.m:
                action_to_take = Action.D
            elif self.my_action_last_round == Action.C:
                action_to_take = Action.D
            else:
                action_to_take = Action.C
        self.my_action_last_round = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_57(BaseStrategy):
    """
    Adaptive Free-Rider with Threshold Insurance strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding on others' cooperation,
    only cooperating when it is critically necessary and sufficient to trigger the
    collective reward (k), and only when that reward outweighs the individual cost.
    It adapts to observed past behavior and accounts for the game's termination.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Free-Rider with Threshold Insurance strategy.

        Args:
            game_description (CollectiveRiskDescription): Description of the game
                                                         parameters (n, r, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): Past actions and payoffs of this player
                                            and opponents. None for the very first round.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_number == 0:
            return Action.D
        if current_round_number == total_rounds - 1:
            return Action.D
        last_round_cooperators = np.sum(history.opponent_actions[current_round_number - 1, :]) + history.my_actions[current_round_number - 1]
        if last_round_cooperators >= min_cooperators_m:
            return Action.D
        elif last_round_cooperators == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_58(BaseStrategy):
    """
    Adaptive Free-Rider with Pivotal Contribution (AFRPC) strategy.

    This strategy aims to maximize payoff by primarily defecting to free-ride,
    but it will strategically cooperate if its contribution is pivotal to
    achieving the collective reward 'k', given that 'k' is greater than 1.
    It adapts based on the previous round's outcome, assuming opponents
    generally repeat their last actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        cooperators_last_round = np.sum(history.opponent_actions[-1]) + history.my_actions[-1]
        if cooperators_last_round >= self.m:
            return Action.D
        elif cooperators_last_round + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_59(BaseStrategy):
    """
    Adaptive Free-Rider with Critical Contribution (AFR-CC) strategy.

    This strategy aims to maximize individual payoff by defaulting to defection (free-riding)
    but strategically cooperating only when that cooperation is uniquely critical for securing
    the collective reward 'k' (which also benefits this player due to k > 1), and when other
    players have demonstrated a willingness to cooperate up to that critical point.
    It employs a "backward induction" principle for the last round and adapts based on
    observed past behavior for intermediate rounds, starting with an exploratory defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        my_action_last_round_was_cooperation = int(history.my_actions[-1])
        num_cooperators_last_round = num_opponent_cooperators_last_round + my_action_last_round_was_cooperation
        if num_cooperators_last_round >= m:
            return Action.D
        else:
            needed_cooperators = m - num_cooperators_last_round
            if needed_cooperators == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_60(BaseStrategy):
    """
    Implements the Adaptive Conditional Free-Rider (ACFR) strategy for the Collective Risk Dilemma.
    This strategy prioritizes free-riding (Defect) to maximize individual payoff but
    strategically cooperates when the collective reward ('k' bonus) is at risk or
    when its cooperation is pivotal to achieving it, aiming to exploit others'
    cooperation while ensuring the collective good is maintained for future exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        EXPLOITATION_BUFFER = 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t == total_rounds:
            return Action.D
        num_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :]) + int(history.my_actions[state.round_number - 1])
        if num_cooperators_prev < min_cooperators_m:
            if num_cooperators_prev == min_cooperators_m - 1:
                return Action.C
            else:
                return Action.D
        elif num_cooperators_prev >= min_cooperators_m + EXPLOITATION_BUFFER:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_61(BaseStrategy):
    """
    The Strategic Opportunist aims to maximize its total payoff by free-riding on the
    cooperation of other players whenever the collective reward (k) can be secured
    without its contribution. It will only cooperate when its action is deemed
    essential to meet the m threshold, thereby preventing a complete loss of k for the group.
    It fully exploits the final round effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_t == 1:
            return Action.C
        if current_round_t == n_rounds:
            return Action.D
        total_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        num_others_cooperated = total_cooperators_prev_round
        if history.my_actions[-1]:
            num_others_cooperated -= 1
        if num_others_cooperated >= m_threshold:
            return Action.D
        elif num_others_cooperated == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_62(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aggressively free-rides, only cooperating
    when it is a calculated, self-serving action to either secure the collective
    reward (k) or prevent sustained losses of that reward.

    It prioritizes its own payoff by minimizing contributions while maximizing
    access to the collective reward. It assumes other players will attempt to
    achieve the threshold, and it will exploit this tendency.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.consecutive_failures: int = 0
        m_param = self.game_description.m
        self.FAILURE_THRESHOLD: int = max(2, min(m_param, 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_t == total_rounds_r:
            final_action = Action.D
            self.my_last_action = final_action
            return final_action
        if current_round_t == 1:
            first_round_action = Action.D
            self.my_last_action = first_round_action
            return first_round_action
        else:
            prev_C = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if prev_C >= m:
                self.consecutive_failures = 0
                action_for_round = Action.D
                self.my_last_action = action_for_round
                return action_for_round
            else:
                self.consecutive_failures += 1
                if prev_C == m - 1:
                    action_for_round = Action.C
                    self.my_last_action = action_for_round
                    return action_for_round
                elif self.consecutive_failures >= self.FAILURE_THRESHOLD:
                    action_for_round = Action.C
                    self.my_last_action = action_for_round
                    return action_for_round
                else:
                    action_for_round = Action.D
                    self.my_last_action = action_for_round
                    return action_for_round

class Strategy_EXPLOITATIVE_63(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own payoff by primarily free-riding on the
    cooperation of others, while strategically contributing only when necessary
    to ensure the collective reward 'k' is achieved.

    Decision Rules:
    1. Round 1 (first round, 0-indexed): Cooperate (C) as an initial probe/investment.
    2. Intermediate Rounds (subsequent rounds until the final round):
       a. If the 'm' threshold was met in the previous round (`C_prev >= m`), defect (D) to free-ride.
       b. If cooperation in the previous round was exactly one short of 'm' (`C_prev == m-1`),
          cooperate (C) as a strategic nudge to secure the 'k' bonus for all.
       c. If cooperation in the previous round was significantly below 'm' (`C_prev < m-1`),
          defect (D) to abandon a severely failing project and conserve endowment.
    3. Final Round: Defect (D) to exploit the endgame, as there are no future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_cooperators = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_round_cooperators >= self.m:
            return Action.D
        elif previous_round_cooperators == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_64(BaseStrategy):
    """
    The Responsive Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize personal payoff by free-riding whenever possible,
    while strategically contributing to ensure the collective reward is met when necessary,
    but avoiding being a "sucker" in a persistently uncooperative environment.

    It adapts based on historical observations of total cooperation and its own past actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_round_action = Action.D
        self._n_cooperated_in_last_round = -1
        self._consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round_idx = state.round_number
        chosen_action = Action.D
        if current_round_idx > 0:
            previous_round_history_idx = current_round_idx - 1
            my_actual_action_prev_round_bool = history.my_actions[previous_round_history_idx]
            self._my_last_round_action = Action.C if my_actual_action_prev_round_bool else Action.D
            n_coop_prev_round_actual = sum(history.opponent_actions[previous_round_history_idx, :]) + (1 if my_actual_action_prev_round_bool else 0)
            self._n_cooperated_in_last_round = n_coop_prev_round_actual
            if self._n_cooperated_in_last_round < m:
                self._consecutive_failures += 1
            else:
                self._consecutive_failures = 0
        if current_round_idx == 0:
            chosen_action = Action.D
        elif current_round_idx == r - 1:
            chosen_action = Action.D
        else:
            threshold_for_giving_up = max(2, math.floor(r / 4))
            if self._consecutive_failures >= threshold_for_giving_up:
                chosen_action = Action.D
            elif self._n_cooperated_in_last_round < m:
                chosen_action = Action.C
            elif self._n_cooperated_in_last_round > m:
                chosen_action = Action.D
            elif self._my_last_round_action == Action.C:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        self._my_last_round_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_65(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by strategically
    encouraging cooperation to unlock the communal 'k' reward, then free-riding
    on others' contributions whenever possible without jeopardizing that reward.
    It will abandon cooperation if it proves futile after a set number of
    consecutive failures.

    Key principles:
    - Cooperates in Round 1 to establish 'k' and gather data.
    - Defects in the last round as there are no future interactions.
    - In intermediate rounds:
        - If 'k' was missed: increments a failure counter. Cooperates if tolerance
          is not exceeded, otherwise defects.
        - If 'k' was met: resets failure counter.
            - Continues to free-ride if it defected successfully last round.
            - If it cooperated last round and there's a buffer of cooperators
              (num_coop_prev > m), it switches to defect to free-ride.
            - If it cooperated last round and was a critical cooperator
              (num_coop_prev == m), it continues to cooperate to maintain 'k'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.my_action_prev: Action | None = None
        self.max_failure_tolerance: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            current_action = Action.C
        elif state.round_number == n_rounds - 1:
            current_action = Action.D
        else:
            my_action_in_prev_round = self.my_action_prev
            num_coop_opponents_prev = np.sum(history.opponent_actions[-1, :])
            num_coop_total_prev = num_coop_opponents_prev + my_action_in_prev_round.value
            if num_coop_total_prev < m:
                self.consecutive_failures += 1
                if self.consecutive_failures > self.max_failure_tolerance:
                    current_action = Action.D
                else:
                    current_action = Action.C
            else:
                self.consecutive_failures = 0
                if my_action_in_prev_round == Action.D:
                    current_action = Action.D
                elif num_coop_total_prev > m:
                    current_action = Action.D
                else:
                    current_action = Action.C
        self.my_action_prev = current_action
        return current_action

class Strategy_EXPLOITATIVE_66(BaseStrategy):
    """
    Reciprocal Exploiter with Deterrence (RED) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by free-riding on others' cooperation,
    contributing only when essential, and punishing the group through defection if
    cooperation levels are too low, especially after a futile cooperation attempt.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_rounds_left: int = 0
        self.punishment_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_idx == 0:
            self.punishment_rounds_left = 0
            return Action.C
        if current_round_idx == total_rounds - 1:
            return Action.D
        prev_round_history_idx = current_round_idx - 1
        my_prev_action: bool = history.my_actions[prev_round_history_idx]
        others_cooperated_prev: int = np.sum(history.opponent_actions[prev_round_history_idx, :])
        N_C_prev: int = others_cooperated_prev + (1 if my_prev_action else 0)
        threshold_met_prev: bool = N_C_prev >= m_threshold
        if self.punishment_rounds_left > 0:
            self.punishment_rounds_left -= 1
            return Action.D
        elif others_cooperated_prev >= m_threshold:
            return Action.D
        elif others_cooperated_prev == m_threshold - 1:
            return Action.C
        elif my_prev_action and (not threshold_met_prev):
            self.punishment_rounds_left = self.punishment_duration
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_67(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to free-ride when possible
    while ensuring the collective reward 'k' is secured. It adapts its behavior
    based on the previous round's cooperation levels.

    - In the first round, it cooperates to establish a baseline.
    - In the last round, it defects to maximize immediate payoff (endgame effect).
    - In intermediate rounds, it observes the number of cooperators from the previous round (C_prev):
        - If C_prev was significantly above the threshold 'm' (C_prev > m), it defects to free-ride.
        - If C_prev barely met the threshold (C_prev == m), it cooperates to safeguard 'k'.
        - If C_prev failed to meet the threshold (C_prev < m), it cooperates to help re-establish 'k'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds_r - 1:
            return Action.D
        prev_round_index = current_round_number - 1
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_index, :])
        my_action_prev_was_C = history.my_actions[prev_round_index]
        c_prev = num_opponent_cooperators_prev + (1 if my_action_prev_was_C else 0)
        if c_prev >= min_cooperators_m:
            if c_prev > min_cooperators_m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_EXPLOITATIVE_68(BaseStrategy):
    """
    The "Opportunistic Free-Rider (OFR)" strategy aims to maximize individual
    payoff in the Collective Risk Dilemma. It starts by probing for cooperation,
    aggressively free-rides when the collective good is achieved by others,
    and strategically contributes only when its own cooperation is critical to
    securing the 'k' bonus. It avoids sucker's payoffs and cuts losses by
    reverting to defection if cooperation consistently fails. In the final
    round, it always defects to exploit the endgame.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: None | Action = None
        self.total_cooperators_last_round: None | int = None
        self.consecutive_failures: int = 0
        self.FAILURE_TOLERANCE_THRESHOLD: int = max(2, int(game_description.n_rounds / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self.my_last_action = Action.C
            self.total_cooperators_last_round = n
            return Action.C
        self.total_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        was_successful_last_round = self.total_cooperators_last_round >= m
        if was_successful_last_round:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if state.round_number == r - 1:
            self.my_last_action = Action.D
            return Action.D
        if self.consecutive_failures > self.FAILURE_TOLERANCE_THRESHOLD:
            self.my_last_action = Action.D
            return Action.D
        elif was_successful_last_round:
            self.my_last_action = Action.D
            return Action.D
        else:
            N_C_others_last_round = np.sum(history.opponent_actions[-1, :])
            if N_C_others_last_round >= m - 1:
                self.my_last_action = Action.C
                return Action.C
            else:
                self.my_last_action = Action.D
                return Action.D

class Strategy_EXPLOITATIVE_69(BaseStrategy):
    """
    Adaptive Free-Rider with Reset (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by defaulting to free-riding (Defect)
    but strategically cooperating only when necessary to prevent prolonged collective failure.
    Cooperation is seen as an investment to restore conditions for future free-riding.
    It adapts its patience (`C_reset_gap_calc`) based on game parameters and learns from
    previous round outcomes. It avoids being a "sucker" and always defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.rounds_since_my_last_cooperation: int = 0
        n = self.game_description.n_players
        m = self.game_description.m
        self.c_reset_gap_calc: int = max(1, math.floor((n - m + 1) / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_action: Action
        if current_round_t == n_rounds:
            current_round_action = Action.D
        elif current_round_t == 1:
            current_round_action = Action.D
        else:
            my_action_prev_round_bool = history.my_actions[-1]
            opponent_actions_prev_round_bool = history.opponent_actions[-1, :]
            n_cooperators_prev_round = np.sum(opponent_actions_prev_round_bool).item()
            if my_action_prev_round_bool:
                n_cooperators_prev_round += 1
            if n_cooperators_prev_round >= m:
                current_round_action = Action.D
            elif my_action_prev_round_bool:
                current_round_action = Action.D
            elif self.rounds_since_my_last_cooperation >= self.c_reset_gap_calc:
                if n_rounds - current_round_t > self.c_reset_gap_calc:
                    current_round_action = Action.C
                else:
                    current_round_action = Action.D
            else:
                current_round_action = Action.D
        self.my_last_action = current_round_action
        if current_round_action == Action.D:
            self.rounds_since_my_last_cooperation += 1
        else:
            self.rounds_since_my_last_cooperation = 0
        return current_round_action

class Strategy_EXPLOITATIVE_70(BaseStrategy):
    """
    The Opportunistic Reciprocator strategy for the Collective Risk Dilemma.

    This strategy aims to free-ride whenever possible but strategically cooperates
    when its contribution is critical and individually beneficial. It adapts its
    behavior based on the collective outcome of the previous round and exploits
    the finite nature of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_index = state.round_number
        if current_round_index == self.n_rounds - 1:
            return Action.D
        if current_round_index == 0:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_was_C = history.my_actions[-1]
        N_C_prev = num_opponent_cooperators_prev + int(my_action_prev_was_C)
        if N_C_prev >= self.m_threshold:
            return Action.D
        elif N_C_prev == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_71(BaseStrategy):
    """
    Adaptive Free-Rider with Contingent Contribution (AFRCC) strategy.

    This strategy aims to maximize individual payoff by defaulting to defection (free-riding)
    while adaptively contributing to cooperation only when it's strategically necessary
    to secure the collective reward 'k', or to influence future outcomes to my benefit.
    It leverages the game's repeated nature and perfect information, avoiding "wasted" cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round = state.round_number + 1
        if current_round == 1:
            return Action.D
        if current_round == n_rounds:
            return Action.D
        my_action_prev_is_C = history.my_actions[state.round_number - 1]
        n_c_prev_opponents = np.sum(history.opponent_actions[state.round_number - 1, :])
        n_c_prev = int(my_action_prev_is_C) + n_c_prev_opponents
        if n_c_prev >= m:
            if n_c_prev > m:
                return Action.D
            elif my_action_prev_is_C:
                return Action.C
            else:
                return Action.D
        elif n_c_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_72(BaseStrategy):
    """
    Opportunistic Exploiter strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own payoff by strategically free-riding while being
    prepared to contribute minimally only when its contribution is critically necessary
    and individually profitable to achieve the collective reward `k`.
    It explicitly handles end-game and beginning-game scenarios and adapts based on
    observed collective behavior, not individual opponent actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == n_rounds - 1:
            return Action.D
        n_c_prev_opponents = np.sum(history.opponent_actions[current_round_idx - 1, :])
        my_action_prev = history.my_actions[current_round_idx - 1]
        n_c_prev = n_c_prev_opponents + my_action_prev
        if n_c_prev >= m_threshold:
            return Action.D
        elif n_c_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_73(BaseStrategy):
    """
    Adaptive Free-Rider with Minimal Contribution: Maximizes individual payoff by defecting
    whenever possible, especially when the cooperation threshold `m` is already being met
    by others. It only cooperates if its contribution is both necessary and sufficient
    to trigger the collective reward `k`, and `k` outweighs the individual cost of cooperation.
    The strategy prioritizes defecting in the absence of a clear and necessary reason to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_actions_prev = history.opponent_actions[-1, :]
        total_cooperators_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        if total_cooperators_prev >= m_threshold:
            return Action.D
        else:
            other_cooperators_prev = total_cooperators_prev - int(my_action_prev)
            if other_cooperators_prev == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_74(BaseStrategy):
    """
    The Calculated Opportunist (CO) strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly adaptive, robust, and exploitative.
    It prioritizes free-riding when possible, but strategically cooperates only when
    its contribution is likely to trigger the collective good and when it's
    individually profitable to do so. It avoids being a "sucker" by refusing
    to cooperate if the collective effort is clearly insufficient.

    Core Principles:
    1. Initial Exploitation: Begin by defecting to gauge opponents.
    2. Last-Round Defection: Always defect in the final round.
    3. Conditional Free-Riding: Defect when the public good threshold was
       comfortably met in the previous round, relying on others.
    4. Strategic Support: Cooperate only when your action is crucial to meet
       the threshold AND it's individually profitable (k > 1).
    5. Avoid Sucker's Payoff: Refuse to cooperate if your contribution alone
       is unlikely to achieve the public good, to avoid a 0 payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        my_action_prev = history.my_actions[-1]
        if C_prev >= self.m_threshold:
            if C_prev > self.m_threshold:
                return Action.D
            elif my_action_prev == Action.C.value:
                return Action.C
            else:
                return Action.D
        else:
            C_others_prev = np.sum(history.opponent_actions[-1, :])
            if C_others_prev >= self.m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_75(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its own payoff by strategically leveraging
    the collective outcome of previous rounds. It aims to free-ride whenever possible, ensuring the
    community project is successful while minimizing its own contribution. It only cooperates when
    its individual contribution is pivotal to achieving the collective good, thus securing the higher
    payoff ('k' vs '1') for itself. It avoids being a "sucker" by not cooperating when its contribution
    is futile.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round >= self.m:
                return Action.D
            elif num_cooperators_prev_round == self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_76(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    It aims to maximize individual total payoff by leveraging the collective reward (k)
    while minimizing personal contribution. It adapts based on the collective outcome
    of the previous round, strategically cooperating when its contribution is critical
    and defecting to free-ride or avoid unrewarded cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_prev_action_bool = history.my_actions[-1]
        opponents_prev_cooperators_count = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = int(my_prev_action_bool) + opponents_prev_cooperators_count
        my_action_prev_char = 'C' if my_prev_action_bool else 'D'
        if total_cooperators_prev_round >= self.m_threshold:
            if my_action_prev_char == 'C':
                if total_cooperators_prev_round - 1 >= self.m_threshold:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D
        elif total_cooperators_prev_round == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_77(BaseStrategy):
    """
    The "Adaptive Free-Rider with Nudge" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by defaulting to defection,
    free-riding on successful cooperation, and only contributing when its
    cooperation is likely to be the marginal factor in achieving the collective
    reward 'k' for itself.

    Key behaviors:
    - Defects in the first and last rounds to maximize individual gain.
    - If the cooperation threshold 'm' was met in the previous round, it defects
      (free-rides) to secure a payoff of 1+k.
    - If 'm' was not met and the strategy cooperated in the previous round, it
      switches to defection to avoid wasted contributions.
    - If 'm' was not met and the strategy defected in the previous round, it
      considers cooperating (the "nudge") only if its contribution alone would
      bring the total cooperators to 'm' (i.e., if 'm-1' cooperators were observed).
      Otherwise, it defects to avoid contributing futilely.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_number - 1
        num_cooperators_among_opponents_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_is_C = history.my_actions[prev_round_idx]
        num_cooperators_prev = num_cooperators_among_opponents_prev + int(my_action_prev_is_C)
        m_threshold = self.game_description.m
        if num_cooperators_prev >= m_threshold:
            return Action.D
        elif my_action_prev_is_C:
            return Action.D
        elif num_cooperators_prev >= m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_78(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy: Maximizes personal payoff by defaulting to defection (free-riding).
    Only cooperates under specific, calculated conditions where cooperation is likely to be pivotal
    in achieving the collective benefit 'k', or where abstaining would lead to a guaranteed worse outcome.
    Always exploits the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0idx = state.round_number
        if current_round_0idx == self.n_rounds - 1:
            return Action.D
        if current_round_0idx == 0:
            return Action.D
        prev_round_0idx = current_round_0idx - 1
        prev_opponent_cooperators = np.sum(history.opponent_actions[prev_round_0idx])
        prev_my_action_was_cooperate = history.my_actions[prev_round_0idx]
        prev_total_cooperators = prev_opponent_cooperators + (1 if prev_my_action_was_cooperate else 0)
        if prev_total_cooperators >= self.m:
            return Action.D
        elif prev_total_cooperators == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_79(BaseStrategy):
    """
    Adaptive Free-Rider with Strategic Intervention (AFR-SI) for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by defaulting to free-riding (defecting)
    whenever the collective threshold for the 'k' bonus is met. However, it understands
    that a complete collective failure to meet the threshold (where everyone receives a
    minimal payoff) is detrimental. Therefore, it strategically intervenes with minimal
    cooperation in specific situations to "nudge" the collective back to success, creating
    future opportunities for exploitation, and then immediately reverts to free-riding.
    It leverages backward induction for the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round >= m_threshold:
            return Action.D
        else:
            cooperators_needed = m_threshold - num_cooperators_last_round
            if cooperators_needed <= 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_80(BaseStrategy):
    """
    The "Calculated Opportunist" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff over `r` rounds by primarily
    free-riding on the cooperation of others. It strategically cooperates only when
    the collective good (k bonus) is consistently at risk and its own cooperation
    is critically needed to achieve the threshold 'm', thereby enabling future
    opportunities for exploitation. It immediately reverts to exploitation once
    the threshold is met or likely to be met by others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.r - 1:
            return Action.D
        past_total_cooperators = []
        for round_idx in range(state.round_number):
            my_action_in_round = history.my_actions[round_idx]
            opponent_cooperators_in_round = np.sum(history.opponent_actions[round_idx, :])
            total_cooperators_in_round = opponent_cooperators_in_round + my_action_in_round
            past_total_cooperators.append(total_cooperators_in_round)
        N_C_prev = past_total_cooperators[-1]
        if N_C_prev >= self.m:
            return Action.D
        sum_cooperators_past = sum(past_total_cooperators)
        avg_cooperators_overall = sum_cooperators_past / len(past_total_cooperators)
        if avg_cooperators_overall >= self.m - 1:
            return Action.D
        elif N_C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_81(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma aims to maximize
    individual payoff by prioritizing defection (free-riding) while strategically cooperating
    only when its contribution is marginal and individually profitable. It exploits the
    incentive to free-ride when the collective reward 'k' is secured, avoids wasteful
    cooperation, and defects in endgame scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        c_prev_opponents = np.sum(history.opponent_actions[-1, :])
        c_prev_me = int(history.my_actions[-1])
        C_prev = c_prev_opponents + c_prev_me
        if C_prev >= m:
            return Action.D
        elif C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_82(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff over the rounds by
    exploiting other players' cooperative tendencies. It primarily free-rides,
    only contributing (Cooperating) when its contribution is strictly necessary
    to meet the collective reward threshold 'm' and doing so is individually
    optimal (i.e., when k > 1 makes cooperation better than defection).

    Decision Rules:
    - Round 1 (t=1): Defect (D) - Initial Probe to gauge others' cooperativeness.
    - Intermediate Rounds (1 < t < r):
        - If C_prev >= m: Defect (D) - Threshold met, continue free-riding.
        - If C_prev == m - 1: Cooperate (C) - My cooperation is critical and
          individually optimal (since k > 1, payoff k > 1).
        - If C_prev < m - 1: Defect (D) - My cooperation is insufficient, avoid
          wasted effort (payoff 1 > 0).
    - Last Round (t=r): Defect (D) - Final Exploitation, as defection is the
      dominant strategy in the last round due to no future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_previous_action_was_C = history.my_actions[current_round_0_indexed - 1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        C_prev = int(my_previous_action_was_C) + opponent_cooperators_prev
        if C_prev >= self.m:
            return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_83(BaseStrategy):
    """
    The Pragmatic Instigator strategy for the Collective Risk Dilemma.

    Core Philosophy:
    Aims to be a strategic free-rider. It defaults to defection, but is willing
    to cooperate under two main conditions:
    1. Critical Contribution: If its cooperation is exactly what is needed to
       meet the `m` threshold, and `k > 1`, it cooperates.
    2. Collective Failure Response: If the `m` threshold is consistently missed,
       it will temporarily instigate cooperation to "reset" the system.
    3. Last Round Exploitation: In the final round, it reverts to purely selfish play.
    """
    COOPERATION_RESET_THRESHOLD: int = 1
    COOPERATION_FORCE_DURATION: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._rounds_since_bonus_missed: int = 0
        self._current_force_cooperation_countdown: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_num = state.round_number + 1
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        action_for_current_round = Action.D
        if state.round_number == 0:
            action_for_current_round = Action.D
        else:
            round_index_prev = state.round_number - 1
            my_prev_action_bool = history.my_actions[round_index_prev]
            opponents_cooperated_prev_round = np.sum(history.opponent_actions[round_index_prev, :])
            num_cooperators_prev_round = int(my_prev_action_bool) + opponents_cooperated_prev_round
            if num_cooperators_prev_round < m:
                self._rounds_since_bonus_missed += 1
            else:
                self._rounds_since_bonus_missed = 0
            if self._current_force_cooperation_countdown > 0:
                self._current_force_cooperation_countdown -= 1
            elif self._rounds_since_bonus_missed >= self.COOPERATION_RESET_THRESHOLD:
                self._current_force_cooperation_countdown = self.COOPERATION_FORCE_DURATION
            if current_game_round_num == r:
                cooperators_without_my_action_prev_round = num_cooperators_prev_round
                if my_prev_action_bool:
                    cooperators_without_my_action_prev_round -= 1
                if cooperators_without_my_action_prev_round >= m:
                    action_for_current_round = Action.D
                elif cooperators_without_my_action_prev_round == m - 1:
                    action_for_current_round = Action.C
                else:
                    action_for_current_round = Action.D
            elif self._current_force_cooperation_countdown > 0:
                action_for_current_round = Action.C
            elif my_prev_action_bool and num_cooperators_prev_round == m:
                action_for_current_round = Action.C
            else:
                action_for_current_round = Action.D
        self._my_last_action = action_for_current_round
        return action_for_current_round

class Strategy_EXPLOITATIVE_84(BaseStrategy):
    """
    The Adaptive Free-Rider strategy prioritizes defecting while being conditionally cooperative
    only when its contribution appears crucial for securing the collective reward 'k'.
    It starts by defecting, then free-rides if the threshold 'm' is met. If 'm' is not met,
    it checks if its own cooperation would have made the difference in the previous round,
    and if so, cooperates in the current round to try and meet 'm'. It always defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if state.round_number == 0:
            return Action.D
        previous_round_idx = state.round_number - 1
        last_round_opponent_cooperators = np.sum(history.opponent_actions[previous_round_idx, :])
        last_round_my_action_was_C = history.my_actions[previous_round_idx]
        last_round_total_cooperators = last_round_opponent_cooperators + (1 if last_round_my_action_was_C else 0)
        threshold_met_prev = last_round_total_cooperators >= self.m_threshold
        if threshold_met_prev:
            return Action.D
        else:
            cooperators_from_others_prev = last_round_opponent_cooperators
            if cooperators_from_others_prev + 1 >= self.m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_85(BaseStrategy):
    """
    The Calculated Opportunist strategy for the Collective Risk Dilemma.

    Core Principle: Prioritize free-riding when the collective reward is secured by others.
    Intervene with cooperation only when the collective reward is at risk, to ensure its
    availability for personal gain. Defects unconditionally in the final round to
    maximize terminal payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: A CollectiveRiskDescription object containing game parameters
                              like n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the "Calculated Opportunist" strategy rules.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past game outcomes (own and opponent
                     actions and payoffs), or None if it's the very first round.

        Returns:
            An Action (Action.C for Cooperate or Action.D for Defect) for the current round.
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev_round_was_cooperate = history.my_actions[current_round_0_indexed - 1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + my_action_prev_round_was_cooperate
        if total_cooperators_prev_round >= self.m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_86(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize personal payoff by exploiting the collective risk dilemma.
    It prioritizes free-riding (defecting) but strategically cooperates to ensure the collective reward 'k' is
    triggered when beneficial or necessary for long-term gain. The strategy adapts to past outcomes and
    is robust to various opponent behaviors across different phases of the game.

    Decision Rules:
    1. Round 1 (Initial Probe): Always Defect. This attempts immediate free-riding and probes opponent behavior.
    2. Rounds t from 2 to r-1 (Adaptive Exploitation Phase):
       - If collective reward 'k' was met in the last round (N_c_last_round >= m):
         - If this player cooperated last round AND N_c_last_round == m (meaning this player was critically
           part of the exact minimum 'm' cooperators): Cooperates. This is a strategic investment to maintain
           the collective reward 'k', as defecting would cause 'k' to be lost.
         - Otherwise (this player defected last round OR N_c_last_round > m, implying there was an excess of
           cooperators): Defects. This allows for free-riding when 'k' is secured or there's a buffer.
       - If collective reward 'k' was NOT met in the last round (N_c_last_round < m): Cooperates. This acts
         as a trigger to help restore the collective benefit, encouraging others to cooperate and enabling
         future free-riding opportunities.
    3. Round r (Final Round Exploitation): Always Defect. In the final round, there are no future consequences,
       so the strategy reverts to the immediate payoff-maximizing action.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m_threshold = self.game_description.m
        n_rounds = self.game_description.n_rounds
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == n_rounds - 1:
            return Action.D
        my_last_action_was_C = history.my_actions[-1]
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        N_c_last_round = num_opponent_cooperators_last_round + int(my_last_action_was_C)
        if N_c_last_round >= m_threshold:
            if my_last_action_was_C and N_c_last_round == m_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_87(BaseStrategy):
    """
    The "Terminal Exploit" strategy aims to maximize individual payoff by defaulting to free-riding,
    but strategically cooperating only when its contribution is critical and directly beneficial.
    It adapts to observed group behavior and explicitly handles the end-game scenario for maximum exploitation.

    Core Principles:
    1.  Default to Defection: The primary action is to Defect, leveraging the opportunity to earn 1+k if others cooperate.
    2.  Strategic, Self-Interested Cooperation: Cooperate *only* if the player's action is the marginal contribution
        that pushes the group to meet the threshold (`m`), and if doing so yields a higher payoff (`k`) than
        defecting when the threshold would otherwise fail (`1`).
    3.  Terminal Exploitation: Always defect in the final round to secure the highest possible immediate payoff
        without future repercussions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        prev_num_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_my_action_is_cooperate = history.my_actions[-1]
        prev_num_all_cooperators = prev_num_opponent_cooperators + (1 if prev_my_action_is_cooperate else 0)
        if prev_num_all_cooperators >= self.m_threshold:
            return Action.D
        else:
            needed_for_success = self.m_threshold - prev_num_all_cooperators
            if needed_for_success == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_88(BaseStrategy):
    """
    This strategy, an "Adaptive Free-Rider with Strategic Intervention", prioritizes free-riding by defecting
    whenever possible. It initiates by defecting to probe opponents' willingness to cooperate.
    If the collective cooperation threshold (`m`) is met, it continues to defect, benefiting from others' contributions.

    The strategy only reluctantly cooperates if the group consistently fails to meet the threshold,
    specifically after more than one consecutive round of failure (`cooperation_failure_count > 1`).
    This intervention is strategic: it's a "bail-out" to prevent the loss of the `k` bonus,
    which would negatively impact its own overall payoff. As soon as the threshold is met again,
    it immediately reverts to free-riding (defection).

    In the final round, it always defects, as there are no future interactions to incentivize cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_failure_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            self.cooperation_failure_count = 0
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        my_action_last_round = history.my_actions[current_round_0_indexed - 1]
        opponent_actions_last_round = history.opponent_actions[current_round_0_indexed - 1, :]
        total_cooperators_last_round = int(my_action_last_round) + np.sum(opponent_actions_last_round)
        if total_cooperators_last_round >= m_threshold:
            self.cooperation_failure_count = 0
        else:
            self.cooperation_failure_count += 1
        if self.cooperation_failure_count == 0:
            return Action.D
        elif self.cooperation_failure_count == 1:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_89(BaseStrategy):
    """
    Adaptive Reciprocity and Free-Riding (ARFR) strategy.

    ARFR aims to maximize its own total payoff in the Collective Risk Dilemma by
    leveraging the collective reward 'k' as frequently as possible, while
    contributing minimally. It adapts to opponent behavior by cycling through
    states of Attempting Cooperation, Free-Riding, and Punishment, aiming to
    coerce others into maintaining a cooperative environment for its own benefit.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_grace_period: int = 1
        self.defection_punishment_rounds: int = 2
        self.in_punishment_mode: bool = False
        self.punishment_rounds_left: int = 0
        self.in_attempt_cooperate_mode: bool = False
        self.attempt_cooperate_rounds_left: int = 0
        self.my_action_last_round: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            self.my_action_last_round = Action.D
            return Action.D
        if state.round_number == 0:
            self.in_attempt_cooperate_mode = True
            self.attempt_cooperate_rounds_left = self.cooperation_grace_period
            self.in_punishment_mode = False
            self.punishment_rounds_left = 0
        else:
            C_prev = np.sum(history.opponent_actions[-1, :]) + (1 if self.my_action_last_round == Action.C else 0)
            if self.in_punishment_mode:
                self.punishment_rounds_left -= 1
                if self.punishment_rounds_left <= 0:
                    self.in_punishment_mode = False
            elif self.in_attempt_cooperate_mode:
                self.attempt_cooperate_rounds_left -= 1
                if self.attempt_cooperate_rounds_left <= 0:
                    self.in_attempt_cooperate_mode = False
            if self.my_action_last_round == Action.C:
                if C_prev >= self.game_description.m:
                    self.in_attempt_cooperate_mode = False
                    self.in_punishment_mode = False
                else:
                    self.in_attempt_cooperate_mode = False
                    self.in_punishment_mode = True
                    self.punishment_rounds_left = self.defection_punishment_rounds
            elif self.my_action_last_round == Action.D:
                if C_prev < self.game_description.m:
                    if not self.in_punishment_mode and (not self.in_attempt_cooperate_mode):
                        self.in_punishment_mode = True
                        self.punishment_rounds_left = self.defection_punishment_rounds
            if not self.in_punishment_mode and (not self.in_attempt_cooperate_mode):
                self.in_attempt_cooperate_mode = True
                self.attempt_cooperate_rounds_left = self.cooperation_grace_period
        my_action_this_round: Action
        if self.in_punishment_mode:
            my_action_this_round = Action.D
        elif self.in_attempt_cooperate_mode:
            my_action_this_round = Action.C
        else:
            my_action_this_round = Action.D
        self.my_action_last_round = my_action_this_round
        return my_action_this_round

class Strategy_EXPLOITATIVE_90(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to maximize personal payoff
    in the Collective Risk Dilemma by strategically cooperating only when
    absolutely necessary to secure the collective reward 'k', otherwise defecting
    to gain a private endowment. It adapts based on the previous round's cooperation
    levels and exploits the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if self.m == self.n:
            if current_round_1_indexed < self.r:
                return Action.C
            else:
                return Action.D
        elif current_round_0_indexed == 0:
            return Action.D
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            my_action_prev_was_C = history.my_actions[-1]
            C_prev = num_opponent_cooperators_prev + (1 if my_action_prev_was_C else 0)
            if C_prev >= self.m:
                return Action.D
            elif C_prev == self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_91(BaseStrategy):
    """
    The Adaptive Conditional Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by consistently attempting to free-ride,
    only cooperating when its contribution is critically needed to secure the collective
    reward 'k', and defecting in all other circumstances. It adapts based on the previous
    round's collective outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        cooperating_opponents_prev = np.sum(history.opponent_actions[current_round_idx - 1])
        my_action_prev_bool = history.my_actions[current_round_idx - 1]
        C_prev = cooperating_opponents_prev + int(my_action_prev_bool)
        if C_prev >= self.m:
            return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_92(BaseStrategy):
    """
    The Adaptive Pivoting Free-Rider (APFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily free-riding on the
    cooperation of others. It only cooperates when its action is calculated to be
    pivotal in achieving or maintaining the collective reward (k), thereby benefiting
    the agent. It employs backward induction for the final round and adapts its
    behavior based on the number of cooperators in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        my_action_prev_round_bool = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round_bools = history.opponent_actions[state.round_number - 1, :]
        cooperators_prev_round = sum(opponent_actions_prev_round_bools) + (1 if my_action_prev_round_bool else 0)
        if cooperators_prev_round >= self.game_description.m:
            if cooperators_prev_round > self.game_description.m:
                return Action.D
            elif not my_action_prev_round_bool:
                return Action.D
            else:
                return Action.C
        elif cooperators_prev_round == self.game_description.m - 1 and self.game_description.n_rounds - current_round_1_indexed > 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_93(BaseStrategy):
    """
    Adaptive Exploiter with Threshold Management strategy for the Collective Risk Dilemma.

    This strategy observes the collective behavior in previous rounds and adjusts its own action
    to capitalize on cooperative efforts or to minimize losses when cooperation is insufficient.
    It aims to maximize its own total payoff by free-riding when the collective reward (k) is being
    met, becoming a critical cooperator when its contribution is essential and individually
    beneficial (since k > 1), and avoiding wasted cooperation. The strategy starts with an
    initial cooperation to establish a baseline and always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= m:
            return Action.D
        elif num_cooperators_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_94(BaseStrategy):
    """
    Adaptive Free-Rider with Corrective Cooperation.

    This strategy prioritizes free-riding: aiming to receive the collective reward 'k'
    without bearing the cost of cooperation (1 unit). It will only cooperate as a last
    resort to "kick-start" or "re-establish" cooperation when persistent global failure
    threatens its own long-term payoff, and will immediately revert to free-riding at
    the first opportunity. It also includes a guaranteed defection in the final round
    to maximize terminal payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_previous_action: Action = Action.D
        self._rounds_global_failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        action_for_this_round: Action = Action.D
        if current_round_1_indexed == n_rounds:
            action_for_this_round = Action.D
        elif current_round_1_indexed == 1:
            action_for_this_round = Action.D
        else:
            C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if C_prev >= m_threshold:
                self._rounds_global_failure_streak = 0
            else:
                self._rounds_global_failure_streak += 1
            if C_prev >= m_threshold:
                action_for_this_round = Action.D
            elif self._my_previous_action == Action.C:
                action_for_this_round = Action.D
            elif self._rounds_global_failure_streak >= 2:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        self._my_previous_action = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_95(BaseStrategy):
    """
    Adaptive Exploiter strategy for the Collective Risk Dilemma.
    Prioritizes defection, only cooperating when it is the marginal contribution
    required to secure the communal reward 'k', and defects in the final round
    to capitalize on the end-game effect. Adapts based on observed cooperation
    in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        else:
            cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if cooperators_last_round >= m_threshold:
                return Action.D
            elif cooperators_last_round + 1 >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_96(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding on others' cooperation
    whenever possible, while providing minimal, strategically timed cooperation
    to prevent the complete collapse of the collective project.
    It adapts to observed past behaviors, cooperating only when pivotal
    and defecting otherwise, especially in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        cooperators_opponents_prev = sum(history.opponent_actions[previous_round_idx, :])
        C_prev = cooperators_opponents_prev + history.my_actions[previous_round_idx]
        if C_prev >= self.m:
            return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_97(BaseStrategy):
    """
    The Calculated Free-Rider (CFR) strategy aims to maximize its own total payoff over the rounds.
    It free-rides on others' cooperation, securing the collective reward 'k' without contributing.
    It only cooperates under specific, calculated circumstances where its individual contribution
    is crucial and directly beneficial to its own payoff in the current round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == n_rounds - 1:
            return Action.D
        prev_round_index = current_round_number - 1
        C_prev = int(history.my_actions[prev_round_index]) + np.sum(history.opponent_actions[prev_round_index, :])
        if C_prev >= m:
            return Action.D
        elif C_prev + 1 >= m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_98(BaseStrategy):
    """
    Adaptive Free-Rider with Conditional Kickstart (AFR-CK) strategy for Collective Risk Dilemma.
    Aims to maximize individual payoff by free-riding when the collective threshold (m) is met.
    If the threshold is missed, it cooperates for a single round to 'kickstart' cooperation,
    then reverts to free-riding if its cooperation was unrewarded. Defaults to Defect in the
    first and last rounds to maximize immediate individual gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            my_action_prev_round = history.my_actions[-1]
            cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_prev_round = cooperators_from_opponents_prev_round + (1 if my_action_prev_round else 0)
            if total_cooperators_prev_round >= self.m_threshold:
                return Action.D
            elif my_action_prev_round == False:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_99(BaseStrategy):
    """
    The Threshold Strategist: An exploitative strategy for the Collective Risk Dilemma.
    This strategy prioritizes maximizing its own payoff by navigating the cooperation
    threshold 'm'. It defects in the first and last rounds to free-ride or avoid futile
    sacrifice. In intermediate rounds, it adapts:
    - If the collective threshold 'm' was met in the previous round, it defects to free-ride.
    - If exactly 'm-1' players cooperated in the previous round (meaning its cooperation
      would be crucial), it cooperates, as 'k' (reward for meeting threshold) is
      individually greater than 1 (payoff for defecting if threshold is not met).
    - If cooperation in the previous round was significantly below 'm-1', it defects to
      avoid futile sacrifice.
    This strategy is designed to be adaptive, robust, and exploitative.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.D
        num_C_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_C_prev_round >= self.m:
            return Action.D
        elif num_C_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_100(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize individual payoff by opportunistically
    defecting, only cooperating when it is critical to secure the collective reward 'k'
    which also benefits the player. It adapts to the observed cooperation level and
    exploits the game's finite horizon.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        num_opponent_cooperators_last_round = sum(history.opponent_actions[state.round_number - 1, :])
        my_action_last_round_was_C = history.my_actions[state.round_number - 1]
        num_cooperators_last_round = num_opponent_cooperators_last_round + (1 if my_action_last_round_was_C else 0)
        if num_cooperators_last_round >= m:
            return Action.D
        elif num_cooperators_last_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_102(BaseStrategy):
    """
    The Adaptive Pivoting Free-Rider strategy for the Collective Risk Dilemma.
    It starts cooperatively, then strategically pivots to cooperate only when its
    contribution is pivotal for the 'm' threshold. It exploits consistent group
    cooperation by free-riding and withdraws to defection if cooperation repeatedly fails.
    In the last round, it acts purely out of immediate self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m: int = game_description.m
        self.r: int = game_description.n_rounds
        self.consecutive_met_m_rounds: int = 0
        self.consecutive_not_met_m_rounds: int = 0
        self.my_last_action: Action | None = None
        self.free_ride_threshold: int = 2
        self.punishment_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed: int = state.round_number + 1
        num_cooperators_actual_prev: int = 0
        num_cooperators_others_prev: int = 0
        if current_round_1_indexed > 1 and history is not None:
            prev_round_idx = state.round_number - 1
            my_action_prev_bool: np.bool_ = history.my_actions[prev_round_idx]
            self.my_last_action = Action.C if my_action_prev_bool else Action.D
            opponent_actions_prev_bool: NDArray[np.bool_] = history.opponent_actions[prev_round_idx, :]
            num_cooperators_actual_prev = np.sum(opponent_actions_prev_bool) + (1 if my_action_prev_bool else 0)
            num_cooperators_others_prev = np.sum(opponent_actions_prev_bool)
            if num_cooperators_actual_prev >= self.m:
                self.consecutive_met_m_rounds += 1
                self.consecutive_not_met_m_rounds = 0
            else:
                self.consecutive_not_met_m_rounds += 1
                self.consecutive_met_m_rounds = 0
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            if num_cooperators_others_prev >= self.m:
                return Action.D
            elif num_cooperators_others_prev == self.m - 1:
                return Action.C
            else:
                return Action.D
        elif self.consecutive_not_met_m_rounds >= self.punishment_threshold:
            return Action.D
        elif self.consecutive_met_m_rounds >= self.free_ride_threshold:
            return Action.D
        elif num_cooperators_others_prev >= self.m:
            return Action.D
        elif num_cooperators_others_prev == self.m - 1:
            return Action.C
        else:
            return Action.C

class Strategy_EXPLOITATIVE_103(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding, but
    strategically intervenes with cooperation when necessary to prevent
    prolonged collective failure or to efficiently tip the balance towards
    success, always with the ultimate goal of returning to a free-riding state.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_last_success = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.rounds_since_last_success = 0
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round >= self.game_description.m:
            self.rounds_since_last_success = 0
            return Action.D
        else:
            self.rounds_since_last_success += 1
            if num_cooperators_last_round == self.game_description.m - 1:
                return Action.C
            elif self.rounds_since_last_success >= 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_104(BaseStrategy):
    """
    The Adaptive Exploiter with Patience strategy aims to maximize payoff by free-riding
    on other players' cooperation while ensuring the 'k' reward is achieved.
    It cooperates initially and when cooperation is needed, but defects to exploit
    a surplus of cooperators. If cooperation consistently fails to meet the threshold
    for a 'patience_threshold' number of rounds, it switches to a permanent
    punishing (defection) state. In the final round, it always defects.
    """
    STATE_ACTIVE_COOPERATION = 'ACTIVE_COOPERATION'
    STATE_EXPLOITING = 'EXPLOITING'
    STATE_PUNISHING = 'PUNISHING'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._current_state: str = self.STATE_ACTIVE_COOPERATION
        self._unmet_threshold_rounds: int = 0
        self._my_action_prev: Action | None = None
        self.patience_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            action_to_take = Action.C
            self._current_state = self.STATE_ACTIVE_COOPERATION
            self._unmet_threshold_rounds = 0
            self._my_action_prev = action_to_take
            return action_to_take
        elif current_round_idx == self.game_description.n_rounds - 1:
            action_to_take = Action.D
            self._my_action_prev = action_to_take
            return action_to_take
        else:
            num_cooperators_prev_opponents = sum(history.opponent_actions[-1, :])
            my_prev_action_was_C = self._my_action_prev == Action.C
            num_cooperators_prev = num_cooperators_prev_opponents + (1 if my_prev_action_was_C else 0)
            if num_cooperators_prev < self.game_description.m:
                self._unmet_threshold_rounds += 1
            else:
                self._unmet_threshold_rounds = 0
            if self._current_state == self.STATE_PUNISHING:
                pass
            elif self._unmet_threshold_rounds >= self.patience_threshold:
                self._current_state = self.STATE_PUNISHING
            elif self._current_state == self.STATE_ACTIVE_COOPERATION and num_cooperators_prev >= self.game_description.m:
                self._current_state = self.STATE_EXPLOITING
            elif self._current_state == self.STATE_EXPLOITING and num_cooperators_prev < self.game_description.m:
                self._current_state = self.STATE_ACTIVE_COOPERATION
            action_to_take: Action
            if self._current_state == self.STATE_PUNISHING:
                action_to_take = Action.D
            elif self._current_state == self.STATE_ACTIVE_COOPERATION:
                action_to_take = Action.C
            elif self._current_state == self.STATE_EXPLOITING:
                cooperator_cushion = num_cooperators_prev - self.game_description.m
                if cooperator_cushion >= 1:
                    action_to_take = Action.D
                elif cooperator_cushion == 0:
                    if my_prev_action_was_C:
                        action_to_take = Action.C
                    else:
                        action_to_take = Action.D
            else:
                action_to_take = Action.D
            self._my_action_prev = action_to_take
            return action_to_take

class Strategy_EXPLOITATIVE_105(BaseStrategy):
    """
    Calculated Catalyst: An exploitative, adaptive, and robust strategy for the
    Collective Risk Dilemma. It prioritizes maximizing its own payoff by free-riding
    when collective action succeeds. It strategically cooperates early or after
    collective failures to re-establish an environment conducive to free-riding,
    but minimizes losses by defecting if cooperation efforts fail or as the game ends.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.rounds_since_last_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        action_for_this_round: Action
        if current_round_0_indexed == 0:
            action_for_this_round = Action.C
        elif current_round_0_indexed == total_rounds - 1:
            action_for_this_round = Action.D
        else:
            previous_round_cooperators = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
            if previous_round_cooperators >= m_threshold:
                self.rounds_since_last_success = 0
                action_for_this_round = Action.D
            else:
                self.rounds_since_last_success += 1
                if total_rounds - current_round_0_indexed <= 3:
                    action_for_this_round = Action.D
                elif self.my_last_action == Action.C:
                    action_for_this_round = Action.D
                else:
                    action_for_this_round = Action.C
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_106(BaseStrategy):
    """
    Adaptive Free-Rider with Critical Contribution (AFRCC) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by defecting whenever the collective threshold `m`
    is likely to be met without its contribution, and only cooperating when its participation is
    essential to secure the `k` bonus. It adapts to the aggregate behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        prev_round_my_action_was_C = history.my_actions[-1]
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        C_prev = prev_round_opponent_cooperators + (1 if prev_round_my_action_was_C else 0)
        if C_prev >= m_threshold:
            return Action.D
        elif C_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_107(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.
    It probes in the first round, defects in the last, and adapts in between
    by free-riding when the threshold is met, contributing when critical,
    and defecting when cooperation is futile.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        else:
            cooperating_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round = history.my_actions[-1]
            c_prev = int(cooperating_opponents_prev_round) + int(my_action_prev_round)
            if c_prev >= min_cooperators:
                return Action.D
            elif c_prev == min_cooperators - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_108(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own total payoff over 'r' rounds by
    free-riding on others' cooperation when the collective project is robustly funded,
    but strategically cooperates when its contribution is essential to maintain
    the collective benefit (the 'k' bonus).

    Decision Rules:
    1. First Round (t=1): Cooperate (C) to establish a baseline for potential collective benefit.
    2. Last Round (t=r): Defect (D) to maximize immediate gain, as there are no future repercussions
       or opportunities to build a reputation.
    3. Intermediate Rounds (1 < t < r), decisions are based on the observed total number of
       cooperators (N_C_prev) from the immediately preceding round (t-1):
       - If N_C_prev < m (Cooperation Failure): Cooperate (C) to try and revive the collective
         project and the 'k' bonus, as its absence severely limits exploitative potential.
       - If N_C_prev = m (Cooperation Barely Met): Cooperate (C) to act as the critical "pivot"
         player, ensuring the 'k' bonus is maintained. Since k > 1, receiving 'k' by cooperating
         is better than receiving '1' by defecting and causing the project to fail.
       - If N_C_prev > m (Cooperation Robust): Defect (D) to free-ride. The strategy's contribution
         is not strictly necessary to meet the threshold, so it defects to maximize individual payoff
         to 1+k, leveraging the surplus cooperation of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        my_previous_action_was_C = history.my_actions[previous_round_index]
        num_opponent_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_index, :])
        num_total_cooperators_previous_round = int(my_previous_action_was_C) + num_opponent_cooperators_previous_round
        if num_total_cooperators_previous_round < m_threshold:
            return Action.C
        elif num_total_cooperators_previous_round == m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_109(BaseStrategy):
    """
    The "Adaptive Exploitation with Minimal Support" strategy aims to be robust by observing past player behavior,
    avoiding being a "sucker," and making targeted, minimal contributions only when they are most impactful.
    It prioritizes free-riding (defecting) whenever the collective good is met or when its own cooperation is
    unlikely to be pivotal or proves futile. It cooperates only when its action guarantees the collective reward
    for that round, transforming a 0 payoff into a k payoff for itself (a net gain of k-1, since k > 1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action = Action.D
        self._burned_by_failed_cooperation: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        current_round_1_indexed = current_round_idx + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_idx > 0:
            prev_round_my_action_is_C = history.my_actions[current_round_idx - 1]
            prev_round_opponent_cooperators = np.sum(history.opponent_actions[current_round_idx - 1, :])
            prev_round_num_cooperators_total = prev_round_opponent_cooperators + (1 if prev_round_my_action_is_C else 0)
            if prev_round_my_action_is_C and prev_round_num_cooperators_total < m_threshold:
                self._burned_by_failed_cooperation = True
            else:
                self._burned_by_failed_cooperation = False
        else:
            self._burned_by_failed_cooperation = False
        if current_round_1_indexed == 1:
            current_round_action = Action.D
            self._my_last_action = current_round_action
            return current_round_action
        if current_round_1_indexed == total_rounds:
            current_round_action = Action.D
            self._my_last_action = current_round_action
            return current_round_action
        current_round_action = Action.D
        if prev_round_num_cooperators_total >= m_threshold:
            current_round_action = Action.D
        elif self._burned_by_failed_cooperation:
            current_round_action = Action.D
        elif prev_round_num_cooperators_total == m_threshold - 1 and current_round_1_indexed < total_rounds - 1:
            current_round_action = Action.C
        else:
            current_round_action = Action.D
        self._my_last_action = current_round_action
        return current_round_action

class Strategy_EXPLOITATIVE_110(BaseStrategy):
    """
    The Conditional Free-Rider (CFR) strategy aims to consistently achieve the highest individual payoff by
    defecting whenever the collective cooperation threshold has been met, and strategically cooperating
    only when necessary to re-establish the conditions for collective reward and future free-riding opportunities.
    It prioritizes self-preservation by avoiding repeated losses from ineffective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx]) + history.my_actions[prev_round_idx]
        my_last_action_was_cooperate = history.my_actions[prev_round_idx]
        if cooperators_prev_round >= m:
            return Action.D
        elif not my_last_action_was_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_111(BaseStrategy):
    """
    The "Opportunistic Payer" strategy is designed to be highly exploitative,
    prioritizing its own payoff maximization by primarily free-riding.
    It defects in the first and last rounds. In intermediate rounds, it
    observes the number of other players who cooperated in the previous round (N_C_prev).
    - If N_C_prev >= m: Defects, as the threshold was met by others.
    - If N_C_prev == m - 1: Cooperates, as its contribution is critical to meet 'm'
                             and the reward 'k' (guaranteed k > 1) is better than individual private gain.
    - If N_C_prev < m - 1: Defects, as its cooperation would be wasted.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        N_C_prev = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if N_C_prev >= self.m:
            return Action.D
        elif N_C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_112(BaseStrategy):
    """
    Adaptive Free-Rider with Self-Preservation strategy for the Collective Risk Dilemma.

    This strategy aims to exploit other players by defecting when enough cooperation exists,
    while judiciously cooperating only when necessary to restore the collective benefit
    or when its cooperation can genuinely push the group towards success.
    It prioritizes maximizing its own payoff (1+k) and avoids futile cooperation (0 payoff).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.min_cooperators_m = game_description.m
        self.k_bonus = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.
        """
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_last_action_bool = history.my_actions[current_round_0_indexed - 1]
        n_c_others_last_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        n_c_last_round = n_c_others_last_round + (1 if my_last_action_bool else 0)
        if n_c_last_round >= self.min_cooperators_m:
            return Action.D
        else:
            potential_cooperators_if_i_cooperate = n_c_others_last_round + 1
            if potential_cooperators_if_i_cooperate >= self.min_cooperators_m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_113(BaseStrategy):
    """
    The "Adaptive Threshold Enforcer with Dynamic Patience" strategy is designed to maximize
    individual payoff in the Collective Risk Dilemma. It strategically balances cooperation
    to secure the collective reward 'k' with exploitation (free-riding) whenever the collective
    good appears stable or attainable by others. It also incorporates a mechanism to cut losses
    when cooperation proves consistently fruitless.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.recession_counter = 0
        self.patience_threshold = self.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.recession_counter = 0
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_my_cooperation_prev = int(history.my_actions[-1])
        num_opponent_cooperation_prev = np.sum(history.opponent_actions[-1, :])
        N_C_prev = num_my_cooperation_prev + num_opponent_cooperation_prev
        if N_C_prev < self.m:
            self.recession_counter += 1
        else:
            self.recession_counter = 0
        if self.recession_counter > self.patience_threshold:
            return Action.D
        elif N_C_prev < self.m:
            return Action.C
        elif N_C_prev == self.m:
            return Action.C
        elif N_C_prev > self.m:
            return Action.D

class Strategy_EXPLOITATIVE_114(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by defaulting to free-riding
    (defecting) and only strategically cooperating as a last resort to prevent
    repeated failures to meet the collective reward threshold (`m`), especially
    when the group is just one cooperator short. It exploits success by immediately
    reverting to defection and avoids being a "sucker" by defecting if its
    cooperation fails to meet the threshold. The strategy always defects in the
    final round to maximize terminal payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.consecutive_failures_while_defecting = 0
        self.failure_tolerance_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.n_rounds - 1:
            return Action.D
        prev_round_index = current_round_number - 1
        C_prev = int(history.my_actions[prev_round_index]) + np.sum(history.opponent_actions[prev_round_index, :])
        my_action_prev = history.my_actions[prev_round_index]
        decision = Action.D
        if C_prev >= self.m:
            self.consecutive_failures_while_defecting = 0
            decision = Action.D
        elif my_action_prev == Action.C.value:
            decision = Action.D
        else:
            self.consecutive_failures_while_defecting += 1
            if C_prev == self.m - 1 and self.consecutive_failures_while_defecting >= self.failure_tolerance_threshold:
                decision = Action.C
            else:
                decision = Action.D
        return decision

class Strategy_EXPLOITATIVE_115(BaseStrategy):
    """
    Adaptive Free-Rider with Calculated Minimal Contribution (AFR-CMC) strategy.
    Prioritizes maximizing its own payoff by free-riding on others' cooperation.
    It only contributes when its cooperation is strategically pivotal and individually
    profitable for the current round, or when faced with imminent game end (which it exploits).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Current game state, including the round number (0-indexed).
            history (PlayerHistory | None): History of actions and payoffs from previous rounds.
                                          None for the very first round (round_number = 0).

        Returns:
            Action: 'C' for Cooperate, 'D' for Defect.
        """
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        num_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev >= self.m:
            return Action.D
        elif num_cooperators_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_116(BaseStrategy):
    """
    Implements the "Greedy Conditional Contributor" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its total personal payoff by free-riding whenever possible,
    contributing only when its cooperation is critical to securing the collective reward
    (which is beneficial even to a cooperator), and avoiding wasted contributions. It starts
    by defecting to probe for free-riding opportunities.

    Decision Rules:
    1.  **Round 1 (state.round_number == 0):** Always Defect (D) to probe.
    2.  **Subsequent Rounds (state.round_number > 0):**
        *   Calculate `C_prev`: total cooperators in the immediately preceding round.
        *   **If `C_prev >= m` (threshold met):** Defect (D) to free-ride and secure 1+k.
        *   **If `C_prev == m - 1` (exactly one cooperator needed):** Cooperate (C) to push over the threshold.
            This secures `k` for this player, which is better than `1` (from defecting when threshold not met).
        *   **If `C_prev < m - 1` (far from threshold):** Defect (D) to avoid wasted contribution,
            as contributing C would likely yield `0` (if threshold still not met), worse than `1`.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_was_C = history.my_actions[state.round_number - 1]
        C_prev = opponent_cooperators_prev + (1 if my_action_prev_was_C else 0)
        if C_prev >= self.m:
            return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_117(BaseStrategy):
    """
    Adaptive Conditional Free-Rider (ACFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by selectively free-riding,
    contributing only when its cooperation is likely to be critical to securing
    the collective reward (`k`), and always prioritizing self-interest, especially
    in the absence of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        n_c_others_prev = np.sum(history.opponent_actions[-1, :])
        n_c_prev = n_c_others_prev + (1 if my_action_prev else 0)
        if n_c_prev >= m:
            if n_c_others_prev >= m:
                return Action.D
            else:
                return Action.C
        elif n_c_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_118(BaseStrategy):
    """
    The "Calculated Opportunist" strategy is designed to maximize individual payoff.
    It defaults to defecting to free-ride, but strategically cooperates when its
    contribution is pivotal and sufficient to meet the 'm' cooperator threshold,
    thereby unlocking the significant 'k' reward (which is always individually beneficial if k > 1).
    Decisions are adaptive, based purely on self-interest and past observations from the immediately preceding round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the CalculatedOpportunist strategy with game parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the optimal action (Cooperate or Defect) for the current round
        based on the strategy's rules and previous round's outcomes.

        Args:
            state (GameState): Contains the current round number.
            history (None | PlayerHistory): Contains records of previous rounds' actions and payoffs.
                                           None for the very first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        if state.round_number == 0:
            return Action.D
        else:
            N_C_others_prev = sum(history.opponent_actions[-1])
            m = self.game_description.m
            if N_C_others_prev >= m:
                return Action.D
            elif N_C_others_prev + 1 >= m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_119(BaseStrategy):
    """
    Calculated Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own total payoff by primarily attempting
    to free-ride on the cooperation of other players. It only contributes
    (cooperate) under specific conditions where its contribution is calculated
    to be essential for unlocking the collective reward 'k' or to maintain it,
    thereby directly benefiting the player. It exploits the final round by
    always defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        my_last_action_was_C = history.my_actions[current_round_0_indexed - 1]
        my_last_action_enum = Action.C if my_last_action_was_C else Action.D
        cooperators_prev_round_others = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        last_round_cooperators = cooperators_prev_round_others + (1 if my_last_action_was_C else 0)
        chosen_action = Action.D
        if last_round_cooperators >= m_threshold:
            if my_last_action_enum == Action.D:
                chosen_action = Action.D
            elif last_round_cooperators - 1 >= m_threshold:
                chosen_action = Action.D
            else:
                chosen_action = Action.C
        elif last_round_cooperators == m_threshold - 1:
            chosen_action = Action.C
        else:
            chosen_action = Action.D
        return chosen_action

class Strategy_EXPLOITATIVE_120(BaseStrategy):
    """
    The Threshold Nudger strategy for the Collective Risk Dilemma.
    This strategy is designed with an explicitly exploitative mindset, aiming to
    maximize its own total payoff by free-riding on others' cooperation whenever
    possible, and only contributing when its contribution is critical to either
    establish or maintain the collective reward 'k'. It also adheres to standard
    game theory principles like backward induction for the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        prev_my_action_bool = history.my_actions[prev_round_idx]
        prev_my_action = Action.C if prev_my_action_bool else Action.D
        prev_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        prev_total_cooperators = int(prev_my_action_bool) + prev_opponent_cooperators
        if prev_total_cooperators >= self.m:
            if prev_my_action == Action.D:
                return Action.D
            elif prev_total_cooperators - 1 >= self.m:
                return Action.D
            else:
                return Action.C
        elif prev_total_cooperators + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_121(BaseStrategy):
    """
    An Adaptive Free-Rider strategy for the Collective Risk Dilemma game.
    It aims to maximize personal payoff by free-riding whenever possible,
    only cooperating as a calculated move to re-establish the collective reward
    after a period of sustained failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failed_rounds_in_a_row = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        if state.round_number > 0:
            cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
            if cooperators_in_prev_round >= self.game_description.m:
                self.failed_rounds_in_a_row = 0
            else:
                self.failed_rounds_in_a_row += 1
            C_prev = cooperators_in_prev_round
        else:
            C_prev = -1
        if current_t == 1:
            return Action.D
        if current_t == self.game_description.n_rounds:
            return Action.D
        if C_prev >= self.game_description.m:
            return Action.D
        else:
            cooperation_failure_threshold = max(3, math.ceil(self.game_description.m / 2))
            if self.failed_rounds_in_a_row >= cooperation_failure_threshold and self.game_description.n_rounds - current_t > 1:
                self.failed_rounds_in_a_row = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_122(BaseStrategy):
    """
    Opportunistic Pumper-Dumper strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by generally defecting (D),
    viewing cooperation (C) as a cost. It only initiates cooperation as a
    strategic "pump"  a temporary, minimal investment to unlock the larger
    'k' bonus when the collective is stuck in a low-payoff failure state.
    Immediately after a "pump", it reverts to defecting ("dumping") to free-ride.

    Internal State:
    - consecutive_failures_count: Tracks consecutive rounds where the
      collective cooperation threshold 'm' was not met. Resets to 0 on success.
    - my_action_prev_was_C_to_pump: A flag indicating if this player
      strategically cooperated in the *previous* round to "pump" the system.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_count: int = 0
        self.my_action_prev_was_C_to_pump: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        r_total = self.game_description.n_rounds
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        c_prev_round_count = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if c_prev_round_count >= m:
            self.consecutive_failures_count = 0
        else:
            self.consecutive_failures_count += 1
        if current_round_idx == r_total - 1:
            return Action.D
        if self.my_action_prev_was_C_to_pump:
            self.my_action_prev_was_C_to_pump = False
            return Action.D
        elif c_prev_round_count >= m:
            self.my_action_prev_was_C_to_pump = False
            return Action.D
        elif self.consecutive_failures_count >= 2:
            self.my_action_prev_was_C_to_pump = True
            return Action.C
        else:
            self.my_action_prev_was_C_to_pump = False
            return Action.D

class Strategy_EXPLOITATIVE_123(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to be highly exploitative,
    adaptive, and robust in the Collective Risk Dilemma. It aims to maximize
    its own payoff by leveraging the cooperation of others while minimizing
    its own contribution when possible.

    It operates in three distinct phases: an initial exploration phase, an
    adaptive learning phase, and a final round exploitation phase. It continuously
    monitors the collective behavior of other players to adjust its own actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures_count: int = 0
        self.my_actions_history: list[Action] = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        action: Action
        if current_round_1_indexed == 1:
            action = Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            action = Action.D
        else:
            my_action_last_round: Action = self.my_actions_history[-1]
            prev_round_my_action_bool = history.my_actions[-1]
            prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            prev_round_cooperators = int(prev_round_my_action_bool) + prev_round_opponent_cooperators
            if prev_round_cooperators < self.game_description.m:
                self.consecutive_failures_count += 1
                if self.consecutive_failures_count >= 3:
                    action = Action.D
                else:
                    action = Action.C
            else:
                self.consecutive_failures_count = 0
                if my_action_last_round == Action.C:
                    cooperators_without_me = prev_round_cooperators - 1
                    if cooperators_without_me >= self.game_description.m:
                        action = Action.D
                    else:
                        action = Action.C
                else:
                    action = Action.D
        self.my_actions_history.append(action)
        return action

class Strategy_EXPLOITATIVE_124(BaseStrategy):
    """
    Adaptive Free-Rider with Collective Nudge strategy for the Collective Risk Dilemma.
    This strategy focuses on maximizing personal payoff by primarily free-riding on others' cooperation.
    It is adaptive, using past round outcomes to inform current decisions, and strategic in its willingness
    to cooperate only when its contribution is critical for achieving the collective reward and
    when doing so is individually rational (i.e., k > 1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        reward_factor = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == total_rounds - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        num_opponent_cooperators_prev = sum(history.opponent_actions[prev_round_index, :])
        my_action_prev = int(history.my_actions[prev_round_index])
        N_C_prev = num_opponent_cooperators_prev + my_action_prev
        if N_C_prev >= min_cooperators_needed:
            return Action.D
        elif N_C_prev == min_cooperators_needed - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_125(BaseStrategy):
    """
    The Adaptable Free-Rider strategy for the Collective Risk Dilemma.
    This strategy prioritizes free-riding (Defect) to maximize individual payoff.
    It adaptively cooperates only when the collective reward is consistently missed,
    and its cooperation is pivotal to achieving the cooperation threshold 'm'.
    It avoids being a "sucker" by defecting if its previous cooperation failed to
    meet the threshold. It always defects in the final round to exploit the end-game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.rounds_since_success: int = 0
        self.MAX_ROUNDS_TO_FORCE_COOPERATION: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        my_action: Action
        if current_round_0_indexed == 0:
            my_action = Action.D
            self.my_last_action = my_action
            return my_action
        if current_round_0_indexed == total_rounds - 1:
            my_action = Action.D
            self.my_last_action = my_action
            return my_action
        N_C_t_minus_1 = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if N_C_t_minus_1 >= min_cooperators_m:
            self.rounds_since_success = 0
        else:
            self.rounds_since_success += 1
        if N_C_t_minus_1 >= min_cooperators_m:
            my_action = Action.D
        elif history.my_actions[-1]:
            my_action = Action.D
        elif self.rounds_since_success >= self.MAX_ROUNDS_TO_FORCE_COOPERATION and N_C_t_minus_1 == min_cooperators_m - 1:
            my_action = Action.C
        else:
            my_action = Action.D
        self.my_last_action = my_action
        return my_action

class Strategy_EXPLOITATIVE_126(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff (1+k) by selectively free-riding.
    It defects whenever possible, exploiting others' contributions, and only cooperates
    when its contribution is *critical* to reach the collective threshold 'm'.

    Decision rules by round:
    - Round 1 (First Round): Defects (D) to probe the environment and observe others' willingness to cooperate.
    - Last Round (Round r): Defects (D) as there are no future interactions to influence.
    - Intermediate Rounds (1 < t < r):
        - If the collective good was achieved in the previous round (N_C_prev >= m): Defects (D)
          to free-ride on established cooperation.
        - If the collective good was NOT achieved in the previous round (N_C_prev < m):
            - If my cooperation is critical (N_C_prev == m - 1): Cooperates (C) to ensure the
              collective good is met, securing a payoff of 'k' (which is > 1).
            - Otherwise (N_C_prev < m - 1): Defects (D) to avoid futile cooperation when
              my single contribution is insufficient.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_rounds = game_description.n_rounds
        self.min_cooperators = game_description.m
        self.n_players = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        n_c_prev_opponents = np.sum(history.opponent_actions[state.round_number - 1, :])
        n_c_prev_my_action = int(history.my_actions[state.round_number - 1])
        n_c_prev = n_c_prev_opponents + n_c_prev_my_action
        if n_c_prev >= self.min_cooperators:
            return Action.D
        elif n_c_prev == self.min_cooperators - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_127(BaseStrategy):
    """
    The Pragmatic Opportunist strategy aims to maximize its own payoff in the Collective Risk Dilemma
    by adaptively responding to the past behavior of other players. It prioritizes securing the collective
    'k' bonus, exploits opportunities to free-ride when the 'k' bonus is already likely, facilitates
    cooperation when its contribution is critical, and avoids the 'sucker's payoff' when cooperation
    is unlikely to meet the threshold. It uses a sliding window of recent actions to classify other
    players as likely cooperators, likely defectors, or uncertain, making calculated decisions based
    on these predictions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.WINDOW_SIZE = max(1, min(5, self.r - 1))
        self.COOPERATION_THRESHOLD_HIGH = 0.8
        self.DEFECTION_THRESHOLD_HIGH = 0.2
        self.UNCERTAIN_OPTIMISM_FACTOR = 0.6
        self.opponent_action_history = {op_idx: [] for op_idx in range(self.n - 1)}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0:
            for op_idx in range(self.n - 1):
                action = history.opponent_actions[state.round_number - 1, op_idx]
                history_list = self.opponent_action_history[op_idx]
                history_list.append(action)
                if len(history_list) > self.WINDOW_SIZE:
                    history_list.pop(0)
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        num_likely_cooperators = 0
        num_uncertain_players = 0
        for op_idx in range(self.n - 1):
            op_history = self.opponent_action_history[op_idx]
            if not op_history:
                num_uncertain_players += 1
                continue
            coop_count = op_history.count(True)
            coop_rate = coop_count / len(op_history)
            if coop_rate >= self.COOPERATION_THRESHOLD_HIGH:
                num_likely_cooperators += 1
            elif coop_rate < self.DEFECTION_THRESHOLD_HIGH:
                pass
            else:
                num_uncertain_players += 1
        if num_likely_cooperators >= self.m:
            return Action.D
        elif num_likely_cooperators == self.m - 1:
            return Action.C
        else:
            required_coop_total = self.m - num_likely_cooperators
            if required_coop_total > num_uncertain_players + 1:
                return Action.D
            needed_from_uncertain_if_I_C = required_coop_total - 1
            if num_uncertain_players > 0:
                if needed_from_uncertain_if_I_C / num_uncertain_players <= self.UNCERTAIN_OPTIMISM_FACTOR:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_EXPLOITATIVE_128(BaseStrategy):
    """
    Adaptive Conditional Free-Rider (ALF) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize payoff by free-riding on the cooperation of others,
    only contributing when it's critically necessary to achieve or restore the collective
    reward 'k'. It is an exploitative strategy that prioritizes individual gain but
    recognizes the need for collective action to unlock the highest payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_my_action = history.my_actions[state.round_number - 1]
        prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = sum(prev_round_opponent_actions) + int(prev_round_my_action)
        if num_cooperators_prev_round < m_threshold:
            return Action.C
        else:
            cooperators_from_others_prev_round = sum(prev_round_opponent_actions)
            if cooperators_from_others_prev_round >= m_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_129(BaseStrategy):
    """
    The Smart Free-Rider (SFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by free-riding (defecting)
    as much as possible. It only cooperates under specific circumstances:
    when its cooperation is the single critical contribution needed to reach
    the 'm' threshold, and doing so yields a higher immediate payoff (k vs 1)
    than defecting would. This ensures the strategy avoids contributing when
    its contribution is either redundant or insufficient, thereby maximizing
    its selfish gain. It explicitly defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.n_rounds - 1:
            return Action.D
        elif state.round_number == 0:
            return Action.D
        else:
            cooperators_in_previous_round = sum(history.opponent_actions[state.round_number - 1, :])
            if cooperators_in_previous_round >= self.m:
                return Action.D
            elif cooperators_in_previous_round == self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_130(BaseStrategy):
    """
    This strategy aims to free-ride on the cooperation of other players.
    It will defect whenever the collective reward threshold (`m`) is met,
    thus securing the maximum possible payoff ($1+k$). If cooperation levels
    drop below the threshold, leading to lower payoffs for everyone, the strategy
    will strategically cooperate to "recalibrate" the system and nudge it back
    towards collective success, but only if its own cooperation can make a difference.
    It avoids futile sacrifices.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.global_failure_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            self.global_failure_streak = 0
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        num_cooperators_last_round = np.sum(history.opponent_actions[previous_round_idx, :]) + int(history.my_actions[previous_round_idx])
        if num_cooperators_last_round >= m_threshold:
            self.global_failure_streak = 0
            return Action.D
        else:
            self.global_failure_streak += 1
            if num_cooperators_last_round + 1 >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_131(BaseStrategy):
    """
    Adaptive Free-Rider with Pessimism strategy for the Collective Risk Dilemma.
    This strategy aims to consistently free-ride (defect) when the collective threshold is met,
    benefiting from others' cooperation. It strategically cooperates only when the collective
    project is repeatedly failing, seeing this as a necessary investment to bring about the state
    where free-riding becomes possible again. However, it has a built-in "pessimism" or "patience"
    mechanism: if its attempts to "salvage" the collective effort consistently fail, it stops
    cooperating to cut its losses. It defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_tolerance_threshold = 2
        self.failure_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        c_prev_opponents = np.count_nonzero(history.opponent_actions[state.round_number - 1, :])
        c_prev_my_action = history.my_actions[state.round_number - 1].item()
        c_prev = c_prev_opponents + c_prev_my_action
        if c_prev >= self.game_description.m:
            self.failure_count = 0
            return Action.D
        else:
            self.failure_count += 1
            if self.failure_count < self.failure_tolerance_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_132(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to consistently achieve the highest possible individual payoff
    by free-riding on the cooperation of other players. It employs strategic cooperation only when
    necessary to ensure the public good threshold (`m`) is met, thus securing the `k` reward,
    which it then attempts to exploit by defecting.

    It works as follows:
    - Round 1: Cooperate to "invest" in setting up the public good.
    - Last Round: Always Defect to maximize immediate payoff as there are no future interactions.
    - Intermediate Rounds:
        - If the public good was met in the previous round (N_C_prev >= m), Defect to free-ride.
        - If the public good was NOT met (N_C_prev < m):
            - If only one more cooperator is needed (m - N_C_prev == 1), Cooperate to secure the `k` bonus (since k > 1).
            - Otherwise (more than one cooperator needed), Defect to avoid a wasted contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        n_rounds = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_prev_action = history.my_actions[state.round_number - 1]
        opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = np.sum(opponent_prev_actions) + my_prev_action
        if num_cooperators_prev_round >= m:
            return Action.D
        else:
            needed_cooperators = m - num_cooperators_prev_round
            if needed_cooperators == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_133(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize individual payoff by defaulting to
    free-riding. It strategically cooperates only when its contribution is pivotal to reaching
    the collective cooperation threshold (m) in intermediate rounds, thereby securing the reward
    'k' for itself. In the first and last rounds, it always defects, following an exploitative
    test-of-the-waters approach in the beginning and backward induction in the end.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_round_my_action_was_coop = history.my_actions[-1]
        prev_round_total_cooperators = prev_round_opponent_cooperators + prev_round_my_action_was_coop
        if prev_round_total_cooperators >= self.m_threshold:
            return Action.D
        elif prev_round_total_cooperators == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_134(BaseStrategy):
    """
    Adaptive Trigger-Defect strategy for the Collective Risk Dilemma.
    This strategy aims to secure the collective reward 'k' by strategically
    cooperating, and then free-ride as much as possible. It conditionally
    cooperates to establish or re-establish the 'k' reward but defects
    when conditions allow for free-riding, or after repeated failures to
    induce sufficient cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action_was_C: bool = False
        self.consecutive_threshold_failures: int = 0
        self.despair_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        chosen_action: Action
        if current_round_t == total_rounds_r:
            chosen_action = Action.D
        elif current_round_t == 1:
            chosen_action = Action.C
        else:
            previous_round_index = state.round_number - 1
            my_action_prev_round = history.my_actions[previous_round_index]
            opponent_actions_prev_round = history.opponent_actions[previous_round_index, :]
            C_prev = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
            if C_prev >= min_cooperators_m:
                self.consecutive_threshold_failures = 0
                if C_prev > min_cooperators_m:
                    chosen_action = Action.D
                elif self.my_last_action_was_C:
                    chosen_action = Action.C
                else:
                    chosen_action = Action.D
            else:
                self.consecutive_threshold_failures += 1
                if self.consecutive_threshold_failures <= self.despair_threshold:
                    chosen_action = Action.C
                else:
                    chosen_action = Action.D
        self.my_last_action_was_C = chosen_action == Action.C
        return chosen_action

class Strategy_EXPLOITATIVE_135(BaseStrategy):
    """
    Minimal Contributor Exploiter: This strategy adapts its contribution based on past group success.
    It seeds cooperation in the first round, free-rides when the collective threshold 'm' is met,
    and strategically contributes only when its single cooperation can push the group to meet 'm'.
    In the final round, it always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        reward_factor = self.game_description.k
        if current_round_number_1_indexed == 1:
            return Action.C
        if current_round_number_1_indexed == total_rounds:
            return Action.D
        my_action_prev_round = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
        n_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
        if n_cooperators_prev_round >= min_cooperators:
            return Action.D
        else:
            needed_cooperators_if_others_maintain = min_cooperators - n_cooperators_prev_round
            if needed_cooperators_if_others_maintain == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_136(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly exploitative, adaptive, and robust.
    It prioritizes maximizing its own payoff by free-riding on others' cooperation
    whenever possible, while also making calculated decisions to ensure the
    collective reward mechanism remains viable for future exploitation.
    It does not assume any specific opponent behavior, only reacting to
    the observed outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_number - 1
        num_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_idx]) + history.my_actions[previous_round_idx].item()
        if num_cooperators_previous_round < m_threshold:
            return Action.D
        elif num_cooperators_previous_round > m_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_137(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aims to maximize individual payoff by strategically free-riding on others' 
    cooperation, while also demonstrating conditional cooperation when it is individually profitable and critical 
    for achieving the collective reward. It adapts to observed behavior and leverages the game's payoff structure.

    The strategy starts by cooperating in Round 1 to probe the group.
    In intermediate rounds, it observes the number of cooperators from the previous round (C_prev):
    - If C_prev >= m: Defects to free-ride, securing 1+k.
    - If C_prev == m - 1: Cooperates, as its contribution is pivotal to reach m, securing k (since k > 1, k > 1).
    - If C_prev < m - 1: Defects, as its cooperation would be futile and wasted, securing 1.
    In the final round, it always defects due to no future repercussions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_cooperate = history.my_actions[-1]
            C_prev = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_cooperate else 0)
            if C_prev >= self.m_threshold:
                return Action.D
            elif C_prev == self.m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_138(BaseStrategy):
    """
    Adaptive Free-rider with Threshold Maintenance (AFM) strategy for the Collective Risk Dilemma.

    This strategy combines an initial investment in cooperation with dynamic decision-making
    that responds to opponents' actions. Its goal is to maximize its own payoff by free-riding
    when possible and contributing strategically when its cooperation is pivotal or necessary
    to maintain the collective benefit for future exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.initial_coop_rounds = max(2, min(self.m, self.n // 2))
        self.history_length = max(3, self.r // 4)
        self.opponent_history_cooperations = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        if state.round_number > 0 and history is not None:
            others_cooperated_prev = sum(history.opponent_actions[-1, :])
            self.opponent_history_cooperations.append(others_cooperated_prev)
        action_for_this_round = Action.D
        if current_t <= self.initial_coop_rounds:
            action_for_this_round = Action.C
        elif current_t == self.r:
            avg_others_coop_estimate = 0
            if self.opponent_history_cooperations:
                num_history_points = min(len(self.opponent_history_cooperations), self.history_length)
                if num_history_points > 0:
                    recent_history = self.opponent_history_cooperations[-num_history_points:]
                    avg_others_coop_estimate = round(sum(recent_history) / num_history_points)
            if avg_others_coop_estimate >= self.m:
                action_for_this_round = Action.D
            elif avg_others_coop_estimate == self.m - 1:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        elif not self.opponent_history_cooperations:
            action_for_this_round = Action.D
        else:
            others_cooperated_prev_round = self.opponent_history_cooperations[-1]
            if others_cooperated_prev_round >= self.m:
                action_for_this_round = Action.D
            elif others_cooperated_prev_round == self.m - 1:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        return action_for_this_round

class Strategy_EXPLOITATIVE_139(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to maximize its individual total payoff over the game
    by intelligently adapting its actions based on the observed history of cooperation and the critical
    game parameters, particularly `k`. It seeks to free-ride whenever possible, only contributing
    when its contribution is both pivotal and individually beneficial.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0 or current_round_0_indexed == self.r - 1:
            return Action.D
        cooperators_last_round_opponents = np.sum(history.opponent_actions[current_round_0_indexed - 1])
        my_action_last_round_bool = history.my_actions[current_round_0_indexed - 1]
        cooperators_last_round_my_contribution = 1 if my_action_last_round_bool else 0
        total_cooperators_last_round = cooperators_last_round_opponents + cooperators_last_round_my_contribution
        if total_cooperators_last_round >= self.m:
            return Action.D
        potential_total_cooperators_if_I_cooperate = total_cooperators_last_round + 1
        if potential_total_cooperators_if_I_cooperate < self.m:
            return Action.D
        elif self.k - 1 > 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_140(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma that maximizes individual payoff.
    It acts as a free-rider, prioritizing defection to gain a higher payoff (k+1) when the
    collective reward threshold `m` is met by others.

    Decision Rules:
    1.  **Initial Round (t=1 / state.round_number == 0):** Cooperate (C) as an initial probe
        to encourage collective action and observe the environment.
    2.  **Last Round (t=r / state.round_number == n_rounds - 1):** Defect (D) as there are
        no future interactions to influence, making defection the dominant strategy.
    3.  **Intermediate Rounds (1 < t < r):**
        *   If the collective threshold `m` was met in the *previous* round: Defect (D) to
            free-ride on the established cooperation.
        *   If the collective threshold `m` was *not* met in the *previous* round:
            *   If our single cooperation *in the current round* would be *exactly* what's
                needed to meet the threshold `m`: Cooperate (C). This is a calculated,
                minimal investment to 'prime the pump' and enable the collective reward,
                potentially leading to future free-riding opportunities.
            *   Otherwise (our cooperation alone is still insufficient): Defect (D) to
                protect our endowment and signal non-compliance, as our contribution would
                be wasted.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_idx]
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev = int(my_action_prev) + opponent_cooperators_prev
        if num_cooperators_prev >= m:
            return Action.D
        elif opponent_cooperators_prev + 1 == m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_141(BaseStrategy):
    """
    The Adaptive Freerider strategy for the Collective Risk Dilemma.
    It aims to maximize individual payoff by defecting when possible,
    but cooperates to initiate the collective project, repair it when it fails,
    or maintain it when its own contribution is critical.

    The strategy operates based on observed collective behavior from the previous round:
    - In Round 1, it always cooperates to encourage initial collective action.
    - In the final round, it always defects for immediate payoff maximization.
    - In intermediate rounds:
        - If the collective project failed previously, it cooperates to help restart it.
        - If the project succeeded:
            - If it cooperated previously and there was a surplus of cooperators, it defects to freeride.
            - If it cooperated previously and its contribution was critical, it cooperates again to maintain the project.
            - If it defected previously and the project still succeeded, it continues to defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + int(my_action_prev)
        if num_cooperators_prev < self.m:
            return Action.C
        elif my_action_prev:
            if num_cooperators_prev > self.m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_142(BaseStrategy):
    """
    Adaptive Free-Rider with Contingent Support (AFR-CS)

    This strategy aims to maximize individual gain by free-riding whenever the collective
    good is successfully achieved by others. It strategically contributes to the community
    project only when necessary to re-establish a cooperative environment, thereby ensuring
    future opportunities for free-riding. It adapts its behavior based on the outcome of
    the previous round and its own previous action.

    Decision Rules:
    - Round 1: Always Cooperate (C) to probe for cooperation.
    - Final Round: Always Defect (D) to maximize terminal payoff due to no future repercussions.
    - Intermediate Rounds:
        - If the community project succeeded in the previous round: Defect (D) to free-ride.
        - If the community project failed in the previous round:
            - If this player cooperated previously: Defect (D) to avoid being a "sucker".
            - If this player defected previously: Cooperate (C) to try and "kickstart" cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        n_c_prev = num_opponent_cooperators_prev + int(my_action_prev)
        if n_c_prev >= min_cooperators_m:
            return Action.D
        elif my_action_prev == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_143(BaseStrategy):
    """
    The Adaptive Opportunist (AO) strategy is designed to free-ride whenever possible,
    only contributing to the collective good when it is individually critical and
    profitable to do so. It aims to maximize individual payoff over 'r' rounds by
    leveraging opponent behavior and the game's mechanics, avoiding being a "sucker"
    by not cooperating when contribution is futile or unnecessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        my_prev_action_cooperated = int(history.my_actions[-1])
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        C_prev = my_prev_action_cooperated + opponent_prev_cooperators
        if C_prev >= m_threshold:
            return Action.D
        elif C_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_144(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) Strategy for the Collective Risk Dilemma.
    This strategy is highly exploitative, adaptive, and robust. It prioritizes maximizing its own payoff
    by free-riding whenever possible, while conditionally cooperating only when its contribution is critical
    to unlock the collective reward, which it then seeks to exploit.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_prev_action_is_C = history.my_actions[current_round_0_indexed - 1]
        opponent_prev_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        cooperators_in_prev_round = int(my_prev_action_is_C) + np.sum(opponent_prev_actions)
        if cooperators_in_prev_round >= self.m_threshold:
            return Action.D
        else:
            cooperators_needed_to_meet_m = self.m_threshold - cooperators_in_prev_round
            if cooperators_needed_to_meet_m == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_145(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own total payoff by primarily free-riding
    on the cooperation of other players. It will only contribute to the collective
    good (cooperate) when there's a significant and sustained risk of the collective
    reward 'k' being lost for multiple rounds. Once the threat is averted, it
    immediately reverts to free-riding. It adapts by reacting to the observed
    collective behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_last_success = 0
        self.FAILURE_TOLERANCE_THRESHOLD = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.rounds_since_last_success = 0
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        did_i_cooperate_last_round = history.my_actions[-1]
        total_cooperators_last_round = num_opponent_cooperators_last_round + did_i_cooperate_last_round
        threshold_met_last_round = total_cooperators_last_round >= self.game_description.m
        if threshold_met_last_round:
            self.rounds_since_last_success = 0
        else:
            self.rounds_since_last_success += 1
        if self.rounds_since_last_success >= self.FAILURE_TOLERANCE_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_146(BaseStrategy):
    """
    The Adaptive Exploiter strategy prioritizes defection (free-riding) to maximize personal
    payoff in the Collective Risk Dilemma. It strategically contributes only when its
    contribution is deemed critical to achieve the collective reward (k), and it hardens
    its stance to consistent defection if the collective effort consistently fails.
    The strategy defects in the first and last rounds as a default exploitative move.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Exploiter strategy with game parameters.
        No persistent internal state beyond game description is needed across rounds
        as all historical information is provided via the `history` object in `__call__`.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the strategy's adaptive rules.

        Args:
            state: The current state of the game, including the current round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     It is None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        my_last_action_is_cooperate = history.my_actions[-1]
        total_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + my_last_action_is_cooperate
        if total_cooperators_last_round >= m:
            if total_cooperators_last_round > m:
                return Action.D
            elif my_last_action_is_cooperate:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_147(BaseStrategy):
    """
    Calculated Opportunist (CO) strategy for the Collective Risk Dilemma.
    This strategy probes for cooperation, free-rides when possible, contributes
    only when pivotal, and defects in end-game.

    Strategy Details:
    1.  Round 1: Cooperate to probe for others' cooperation.
    2.  Last Round: Defect to maximize immediate payoff, as there are no future interactions.
    3.  Intermediate Rounds:
        a.  If threshold (m) was met last round:
            i.  If N_C_prev > m (buffer of cooperators): Defect to free-ride (1+k).
            ii. If N_C_prev == m (exactly m cooperators):
                - If my previous action was C: Cooperate to sustain the threshold and secure 'k' (k > 1).
                - If my previous action was D: Continue defecting, as threshold was met without my help.
        b.  If threshold (m) was NOT met last round:
            i.  If N_C_prev == m - 1 (one cooperator short): Cooperate, as my contribution is pivotal and yields 'k'.
            ii. If N_C_prev < m - 1 (more than one short): Defect to avoid 'sucker's payoff' (0).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        prev_my_action_is_C = history.my_actions[-1]
        prev_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        N_C_prev = prev_opponent_cooperators + (1 if prev_my_action_is_C else 0)
        if N_C_prev >= m:
            if N_C_prev > m:
                return Action.D
            elif prev_my_action_is_C:
                return Action.C
            else:
                return Action.D
        elif N_C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_148(BaseStrategy):
    """
    The "Adaptive Exploit & Nudge" (AEN) strategy for the Collective Risk Dilemma.
    It dynamically balances free-riding with strategic cooperation, aiming to exploit
    existing cooperation, contribute minimally when critical, and "nudge" the group
    towards cooperation if it's failing but there's still time and potential.
    It becomes purely self-interested in the game's final stages.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: A dataclass containing n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on the AEN strategy.

        Args:
            state: A dataclass containing the current round_number (0-indexed).
            history: A dataclass containing actions and payoffs from previous rounds.
                     Is None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        c_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if c_prev_round >= self.game_description.m:
            return Action.D
        elif c_prev_round == self.game_description.m - 1:
            return Action.C
        else:
            rounds_remaining = self.game_description.n_rounds - (state.round_number + 1)
            if rounds_remaining <= self.game_description.n_rounds / 3 or c_prev_round <= self.game_description.m / 2:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_149(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to be highly self-interested and opportunistic.
    It monitors the collective behavior of other players to determine the minimum effort required
    (if any) to secure the communal reward 'k', or to at least guarantee a private payoff of '1'.
    It prioritizes its own gains (1+k or k) and avoids situations that yield 0.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        last_round_0_indexed = n_rounds - 1
        if current_round_0_indexed == last_round_0_indexed:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        prev_round_index = current_round_0_indexed - 1
        N_C_others_prev = sum(history.opponent_actions[prev_round_index, :])
        if N_C_others_prev >= m_threshold:
            return Action.D
        elif N_C_others_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_150(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma, designed to maximize
    individual payoff by free-riding when possible and cooperating strategically
    when cooperation is pivotal to achieving the threshold or initiating collective action.

    Decision Rules:
    1. Round 1: Cooperate (C) to signal willingness and gather data.
    2. Intermediate Rounds (2 to r-1):
       - If the collective threshold (`m`) was met in the previous round: Defect (D) to free-ride.
       - If the collective threshold (`m`) was NOT met in the previous round:
         - If other players were "close" to meeting the threshold
           (i.e., (m-1) or more cooperators from other players): Cooperate (C) because my contribution
           could be pivotal to achieve the threshold in the current round.
         - If other players were "far" from meeting the threshold
           (i.e., fewer than (m-1) cooperators from other players): Defect (D) to avoid
           being exploited as a lone cooperator in a losing cause.
    3. Last Round (r): Defect (D) due to backward induction, as there are no future rounds to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        my_action_prev_round_bool = history.my_actions[-1]
        opponent_actions_prev_round_bool = history.opponent_actions[-1, :]
        N_C_others_prev_round = np.sum(opponent_actions_prev_round_bool)
        N_C_total_prev_round = N_C_others_prev_round + (1 if my_action_prev_round_bool else 0)
        if N_C_total_prev_round >= m_threshold:
            return Action.D
        elif N_C_others_prev_round >= m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_151(BaseStrategy):
    """
    The "Opportunistic Pusher" strategy for Collective Risk Dilemma.

    This strategy aims to consistently achieve the 'k' bonus for the group, primarily by free-riding
    but being willing to "push" the group over the threshold (or restart cooperation from scratch)
    when necessary, as a calculated self-interested investment.

    Decision Rules per Round t (where round_num is 0-indexed):

    1.  Round 0 (Initial Probe):
        Action: Cooperate (C)
        Rationale: Initial investment to signal willingness and test cooperation.

    2.  Last Round (state.round_number == self.game_description.n_rounds - 1):
        Action: Defect (D)
        Rationale: No future rounds to influence; end-game rationality.

    3.  Middle Rounds:
        Observe N_C_prev: Total number of players who cooperated in the previous round.
        Recall my_previous_action_is_C: My own action in the previous round.

        Rule A: Free-Ride on Established Cooperation
            Condition: N_C_prev >= self.game_description.m
            Action: Defect (D)
            Rationale: The group successfully achieved the 'k' bonus. Exploit by defecting for 1 + k.

        Rule B: Re-Induce Cooperation from Total Failure
            Condition: N_C_prev == 0 (No one cooperated in the previous round).
            Action: Cooperate (C)
            Rationale: If 'k' was missed entirely, make a self-interested investment (cost 0, potential k)
                       to re-establish 'k' potential, which is better than guaranteed 1.

        Rule C: Push to Meet the Threshold When Close (Partial Failure)
            Condition: 0 < N_C_prev < self.game_description.m, AND my cooperation would meet 'm'.
                       Specifically: actual_other_cooperators_prev_round >= (self.game_description.m - 1)
            Action: Cooperate (C)
            Rationale: My cooperation is exactly what's needed to meet 'm'. Payoff is 'k' if I cooperate,
                       '1' if I defect (threshold not met). Since k > 1, C is better.

        Rule D: Defect When Cooperation is Insufficient/Wasted
            Condition: All other conditions for middle rounds are not met.
            Action: Defect (D)
            Rationale: Cooperation would be a wasted cost, as my contribution wouldn't meet 'm',
                       or too few others are cooperating. Prioritize securing base payoff of 1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_previous_action_is_C = history.my_actions[-1]
        cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        N_C_prev = cooperators_from_opponents_prev_round + (1 if my_previous_action_is_C else 0)
        if N_C_prev >= m_threshold:
            return Action.D
        else:
            actual_other_cooperators_prev_round = N_C_prev - (1 if my_previous_action_is_C else 0)
            if N_C_prev == 0:
                return Action.C
            elif actual_other_cooperators_prev_round >= m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_152(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by free-riding on the cooperation
    of others, only contributing when its cooperation is critical to meet the 'm'
    threshold and secure the 'k' bonus for all, including itself. It performs
    an initial probe in round 1, adapts its behavior based on previous round's
    cooperation levels, and defects unconditionally in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        else:
            opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_action_prev = int(history.my_actions[state.round_number - 1])
            C_prev = opponent_cooperators_prev + my_action_prev
            if C_prev >= m:
                return Action.D
            elif C_prev == m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_153(BaseStrategy):
    """
    Opportunistic Reset strategy for the Collective Risk Dilemma.
    Defaults to defection, but intervenes with a temporary cooperation if
    the collective threshold consistently fails to be met, to prevent
    prolonged loss of the collective reward 'k'. Reverts to defection
    as soon as the threshold is met again.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._reset_cooperation_duration = 1
        self._tolerance_before_reset = 2
        self._consecutive_failed_threshold_rounds = 0
        self._forced_cooperation_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        my_prev_action_is_C = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        N_C_prev = int(my_prev_action_is_C) + opponent_prev_cooperators
        met_threshold_prev = N_C_prev >= m_threshold
        if met_threshold_prev:
            self._consecutive_failed_threshold_rounds = 0
            self._forced_cooperation_counter = 0
        else:
            self._consecutive_failed_threshold_rounds += 1
        if current_round_1_indexed == n_rounds:
            return Action.D
        if self._forced_cooperation_counter > 0:
            self._forced_cooperation_counter -= 1
            return Action.C
        if met_threshold_prev:
            return Action.D
        if self._consecutive_failed_threshold_rounds >= self._tolerance_before_reset:
            self._forced_cooperation_counter = self._reset_cooperation_duration - 1
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_154(BaseStrategy):
    """
    Calculated Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding on the
    cooperation of others. It strategically cooperates in the first round as a
    probe, and then defects if the cooperation threshold is met. If cooperation
    fails, it uses a calculated approach:
    - If it previously defected and cooperation failed, it tries to re-establish
      cooperation by cooperating itself.
    - If it previously cooperated and cooperation still failed, it defects to
      avoid being a 'sucker'.
    - If cooperation fails for two or more consecutive rounds, it defects to
      prioritize the base payoff, abandoning efforts to achieve the 'k' bonus.
    In the final round, it always defects due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_streak_fail_count: int = 0
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        action_to_take: Action
        if current_round_t == 1:
            action_to_take = Action.C
        elif current_round_t == self.game_description.n_rounds:
            action_to_take = Action.D
        else:
            num_cooperators_opponents_prev = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_bool = history.my_actions[-1]
            num_cooperators_prev = num_cooperators_opponents_prev + int(my_action_prev_round_bool)
            threshold_met_prev = num_cooperators_prev >= self.game_description.m
            if threshold_met_prev:
                self.cooperation_streak_fail_count = 0
                action_to_take = Action.D
            else:
                self.cooperation_streak_fail_count += 1
                if self.cooperation_streak_fail_count >= 2:
                    action_to_take = Action.D
                elif self.my_last_action == Action.D:
                    action_to_take = Action.C
                else:
                    action_to_take = Action.D
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_155(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to maximize individual payoff
    by free-riding whenever the collective reward is sufficiently secured by others.
    It adapts to the aggregated behavior of opponents in the previous round,
    only contributing when the collective reward 'k' is at significant risk,
    or when it needs to signal a willingness to restore collective action.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_is_C = history.my_actions[-1]
        N_C_prev = num_opponent_cooperators_prev + int(my_action_prev_is_C)
        if N_C_prev >= self.m_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_156(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the Collective Risk Dilemma.

    This strategy aims to exploit other players' cooperation by selectively
    contributing when cooperation is critically needed to trigger the 'k' bonus,
    and then aggressively free-riding on that success. It acts as a "scaffolder"
    when the group is near success and a "free-rider" when cooperation is established.
    It avoids being a "sucker" by not repeatedly contributing when its previous
    cooperation failed to meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing past actions and payoffs
                                           for this player and opponents. Is None for round 0.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        previous_round_0_indexed = current_round_0_indexed - 1
        my_action_prev_is_C = history.my_actions[previous_round_0_indexed]
        opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        N_C_prev = opponent_cooperators_prev + (1 if my_action_prev_is_C else 0)
        if N_C_prev >= m:
            return Action.D
        elif my_action_prev_is_C:
            return Action.D
        elif N_C_prev >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_157(BaseStrategy):
    """
    The Adaptive Conditional Free-Rider (ACFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by balancing free-riding with strategic
    cooperation. It observes the previous round's cooperation levels and its own action to decide
    whether to defect and free-ride on others' cooperation, or to cooperate strategically when
    its contribution is pivotal to secure the collective reward 'k' (given k > 1).

    Decision rules adapt to the current round number (first, last, or intermediate) and the
    observed cooperation level in the immediate preceding round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_idx = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_idx == n_rounds - 1:
            return Action.D
        if current_round_0_idx == 0:
            return Action.D
        prev_round_0_idx = current_round_0_idx - 1
        my_prev_action_bool = history.my_actions[prev_round_0_idx]
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_0_idx, :])
        prev_round_N_C = int(opponent_prev_cooperators) + int(my_prev_action_bool)
        if prev_round_N_C > m_threshold:
            return Action.D
        if prev_round_N_C == m_threshold:
            if my_prev_action_bool:
                return Action.C
            else:
                return Action.D
        if prev_round_N_C == m_threshold - 1:
            if my_prev_action_bool:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_158(BaseStrategy):
    """
    The Adaptive Provocateur strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily free-riding on
    the cooperation of others. It defaults to defection, only cooperating under
    specific conditions to shift the collective outcome from a low-payoff state
    to one where the cooperation threshold 'm' is met. It quickly punishes
    situations where its own cooperation fails to yield the collective good.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_history_idx = state.round_number - 1
        my_action_t_minus_1 = history.my_actions[prev_round_history_idx]
        my_cooperation_count = int(my_action_t_minus_1)
        opponent_cooperation_count = np.sum(history.opponent_actions[prev_round_history_idx, :])
        N_C_t_minus_1 = my_cooperation_count + opponent_cooperation_count
        if N_C_t_minus_1 >= self.m:
            return Action.D
        elif my_action_t_minus_1 == Action.C:
            return Action.D
        else:
            cooperators_from_others_prev = N_C_t_minus_1
            if self.m - cooperators_from_others_prev == 1:
                return Action.C
            elif cooperators_from_others_prev >= math.floor(self.m / 2.0):
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_159(BaseStrategy):
    """
    Adaptive Defector with Threshold Push strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by defecting to free-ride
    when the cooperation threshold is met or likely to be met without its contribution.
    It strategically cooperates only when its contribution is pivotal to reach the
    cooperation threshold 'm', thereby securing the 'k' bonus which is strictly
    greater than the private payoff from defecting without the bonus (k > 1 vs 1).
    It will never intentionally choose an action that results in a zero payoff,
    meaning it avoids contributing when its cooperation alone is insufficient to
    meet the threshold. The strategy also adapts to the finite game horizon,
    changing its behavior in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        my_action_prev_str = 'C' if my_action_prev_bool else 'D'
        N_C_prev_opponents = np.sum(history.opponent_actions[-1, :])
        N_C_prev = N_C_prev_opponents + (1 if my_action_prev_bool else 0)
        if current_round_t == self.n_rounds:
            if N_C_prev >= self.m_threshold:
                return Action.D
            elif N_C_prev == self.m_threshold - 1:
                return Action.C
            else:
                return Action.D
        elif N_C_prev >= self.m_threshold:
            if N_C_prev == self.m_threshold and my_action_prev_str == 'C':
                return Action.C
            else:
                return Action.D
        elif N_C_prev == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_160(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the Collective Risk Dilemma.
    It primarily free-rides on others' contributions, only cooperating
    when its contribution is singularly critical for its own benefit,
    and always abandoning cooperation when the game ends.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        num_cooperators_last_round = int(history.my_actions[previous_round_0_indexed]) + np.sum(history.opponent_actions[previous_round_0_indexed, :])
        threshold_met_last_round = num_cooperators_last_round >= self.m
        if threshold_met_last_round:
            return Action.D
        else:
            needed_for_m = self.m - num_cooperators_last_round
            if needed_for_m == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_161(BaseStrategy):
    """
    The "Adaptive Free-Rider" (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by strategically alternating
    between contributing to the collective good and free-riding on others' contributions.
    It leverages the repeated nature of the game to adapt to observed outcomes,
    ensuring the collective reward ('k') is secured whenever possible, while
    simultaneously aiming to be a defector to claim the highest individual payoff ('1+k').

    - In the first round (Round 0), it cooperates to "seed" the collective good and
      gather information.
    - In intermediate rounds, it observes the outcome of the previous round:
        - If the cooperation threshold ('m') was met, it defects to free-ride.
        - If the threshold was not met, it cooperates to help restore the collective benefit,
          treating this as an investment for future free-riding opportunities.
    - In the last round, it always defects, as there are no future consequences to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            prev_round_index = current_round_number - 1
            n_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_index, :])
            my_action_prev = history.my_actions[prev_round_index]
            n_cooperators_prev = n_opponent_cooperators_prev + my_action_prev
            if n_cooperators_prev >= self.m:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_162(BaseStrategy):
    """
    The "Calculated Opportunist" strategy is designed for aggressive free-riding
    while being pragmatic enough to ensure the collective good is occasionally provided,
    thus enabling future exploitation. It operates on the principle of maximizing
    its own payoff of (1+k) by free-riding, only contributing its endowment (ci=1)
    as a last resort to "kickstart" the system if it consistently fails and the
    contribution is perceived as critical.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.F: int = 2
        self.consecutive_failure_rounds: int = 0
        self.my_last_action: Action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed: int = state.round_number + 1
        if current_round_1_indexed == self.r:
            self.my_last_action = Action.D
            return Action.D
        if current_round_1_indexed == 1:
            self.consecutive_failure_rounds = 0
            self.my_last_action = Action.D
            return Action.D
        num_cooperators_prev_round: int = np.sum(history.opponent_actions[state.round_number - 1, :]) + int(history.my_actions[state.round_number - 1])
        my_action_prev_round_was_C: bool = history.my_actions[state.round_number - 1]
        if num_cooperators_prev_round >= self.m:
            self.consecutive_failure_rounds = 0
            self.my_last_action = Action.D
            return Action.D
        else:
            self.consecutive_failure_rounds += 1
            if my_action_prev_round_was_C:
                self.my_last_action = Action.D
                return Action.D
            elif self.consecutive_failure_rounds >= self.F and num_cooperators_prev_round == self.m - 1:
                self.my_last_action = Action.C
                return Action.C
            else:
                self.my_last_action = Action.D
                return Action.D

class Strategy_EXPLOITATIVE_163(BaseStrategy):
    """
    The "Opportunistic K-Seeker" (OKS) strategy aims to maximize its total payoff by primarily attempting to free-ride
    (defect while others cooperate) to achieve the collective reward `k`. However, it is designed to strategically
    cooperate when its contribution is critical to unlock `k` or to revive cooperation, ensuring that it doesn't leave
    potential `k` gains on the table if it can help secure them. It adapts its behavior based on the observed
    cooperation level in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        my_action_prev = history.my_actions[current_round_0_indexed - 1]
        n_cooperators_opponents_prev = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        n_c_prev = n_cooperators_opponents_prev + (1 if my_action_prev else 0)
        if n_c_prev >= self.m:
            if n_c_prev > self.m:
                return Action.D
            elif my_action_prev:
                return Action.C
            else:
                return Action.D
        elif n_c_prev + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_164(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by free-riding when possible,
    ensuring the collective reward when critical, and avoiding futile contributions.
    It adapts its behavior based on the observed cooperation levels in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        n_other_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        if n_other_cooperators_prev_round >= m_threshold:
            return Action.D
        elif n_other_cooperators_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_165(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy aims to maximize personal payoff over the entire game
    by primarily free-riding on the cooperation of others, but making calculated, minimal
    contributions when necessary to unlock or sustain the collective reward.

    Core Principles:
    1. Prioritize Defection: Always defect if the collective benefit ('k') is already secured by others,
       allowing for the maximum individual payoff ('1+k').
    2. Strategic Contribution: Only cooperate when a contribution is likely to directly benefit *my*
       current round's payoff (e.g., securing 'k' instead of '1'), or when a minimal, high-leverage
       investment is made to initiate or prevent the collapse of the collective benefit for future
       free-riding opportunities.
    3. Avoid Being a Sucker: Do not cooperate if the contribution is unlikely to move the collective
       closer to the threshold 'm', especially if it results in a lower payoff for the player (e.g., '0' instead of '1').
    4. Endgame Exploitation: Revert to pure self-interest (defect) in the final round as there are
       no future interactions to consider.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        cooperators_from_others_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_round_was_C_val = int(history.my_actions[state.round_number - 1])
        cooperators_in_prev_round = cooperators_from_others_prev_round + my_action_prev_round_was_C_val
        if cooperators_in_prev_round >= self.m_threshold:
            return Action.D
        elif cooperators_in_prev_round == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_166(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by intelligently free-riding,
    making critical contributions when necessary to achieve the collective reward,
    and avoiding "sucker" payoffs. It adapts its behavior based on the observed
    actions of other players in the previous round.

    Core principles:
    1. Initial Investment: Cooperate in Round 1 to signal willingness and unlock potential collective rewards.
    2. Adaptive Exploitation: In middle rounds, analyze if others' cooperation (excluding mine) meets,
       is one short of, or falls far short of the threshold 'm'.
       - If 'm' is met by others: Defect and free-ride (1+k payoff).
       - If 'm' is one short: Cooperate to critically achieve 'm' (k payoff, which is > 1).
       - If 'm' is far short: Defect to avoid a sucker's payoff (1 payoff, versus 0 for ineffective cooperation).
    3. Terminal Exploitation: Defect in the final round as there are no future consequences to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        num_other_cooperators_prev = np.sum(previous_round_opponent_actions)
        if num_other_cooperators_prev >= m:
            return Action.D
        elif num_other_cooperators_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_167(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily free-riding on
    others' cooperation. It strategically defects when the collective has met
    the cooperation threshold, avoiding personal cost while receiving the 'k' bonus.
    However, it will cooperate in a specific scenario: when its individual cooperation
    is decisive for meeting the 'm' threshold, and doing so provides a better
    individual payoff (k) than defecting (1), given that k > 1.
    The strategy also "punishes" wasteful cooperation by defecting if its previous
    cooperation failed to secure the collective bonus.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATE strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the ATE strategy rules.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for
                     this player and opponents, or None if it's the first round.

        Returns:
            Action: Action.C for Cooperate or Action.D for Defect.
        """
        if state.round_number == 0:
            return Action.D
        prev_cooperators = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        my_prev_action_was_C = history.my_actions[-1]
        if prev_cooperators >= self.m:
            return Action.D
        elif my_prev_action_was_C:
            return Action.D
        elif prev_cooperators == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_168(BaseStrategy):
    """
    The Calculated Free-Rider (CFR) strategy aims to exploit the cooperative
    tendencies of others in the Collective Risk Dilemma. It free-rides by defecting,
    only cooperating when its individual action is critical to achieving the
    collective reward 'k' and doing so yields a higher individual payoff than defecting.
    It adapts to the observed level of cooperation from other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_zero_indexed = state.round_number
        last_round_zero_indexed = n_rounds - 1
        if current_round_zero_indexed == last_round_zero_indexed:
            return Action.D
        if current_round_zero_indexed == 0:
            return Action.C
        cooperators_among_others_prev = np.sum(history.opponent_actions[-1, :])
        if cooperators_among_others_prev >= m:
            return Action.D
        elif cooperators_among_others_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_169(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to be highly exploitative, prioritizing individual payoff
    maximization while adapting to collective behavior and cutting losses when
    cooperation is futile. It navigates the Collective Risk Dilemma by strategically
    contributing to (or withholding from) the common good based on observed outcomes
    and the progression of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.endgame_horizon = 2
        self.failure_streak_threshold = 3
        self.consecutive_failure_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds_total = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        my_last_action_was_C = history.my_actions[-1]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        cooperators_last_round = opponent_cooperators_last_round + (1 if my_last_action_was_C else 0)
        if cooperators_last_round < m_threshold:
            self.consecutive_failure_rounds += 1
        else:
            self.consecutive_failure_rounds = 0
        if n_rounds_total - current_round_0_indexed <= self.endgame_horizon:
            return Action.D
        if self.consecutive_failure_rounds >= self.failure_streak_threshold:
            return Action.D
        if cooperators_last_round < m_threshold:
            return Action.C
        elif my_last_action_was_C and cooperators_last_round == m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_170(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily defecting to free-ride
    on others' cooperation. It only cooperates when its contribution is absolutely
    critical to achieving the collective reward 'k', and doing so yields a higher
    personal payoff (k vs 1) than defecting in that specific scenario. It avoids
    cooperating when its contribution would be insufficient to meet the threshold.

    The strategy adapts its behavior based on the number of cooperators observed in
    the previous round among other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        action = Action.D
        if current_round_0_indexed == 0:
            action = Action.D
        elif current_round_0_indexed == self.n_rounds - 1:
            action = Action.D
        else:
            num_other_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
            if num_other_cooperators_prev_round >= self.m:
                action = Action.D
            elif num_other_cooperators_prev_round == self.m - 1:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_171(BaseStrategy):
    """
    The Adaptive Pivotal Free-Rider (APFR) strategy aims to maximize its own total payoff
    by leveraging the collective reward (k) while minimizing its individual contribution.
    It prioritizes free-riding but strategically cooperates when its action is pivotal
    to achieving the collective good, which in turn benefits the strategy player.

    Strategic Principles:
    1. Default to Defect (D): Primary inclination is to defect.
    2. Pivotal Cooperation: Cooperates only when its contribution is precisely what's needed
       to meet the cooperation threshold (m), changing failure to success.
    3. Adaptive Learning: Observes the number of cooperators in the previous round to
       infer the current state of collective action.
    4. Endgame Exploitation: Defects in the final round, anticipating rational defection
       from others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators_history: list[int] = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed_last_round = self.game_description.n_rounds - 1
        if current_round_0_indexed > 0 and history is not None:
            prev_round_index = current_round_0_indexed - 1
            cooperators_from_opponents = np.sum(history.opponent_actions[prev_round_index, :])
            my_prev_action_cooperated = 1 if history.my_actions[prev_round_index] else 0
            c_prev_total = cooperators_from_opponents + my_prev_action_cooperated
            self.num_cooperators_history.append(c_prev_total)
        if current_round_0_indexed == 0:
            return Action.D
        elif current_round_0_indexed == total_rounds_0_indexed_last_round:
            return Action.D
        else:
            c_prev_from_history = self.num_cooperators_history[-1]
            m_threshold = self.game_description.m
            if c_prev_from_history >= m_threshold:
                return Action.D
            elif c_prev_from_history == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_172(BaseStrategy):
    """
    Adaptive Free-Rider with Contingent Cooperation strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own payoff by generally defecting. It only cooperates
    in intermediate rounds if its single cooperation is precisely what is needed to reach
    the collective threshold (m), thereby securing the k reward for itself.
    It always defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        N_c_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if N_c_others_prev_round >= m:
            return Action.D
        elif N_c_others_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_173(BaseStrategy):
    """
    Opportunistic Pumper: This strategy aims to maximize its own total payoff by ensuring
    the collective reward 'k' is frequently obtained, while minimizing individual contribution
    by free-riding whenever the cooperation threshold 'm' is met.

    It cooperates in the first round as an initial investment and probe.
    In intermediate rounds, it dynamically adjusts: it defects (free-rides) if the collective
    threshold 'm' was met in the previous round, and cooperates ("invests") if the threshold
    was missed.
    In the final round, it defects to ruthlessly exploit the lack of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_prev_action_is_C = history.my_actions[state.round_number - 1]
            opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
            prev_C_count = np.sum(opponent_prev_actions) + my_prev_action_is_C
            if prev_C_count >= self.game_description.m:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_174(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to maximize payoff by primarily free-riding,
    but adapts to maintain cooperation conditions. It strategically cooperates in the first round
    to prime the pump, cooperates when exactly one cooperator is needed to meet the threshold (k > 1 implies this is beneficial),
    and otherwise defects, especially when the threshold was met (free-riding) or when cooperation is futile/too costly (punishment).
    It always defects in the last round to exploit endgame unraveling.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == n_rounds - 1:
            return Action.D
        elif state.round_number == 0:
            return Action.C
        n_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            n_cooperators_prev_round += 1
        if n_cooperators_prev_round >= m:
            return Action.D
        elif n_cooperators_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_175(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by initially probing for
    cooperation, then free-riding when possible, contributing only when its
    cooperation is essential to secure the collective reward 'k' (since k > 1),
    and punishing non-cooperation by defecting. It exploits the last round by
    always defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the
        strategy's logic and game history.

        Args:
            state: Current game state, including the current round number.
            history: Past actions and payoffs of this player and opponents.
                     None for the very first round (round_number == 0).

        Returns:
            An Action enum value (C for Cooperate, D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_number = state.round_number
        if current_round_number == n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.C
        previous_round_my_action = history.my_actions[current_round_number - 1]
        previous_round_opponent_actions = history.opponent_actions[current_round_number - 1, :]
        total_cooperators_in_prev_round = np.sum(previous_round_opponent_actions) + int(previous_round_my_action)
        num_cooperators_if_i_defected_prev_round = np.sum(previous_round_opponent_actions)
        if num_cooperators_if_i_defected_prev_round >= m_threshold:
            return Action.D
        elif total_cooperators_in_prev_round >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_176(BaseStrategy):
    """
    The Adaptive Buffer Defector strategy aims to free-ride on others' cooperation,
    but it's adaptive and robust. It contributes when cooperation is critical
    to secure the collective reward 'k', especially when the margin of cooperators
    is thin or after consecutive failures to meet the threshold 'm'.
    It also accounts for the finite number of rounds, defecting in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def _get_consecutive_failures(self, history: PlayerHistory, m: int) -> int:
        """
        Calculates the number of consecutive previous rounds where the total
        number of cooperators was less than the minimum required 'm'.
        """
        count = 0
        for i in range(len(history.my_actions) - 1, -1, -1):
            my_action_round_i = history.my_actions[i]
            opponent_actions_round_i = history.opponent_actions[i, :]
            total_cooperators_round_i = int(my_action_round_i) + np.sum(opponent_actions_round_i)
            if total_cooperators_round_i < m:
                count += 1
            else:
                break
        return count

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_t_1_indexed = state.round_number + 1
        if current_round_t_1_indexed == 1:
            return Action.D
        my_last_action_bool = history.my_actions[-1]
        num_cooperators_prev_my_action = int(my_last_action_bool)
        num_cooperators_prev_opponents = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev = num_cooperators_prev_my_action + num_cooperators_prev_opponents
        consecutive_failures = self._get_consecutive_failures(history, m)
        if current_round_t_1_indexed == r:
            return Action.D
        if num_cooperators_prev >= m:
            buffer_prev = num_cooperators_prev - m
            if buffer_prev >= 2:
                return Action.D
            elif buffer_prev == 1:
                if my_last_action_bool:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.C
        elif consecutive_failures >= 2:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_177(BaseStrategy):
    """
    The Opportunistic Free-Rider (OFR) strategy maximizes individual payoff by primarily
    free-riding on the cooperation of others, while strategically contributing only when
    such contribution is pivotal to ensuring the collective reward (k) and thereby
    maximizing one's own expected payoff. It leverages the "free-rider" incentive
    inherent in the Collective Risk Dilemma, while being adaptive to the observed
    behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number + 1
        if current_round_num == self.r:
            return Action.D
        if current_round_num == 1:
            return Action.D
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        num_cooperators_prev_round = int(my_action_prev_round_is_cooperate) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.m:
            if not my_action_prev_round_is_cooperate:
                return Action.D
            elif num_cooperators_prev_round > self.m:
                return Action.D
            else:
                return Action.C
        elif num_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_178(BaseStrategy):
    """
    The Conditional Free-Rider (CFR) strategy aims to maximize individual payoff
    by predominantly free-riding. It only contributes when its cooperation is
    both necessary to achieve the collective reward and individually profitable,
    while avoiding being a "sucker" by contributing when success is unlikely
    or unbeneficial. The strategy adapts to observed group behavior and accounts
    for the finite nature of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_was_C = history.my_actions[-1]
        N_C_prev = opponent_cooperators_prev + (1 if my_action_prev_was_C else 0)
        if N_C_prev >= self.m:
            return Action.D
        if N_C_prev == self.m - 1:
            if self.k > 2.0:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_179(BaseStrategy):
    """
    Adaptive Free-rider with Critical Contribution strategy.
    This strategy aims to maximize individual payoff by free-riding when possible
    and only cooperating when it critically ensures project success and a
    higher individual reward (k > 1). It adapts based on the total number of
    cooperators observed in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        prev_round_history_index = state.round_number - 1
        cooperators_in_prev_round = int(history.my_actions[prev_round_history_index]) + np.sum(history.opponent_actions[prev_round_history_index, :])
        if cooperators_in_prev_round >= self.m:
            return Action.D
        elif cooperators_in_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_180(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize individual payoff by exploiting opportunities
    to free-ride when the collective cooperation threshold is met. It strategically contributes only
    when its cooperation can critically achieve the collective reward `k` (given k > 1), or to probe
    the environment for cooperative potential in the first round. The strategy adapts its behavior
    based on the number of cooperators observed in the preceding round and exploits the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        N_C_prev_opponents = np.sum(history.opponent_actions[state.round_number - 1])
        N_C_prev_my_action = history.my_actions[state.round_number - 1]
        N_C_prev = N_C_prev_opponents + N_C_prev_my_action
        if N_C_prev >= self.m:
            return Action.D
        elif N_C_prev + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_181(BaseStrategy):
    """
    Adaptive Free-Rider with Threshold Enforcement (AFR-TE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize total payoff by:
    1.  **Initiating Cooperation (Cautiously):** Cooperating in the very first round to probe for collective cooperation.
    2.  **Free-Riding:** Defecting in subsequent rounds if the cooperation threshold was met previously,
        benefiting from others' contributions while keeping the private endowment.
    3.  **Punishing/Minimizing Loss:** Defecting in subsequent rounds if the cooperation threshold was NOT met
        previously, to avoid being a "sucker" and to signal that collective failure will not be rewarded.
    4.  **Terminal Defection:** Defecting in the final round due to backward induction.

    Crucially, for intermediate rounds (after the first and before the last), this strategy always defects,
    regardless of whether the threshold was met in the previous round, to consistently free-ride on success
    or to consistently punish/minimize loss on failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as n_players, n_rounds, m, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round based on the AFR-TE logic.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the current player and opponents,
                     or None if it's the first round.

        Returns:
            Action.C for Cooperate, Action.D for Defect.
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_182(BaseStrategy):
    """
    Selfish Opportunist (SO) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding whenever possible,
    only cooperating if its contribution is absolutely critical to meet the threshold
    'm' AND the reward 'k' makes that individual contribution strictly profitable (k > 2).
    It always defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1])
        my_action_prev_round = history.my_actions[state.round_number - 1]
        my_contribution_prev_round = 1 if my_action_prev_round else 0
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_contribution_prev_round
        if total_cooperators_prev_round >= self.m:
            return Action.D
        elif total_cooperators_prev_round == self.m - 1:
            if self.k > 2:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_183(BaseStrategy):
    """
    The Adaptive Critical Contributor (ACC) strategy aims to always secure the collective reward 'k' for itself,
    but at the lowest possible personal cost. It will free-ride whenever the threshold for 'k' is met or appears
    likely to be met without its direct contribution. It will only contribute (cooperate) if its cooperation
    is decisive in achieving the threshold 'm', otherwise, it defaults to defection to avoid futile costs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        cooperators_prev_round_opponents = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round = history.my_actions[-1]
        C_prev = cooperators_prev_round_opponents + (1 if my_action_prev_round else 0)
        if C_prev >= m:
            return Action.D
        elif C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_184(BaseStrategy):
    """
    The Opportunistic Free-Rider aims to maximize its total payoff over `r` rounds by
    predominantly defecting and free-riding on the cooperation of others. It only
    contributes (cooperates) under specific, calculated circumstances:
    1.  **To probe the environment:** In the first round, it defects to observe initial opponent behavior without personal cost.
    2.  **To exploit established cooperation:** If the collective threshold `m` is met, it defects to receive the maximum possible payoff (`1+k`).
    3.  **To strategically invest:** If cooperation is just one player short of the threshold, it cooperates to secure `k` (which is greater than `1` from assured failure).
    4.  **To "reboot" a failing system:** If the group consistently fails to meet `m` for a prolonged period, it will periodically cooperate to attempt to break the cycle of non-cooperation and restore the possibility of free-riding on `k` in future rounds.
    5.  **To avoid future consequences:** In the final round, it defects as there are no subsequent rounds to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_failed_bonus_since_last_success = 0
        r = self.game_description.n_rounds
        self.R_coop_reboot = max(3, min(r // 3, 5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number > 0:
            cooperators_in_last_completed_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            bonus_met_in_last_completed_round = cooperators_in_last_completed_round >= m
            if bonus_met_in_last_completed_round:
                self.rounds_failed_bonus_since_last_success = 0
            else:
                self.rounds_failed_bonus_since_last_success += 1
        else:
            self.rounds_failed_bonus_since_last_success = 0
        if state.round_number == r - 1:
            return Action.D
        if state.round_number == 0:
            return Action.D
        cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        bonus_met_prev = cooperators_prev_round >= m
        if bonus_met_prev:
            return Action.D
        else:
            if cooperators_prev_round == m - 1:
                return Action.C
            if self.rounds_failed_bonus_since_last_success >= self.R_coop_reboot:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_185(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the Collective Risk Dilemma.
    This strategy aims to free-ride when possible but will strategically cooperate
    only when its cooperation is critical to achieving the collective bonus 'k',
    which benefits the player more than simply defecting and missing the bonus.
    It also signals a demand for cooperation from others if the threshold is not met.

    Decision Rules:
    1.  Initialization (Round t = 1, i.e., state.round_number == 0):
        Action: Cooperate (C)
        Rationale: Test the waters and signal willingness for k.

    2.  General Rounds (t = 2 to r-1, i.e., state.round_number from 1 to n_rounds - 2):
        Let C_prev be the observed number of cooperators in the previous round.
        Let my_action_prev be this strategy's action in the previous round.

        If C_prev >= m (Threshold was met last round):
            If C_prev == m (Exactly m cooperators met the threshold):
                If my_action_prev == C: Cooperate (C) - I was critical, need to secure k.
                If my_action_prev == D: Defect (D) - Successfully free-rode, continue for 1+k.
            If C_prev > m (Surplus of cooperators):
                Defect (D) - Exploit the surplus, my defection won't jeopardize k.
        Else (C_prev < m, Threshold was NOT met last round):
            Defect (D) - Signal unwillingness to be a sucker, demand more cooperation.

    3.  Final Round (t = r, i.e., state.round_number == n_rounds - 1):
        Action: Defect (D)
        Rationale: No future interactions to influence, maximize current round's private gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        num_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        my_action_prev_was_C = history.my_actions[-1]
        if num_cooperators_prev >= m_threshold:
            if num_cooperators_prev == m_threshold:
                if my_action_prev_was_C:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_186(BaseStrategy):
    """
    Adaptive Opportunist (AO) strategy for the Collective Risk Dilemma.

    This strategy prioritizes free-riding. It defects by default and only
    cooperates strategically and minimally to prevent collective failure
    (loss of the reward 'k') when the group is just one cooperator short
    of the threshold 'm'. It exploits the last round by always defecting,
    as there are no future interactions to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Opportunist strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (threshold), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for this player
                     and opponents. It is None for the first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        n_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        k_achieved_prev = n_cooperators_prev >= self.game_description.m
        if k_achieved_prev:
            return Action.D
        elif n_cooperators_prev == self.game_description.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_187(BaseStrategy):
    """
    Adaptive Opportunist strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by:
    1. Cooperating in the first round to "prime the pump" for collective reward.
    2. Defecting in the last round to free-ride without future consequences.
    3. In intermediate rounds:
       a. Free-riding (Defect) if the cooperation threshold was met in the previous round.
       b. Making a marginal contribution (Cooperate) if own cooperation is exactly what's needed to meet the threshold.
       c. Persistently probing (Cooperate) in early rounds if cooperation has never been established and own contribution is not marginal,
          to try and enable the 'k' reward for future exploitation.
       d. Cutting losses (Defect) if cooperation has previously failed or it's past the early probing phase and own contribution is insufficient.

    Internal state:
    - cooperation_ever_met: Tracks if the 'm' threshold has ever been met.
    - early_game_persistence_rounds: Determines the length of the initial probing phase.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_ever_met: bool = False
        self.early_game_persistence_rounds: int = min(self.game_description.n_rounds - 1, 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        cooperators_opponents_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_was_C = history.my_actions[prev_round_idx]
        C_prev = cooperators_opponents_prev + (1 if my_action_prev_was_C else 0)
        if C_prev >= self.game_description.m:
            self.cooperation_ever_met = True
            return Action.D
        elif C_prev == self.game_description.m - 1:
            return Action.C
        elif not self.cooperation_ever_met and state.round_number < self.early_game_persistence_rounds:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_188(BaseStrategy):
    """
    The Adaptive Pivotal Defector (APD) strategy.
    Prioritizes free-riding, cooperating only when pivotal and when historical
    evidence suggests others are reliable. Employs a punishment mechanism for
    consistent collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self._punishment_mode_active = False
        self._punishment_rounds_left = 0
        self._history_num_cooperators_overall = []
        self._history_my_actions = []
        _n_minus_1 = self.n - 1
        _m_minus_1 = self.m - 1
        self._WINDOW_SIZE_FOR_HISTORY = min(_n_minus_1, 5)
        self._SUCCESS_RATE_PIVOT_THRESHOLD = _m_minus_1 / float(_n_minus_1)
        self._FAILURE_RATE_PUNISH_THRESHOLD = 0.75
        self._MIN_FAILURES_FOR_PUNISHMENT = max(2, math.floor(self.m / 2.0))
        self._PUNISHMENT_DURATION_ROUNDS = max(2, self.n - self.m + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        num_cooperators_in_last_round = 0
        if state.round_number > 0 and history is not None:
            num_cooperators_in_last_round = sum(history.opponent_actions[-1])
            if history.my_actions[-1]:
                num_cooperators_in_last_round += 1
        if current_round_t > 1:
            self._history_num_cooperators_overall.append(num_cooperators_in_last_round)
            self._history_my_actions.append(history.my_actions[-1])
        if current_round_t == self.r:
            return Action.D
        if self._punishment_mode_active:
            if self._punishment_rounds_left > 0:
                self._punishment_rounds_left -= 1
                if self._punishment_rounds_left == 0:
                    self._punishment_mode_active = False
                return Action.D
            else:
                self._punishment_mode_active = False
        others_cooperated_t_minus_1 = 0
        if current_round_t > 1:
            my_action_last_round_bool = self._history_my_actions[current_round_t - 2]
            if my_action_last_round_bool:
                others_cooperated_t_minus_1 = num_cooperators_in_last_round - 1
            else:
                others_cooperated_t_minus_1 = num_cooperators_in_last_round
        if current_round_t <= min(self.r - 1, 3):
            if current_round_t == 1:
                return Action.D
            elif others_cooperated_t_minus_1 >= self.m:
                return Action.D
            elif others_cooperated_t_minus_1 == self.m - 1:
                return Action.C
            else:
                return Action.D
        recent_history_window = []
        start_index = max(0, current_round_t - 1 - self._WINDOW_SIZE_FOR_HISTORY)
        if len(self._history_num_cooperators_overall) > 0:
            recent_history_window = self._history_num_cooperators_overall[start_index:current_round_t - 1]
        recent_success_count = 0
        recent_failure_count = 0
        current_window_size_actual = len(recent_history_window)
        if current_window_size_actual > 0:
            for nc_total in recent_history_window:
                if nc_total >= self.m:
                    recent_success_count += 1
                else:
                    recent_failure_count += 1
            cooperation_success_rate = recent_success_count / float(current_window_size_actual)
            cooperation_failure_rate = recent_failure_count / float(current_window_size_actual)
        else:
            cooperation_success_rate = 0.0
            cooperation_failure_rate = 1.0
        if others_cooperated_t_minus_1 >= self.m:
            return Action.D
        elif others_cooperated_t_minus_1 == self.m - 1:
            if cooperation_success_rate >= self._SUCCESS_RATE_PIVOT_THRESHOLD:
                return Action.C
            else:
                return Action.D
        elif cooperation_failure_rate >= self._FAILURE_RATE_PUNISH_THRESHOLD and recent_failure_count >= self._MIN_FAILURES_FOR_PUNISHMENT:
            self._punishment_mode_active = True
            self._punishment_rounds_left = self._PUNISHMENT_DURATION_ROUNDS - 1
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_189(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy prioritizes defecting (free-riding) to maximize individual payoff.
    It only cooperates when its individual contribution is both critical to meet
    the collective threshold and demonstrably more profitable than defecting,
    thereby preventing a universally low payoff outcome when k > 1.
    The strategy always defects in the first and last rounds to exploit these edge cases.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Free-Rider strategy.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters such as
                                                          number of players, rounds,
                                                          minimum cooperators needed (m),
                                                          and reward factor (k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing records of past actions
                                            and payoffs for the current player and opponents.
                                            It is None for the very first round.

        Returns:
            Action: The chosen action for the current round, either Action.C (Cooperate)
                    or Action.D (Defect).
        """
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_number == 0:
            return Action.D
        if current_round_number == total_rounds - 1:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        cooperators_in_prev_round = int(my_prev_action_was_C) + opponent_prev_cooperators
        if cooperators_in_prev_round >= min_cooperators_m:
            return Action.D
        elif cooperators_in_prev_round == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_190(BaseStrategy):
    """
    Opportunistic K-Seeker strategy for the Collective Risk Dilemma.
    This strategy aims to free-ride on others' cooperation to get a payoff of 1+k.
    It strategically cooperates only when its contribution is critically decisive
    to restore the collective reward 'k' after a failure to meet the threshold.
    It defects in the first and last rounds to maximize immediate private gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self._rounds_since_k_success = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.r - 1:
            return Action.D
        prev_round_idx = current_round_number - 1
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev = history.my_actions[prev_round_idx]
        N_C_prev = num_opponent_cooperators_prev + my_action_prev
        if N_C_prev >= self.m:
            self._rounds_since_k_success = 0
        else:
            self._rounds_since_k_success += 1
        if self._rounds_since_k_success == 0:
            return Action.D
        else:
            effective_cooperators_needed = self.m - N_C_prev
            if effective_cooperators_needed == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_191(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for the Collective Risk Dilemma.

    This strategy aims to free-ride as much as possible, only contributing its
    endowment when its cooperation is the deciding factor in meeting the
    collective threshold 'm', thus securing the 'k' reward. It consistently
    defects in the first and last rounds, or when its contribution alone
    cannot meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == n_rounds - 1:
            return Action.D
        total_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_as_int = 1 if history.my_actions[-1] else 0
        C_prev = total_opponent_cooperators_prev_round + my_action_prev_round_as_int
        if C_prev >= m:
            return Action.D
        elif C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_192(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly exploitative, prioritizing individual gain
    while adapting to the collective behavior of other players. It aims to free-ride
    whenever the collective benefit is assured and only cooperates strategically to
    either initiate cooperation or prevent its complete collapse, always with an eye
    on its own long-term payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.my_last_payoff: float | None = None
        self.failed_cooperation_streak: int = 0
        self.max_sucker_streak: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_coop_needed = self.game_description.m
        if state.round_number == 0:
            return Action.C
        prev_my_action_bool = history.my_actions[-1]
        prev_my_payoff = history.my_payoffs[-1]
        self.my_last_action = Action.C if prev_my_action_bool else Action.D
        self.my_last_payoff = prev_my_payoff
        if self.my_last_action == Action.C and math.isclose(self.my_last_payoff, 0.0):
            self.failed_cooperation_streak += 1
        else:
            self.failed_cooperation_streak = 0
        if current_round_num_1_indexed == total_rounds:
            return Action.D
        C_prev = np.sum(history.opponent_actions[-1, :]) + (1 if prev_my_action_bool else 0)
        if C_prev >= min_coop_needed:
            if C_prev > min_coop_needed:
                return Action.D
            elif self.my_last_action == Action.C:
                return Action.C
            else:
                return Action.D
        elif self.failed_cooperation_streak >= self.max_sucker_streak:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_193(BaseStrategy):
    """
    The "Calculated Free Rider" strategy is designed for the Collective Risk Dilemma,
    focusing on maximizing individual payoff by exploiting the repeated nature of the game.
    It strategically defects in early/mid-game to pressure others into cooperating,
    while ensuring the collective reward (k) is secured in the late-game or final round
    when its contribution is the tipping point. The strategy never cooperates if its
    contribution is clearly insufficient or unnecessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_action = Action.D
        if state.round_number == 0:
            current_action = Action.D
        else:
            other_coop_prev = sum(history.opponent_actions[-1, :])
            if state.round_number == n_rounds - 1:
                if other_coop_prev >= m:
                    current_action = Action.D
                elif other_coop_prev == m - 1:
                    current_action = Action.C
                else:
                    current_action = Action.D
            elif other_coop_prev >= m:
                current_action = Action.D
            elif other_coop_prev == m - 1:
                if state.round_number < n_rounds / 2:
                    current_action = Action.D
                else:
                    current_action = Action.C
            else:
                current_action = Action.D
        self._my_last_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_194(BaseStrategy):
    """
    Strategic Public Good Enabler strategy for the Collective Risk Dilemma.
    This strategy is exploitative, adaptive, and robust, aiming to maximize
    its own payoff by free-riding when possible, but strategically cooperating
    to ensure the public good is achieved when it's critical or when
    a system failure threatens future rewards.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_public_good_failures = 0
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            self.my_last_action = Action.C
            return Action.C
        if state.round_number == r - 1:
            self.my_last_action = Action.D
            return Action.D
        N_C_prev_opponents = np.sum(history.opponent_actions[-1, :])
        N_C_prev_me = 1 if history.my_actions[-1] else 0
        N_C_prev = N_C_prev_opponents + N_C_prev_me
        if N_C_prev >= m:
            self.consecutive_public_good_failures = 0
        else:
            self.consecutive_public_good_failures += 1
        current_action = Action.D
        if N_C_prev >= m:
            current_action = Action.D
        elif N_C_prev == m - 1:
            current_action = Action.C
        elif self.consecutive_public_good_failures >= 2:
            if N_C_prev_opponents > 0:
                current_action = Action.C
            else:
                current_action = Action.D
        self.my_last_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_195(BaseStrategy):
    """
    Adaptive Threshold Investor (ATI) strategy for the Collective Risk Dilemma.
    This strategy balances opportunistic defection with a calculated investment in cooperation,
    driven by observed collective behavior and specific game parameters. It aims to maximize
    individual payoff by free-riding when the collective project is successfully funded,
    but strategically contributes when its cooperation is critical to push the group
    over the 'm' cooperation threshold and secure the larger collective reward 'k'.
    In the final round, it always defects to secure immediate gains, leveraging backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == n_rounds:
            return Action.D
        opponents_cooperated_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_round_was_cooperation = history.my_actions[state.round_number - 1]
        C_prev = opponents_cooperated_prev_round + (1 if my_action_prev_round_was_cooperation else 0)
        if C_prev >= m_threshold:
            return Action.D
        elif C_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_196(BaseStrategy):
    """
    The "Adaptive Exploit & Rescue" strategy is designed to maximize individual payoff
    in the Collective Risk Dilemma. It prioritizes free-riding by defecting (D) but
    adaptively contributes (C) only when its cooperation is critical to securing the
    collective reward 'k' (i.e., meeting the threshold 'm'). The strategy avoids futile
    cooperation and defects in the final round based on backward induction.
    It estimates others' tendencies to cooperate from the immediately preceding round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        estimated_cooperators_from_others_this_round = np.sum(history.opponent_actions[-1, :])
        if estimated_cooperators_from_others_this_round >= self.m_threshold:
            return Action.D
        else:
            cooperators_if_i_cooperate = estimated_cooperators_from_others_this_round + 1
            needed_to_reach_m_if_i_cooperate = self.m_threshold - cooperators_if_i_cooperate
            if needed_to_reach_m_if_i_cooperate <= 0:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_197(BaseStrategy):
    """
    The "Calculated Opportunist" strategy is designed to be highly exploitative, adaptive, and robust
    in the Collective Risk Dilemma. It prioritizes maximizing its own total payoff by free-riding
    whenever possible, only contributing when strategically necessary to secure the collective reward 'k',
    and withdrawing cooperation if it proves ineffective.

    Summary of Decision Rules:
    - First Round (t=1): Always Defect (D). This tests others' willingness to cooperate and attempts
      to free-ride from the outset.
    - Last Round (t=r): Always Defect (D). Due to the absence of future interactions, the purely
      self-interested choice is to maximize current payoff without repercussions.
    - Intermediate Rounds (1 < t < r):
        - If the cooperation threshold 'm' was met in the previous round (t-1):
          Defect (D). Free-ride on the collective effort, earning (1+k) which is better than (k).
        - If the cooperation threshold 'm' was NOT met in the previous round (t-1):
            - If only one more cooperator (i.e., this player) is needed to meet 'm':
              Cooperate (C). This player's action directly secures the 'k' reward for everyone,
              and since k > 1, contributing (0+k) is better than defecting (1) if 'm' is met.
            - Else if this player cooperated in the previous round (t-1) but 'm' was still missed:
              Defect (D). This signals a withdrawal of effort, as the previous contribution was in vain.
            - Else (this player defected in the previous round (t-1) and 'm' was still missed):
                - If 'm' is close (needed_to_meet_m <= 2):
                  Cooperate (C). This is a "calculated probe"  a temporary cooperation to nudge
                  the system towards meeting 'm', as the collective is only slightly below the threshold.
                - Else (more than 2 cooperators are needed):
                  Defect (D). The threshold is too far off, and a single cooperation is unlikely to make
                  a significant difference, maintaining the exploitative stance.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        action: Action
        if current_round_1_indexed == 1:
            action = Action.D
        elif current_round_1_indexed == n_rounds:
            action = Action.D
        else:
            previous_round_0_indexed = current_round_0_indexed - 1
            N_C_prev = int(history.my_actions[previous_round_0_indexed]) + np.sum(history.opponent_actions[previous_round_0_indexed, :])
            if N_C_prev >= m_threshold:
                action = Action.D
            else:
                needed_to_meet_m = m_threshold - N_C_prev
                if needed_to_meet_m == 1:
                    action = Action.C
                elif self.my_last_action == Action.C:
                    action = Action.D
                elif needed_to_meet_m <= 2:
                    action = Action.C
                else:
                    action = Action.D
        self.my_last_action = action
        return action

class Strategy_EXPLOITATIVE_198(BaseStrategy):
    """
    Recalibrating Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy is highly exploitative, defecting whenever possible and
    only cooperating as a calculated, temporary measure to "reset" the
    collective's ability to meet the threshold, so that it can resume free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.Patience_Threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.game_description.n_rounds
        m = self.game_description.m
        if t == r:
            return Action.D
        if t == 1:
            return Action.D
        current_my_last_action = Action.D
        current_rounds_missed_threshold_while_defecting = 0
        for round_idx in range(state.round_number):
            prev_my_action_bool = history.my_actions[round_idx]
            prev_my_action = Action.C if prev_my_action_bool else Action.D
            prev_n_cooperators = np.sum(history.opponent_actions[round_idx, :]) + (1 if prev_my_action_bool else 0)
            if prev_n_cooperators >= m:
                current_rounds_missed_threshold_while_defecting = 0
            elif prev_my_action == Action.C:
                current_rounds_missed_threshold_while_defecting = 0
            else:
                current_rounds_missed_threshold_while_defecting += 1
            current_my_last_action = prev_my_action
        last_round_my_action_bool = history.my_actions[state.round_number - 1]
        last_round_n_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :]) + (1 if last_round_my_action_bool else 0)
        if last_round_n_cooperators >= m:
            return Action.D
        elif current_my_last_action == Action.C:
            return Action.D
        elif current_rounds_missed_threshold_while_defecting >= self.Patience_Threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_199(BaseStrategy):
    """
    The Opportunistic Adaptive Cooperator strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its total payoff over the rounds by adopting a flexible, self-interested approach.
    It attempts to free-ride whenever possible, defecting to gain the 1+k bonus, and only cooperates when a lack of
    collective contribution threatens the overall success and thus its own access to the k reward.
    In the final round, it acts purely selfishly, as there are no future consequences for its actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: A CollectiveRiskDescription object containing n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for the player and opponents,
                     or None if it's the first round.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        current_strategy_round = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_strategy_round == 1:
            return Action.D
        if current_strategy_round == n_rounds:
            return Action.D
        prev_round_0_idx = state.round_number - 1
        opp_cooperators_prev = np.sum(history.opponent_actions[prev_round_0_idx, :])
        my_action_prev_contribution = 1 if history.my_actions[prev_round_0_idx] else 0
        N_C_prev = opp_cooperators_prev + my_action_prev_contribution
        if N_C_prev >= m_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_200(BaseStrategy):
    """
    Adaptive Free-Rider with Threshold Maintenance Strategy.

    This strategy aims to free-ride (defect) whenever possible, securing the highest possible payoff.
    It strategically cooperates when its contribution is perceived as crucial for achieving the
    collective reward (k), thereby safeguarding the collective outcome from which it ultimately benefits.
    It avoids contributing when its effort would likely be wasted.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        num_my_previous_cooperation = 1 if history.my_actions[-1] else 0
        num_opponent_previous_cooperation = sum(history.opponent_actions[-1, :])
        previous_total_cooperators = num_my_previous_cooperation + num_opponent_previous_cooperation
        if previous_total_cooperators >= self.m:
            if previous_total_cooperators > self.m:
                return Action.D
            else:
                return Action.C
        elif previous_total_cooperators + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_201(BaseStrategy):
    """
    Coercive Reciprocity with Opportunistic Free-Riding:
    This strategy aims to maximize its own total payoff by securing the collective reward (`k`)
    while minimizing its personal contribution. It employs a "stick-and-carrot" approach
    based on the immediate past round's outcome and its own previous action:

    -   **Carrot:** If the group failed to cooperate in the previous round *and* this player
                 was free-riding (defected), it will cooperate in the current round to
                 try and re-establish collective cooperation.
    -   **Stick:** If the group failed to cooperate in the previous round *and* this player
                contributed (cooperated), it will defect in the current round as a signal
                of disapproval and to cut losses.
    -   **Opportunism:** If the group successfully cooperated (met the threshold `m`) in the
                     previous round, this player will defect in the current round to free-ride
                     on the established cooperation.

    The strategy always cooperates in the first round to kickstart cooperation and defects
    in the last round to maximize immediate gain without future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == n_rounds:
            return Action.D
        if state.round_number == 0:
            return Action.C
        else:
            my_action_prev_bool = history.my_actions[-1]
            n_c_prev_opponents = np.sum(history.opponent_actions[-1, :])
            n_c_prev_total = n_c_prev_opponents + (1 if my_action_prev_bool else 0)
            threshold_met_prev = n_c_prev_total >= m
            if threshold_met_prev:
                return Action.D
            elif my_action_prev_bool == True:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_202(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding on the contributions
    of others. It defaults to defection, only cooperating as a calculated, pivotal move
    to re-establish the collective reward 'k' when the system is failing, and immediately
    reverts to defection if that cooperation proves ineffective.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.
        No internal state is explicitly maintained across rounds for ATE,
        as all necessary history is derived from the 'history' object passed to __call__.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Threshold Exploiter strategy.
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds_1_indexed:
            return Action.D
        prev_round_idx = state.round_number - 1
        num_opp_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_bool = history.my_actions[prev_round_idx]
        N_c_prev = num_opp_cooperators_prev_round + (1 if my_action_prev_bool else 0)
        my_action_prev = Action.C if my_action_prev_bool else Action.D
        if N_c_prev >= m:
            return Action.D
        elif my_action_prev == Action.C:
            return Action.D
        elif N_c_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_203(BaseStrategy):
    """
    The "Adaptive Threshold Nudger" strategy for the Collective Risk Dilemma.

    This strategy aims to secure the collective reward 'k' with minimal personal cost.
    It leverages observations of prior rounds to strategically cooperate only when its
    contribution is critical to achieving or maintaining the 'm' threshold, and
    otherwise defects to freeride. It acts as a "nudge" to ensure the threshold is met,
    but only when necessary and beneficial.

    Core Philosophy:
    1.  Strategic Initial Investment: Cooperate in the first round to "seed" cooperation.
    2.  Opportunistic Freeriding: Defect when `C_observed_t-1 > m` (more than enough cooperators).
    3.  Minimal Necessary Contribution: Cooperate only when `C_observed_t-1 == m` (to maintain)
        or `C_observed_t-1 == m-1` (to achieve the threshold).
    4.  Resource Conservation in Failure: Defect when `C_observed_t-1 < m-1` (too few cooperators
        to make a difference).
    5.  Rational End-Game: Defect in the last round to maximize immediate gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_prev_action_cooperated = int(history.my_actions[-1])
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_prev_action_cooperated
        if total_cooperators_prev_round >= m:
            if total_cooperators_prev_round > m:
                return Action.D
            else:
                return Action.C
        elif total_cooperators_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_204(BaseStrategy):
    """
    The Aggressive Free-Rider (AFR) strategy aims to maximize personal payoff
    in the Collective Risk Dilemma by prioritizing free-riding. It only
    cooperates when its contribution is critically needed to meet the
    collective cooperation threshold and secure the reward, otherwise
    it defects to avoid wasted effort or exploit successful collective action.
    It adapts to observed opponent behavior from the previous round and
    exploits the finite game horizon by defecting in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        if state.round_number == 0:
            current_action = Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            current_action = Action.D
        else:
            previous_round_idx = state.round_number - 1
            others_cooperated_prev_round_count = np.sum(history.opponent_actions[previous_round_idx, :])
            my_contribution_prev_round = 1 if self.my_last_action == Action.C else 0
            total_cooperators_prev_round = others_cooperated_prev_round_count + my_contribution_prev_round
            m = self.game_description.m
            threshold_met_prev_round = total_cooperators_prev_round >= m
            if threshold_met_prev_round:
                current_action = Action.D
            elif others_cooperated_prev_round_count + 1 >= m:
                current_action = Action.C
            else:
                current_action = Action.D
        self.my_last_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_205(BaseStrategy):
    """
    The Calculated Opportunist strategy adapts to observed cooperation levels.
    It prioritizes maximizing its own payoff by free-riding when the collective
    good is achieved by others. When the collective good consistently fails,
    it takes a calculated, temporary risk to cooperate, hoping to "kickstart"
    collective success that it can then free-ride on.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.has_tried_cooperating_in_failed_state: bool = False
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.has_tried_cooperating_in_failed_state = False
            self.consecutive_failures = 0
            return Action.D
        if current_round_0_indexed == self.game_description.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        my_action_last_round_was_C = history.my_actions[-1]
        total_cooperators_last_round = num_opponent_cooperators_last_round + (1 if my_action_last_round_was_C else 0)
        if total_cooperators_last_round >= self.game_description.m:
            self.has_tried_cooperating_in_failed_state = False
            self.consecutive_failures = 0
            return Action.D
        else:
            self.consecutive_failures += 1
            should_nudge = self.consecutive_failures >= 2 and (not self.has_tried_cooperating_in_failed_state) and (num_opponent_cooperators_last_round + 1 >= self.game_description.m)
            if should_nudge:
                self.has_tried_cooperating_in_failed_state = True
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_206(BaseStrategy):
    """
    The "Adaptive Opportunistic Free-Rider" (AOF) strategy. This strategy defaults to free-riding (Defecting)
    but will temporarily cooperate if the collective reward ('k' bonus) is consistently missed. It cooperates
    for a minimal duration, then reverts to defecting, and always defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FAILURE_THRESHOLD_FOR_COOPERATION: int = 2
        self.COOPERATION_BURST_LENGTH: int = 0
        self.my_last_action: Action = Action.D
        self.my_current_coop_streak_rounds: int = 0
        self.system_fail_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        chosen_action: Action
        if current_round_num == 0:
            chosen_action = Action.D
        elif current_round_num == total_rounds - 1:
            chosen_action = Action.D
        else:
            num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
            num_cooperators_last_round = num_opponent_cooperators_last_round + (1 if self.my_last_action == Action.C else 0)
            if num_cooperators_last_round < m:
                self.system_fail_streak += 1
            else:
                self.system_fail_streak = 0
            if self.my_current_coop_streak_rounds > 0:
                self.my_current_coop_streak_rounds -= 1
                chosen_action = Action.C
            elif self.system_fail_streak >= self.FAILURE_THRESHOLD_FOR_COOPERATION:
                chosen_action = Action.C
                self.my_current_coop_streak_rounds = self.COOPERATION_BURST_LENGTH
                self.system_fail_streak = 0
            else:
                chosen_action = Action.D
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_207(BaseStrategy):
    """
    Adaptive Free-Rider with Threshold Maintenance: Prioritizes defecting to maximize immediate payoff,
    but strategically cooperates when necessary to secure or maintain the 'k' bonus, especially when
    its own contribution can critically influence the outcome. Defaults to defecting in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        my_action_in_prev_round_is_C = history.my_actions[previous_round_index]
        num_cooperators_in_prev_round = np.sum(history.opponent_actions[previous_round_index, :]) + (1 if my_action_in_prev_round_is_C else 0)
        if num_cooperators_in_prev_round >= self.m:
            if my_action_in_prev_round_is_C:
                if num_cooperators_in_prev_round > self.m:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D
        elif num_cooperators_in_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_208(BaseStrategy):
    """
    Adaptive Free-Rider with Conditional Investment: This strategy aims to maximize individual payoff by
    consistently free-riding when the collective cooperation threshold 'm' is met. It strategically
    invests (cooperates) only when necessary to re-establish the collective good for future exploitation,
    but with a limited 'patience' for unrequited cooperation.
    """
    PATIENCE_THRESHOLD = 1

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_m_met = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.rounds_since_m_met = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_cooperators = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_round_cooperators >= self.game_description.m:
            self.rounds_since_m_met = 0
        else:
            self.rounds_since_m_met += 1
        if prev_round_cooperators >= self.game_description.m:
            return Action.D
        elif self.rounds_since_m_met <= self.PATIENCE_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_209(BaseStrategy):
    """
    The Adaptive Opportunist strategy prioritizes individual gain. It is designed to
    benefit from others' cooperation, only stepping in to contribute when its
    participation is critical to achieve the collective reward and when that
    reward (k) is sufficiently high (k > 2) to make individual contribution worthwhile.
    Otherwise, it defaults to defection to avoid costs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        reward_factor = self.game_description.k
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        n_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if n_cooperators_prev >= min_cooperators:
            return Action.D
        elif reward_factor <= 2:
            return Action.D
        else:
            needed_for_threshold = min_cooperators - n_cooperators_prev
            if needed_for_threshold == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_210(BaseStrategy):
    """
    Conditional Exploiter strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by primarily attempting to free-ride
    on the cooperation of others. However, it incorporates adaptive mechanisms to ensure
    the collective reward ('k') is accessible, and to prevent itself from being persistently
    exploited if others fail to cooperate. It leverages the repeated nature of the game
    by learning from past rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.consecutive_failed_threshold_rounds = 0
        self.patience_threshold = self.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_1_indexed = state.round_number + 1
        if current_game_round_1_indexed == 1:
            return Action.D
        my_action_last_round_was_C = history.my_actions[-1]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_last_round = (1 if my_action_last_round_was_C else 0) + opponent_cooperators_last_round
        if total_cooperators_last_round >= self.m:
            self.consecutive_failed_threshold_rounds = 0
        else:
            self.consecutive_failed_threshold_rounds += 1
        if current_game_round_1_indexed == self.r:
            return Action.D
        if total_cooperators_last_round >= self.m:
            return Action.D
        elif self.consecutive_failed_threshold_rounds <= self.patience_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_211(BaseStrategy):
    """
    The "Adaptive Threshold Exploiter" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own payoff by constantly assessing the collective's willingness to cooperate.
    It defects in the first and last rounds. In intermediate rounds, it calculates the average cooperation
    tendency of other players from past rounds. It defects if collective cooperation is high enough to meet
    the threshold without its contribution (exploitative). It cooperates if its contribution is likely
    crucial to meet the threshold (strategic contribution). It defects if collective cooperation is
    consistently too low to meet the threshold even with its contribution (withdraw/punish).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        num_previous_rounds = state.round_number
        sum_other_cooperators = np.sum(history.opponent_actions)
        if num_previous_rounds == 0:
            return Action.D
        avg_other_cooperation = sum_other_cooperators / num_previous_rounds
        decision: Action
        if avg_other_cooperation >= m:
            decision = Action.D
        elif m - 1 <= avg_other_cooperation < m:
            decision = Action.C
        else:
            decision = Action.D
        return decision

class Strategy_EXPLOITATIVE_212(BaseStrategy):
    """
    Adaptive Opportunist strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by strategically oscillating
    between cooperation (to ensure the collective benefit 'k' is achieved) and
    defection (to free-ride on others' contributions). It is adaptive to observed
    behavior and robust against various opponent strategies, focusing on exploiting
    the "reward if threshold met" aspect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self._my_last_action_chosen_in_current_round: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        action_to_take: Action
        if current_round_0_indexed == 0:
            action_to_take = Action.C
        elif current_round_0_indexed == self.r - 1:
            action_to_take = Action.D
        else:
            cooperators_from_opponents_in_prev_round = np.sum(history.opponent_actions[-1, :])
            my_actual_action_in_prev_round_bool = history.my_actions[-1]
            C_prev_round = cooperators_from_opponents_in_prev_round + (1 if my_actual_action_in_prev_round_bool else 0)
            if C_prev_round < self.m:
                action_to_take = Action.C
            else:
                cooperators_excluding_me_in_prev_round = C_prev_round
                if my_actual_action_in_prev_round_bool == True:
                    cooperators_excluding_me_in_prev_round = C_prev_round - 1
                if cooperators_excluding_me_in_prev_round >= self.m:
                    action_to_take = Action.D
                else:
                    action_to_take = Action.C
        self._my_last_action_chosen_in_current_round = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_213(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its total payoff by free-riding whenever
    the collective good (the 'k' bonus) can be achieved without its contribution,
    or when the collective good is not being met by others. It strategically
    cooperates only when its contribution is essential to secure the 'k' bonus
    for the group, thereby ensuring its own share of the bonus.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t_1_indexed = state.round_number + 1
        if current_round_t_1_indexed == 1:
            return Action.D
        if current_round_t_1_indexed == self.n_rounds:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_last_round >= self.m:
            if num_cooperators_last_round > self.m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_214(BaseStrategy):
    """
    Adaptive Free Rider with Safety Net (AFRWSN) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by primarily free-riding (defecting)
    whenever possible. It only cooperates when its contribution is strictly necessary
    and beneficial to shift the collective outcome to meet the 'm' threshold, thereby
    securing a better individual payoff than if it were to defect.

    Decision Rules:
    1.  **First Round (round 0):** Defaults to Defect to probe the environment and potentially free-ride.
    2.  **Last Round (round r-1):** Defaults to Defect as there are no future rounds for reputation or influence.
    3.  **Intermediate Rounds:**
        a.  **If others already provide enough cooperation (or more):** Defect. The threshold 'm' will be met regardless of this player's action, so free-ride to get 1+k instead of k.
        b.  **If others are exactly one cooperator short of 'm':** Cooperate. This player's cooperation pushes the total to 'm', yielding 'k' (better than '1' if defecting). This is the "safety net".
        c.  **If even with this player's cooperation, 'm' won't be met:** Defect. Cooperating would be futile (0 payoff) while defecting yields 1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        N_C_others_last_round = np.sum(history.opponent_actions[-1, :])
        if N_C_others_last_round >= self.m:
            return Action.D
        elif N_C_others_last_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_215(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy defaults to defection, aiming to free-ride.
    It only cooperates as a calculated, temporary measure to restore the collective reward (k)
    if it detects that the group is on the verge of failure (exactly one cooperator short of 'm' in the previous round)
    and its contribution can be pivotal. Once the collective reward is secured, it immediately
    reverts to defection. It always defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_0_indexed = current_round_0_indexed - 1
        num_cooperators_prev = int(history.my_actions[prev_round_0_indexed]) + np.sum(history.opponent_actions[prev_round_0_indexed, :])
        if num_cooperators_prev >= self.m:
            return Action.D
        elif num_cooperators_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_216(BaseStrategy):
    """
    The Adaptive Instigator-Free Rider strategy for the Collective Risk Dilemma.

    This strategy aims to:
    1. Free-ride whenever the collective good is provided.
    2. Instigate/Restore cooperation only when its own contribution is critical
       to achieving the collective good, thus unlocking the 'k' bonus for itself.
    3. Defect by default to avoid being a sucker if its cooperation is not critical
       or if collective cooperation proves consistently unviable.
    4. Maximize individual gain in the final round by always defecting.

    It maintains state regarding consecutive failures to meet the cooperation threshold
    and the number of cooperators observed in the previous round to make adaptive decisions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failed_rounds: int = 0
        self.observed_cooperators_last_round: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.D
        prev_round_index = state.round_number - 1
        my_prev_action_was_cooperate = history.my_actions[prev_round_index] == Action.C.value
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :] == Action.C.value)
        self.observed_cooperators_last_round = (1 if my_prev_action_was_cooperate else 0) + opponent_cooperators_prev_round
        if self.observed_cooperators_last_round < self.game_description.m:
            self.consecutive_failed_rounds += 1
        else:
            self.consecutive_failed_rounds = 0
        if self.consecutive_failed_rounds >= max(3, self.game_description.m):
            return Action.D
        elif self.observed_cooperators_last_round >= self.game_description.m:
            return Action.D
        elif self.observed_cooperators_last_round == self.game_description.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_217(BaseStrategy):
    """
    Focused Exploiter strategy: Aggressively free-rides by default, contributes
    only when strategically pivotal to reaching the cooperation threshold (m-1 cooperators
    observed in the previous round, making self the potential 'pivot'). It avoids
    contributing when success is unlikely (too few cooperators).
    Always defects in the first and last rounds to maximize immediate self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t == n_rounds:
            return Action.D
        my_action_prev_round = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
        N_C_prev = int(my_action_prev_round) + sum(opponent_actions_prev_round)
        if N_C_prev >= m:
            return Action.D
        elif N_C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_218(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.
    This strategy prioritizes individual gain by exploiting others' cooperation when possible,
    and only contributing when absolutely necessary and individually beneficial to secure
    the collective reward. It probes by defecting, maximizes free-riding when the threshold
    is met, makes a minimal necessary contribution if critical for meeting the threshold
    and individually beneficial, and defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        my_action_prev_round = history.my_actions[-1]
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + my_action_prev_round
        if num_cooperators_prev_round >= m_threshold:
            return Action.D
        else:
            other_cooperators_prev_round = num_cooperators_prev_round - my_action_prev_round
            if other_cooperators_prev_round == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_219(BaseStrategy):
    """
    The "Smart Adaptive Free-Rider" (SAFR) strategy aims to maximize individual payoff
    by primarily defecting, but strategically cooperates when its contribution is critical
    to securing the collective reward (k), only to revert to defecting once the collective
    good is secured. It adapts based on the previous round's observed cooperation levels
    from opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t == total_rounds_r:
            return Action.D
        if current_round_t == 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        num_other_cooperated_last_round = int(np.sum(history.opponent_actions[previous_round_idx, :]))
        if num_other_cooperated_last_round >= min_cooperators_m:
            return Action.D
        elif num_other_cooperated_last_round == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_220(BaseStrategy):
    """
    The Adaptive Exploiter with Minimum Contribution (AEMC) strategy aims to maximize
    individual payoff by free-riding on the contributions of others. It strategically
    cooperates only when the collective reward 'k' is at risk of not being met,
    viewing cooperation as an investment to ensure the highest possible individual
    payoff (1+k) remains attainable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        my_prev_action_was_C = history.my_actions[previous_round_index]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[previous_round_index, :])
        num_cooperators_last_round = int(my_prev_action_was_C) + opponent_cooperators_last_round
        if num_cooperators_last_round >= self.game_description.m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_221(BaseStrategy):
    """
    Adaptive Free-Rider with End-Game Defection strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by leveraging others' contributions,
    cooperating only when critical for self-benefit, and defecting when possible or when cooperation is futile.
    It anticipates and exploits the "endgame effect" in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        elif current_round_t == n_rounds:
            return Action.D
        else:
            n_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
            n_cooperators_prev += history.my_actions[state.round_number - 1]
            if n_cooperators_prev >= m_threshold:
                return Action.D
            elif n_cooperators_prev == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_222(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma that aims to maximize total payoff
    over `r` rounds. It adapts its behavior based on the group's past cooperation, striving to
    free-ride when possible but contributing strategically when its cooperation is critical
    to achieving the collective reward `k`.

    Core Principles:
    1.  **First Round Cooperation:** Start by cooperating to signal willingness and encourage initial group success.
    2.  **Endgame Exploitation:** Defect in the final round to maximize immediate personal gain without future consequences.
    3.  **Intermediate Rounds:**
        *   If the collective threshold `m` was met in the previous round, free-ride by defecting.
        *   If the threshold was *not* met:
            *   If my cooperation would single-handedly meet the threshold (`needed_cooperators == 1`), cooperate to secure `k` (since `k > 1`).
            *   Otherwise (`needed_cooperators > 1`), defect to avoid a wasted contribution (payoff of 0) and secure 1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        N_c_prev = num_opponent_cooperators_prev + history.my_actions[-1]
        if N_c_prev >= m_threshold:
            return Action.D
        else:
            needed_cooperators = m_threshold - N_c_prev
            if needed_cooperators == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_223(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to free-ride on others' cooperation whenever possible,
    ensuring it still benefits from the collective reward 'k'. It will only
    cooperate when its contribution is critical to achieving the 'm' cooperator
    threshold AND when receiving the 'k' reward (from cooperating) is
    individually more beneficial than defecting (and potentially causing the
    threshold to fail). If the group consistently fails to meet the threshold,
    AFR will defect to at least secure its private endowment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_action_prev_was_c = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        C_prev = opponent_cooperators_prev + (1 if my_action_prev_was_c else 0)
        Met_prev = C_prev >= self.m
        if Met_prev:
            C_prev_not_us = opponent_cooperators_prev
            if C_prev_not_us >= self.m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_224(BaseStrategy):
    """
    The Adaptive Free-Rider strategy maximizes individual payoff by strategically leveraging 
    the collective effort of others while minimizing personal contribution. It adapts to 
    the observed level of cooperation in the preceding round, prioritizing defection 
    when the collective reward is secured or when individual cooperation would be ineffective.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        cooperators_prev_round_opponents = sum(history.opponent_actions[-1, :])
        my_action_prev_round = history.my_actions[-1]
        cooperators_prev_round = cooperators_prev_round_opponents + (1 if my_action_prev_round else 0)
        if cooperators_prev_round >= self.m:
            return Action.D
        elif cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_225(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy for the Collective Risk Dilemma.
    This strategy attempts to free-ride aggressively but is also willing to
    make strategic, calculated contributions to prevent the collapse of the
    collective project, thereby preserving future free-riding opportunities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: bool = False
        self._last_success_status: bool = False
        self._last_num_cooperators: int = 0
        self._num_times_i_cooperated_and_failed: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number > 0 and history is not None:
            my_action_prev_round_bool = history.my_actions[-1]
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            self._last_num_cooperators = int(opponent_cooperators_prev_round + (1 if my_action_prev_round_bool else 0))
            self._last_success_status = self._last_num_cooperators >= self.game_description.m
        action_for_current_round_bool = False
        if current_round_1_indexed == 1:
            action_for_current_round_bool = False
        elif current_round_1_indexed == self.game_description.n_rounds:
            action_for_current_round_bool = False
        elif self._last_success_status:
            action_for_current_round_bool = False
            self._num_times_i_cooperated_and_failed = 0
        elif self._my_last_action:
            self._num_times_i_cooperated_and_failed += 1
            if self._num_times_i_cooperated_and_failed >= 2:
                action_for_current_round_bool = False
            else:
                action_for_current_round_bool = True
        else:
            self._num_times_i_cooperated_and_failed = 0
            if self._last_num_cooperators >= self.game_description.m - 1:
                action_for_current_round_bool = True
            else:
                action_for_current_round_bool = True
        self._my_last_action = action_for_current_round_bool
        return Action.C if action_for_current_round_bool else Action.D

class Strategy_EXPLOITATIVE_226(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy is designed to maximize individual payoff
    by strategically leveraging observed opponent behavior in the Collective Risk Dilemma.
    It aims to free-ride on others' cooperation whenever possible, while also making
    calculated "investments" to ensure collective success that can then be exploited.
    It incorporates initial probing, adaptive response, and end-game defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.initial_coop_rounds = 3

    def _get_num_cooperators_in_round(self, target_round_idx: int, history: PlayerHistory) -> int:
        """
        Helper function: Counts how many players (including self) chose 'C'
        in the specified 0-indexed round.
        """
        my_action = history.my_actions[target_round_idx]
        opponent_actions = history.opponent_actions[target_round_idx, :]
        cooperators_count = int(my_action) + np.sum(opponent_actions)
        return cooperators_count

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed < self.initial_coop_rounds:
            return Action.C
        prev_round_0_indexed = current_round_0_indexed - 1
        num_cooperators_prev_round = self._get_num_cooperators_in_round(prev_round_0_indexed, history)
        if num_cooperators_prev_round >= m:
            return Action.D
        elif num_cooperators_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_227(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by defaulting to defection,
    free-riding on others' cooperation, and only cooperating strategically when:
    1. Its cooperation is pivotal to reaching the 'm' cooperator threshold.
    2. The 'k' reward factor is high enough (k >= 2) to make the individual
       sacrifice profitable (payoff of k-1 >= 1).
    It avoids cooperation when not pivotal, when 'k' is low, or in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        reward_factor_k = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == total_rounds:
            return Action.D
        if state.round_number == 0:
            return Action.D
        prev_round_idx = state.round_number - 1
        n_c_others_prev = int(np.sum(history.opponent_actions[prev_round_idx, :]))
        if n_c_others_prev >= min_cooperators_needed:
            return Action.D
        elif n_c_others_prev == min_cooperators_needed - 1:
            if reward_factor_k >= 2:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_228(BaseStrategy):
    """
    Adaptive Opportunist strategy for the Collective Risk Dilemma.

    This strategy aims to free-ride on the cooperation of others. It defects
    whenever the collective good is provided. If collective cooperation fails
    consistently, it will make a calculated, temporary cooperation only if its
    contribution has a high chance of restoring the public good, thereby
    enabling future free-riding. It avoids being a "sucker" by never
    cooperating when it's unlikely to make a difference or when it's just
    been exploited.
    """
    COOPERATION_RESEED_THRESHOLD_M_DIFF: int = 1
    FAILURE_STREAK_FOR_RESEED: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_action: Action
        if state.round_number == 0:
            current_round_action = Action.D
            self.consecutive_failures = 0
            self.my_last_action = current_round_action
            return current_round_action
        if state.round_number == self.game_description.n_rounds - 1:
            current_round_action = Action.D
            return current_round_action
        N_C_prev_opponents: int = np.sum(history.opponent_actions[-1, :])
        N_C_prev_me: int = 1 if self.my_last_action == Action.C else 0
        N_C_prev: int = N_C_prev_opponents + N_C_prev_me
        if N_C_prev >= self.game_description.m:
            current_round_action = Action.D
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            condition_1_failure_streak_met = self.consecutive_failures >= self.FAILURE_STREAK_FOR_RESEED
            condition_2_close_to_m_met = self.game_description.m - N_C_prev <= self.COOPERATION_RESEED_THRESHOLD_M_DIFF
            condition_3_not_sucker_last_round = self.my_last_action == Action.D
            if condition_1_failure_streak_met and condition_2_close_to_m_met and condition_3_not_sucker_last_round:
                current_round_action = Action.C
            else:
                current_round_action = Action.D
        self.my_last_action = current_round_action
        return current_round_action

class Strategy_EXPLOITATIVE_229(BaseStrategy):
    """
    Adaptive Free-Rider with Safety Net and End-Game Exploitation strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding whenever the collective threshold `m`
    is likely to be met by others or if its own contribution is not strictly necessary. It contributes
    only when essential to ensure the threshold is met after a collective failure, or to maintain a
    barely successful project. In the final round, it always defects for immediate gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_opponent_cooperators_prev = sum(history.opponent_actions[-1, :])
        N_c_prev = num_opponent_cooperators_prev + int(my_action_prev)
        if N_c_prev >= self.m:
            if N_c_prev > self.m:
                return Action.D
            elif my_action_prev == Action.C:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_230(BaseStrategy):
    """
    The Opportunistic Pacer aims to free-ride as much as possible, always seeking the 1+k payoff.
    It conditionally cooperates for a single round when it observes a critical shortfall,
    specifically when its cooperation alone can meet the threshold (m-1 others cooperated).
    After this "pacing" cooperation, it immediately reverts to free-riding to test if the
    collective has re-established sufficient cooperation.
    It defaults to defecting, especially in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.last_action_was_coop = False
        self.consecutive_misses_while_defecting = 0
        self.Patience_Threshold = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        current_action = Action.D
        n_cooperators_total_in_prev_round = 0
        my_action_in_prev_round_is_coop = False
        if state.round_number > 0:
            my_action_in_prev_round_is_coop = history.my_actions[-1]
            n_cooperators_opponents_in_prev_round = np.sum(history.opponent_actions[-1, :])
            n_cooperators_total_in_prev_round = n_cooperators_opponents_in_prev_round + int(my_action_in_prev_round_is_coop)
            if n_cooperators_total_in_prev_round >= self.m:
                self.consecutive_misses_while_defecting = 0
            elif not my_action_in_prev_round_is_coop:
                self.consecutive_misses_while_defecting += 1
            else:
                self.consecutive_misses_while_defecting = 0
            self.last_action_was_coop = my_action_in_prev_round_is_coop
        if current_round_1_indexed == 1:
            current_action = Action.D
        elif current_round_1_indexed == self.n_rounds:
            current_action = Action.D
        elif n_cooperators_total_in_prev_round >= self.m:
            current_action = Action.D
        elif self.last_action_was_coop and n_cooperators_total_in_prev_round < self.m:
            current_action = Action.D
        elif self.consecutive_misses_while_defecting >= self.Patience_Threshold and n_cooperators_total_in_prev_round == self.m - 1:
            current_action = Action.C
        else:
            current_action = Action.D
        return current_action

class Strategy_EXPLOITATIVE_231(BaseStrategy):
    """
    Calculated Free-Rider with Collective Goal Insurance strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by primarily defecting and free-riding on others'
    contributions. It only cooperates in specific, pivotal situations where its contribution is
    critical to achieving the collective reward and the personal benefit of securing that reward
    outweighs the benefit of defecting.

    Key decision points:
    - Defect in the first and last rounds as there are no future consequences or prior history.
    - Free-ride (Defect) if the number of cooperators in the previous round was already sufficient
      to meet the threshold 'm'.
    - Cooperate if its contribution is exactly what's needed to meet 'm' (i.e., m-1 cooperators
      last round) AND the reward factor 'k' is high enough (k > 2) such that personal gain from
      cooperating (k-1) exceeds defecting (1).
    - Defect if cooperation is futile (not enough cooperators even with self's contribution).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            cooperators_last_round = 0
        else:
            cooperators_last_round = sum(history.opponent_actions[-1, :])
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if cooperators_last_round >= self.m:
            return Action.D
        if cooperators_last_round == self.m - 1:
            if self.k > 2:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_232(BaseStrategy):
    """
    The Adaptive Free-Rider strategy exploits the Collective Risk Dilemma by prioritizing
    defection to maximize individual gain. It only cooperates when its single contribution
    is pivotal to meeting the collective reward threshold 'm' and doing so is strictly
    more profitable than defecting (i.e., when k > 1 and N_C_prev == m - 1). It
    defaults to defection in the first and last rounds, and whenever the threshold 'm'
    was met in the previous round, or if its cooperation is insufficient to meet 'm'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
        if num_cooperators_prev_round >= m_threshold:
            return Action.D
        elif num_cooperators_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_233(BaseStrategy):
    """
    Adaptive Free-Rider with Critical Contribution Strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by free-riding whenever possible,
    only cooperating when its contribution is critically needed to meet the collective
    reward threshold 'm'. It defects in the first round to test for free-riding opportunities
    and in the last round due to lack of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        num_other_cooperators_previous_round = np.sum(previous_round_opponent_actions)
        if num_other_cooperators_previous_round >= self.m:
            return Action.D
        elif num_other_cooperators_previous_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_234(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to maximize its own payoff in the
    Collective Risk Dilemma by primarily defecting, while strategically cooperating only
    when it is individually profitable and decisive for achieving the collective reward `k`.

    It defects in the first and last rounds. In intermediate rounds, it free-rides if the
    cooperation threshold was met in the previous round. If the threshold was not met,
    it defects if its cooperation would be futile. It only cooperates if its contribution
    is decisive (bringing the total to `m`) AND the individual payoff from cooperating
    (k-1) is strictly greater than the payoff from defecting (1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == n_rounds - 1:
            return Action.D
        else:
            prev_round_idx = current_round - 1
            num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + history.my_actions[prev_round_idx]
            if num_cooperators_prev_round >= m_threshold:
                return Action.D
            elif num_cooperators_prev_round + 1 < m_threshold:
                return Action.D
            elif k_reward > 2.0:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_235(BaseStrategy):
    """
    Adaptive Free-Rider with Resilience strategy for Collective Risk Dilemma.
    Aims to maximize individual payoff by strategically leveraging cooperation
    to enable collective rewards (k), then free-riding on that success.
    Includes a mechanism to prevent prolonged self-sacrifice.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.F_max: int = 3
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        my_action_prev_round_bool = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round_bools = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(my_action_prev_round_bool) + np.sum(opponent_actions_prev_round_bools)
        if num_cooperators_prev_round >= m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if num_cooperators_prev_round >= m:
            return Action.D
        elif self.consecutive_failures >= self.F_max:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_236(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding on the cooperation of others.
    It strategically cooperates only when necessary to establish or re-establish the collective
    benefit, providing opportunities for exploitation. It always defects in the final round
    due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_action_last_round_bool = history.my_actions[-1]
        num_cooperators_last_round = int(my_action_last_round_bool) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_last_round >= m_threshold:
            return Action.D
        elif my_action_last_round_bool:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_237(BaseStrategy):
    """
    Adaptive Threshold Free-Rider Strategy for the Collective Risk Dilemma.

    This strategy is designed to free-ride aggressively when cooperation is abundant,
    but also to contribute strategically when cooperation is scarce or when a
    "re-seeding" of cooperation is necessary to maintain the collective reward in the long run.

    It adapts to observed opponent behavior and game parameters, maximizing personal
    payoff over 'r' rounds while operating under standard game theory assumptions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_my_cooperation: int = 0
        self.defection_streak_limit: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            self.rounds_since_my_cooperation = 0
            return Action.C
        elif state.round_number == n_rounds - 1:
            return Action.D
        else:
            prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            my_prev_action_was_C = history.my_actions[-1]
            C_prev = prev_round_opponent_cooperators + (1 if my_prev_action_was_C else 0)
            if C_prev >= m_threshold:
                if self.rounds_since_my_cooperation < self.defection_streak_limit:
                    self.rounds_since_my_cooperation += 1
                    return Action.D
                else:
                    self.rounds_since_my_cooperation = 0
                    return Action.C
            elif C_prev + 1 >= m_threshold:
                self.rounds_since_my_cooperation = 0
                return Action.C
            else:
                self.rounds_since_my_cooperation += 1
                return Action.D

class Strategy_EXPLOITATIVE_238(BaseStrategy):
    """
    The Opportunistic Trigger strategy: Defect by default, cooperating only as a
    calculated, minimal investment to "trigger" or sustain the collective good (k),
    then immediately returning to defection to free-ride. It avoids being a "sucker"
    by not cooperating when contributions are unlikely to be pivotal or when others
    show no willingness to contribute.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        elif current_round_0_indexed == n_rounds - 1:
            return Action.D
        else:
            prev_total_cooperators = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
            if prev_total_cooperators >= m:
                return Action.D
            elif prev_total_cooperators == 0:
                return Action.D
            elif prev_total_cooperators >= m - 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_239(BaseStrategy):
    """
    The Pragmatic Free-Rider strategy prioritizes defection but will temporarily cooperate
    if consistent collective failure to meet the cooperation threshold 'm' occurs,
    to prevent total collapse and ensure the 'k' bonus remains achievable. It seeks
    to free-ride whenever possible, including exploiting situations where its cooperation
    is redundant and betraying in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._num_cooperators_last_round: int | None = None
        self.rounds_since_last_cooperation_failure: int = 0
        self.rounds_committed_to_cooperation: int = 0
        self.FAILURE_THRESHOLD_FOR_COMMITMENT: int = 2
        self.COMMITMENT_DURATION: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        if state.round_number > 0:
            observed_num_cooperators_in_t_minus_1 = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            self._my_last_action = Action.C if history.my_actions[-1] else Action.D
            self._num_cooperators_last_round = observed_num_cooperators_in_t_minus_1
            if observed_num_cooperators_in_t_minus_1 < m:
                self.rounds_since_last_cooperation_failure += 1
            else:
                self.rounds_since_last_cooperation_failure = 0
            if self.rounds_committed_to_cooperation > 0:
                self.rounds_committed_to_cooperation -= 1
        action_for_this_round: Action
        if state.round_number == self.game_description.n_rounds - 1:
            action_for_this_round = Action.D
        elif state.round_number == 0:
            action_for_this_round = Action.D
        elif self.rounds_committed_to_cooperation > 0:
            action_for_this_round = Action.C
        elif self.rounds_since_last_cooperation_failure >= self.FAILURE_THRESHOLD_FOR_COMMITMENT:
            self.rounds_committed_to_cooperation = self.COMMITMENT_DURATION
            action_for_this_round = Action.C
        elif self._my_last_action == Action.C and self._num_cooperators_last_round is not None and (self._num_cooperators_last_round >= m) and (self._num_cooperators_last_round > m):
            action_for_this_round = Action.D
        else:
            action_for_this_round = Action.D
        return action_for_this_round

class Strategy_EXPLOITATIVE_240(BaseStrategy):
    """
    The "System Exploiter with Viability Guard" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily free-riding on others'
    contributions. It strategically cooperates only when necessary:
    1. In the first round, to "kickstart" cooperation and assess the environment.
    2. In intermediate rounds, if the collective effort was exactly one cooperator
       short of the threshold in the previous round. This "Viability Guard" move
       prevents the collapse of the collective benefit (k), allowing future exploitation.
    It defects in all other scenarios, including the final round (terminal exploitation),
    and when the collective effort is far from the threshold (signaling for others to contribute).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators), k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing historical actions and payoffs for the player
                     and opponents. It is None for the first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == total_rounds - 1:
            return Action.D
        prev_round_idx = current_round_idx - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_round_was_C = history.my_actions[prev_round_idx]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if num_cooperators_prev_round >= m_threshold:
            return Action.D
        elif num_cooperators_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_241(BaseStrategy):
    """
    The Adaptive Opportunist strategy aims to maximize individual payoff in the
    Collective Risk Dilemma by being primarily exploitative but also strategically
    cooperative when it's individually advantageous and critical to secure
    the collective benefit. It defects in the first and last rounds and
    after a successful collective effort, but cooperates in intermediate rounds
    only when its contribution is precisely what's needed to meet the threshold (m)
    after a previous round's failure, ensuring a personal gain (k > 1) that outweighs the cost of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_C = history.my_actions[-1]
            C_prev = opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
            if C_prev >= self.m:
                return Action.D
            else:
                coop_needed = self.m - C_prev
                if coop_needed == 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_242(BaseStrategy):
    """
    The "Opportunistic Defector" strategy prioritizes self-interest by free-riding whenever
    possible and only cooperating when it's critically necessary and more profitable than defecting.
    It leverages backward induction by defecting in the final round and starts by defecting
    in the first round due to lack of information. In intermediate rounds, it assesses the
    level of cooperation from other players in the previous round to decide its action.

    Decision Rules:
    1.  First Round: Always Defect (D) to attempt immediate free-riding.
    2.  Last Round: Always Defect (D) due to backward induction (no future consequences).
    3.  Intermediate Rounds:
        a.  If `m` or more opponents cooperated in the previous round: Defect (D) to free-ride,
            as the threshold is likely to be met, yielding max payoff (1+k).
        b.  If exactly `m-1` opponents cooperated in the previous round: Cooperate (C),
            as my cooperation is critical to meet the threshold, yielding `k` (which is > 1).
        c.  If fewer than `m-1` opponents cooperated in the previous round: Defect (D),
            as even my cooperation won't meet the threshold, yielding `0`, whereas defecting yields `1`.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.r - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        N_others_cooperated_last_round = np.sum(history.opponent_actions[previous_round_index, :])
        if N_others_cooperated_last_round >= self.m:
            return Action.D
        elif N_others_cooperated_last_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_243(BaseStrategy):
    """
    Conditional Free-Rider with Reciprocal Incentive (CFRRI) strategy.
    This strategy aims to maximize individual payoff by intelligently combining
    free-riding, opportunistic cooperation, strategic investment, and loss aversion.
    It adapts to collective behavior, contributes when its action is critical,
    and cuts losses when cooperation consistently fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators_last_round: int = 0
        self.consecutive_rounds_below_m: int = 0
        self.defection_streak_tolerance: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number > 0:
            opponents_cooperated_last_round = history.opponent_actions[current_round_number - 1, :].sum()
            my_action_last_round_was_cooperate = history.my_actions[current_round_number - 1]
            self.num_cooperators_last_round = opponents_cooperated_last_round + int(my_action_last_round_was_cooperate)
            if self.num_cooperators_last_round >= self.game_description.m:
                self.consecutive_rounds_below_m = 0
            else:
                self.consecutive_rounds_below_m += 1
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        required_m = self.game_description.m
        if self.num_cooperators_last_round >= required_m:
            return Action.D
        elif self.num_cooperators_last_round == required_m - 1:
            return Action.C
        elif self.consecutive_rounds_below_m <= self.defection_streak_tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_244(BaseStrategy):
    """
    The Opportunistic Initiator strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly exploitative and robust. It makes a minimal
    initial investment by cooperating in the first round to test for a cooperative
    environment. Subsequently, it relentlessly defects: either to free-ride and
    exploit if cooperation was established (threshold met in previous round), or
    to strictly minimize losses if cooperation failed (threshold not met). In the
    final round, it employs the dominant strategy of defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_245(BaseStrategy):
    """
    This strategy implements an 'Adaptive Free-Rider with Minimal Necessary Contribution' for the
    Collective Risk Dilemma. It aims to maximize individual payoff by defecting whenever possible
    (free-riding), only cooperating when its contribution is pivotal to secure the collective
    reward 'k', or when free-riding would be futile. It specifically defects in the first and last
    rounds due to game-theoretic considerations, and in intermediate rounds, adapts based on the
    previous round's collective outcome and its own previous action.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev = history.my_actions[-1]
        my_cooperation_prev_int = int(my_action_prev)
        C_prev = num_opponent_cooperators_prev + my_cooperation_prev_int
        if C_prev >= m:
            if C_prev > m:
                return Action.D
            elif my_action_prev:
                return Action.C
            else:
                return Action.D
        elif C_prev < m - 1:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_246(BaseStrategy):
    """
    The Adaptive Exploiter strategy combines initial generosity, a reactive free-riding mechanism,
    and a "collective punishment" system to coerce cooperation when necessary, all with the
    ultimate goal of maximizing individual payoff. It adapts to observed collective behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_last_action: Action | None = None
        self.others_cooperation_observations: list[int] = []
        self.consecutive_failures: int = 0
        self.punishment_rounds_active: int = 0
        self.PUNISHMENT_DURATION: int = 2
        self.FAILURE_THRESHOLD_FOR_PUNISHMENT: int = 2
        self.HISTORY_WINDOW_SIZE: int = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round > 0:
            my_action_prev_round_bool = history.my_actions[-1]
            others_cooperation_prev_round = int(np.sum(history.opponent_actions[-1, :]))
            C_total_prev = others_cooperation_prev_round + (1 if my_action_prev_round_bool else 0)
            if self.my_last_action == Action.C:
                self.others_cooperation_observations.append(C_total_prev - 1)
            else:
                self.others_cooperation_observations.append(C_total_prev)
            if C_total_prev < self.m:
                self.consecutive_failures += 1
            else:
                self.consecutive_failures = 0
            if self.consecutive_failures >= self.FAILURE_THRESHOLD_FOR_PUNISHMENT:
                self.punishment_rounds_active = self.PUNISHMENT_DURATION
                self.consecutive_failures = 0
        current_action = Action.D
        if current_round == 0:
            current_action = Action.C
        elif current_round == self.n_rounds - 1:
            current_action = Action.D
        elif self.punishment_rounds_active > 0:
            current_action = Action.D
            self.punishment_rounds_active -= 1
        else:
            estimated_others_cooperation_float = self._calculate_rolling_average(self.others_cooperation_observations, self.HISTORY_WINDOW_SIZE)
            estimated_others_cooperation = round(estimated_others_cooperation_float)
            if estimated_others_cooperation >= self.m:
                current_action = Action.D
            elif estimated_others_cooperation == self.m - 1:
                current_action = Action.C
            else:
                current_action = Action.D
        self.my_last_action = current_action
        return current_action

    def _calculate_rolling_average(self, data_list: list[int], window_size: int) -> float:
        """
        Helper function to calculate a rolling average of a list of data points.
        Returns 0.0 if the list is empty or the relevant window is empty.
        """
        if not data_list:
            return 0.0
        start_index = max(0, len(data_list) - window_size)
        relevant_observations = data_list[start_index:]
        if not relevant_observations:
            return 0.0
        return sum(relevant_observations) / len(relevant_observations)

class Strategy_EXPLOITATIVE_247(BaseStrategy):
    """
    Adaptive Exploiter with Pivotal Correction strategy for the Collective Risk Dilemma.
    This strategy aims to maximize own payoff by free-riding when possible,
    but cooperating when pivotal to secure the collective reward 'k', or to probe initial behavior.
    It adapts to observed collective behavior and prioritizes self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        c_prev = opponent_cooperators_prev + (1 if my_action_prev_bool else 0)
        my_last_action_prev = Action.C if my_action_prev_bool else Action.D
        if c_prev >= self.m:
            return Action.D
        elif my_last_action_prev == Action.C:
            return Action.D
        elif c_prev + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_248(BaseStrategy):
    """
    Adaptive Opportunist with Strategic Probing strategy for Collective Risk Dilemma.

    This strategy aims to maximize individual payoff over 'r' rounds by:
    1.  Always striving for the '1+k' payoff when the 'k' bonus is achievable.
    2.  Minimizing the risk of receiving the '0' "sucker's payoff".
    3.  Strategically investing in cooperation only when it has the highest leverage to trigger
        the 'k' bonus, enabling future exploitation.

    It incorporates backward induction by defecting in the first and last rounds, and
    dynamically adjusts its behavior in intermediate rounds based on observed cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1 or current_round_1_indexed == self.r:
            return Action.D
        prev_round_0_indexed = state.round_number - 1
        my_prev_action_was_coop = history.my_actions[prev_round_0_indexed]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_0_indexed, :])
        C_prev = int(my_prev_action_was_coop) + opponent_cooperators_prev_round
        if C_prev >= self.m:
            if self.n == self.m and C_prev == self.n:
                return Action.C
            else:
                return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_249(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by defaulting to defecting,
    only cooperating when its contribution is crucial to ensure collective success
    (and thus individual reward k), or to prevent the collapse of the project
    when success is precarious. It free-rides on robust success and cuts losses
    on deep failure, defecting unconditionally in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        opponent_cooperators_last_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        my_action_last_round_was_cooperate = history.my_actions[current_round_0_indexed - 1]
        C_last = opponent_cooperators_last_round + (1 if my_action_last_round_was_cooperate else 0)
        if C_last >= self.m_threshold + 1:
            return Action.D
        elif C_last == self.m_threshold or C_last == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_250(BaseStrategy):
    """
    The "Calculated Opportunist" strategy for the Collective Risk Dilemma.

    This strategy is designed to be highly self-interested, observing the
    collective behavior and making decisions to maximize its own payoff.
    It will free-ride whenever the collective good is provided and will only
    contribute when its contribution is critically necessary and individually
    profitable (as k > 1 is a given parameter).

    Key decisions are based on the outcome of the previous round:
    - In the last round, it always defects.
    - In the first round, it cooperates to probe the collective's willingness.
    - In intermediate rounds, it defects if the threshold was met (to free-ride).
    - If the threshold was not met, it cooperates only if its contribution
      alone is sufficient to meet the threshold (because k > 1 makes this
      individually profitable). Otherwise, it defects to avoid a wasted contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters like
                                                          n_players, n_rounds, m, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing records of
                                            past actions and payoffs for this
                                            player and opponents. None for round 0.

        Returns:
            Action: The chosen action for the current round (Action.C or Action.D).
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        cooperators_prev_round = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
        if cooperators_prev_round >= m_threshold:
            return Action.D
        elif cooperators_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_251(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy defaults to defection to free-ride. It strategically cooperates
    only when its contribution is critically necessary to meet the collective
    threshold, aiming to stabilize cooperation among others for future exploitation.
    It avoids cooperating when its contribution won't make a difference.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        C_prev_opponents = np.sum(history.opponent_actions[-1, :])
        C_prev_my_action = 1 if history.my_actions[-1] else 0
        C_prev = C_prev_opponents + C_prev_my_action
        if C_prev >= self.m:
            return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_252(BaseStrategy):
    """
    Adaptive Exploitative Free-Rider (AEFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own total payoff by strategically cooperating
    when necessary to achieve the collective reward `k` and free-riding (defecting)
    whenever possible. It adapts to opponent behavior and implements a reactive
    punishment mechanism.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.ALPHA = 0.5
        self.FORGIVENESS_THRESHOLD = 2
        self.PUNISHMENT_DURATION = max(1, min(5, math.floor(self.game_description.n_rounds / 4)))
        self.cooperation_target = float(self.game_description.m)
        self.punishment_mode = False
        self.punishment_rounds_left = 0
        self.defection_streak_failure = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        my_action_prev_round_is_C = history.my_actions[-1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        self.cooperation_target = self.ALPHA * num_cooperators_prev_round + (1 - self.ALPHA) * self.cooperation_target
        self.cooperation_target = max(0.0, min(float(n), self.cooperation_target))
        if num_cooperators_prev_round < m:
            self.defection_streak_failure += 1
            if self.defection_streak_failure >= self.FORGIVENESS_THRESHOLD and (not self.punishment_mode):
                self.punishment_mode = True
                self.punishment_rounds_left = self.PUNISHMENT_DURATION
        else:
            self.defection_streak_failure = 0
        if state.round_number == r - 1:
            return Action.D
        if self.punishment_mode:
            if self.punishment_rounds_left > 0:
                self.punishment_rounds_left -= 1
                return Action.D
            else:
                self.punishment_mode = False
        expected_cooperators_total = round(self.cooperation_target)
        if expected_cooperators_total >= m:
            return Action.D
        elif expected_cooperators_total == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_253(BaseStrategy):
    """
    The "Opportunistic Free-Rider" strategy aims to maximize individual payoff by
    consistently attempting to free-ride on the cooperation of other players.
    It adapts to observed group behavior, cooperating only when strategically
    necessary to achieve the collective reward `k`, and always prioritizing
    its private gain by defecting when possible. It explicitly accounts for the
    first and last rounds of the game, which have unique strategic considerations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds:
            return Action.D
        my_action_prev_was_C = history.my_actions[-1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = num_opponent_cooperators_prev + int(my_action_prev_was_C)
        if total_cooperators_prev_round >= min_cooperators_needed:
            return Action.D
        elif not my_action_prev_was_C:
            if total_cooperators_prev_round + 1 >= min_cooperators_needed:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_254(BaseStrategy):
    """
    The Calculated Exploiter strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by primarily defecting (free-riding),
    but intelligently cooperates (stimulates) when a sustained lack of collective
    action threatens to prevent the desirable 'k' reward from being realized.
    It's adaptive, robust, and prioritizes self-interest by investing only
    when necessary to create exploitable opportunities, and always seeking to
    free-ride on others' contributions when those opportunities arise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.tolerance_rounds = 2
        self._reset_state_for_new_game()

    def _reset_state_for_new_game(self):
        """
        Resets all internal state variables for a fresh game instance.
        This helps ensure that state does not leak between different game runs
        if a single object instance were reused, aligning with the pseudocode.
        """
        self.my_current_stance = 'DEFECT'
        self.rounds_in_current_stance = 0
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number
        if current_game_round == self.game_description.n_rounds - 1:
            action_for_this_round = Action.D
            self._reset_state_for_new_game()
            return action_for_this_round
        if current_game_round == 0:
            action_for_this_round = Action.D
            self.my_current_stance = 'DEFECT'
            self.rounds_in_current_stance = 1
            self.my_last_action = action_for_this_round
            return action_for_this_round
        prev_round_idx = current_game_round - 1
        my_action_in_prev_round = self.my_last_action
        prev_my_action_is_C = my_action_in_prev_round == Action.C
        prev_opponent_cooperators_count = np.sum(history.opponent_actions[prev_round_idx, :])
        prev_total_cooperators = prev_opponent_cooperators_count + (1 if prev_my_action_is_C else 0)
        if prev_total_cooperators >= self.game_description.m:
            self.my_current_stance = 'DEFECT'
            self.rounds_in_current_stance = 0
        elif not prev_my_action_is_C and self.my_current_stance == 'DEFECT':
            self.rounds_in_current_stance += 1
        elif prev_my_action_is_C and self.my_current_stance == 'COOPERATE_TO_STIMULATE':
            self.my_current_stance = 'DEFECT'
            self.rounds_in_current_stance = 0
        action_for_this_round = Action.D
        if self.my_current_stance == 'DEFECT':
            if self.rounds_in_current_stance >= self.tolerance_rounds:
                action_for_this_round = Action.C
                self.my_current_stance = 'COOPERATE_TO_STIMULATE'
                self.rounds_in_current_stance = 1
        elif self.my_current_stance == 'COOPERATE_TO_STIMULATE':
            action_for_this_round = Action.C
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_255(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to exploit the Collective Risk Dilemma
    by free-riding whenever the cooperation threshold 'm' is met. It strategically
    probes by cooperating when 'm' is not met to encourage collective action, but
    quickly reverts to defection if its cooperation is futile or if collective
    cooperation fails. It enters an "aggressive defection" mode if its cooperation
    repeatedly fails, and defects consistently in endgame rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PROBE_AT_START_ROUNDS = 1
        self.REPROBE_AFTER_FAILURE_COUNT = 2
        self.ENDGAME_ROUNDS = 2
        self.AGGRESSIVE_DEFECTION_THRESHOLD = 3
        self.consecutive_D_rounds_since_m_failed = 0
        self.consecutive_C_rounds_where_m_failed = 0
        self.in_aggressive_defection_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t_1_indexed = state.round_number + 1
        action_this_round = Action.D
        if state.round_number > 0:
            last_round_index = state.round_number - 1
            my_action_last_round = Action.C if history.my_actions[last_round_index] else Action.D
            num_cooperators_opponents_last_round = np.sum(history.opponent_actions[last_round_index])
            num_cooperators_last_round = num_cooperators_opponents_last_round + (1 if my_action_last_round == Action.C else 0)
            threshold_met_last_round = num_cooperators_last_round >= self.game_description.m
            if threshold_met_last_round:
                self.consecutive_D_rounds_since_m_failed = 0
                self.consecutive_C_rounds_where_m_failed = 0
                self.in_aggressive_defection_mode = False
            elif my_action_last_round == Action.C:
                self.consecutive_C_rounds_where_m_failed += 1
                self.consecutive_D_rounds_since_m_failed = 0
                self.in_aggressive_defection_mode = False
            else:
                self.consecutive_D_rounds_since_m_failed += 1
                if not self.in_aggressive_defection_mode:
                    self.consecutive_C_rounds_where_m_failed = 0
        else:
            threshold_met_last_round = False
        if current_round_t_1_indexed <= self.PROBE_AT_START_ROUNDS:
            action_this_round = Action.C
            self.in_aggressive_defection_mode = False
        elif current_round_t_1_indexed >= self.game_description.n_rounds - self.ENDGAME_ROUNDS + 1:
            action_this_round = Action.D
            self.in_aggressive_defection_mode = False
        elif self.in_aggressive_defection_mode:
            action_this_round = Action.D
        elif threshold_met_last_round:
            action_this_round = Action.D
        elif self.consecutive_C_rounds_where_m_failed >= self.AGGRESSIVE_DEFECTION_THRESHOLD:
            action_this_round = Action.D
            self.in_aggressive_defection_mode = True
        elif self.consecutive_D_rounds_since_m_failed >= self.REPROBE_AFTER_FAILURE_COUNT:
            action_this_round = Action.C
        else:
            action_this_round = Action.D
        return action_this_round

class Strategy_EXPLOITATIVE_256(BaseStrategy):
    """
    Adaptive Opportunist strategy for the Collective Risk Dilemma.
    Aims to maximize individual payoff by primarily free-riding on the cooperation of others.
    It is highly adaptive, reacting to observed group behavior, and makes strategic,
    self-interested contributions only when decisive and beneficial.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t == total_rounds_r:
            return Action.D
        if current_round_t == 1:
            return Action.D
        my_last_action_was_C = history.my_actions[-1]
        n_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        if my_last_action_was_C:
            n_cooperators_last_round += 1
        if n_cooperators_last_round >= min_cooperators_m:
            return Action.D
        elif my_last_action_was_C:
            return Action.D
        elif n_cooperators_last_round == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_257(BaseStrategy):
    """
    The Calculated Opportunist strategy aims to maximize personal total payoff in the Collective Risk Dilemma.
    It strategically cooperates in the first round to probe group behavior, and defects in the final round
    for maximal personal gain. In intermediate rounds, it adapts by punishing collective failure through defection,
    exploiting surplus cooperation by free-riding, and maintaining critical mass when its own cooperation
    was essential to meet the threshold. This strategy is robust and exploitative, prioritizing individual
    benefit through calculated contributions and opportunistic free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        C_prev = int(history.my_actions[prev_round_index]) + np.sum(history.opponent_actions[prev_round_index, :])
        my_last_action_was_C = history.my_actions[prev_round_index]
        if C_prev < m_threshold:
            return Action.D
        elif C_prev > m_threshold:
            return Action.D
        elif my_last_action_was_C:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_258(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.
    It primarily defects (free-rides), but strategically cooperates when its
    contribution is individually necessary and sufficient to meet the cooperation
    threshold, or when it was critical in the previous round to maintain the bonus.
    It defects in the first and last rounds as exploitative moves.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the Adaptive Free-Rider strategy.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing historical actions and payoffs for
                     this player and opponents, or None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        N_C_last_opponents = np.sum(history.opponent_actions[prev_round_idx, :])
        Our_Action_last = history.my_actions[prev_round_idx]
        N_C_last = N_C_last_opponents + (1 if Our_Action_last else 0)
        if N_C_last >= m_threshold:
            if Our_Action_last == True and N_C_last == m_threshold:
                return Action.C
            else:
                return Action.D
        elif N_C_last + 1 >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_259(BaseStrategy):
    """
    Adaptive Free-Rider with Minimum Contribution strategy for the Collective Risk Dilemma.
    This strategy aims to benefit from the collective reward by ensuring the cooperation
    threshold (`m`) is met, but minimizes its own contribution. It cooperates only when
    necessary to achieve or maintain the `m` threshold and defects otherwise, being
    particularly aggressive in free-riding in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        elif current_round == n_rounds - 1:
            return Action.D
        else:
            last_round_number = current_round - 1
            my_action_cooperated_last_round = history.my_actions[last_round_number]
            opponent_cooperators_in_last_round = np.sum(history.opponent_actions[last_round_number, :])
            total_cooperators_in_last_round = opponent_cooperators_in_last_round
            if my_action_cooperated_last_round:
                total_cooperators_in_last_round += 1
            if total_cooperators_in_last_round < m_threshold:
                return Action.C
            elif not my_action_cooperated_last_round:
                return Action.D
            else:
                num_other_cooperators = total_cooperators_in_last_round - 1
                if num_other_cooperators >= m_threshold:
                    return Action.D
                else:
                    return Action.C

class Strategy_EXPLOITATIVE_260(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy.

    This strategy aims to maximize individual payoff by defaulting to free-riding (Defect).
    It strategically cooperates only when necessary to:
    1. Initiate collective success in the first round.
    2. Maintain critical cooperation if its contribution is essential to meet the threshold 'm'.
    3. Recover from collective failure for a limited number of rounds.

    The strategy adapts to opponent behavior by monitoring the number of cooperators
    in the previous round and adjusts its own action to either exploit a surplus,
    contribute critically, or attempt to re-establish a cooperative environment.
    It includes a mechanism to cut losses if cooperation repeatedly fails.
    """
    MAX_FAILURE_STREAK = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        n_cooperators_prev = sum(history.opponent_actions[-1, :])
        my_action_prev = history.my_actions[-1]
        if my_action_prev:
            n_cooperators_prev += 1
        current_round_action = Action.D
        if n_cooperators_prev >= m:
            self.failure_streak = 0
            if not my_action_prev:
                current_round_action = Action.D
            elif n_cooperators_prev > m:
                current_round_action = Action.D
            else:
                current_round_action = Action.C
        else:
            self.failure_streak += 1
            if self.failure_streak >= self.MAX_FAILURE_STREAK:
                current_round_action = Action.D
            else:
                current_round_action = Action.C
        return current_round_action

class Strategy_EXPLOITATIVE_261(BaseStrategy):
    """
    Pivotal Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding when possible
    and contributing only when its cooperation is pivotal and beneficial (k > 1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == r:
            return Action.D
        else:
            prev_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            my_prev_action_was_C = history.my_actions[-1]
            C_prev_round = prev_opponent_cooperators + (1 if my_prev_action_was_C else 0)
            if C_prev_round >= m:
                return Action.D
            elif C_prev_round == m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_262(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    It defaults to defection, free-rides on success, and only cooperates when
    its contribution is pivotal to reaching the collective threshold.
    It always defects in the first and final rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Free-Rider strategy with game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters like
                                                          n_players, n_rounds, m, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current player
        based on the Adaptive Free-Rider strategy rules.

        Args:
            state (GameState): Contains the current round number (0-indexed).
            history (None | PlayerHistory): Contains past actions and payoffs.
                                            It is None for the first round (round_number=0).

        Returns:
            Action: Action.C for Cooperate or Action.D for Defect.
        """
        n = self.game_description.n_players
        r_total = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        if state.round_number == r_total - 1:
            return Action.D
        if state.round_number == 0:
            return Action.D
        previous_round_index = state.round_number - 1
        previous_round_my_action = history.my_actions[previous_round_index]
        previous_round_opponent_actions = history.opponent_actions[previous_round_index, :]
        num_cooperators_in_prev_round = int(previous_round_my_action) + np.sum(previous_round_opponent_actions)
        if num_cooperators_in_prev_round >= m_threshold:
            return Action.D
        else:
            cooperators_needed_for_threshold = m_threshold - num_cooperators_in_prev_round
            if cooperators_needed_for_threshold == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_263(BaseStrategy):
    """
    The Adaptive Grudging Contributor (AGC) strategy:
    Aggressively free-rides by defecting when the collective threshold is met or when its
    cooperation is unlikely to make a difference. It grudgingly contributes only when there's a
    clear pattern of collective failure and its cooperation is precisely what's needed to
    secure the community reward 'k'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.my_current_contribution_streak: int = 0
        self.PATIENCE_THRESHOLD: int = 2
        self.CONTRIBUTION_STREAK_LENGTH: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m_threshold = self.game_description.m
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.consecutive_failures = 0
            self.my_current_contribution_streak = 0
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        n_c_prev_opponents = np.sum(history.opponent_actions[-1, :])
        n_c_prev_my_action = int(history.my_actions[-1])
        n_c_prev = n_c_prev_opponents + n_c_prev_my_action
        if n_c_prev >= m_threshold:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.my_current_contribution_streak > 0:
            self.my_current_contribution_streak -= 1
            return Action.C
        elif n_c_prev >= m_threshold:
            return Action.D
        elif self.consecutive_failures < self.PATIENCE_THRESHOLD:
            return Action.D
        elif n_c_prev + 1 >= m_threshold:
            self.my_current_contribution_streak = self.CONTRIBUTION_STREAK_LENGTH - 1
            self.consecutive_failures = 0
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_264(BaseStrategy):
    """
    Adaptive Opportunist with Collective Safeguard strategy for the Collective Risk Dilemma.
    This strategy aims to free-ride when possible but cooperates to ensure the collective
    reward 'k' is met when it's at risk. It also includes a mechanism to cut losses if
    cooperation proves consistently futile.
    
    Core Philosophy:
    - Round 1: Always Cooperate to signal and gather data.
    - Intermediate Rounds:
      - If 'k' was met previously: Defect to free-ride.
      - If 'k' was missed previously:
        - Cooperate for a few rounds (up to DEFECTION_STREAK_THRESHOLD) to try and restore 'k'.
        - If 'k' is missed for too many consecutive rounds, Defect to cut losses.
    - Final Round: Always Defect due to no future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_rounds_k_missed: int = 0
        self.defection_streak_threshold: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_total_cooperators = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        k_was_met_prev_round = prev_total_cooperators >= self.game_description.m
        if k_was_met_prev_round:
            self.consecutive_rounds_k_missed = 0
            return Action.D
        else:
            self.consecutive_rounds_k_missed += 1
            if self.consecutive_rounds_k_missed >= self.defection_streak_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_265(BaseStrategy):
    """
    The Calculated Opportunist strategy navigates the Collective Risk Dilemma by prioritizing
    self-interest. It defaults to defection to maximize private gain, relentlessly free-rides
    when others meet the cooperation threshold, and only contributes (cooperates) if its
    contribution is perceived as pivotal to achieving the threshold and unlocking the public
    good 'k' for itself. This strategy defects unconditionally in the first and final rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_t == 1:
            return Action.D
        if current_round_t == total_rounds_r:
            return Action.D
        my_action_last_round = int(history.my_actions[-1])
        opponent_cooperators_last_round = sum(history.opponent_actions[-1, :])
        total_cooperators_last_round = my_action_last_round + opponent_cooperators_last_round
        if total_cooperators_last_round >= min_cooperators_m:
            return Action.D
        else:
            expected_total_cooperators_if_i_cooperate = total_cooperators_last_round + 1
            if expected_total_cooperators_if_i_cooperate >= min_cooperators_m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_266(BaseStrategy):
    """
    The "Calculated Freerider" strategy for the Collective Risk Dilemma.

    This strategy is designed to be exploitative, adaptive, and robust.
    It prioritizes maximizing its own payoff by leveraging the game's structure,
    free-riding whenever possible, and only contributing when doing so offers a
    clear individual advantage.

    Core Principles:
    1.  Exploit Success: If the collective threshold (`m`) is met by other players,
        always defect to maximize individual gain (`1+k`).
    2.  Minimize Loss from Failure: If the threshold is not met, prefer defecting (`1`)
        over cooperating (`0`) if cooperation is futile.
    3.  Strategic Contribution: Only cooperate if its contribution is *decisive*
        in meeting the `m` threshold *and* the resulting reward (`k`) is
        individually superior to defecting (`1`).
    4.  End-Game Defection: Always defect in the final round to avoid any potential
        for altruistic loss.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        c_prev_opponents = sum(history.opponent_actions[-1, :])
        c_prev_my_action = 1 if history.my_actions[-1] else 0
        C_prev = c_prev_opponents + c_prev_my_action
        if C_prev >= m:
            return Action.D
        elif C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_267(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its individual payoff by leveraging others'
    potential for cooperation, free-riding whenever possible, and only contributing when it is strictly
    individually rational and pivotal. It is an adaptive strategy that responds to the collective outcome
    of past rounds, without assuming any specific opponent behavior or shared norms.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_action_prev_was_cooperation = history.my_actions[prev_round_idx]
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        N_C_prev = opponent_cooperators_prev + (1 if my_action_prev_was_cooperation else 0)
        if N_C_prev >= self.m:
            return Action.D
        elif N_C_prev == self.m - 1:
            if self.k > 2:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_268(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly exploitative, primarily seeking to free-ride
    on the cooperation of others. It strategically contributes only when its cooperation
    is pivotal to achieve the collective reward 'k', or when it perceives a high enough
    chance that its contribution, combined with others, will help secure 'k'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Opportunist strategy with game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          n_players, n_rounds, m, and k.
        """
        self.game_description = game_description
        self.N_C_history: list[int] = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Contains the current round number (0-indexed).
            history (None | PlayerHistory): Contains actions and payoffs from
                                            previous rounds for this player and opponents.
                                            Will be None for the very first round (round_number = 0).

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed > 0 and history is not None:
            prev_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            prev_round_my_action = history.my_actions[current_round_0_indexed - 1]
            num_cooperators_prev_round = int(prev_round_my_action) + np.sum(prev_round_opponent_actions.astype(int))
            self.N_C_history.append(num_cooperators_prev_round)
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == r - 1:
            return Action.D
        else:
            N_C_prev = self.N_C_history[-1]
            if N_C_prev >= m:
                return Action.D
            elif N_C_prev + 1 >= m:
                return Action.C
            else:
                min_N_C_for_push = max(0, m - 2)
                cooperate_based_on_prev = N_C_prev >= min_N_C_for_push
                cooperate_based_on_avg = False
                num_recent_rounds_for_avg = min(3, len(self.N_C_history))
                if num_recent_rounds_for_avg > 0:
                    avg_N_C_recent = sum(self.N_C_history[-num_recent_rounds_for_avg:]) / num_recent_rounds_for_avg
                    cooperate_based_on_avg = avg_N_C_recent >= min_N_C_for_push
                if cooperate_based_on_prev or cooperate_based_on_avg:
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_269(BaseStrategy):
    """
    The "Adaptive Parasite" strategy aims to maximize its own payoff by free-riding on others'
    cooperation when possible, while being adaptive to opponent behaviors. It contributes to
    the community project (plays C) only if its contribution is crucial for unlocking the
    collective reward 'k' (given k > 1).

    Decision Rules:
    1. Last Round: Always Defect (D), due to no future consequences.
    2. First Round: Initiate Cooperation (C), to potentially kickstart a cooperative environment
       that can later be exploited.
    3. Intermediate Rounds: Adaptively exploit previous round's outcome:
       - If collective threshold 'm' WAS met previously:
         - If 'm' would still have been met without this player's cooperation: Defect (D) to free-ride (get 1+k).
         - Else (this player's cooperation was essential): Cooperate (C) to maintain 'k' (since k > 1).
       - If collective threshold 'm' was NOT met previously:
         - If this player's cooperation alone would meet 'm': Cooperate (C) to achieve 'k' (since k > 1).
         - Else (this player's cooperation is insufficient): Defect (D) to minimize loss (get 1 instead of 0).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.n_rounds:
            return Action.D
        if current_round_t == 1:
            return Action.C
        else:
            prev_round_idx = state.round_number - 1
            N_C_if_i_defected_prev = np.sum(history.opponent_actions[prev_round_idx, :])
            my_action_prev_was_C = history.my_actions[prev_round_idx]
            N_C_overall_prev = N_C_if_i_defected_prev + (1 if my_action_prev_was_C else 0)
            if N_C_overall_prev >= self.m:
                if N_C_if_i_defected_prev >= self.m:
                    return Action.D
                else:
                    return Action.C
            elif N_C_overall_prev + 1 >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_270(BaseStrategy):
    """
    Adaptive Free-Rider with Conditional Reciprocity for the Collective Risk Dilemma.
    This strategy primarily defects to free-ride but cooperates as a last resort
    to "prime the pump" if the collective bonus 'k' is consistently missed.
    It reverts to defecting once the bonus is achieved and always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._cooperation_prime_threshold: int = 2
        self._rounds_failed_to_meet_threshold: int = 0
        self._my_previous_action: Action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        collective_bonus_achieved_prev: bool = False
        if state.round_number > 0:
            last_round_idx = state.round_number - 1
            self._my_previous_action = Action.C if history.my_actions[last_round_idx] else Action.D
            cooperators_in_prev_round = np.sum(history.opponent_actions[last_round_idx, :]) + int(history.my_actions[last_round_idx])
            collective_bonus_achieved_prev = cooperators_in_prev_round >= self.game_description.m
            if collective_bonus_achieved_prev:
                self._rounds_failed_to_meet_threshold = 0
            else:
                self._rounds_failed_to_meet_threshold += 1
        if current_round_1_indexed == total_rounds_1_indexed:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        if collective_bonus_achieved_prev:
            return Action.D
        elif self._rounds_failed_to_meet_threshold >= self._cooperation_prime_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_271(BaseStrategy):
    """
    The "Adaptive Threshold Exploiter" strategy is designed to maximize individual payoff in the 
    Collective Risk Dilemma by strategically contributing only when necessary to trigger 
    the collective reward, and free-riding or minimizing losses otherwise. It leverages 
    the game's repeated nature and perfect information to adapt to opponent behavior.

    - In Round 1, it cooperates to encourage a baseline of cooperation.
    - In subsequent rounds, it predicts the number of cooperators among other players
      based on historical data.
    - If enough others are predicted to cooperate (>= m), it defects to free-ride and gain 1+k.
    - If it is the critical "swing vote" (others = m-1), it cooperates to ensure the threshold 
      is met and gain k (since k > 1, this is better than 1).
    - If cooperation is predicted to be futile (others < m-1), it defects to minimize losses and gain 1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed == 0:
            return Action.C
        num_past_rounds = current_round_number_0_indexed
        sum_others_C = np.sum(history.opponent_actions[:num_past_rounds, :])
        predicted_N_C_others_avg = sum_others_C / num_past_rounds
        predicted_N_C_others_rounded = round(predicted_N_C_others_avg)
        predicted_N_C_others_rounded = max(0, min(predicted_N_C_others_rounded, n - 1))
        if predicted_N_C_others_rounded >= m:
            return Action.D
        elif predicted_N_C_others_rounded == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_272(BaseStrategy):
    """
    Adaptive Exploiter Strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by free-riding on the cooperation
    of others. It only cooperates when its contribution is absolutely necessary
    and individually beneficial to unlock the 'k' reward, otherwise it defects.

    Decision rules:
    - In the first round, it defects to probe the environment.
    - In the last round, it defects as there are no future consequences.
    - In intermediate rounds, it observes the number of cooperators from the
      immediately preceding round:
        - If the cooperation threshold 'm' was met, it defects (free-rides).
        - If the threshold was *not* met, but its cooperation would *exactly*
          reach the threshold (i.e., it was 'm-1' cooperators short), it cooperates,
          as 'k > 1' makes this individually rational.
        - Otherwise (more than one cooperator short), it defects to avoid the
          "sucker's payoff".
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t == self.r:
            return Action.D
        num_cooperators_prev = history.my_actions[state.round_number - 1] + history.opponent_actions[state.round_number - 1, :].sum()
        if num_cooperators_prev >= self.m:
            return Action.D
        elif num_cooperators_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_273(BaseStrategy):
    """
    Adaptive Free-Rider with Calculated Intervention strategy for Collective Risk Dilemma.
    This strategy prioritizes free-riding by defaulting to defecting. It strategically
    cooperates only when exactly `m-1` cooperators were observed in the previous round,
    making its own cooperation crucial to meet the `m` threshold and secure the `k` bonus.
    It defects in the first and last rounds to maximize immediate individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_cooperation_prev_round_value = int(history.my_actions[-1])
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_cooperation_prev_round_value
        if total_cooperators_prev_round >= m_threshold:
            return Action.D
        elif total_cooperators_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_274(BaseStrategy):
    """
    Adaptive Free-Rider with Minimal Critical Contribution strategy for the Collective Risk Dilemma.
    This strategy aims to free-ride on others' cooperation whenever the public good threshold (`m`) is met.
    When the threshold is not met, it will only cooperate if its contribution is critical and sufficient
    to push the group over the `m` threshold, thereby securing the reward `k` for itself. Otherwise, it defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.D
        previous_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
        previous_round_my_action_was_C = history.my_actions[-1]
        previous_N_C = previous_round_opponent_cooperators + (1 if previous_round_my_action_was_C else 0)
        if previous_N_C >= m_threshold:
            return Action.D
        elif previous_N_C + 1 >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_275(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by free-riding on the cooperation
    of others whenever possible. It defaults to defection (D) but strategically
    cooperates (C) only when its contribution is precisely what's needed to meet
    the collective 'm' threshold, thereby unlocking the 'k' reward for itself,
    given that k > 1 makes this a self-beneficial action. It also defects in the
    first and last rounds to exploit the absence of past history or future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + int(history.my_actions[-1])
        if num_cooperators_prev_round >= m:
            return Action.D
        elif num_cooperators_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_276(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own payoff by free-riding on others' cooperation.
    It strategically cooperates only when necessary to ensure the collective reward 'k' is achieved (or to induce
    others to cooperate if 'k' was not met previously), and defects otherwise, especially in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_previous_contribution = 1 if history.my_actions[state.round_number - 1] else 0
        opponent_previous_cooperators_count = np.sum(history.opponent_actions[state.round_number - 1, :])
        total_previous_cooperators = my_previous_contribution + opponent_previous_cooperators_count
        if total_previous_cooperators >= self.game_description.m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_277(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy is designed to maximize individual payoff by free-riding
    on the cooperation of others. It only cooperates when strategically necessary to either
    ensure the community project's success (and thus the bonus k) or to prevent a sustained
    collapse that would reduce overall payoffs.

    Decision Rules:
    1. First Round (t=1): Defect (D) to attempt free-riding and observe others.
    2. Last Round (t=r): Defect (D) as there are no future interactions to influence.
    3. Intermediate Rounds (1 < t < r):
       - If the community project succeeded in the previous round (C_{t-1} >= m): Defect (D)
         to continue free-riding.
       - If the community project failed in the previous round (C_{t-1} < m):
         - If C_{t-1} == m-1 (near-miss): Cooperate (C) to secure the 'k' bonus.
         - If C_{t-1} < m-1 (sustained failure):
           - If there have been 2 or more consecutive failures: Cooperate (C) to try and
             "pump" the system back to success.
           - Otherwise: Defect (D), as it's not a near-miss and the failure streak isn't
             long enough to warrant intervention.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_my_action_cooperated = history.my_actions[-1]
        prev_opponent_cooperators_count = np.sum(history.opponent_actions[-1, :])
        prev_total_cooperators = int(prev_my_action_cooperated) + prev_opponent_cooperators_count
        if prev_total_cooperators >= m:
            return Action.D
        elif prev_total_cooperators == m - 1:
            return Action.C
        else:
            consecutive_failures = 0
            for r_idx in range(state.round_number - 1, -1, -1):
                round_my_action_cooperated = history.my_actions[r_idx]
                round_opponent_cooperators_count = np.sum(history.opponent_actions[r_idx, :])
                round_total_cooperators = int(round_my_action_cooperated) + round_opponent_cooperators_count
                if round_total_cooperators < m:
                    consecutive_failures += 1
                else:
                    break
            if consecutive_failures >= 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_278(BaseStrategy):
    """
    The "Adaptive Free Rider" strategy for the Collective Risk Dilemma.

    This strategy leverages the repeated nature of the game and perfect information
    to react to the collective behavior of other players. It aims to maximize
    individual payoff by free-riding whenever possible and cooperating only
    when doing so yields a direct, strictly higher personal benefit.

    Decision Rules:
    - Round 1 (state.round_number == 0): Defect (D) to assess group behavior without cost.
    - Subsequent Rounds (observing C_prev = total cooperators in previous round):
        - If C_prev < m - 1: Defect (D). Group is far from threshold; cooperation is futile.
        - If C_prev == m - 1: Cooperate (C). My cooperation is pivotal to meet 'm', yielding 'k' (since k > 1).
        - If C_prev >= m: Defect (D). Threshold already met; free-ride for '1+k' (vs 'k' for cooperating).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_was_cooperate = history.my_actions[-1]
        num_cooperators_prev = num_opponent_cooperators_prev + (1 if my_action_prev_was_cooperate else 0)
        if num_cooperators_prev < self.m - 1:
            return Action.D
        elif num_cooperators_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_279(BaseStrategy):
    """
    This strategy prioritizes free-riding. It will only cooperate under very specific,
    critical conditions where its contribution directly triggers the communal reward (k),
    which then benefits the player more than simply defecting. Otherwise, it defaults to defecting.

    Decision Rules:
    1. In the first round (t=1), defects to free-ride and test the waters.
    2. In the last round (t=r), defects due to the unraveling effect, maximizing immediate payoff.
    3. In intermediate rounds (1 < t < r):
        a. If the 'm' threshold was met in the previous round (C_{t-1} >= m), defects to free-ride and gain 1+k.
        b. If the 'm' threshold was NOT met in the previous round (C_{t-1} < m):
           i. If exactly one more cooperator is needed (C_{t-1} == m-1), cooperates to activate 'k' and receive a payoff of 'k' (since k > 1, this is better than 1 from defecting without 'k').
           ii. If more than one cooperator is needed (C_{t-1} < m-1), defects to avoid the cost of cooperation if it won't single-handedly activate 'k', ensuring a payoff of 1 instead of 0.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_round_cooperators >= m_threshold:
            return Action.D
        elif prev_round_cooperators == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_280(BaseStrategy):
    """
    The "Adaptive Trigger-Punisher" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by free-riding on others' cooperation.
    It defects in the first and final rounds. In intermediate rounds, it adapts its
    behavior based on the previous round's outcome:
    - If the cooperation threshold was met, it defects to free-ride.
    - If the threshold was not met:
        - If it cooperated in the previous round, it defects (punishment/withdrawal).
        - If it defected in the previous round, it cooperates (to trigger/reset cooperation).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        prev_round_index = current_round_0_indexed - 1
        my_action_prev_round = history.my_actions[prev_round_index]
        opponent_actions_prev_round = history.opponent_actions[prev_round_index, :]
        num_cooperators_prev_round = my_action_prev_round + np.sum(opponent_actions_prev_round)
        if num_cooperators_prev_round >= min_cooperators_needed:
            return Action.D
        elif my_action_prev_round == True:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_281(BaseStrategy):
    """
    Adaptive Free-Rider with Minimal Investment (AFRMI) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual gain by prioritizing free-riding (Defect).
    It adapts by temporarily cooperating to re-establish the collective threshold if it's
    consistently missed, but only as a calculated investment, immediately reverting to
    defecting if the trigger fails or if the threshold is met again. The strategy
    also employs unconditional defection during initial observation rounds and in the
    final rounds of the game to exploit end-game conditions.

    Core Principles:
    1. Prioritize Free-Riding: Default to Defect (D) if the cooperation threshold was met.
    2. Minimal Investment: Cooperate (C) only to trigger cooperation if the threshold
       has been missed for `TRIGGER_COOPERATION_THRESHOLD` consecutive rounds.
    3. No Sucker Play: If a cooperative trigger attempt fails, revert to Defect (D).
    4. End Game Defection: Defect in the final `DEFECT_BEFORE_END_ROUNDS` rounds.
    5. Initial Exploration: Defect in the first `EXPLORATION_ROUNDS` to observe opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploration_rounds: int = 1
        self.trigger_cooperation_threshold: int = 1
        self.defect_before_end_rounds: int = 2
        self.my_prev_action_was_C_to_trigger: bool = False
        self.consecutive_rounds_threshold_not_met: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_index = state.round_number - 1
        my_action_in_prev_round = history.my_actions[prev_round_index]
        opponents_cooperated_in_prev_round = np.sum(history.opponent_actions[prev_round_index, :])
        prev_total_cooperators = my_action_in_prev_round + opponents_cooperated_in_prev_round
        if prev_total_cooperators >= self.game_description.m:
            self.consecutive_rounds_threshold_not_met = 0
            self.my_prev_action_was_C_to_trigger = False
        else:
            self.consecutive_rounds_threshold_not_met += 1
        if state.round_number >= self.game_description.n_rounds - self.defect_before_end_rounds:
            self.my_prev_action_was_C_to_trigger = False
            return Action.D
        if state.round_number < self.exploration_rounds:
            self.my_prev_action_was_C_to_trigger = False
            return Action.D
        if prev_total_cooperators >= self.game_description.m:
            self.my_prev_action_was_C_to_trigger = False
            return Action.D
        elif self.my_prev_action_was_C_to_trigger:
            self.my_prev_action_was_C_to_trigger = False
            return Action.D
        elif self.consecutive_rounds_threshold_not_met >= self.trigger_cooperation_threshold:
            self.my_prev_action_was_C_to_trigger = True
            return Action.C
        else:
            self.my_prev_action_was_C_to_trigger = False
            return Action.D

class Strategy_EXPLOITATIVE_282(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize personal payoff by primarily free-riding on
    the cooperation of others. It only contributes when its own cooperation is
    both necessary to reach the collective reward threshold ('m') and
    individually beneficial (i.e., the 'k' bonus outweighs the cost of cooperation).
    The strategy adapts to observed group behavior and employs backward induction
    for the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        if current_round == 0:
            return Action.D
        if current_round == total_rounds - 1:
            return Action.D
        cooperators_last_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if cooperators_last_round >= m_threshold:
            return Action.D
        elif cooperators_last_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_283(BaseStrategy):
    """
    The Adaptive Opportunist with Threshold Enforcement (AOTE) strategy aims to maximize its
    own payoff in the Collective Risk Dilemma by actively probing for opportunities to free-ride,
    strategically contributing only when its contribution is critical and directly beneficial,
    and disengaging when cooperation is ineffective or unnecessary. It prioritizes individual gain
    and adapts to observed collective actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AOTE strategy with the game's parameters.

        Args:
            game_description: An object containing game parameters such as n_players,
                              n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the AOTE strategy.

        Args:
            state: An object containing the current round number.
            history: An object containing the player's and opponents' actions and payoffs
                     from previous rounds. It will be None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        previous_round_0_indexed = current_round_0_indexed - 1
        opponents_cooperated_last_round = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        my_action_last_round_bool = history.my_actions[previous_round_0_indexed]
        total_cooperated_last_round = opponents_cooperated_last_round + int(my_action_last_round_bool)
        if total_cooperated_last_round >= m_threshold:
            return Action.D
        elif my_action_last_round_bool == True:
            return Action.D
        elif total_cooperated_last_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_284(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own payoff by consistently attempting
    to free-ride on the cooperation of others. It only contributes (cooperates)
    when its contribution is critically necessary to achieve or maintain the
    collective reward 'k', or when its previous defection led to a collective
    failure that it now seeks to strategically remedy. Decisions are adaptive
    to opponent behavior, observing the number of cooperators in the previous
    round to inform the current decision, with a focus on endgame exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_idx == total_rounds - 1:
            return Action.D
        if current_round_idx == 0:
            return Action.D
        prev_round_idx = current_round_idx - 1
        my_prev_action_was_C = history.my_actions[prev_round_idx]
        prev_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx])
        prev_total_cooperators = prev_opponent_cooperators + (1 if my_prev_action_was_C else 0)
        if prev_total_cooperators >= m:
            if prev_total_cooperators >= m + 1:
                return Action.D
            elif my_prev_action_was_C:
                return Action.C
            else:
                return Action.D
        elif my_prev_action_was_C:
            return Action.D
        elif prev_total_cooperators == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_285(BaseStrategy):
    """
    The Opportunistic Pendulum strategy for the Collective Risk Dilemma.

    This strategy aims to primarily free-ride on the cooperation of others. It defects
    whenever the collective reward 'k' is secured or when its own cooperation would be
    ineffective. It strategically cooperates only when its contribution is precisely
    what's needed to unlock the collective reward, thereby creating an environment where
    it can subsequently free-ride.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        my_action_prev_round_is_C = history.my_actions[-1]
        C_prev_round = int(my_action_prev_round_is_C) + np.sum(history.opponent_actions[-1, :])
        if C_prev_round >= m_threshold:
            return Action.D
        elif my_action_prev_round_is_C:
            return Action.D
        elif C_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_286(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy operates on the principle of minimal necessary
    contribution. It aims to free-ride whenever possible by defecting, thereby securing
    a higher individual payoff. It only resorts to cooperating if its contribution is
    crucial to activate the collective reward (`k`), and doing so directly results in
    a higher individual payoff for itself than defecting would. This strategy is robust
    as it adapts to observable past outcomes without making strong assumptions about
    opponent types or coordination.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveFreeRiderStrategy with the game's parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing game parameters
                                                         (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Free-Rider strategy rules.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing historical actions and payoffs
                                            from previous rounds, or None for the first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(np.sum(history.opponent_actions[-1, :]))
        if history.my_actions[-1]:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= m_threshold:
            return Action.D
        elif num_cooperators_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_287(BaseStrategy):
    """
    The Adaptive Freeloader strategy aims to maximize its own payoff by free-riding on others' cooperation.
    It cooperates in the first round to probe the environment and encourage initial cooperation.
    In intermediate rounds, it defects if the threshold was met previously. If the threshold was not met,
    it defects if it cooperated previously (punishing failure) or if its cooperation is not critical.
    It cooperates only if its single contribution makes the threshold achievable.
    In the final round, it always defects to maximize immediate payoff, as there are no future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        my_action_previous_round_is_C = history.my_actions[-1]
        opponent_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_previous_round = opponent_cooperators_previous_round + (1 if my_action_previous_round_is_C else 0)
        if total_cooperators_previous_round >= m:
            return Action.D
        elif my_action_previous_round_is_C:
            return Action.D
        else:
            needed_for_threshold = m - total_cooperators_previous_round
            if needed_for_threshold == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_288(BaseStrategy):
    """
    Adaptive Conditional Free-Rider (ACFR) strategy for the Collective Risk Dilemma.
    Aims to maximize individual payoff by free-riding when the collective threshold is met,
    punishing collective failure by defecting, and strategically cooperating only when
    its contribution is likely to be pivotal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing the game's parameters like
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing all past actions and payoffs for the current
                     player and opponents. It is None for the first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_actions_prev = history.opponent_actions[-1, :]
        n_c_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        if n_c_prev >= m_threshold:
            return Action.D
        elif my_action_prev:
            return Action.D
        elif n_c_prev + 1 >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_289(BaseStrategy):
    """
    The "Adaptive Pivotal Free-Rider" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by opportunistically free-riding on the
    cooperation of others. It selectively contributes only when its own contribution is
    pivotal to achieving the collective reward and directly beneficial to its own outcome.
    The strategy adapts to observed cooperation levels from the immediately preceding round
    and accounts for the specific structure of repeated games, including an endgame
    exploitation phase.

    Decision Rules:
    1.  **First Round (t=1, round_number=0):** Defect (D) to probe the environment and secure at least a private payoff.
    2.  **Last Round (t=r, round_number=n_rounds-1):** Defect (D) due to backward induction; no future rounds to influence.
    3.  **Intermediate Rounds (1 < t < r):**
        *   **If total cooperators in the previous round (C_t-1) >= m:** Defect (D). Free-ride, as the threshold was met by others.
        *   **If total cooperators in the previous round (C_t-1) == m-1:** Cooperate (C). Your contribution is pivotal, and since k > 1, cooperating yields a higher personal payoff (k vs 1 from defecting).
        *   **If total cooperators in the previous round (C_t-1) < m-1:** Defect (D). Your single contribution is insufficient to meet the threshold, so prioritize private payoff (1 vs 0 from cooperating).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == n_rounds - 1:
            return Action.D
        opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_cooperation = history.my_actions[-1]
        total_cooperators_in_previous_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_was_cooperation else 0)
        if total_cooperators_in_previous_round >= m_threshold:
            return Action.D
        elif total_cooperators_in_previous_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_290(BaseStrategy):
    """
    Exploitative Threshold Manipulator (ETM) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily free-riding
    on the cooperation of others. It strategically contributes cooperation only
    when it is critically necessary to ensure the collective reward 'k' is met,
    adapting behavior based on past round outcomes and accounting for end-game effects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        opponents_cooperated_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_was_C = history.my_actions[state.round_number - 1]
        my_contribution_prev = 1 if my_action_prev_was_C else 0
        N_C_prev = opponents_cooperated_prev + my_contribution_prev
        if N_C_prev >= m_threshold:
            return Action.D
        elif N_C_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_291(BaseStrategy):
    """
    The "Adaptive Free-Rider" (AFR) strategy aims to maximize individual payoff by
    exploiting any existing cooperation. It defaults to defecting to free-ride,
    only cooperating if strictly necessary to meet the collective reward threshold 'm'
    and if that cooperation ensures a higher personal payoff (k) than defecting would.
    In situations of collective failure (threshold not met), it defects to minimize losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_action_prev_round: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        if state.round_number == 0:
            current_action = Action.D
        elif self.my_action_prev_round is None:
            current_action = Action.D
        else:
            num_opponent_cooperated_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            num_cooperated_prev_round_total = int(self.my_action_prev_round == Action.C) + num_opponent_cooperated_prev_round
            m_threshold = self.game_description.m
            k_reward = self.game_description.k
            if num_cooperated_prev_round_total >= m_threshold:
                if num_cooperated_prev_round_total > m_threshold:
                    current_action = Action.D
                elif self.my_action_prev_round == Action.D:
                    current_action = Action.D
                else:
                    current_action = Action.C
            else:
                current_action = Action.D
        self.my_action_prev_round = current_action
        return current_action

class Strategy_EXPLOITATIVE_292(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its own payoff by strategically leveraging defection
    and minimal cooperation based on observed group behavior. It prioritizes free-riding but will make
    calculated "investments" in cooperation when its contribution is critical for achieving the collective reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        cooperators_among_opponents_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
        my_action_prev_round = history.my_actions[previous_round_index]
        C_prev = cooperators_among_opponents_prev_round + int(my_action_prev_round)
        if C_prev >= m_threshold:
            return Action.D
        elif C_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_293(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy aims to maximize its own payoff in the Collective Risk Dilemma
    by opportunistically defecting when collective cooperation is stable, while also strategically
    contributing when necessary to re-establish cooperation or cut losses when it consistently fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CONSEC_SUCCESS_TO_DEFECT = 2
        self.CONSEC_FAIL_TO_COOPERATE = 2
        self.MIN_CONSEC_C_AFTER_FAIL = 1
        self.my_previous_action: Action | None = None
        self.previous_round_nc_met: bool = False
        self.consecutive_successes: int = 0
        self.consecutive_failures: int = 0
        self.rounds_cooperating_after_fail: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_py_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        chosen_action: Action
        if current_round_py_idx == 0:
            chosen_action = Action.C
            self.previous_round_nc_met = False
            self.consecutive_successes = 0
            self.consecutive_failures = 0
            self.rounds_cooperating_after_fail = 0
        elif current_round_py_idx == total_rounds - 1:
            chosen_action = Action.D
        else:
            n_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if self.my_previous_action == Action.C else 0)
            threshold_met_in_prev_round = n_cooperators_prev_round >= m_threshold
            if threshold_met_in_prev_round:
                self.consecutive_successes += 1
                self.consecutive_failures = 0
                self.rounds_cooperating_after_fail = 0
            else:
                self.consecutive_failures += 1
                self.consecutive_successes = 0
            self.previous_round_nc_met = threshold_met_in_prev_round
            if self.consecutive_successes >= self.CONSEC_SUCCESS_TO_DEFECT:
                chosen_action = Action.D
            elif self.consecutive_failures >= self.CONSEC_FAIL_TO_COOPERATE:
                if self.my_previous_action == Action.D or self.rounds_cooperating_after_fail == 0:
                    chosen_action = Action.C
                    self.rounds_cooperating_after_fail = 1
                elif self.my_previous_action == Action.C and self.rounds_cooperating_after_fail < self.MIN_CONSEC_C_AFTER_FAIL:
                    chosen_action = Action.C
                    self.rounds_cooperating_after_fail += 1
                else:
                    chosen_action = Action.D
                    self.rounds_cooperating_after_fail = 0
            else:
                chosen_action = Action.C
        self.my_previous_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_294(BaseStrategy):
    """
    Adaptive Pivot-and-Free-Ride strategy for the Collective Risk Dilemma.

    This strategy is designed to be highly self-interested and exploitative.
    It primarily aims to free-ride, securing the highest possible payoff of (1+k)
    by defecting when 'm' or more players cooperate. It will only contribute (cooperate)
    under specific, calculated conditions: an initial probe, or when its own cooperation
    is absolutely essential to achieve the collective reward 'k' (pivotal contribution)
    and doing so yields a higher payoff than defecting without 'k'.
    It avoids futile cooperation and always defects in the final round to maximize
    immediate payoff without future repercussions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: A dataclass containing the game's parameters
                              (n_players, n_rounds, m, k).
        """
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the
        Adaptive Pivot-and-Free-Ride strategy's rules.

        Args:
            state: A dataclass containing the current round number (0-indexed).
            history: A dataclass containing records of past actions and payoffs
                     for the current player and opponents. This will be None for the first round (round 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        c_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if c_prev_round >= self.m_threshold:
            return Action.D
        elif c_prev_round == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_295(BaseStrategy):
    """
    The Strategic Free-Rider (SFR) strategy aims to maximize its own total payoff by free-riding whenever possible,
    while strategically cooperating only when its contribution is essential to achieve the collective reward (`k`)
    and doing so offers a direct individual benefit (as k > 1 ensures k is better than 1).
    It adapts by observing the collective cooperation level of other players in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        my_action_prev_round_val = int(history.my_actions[-1])
        C_others_prev = sum(history.opponent_actions[-1, :])
        if C_others_prev >= self.m_threshold:
            return Action.D
        elif C_others_prev == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_296(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to secure the collective reward 'k' with minimum personal contribution.
    It leverages the observed total cooperation level from the previous round to predict the
    current round's outcome and makes a self-serving decision. It always defects in the
    first and terminal rounds, as there are no past observations or future consequences to
    influence, respectively. In intermediate rounds, it cooperates only when its contribution
    is decisive for reaching the 'm' threshold and securing the 'k' reward (which is assumed
    to be individually profitable, i.e., k > 1 as per game parameters). Otherwise, it defects
    to free-ride or avoid futile contributions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = int(history.my_actions[state.round_number - 1]) + np.sum(history.opponent_actions[state.round_number - 1, :])
            if num_cooperators_prev_round >= m:
                return Action.D
            elif num_cooperators_prev_round == m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_297(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the Collective Risk Dilemma.
    Prioritizes individual payoff maximization by defaulting to defection,
    while intelligently adapting to leverage or induce collective cooperation
    when it benefits the agent, acting as a strategic investment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        my_current_action: Action = Action.D
        if current_round_idx == 0:
            last_round_total_cooperators = 0
        else:
            last_round_total_cooperators = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if current_round_idx == n_rounds - 1:
            my_current_action = Action.D
        else:
            if current_round_idx > 0:
                if last_round_total_cooperators >= m:
                    self.consecutive_failures = 0
                else:
                    self.consecutive_failures += 1
            if last_round_total_cooperators >= m:
                my_current_action = Action.D
            elif self.my_last_action == Action.C:
                my_current_action = Action.D
            elif self.consecutive_failures >= 3:
                my_current_action = Action.C
            else:
                my_current_action = Action.D
        self.my_last_action = my_current_action
        return my_current_action

class Strategy_EXPLOITATIVE_298(BaseStrategy):
    """
    Adaptive Free-Rider with Pivotal Contribution (AFPC) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize its own payoff by free-riding on others' cooperation whenever
    possible and only contributing when its cooperation is pivotal to meet the collective
    threshold 'm' and doing so is personally beneficial (due to k > 1). It also exploits
    the first and last rounds by defecting due to the lack of historical data or future
    consequences, respectively.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_number == 0:
            return Action.D
        if current_round_number == total_rounds - 1:
            return Action.D
        cooperators_among_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        if cooperators_among_opponents_prev_round >= min_cooperators_needed:
            return Action.D
        elif cooperators_among_opponents_prev_round == min_cooperators_needed - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_299(BaseStrategy):
    """
    The "Adaptive Freerider with Crisis Aversion" strategy for the Collective Risk Dilemma.
    This strategy prioritizes individual gain by defaulting to defection (exploiting) unless
    specific conditions indicate that cooperation is necessary to either ensure the collective
    reward 'k' or prevent a systemic breakdown. It aims to free-ride as much as possible,
    but also acts to prevent a complete collapse of cooperation that would deny everyone,
    including itself, the 'k' reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_as_int = 1 if history.my_actions[-1] else 0
        last_round_cooperators = num_opponent_cooperators_prev + my_action_prev_as_int
        if current_round_1_indexed == self.n_rounds:
            if last_round_cooperators >= self.m:
                return Action.D
            elif last_round_cooperators == self.m - 1:
                return Action.C
            else:
                return Action.D
        elif last_round_cooperators > self.m:
            return Action.D
        elif last_round_cooperators == self.m:
            return Action.C
        elif last_round_cooperators == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_300(BaseStrategy):
    """
    Predictive Free-Rider (PFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding on the cooperation
    of others whenever possible. It uses a simple prediction model (repeating the
    previous round's action) to decide whether its cooperation is critical to meet
    the collective threshold `m`, whether it can safely defect, or whether cooperation
    would be futile. It strategically cooperates in Round 1 to incentivize future
    collective rewards and defects unconditionally in the final round for pure
    exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        predicted_cooperators_from_others = sum(previous_round_opponent_actions)
        if predicted_cooperators_from_others >= m:
            return Action.D
        elif predicted_cooperators_from_others == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_301(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to consistently achieve the 1 + k payoff by
    free-riding on others' cooperation. It probes by defecting initially. If the threshold `m`
    is met, it continues to defect to free-ride. If the threshold is not met, it temporarily
    cooperates as a "strategic investment" only if there's recent evidence that the group
    *can* cooperate (within the last 3 rounds). If the group consistently fails to meet `m`,
    it defects permanently to cut losses. In the final round, it always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m

        def get_total_cooperators_for_past_round(round_idx_0_indexed: int) -> int:
            if history is None or round_idx_0_indexed < 0:
                return 0
            opponent_cooperators = np.sum(history.opponent_actions[round_idx_0_indexed, :])
            my_cooperation = 1 if history.my_actions[round_idx_0_indexed] else 0
            return opponent_cooperators + my_cooperation
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        cooperators_prev_round = get_total_cooperators_for_past_round(current_round_0_indexed - 1)
        if cooperators_prev_round >= m_threshold:
            return Action.D
        else:
            W_effective = min(current_round_0_indexed, 3)
            recent_success = False
            for j in range(1, W_effective + 1):
                past_round_idx = current_round_0_indexed - j
                if get_total_cooperators_for_past_round(past_round_idx) >= m_threshold:
                    recent_success = True
                    break
            if recent_success:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_302(BaseStrategy):
    """
    The Adaptive Demander with Futility Check strategy aims for exploitative free-riding.
    It adapts its demand for cooperation from others based on past success or failure
    to meet the collective threshold. It only cooperates when deemed necessary to secure
    the collective reward 'k', and it includes a futility check to prevent continuous
    self-sacrificial cooperation if the group consistently fails to meet the threshold.
    It defects in the first and last rounds to maximize individual gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.current_demand_from_others: int = self.m
        self.my_last_action: Action = Action.D
        self.consecutive_failures_despite_cooperating: int = 0
        self.FUTILITY_THRESHOLD: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_action: Action
        if state.round_number == 0:
            current_round_action = Action.D
        elif state.round_number == self.r - 1:
            current_round_action = Action.D
        else:
            prev_round_idx = state.round_number - 1
            C_hist_t_minus_1 = np.sum(history.opponent_actions[prev_round_idx, :]) + (1 if history.my_actions[prev_round_idx] else 0)
            prev_my_action_was_C = history.my_actions[prev_round_idx]
            others_cooperated_t_minus_1 = C_hist_t_minus_1 - (1 if prev_my_action_was_C else 0)
            if C_hist_t_minus_1 < self.m:
                if prev_my_action_was_C:
                    self.consecutive_failures_despite_cooperating += 1
                else:
                    self.consecutive_failures_despite_cooperating = 0
            else:
                self.consecutive_failures_despite_cooperating = 0
            if self.consecutive_failures_despite_cooperating >= self.FUTILITY_THRESHOLD:
                current_round_action = Action.D
                self.consecutive_failures_despite_cooperating = 0
            else:
                if C_hist_t_minus_1 >= self.m:
                    self.current_demand_from_others = max(0, self.current_demand_from_others - 1)
                else:
                    self.current_demand_from_others = min(self.n - 1, self.current_demand_from_others + 1)
                if others_cooperated_t_minus_1 >= self.current_demand_from_others:
                    current_round_action = Action.D
                else:
                    current_round_action = Action.C
        self.my_last_action = current_round_action
        return current_round_action

class Strategy_EXPLOITATIVE_303(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma, maximizing individual payoff.
    It defaults to free-riding (defecting) and only cooperates if its contribution is
    pivotal to reaching the 'm' cooperator threshold and securing the 'k' bonus.
    It exploits the first and last rounds by always defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players,
                              n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the strategy.

        Args:
            state: The current state of the game, including the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for this player
                     and opponents, or None if it's the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_0_indexed - 1
        my_previous_action_cooperated = history.my_actions[previous_round_idx]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        total_cooperators_prev_round = int(my_previous_action_cooperated) + num_opponent_cooperators_prev_round
        if total_cooperators_prev_round >= self.m:
            return Action.D
        elif total_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_304(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy is designed to maximize individual payoff
    in the Collective Risk Dilemma by exploiting others' willingness to cooperate,
    while also strategically contributing when necessary to unlock the collective reward,
    and conserving resources when cooperation is futile. It is adaptive to the observed
    collective behavior and robust against various opponent types due to its focus
    on outcomes rather than complex opponent modeling.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        n_opponents_cooperated_last_round = np.sum(history.opponent_actions[-1, :])
        my_action_last_round_was_cooperate = history.my_actions[-1]
        n_cooperators_last_round = n_opponents_cooperated_last_round + int(my_action_last_round_was_cooperate)
        if n_cooperators_last_round >= self.m_threshold:
            return Action.D
        elif n_cooperators_last_round == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_305(BaseStrategy):
    """
    The Adaptive Threshold-Maintainer (ATM) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by free-riding on others' cooperation
    whenever possible. It will only contribute (Cooperate) if its contribution is absolutely
    critical to meet the collective threshold `m`, and if doing so yields a higher individual
    payoff than defecting. It adapts to the observed level of cooperation from other players
    in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        num_cooperators_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if num_cooperators_others_prev_round >= m:
            return Action.D
        elif num_cooperators_others_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_306(BaseStrategy):
    """
    Adaptive Free-Rider with Pivotal Contribution (AFRPC) strategy for the Collective Risk Dilemma.
    This strategy is designed to exploit the game mechanics by free-riding when possible,
    contributing only when its contribution is pivotal to secure the collective reward,
    and cutting losses when collective cooperation is too low.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_prev_action_was_C = history.my_actions[prev_round_idx]
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        n_c_prev_round = opponent_prev_cooperators + (1 if my_prev_action_was_C else 0)
        if n_c_prev_round >= min_cooperators_m:
            return Action.D
        elif n_c_prev_round == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_307(BaseStrategy):
    """
    The Reactive Free-Rider strategy exploits the Collective Risk Dilemma by defaulting to defection.
    It strategically cooperates only when the collective reward 'k' is at risk of not being met,
    thereby preserving the opportunity for future exploitation and maximizing individual payoff.
    It adapts based on the previous round's cooperation level, with specific logic for the first
    and final rounds to account for absence of history and end-game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        my_cooperation_prev_round = int(history.my_actions[-1])
        opponent_cooperation_prev_round = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = my_cooperation_prev_round + opponent_cooperation_prev_round
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators_prev_round >= self.game_description.m:
                return Action.D
            elif num_cooperators_prev_round == self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        elif num_cooperators_prev_round >= self.game_description.m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_309(BaseStrategy):
    """
    Adaptive Bait-and-Switch Free Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding on others'
    cooperation. It strategically defects when the community project is successful.
    If cooperation falters, it will attempt to "bait" others into cooperating
    by contributing for a limited number of rounds after its own defection,
    before reverting to permanent defection if the collective failure persists.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.consecutive_failed_rounds: int = 0
        self.N_BAIT_ATTEMPTS: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        action_for_this_round: Action
        if current_round == 0:
            action_for_this_round = Action.D
            self.my_last_action = action_for_this_round
            return action_for_this_round
        if current_round == total_rounds - 1:
            action_for_this_round = Action.D
            self.my_last_action = action_for_this_round
            return action_for_this_round
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if previous_round_cooperators >= min_cooperators_m:
            self.consecutive_failed_rounds = 0
        else:
            self.consecutive_failed_rounds += 1
        if previous_round_cooperators >= min_cooperators_m:
            action_for_this_round = Action.D
        elif self.my_last_action == Action.C:
            action_for_this_round = Action.D
        elif self.consecutive_failed_rounds <= self.N_BAIT_ATTEMPTS:
            action_for_this_round = Action.C
        else:
            action_for_this_round = Action.D
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_310(BaseStrategy):
    """
    Adaptive Free-Rider with Failure Tolerance: This strategy is highly exploitative, 
    adaptive, and robust, aiming to maximize personal payoff by free-riding as much 
    as possible. It only cooperates when necessary to secure the collective reward 'k', 
    or to prevent the system from completely collapsing. It employs initial exploration 
    to probe opponent behavior, a failure tolerance mechanism to avoid system collapse, 
    and strategic defection to maximize individual gain and test system resilience.
    """
    EXPLORATION_ROUNDS: int = 1
    FAILURE_TOLERANCE: int = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_action_history: NDArray[np.bool_] = np.full(self.game_description.n_rounds, False, dtype=bool)
        self.num_other_cooperators_history: NDArray[np.int_] = np.zeros(self.game_description.n_rounds, dtype=int)
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round > 0:
            prev_round_idx = current_round - 1
            observed_other_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            self.num_other_cooperators_history[prev_round_idx] = observed_other_cooperators_prev_round
        my_current_action: Action
        if current_round < self.EXPLORATION_ROUNDS:
            my_current_action = Action.D
        elif current_round == self.game_description.n_rounds - 1:
            my_current_action = Action.D
        else:
            prev_round_idx = current_round - 1
            other_coop_prev = self.num_other_cooperators_history[prev_round_idx]
            my_prev_action_is_C = self.my_action_history[prev_round_idx]
            total_coop_prev = other_coop_prev + (1 if my_prev_action_is_C else 0)
            if total_coop_prev >= self.game_description.m:
                self.consecutive_failures = 0
            else:
                self.consecutive_failures += 1
            if self.consecutive_failures >= self.FAILURE_TOLERANCE:
                my_current_action = Action.C
            elif total_coop_prev >= self.game_description.m:
                if other_coop_prev >= self.game_description.m:
                    my_current_action = Action.D
                else:
                    my_current_action = Action.D
            elif other_coop_prev + 1 >= self.game_description.m:
                my_current_action = Action.C
            else:
                my_current_action = Action.D
        self.my_action_history[current_round] = my_current_action == Action.C
        return my_current_action

class Strategy_EXPLOITATIVE_311(BaseStrategy):
    """
    The Conditional Free-Rider strategy aims to maximize its own payoff by free-riding on others' contributions.
    It defects in the first and last rounds to exploit the lack of future influence.
    In intermediate rounds, it defects if the cooperation threshold was met in the previous round,
    thus benefiting from the collective reward 'k' without contributing. If the threshold was not met,
    it temporarily cooperates to "nudge" the system into meeting the threshold, thereby re-enabling
    future free-riding opportunities for itself, then reverts to defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == total_rounds - 1:
            return Action.D
        previous_round_opponent_cooperators = history.opponent_actions[state.round_number - 1].sum()
        previous_round_my_action_val = 1 if history.my_actions[state.round_number - 1] else 0
        total_cooperators_previous_round = previous_round_opponent_cooperators + previous_round_my_action_val
        if total_cooperators_previous_round >= min_cooperators_needed:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_312(BaseStrategy):
    """
    Adaptive Opportunist with Collective Punishment strategy for the Collective Risk Dilemma.

    This strategy aims to secure the collective reward 'k' as frequently as possible while
    minimizing its own contribution. It prioritizes free-riding but will strategically cooperate
    when its contribution is pivotal to achieving the 'm' threshold, provided the collective
    hasn't repeatedly failed in the past. If the group consistently fails to meet the 'm' threshold,
    the strategy will disengage from contributing, signaling a demand for others to step up
    or forgoing the 'k' reward entirely.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description (CollectiveRiskDescription): Description of the game's parameters.
        """
        self.game_description = game_description
        self.failure_tolerance_threshold: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action ('C' for Cooperate, 'D' for Defect) for the current player
        in the Collective Risk Dilemma.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): A historical record of actions and payoffs.
                                           None for the very first round (round_number 0).

        Returns:
            Action: 'C' for Cooperate, or 'D' for Defect.
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        my_action_prev_val = 1 if history.my_actions[-1] else 0
        other_c_prev = np.sum(history.opponent_actions[-1, :])
        consecutive_failures = 0
        for r_idx in range(state.round_number - 1, -1, -1):
            round_my_action_val = 1 if history.my_actions[r_idx] else 0
            round_other_c = np.sum(history.opponent_actions[r_idx, :])
            round_total_c = round_my_action_val + round_other_c
            if round_total_c < m:
                consecutive_failures += 1
            else:
                break
        action = Action.D
        if state.round_number == r - 1:
            if other_c_prev >= m:
                action = Action.D
            elif other_c_prev == m - 1:
                action = Action.C
            else:
                action = Action.D
        elif other_c_prev >= m:
            action = Action.D
        elif other_c_prev == m - 1:
            if consecutive_failures >= self.failure_tolerance_threshold:
                action = Action.D
            else:
                action = Action.C
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_313(BaseStrategy):
    """
    An exploitative, adaptive, and robust strategy for the Collective Risk Dilemma.
    It primarily free-rides, aiming to maximize its own total payoff. It only
    contributes (cooperates) under specific, self-serving conditions where its
    contribution is pivotal to achieving the collective reward 'k', and failing
    to contribute would result in a lower personal payoff. It aggressively defects
    in all other scenarios, prioritizing its private endowment and minimizing its
    "sucker" risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == r - 1:
            return Action.D
        cooperators_prev_round_opponents = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        my_action_prev_round = history.my_actions[current_round_0_indexed - 1]
        total_cooperators_prev = cooperators_prev_round_opponents + my_action_prev_round
        if total_cooperators_prev >= m:
            return Action.D
        elif total_cooperators_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_314(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to dynamically balance the individual desire to defect (for 1+k payoff)
    with the collective need for cooperation (to achieve k). It defaults to exploitation but makes strategic
    contributions to maintain the cooperative environment when it's precisely at the brink of collapse,
    or to probe for cooperation initially. It abandons cooperation if the collective good is consistently not met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_cooperate = int(history.my_actions[-1])
        C_prev = cooperators_from_opponents_prev_round + my_action_prev_round_is_cooperate
        if C_prev < min_cooperators_needed:
            return Action.D
        elif C_prev == min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_315(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by selectively cooperating
    only when its contribution is predicted to be critical for reaching the
    collective threshold 'm'. Otherwise, it defects to secure its private
    endowment, either free-riding when 'm' is met by others, or avoiding
    wasted effort when 'm' is unlikely to be met.

    It begins with an initial cooperative probe in the first round to gather
    information and potentially encourage cooperation from others. In
    subsequent rounds, it adaptively predicts opponent behavior based on the
    average number of past cooperators and makes a decision accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        num_past_rounds = current_round_zero_indexed
        others_cooperated_in_past_rounds = np.sum(history.opponent_actions[:num_past_rounds, :], axis=1)
        avg_others_cooperated = np.mean(others_cooperated_in_past_rounds)
        predicted_N_C_others = round(avg_others_cooperated)
        predicted_N_C_others = int(max(0, min(self.n - 1, predicted_N_C_others)))
        if predicted_N_C_others >= self.m:
            my_current_action = Action.D
        elif predicted_N_C_others == self.m - 1:
            my_current_action = Action.C
        else:
            my_current_action = Action.D
        return my_current_action

class Strategy_EXPLOITATIVE_316(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize personal payoff by defecting whenever possible,
    and only cooperating when it is strategically advantageous to achieve a
    higher personal payoff that would otherwise be unattainable. It free-rides
    on others' cooperation and avoids "sucker's payoffs" from wasted cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == r - 1:
            return Action.D
        my_action_prev_bool = history.my_actions[current_round - 1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[current_round - 1, :])
        cooperators_prev = opponent_cooperators_prev + int(my_action_prev_bool)
        if cooperators_prev >= m:
            return Action.D
        elif my_action_prev_bool == Action.C:
            return Action.D
        elif cooperators_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_317(BaseStrategy):
    """
    The "Exploitative Opportunist" strategy aims to maximize its own payoff by free-riding on others'
    contributions when the collective risk threshold is met, and only cooperating when it is
    strategically profitable or necessary to nudge the system towards a more exploitable state.

    It operates with a hierarchical decision process:
    1.  **Round 1:** Always Defect to observe initial cooperation.
    2.  **Last Round:** Always Defect as there's no future for cooperation incentives.
    3.  **Intermediate Rounds:**
        a.  **Exploit Success:** If the threshold was met in the previous round, Defect (free-ride).
        b.  **Pivotal Cooperation:** If cooperation in the previous round was `m-1` (one less than threshold), Cooperate to ensure success and gain the `k` reward, as `k > 1`.
        c.  **Dynamic Poke:** If cooperation was far from `m` and the project failed repeatedly while this player defected, occasionally Cooperate ("poke") to try and stimulate collective action, otherwise Defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.poke_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t == n_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        n_c_prev = np.sum(history.opponent_actions[prev_round_idx]) + int(history.my_actions[prev_round_idx])
        my_prev_action = history.my_actions[prev_round_idx]
        if n_c_prev >= m_threshold:
            return Action.D
        if n_c_prev == m_threshold - 1:
            return Action.C
        consecutive_failures_where_i_defected = 0
        for i in range(prev_round_idx, -1, -1):
            num_cooperators_at_i = np.sum(history.opponent_actions[i]) + int(history.my_actions[i])
            my_action_at_i = history.my_actions[i]
            if num_cooperators_at_i < m_threshold and (not my_action_at_i):
                consecutive_failures_where_i_defected += 1
            else:
                break
        if consecutive_failures_where_i_defected >= self.poke_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_318(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to maximize total payoff by
    opportunistically free-riding when the collective benefit (k) is secured,
    and strategically contributing only when its cooperation is critical to
    unlocking the collective reward and enhancing its own payoff. It adapts
    to observed collective outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        my_action_prev_round_was_C = history.my_actions[current_round_0_indexed - 1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_was_C
        if total_cooperators_prev_round >= self.m_threshold:
            return Action.D
        else:
            cooperators_needed = self.m_threshold - total_cooperators_prev_round
            if cooperators_needed == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_319(BaseStrategy):
    """
    Adaptive Conditional Free-Rider (ACFR) strategy for the Collective Risk Dilemma.
    Prioritizes defection and free-riding, cooperating only when pivotal to meet
    the collective threshold for reward 'k'. It adapts based on the total number
    of cooperators observed in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        n_rounds = self.game_description.n_rounds
        m_min_cooperators = self.game_description.m
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
            my_action_prev_round_was_cooperate = history.my_actions[current_round_0_indexed - 1]
            num_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_was_cooperate
            if num_cooperators_prev_round >= m_min_cooperators:
                return Action.D
            else:
                needed_for_threshold = m_min_cooperators - num_cooperators_prev_round
                if needed_for_threshold == 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_320(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize individual payoff by strategically free-riding
    while being sufficiently adaptive to ensure the collective reward 'k' is obtained when possible.
    It continuously probes the willingness of other players to cooperate, defecting when cooperation
    seems robust, and re-engaging with cooperation when the collective project fails, but not indefinitely
    if its efforts prove futile.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.my_action_prev: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        C_prev_round_actual: int = 0
        if current_round_t > 1:
            my_action_in_prev_round_was_C = history.my_actions[-1]
            C_prev_round_actual = int(my_action_in_prev_round_was_C) + np.sum(history.opponent_actions[-1, :])
            if C_prev_round_actual < min_cooperators_m:
                self.consecutive_failures += 1
            else:
                self.consecutive_failures = 0
        my_current_action: Action
        if current_round_t == 1:
            my_current_action = Action.C
        elif current_round_t == total_rounds_r:
            my_current_action = Action.D
        elif self.consecutive_failures > 0:
            if self.my_action_prev == Action.C and C_prev_round_actual < min_cooperators_m:
                my_current_action = Action.D
            else:
                my_current_action = Action.C
        else:
            my_current_action = Action.D
        self.my_action_prev = my_current_action
        return my_current_action

class Strategy_EXPLOITATIVE_321(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the Collective Risk Dilemma.
    It aims to maximize its own payoff by leveraging free-rider incentives
    when the collective benefit is assured by others, and by strategically
    contributing only when its cooperation is critical and beneficial for
    itself to achieve the collective benefit. Otherwise, it avoids costly
    or futile contributions. Prioritizes individual gain and risk mitigation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        else:
            N_C_prev_other = np.sum(history.opponent_actions[state.round_number - 1, :])
            if N_C_prev_other < self.m - 1:
                return Action.D
            elif N_C_prev_other == self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_322(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own total payoff by
    exploiting the contributions of others while strategically contributing only when
    absolutely necessary to secure the collective reward.

    This strategy balances self-interest (defecting) with the necessity of ensuring
    the 'm' threshold is met to gain 'k'. It contributes only when its contribution
    is critical and sufficient to achieve 'k', otherwise, it defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        if current_round_1_indexed == self.total_rounds:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        other_cooperators_prev_round = int(np.sum(prev_round_opponent_actions))
        if other_cooperators_prev_round >= self.m_threshold:
            return Action.D
        elif other_cooperators_prev_round == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_323(BaseStrategy):
    """
    Adaptive Free-Rider with Minimum Trigger strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by prioritizing defection to free-ride,
    only cooperating when it is critically necessary to meet the 'm' threshold and
    doing so provides a direct individual gain (i.e., when k > 1, which is a game parameter).
    Decisions are adaptive, based on the aggregate cooperation level observed in the
    immediately preceding round.

    Core Principles:
    1. Prioritize Defection: Default action is 'D' to secure private endowment and potentially free-ride.
    2. Only Cooperate to Trigger 'k': Cooperate only if it's the specific action needed to meet 'm'
       and unlock the collective reward 'k', which is individually rational as k > 1.
    3. Adapt to Collective Outcome: Base decisions on the total number of cooperators from the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        else:
            num_cooperated_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperated_last_round >= self.m:
                return Action.D
            elif num_cooperated_last_round == self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_324(BaseStrategy):
    """
    The "Reactive Free-Rider" strategy for the Collective Risk Dilemma.
    This strategy focuses on maximizing individual payoff by free-riding whenever the
    collective good (reward 'k') is already being provided. It strategically contributes
    only when necessary to prevent the complete loss of the collective reward or to
    test the environment, thereby preventing a total collapse into non-cooperation that
    would harm its own long-term payoff. It combines initial probing, free-riding,
    conditional contribution, and end-game exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev = num_opponent_cooperators_prev + int(my_action_prev_bool)
        if total_cooperators_prev >= self.game_description.m:
            return Action.D
        elif my_action_prev_bool is True:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_325(BaseStrategy):
    """
    An exploitative, adaptive, and robust strategy for the Collective Risk Dilemma.
    It free-rides when others already meet the threshold, cooperates when its
    contribution is pivotal, and defects when cooperation is futile or in the
    final round. It starts with an initial cooperation to probe the environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        n_c_others_last_round = sum(history.opponent_actions[-1, :])
        if n_c_others_last_round >= m_threshold:
            return Action.D
        elif n_c_others_last_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_326(BaseStrategy):
    """
    This strategy, "Adaptive Free-Rider with Reactive Cooperation", is designed to be highly exploitative.
    It prioritizes maximizing its own payoff by free-riding whenever possible, only contributing when its
    action is critical to securing a collective benefit that directly outweighs the cost of cooperation
    for itself, and completely disengaging from cooperation in the final round.

    Decision Rules:
    1. Round 1: Defect to free-ride and gather data.
    2. Intermediate Rounds (2 to r-1):
       - If the threshold 'm' was met last round: Defect (free-ride).
       - If the threshold 'm' was NOT met last round:
         - If our cooperation *could* be critical (num_cooperators_prev_round == m - 1): Cooperate.
         - Otherwise (num_cooperators_prev_round < m - 1): Defect (avoid cost for no benefit).
    3. Last Round (r): Defect, as there are no future interactions to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        opponents_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = (1 if my_prev_action_was_C else 0) + opponents_prev_cooperators
        if num_cooperators_prev_round >= m_threshold:
            return Action.D
        elif num_cooperators_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_327(BaseStrategy):
    """
    An Adaptive Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by primarily defecting to free-ride
    on others' cooperation. It strategically cooperates only when the community project
    is very close to success and its cooperation might be pivotal, and only if it
    didn't cooperate fruitlessly in the previous round. It always defects in the
    first and last rounds to maximize immediate gains or avoid future costs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_action_last_round: Action = Action.D
        self.CT_TOLERANCE = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        action_for_this_round: Action
        if current_0_indexed_round == total_rounds - 1:
            action_for_this_round = Action.D
        elif current_0_indexed_round == 0:
            action_for_this_round = Action.D
        else:
            C_prev = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
            if C_prev >= m:
                action_for_this_round = Action.D
            elif self.my_action_last_round == Action.C:
                action_for_this_round = Action.D
            else:
                missing_cooperators = m - C_prev
                if missing_cooperators <= self.CT_TOLERANCE:
                    action_for_this_round = Action.C
                else:
                    action_for_this_round = Action.D
        self.my_action_last_round = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_328(BaseStrategy):
    """
    The Adaptive Free-Rider aims to maximize its payoff by free-riding on the collective
    action of others. It cooperates initially to establish the collective benefit (k),
    then adapts based on observed behavior: cooperating only when pivotal, and defecting
    otherwise. In the final round, it defects unconditionally due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Free-Rider strategy with the game parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for
                     this player and opponents, or None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        num_others_cooperated_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        if num_others_cooperated_prev_round >= m:
            return Action.D
        elif num_others_cooperated_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_329(BaseStrategy):
    """
    Strategy Name: The Pragmatic Exploiter

    This strategy is highly exploitative, only cooperating when it's individually beneficial
    and critical for achieving a better payoff, or as a minimal, cautious initial probe.
    It adapts to opponent behavior by evaluating the previous round's outcome and considering
    its own marginal impact on the threshold 'm'.

    Decision Rules Summary:
    1. Last Round Check: In the final round, always Defect (D) to maximize immediate payoff,
       as there are no future consequences.
    2. Initial Probe: In the very first round (round 0), Cooperate (C) as a probe to test
       the cooperativeness of other players. This is a one-time investment to potentially
       establish a base for the collective bonus.
    3. Intermediate Rounds (0 < current_round < r-1):
       - If my cooperation is NOT strictly necessary for 'm' to be met by others (i.e., n-1 >= m):
         Always Defect (D) to free-ride on the potential cooperation of others.
       - If my cooperation IS strictly necessary IF exactly (m-1) opponents cooperate (i.e., n-1 < m):
         Evaluate if enough opponents are cooperating (at least m-1) such that *my* single
         cooperation would meet or exceed the threshold 'm'.
         - If yes, and if cooperating yields a strictly better individual payoff (k-1 > 1, i.e., k > 2):
           Cooperate (C).
         - Otherwise (if k-1 <= 1, i.e., k <= 2, or not enough opponents cooperated):
           Defect (D). Don't sacrifice for a lost cause or for insufficient personal gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.r - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_index = current_round_0_indexed - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :])
        if self.n - 1 >= self.m:
            return Action.D
        elif num_opponent_cooperators_prev_round >= self.m - 1:
            if self.k - 1 > 1:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_330(BaseStrategy):
    """
    Adaptive Free-Rider with Immediate Collective Restoration: This strategy attempts to free-ride
    (defect) as much as possible to maximize private endowment. It only cooperates when the
    collective reward (k) is at immediate risk due to a failure to meet the cooperation
    threshold (m) in the previous round. It also defects in the final round to exploit
    the end of the game. This approach aims for an exploitative, adaptive, and robust balance.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failure_rounds_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed > 0:
            prev_round_0_indexed = current_round_0_indexed - 1
            my_action_prev_round = history.my_actions[prev_round_0_indexed]
            opponent_actions_prev_round = history.opponent_actions[prev_round_0_indexed, :]
            total_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
            if total_cooperators_prev_round < self.game_description.m:
                self.consecutive_failure_rounds_count += 1
            else:
                self.consecutive_failure_rounds_count = 0
        if current_round_1_indexed == total_rounds:
            return Action.D
        if self.consecutive_failure_rounds_count >= 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_331(BaseStrategy):
    """
    The Adaptive Trigger-Free Rider strategy prioritizes individual payoff by defaulting to defection (free-riding).
    It strategically cooperates only when its contribution is pivotal to achieving the collective reward 'k'
    (i.e., when N_C(t-1) == m-1), and k > 1 makes this a self-serving decision.
    It defects in the first round to probe and in the final round to exploit the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_was_C)
        if num_cooperators_prev_round >= self.m:
            return Action.D
        elif num_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_332(BaseStrategy):
    """
    Adaptive Critical Exploiter (ACE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own cumulative payoff by primarily free-riding
    on the cooperation of other players, while being strategically cooperative only
    when its contribution is critical and individually profitable. It accounts for
    the repeated nature of the game and end-game effects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        reward_k = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= min_cooperators_m:
            return Action.D
        else:
            shortfall = min_cooperators_m - num_cooperators_prev_round
            if shortfall == 1:
                if reward_k > 2.0:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_EXPLOITATIVE_333(BaseStrategy):
    """
    Strategy Name: The Adaptive Exploiter (AE)

    Core Philosophy: The Adaptive Exploiter aims to consistently receive the collective reward 'k'
    while contributing the minimum necessary. It is quick to exploit existing
    cooperation and reluctant to invest in re-establishing it when it breaks down,
    expecting others to take that risk.

    Decision Rules:
    1.  **Last Round:** Always Defect (D) due to backward induction.
    2.  **First Round:** Cooperate (C) to establish a cooperative baseline, creating an environment ripe for exploitation.
    3.  **Intermediate Rounds:**
        a.  **If threshold met in previous round (num_cooperators_prev_round >= m):**
            -   If this player defected previously and the threshold was still met: Continue to Defect (D) (successful exploitation).
            -   If this player cooperated previously:
                -   If there was a surplus of cooperators (num_cooperators_prev_round > m): Defect (D) to exploit the surplus.
                -   If this player was a critical cooperator (num_cooperators_prev_round == m): Cooperate (C) to ensure the 'k' bonus is secured for self-interest.
        b.  **If threshold NOT met in previous round (num_cooperators_prev_round < m):**
            -   Defect (D). The strategy does not take the lead in repairing a broken collective; it waits for others to invest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.C
        prev_round_idx = current_round_number - 1
        my_action_prev_round_bool = history.my_actions[prev_round_idx]
        opponent_actions_prev_round_bool = history.opponent_actions[prev_round_idx, :]
        num_cooperators_prev_round = np.sum(opponent_actions_prev_round_bool) + int(my_action_prev_round_bool)
        if num_cooperators_prev_round >= self.m:
            if not my_action_prev_round_bool:
                return Action.D
            elif num_cooperators_prev_round > self.m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_334(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize its own total payoff in the Collective Risk Dilemma.
    It primarily defects to secure its private endowment, only cooperating under specific conditions
    where its contribution is critical and individually beneficial to reach the collective reward 'k'.

    Key principles:
    - Default to Defect: Always prefer defecting to keep the private endowment.
    - Condition for Cooperation: Only cooperate if the previous round's cooperation level was exactly 'm-1',
      making its cooperation the critical factor to achieve the collective reward 'k'. Since k > 1,
      obtaining k (by cooperating) is better than obtaining 1 (by defecting when m is missed).
    - End-game Exploitation: Defects in the final round to maximize immediate gain without future consequences.
    - Initial Probe: Defects in the first round to gather information on opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_prev_action_cooperated = history.my_actions[prev_round_idx]
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        C_last_round = int(my_prev_action_cooperated) + opponent_prev_cooperators
        if C_last_round >= m_threshold:
            return Action.D
        elif C_last_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_335(BaseStrategy):
    """
    The Smart Conditional Cooperator (S.C.C.) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by strategically leveraging collective
    cooperation. It prioritizes free-riding when the collective threshold 'm' is met by others,
    contributes only when its single contribution is precisely what's needed to meet 'm'
    and results in a net gain (k > 1), and otherwise defects to avoid being a 'sucker'
    when contribution would be futile or suboptimal.
    It defects in the first and last rounds due to lack of history or no future influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_other_cooperators_last_round = sum(history.opponent_actions[-1, :])
        if num_other_cooperators_last_round >= self.m:
            return Action.D
        elif num_other_cooperators_last_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_336(BaseStrategy):
    """
    The Adaptive Free-Rider strategy is designed to be highly exploitative, maximizing its own payoff by free-riding on others' cooperation whenever possible, while strategically cooperating only when its contribution is critical to securing the collective reward. It adapts to the observed behavior of other players over time but remains fundamentally self-interested.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_previous_action: None | Action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        my_action_for_this_round: Action
        if current_round_1_indexed == 1:
            my_action_for_this_round = Action.D
        elif current_round_1_indexed == n_rounds:
            my_action_for_this_round = Action.D
        else:
            num_cooperators_total_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            effective_others_c_prev_round = np.sum(history.opponent_actions[-1, :])
            if num_cooperators_total_prev_round >= m_threshold:
                my_action_for_this_round = Action.D
            elif effective_others_c_prev_round >= m_threshold - 1:
                my_action_for_this_round = Action.C
            else:
                my_action_for_this_round = Action.D
        self.my_previous_action = my_action_for_this_round
        return my_action_for_this_round

class Strategy_EXPLOITATIVE_337(BaseStrategy):
    """
    Cynical Free-Rider with Limited Patience strategy for the Collective Risk Dilemma.
    This strategy prioritizes maximizing its own total payoff by aggressively free-riding
    on the cooperation of others. It is pragmatically willing to contribute to the collective
    good (triggering the 'k' bonus) only when it deems it necessary and potentially achievable,
    and it will cease cooperation if its efforts to initiate or maintain the collective good
    are consistently unsuccessful.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_rounds = game_description.n_rounds
        self.min_cooperators_needed = game_description.m
        self.failure_streak = 0
        self.patience_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed > 0:
            previous_round_idx = current_round_0_indexed - 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
            my_action_prev_round_is_cooperate = history.my_actions[previous_round_idx]
            num_cooperators_prev_round = opponent_cooperators_prev_round + int(my_action_prev_round_is_cooperate)
            if num_cooperators_prev_round >= self.min_cooperators_needed:
                self.failure_streak = 0
            else:
                self.failure_streak += 1
        if current_round_0_indexed == self.total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        if num_cooperators_prev_round >= self.min_cooperators_needed:
            return Action.D
        elif self.failure_streak <= self.patience_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_338(BaseStrategy):
    """
    Adaptive Exploitation with Recalibration (AER) strategy for Collective Risk Dilemma.
    AER prioritizes self-interest, adapts to opponent behavior, and recalibrates
    cooperation when collective reward falters, aiming to maximize total payoff.

    Core Principles:
    1. Prioritize Self-Interest: Always aim for the highest individual payoff.
    2. Ensure Collective Benefit (when profitable): Secure the reward 'k' if it's
       efficient or necessary for exploitation or recalibration.
    3. Minimize Cost: Cooperate only when strategically advantageous.
    4. Adaptation: Continuously monitor and react to observed cooperation levels.
    5. Backward Induction (Last Round): Maximize immediate gains in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_previous_action: Action | None = None
        self.cooperators_in_previous_round: int = 0
        self.consecutive_m_misses: int = 0
        self.defection_penalty_threshold: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        is_first_round = state.round_number == 0
        is_last_round = state.round_number == self.game_description.n_rounds - 1
        if not is_first_round and history is not None:
            prev_my_action_bool = history.my_actions[-1]
            self.my_previous_action = Action.C if prev_my_action_bool else Action.D
            prev_total_cooperators = sum(history.opponent_actions[-1, :]) + (1 if prev_my_action_bool else 0)
            self.cooperators_in_previous_round = prev_total_cooperators
            if self.cooperators_in_previous_round >= self.game_description.m:
                self.consecutive_m_misses = 0
            else:
                self.consecutive_m_misses += 1
        other_cooperators_prev = 0
        if not is_first_round and history is not None:
            other_cooperators_prev = sum(history.opponent_actions[-1, :])
        action = Action.D
        if is_first_round:
            action = Action.C
        elif is_last_round:
            if other_cooperators_prev >= self.game_description.m:
                action = Action.D
            elif other_cooperators_prev == self.game_description.m - 1:
                action = Action.C
            else:
                action = Action.D
        elif other_cooperators_prev >= self.game_description.m:
            action = Action.D
        elif other_cooperators_prev == self.game_description.m - 1:
            action = Action.C
        elif self.consecutive_m_misses >= self.defection_penalty_threshold:
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_339(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed for the Collective Risk Dilemma.
    It aims to maximize individual payoff (1+k) by strategically free-riding on others'
    cooperation when there's a surplus. It also strategically cooperates when necessary
    to establish or maintain the threshold for the collective reward 'k', understanding
    that 'k' is a prerequisite for maximal payoffs. The strategy cuts its losses by
    permanently defecting if the group consistently fails to achieve the collective good.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Free-Rider strategy with game parameters
        and its internal state variables.
        """
        self.game_description = game_description
        self.consecutive_failures = 0
        self.GIVE_UP_THRESHOLD = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: GameState object containing the current round number (0-indexed).
            history: PlayerHistory object containing past actions and payoffs.
                     None for the very first round (round_number == 0).

        Returns:
            Action.C for Cooperate, Action.D for Defect.
        """
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        my_action_prev_round = history.my_actions[-1]
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + my_action_prev_round
        if num_cooperators_prev_round < m:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        if current_round_number == n_rounds - 1:
            return Action.D
        if self.consecutive_failures >= self.GIVE_UP_THRESHOLD:
            return Action.D
        if num_cooperators_prev_round >= m + 1:
            return Action.D
        elif num_cooperators_prev_round == m:
            if my_action_prev_round == Action.D.value:
                return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_EXPLOITATIVE_340(BaseStrategy):
    """
    The "Adaptive Threshold Free-Rider" (ATFR) strategy aims to maximize personal payoff
    in the Collective Risk Dilemma. It defaults to defection (free-riding) and only
    cooperates when such cooperation is deemed critical and effective in securing
    the collective reward 'k' for itself.

    The strategy's decision rules are:
    1.  **First Round:** Always Defect (D) to probe the environment for others' willingness to cooperate.
    2.  **Last Round:** Always Defect (D) because there are no future consequences, maximizing immediate private payoff.
    3.  **Intermediate Rounds:**
        *   If the collective threshold 'm' was met in the previous round, Defect (D) to free-ride on others' cooperation, securing '1+k'.
        *   If the threshold was NOT met in the previous round, evaluate if player's own cooperation would be enough to meet the threshold, assuming all other players repeat their previous actions.
            *   If player's cooperation would make the difference (`(cooperators from others in t-1) + 1 >= m`), then Cooperate (C) to secure the 'k' bonus.
            *   Otherwise (`(cooperators from others in t-1) + 1 < m`), Defect (D) to avoid a wasted contribution (0 payoff) and secure 1 instead.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed == 0:
            return Action.D
        if current_round_number_0_indexed == n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_number_0_indexed - 1
        total_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + history.my_actions[prev_round_idx]
        if total_cooperators_prev_round >= min_cooperators_needed:
            return Action.D
        else:
            cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            projected_total_cooperators_if_i_cooperate = cooperators_from_opponents_prev_round + 1
            if projected_total_cooperators_if_i_cooperate >= min_cooperators_needed:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_341(BaseStrategy):
    """
    The Adaptive Free-Rider strategy is designed to be highly exploitative.
    It prioritizes defecting to secure a private payoff and only cooperates
    when its contribution is critically necessary and individually profitable
    to meet the collective threshold 'm' and unlock the reward 'k'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        reward_factor = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == total_rounds:
            return Action.D
        elif current_round_1_indexed == 1:
            return Action.D
        else:
            C_count_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if C_count_prev_round >= min_cooperators:
                return Action.D
            elif C_count_prev_round == min_cooperators - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_342(BaseStrategy):
    """
    The Calculated Free-Rider (CFR) strategy aims to maximize personal gain in the
    Collective Risk Dilemma by carefully observing and reacting to collective behavior.
    It prioritizes free-riding when the cooperation threshold is met and conserves
    resources (defects) when collective cooperation appears futile or unnecessary.
    Strategic cooperation is used as an investment to maintain or restore an
    exploitable environment.

    Decision Rules:
    1. Last Round: Always Defect (D) to maximize final round payoff.
    2. Early Probing Phase (first 2 rounds): Always Cooperate (C) to test the environment
       and encourage initial collective cooperation.
    3. Main Adaptive Logic:
       a. If 'm' cooperators were met last round: Defect (D) to free-ride on others' success.
       b. If 'm' cooperators were NOT met last round:
          i. Analyze success rate in recent history (up to 5 rounds, excluding probing).
          ii. If success rate >= 50%: Cooperate (C) to strategically re-establish cooperation.
          iii. Else (success rate < 50%): Defect (D) to minimize losses when cooperation seems unsustainable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed <= 2:
            return Action.C
        num_cooperators_last_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_last_round >= self.m:
            return Action.D
        else:
            recent_history_window_size = max(1, min(state.round_number - 2, 5))
            successful_rounds_in_recent = 0
            start_index = state.round_number - recent_history_window_size
            end_index = state.round_number
            for i in range(start_index, end_index):
                cooperators_in_round_i = history.my_actions[i] + np.sum(history.opponent_actions[i, :])
                if cooperators_in_round_i >= self.m:
                    successful_rounds_in_recent += 1
            success_rate_recent = successful_rounds_in_recent / recent_history_window_size
            if success_rate_recent >= 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_343(BaseStrategy):
    """
    The Conditional Defector strategy prioritizes individual payoff by largely free-riding on the cooperation
    of others. It defects in the first round and in subsequent rounds if the collective reward threshold 'm'
    was met previously. However, if the threshold was not met in the prior round, it cooperates only if
    its contribution is pivotal to reach 'm' in the current round, thereby securing the 'k' reward which
    is individually beneficial (since k > 1). If its cooperation wouldn't be enough to meet 'm', it defects
    to avoid a wasted contribution and secure a payoff of 1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_is_C = history.my_actions[prev_round_idx]
        opponent_actions_prev = history.opponent_actions[prev_round_idx, :]
        n_cooperators_total_prev = int(my_action_prev_is_C) + np.sum(opponent_actions_prev)
        threshold_met_prev = n_cooperators_total_prev >= self.game_description.m
        if threshold_met_prev:
            return Action.D
        else:
            n_cooperators_others_prev = n_cooperators_total_prev - int(my_action_prev_is_C)
            if n_cooperators_others_prev + 1 >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_344(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by strategically balancing
    cooperation and defection. It adapts to observed collective behavior,
    free-riding whenever possible while ensuring the collective reward `k` is
    secured when the agent's contribution is critical and beneficial.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            if self.n_players - 1 >= self.m:
                return Action.D
            else:
                return Action.C
        elif current_round_t == self.n_rounds:
            return Action.D
        else:
            n_c_prev_round_t_minus_1 = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if n_c_prev_round_t_minus_1 >= self.m:
                return Action.D
            elif n_c_prev_round_t_minus_1 == self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_345(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Collective Risk Dilemma.
    It prioritizes individual payoff maximization by free-riding on other players'
    contributions, only cooperating when its action is critical for achieving the
    collective reward and doing so is individually profitable. It specifically
    handles the first and last rounds as special cases for exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds_r = self.game_description.n_rounds
        m_min_cooperators = self.game_description.m
        current_1_indexed_round = state.round_number + 1
        if current_1_indexed_round == 1:
            return Action.D
        if current_1_indexed_round == total_rounds_r:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= m_min_cooperators:
            return Action.D
        elif num_cooperators_prev_round == m_min_cooperators - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_346(BaseStrategy):
    """
    This strategy implements an Adaptive Free-Rider with Contingent Contribution
    for the Collective Risk Dilemma. It prioritizes individual gain, defects
    by default, and only cooperates when its contribution is critical to secure
    the collective reward 'k' for itself, or when it perceives that other players
    are sufficiently carrying the burden. It leverages the repeated nature of
    the game by observing past collective outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round == 0:
            return Action.D
        if current_round == total_rounds - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= m:
            return Action.D
        elif num_cooperators_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_347(BaseStrategy):
    """
    The Collective Exploiter strategy aims to maximize individual payoff by defaulting to defection (free-riding)
    when the collective good is achieved, and only cooperating when such cooperation is necessary and directly
    impactful to restore the collective good from which to free-ride again. It is adaptive to observed outcomes
    and robust to various opponent behaviors without relying on shared norms or specific coordination.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_idx]
        opponent_actions_prev = history.opponent_actions[prev_round_idx, :]
        N_C_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        if N_C_prev >= self.game_description.m:
            return Action.D
        else:
            num_other_cooperators_prev = np.sum(opponent_actions_prev)
            potential_cooperators_if_I_play_C = num_other_cooperators_prev + 1
            if potential_cooperators_if_I_play_C >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_348(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy is designed to maximize individual payoff in the
    Collective Risk Dilemma. It cooperates in the first round as an initial investment.
    In intermediate rounds, it defects if the cooperation threshold (m) was met in the
    previous round (free-riding). If the threshold was not met, it cooperates to
    re-establish the collective reward. In the final round, it always defects for
    terminal exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_number - 1
            num_opponent_cooperators_prev = sum(history.opponent_actions[previous_round_index, :])
            my_action_prev = history.my_actions[previous_round_index]
            C_count_prev = num_opponent_cooperators_prev + my_action_prev
            if C_count_prev >= self.m_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_349(BaseStrategy):
    """
    The Opportunistic Exploiter strategy aims to maximize individual payoff in the Collective Risk Dilemma.
    It prioritizes free-riding by defecting by default. It only cooperates under specific conditions:
    when its cooperation is critical to securing a significant personal benefit (k > 2) that would
    otherwise be lost due to its past defection, or to avoid being a "sucker" if the group consistently
    fails to cooperate. It always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_failures_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        reward_factor_k = self.game_description.k
        current_round = state.round_number + 1
        if state.round_number > 0 and history is not None:
            cooperators_in_prev_completed_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
            if cooperators_in_prev_completed_round < min_cooperators_needed:
                self._consecutive_failures_count += 1
            else:
                self._consecutive_failures_count = 0
        my_prev_action_for_decision = Action.D
        num_cooperators_prev_round_for_decision = 0
        if current_round > 1 and history is not None:
            my_prev_action_for_decision = Action.C if history.my_actions[-1] else Action.D
            num_cooperators_prev_round_for_decision = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if current_round == total_rounds:
            return Action.D
        if current_round == 1:
            return Action.D
        action_to_play = Action.D
        if reward_factor_k <= 2:
            action_to_play = Action.D
        elif self._consecutive_failures_count >= min_cooperators_needed:
            action_to_play = Action.D
        elif num_cooperators_prev_round_for_decision == min_cooperators_needed - 1 and my_prev_action_for_decision == Action.D:
            action_to_play = Action.C
        return action_to_play

class Strategy_EXPLOITATIVE_350(BaseStrategy):
    """
    Opportunistic Reciprocator with End-Game Exploitation: A highly exploitative strategy
    that primarily free-rides, cooperates only to re-establish collective good after failure,
    and fully exploits the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.D
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            my_action_prev = history.my_actions[-1]
            num_my_cooperators_prev = int(my_action_prev)
            N_c_prev = num_opponent_cooperators_prev + num_my_cooperators_prev
            if N_c_prev >= min_cooperators_m:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_351(BaseStrategy):
    """
    The "Adaptive Group Steward" strategy balances a willingness to cooperate to
    ensure the collective good (the 'k' bonus) with a pragmatic approach to
    individual contribution, always keeping the total score maximization as
    the driving force. It aims to ensure the 'm' threshold is met consistently,
    and secondarily, to optimize individual contributions to maximize overall
    efficiency by defecting when there's a surplus of cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        previous_round_idx = state.round_number - 1
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[previous_round_idx, :])
        my_last_action_was_C = history.my_actions[previous_round_idx]
        N_C_last_round = num_opponent_cooperators_last_round + (1 if my_last_action_was_C else 0)
        was_successful_last_round = N_C_last_round >= self.m
        if not was_successful_last_round:
            return Action.C
        elif N_C_last_round > self.m:
            return Action.D
        elif my_last_action_was_C:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_352(BaseStrategy):
    """
    Adaptive Free-Rider with Threshold Enforcement strategy.

    This strategy aims to maximize individual payoff in the Collective Risk Dilemma
    by primarily defecting to free-ride on others' cooperation. It strategically
    cooperates only under specific conditions:
    1. In the first round (if k > 2) as an initial probe to encourage cooperation.
    2. When its cooperation is absolutely critical to meet the 'm' threshold, AND
       the individual benefit from doing so (k-1) is strictly greater than what
       it would get by defecting if the threshold is missed (1). This occurs when k > 2.

    The strategy always defects in the final round due to backward induction.
    If k <= 2, the strategy always defects, as there's no sufficient individual
    incentive to contribute, even if critical for the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if self.k <= 2.0:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        prev_round_0_indexed = state.round_number - 1
        n_c_opponent_prev = np.sum(history.opponent_actions[prev_round_0_indexed, :])
        if n_c_opponent_prev >= self.m:
            return Action.D
        elif n_c_opponent_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_353(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy is designed to be greedy and exploitative, maximizing its own
    payoff in the current round based on the previous round's observed cooperation levels.
    It defaults to defection to free-ride, but will strategically cooperate if
    its contribution is predicted to be critical for meeting the threshold 'm'
    and results in a strictly higher immediate private payoff (given k > 1).
    It also defaults to defection in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        num_C_others_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        expected_C_from_others_this_round = num_C_others_prev_round
        total_C_if_I_C = expected_C_from_others_this_round + 1
        my_payoff_if_I_C = self.k if total_C_if_I_C >= self.m else 0.0
        total_C_if_I_D = expected_C_from_others_this_round
        my_payoff_if_I_D = 1.0 + self.k if total_C_if_I_D >= self.m else 1.0
        if my_payoff_if_I_D > my_payoff_if_I_C:
            return Action.D
        elif my_payoff_if_I_C > my_payoff_if_I_D:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_354(BaseStrategy):
    """
    Adaptive Threshold Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by prioritizing defection (free-riding).
    It cooperates only when its contribution is absolutely critical to meet the collective
    threshold 'm' and secure the 'k' reward, which also benefits itself.
    It defects in the first and last rounds, and when other players' cooperation is either
    sufficient or clearly insufficient to meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveThresholdFreeRider strategy.

        Args:
            game_description (CollectiveRiskDescription): Description of the game
                                                         parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Current state of the game, including the round number (0-indexed).
            history (PlayerHistory | None): History of actions and payoffs from previous rounds.
                                            None for the very first round (state.round_number == 0).

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_0_indexed = state.round_number
        last_round_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == last_round_0_indexed:
            return Action.D
        cooperators_others_prev = np.sum(history.opponent_actions[-1, :])
        m_threshold = self.game_description.m
        if cooperators_others_prev >= m_threshold:
            return Action.D
        elif cooperators_others_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_355(BaseStrategy):
    """
    The "Adaptive Free-Rider" (AFL) strategy is designed to maximize its own total payoff
    in the Collective Risk Dilemma. It predominantly defects to gain the private '1' bonus,
    but intelligently cooperates when its contribution is critical to meet the collective
    cooperation threshold `m` (to secure the 'k' reward), especially if cooperation has
    been failing recently. It always defects in the first and final rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_AVG = 5
        self.COOP_SAVE_THRESHOLD = 0.5
        self.history_others_cooperators_window: list[int] = []
        self.history_m_met_window: list[bool] = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        my_action_prev_is_C = history.my_actions[-1]
        observed_total_cooperators_prev = int(my_action_prev_is_C) + np.sum(history.opponent_actions[-1, :])
        num_others_cooperated_prev = observed_total_cooperators_prev
        if my_action_prev_is_C:
            num_others_cooperated_prev -= 1
        self.history_others_cooperators_window.append(num_others_cooperated_prev)
        if len(self.history_others_cooperators_window) > self.W_AVG:
            self.history_others_cooperators_window.pop(0)
        m_was_met_prev = observed_total_cooperators_prev >= self.game_description.m
        self.history_m_met_window.append(m_was_met_prev)
        if len(self.history_m_met_window) > self.W_AVG:
            self.history_m_met_window.pop(0)
        estimated_C_others_t: int = 0
        if self.history_others_cooperators_window:
            estimated_C_others_t = round(np.mean(self.history_others_cooperators_window))
        recent_success_rate: float = 1.0
        if self.history_m_met_window:
            recent_success_rate = np.mean(self.history_m_met_window)
        current_action = Action.D
        if estimated_C_others_t >= self.game_description.m:
            current_action = Action.D
        elif estimated_C_others_t == self.game_description.m - 1:
            if recent_success_rate <= self.COOP_SAVE_THRESHOLD:
                current_action = Action.C
            else:
                current_action = Action.D
        elif estimated_C_others_t < self.game_description.m - 1:
            current_action = Action.D
        return current_action

class Strategy_EXPLOITATIVE_356(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy for the Collective Risk Dilemma.
    It primarily defaults to defection to free-ride, but adapts to ensure the collective project
    succeeds when failure becomes too costly. It cooperates to restart a failing project
    or to sustain a critically successful one, but avoids being a "sucker" if cooperation is futile.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.my_last_action: Action | None = None
        self.my_last_cooperation_was_successful: bool = False
        self.failure_tolerance_rounds: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_1_indexed = state.round_number + 1
        prev_round_total_cooperators: int = 0
        project_succeeded_prev_round: bool = False
        my_action_in_prev_round: Action | None = self.my_last_action
        if state.round_number > 0:
            prev_round_opponent_actions_bool: NDArray[np.bool_] = history.opponent_actions[state.round_number - 1, :]
            my_action_in_prev_round_bool: bool = my_action_in_prev_round == Action.C if my_action_in_prev_round else False
            prev_round_total_cooperators = int(sum(prev_round_opponent_actions_bool)) + (1 if my_action_in_prev_round_bool else 0)
            project_succeeded_prev_round = prev_round_total_cooperators >= self.game_description.m
            if not project_succeeded_prev_round:
                self.consecutive_failures += 1
            else:
                self.consecutive_failures = 0
            if my_action_in_prev_round == Action.C and (not project_succeeded_prev_round):
                self.my_last_cooperation_was_successful = False
            elif project_succeeded_prev_round:
                self.my_last_cooperation_was_successful = True
            elif my_action_in_prev_round == Action.D and (not project_succeeded_prev_round):
                self.my_last_cooperation_was_successful = False
        action_for_current_round: Action = Action.D
        if current_round_num_1_indexed == self.game_description.n_rounds:
            action_for_current_round = Action.D
        elif current_round_num_1_indexed == 1:
            action_for_current_round = Action.D
        elif project_succeeded_prev_round:
            if prev_round_total_cooperators == self.game_description.m and my_action_in_prev_round == Action.C:
                action_for_current_round = Action.C
            else:
                action_for_current_round = Action.D
        elif my_action_in_prev_round == Action.C and (not self.my_last_cooperation_was_successful):
            action_for_current_round = Action.D
        elif self.consecutive_failures >= self.failure_tolerance_rounds:
            action_for_current_round = Action.C
        else:
            action_for_current_round = Action.D
        self.my_last_action = action_for_current_round
        return action_for_current_round

class Strategy_EXPLOITATIVE_357(BaseStrategy):
    """
    Adaptive Free-Rider with Self-Preservation strategy for the Collective Risk Dilemma.

    This strategy aims to be highly exploitative by maximizing individual payoff.
    It primarily aims to free-ride (defect) when the collective reward 'k' is being
    achieved by others, and only cooperates as a strategic investment to re-establish
    the 'k' bonus if it's consistently missed and the player's contribution can be pivotal.
    It leverages historical data to adapt its behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def get_total_cooperators_in_round(round_idx: int) -> int:
            my_action_val = 1 if history.my_actions[round_idx] else 0
            opponent_cooperators = np.sum(history.opponent_actions[round_idx, :])
            return int(my_action_val + opponent_cooperators)
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.r - 1:
            return Action.D
        previous_cooperators = get_total_cooperators_in_round(current_round_0_indexed - 1)
        if previous_cooperators >= self.m:
            return Action.D
        elif previous_cooperators + 1 < self.m:
            return Action.D
        else:
            failure_streak = 0
            for hist_round_idx in range(current_round_0_indexed - 1, -1, -1):
                if get_total_cooperators_in_round(hist_round_idx) < self.m:
                    failure_streak += 1
                else:
                    break
            if failure_streak >= 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_358(BaseStrategy):
    """
    The Opportunistic Restorer strategy for the Collective Risk Dilemma.
    It prioritizes maximizing its own payoff by generally free-riding,
    but it incorporates a limited "investment" phase to restore the collective reward (k)
    when it's consistently missed, allowing for renewed exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_k_missed_streak: int = 0
        self.cooperation_investment_rounds_remaining: int = 0
        self.miss_streak_to_invest: int = 1
        self.investment_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= m:
            self.rounds_k_missed_streak = 0
        else:
            self.rounds_k_missed_streak += 1
        if self.cooperation_investment_rounds_remaining > 0:
            self.cooperation_investment_rounds_remaining -= 1
            return Action.C
        elif self.rounds_k_missed_streak >= self.miss_streak_to_invest:
            self.cooperation_investment_rounds_remaining = self.investment_duration - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_359(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by strategically free-riding
    (defecting) when the cooperation threshold is met by others. It only
    cooperates when necessary to "seed" cooperation in the first round,
    or to re-establish the conditions for collective success after a failure
    where the strategy itself contributed to the failure. Conversely, it cuts
    losses and signals dissatisfaction by defecting if its cooperation in a
    previous round failed to achieve the threshold. In the final round, it
    always defects as there are no future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        threshold_m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        n_c_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        my_last_action_was_C = history.my_actions[-1]
        if n_c_prev >= threshold_m:
            return Action.D
        elif my_last_action_was_C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_360(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize individual payoff in the Collective Risk Dilemma
    by primarily defecting (free-riding) to receive a higher payoff when the cooperation threshold 'm'
    is met by others. It is adaptive, strategically cooperating only when its contribution is critical
    to meet the 'm' threshold, or as a "kickstart" to encourage cooperation after a period of collective failure.
    It defects unconditionally in the final round and cooperates as an initial investment in the first round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failure_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        if current_round == n_rounds:
            return Action.D
        if current_round == 1:
            return Action.C
        else:
            last_round_idx = state.round_number - 1
            last_round_my_action_bool = history.my_actions[last_round_idx]
            last_round_opponent_cooperators = np.sum(history.opponent_actions[last_round_idx, :])
            last_round_total_cooperators = last_round_opponent_cooperators + (1 if last_round_my_action_bool else 0)
            if last_round_total_cooperators < m_threshold:
                self.failure_streak += 1
            else:
                self.failure_streak = 0
            opponent_cooperation_estimate = last_round_opponent_cooperators
            if opponent_cooperation_estimate >= m_threshold:
                return Action.D
            elif opponent_cooperation_estimate == m_threshold - 1:
                return Action.C
            elif opponent_cooperation_estimate < m_threshold - 1:
                if self.failure_streak >= 2:
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_361(BaseStrategy):
    """
    The Opportunistic Coercion strategy aims to maximize individual payoff in the Collective Risk Dilemma.
    It defaults to defection, but strategically cooperates when its action is critical to securing
    the collective reward 'k', or to "coercively" kickstart cooperation after a streak of failures.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.failed_threshold_streak = 0
        self.S_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + int(history.my_actions[prev_round_idx])
        self.failed_threshold_streak = 0
        for i in range(prev_round_idx, -1, -1):
            cooperators_in_round_i = np.sum(history.opponent_actions[i, :]) + int(history.my_actions[i])
            if cooperators_in_round_i < m:
                self.failed_threshold_streak += 1
            else:
                break
        if num_cooperators_prev_round > m:
            return Action.D
        if num_cooperators_prev_round == m or num_cooperators_prev_round == m - 1:
            return Action.C
        if num_cooperators_prev_round < m - 1:
            if self.failed_threshold_streak >= self.S_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_362(BaseStrategy):
    """
    The Opportunistic Catalyst strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily free-riding.
    It defects in the first and last rounds. In intermediate rounds, it defects
    if the collective goal was met previously (to free-ride on others' cooperation).
    If the collective goal was not met, it defects if its previous cooperation was futile,
    or if its current cooperation wouldn't be pivotal. It only cooperates if it defected
    in the previous round (and the goal wasn't met) AND its cooperation would be exactly
    what's needed to reach the threshold 'm'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        action_for_this_round: Action
        if current_round_1_indexed == 1:
            action_for_this_round = Action.D
        elif current_round_1_indexed == n_rounds:
            action_for_this_round = Action.D
        else:
            my_action_in_prev_round_is_C = history.my_actions[-1]
            num_cooperators_prev_round_among_opponents = np.sum(history.opponent_actions[-1, :])
            num_cooperators_prev_round = num_cooperators_prev_round_among_opponents + (1 if my_action_in_prev_round_is_C else 0)
            m_met_prev = num_cooperators_prev_round >= m_threshold
            if m_met_prev:
                action_for_this_round = Action.D
            elif my_action_in_prev_round_is_C:
                action_for_this_round = Action.D
            elif num_cooperators_prev_round == m_threshold - 1:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_363(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily defecting
    and free-riding on others' cooperation. It adapts by cooperating for
    a single round after the first collective failure to re-establish the
    collective reward, then reverts to defection if failures persist, or
    if the last round is reached.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.consecutive_failures = 0
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if total_cooperators_prev_round >= self.m:
            self.consecutive_failures = 0
            return Action.D
        else:
            self.consecutive_failures += 1
            if self.consecutive_failures == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_364(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to free-ride on established
    cooperation. It attempts to initiate cooperation in the first round. If the collective
    cooperation threshold 'm' is met in a round, it defects in the subsequent round to
    maximize individual payoff. If 'm' is not met, it tolerates one consecutive failure
    to meet the threshold before switching to a pure defecting strategy, avoiding being
    a "sucker" in non-cooperative environments. In the final round, it always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_threshold_met: int = 0
        self.defection_tolerance_threshold: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round == total_rounds - 1:
            return Action.D
        if current_round == 0:
            self.rounds_since_threshold_met = 0
            return Action.C
        cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if cooperators_in_prev_round >= min_cooperators_m:
            self.rounds_since_threshold_met = 0
            return Action.D
        else:
            self.rounds_since_threshold_met += 1
            if self.rounds_since_threshold_met < self.defection_tolerance_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_365(BaseStrategy):
    """
    Adaptive Exploitation strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by predominantly free-riding on the cooperation of others,
    strategically cooperating only when its contribution is likely to be critical for the group to achieve the
    collective reward `k`, thus securing the bonus for itself. It adapts to the observed behavior of opponents
    and the collective success rate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_idx = state.round_number
        if current_round_idx == r - 1:
            return Action.D
        if current_round_idx == 0:
            return Action.D
        else:
            cooperators_in_round_history = []
            for j in range(current_round_idx):
                my_action_in_j = history.my_actions[j]
                opponent_actions_in_j = history.opponent_actions[j, :]
                total_cooperators_in_j = int(my_action_in_j) + np.sum(opponent_actions_in_j)
                cooperators_in_round_history.append(total_cooperators_in_j)
            W = max(3, min(current_round_idx, math.floor(n / 2) + 1))
            successful_rounds_in_window = 0
            for j in range(max(0, current_round_idx - W), current_round_idx):
                if cooperators_in_round_history[j] >= m:
                    successful_rounds_in_window += 1
            success_rate = float(successful_rounds_in_window) / W
            prev_cooperators = cooperators_in_round_history[-1]
            if success_rate >= 0.7:
                return Action.D
            elif prev_cooperators == m - 1:
                return Action.C
            elif 0.3 < success_rate < 0.7:
                if prev_cooperators >= m:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_366(BaseStrategy):
    """
    Implements the "Calculated Free-Rider with Re-Seeding" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by primarily free-riding (defecting),
    only cooperating when the collective project consistently fails, to "re-seed" cooperation
    and preserve the opportunity for future free-riding. It always defects in the final round
    due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.F_threshold = 2
        self.failed_streak = 0
        self.consecutive_successes = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if state.round_number == 0:
            return Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
        if cooperators_prev_round >= self.m_threshold:
            self.consecutive_successes += 1
            self.failed_streak = 0
        else:
            self.failed_streak += 1
            self.consecutive_successes = 0
        if current_round_t == self.n_rounds:
            return Action.D
        if cooperators_prev_round >= self.m_threshold:
            return Action.D
        elif self.failed_streak >= self.F_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_367(BaseStrategy):
    """
    Adaptive Opportunist with Critical Contribution Awareness strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily free-riding on the cooperation of others.
    It strategically cooperates only when necessary to trigger or maintain the collective reward 'k',
    ensuring it always acts in its own best interest, even when contributing.

    Key Principles:
    1.  Default to Defect (Free-Ride): The primary mode of operation is to defect.
    2.  Strategic Cooperation as an Investment: Cooperates when the collective reward 'k' is at risk
        of not being achieved or maintained, and my contribution is essential to restore it,
        enabling future free-riding opportunities.
    3.  End-Game Exploitation: Leverages the finite nature of the game by defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing game parameters
                                                         (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on game state and history.

        Args:
            state (GameState): Contains the current round number.
            history (None | PlayerHistory): Contains past actions and payoffs.
                                            None for the very first round (round_number == 0).

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_round = history.my_actions[prev_round_idx]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev_round = my_action_prev_round + opponent_cooperators_prev_round
        if num_cooperators_prev_round < m_threshold:
            return Action.C
        else:
            num_cooperators_others_prev_round = num_cooperators_prev_round - my_action_prev_round
            if num_cooperators_others_prev_round >= m_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_368(BaseStrategy):
    """
    The Calculated Exploiter strategy aims to free-ride whenever possible,
    cooperating only when pivotal or as a strategic investment to restore
    conditions for future exploitation. It tracks consecutive failures to adapt.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.consecutive_failure_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.consecutive_failure_rounds = 0
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
        if num_cooperators_last_round >= self.m:
            self.consecutive_failure_rounds = 0
            return Action.D
        else:
            self.consecutive_failure_rounds += 1
            if num_cooperators_last_round == self.m - 1:
                return Action.C
            elif self.consecutive_failure_rounds >= 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_369(BaseStrategy):
    """
    Adaptive Free-Rider with Despair and Last Round Exploit strategy for the Collective Risk Dilemma.
    This strategy aggressively free-rides when the collective threshold is met, strategically
    cooperates to initiate or rescue collective efforts when necessary, employs a despair mechanism
    to cut losses from futile cooperation, and exploits the last round by always defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_consecutive_failed_C: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round == total_rounds - 1:
            return Action.D
        if current_round == 0:
            return Action.C
        previous_round_my_action: bool = history.my_actions[-1]
        previous_round_opponent_cooperators: int = np.sum(history.opponent_actions[-1, :])
        previous_round_total_cooperators: int = previous_round_opponent_cooperators + int(previous_round_my_action)
        if not previous_round_my_action:
            self.my_consecutive_failed_C = 0
        elif previous_round_total_cooperators >= min_cooperators:
            self.my_consecutive_failed_C = 0
        else:
            self.my_consecutive_failed_C += 1
        if self.my_consecutive_failed_C >= 2:
            return Action.D
        if previous_round_total_cooperators >= min_cooperators:
            return Action.D
        else:
            num_cooperators_if_i_cooperate = previous_round_total_cooperators + 1
            if num_cooperators_if_i_cooperate < min_cooperators:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_370(BaseStrategy):
    """
    Adaptive Trigger-Happy Free-Rider (ATHFR) strategy for the Collective Risk Dilemma.

    This strategy is designed to be exploitative, prioritizing free-riding,
    avoiding "sucker" payoffs, and making strategic contributions only when they
    are likely to unlock or maintain collective benefits that can later be exploited.
    It adapts to observed opponent behavior and aims for maximal personal total payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def _get_total_cooperators_in_round(self, history: PlayerHistory, round_idx: int) -> int:
        """
        Calculates the total number of cooperators (including this player) for a given 0-indexed round.
        """
        opponent_cooperators = np.sum(history.opponent_actions[round_idx, :])
        my_action_was_cooperate = history.my_actions[round_idx]
        return int(opponent_cooperators + (1 if my_action_was_cooperate else 0))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        elif current_round_1_indexed == 1:
            return Action.D
        if history is None:
            return Action.D
        elif current_round_1_indexed == 2:
            N_C_R1 = self._get_total_cooperators_in_round(history, 0)
            if N_C_R1 >= self.m_threshold:
                return Action.D
            else:
                return Action.C
        else:
            N_C_prev = self._get_total_cooperators_in_round(history, current_round_0_indexed - 1)
            if N_C_prev >= self.m_threshold:
                return Action.D
            elif N_C_prev == self.m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_371(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy is designed to exploit the collective risk dilemma by maximizing individual payoff,
    primarily through defection, while strategically cooperating only when it is critically necessary to trigger the
    collective reward, which then allows for further exploitation. It adapts to the observed behavior of other players
    and is robust against various opponent types.

    Decision Rules:
    1.  Round 1 (Initial Probe): Defect (D) - No history, test waters, maximize baseline payoff.
    2.  Middle Rounds (2 to r-1):
        - If collective threshold 'm' was met in the previous round: Defect (D) - Free-ride on others' cooperation.
        - If collective threshold 'm' was NOT met:
            - If exactly one cooperator short (C_prev == m - 1): Cooperate (C) - Critical contribution to meet threshold,
              aiming for k payoff this round and potential future free-riding.
            - If more than one cooperator short (C_prev < m - 1): Defect (D) - My cooperation alone is insufficient,
              avoid costly contribution for no return.
    3.  Last Round (r): Defect (D) - No future rounds, maximize immediate payoff as D is dominant.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        prev_round_history_index = state.round_number - 1
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_history_index, :])
        my_action_prev = history.my_actions[prev_round_history_index]
        C_prev = opponent_cooperators_prev + my_action_prev
        if C_prev >= self.m:
            return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_372(BaseStrategy):
    """
    The "Opportunistic Free-Rider v2" strategy is designed to be highly exploitative and adaptive
    in the Collective Risk Dilemma. It prioritizes maximizing its own payoff in every round,
    free-riding whenever possible, and only contributing when it is strictly in its self-interest
    to do so, even if it means letting the collective outcome fail for others.

    It operates in three phases:
    1.  **First Round:** Defaults to Defect to probe other players' behavior and avoid being a "sucker".
    2.  **Last Round:** Always Defects, as there are no future interactions to incentivize cooperation.
    3.  **Intermediate Rounds:** Adaptively decides based on the previous round's outcome:
        -   If the threshold was met, it defects if its contribution was not essential, otherwise,
            it cooperates only if its payoff from cooperating (k-1) is greater than or equal to
            its payoff from defecting and letting the threshold fail (1).
        -   If the threshold was not met, it defects to avoid a payoff of 0.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        else:
            previous_round_0_indexed = state.round_number - 1
            my_previous_action = history.my_actions[previous_round_0_indexed]
            previous_round_opponent_actions = history.opponent_actions[previous_round_0_indexed, :]
            num_others_cooperating_prev = np.sum(previous_round_opponent_actions)
            num_cooperators_prev = num_others_cooperating_prev + (1 if my_previous_action else 0)
            if num_cooperators_prev >= self.m:
                if num_others_cooperating_prev >= self.m:
                    return Action.D
                elif self.k - 1 >= 1:
                    return Action.C
                else:
                    return Action.D
            elif num_cooperators_prev < self.m:
                return Action.D

class Strategy_EXPLOITATIVE_373(BaseStrategy):
    """
    The "Opportunistic Pumper" strategy for the Collective Risk Dilemma.

    This strategy aims to free-ride as much as possible, only contributing to the
    collective project when its cooperation is both critical and sufficient to
    secure the collective reward (k > 1), leveraging the end-game effect.

    Key Principles:
    1. Default to Defection (Free-Riding): Whenever possible, play Defect.
    2. Conditional Cooperation (Pump-Priming): Cooperate only when the collective
       project is on the brink of failure (total cooperators in previous round < m)
       AND my cooperation alone is sufficient to tip it over the threshold (total
       cooperators in previous round == m-1). In this specific case, cooperating
       yields a net payoff of 'k' for the round, which is better than '1' (from
       defecting when the project fails).
    3. Exploit the Last Round: Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the strategy rules.
        """
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        current_round_index = state.round_number
        if current_round_index == total_rounds - 1:
            return Action.D
        if current_round_index == 0:
            return Action.D
        cooperators_in_prev_round = np.sum(history.opponent_actions[current_round_index - 1, :]) + (1 if history.my_actions[current_round_index - 1] else 0)
        if cooperators_in_prev_round >= min_cooperators_needed:
            return Action.D
        else:
            needed_cooperators_to_meet_threshold = min_cooperators_needed - cooperators_in_prev_round
            if needed_cooperators_to_meet_threshold == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_374(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by strategically free-riding on
    others' cooperation. It defects in the first and last rounds. In intermediate
    rounds, it defects if enough others cooperate (pure free-riding) or if cooperation
    is futile. It only cooperates if its contribution is critically needed to meet
    the threshold 'm' and unlock the collective reward 'k', as 'k' > 1 makes this
    a self-beneficial action.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        previous_round_idx_0_indexed = state.round_number - 1
        C_others_prev = np.sum(history.opponent_actions[previous_round_idx_0_indexed, :])
        if C_others_prev >= m:
            return Action.D
        elif C_others_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_375(BaseStrategy):
    """
    Opportunistic Piranha: A highly exploitative strategy for the Collective Risk Dilemma.
    It prioritizes free-riding by defaulting to defection, observing others' cooperation.
    It only cooperates as a calculated, self-serving gamble when its single contribution
    can precisely tip the scales to meet the 'm' threshold, thereby securing the 'k' reward
    for itself (and others). It avoids 'sucker' payoffs by defecting if its previous
    cooperation failed to meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_number == 0:
            return Action.D
        if current_round_number == total_rounds - 1:
            return Action.D
        prev_round_idx = current_round_number - 1
        my_action_prev_was_C = history.my_actions[prev_round_idx]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        cooperators_count_prev = num_opponent_cooperators_prev
        if my_action_prev_was_C:
            cooperators_count_prev += 1
        if cooperators_count_prev >= m_threshold:
            return Action.D
        elif my_action_prev_was_C:
            return Action.D
        elif cooperators_count_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_376(BaseStrategy):
    """
    The Strategic Catalyst is an exploitative strategy for the Collective Risk Dilemma.
    It primarily free-rides, defecting by default. It strategically cooperates only
    when its contribution is critically necessary to meet the collective reward
    threshold 'm' AND doing so is individually profitable (i.e., when k > 2.0).
    It defects in the first and last rounds to maximize immediate payoff, as
    there's no past history to react to or future rounds to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        my_action_t_minus_1 = history.my_actions[previous_round_idx]
        total_opponent_cooperators_t_minus_1 = np.sum(history.opponent_actions[previous_round_idx, :])
        N_C_t_minus_1 = total_opponent_cooperators_t_minus_1 + (1 if my_action_t_minus_1 else 0)
        N_C_others_t_minus_1 = total_opponent_cooperators_t_minus_1
        if N_C_t_minus_1 >= self.game_description.m:
            return Action.D
        elif N_C_others_t_minus_1 == self.game_description.m - 1:
            if self.game_description.k > 2.0:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_377(BaseStrategy):
    """
    Resilient Free-Rider (RFR) strategy for the Collective Risk Dilemma.

    This strategy aggressively attempts to free-ride, but is also pragmatic. It will
    strategically cooperate as a last resort to "kick-start" collective action when
    consistent failure threatens its own overall payoff. It quickly reverts to defection
    if its cooperation proves futile or if the situation appears genuinely hopeless.

    Core Principles:
    1.  Default to Defection: Always try to defect.
    2.  Exploit Success: If the collective threshold was met, continue to free-ride.
    3.  Strategic Intervention: Cooperate only if the group has consistently failed to meet
        the threshold for a short period, to try and re-establish the collective reward.
    4.  Resilient to Futility: If my cooperation did not lead to collective success, revert
        to defection immediately.
    5.  Hopeless Scenario Detection: If collective failure persists for too long, revert
        to permanent defection to avoid pointless contributions.
    6.  End-Game Exploitation: Always defect in the last round.
    7.  First-Round Probe: Always defect in the first round to test for free-riding opportunities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_since_last_success: int = 0
        self.my_last_action_was_C: bool = False
        self.my_cooperation_was_futile: bool = False
        self.intervention_threshold: int = 2
        self.hopeless_threshold: int = max(3, self.game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        action_for_this_round = Action.D
        if current_round_number > 0:
            prev_round_idx = current_round_number - 1
            my_action_prev_bool = history.my_actions[prev_round_idx]
            C_prev_opponents = np.sum(history.opponent_actions[prev_round_idx, :])
            C_prev_total = C_prev_opponents + (1 if my_action_prev_bool else 0)
            if my_action_prev_bool and C_prev_total < m_threshold:
                self.my_cooperation_was_futile = True
            else:
                self.my_cooperation_was_futile = False
            if C_prev_total >= m_threshold:
                self.rounds_since_last_success = 0
            else:
                self.rounds_since_last_success += 1
        if current_round_number == 0:
            action_for_this_round = Action.D
        elif current_round_number == n_rounds - 1:
            action_for_this_round = Action.D
        elif self.my_cooperation_was_futile:
            action_for_this_round = Action.D
        elif self.rounds_since_last_success >= self.hopeless_threshold:
            action_for_this_round = Action.D
        elif self.rounds_since_last_success == 0:
            action_for_this_round = Action.D
        elif self.rounds_since_last_success >= self.intervention_threshold:
            action_for_this_round = Action.C
        else:
            action_for_this_round = Action.D
        self.my_last_action_was_C = action_for_this_round == Action.C
        return action_for_this_round

class Strategy_EXPLOITATIVE_378(BaseStrategy):
    """
    The Adaptive Free-Rider (A.F.R.) strategy for the Collective Risk Dilemma.
    It aims to maximize individual payoff by exploiting the collective's need for cooperation
    while minimizing personal contribution. It leverages historical data to make adaptive,
    self-interested decisions, and explicitly accounts for the game's finite horizon.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev = history.my_actions[-1]
        num_my_cooperation_prev = 1 if my_action_prev else 0
        N_C_prev = num_opponent_cooperators_prev + num_my_cooperation_prev
        if N_C_prev >= self.m:
            return Action.D
        else:
            N_needed_for_m = self.m - N_C_prev
            if N_needed_for_m == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_379(BaseStrategy):
    """
    Adaptive Free-Rider with Patience (AFRP) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff over 'r' rounds by strategically
    free-riding on others' cooperation whenever possible, while contributing only
    when necessary to achieve the collective 'k' reward and avoid prolonged,
    unrewarded cooperation. It adapts its behavior based on past outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.patience_threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.r - 1:
            return Action.D
        prev_round_idx = current_round_number - 1
        my_prev_action = history.my_actions[prev_round_idx]
        C_prev = int(my_prev_action) + np.sum(history.opponent_actions[prev_round_idx, :])
        unrewarded_cooperation_streak_length = 0
        for i in range(prev_round_idx, -1, -1):
            my_action_in_round_i = history.my_actions[i]
            num_coop_in_round_i = int(my_action_in_round_i) + np.sum(history.opponent_actions[i, :])
            if my_action_in_round_i == Action.C and num_coop_in_round_i < self.m:
                unrewarded_cooperation_streak_length += 1
            else:
                break
        if C_prev >= self.m:
            if C_prev > self.m:
                return Action.D
            elif my_prev_action == Action.C:
                return Action.C
            else:
                return Action.D
        elif unrewarded_cooperation_streak_length >= self.patience_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_380(BaseStrategy):
    """
    An exploitative, adaptive, and robust strategy for the Collective Risk Dilemma.
    It defaults to defecting, free-rides when possible, and only cooperates when its
    contribution is critical to meet the threshold and personally beneficial (k > 1).
    It explicitly leverages the endgame effect by always defecting in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        prev_round_index = current_round_0_indexed - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :])
        my_action_prev_round_was_C = history.my_actions[prev_round_index]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if num_cooperators_prev_round >= self.m:
            return Action.D
        elif num_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_381(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to maximize its own total payoff.
    It probes for cooperation, strategically contributes when its action is critical
    and profitable to ensure the 'k' bonus, and aggressively free-rides or defects
    when collective cooperation is sufficient or consistently insufficient.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_actions_history: list[Action] = []
        self.N_c_history: list[int] = []
        self.rounds_since_last_success: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if state.round_number > 0:
            my_action_prev_actual_bool = history.my_actions[-1]
            my_action_prev_actual = Action.C if my_action_prev_actual_bool else Action.D
            n_others_c_prev_actual = sum(history.opponent_actions[-1, :])
            n_cooperators_prev_actual = n_others_c_prev_actual + (1 if my_action_prev_actual == Action.C else 0)
            self.my_actions_history.append(my_action_prev_actual)
            self.N_c_history.append(n_cooperators_prev_actual)
            if n_cooperators_prev_actual >= m_threshold:
                self.rounds_since_last_success = 0
            else:
                self.rounds_since_last_success += 1
        else:
            self.rounds_since_last_success = 0
        action_to_take: Action
        if current_round_1_indexed == 1:
            action_to_take = Action.C
        elif current_round_1_indexed == n_rounds:
            action_to_take = Action.D
        else:
            N_c_prev = self.N_c_history[state.round_number - 1]
            my_action_prev = self.my_actions_history[state.round_number - 1]
            N_others_C_prev = N_c_prev - (1 if my_action_prev == Action.C else 0)
            FAILURE_TOLERANCE_THRESHOLD = 2
            if N_c_prev >= m_threshold:
                if N_others_C_prev >= m_threshold:
                    action_to_take = Action.D
                else:
                    action_to_take = Action.D
            elif N_c_prev == m_threshold - 1:
                action_to_take = Action.C
            elif self.rounds_since_last_success >= FAILURE_TOLERANCE_THRESHOLD:
                action_to_take = Action.D
            else:
                action_to_take = Action.D
        return action_to_take

class Strategy_EXPLOITATIVE_382(BaseStrategy):
    """
    Adaptive Free-Rider with Proactive Incentive strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by dynamically responding to the
    cooperative behavior of other players. It starts with an initial cooperative
    stance to encourage collective action, then seeks to free-ride when possible
    without jeopardizing the collective reward 'k'. It only contributes when its
    cooperation is critical and beneficial to achieve the 'm' threshold, and
    protects its endowment when cooperation would be futile.

    Core Principles:
    1.  **Proactive Initialization:** Cooperate in Round 1 to "prime the pump" and establish
        the potential for the collective reward 'k'.
    2.  **Conditional Free-riding:** Defect whenever the threshold 'm' is met (or would be met)
        by others' cooperation alone, allowing the strategy to reap 'k' without contributing.
    3.  **Minimum Viable Cooperation:** Cooperate only when its contribution is *critical* to
        reaching the 'm' threshold, and doing so yields a higher payoff ('k') than defecting ('1').
    4.  **Endowment Protection:** Defect if its cooperation is insufficient to meet the 'm'
        threshold, avoiding a wasted contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators_from_others_in_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        if cooperators_from_others_in_prev_round >= self.m:
            return Action.D
        elif cooperators_from_others_in_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_383(BaseStrategy):
    """
    The Adaptive Exploitative Cooperator (AEC) strategy aims to maximize individual payoff
    by consistently attempting to free-ride on the cooperation of others. It only reluctantly
    cooperates as a "last resort" measure to prevent prolonged periods where the community
    reward 'k' is consistently missed, after which it will immediately revert to free-riding.
    The strategy is adaptive to observed collective behavior and robust by reacting to
    aggregate outcomes rather than specific player identities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.F_max: int = self.game_description.m - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            self.consecutive_failures = 0
            return Action.D
        if current_t == n_rounds:
            return Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_prev_round >= m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if self.consecutive_failures <= self.F_max:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_384(BaseStrategy):
    """
    Smart Opportunistic Nudger strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by primarily free-riding,
    but intelligently intervenes with temporary cooperation ("nudges") when the
    collective good is at risk and its contribution could be impactful. It is
    designed to be highly adaptive to the group's overall cooperation levels
    and to avoid futile sacrifices.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        action_for_current_round: Action
        if current_round_1_indexed == 1:
            action_for_current_round = Action.D
        elif current_round_1_indexed == total_rounds:
            action_for_current_round = Action.D
        else:
            num_cooperators_last_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            if num_cooperators_last_round >= m_threshold:
                action_for_current_round = Action.D
            elif num_cooperators_last_round >= m_threshold - 1:
                if self.my_last_action == Action.D:
                    action_for_current_round = Action.C
                else:
                    action_for_current_round = Action.D
            else:
                action_for_current_round = Action.D
        self.my_last_action = action_for_current_round
        return action_for_current_round

class Strategy_EXPLOITATIVE_385(BaseStrategy):
    """
    Adaptive Free-Rider with Threshold Enforcement (AFR-TE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by defaulting to defection (free-riding)
    whenever possible. It strategically cooperates only when its contribution is critical
    to meet the collective threshold 'm' and secure the collective reward 'k', thereby
    benefiting itself. It avoids futile cooperation that would not lead to the threshold
    being met.

    Decision Rules:
    1.  First Round (0-indexed round 0): Defect (D) to probe opponent behavior and attempt free-riding.
    2.  Last Round (0-indexed round n_rounds - 1): Defect (D) as there are no future consequences.
    3.  Intermediate Rounds:
        a.  If the cooperation threshold 'm' was met in the previous round: Defect (D) to free-ride.
        b.  If the cooperation threshold 'm' was NOT met in the previous round:
            i.  If our player's cooperation would cause the threshold to be met: Cooperate (C).
            ii. If even with our player's cooperation, the threshold would still NOT be met: Defect (D)
                to avoid futile sacrifice.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0 or current_round_0_indexed == n_rounds - 1:
            return Action.D
        previous_round_history_index = current_round_0_indexed - 1
        opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_history_index])
        my_action_prev_was_cooperate = history.my_actions[previous_round_history_index]
        N_C_prev = opponent_cooperators_prev + (1 if my_action_prev_was_cooperate else 0)
        if N_C_prev >= m_threshold:
            return Action.D
        elif N_C_prev + 1 >= m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_386(BaseStrategy):
    """
    Adaptive Free-Rider with a Punishing Hand (AFR-PH) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual total payoff by primarily free-riding,
    strategically cooperating when necessary to secure collective reward, and punishing
    repeated failures to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.failure_streak: int = 0
        self.cooperation_streak: int = 0
        self.defection_penalty_active: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        chosen_action: Action
        if current_round_1_indexed == 1:
            chosen_action = Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            chosen_action = Action.D
        else:
            C_previous_round = np.sum(history.opponent_actions[-1, :]) + (1 if self.my_last_action == Action.C else 0)
            if C_previous_round >= self.game_description.m:
                self.cooperation_streak += 1
                self.failure_streak = 0
                self.defection_penalty_active = False
            else:
                self.failure_streak += 1
                self.cooperation_streak = 0
                self.defection_penalty_active = self.failure_streak >= 2
            if C_previous_round >= self.game_description.m:
                chosen_action = Action.D
            elif C_previous_round == self.game_description.m - 1:
                chosen_action = Action.C
            elif self.defection_penalty_active:
                chosen_action = Action.D
            else:
                chosen_action = Action.D
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_387(BaseStrategy):
    """
    The "Dynamic Free-Rider with Adaptive Threshold Support" strategy aims to maximize
    its individual payoff by primarily free-riding on the contributions of other players.
    It is designed to be exploitative, only contributing to the collective good when it is
    strategically necessary to trigger the collective reward (`k`) and with the ultimate
    goal of resuming free-riding. The strategy is adaptive, learning from past rounds,
    and robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.Probing_Phase_Length = min(self.n_players, max(2, math.floor(self.n_rounds / 4)))
        self.Forgiveness_Window = 3
        self.our_action_last_round: bool = False
        self.cooperators_last_round: int = 0
        self.consecutive_failures_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t == self.n_rounds:
            return Action.D
        self.our_action_last_round = history.my_actions[state.round_number - 1]
        my_cooperation_count = 1 if self.our_action_last_round else 0
        opponent_cooperation_count = np.sum(history.opponent_actions[state.round_number - 1, :])
        self.cooperators_last_round = my_cooperation_count + opponent_cooperation_count
        if current_round_t <= self.Probing_Phase_Length:
            if self.cooperators_last_round >= self.m:
                self.consecutive_failures_count = 0
                return Action.D
            elif self.our_action_last_round == Action.C:
                self.consecutive_failures_count = 0
                return Action.D
            else:
                self.consecutive_failures_count += 1
                if self.cooperators_last_round == self.m - 1:
                    self.consecutive_failures_count = 0
                    return Action.C
                else:
                    return Action.D
        elif self.cooperators_last_round >= self.m:
            self.consecutive_failures_count = 0
            return Action.D
        elif self.our_action_last_round == Action.C:
            self.consecutive_failures_count = 0
            return Action.D
        else:
            self.consecutive_failures_count += 1
            if self.consecutive_failures_count >= self.Forgiveness_Window or self.cooperators_last_round == self.m - 1:
                self.consecutive_failures_count = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_388(BaseStrategy):
    """
    Adaptive Risk-Averse Free-Rider: Primarily defects to free-ride, but strategically cooperates
    when collective failure to meet the 'm' threshold is persistent or 'k' is high, and there are
    sufficient future rounds to benefit, aiming to restore conditions for future free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_t = state.round_number
        total_cooperators_prev_round = 0
        opponent_cooperators_prev_round = 0
        if current_round_t > 0:
            previous_round_idx = current_round_t - 1
            total_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :]) + int(history.my_actions[previous_round_idx])
            if total_cooperators_prev_round >= m_threshold:
                self._consecutive_failures = 0
            else:
                self._consecutive_failures += 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        if current_round_t == 0:
            return Action.D
        if current_round_t == n_rounds - 1:
            return Action.D
        rounds_remaining = n_rounds - (current_round_t + 1)
        if total_cooperators_prev_round >= m_threshold:
            return Action.D
        else:
            my_cooperation_would_help_meet_threshold = opponent_cooperators_prev_round + 1 >= m_threshold
            is_failure_persistent = self._consecutive_failures >= 2
            is_k_highly_rewarding = k_reward_factor >= 2.0
            is_worth_investing = rounds_remaining >= 2
            if my_cooperation_would_help_meet_threshold and (is_failure_persistent or is_k_highly_rewarding) and is_worth_investing:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_389(BaseStrategy):
    """
    The Adaptive Manipulator strategy balances aggressive free-riding with strategic
    cooperation to sustain the collective benefit in the Collective Risk Dilemma.
    It aims to exploit cooperative players while subtly encouraging a healthy level
    of overall cooperation to ensure the 'k' bonus is consistently achieved,
    acting as an 'exploiter with a conscience'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.defection_streak_limit = 2
        self.cooperation_recovery_rounds = 2
        self.my_last_action = None
        self.my_defection_streak = 0
        self.rounds_since_last_failure = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.my_last_action = Action.C
            return Action.C
        prev_my_action_bool = history.my_actions[-1]
        prev_num_cooperators = sum(history.opponent_actions[-1, :]) + (1 if prev_my_action_bool else 0)
        was_threshold_met_prev_round = prev_num_cooperators >= self.m
        if was_threshold_met_prev_round:
            self.rounds_since_last_failure += 1
            if not prev_my_action_bool:
                self.my_defection_streak += 1
            else:
                self.my_defection_streak = 0
        else:
            self.rounds_since_last_failure = 0
            self.my_defection_streak = 0
        if state.round_number == self.r - 1:
            self.my_last_action = Action.D
            return Action.D
        decision_action = None
        if not was_threshold_met_prev_round:
            decision_action = Action.C
        else:
            threshold_slack = prev_num_cooperators - self.m
            cooperate_for_low_buffer = threshold_slack <= (self.n - self.m) / 2.0
            cooperate_for_streak_limit = self.my_defection_streak >= self.defection_streak_limit
            cooperate_for_recovery = self.rounds_since_last_failure <= self.cooperation_recovery_rounds
            if cooperate_for_low_buffer or cooperate_for_streak_limit or cooperate_for_recovery:
                decision_action = Action.C
            else:
                decision_action = Action.D
        self.my_last_action = decision_action
        return decision_action

class Strategy_EXPLOITATIVE_390(BaseStrategy):
    """
    Adaptive Conditional Free-Rider (ACFR) strategy for the Collective Risk Dilemma.

    This strategy aims to free-ride as much as possible, always defaulting to defection.
    However, it strategically cooperates if previous attempts at free-riding have
    resulted in the collective 'k' bonus being missed, or if its own prior cooperation
    proved futile. The goal is to always capture the '1+k' maximum individual payoff
    when the threshold 'm' is met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters like n_players,
                                                          n_rounds, m (min_cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Conditional Free-Rider strategy.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing the history of actions
                                            and payoffs for the player and opponents
                                            up to the *previous* round. It is None for the first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_idx == 0:
            return Action.D
        elif current_round_idx == total_rounds - 1:
            return Action.D
        else:
            my_action_in_prev_round_is_C = history.my_actions[-1]
            opponent_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_in_prev_round = opponent_cooperators_in_prev_round + my_action_in_prev_round_is_C
            if total_cooperators_in_prev_round >= min_cooperators_needed:
                return Action.D
            elif not my_action_in_prev_round_is_C:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_391(BaseStrategy):
    """
    The Opportunistic Kicker strategy for the Collective Risk Dilemma.
    It defaults to free-riding (Defect), but is designed to temporarily "kickstart" cooperation
    when the collective failure to meet the threshold 'm' consistently denies players the 'k' reward.
    Once the threshold is met, it immediately reverts to free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.C_prev_round: int = -1
        self.failed_to_meet_threshold_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t > 1:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
            my_action_prev_round_is_C = history.my_actions[state.round_number - 1]
            self.C_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
            if self.C_prev_round >= self.game_description.m:
                self.failed_to_meet_threshold_count = 0
            else:
                self.failed_to_meet_threshold_count += 1
        if current_round_t == 1:
            return Action.D
        elif current_round_t == self.game_description.n_rounds:
            return Action.D
        elif self.C_prev_round >= self.game_description.m:
            return Action.D
        elif self.C_prev_round == self.game_description.m - 1:
            return Action.D
        elif self.failed_to_meet_threshold_count >= 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_392(BaseStrategy):
    """
    The Adaptive Free-Rider with Cooperation Maintenance (AFR-CM) strategy.

    This strategy aims to maximize its own total payoff by exploiting cooperative behavior
    while strategically contributing to maintain a cooperative environment. It free-rides
    when the collective reward threshold is met, contributes only when its action is
    critical to meet the threshold, and defects to cut losses when cooperation is too low.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        if current_round_idx == 0:
            return Action.C
        num_opponents_cooperated_prev_round = sum(history.opponent_actions[current_round_idx - 1, :])
        my_action_prev_round_was_cooperate = history.my_actions[current_round_idx - 1]
        num_cooperators_prev_round = num_opponents_cooperated_prev_round + (1 if my_action_prev_round_was_cooperate else 0)
        if num_cooperators_prev_round >= self.m:
            return Action.D
        elif num_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_393(BaseStrategy):
    """
    Adaptive Free-Rider with Hysteresis (AFRH) strategy for the Collective Risk Dilemma.
    AFRH aims to maximize its total payoff by free-riding (playing Defect) whenever it
    anticipates that the collective cooperation threshold (`m`) will be met by other players.
    It will only cooperate strategically if its contribution is crucial to achieving or
    maintaining the collective reward, and only when such cooperation is deemed a worthwhile
    investment for future free-riding opportunities. It incorporates "hysteresis"
    through the `rounds_since_success` counter to avoid overly rapid shifts in behavior,
    aiming for stability in exploitation. It quickly disengages from collectives
    that consistently fail to meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        last_round_idx = current_round - 1
        my_last_action_is_C = history.my_actions[last_round_idx]
        cooperators_last_round_others = np.sum(history.opponent_actions[last_round_idx, :])
        total_cooperators_last_round = cooperators_last_round_others + (1 if my_last_action_is_C else 0)
        rounds_since_success = 0
        for r_idx in range(last_round_idx, -1, -1):
            round_my_action_is_C = history.my_actions[r_idx]
            round_cooperators_others = np.sum(history.opponent_actions[r_idx, :])
            round_total_cooperators = round_cooperators_others + (1 if round_my_action_is_C else 0)
            if round_total_cooperators < m:
                rounds_since_success += 1
            else:
                break
        if cooperators_last_round_others >= m:
            return Action.D
        if cooperators_last_round_others == m - 1:
            return Action.C
        if total_cooperators_last_round >= m:
            return Action.C
        elif rounds_since_success >= 2:
            return Action.D
        elif cooperators_last_round_others >= m / 2.0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_394(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily free-riding on the
    cooperation of others, while adaptively cooperating only when strategically
    necessary to re-establish the collective reward and prevent a prolonged "bust" state.

    Core Principle: Prioritize defection to achieve the 1+k payoff whenever the
    collective reward is active or expected to be active. If the collective reward (k)
    is lost, strategically cooperate with minimal personal cost to reactivate it,
    then revert to free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        elif current_round_t == n_rounds:
            return Action.D
        else:
            previous_round_index = state.round_number - 1
            num_opponent_cooperators_last_round = np.sum(history.opponent_actions[previous_round_index, :])
            my_action_last_round_was_C = history.my_actions[previous_round_index]
            N_C_last = num_opponent_cooperators_last_round + (1 if my_action_last_round_was_C else 0)
            if N_C_last >= m_threshold:
                return Action.D
            elif N_C_last == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_395(BaseStrategy):
    """
    The "Opportunistic Exploiter" strategy is designed to maximize personal payoff in the
    Collective Risk Dilemma by primarily free-riding, but adapting to collective behavior
    to ensure the 'k' reward is obtained when strategically beneficial and cheap to do so.
    It prioritizes securing the 'k' reward for itself while minimizing personal cost, and
    will revert to pure defection if collective action is consistently insufficient or in the endgame.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Opportunistic Exploiter strategy with the game's parameters.
        No additional internal state is maintained beyond the game description, as
        past round information is retrieved from the `history` parameter in `__call__`.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Opportunistic Exploiter strategy rules.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for
                     this player and opponents, or None if it's the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_prev_round >= m:
            return Action.D
        else:
            needed_cooperators = m - total_cooperators_prev_round
            if needed_cooperators == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_396(BaseStrategy):
    """
    Adaptive Free-Rider with Threshold Enforcement (AFR-TE) strategy.

    This strategy aims to maximize its own total payoff by free-riding whenever
    the collective cooperation threshold is met by others. It strategically
    cooperates only when its own contribution is pivotal to reaching the
    threshold, thereby ensuring it receives the 'k' reward. It defects in the
    first and last rounds for exploitative probing and terminal gain maximization,
    respectively. It avoids 'sucker's payoffs' by defecting when its cooperation
    is unlikely to achieve the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: GameState object containing the current round number.
            history: PlayerHistory object containing past actions and payoffs.
                     Will be None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_0_indexed - 1
        cooperators_others_prev_round = np.sum(history.opponent_actions[previous_round_idx])
        my_action_prev_round_bool = history.my_actions[previous_round_idx]
        total_cooperators_prev_round = cooperators_others_prev_round + (1 if my_action_prev_round_bool else 0)
        if total_cooperators_prev_round >= m_threshold:
            return Action.D
        elif cooperators_others_prev_round >= m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_397(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily free-riding on the cooperation
    of others. It contributes only when necessary to initiate cooperation in the first round,
    or to re-establish conditions for collective success after a failure, which it then
    intends to exploit. It avoids being a persistent "sucker" by defecting if its cooperation
    alone isn't enough to meet the threshold. The strategy ruthlessly exploits the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveExploiter strategy with game parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing game parameters
                                                         like n_players, n_rounds, m, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing past actions and payoffs
                                            for this player and opponents, or None for round 0.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        elif current_round_idx == n_rounds - 1:
            return Action.D
        else:
            previous_round_idx = current_round_idx - 1
            cooperators_last_round_opponents = np.sum(history.opponent_actions[previous_round_idx, :])
            my_previous_action_is_cooperate = history.my_actions[previous_round_idx]
            total_cooperators_last_round = int(cooperators_last_round_opponents) + (1 if my_previous_action_is_cooperate else 0)
            my_previous_action = Action.C if my_previous_action_is_cooperate else Action.D
            if total_cooperators_last_round >= m_threshold:
                return Action.D
            elif my_previous_action == Action.C:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_398(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by carefully balancing free-riding behavior
    with strategic, minimal cooperation when necessary to maintain the collective benefit (the 'k' reward).
    It adapts based on the previous round's outcome and the observed behavior of other players.

    Core Principles:
    1. Initial Probe: Start with C in the first round.
    2. Exploit Success: If 'm' was met last round and my C wasn't essential, defect.
    3. Minimum Viable Contribution: If 'm' wasn't met last round, but my C *would* meet it, cooperate.
    4. Cut Losses: If 'm' wasn't met last round, and my C wouldn't meet it, defect.
    5. Final Round Defection: Defect in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        prev_round_0_indexed = current_round_0_indexed - 1
        my_action_prev_round_was_C = history.my_actions[prev_round_0_indexed]
        num_other_cooperators_prev_round = int(np.sum(history.opponent_actions[prev_round_0_indexed, :]))
        total_cooperators_prev_round = num_other_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        prev_round_was_successful = total_cooperators_prev_round >= m_threshold
        if prev_round_was_successful:
            if my_action_prev_round_was_C and total_cooperators_prev_round - 1 < m_threshold:
                return Action.C
            else:
                return Action.D
        elif num_other_cooperators_prev_round >= m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_399(BaseStrategy):
    """
    This strategy is designed to be highly self-interested, adaptive, and robust to various opponent behaviors,
    always seeking to exploit opportunities for a higher payoff while minimizing its own cost.
    It prioritizes free-riding (defecting) and only cooperates under specific conditions where its
    contribution is calculated to be pivotal and efficient for its own long-term gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: A CollectiveRiskDescription object containing n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object containing past actions and payoffs, or None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        else:
            my_action_prev_round = history.my_actions[state.round_number - 1]
            opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
            N_c_prev = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
            if N_c_prev >= m_threshold:
                return Action.D
            elif N_c_prev == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_400(BaseStrategy):
    """
    This strategy, "Adaptive Trigger-Free Rider," primarily aims to free-ride (defect)
    to achieve the highest possible individual payoff (1 + k). It strategically cooperates
    only when its contribution is *critical* to securing the collective reward 'k' that
    would otherwise be lost, thus minimizing its contribution while ensuring the benefit.
    It immediately reverts to defecting as soon as the 'k' reward is secured or if its
    cooperation is not sufficient to meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (min_cooperators), k (reward_factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs for
                     this player and opponents, or None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        my_prev_action_was_c = history.my_actions[current_round_0_indexed - 1]
        opponent_prev_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        C_prev = int(my_prev_action_was_c) + np.sum(opponent_prev_actions)
        if C_prev >= min_cooperators_needed:
            return Action.D
        else:
            C_potential_if_I_cooperate = C_prev + 1
            if C_potential_if_I_cooperate >= min_cooperators_needed:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_401(BaseStrategy):
    """
    The "Adaptive Threshold Rider" strategy for the Collective Risk Dilemma.
    This strategy aims to free-ride as much as possible, only cooperating when its single
    contribution is critical to securing the collective reward (k) for itself, and when
    doing so yields a direct individual benefit greater than defecting.

    Decision Rules:
    1.  Initial Round (t = 1): Defect. No history, test the waters and align with dominant strategy.
    2.  Intermediate Rounds (1 < t < r):
        a.  If `num_C_t-1 >= m`: Defect. The collective reward `k` was secured; free-ride.
        b.  If `num_C_t-1 == m - 1`: Cooperate. This player's cooperation secures `k` for themselves, which is > 1.
        c.  If `num_C_t-1 < m - 1`: Defect. Player's single cooperation won't reach `m`, so it's "wasted".
    3.  Final Round (t = r): Defect. No future interactions to influence, always defect to maximize final round payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t == self.n_rounds:
            return Action.D
        num_C_t_minus_1 = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if num_C_t_minus_1 >= self.m:
            return Action.D
        elif num_C_t_minus_1 == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_402(BaseStrategy):
    """
    "Strategic Opportunist" for the Collective Risk Dilemma.
    This strategy aims to maximize its own cumulative payoff by free-riding whenever possible,
    and only contributing when its individual cooperation is critically necessary and
    directly benefits its own payoff more than defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_is_cooperate
        if num_cooperators_prev_round >= self.m:
            return Action.D
        elif num_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_403(BaseStrategy):
    """
    Conditional Free-Rider with Limited Investment for the Collective Risk Dilemma.

    This strategy operates on the principle of opportunistic free-riding. It attempts to
    establish a cooperative environment from which it can benefit, but it is not
    willing to sacrifice indefinitely if others do not sufficiently contribute. It
    prioritizes its own payoff by defecting whenever possible, and only cooperates
    strategically to prevent the permanent loss of the collective reward `k`.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.MAX_FAILURES_TOLERATED: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        elif current_round_t == 1:
            self.consecutive_failures = 0
            return Action.C
        else:
            cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if cooperators_last_round >= self.game_description.m:
                self.consecutive_failures = 0
                return Action.D
            elif self.consecutive_failures < self.MAX_FAILURES_TOLERATED:
                self.consecutive_failures += 1
                return Action.C
            else:
                self.consecutive_failures += 1
                return Action.D

class Strategy_EXPLOITATIVE_404(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma.

    This strategy aims to free-ride whenever possible, defecting to secure the
    maximum private payoff. It only cooperates if its contribution is absolutely
    critical to achieving the collective 'k' bonus, and doing so yields a higher
    personal payoff than defecting (which is true when k > 1 and it's the
    decisive cooperator). It also explicitly leverages the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.r - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        C_prev = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if C_prev >= self.m:
            return Action.D
        elif C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_405(BaseStrategy):
    """
    Adaptive Free-Rider with Despair Cutoff (AFR-DC) strategy for the Collective Risk Dilemma.
    This strategy aims to exploit when safe, protect the collective good to maintain the 'k' bonus,
    adapt to observed cooperation levels, and avoid fruitless cooperation through a 'despair' cutoff.
    It also defects in the final round as a rational end-game move.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.consecutive_coop_failures = 0
        self.max_failures_allowed = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        total_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1])
        my_contribution_prev_round = 1 if my_action_prev_round_is_cooperate else 0
        observed_cooperators_prev_round = total_opponent_cooperators_prev_round + my_contribution_prev_round
        if my_action_prev_round_is_cooperate and observed_cooperators_prev_round < self.m:
            self.consecutive_coop_failures += 1
        else:
            self.consecutive_coop_failures = 0
        if self.consecutive_coop_failures >= self.max_failures_allowed:
            return Action.D
        if observed_cooperators_prev_round >= self.m + 1:
            return Action.D
        elif observed_cooperators_prev_round == self.m:
            return Action.C
        elif observed_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.C

class Strategy_EXPLOITATIVE_406(BaseStrategy):
    """
    Adaptive Collective Risk Exploiter (ACRE) strategy.

    ACRE seeks to free-ride on the cooperation of others whenever the collective threshold appears to be met.
    However, it contributes its own cooperation if it identifies itself as the critical "swing vote"
    needed to achieve the collective reward `k`. It defects if cooperation appears futile or in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            return Action.C
        elif state.round_number == r - 1:
            return Action.D
        else:
            rounds_played_so_far = state.round_number
            cooperation_scores_opponents = np.sum(history.opponent_actions, axis=0)
            E_C_others = 0
            for opponent_score in cooperation_scores_opponents:
                cooperation_rate_j = opponent_score / rounds_played_so_far
                if cooperation_rate_j >= 0.5:
                    E_C_others += 1
            if E_C_others >= m:
                return Action.D
            elif E_C_others == m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_407(BaseStrategy):
    """
    The Adaptive Exploiter with Conditional Trust strategy aims to maximize its
    own total payoff by balancing probing for initial cooperation, conditional
    cooperation to meet a minimum threshold, exploiting when cooperation is
    sufficiently established, cutting losses during prolonged failures,
    and re-engaging if the environment becomes cooperative again. It always
    defects in the final round due to backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_cooperators: int = 0
        self.consecutive_failures: int = 0
        self.consecutive_successes: int = 0
        self.gave_up_on_cooperation: bool = False
        self.ever_observed_others_cooperate: bool = False
        self.INITIAL_PROBE_ROUNDS: int = 2
        self.FAILURE_TOLERANCE: int = 3
        self.EXPLOIT_BUFFER: int = 1
        self.RECOOPERATE_THRESHOLD_FACTOR: float = 0.75

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m_threshold = self.game_description.m
        n_rounds = self.game_description.n_rounds
        current_round_num_1_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        prev_round_index = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_index]
        opponent_actions_prev = history.opponent_actions[prev_round_index, :]
        self.last_round_cooperators = int(my_action_prev) + np.sum(opponent_actions_prev)
        if self.last_round_cooperators >= m_threshold:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        if not self.ever_observed_others_cooperate:
            if np.sum(opponent_actions_prev) > 0:
                self.ever_observed_others_cooperate = True
        my_current_action = Action.D
        if current_round_num_1_indexed == n_rounds:
            my_current_action = Action.D
        elif current_round_num_1_indexed <= self.INITIAL_PROBE_ROUNDS:
            my_current_action = Action.C
        elif self.consecutive_failures >= self.FAILURE_TOLERANCE:
            self.gave_up_on_cooperation = True
            my_current_action = Action.D
        elif self.gave_up_on_cooperation and self.last_round_cooperators >= math.ceil(m_threshold * self.RECOOPERATE_THRESHOLD_FACTOR) and self.ever_observed_others_cooperate:
            self.gave_up_on_cooperation = False
            my_current_action = Action.C
        elif self.last_round_cooperators >= m_threshold + self.EXPLOIT_BUFFER:
            my_current_action = Action.D
        else:
            my_current_action = Action.C
        return my_current_action

class Strategy_EXPLOITATIVE_408(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to exploit situations where the cooperation threshold 'm' is met
    by other players, allowing it to defect and gain a private payoff of 1 in addition
    to the collective reward 'k'. It contributes strategically only when its cooperation
    is pivotal to reaching the 'm' threshold, thereby securing the 'k' reward for itself
    (and others). It withdraws cooperation when its efforts are ineffective or futile,
    avoiding being a "sucker." The strategy starts with an initial probe of cooperation
    and defaults to defection in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_prev_round >= self.m:
            return Action.D
        elif cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_409(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by strategically free-riding on others'
    contributions when the collective good is secured, and only contributing when necessary
    to ensure the collective good *can* be secured in subsequent rounds. It adapts to
    collective behavior and prioritizes individual gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        prev_round_my_action_is_C = history.my_actions[-1]
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_round_total_cooperators = prev_round_opponent_cooperators + (1 if prev_round_my_action_is_C else 0)
        if prev_round_total_cooperators < m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_410(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy is an exploitative, adaptive, and robust approach
    for the Collective Risk Dilemma. It maximizes individual payoff by free-riding on
    others' cooperation when the collective reward threshold is met, strategically
    contributing only when its cooperation is critical and individually profitable
    (due to k > 1), and defecting to protect endowment when cooperation is futile.
    It cooperates in the first round to foster a cooperative environment and defects
    in the final round to maximize terminal payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the CalculatedFreeRider strategy.
        Stores game parameters from the game_description.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs,
                     or None for the very first round (state.round_number == 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_last_action: bool = history.my_actions[-1]
        overall_cooperators_last_round: int = np.sum(history.opponent_actions[-1]) + int(my_last_action)
        num_other_players_who_cooperated_last_round: int
        if my_last_action:
            num_other_players_who_cooperated_last_round = overall_cooperators_last_round - 1
        else:
            num_other_players_who_cooperated_last_round = overall_cooperators_last_round
        if num_other_players_who_cooperated_last_round >= m:
            return Action.D
        elif num_other_players_who_cooperated_last_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_411(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the Collective Risk Dilemma.
    This strategy is designed to opportunistically free-ride whenever possible,
    but it is also willing to strategically contribute (cooperate) if its
    contribution is essential to either secure the 'k' bonus or to re-establish
    cooperation after a collective failure, thereby creating future free-riding opportunities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        my_last_action_was_C = history.my_actions[-1]
        opponents_cooperated_last_round = sum(history.opponent_actions[-1, :])
        last_round_c_count = opponents_cooperated_last_round + (1 if my_last_action_was_C else 0)
        if last_round_c_count >= m:
            if my_last_action_was_C:
                cooperators_without_me = last_round_c_count - 1
                if cooperators_without_me >= m:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_412(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to consistently maximize its own payoff by prioritizing free-riding.
    It will only contribute to the collective good (cooperate) when its contribution is critically needed
    to unlock the collective reward, and when that cooperation is expected to yield a higher payoff than defecting.
    It strategically adapts to the observed level of cooperation from other players and exploits the end-game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            my_prev_action_was_C = int(history.my_actions[-1])
            opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
            C_observed_prev = my_prev_action_was_C + opponent_prev_cooperators
            if C_observed_prev >= self.m:
                return Action.D
            elif C_observed_prev == self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_413(BaseStrategy):
    """
    Adaptive Exploiter with Buffer-Checking: An exploitative, adaptive, and robust strategy
    for the Collective Risk Dilemma. It maximizes its own total payoff by strategically
    cooperating to ensure the common good 'k' is met, then free-rides when a buffer of
    cooperators exists, and defects to punish non-cooperation or in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= m:
            if num_cooperators_prev_round > m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_414(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to maximize its individual payoff by primarily defecting (D),
    while strategically cooperating (C) only when it perceives its cooperation is necessary to achieve the collective
    threshold (m) and thus secure the shared reward (k) for all players. It will aggressively free-ride in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        threshold_m = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        previous_round_index = state.round_number - 1
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_index])
        my_prev_action_was_C = history.my_actions[previous_round_index]
        total_cooperators_prev = num_opponent_cooperators_prev + (1 if my_prev_action_was_C else 0)
        if total_cooperators_prev < threshold_m:
            return Action.C
        else:
            cooperators_without_me_prev = total_cooperators_prev
            if my_prev_action_was_C:
                cooperators_without_me_prev -= 1
            if cooperators_without_me_prev >= threshold_m:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_415(BaseStrategy):
    """
    Adaptive Risk-Mitigating Free-Rider strategy for the Collective Risk Dilemma.
    This strategy balances the desire to free-ride with the necessity of
    occasionally contributing to ensure the collective reward 'k' is achieved,
    as 'k' also benefits the exploitative player. It defaults to defection,
    but temporarily cooperates if the collective threshold 'm' is consistently
    at risk (failing K_THRESHOLD consecutive rounds). It then quickly reverts
    to defection after COOP_STREAK_LENGTH rounds to re-test the waters and
    push the burden back onto other players. In the final round, it always defects.
    """
    K_THRESHOLD = 2
    COOP_STREAK_LENGTH = 1
    STATE_EXPLORING_DEFECTION = 'EXPLORING_DEFECTION'
    STATE_COOPERATING_TO_ENSURE_K = 'COOPERATING_TO_ENSURE_K'

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_rounds_failed_to_meet_threshold: int = 0
        self.my_current_strategy_state: str = self.STATE_EXPLORING_DEFECTION
        self.my_cooperation_streak_counter: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        did_i_cooperate_prev_round = history.my_actions[-1]
        num_cooperators_prev_round = num_cooperators_opponents_prev_round + (1 if did_i_cooperate_prev_round else 0)
        if num_cooperators_prev_round < self.game_description.m:
            self.my_rounds_failed_to_meet_threshold += 1
        else:
            self.my_rounds_failed_to_meet_threshold = 0
        if self.my_rounds_failed_to_meet_threshold >= self.K_THRESHOLD:
            self.my_current_strategy_state = self.STATE_COOPERATING_TO_ENSURE_K
            self.my_cooperation_streak_counter = 0
        elif self.my_current_strategy_state == self.STATE_COOPERATING_TO_ENSURE_K and self.my_cooperation_streak_counter >= self.COOP_STREAK_LENGTH:
            self.my_current_strategy_state = self.STATE_EXPLORING_DEFECTION
            self.my_cooperation_streak_counter = 0
        if self.my_current_strategy_state == self.STATE_EXPLORING_DEFECTION:
            return Action.D
        elif self.my_current_strategy_state == self.STATE_COOPERATING_TO_ENSURE_K:
            self.my_cooperation_streak_counter += 1
            return Action.C

class Strategy_EXPLOITATIVE_416(BaseStrategy):
    """
    The "Calculated Freeloader" is an adaptive and exploitative strategy for the Collective Risk Dilemma.
    It prioritizes free-riding but will temporarily cooperate if its contribution is critical
    to meet the 'm' cooperation threshold and re-establish the collective reward 'k', provided
    the group hasn't failed too many consecutive times. It defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k
        self.rounds_failed_threshold = 0
        self.MAX_FAILED_ROUNDS_TOLERANCE = self.m_threshold

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        my_prev_action_bool = history.my_actions[prev_round_idx]
        opponent_prev_actions_bool = history.opponent_actions[prev_round_idx, :]
        N_C_prev = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bool)
        if N_C_prev >= self.m_threshold:
            self.rounds_failed_threshold = 0
        else:
            self.rounds_failed_threshold += 1
        next_action = Action.D
        N_C_others_potential_this_round_if_I_defect = np.sum(opponent_prev_actions_bool)
        potential_total_cooperators_if_I_cooperate = N_C_others_potential_this_round_if_I_defect + 1
        if self.rounds_failed_threshold > 0 and self.rounds_failed_threshold <= self.MAX_FAILED_ROUNDS_TOLERANCE and (potential_total_cooperators_if_I_cooperate >= self.m_threshold):
            next_action = Action.C
        return next_action

class Strategy_EXPLOITATIVE_417(BaseStrategy):
    """
    The Adaptive Pivoting Free-Rider strategy aims to maximize individual payoff by primarily free-riding
    (defecting) when possible, only contributing to the collective good (cooperating) when the system
    is consistently failing, and when the agent's contribution is likely to be pivotal in restoring
    the collective reward.

    It operates under the principle of "defect by default, cooperate strategically to ensure future
    free-riding opportunities."
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures: int = 0
        self.consecutive_successes: int = 0
        self.MAX_FAILURE_TOLERANCE: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.D
        last_round_idx = state.round_number - 1
        my_action_last_round_bool = history.my_actions[last_round_idx]
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[last_round_idx]).item()
        total_cooperators_last_round_game = num_opponent_cooperators_last_round + (1 if my_action_last_round_bool else 0)
        if total_cooperators_last_round_game >= self.game_description.m:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        if self.consecutive_successes > 0:
            return Action.D
        elif self.consecutive_failures >= self.MAX_FAILURE_TOLERANCE:
            potential_cooperators_if_I_play_C = num_opponent_cooperators_last_round + 1
            if potential_cooperators_if_I_play_C >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_418(BaseStrategy):
    """
    The Adaptive Threshold Exploiter with Recalibration strategy is designed to be highly adaptive and exploitative.
    It aims to maximize its own payoff by free-riding on others' cooperation whenever possible, while strategically
    contributing only when it believes its cooperation is necessary to unlock the collective reward or to reset
    a failing cooperative environment. It will cut its losses quickly if collective cooperation proves elusive.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.max_consecutive_failures_before_defection = 2
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_1_indexed = state.round_number + 1
        if current_game_round_1_indexed == 1:
            return Action.C
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        threshold_met_prev_round = num_cooperators_prev_round >= self.game_description.m
        if threshold_met_prev_round:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if current_game_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        if threshold_met_prev_round:
            return Action.D
        elif self.consecutive_failures <= self.max_consecutive_failures_before_defection:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_419(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy is designed to maximize its own payoff by strategically
    free-riding on the contributions of others, while only contributing itself when it is
    critically necessary to enable the collective reward for subsequent exploitation.
    It adapts its behavior based on the observed level of cooperation in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round == 0:
            return Action.D
        if current_round == total_rounds - 1:
            return Action.D
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_cooperation_prev = 1 if history.my_actions[-1] else 0
        N_C_prev = opponent_cooperators_prev + my_cooperation_prev
        if N_C_prev >= min_cooperators_m:
            return Action.D
        elif N_C_prev == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_420(BaseStrategy):
    """
    Adaptive Free-Rider with Collective Nudge (AFRCN) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize free-riding opportunities (1+k payoff) while strategically
    intervening (cooperating for k payoff) only when the collective project is at risk of failing,
    thus ensuring the k bonus remains available for future exploitation.

    The strategy's decision depends on the current round number and the collective outcome of
    the previous round (total cooperators).

    1. Round 1: Initial Probe (Defect)
        - Action: Defect (D) to test for free-riding opportunities.
    2. Intermediate Rounds (2 to r-1): Adaptive Exploitation
        - If the threshold 'm' was met in the previous round: Defect (D) to free-ride.
        - If the threshold 'm' was NOT met in the previous round: Cooperate (C) to "nudge" the collective
          towards success and ensure the 'k' bonus is available for future exploitation.
    3. Round r: Final Defection (Defect)
        - Action: Defect (D), as there are no future interactions to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        previous_round_data_index = current_round_0_indexed - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_data_index, :])
        my_action_prev_round = history.my_actions[previous_round_data_index]
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
        if num_total_cooperators_prev_round >= min_cooperators:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_421(BaseStrategy):
    """
    The Opportunistic Threshold Sustainer (OTS) strategy seeks to exploit the collective nature of the game
    by defecting whenever the threshold 'm' is expected to be met without its cooperation, or when its
    cooperation alone cannot achieve 'm'. It only cooperates when its contribution is critical to
    crossing the threshold 'm', recognizing that the collective reward 'k' (where k > 1) makes
    cooperation in such a scenario personally beneficial. It adapts based on observed cooperation
    in previous rounds and implicitly "punishes" insufficient group effort.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        N_C_prev = int(my_action_prev) + opponent_cooperators_prev
        estimated_others_C_if_I_defect = opponent_cooperators_prev
        if state.round_number == self.r - 1:
            if estimated_others_C_if_I_defect >= self.m:
                return Action.D
            elif estimated_others_C_if_I_defect == self.m - 1:
                return Action.C
            else:
                return Action.D
        elif N_C_prev >= self.m:
            if estimated_others_C_if_I_defect >= self.m:
                return Action.D
            else:
                return Action.C
        elif estimated_others_C_if_I_defect == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_422(BaseStrategy):
    """
    The "Calculated Free-Rider with Collective Trigger" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding on others' cooperation
    when the collective threshold 'm' is met. If the threshold is missed, it strategically
    intervenes (cooperates) only when its contribution is pivotal or serves as a strong,
    calculated 'nudge' to achieve the collective reward 'k', otherwise it defects to
    minimize losses. Endgame and early-game behavior are predefined for exploitation and probing.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        _current_round_1_indexed = state.round_number + 1
        if _current_round_1_indexed == self.r:
            return Action.D
        if _current_round_1_indexed == 1:
            return Action.D
        prev_round_index = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_index]
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[prev_round_index, :])
        prev_round_total_cooperators = prev_round_opponent_cooperators + (1 if my_action_prev else 0)
        failure_streak = 0
        for i in range(prev_round_index, -1, -1):
            round_i_my_action = history.my_actions[i]
            round_i_opponent_cooperators = np.sum(history.opponent_actions[i, :])
            round_i_total_cooperators = round_i_opponent_cooperators + (1 if round_i_my_action else 0)
            if round_i_total_cooperators < self.m:
                failure_streak += 1
            else:
                break
        action = Action.D
        if prev_round_total_cooperators >= self.m:
            if prev_round_total_cooperators == self.m and my_action_prev:
                action = Action.C
            else:
                action = Action.D
        else:
            needed_cooperators = self.m - prev_round_total_cooperators
            MAX_FAILURE_STREAK = 3
            TOO_LATE_THRESHOLD_ROUND = self.r - 3
            if failure_streak > MAX_FAILURE_STREAK:
                action = Action.D
            elif _current_round_1_indexed >= TOO_LATE_THRESHOLD_ROUND:
                action = Action.D
            elif needed_cooperators == 1:
                action = Action.C
            elif needed_cooperators == 2 and prev_round_total_cooperators > 0:
                action = Action.C
            elif prev_round_total_cooperators == 0 and self.m <= self.n / 2 and (_current_round_1_indexed < self.r / 3):
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_423(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding on others' cooperation
    whenever possible, and only strategically intervening when its contribution is
    critical and individually beneficial to secure the collective reward 'k'.
    It leverages the repeated nature of the game and perfect information to adapt
    to observed collective outcomes.

    Key behaviors:
    - Defects in the first round to probe for free-riding opportunities.
    - Always defects in the last round (backward induction).
    - Defects if the cooperation threshold ('m') was met in the previous round,
      free-riding on others' success.
    - If the threshold was NOT met, it cooperates only if its single contribution
      would cause the threshold to be met, thus securing 'k' for itself (since k > 1).
    - Otherwise (if its single contribution is not enough to meet the threshold), it defects
      to avoid a wasted contribution and maximize payoff in a failed state.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.min_cooperators_m = game_description.m
        self.reward_factor_k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.total_rounds - 1:
            return Action.D
        num_cooperated_last_round = int(history.my_actions[state.round_number - 1]) + np.sum(history.opponent_actions[state.round_number - 1, :])
        if num_cooperated_last_round >= self.min_cooperators_m:
            return Action.D
        else:
            deficit_to_meet_threshold = self.min_cooperators_m - num_cooperated_last_round
            if deficit_to_meet_threshold == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_424(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize individual payoff by consistently
    aiming for the 1+k outcome. It defects whenever possible, especially when the cooperation
    threshold 'm' is met by others. AFR only cooperates as a calculated, short-term investment
    to prevent a prolonged breakdown of collective cooperation that would otherwise reduce its
    own payoff to 1 per round, and to re-establish conditions for future free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.D_threshold = 2
        self.D_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        my_prev_action_bool = history.my_actions[state.round_number - 1]
        opponent_prev_actions_bool = history.opponent_actions[state.round_number - 1, :]
        C_prev = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bool)
        if C_prev < m:
            self.D_count += 1
        else:
            self.D_count = 0
        if state.round_number == n_rounds - 1:
            return Action.D
        if self.D_count >= self.D_threshold:
            return Action.D
        elif C_prev >= m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_425(BaseStrategy):
    """
    The Calculated Free-Rider strategy aims to maximize its own payoff by primarily
    defecting and free-riding on others' cooperation. It strategically cooperates
    only when its contribution is both necessary and sufficient to trigger the
    collective reward (k), and when doing so yields a better individual payoff.
    It adapts to observed collective behavior but prioritizes self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          game parameters like
                                                          n_players, n_rounds, m, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) for the current round
        based on the Calculated Free-Rider strategy.

        Args:
            state (GameState): Contains the current round number (0-indexed).
            history (None | PlayerHistory): Contains historical actions and payoffs
                                            for this player and opponents.
                                            `None` for the first round (round_number=0).

        Returns:
            Action: Action.C for Cooperate or Action.D for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds:
            return Action.D
        others_cooperated_previous_round = np.sum(history.opponent_actions[-1, :])
        if others_cooperated_previous_round >= m_threshold:
            return Action.D
        elif others_cooperated_previous_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_426(BaseStrategy):
    """
    The Pragmatic Free-Rider strategy prioritizes maximizing individual payoff in the
    Collective Risk Dilemma. It aims to exploit opportunities for free-riding by
    defecting when the cooperation threshold ('m') is met by others. It is, however,
    pragmatic enough to contribute (cooperate) in the initial round to test group
    cooperation, and critically when its own contribution is essential to reach 'm'
    and secure the 'k' reward (since k > 1). The strategy avoids unrewarded contributions
    that lead to a 0 payoff, preferring to defect for a 1 payoff, especially during
    periods of sustained collective failure, which also serves to signal a need for
    greater group effort. In the final round, it always defects to maximize immediate gains.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.FAILURE_THRESHOLD = max(2, math.floor(self.n_rounds / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        my_previous_action_was_C = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        N_C_prev = opponent_cooperators_prev + (1 if my_previous_action_was_C else 0)
        if N_C_prev >= self.m:
            self.consecutive_failures = 0
        else:
            self.consecutive_failures += 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if N_C_prev >= self.m:
            return Action.D
        elif N_C_prev == self.m - 1:
            return Action.C
        elif self.consecutive_failures >= self.FAILURE_THRESHOLD:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_427(BaseStrategy):
    """
    This strategy, named "Adaptive Exploiter with Periodic Probing," aims to maximize individual payoff
    by defaulting to defection, leveraging other players' cooperation, and only contributing when
    absolutely necessary to secure the collective reward (k), or to probe the system's resilience.
    It's designed to be adaptive to observed collective behavior and robust against various opponent strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.test_period = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.r - 1:
            return Action.D
        prev_round_idx = current_round_number - 1
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        my_prev_action_bool = history.my_actions[prev_round_idx]
        my_prev_action_int = 1 if my_prev_action_bool else 0
        C_prev = num_opponent_cooperators_prev + my_prev_action_int
        if C_prev < self.m:
            if C_prev + 1 >= self.m:
                return Action.C
            else:
                return Action.D
        num_others_C_prev = C_prev - my_prev_action_int
        if num_others_C_prev < self.m:
            if (current_round_number + 1) % self.test_period == 0 and current_round_number < self.r - 2:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_428(BaseStrategy):
    """
    Adaptive Free-Rider with Threshold Management (AFR-TM).

    This strategy aims to free-ride on the cooperation of other players.
    It will defect whenever the collective is robustly meeting the cooperation
    threshold (`m`) based on previous round's opponent actions. However, it
    strategically cooperates when its individual contribution is necessary to
    push the collective *just over* the threshold, thereby restoring the 'k'
    bonus which it can then free-ride on in subsequent rounds. It avoids futile
    cooperation when its effort would not be enough to achieve the threshold.
    The strategy always defects in the final round and cooperates in the first.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_game_rounds = self.game_description.n_rounds
        m_cooperation_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_game_rounds - 1:
            return Action.D
        previous_round_0_indexed = current_round_0_indexed - 1
        num_opponent_cooperators_in_prev_round = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        if num_opponent_cooperators_in_prev_round >= m_cooperation_threshold:
            return Action.D
        elif num_opponent_cooperators_in_prev_round + 1 >= m_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_429(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aims to free-ride whenever possible, ensuring it receives the `1` private payoff
    in addition to the `k` community reward. It adapts its behavior based on the observed cooperation level of other
    players in the previous round. It will only cooperate under specific conditions: to "probe" for cooperation initially,
    or when its cooperation is critically necessary to maintain the collective `k` bonus, which is vital for future
    free-riding opportunities. It explicitly defects in the final round to maximize immediate payoff, as there are no
    future interactions to consider.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        my_action_prev_round: bool = history.my_actions[current_round - 1]
        opponent_actions_prev_round: NDArray[np.bool_] = history.opponent_actions[current_round - 1, :]
        num_cooperators_prev_round = np.sum(opponent_actions_prev_round) + my_action_prev_round
        if num_cooperators_prev_round > self.m:
            return Action.D
        elif num_cooperators_prev_round == self.m:
            if my_action_prev_round:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_430(BaseStrategy):
    """
    The Adaptive Pivoting Defector strategy prioritizes individual gain. It aims to exploit the
    collective benefit (+k) by defecting whenever the cooperation threshold (m) is likely to
    be met by others. It will only cooperate if its contribution is *pivotal* to reach the
    threshold *and* doing so yields a higher personal payoff than defecting in that specific
    scenario (i.e., k > 1). In all other situations, it defects to maximize immediate private gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Pivoting Defector strategy with the game parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate C or Defect D) for the current round based on
        game parameters and historical play.
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_0_idx = state.round_number
        if current_round_0_idx == 0:
            return Action.D
        if current_round_0_idx == n_rounds - 1:
            return Action.D
        prev_round_0_idx = current_round_0_idx - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_0_idx, :])
        my_action_prev_round_was_C = history.my_actions[prev_round_0_idx]
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        estimated_others_cooperating = 0
        if my_action_prev_round_was_C:
            estimated_others_cooperating = num_total_cooperators_prev_round - 1
        else:
            estimated_others_cooperating = num_total_cooperators_prev_round
        estimated_others_cooperating = max(0, estimated_others_cooperating)
        if estimated_others_cooperating >= m_threshold:
            return Action.D
        elif estimated_others_cooperating == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_431(BaseStrategy):
    """
    The Selfish Catalyst is an exploitative strategy for the Collective Risk Dilemma.
    It primarily free-rides on the cooperation of other players, defecting by default.
    It strategically cooperates only when its contribution is critically needed to meet the
    cooperation threshold (m) after a round of failure, aiming to re-establish the
    collective benefit from which it can then free-ride again. It always defects in the
    first and last rounds to maximize immediate personal gain without strategic repercussions.

    State maintained:
    - history_of_total_cooperators_per_round: A list storing the total number of cooperators
      (including this player) observed in each previous round, 0-indexed.
      e.g., history_of_total_cooperators_per_round[0] stores cooperators for round 0.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history_of_total_cooperators_per_round: list[int] = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if history is not None:
            cooperators_in_prev_completed_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            self.history_of_total_cooperators_per_round.append(cooperators_in_prev_completed_round)
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == self.game_description.n_rounds - 1:
            return Action.D
        C_prev = self.history_of_total_cooperators_per_round[current_round_idx - 1]
        m = self.game_description.m
        if C_prev >= m:
            return Action.D
        elif C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_433(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the Collective Risk Dilemma aims to maximize its
    individual total payoff over 'r' rounds by intelligently free-riding on others' cooperation
    while also strategically contributing when its cooperation is essential to unlock the
    collective reward 'k'. It is designed to be adaptive to observed collective behavior
    and exploitative by prioritizing individual gain when possible.

    Decision Rules:
    1.  **First Round (t=1):** Cooperate (C). Initiates cooperation to probe the environment.
    2.  **Last Round (t=r):** Defect (D). Exploits the end-game effect as there are no future consequences.
    3.  **Intermediate Rounds (1 < t < r):**
        *   **If threshold met in previous round (C_{t-1} >= m):** Defect (D). Free-ride on others' success.
        *   **If threshold NOT met in previous round (C_{t-1} < m):**
            *   **If my cooperation is pivotal (C'_{t-1} = m - 1):** Cooperate (C). Contribute to secure the 'k' bonus for myself.
            *   **If my cooperation is NOT pivotal (C'_{t-1} < m - 1):** Defect (D). Avoid wasted cooperation if 'k' won't be met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + (1 if history.my_actions[state.round_number - 1] else 0)
        if num_cooperators_prev_round >= self.m:
            return Action.D
        else:
            num_other_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            if num_other_cooperators_prev_round == self.m - 1:
                return Action.C
            elif num_other_cooperators_prev_round < self.m - 1:
                return Action.D
            return Action.D

class Strategy_EXPLOITATIVE_434(BaseStrategy):
    """
    Adaptive Free-Rider with Contingent Investment (AFRCI) strategy.

    This strategy aims to free-ride when opponents are likely to meet the
    collective reward threshold (`m`), but cooperates ("invests") when its
    own contribution is pivotal to achieving that threshold. It avoids
    cooperating when its contribution would be futile. It always cooperates
    in the first round as an initial investment and defects in the final
    round for terminal payoff maximization.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.history_window_size = max(2, min(5, math.ceil(self.r / 2)))
        self._recent_others_coop_counts_list = []
        self.initial_expected_others_cooperators = max(0, self.m - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0:
            others_cooperated_last_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            self._recent_others_coop_counts_list.append(others_cooperated_last_round)
            if len(self._recent_others_coop_counts_list) > self.history_window_size:
                self._recent_others_coop_counts_list.pop(0)
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        if self._recent_others_coop_counts_list:
            avg_c_others = sum(self._recent_others_coop_counts_list) / len(self._recent_others_coop_counts_list)
            expected_others_cooperators = round(avg_c_others)
        else:
            expected_others_cooperators = self.initial_expected_others_cooperators
        if expected_others_cooperators >= self.m:
            return Action.D
        elif expected_others_cooperators >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_435(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by adaptively free-riding on the cooperation
    of others while ensuring the collective bonus 'k' is met when its own contribution is pivotal.

    Key behaviors:
    - Primes the pump by cooperating in the first round to encourage collective cooperation.
    - Defects in the last round due to backward induction (no future interactions).
    - In intermediate rounds, it predicts others' cooperation based on the previous round.
      - If others are expected to meet the threshold 'm' or more, it defects to free-ride.
      - If its cooperation is exactly needed to meet the threshold ('m-1' others cooperate),
        it cooperates to secure the 'k' bonus (since k > 1).
      - If its cooperation is insufficient to meet the threshold (fewer than 'm-1' others cooperate),
        it defects to maximize private payoff in a losing scenario.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        No additional state variables are needed as previous round's actions/cooperation
        can be derived directly from the 'history' object provided in '__call__'.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: GameState object containing the current round number (0-indexed).
            history: PlayerHistory object containing actions and payoffs from previous rounds.
                     Will be None for state.round_number == 0.

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        others_coop_in_last_round = sum(history.opponent_actions[-1, :])
        predicted_others_coop_this_round = others_coop_in_last_round
        if predicted_others_coop_this_round >= m:
            return Action.D
        elif predicted_others_coop_this_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_436(BaseStrategy):
    """
    The Opportunistic Predator strategy aims to maximize individual total payoff in the
    Collective Risk Dilemma. It primarily free-rides, defecting whenever possible.
    Cooperation is only considered when the reward for successful cooperation (k-1)
    is strictly greater than defecting for failure (1), and only if its contribution
    is pivotal to achieving the cooperation threshold 'm'.

    Strategy Phases:
    1.  Pure Exploitation (k <= 2): Always defects, as cooperating for success (k-1)
        is not better than defecting for failure (1).
    2.  Conditional Exploitation (k > 2):
        - First Round: Cooperates as an investment to 'seed' cooperation.
        - Last Round: Defects, as there are no future rounds to influence.
        - Intermediate Rounds: Estimates opponents' cooperation based on the previous round.
            - If enough others are expected to cooperate to meet 'm', it defects (free-rides).
            - If its cooperation is pivotal to meet 'm' (and k > 2 makes k-1 > 1), it cooperates.
            - Otherwise (cooperation would be futile), it defects to avoid a 0 payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if self.k <= 2:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        previous_round_index = current_round_0_indexed - 1
        num_other_cooperators_expected = np.sum(history.opponent_actions[previous_round_index, :])
        if num_other_cooperators_expected >= self.m:
            return Action.D
        elif num_other_cooperators_expected + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_437(BaseStrategy):
    """
    The Adaptive Critical Mass Defector (ACMD) strategy is designed for maximizing individual payoff
    by primarily free-riding on the cooperation of other players. It will only contribute when its
    cooperation is critical to unlocking the collective reward, and it detects a near-miss scenario.
    This ensures the strategy benefits from the collective good when possible, while minimizing its
    own contribution cost.

    Decision Rules:
    1.  Round 1: Always Defect (D) to establish free-rider stance and gather data.
    2.  Last Round: Always Defect (D) due to backward induction (no future interactions).
    3.  Intermediate Rounds:
        *   If the previous round succeeded (C_prev >= m): Continue to Defect (D) and free-ride.
        *   If the previous round failed (C_prev < m):
            *   If I cooperated in the previous failed round: Revert to Defect (D) as my effort was wasted.
            *   If I defected in the previous failed round:
                *   If my single cooperation would push the group over the threshold (m - C_prev == 1): Cooperate (C).
                *   Otherwise (m - C_prev > 1): Defect (D) to avoid risking 0 payoff without guaranteed success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACMD strategy with the game's parameters.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          n_players, n_rounds, m, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the ACMD strategy rules.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): The history of actions and payoffs for all
                                            players in previous rounds. Is None for round 0.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_prev_action_bool = history.my_actions[prev_round_idx]
        opponents_prev_cooperated_count = np.sum(history.opponent_actions[prev_round_idx, :])
        C_prev = int(my_prev_action_bool) + opponents_prev_cooperated_count
        if C_prev >= m_threshold:
            return Action.D
        elif my_prev_action_bool == True:
            return Action.D
        else:
            needed_cooperators = m_threshold - C_prev
            if needed_cooperators == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_438(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the Collective Risk Dilemma.

    This strategy aims for individual payoff maximization by adapting to opponent
    behavior. It prioritizes free-riding and only contributes when its contribution
    is critically needed to secure the collective benefit, leveraging the game's
    public good nature and end-game effects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveFreeRider strategy.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                          the game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate 'C' or Defect 'D') for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing records of past rounds'
                                            actions and payoffs. Is None for the first round (round 0).

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds:
            return Action.D
        cooperators_prev_round_opponents = np.sum(history.opponent_actions[-1, :])
        cooperated_self_prev_round = history.my_actions[-1]
        C_prev = cooperators_prev_round_opponents + cooperated_self_prev_round
        if C_prev >= m_threshold:
            return Action.D
        else:
            shortfall = m_threshold - C_prev
            if shortfall == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_439(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for the Collective Risk Dilemma.
    This strategy is purely self-interested, aiming to maximize its own payoff.
    It attempts to free-ride whenever the cooperation threshold 'm' was met in
    the previous round, or in the first and last rounds. If the threshold was
    not met in the previous round, it conditionally cooperates to help meet
    the threshold, thereby creating future free-riding opportunities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev_round_bool = history.my_actions[current_round_0_indexed - 1]
        opponent_actions_prev_round_bool = history.opponent_actions[current_round_0_indexed - 1, :]
        total_cooperators_prev_round = int(my_action_prev_round_bool) + np.sum(opponent_actions_prev_round_bool)
        if total_cooperators_prev_round >= self.m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_440(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily free-riding on the
    cooperation of other players. It will strategically cooperate only when
    absolutely necessary to prevent the total collapse of cooperation, thus
    preserving the potential for future free-riding opportunities. However,
    it will quickly revert to defection if its cooperative efforts are not
    met with success, avoiding being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_coop_attempt_failed: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds_r = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_zero_indexed == 0:
            self.last_coop_attempt_failed = False
            return Action.D
        if current_round_zero_indexed == total_rounds_r - 1:
            return Action.D
        my_action_prev_round_is_cooperate = history.my_actions[current_round_zero_indexed - 1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_zero_indexed - 1, :])
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_cooperate else 0)
        threshold_met_prev_round = total_cooperators_prev_round >= min_cooperators_m
        if threshold_met_prev_round:
            action_to_take = Action.D
            self.last_coop_attempt_failed = False
        elif self.last_coop_attempt_failed:
            action_to_take = Action.D
        else:
            action_to_take = Action.C
            self.last_coop_attempt_failed = True
        return action_to_take

class Strategy_EXPLOITATIVE_441(BaseStrategy):
    """
    The "Smart Conditional Free-Rider" strategy aims to maximize personal payoff by aggressively 
    free-riding whenever the collective reward (k) is likely secured by others, and by strategically 
    cooperating only when its contribution is critical and effective in achieving the m cooperation threshold. 
    It explicitly accounts for the repeated game structure and the special conditions of the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        m = self.game_description.m
        n_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_is_C = history.my_actions[prev_round_idx]
        C_others_prev = np.sum(history.opponent_actions[prev_round_idx])
        C_total_prev = C_others_prev + (1 if my_action_prev_is_C else 0)
        if C_total_prev >= m:
            if C_others_prev >= m:
                return Action.D
            else:
                return Action.C
        elif C_others_prev + 1 >= m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_442(BaseStrategy):
    """
    The Threshold-Sensitive Adaptive Exploiter (TSAE) strategy aims to free-ride whenever
    the collective good (k bonus) appears secure without its contribution, and
    strategically contribute only when its contribution is pivotal to achieving that bonus.
    It adapts its behavior based on the observed collective cooperation in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.n_rounds - 1:
            return Action.D
        opponents_prev_cooperators = sum(history.opponent_actions[current_round_number - 1, :])
        my_prev_action_cooperated = history.my_actions[current_round_number - 1]
        N_C_prev = opponents_prev_cooperators + my_prev_action_cooperated
        if N_C_prev >= self.m:
            return Action.D
        elif N_C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_443(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to maximize individual payoff by strategically
    free-riding when possible and only contributing when cooperation is pivotal and individually beneficial.
    It is designed to be adaptive to observed outcomes while maintaining an exploitative core.

    Core Principles:
    1. Start Selfishly: Begin by defecting.
    2. End Selfishly: In the final round, free-ride.
    3. Exploit Success: If the collective project succeeded in the previous round, free-ride.
    4. Punish Failure: If the collective project failed despite my cooperation, stop cooperating.
    5. Pivotal Contribution: Only cooperate if my contribution is absolutely necessary to meet
       the threshold (i.e., I am "pivotal") AND the resulting collective reward (k) makes it
       individually more profitable than defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_num = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        if round_num == 1:
            return Action.D
        if round_num == total_rounds:
            return Action.D
        my_action_t_prev_is_C = history.my_actions[state.round_number - 1]
        my_action_t_prev_int = 1 if my_action_t_prev_is_C else 0
        num_opponent_cooperators_t_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        total_cooperators_t_prev = num_opponent_cooperators_t_prev + my_action_t_prev_int
        if total_cooperators_t_prev >= m_threshold:
            return Action.D
        elif my_action_t_prev_int == 1:
            return Action.D
        elif num_opponent_cooperators_t_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_444(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to maximize individual payoff by skillfully
    free-riding on others' cooperation while minimizing personal contribution. It adapts
    to the observed collective outcome of the previous round.

    Core principles:
    1. Free-ride on success: Defect if the project was funded last round.
    2. Strategic cooperation: Cooperate only if my action alone makes the project succeed,
       and the reward 'k' is greater than defecting '1'. (Given k > 1, this is always true).
    3. Avoid futile cooperation: Never cooperate if my contribution cannot shift failure to success.
    4. Last round: Always defect.
    5. First round: Always defect to establish exploitative stance and avoid sucker's payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.D
        my_previous_action_value = 1 if history.my_actions[-1] else 0
        total_cooperators_in_prev_round = history.opponent_actions[-1, :].sum() + my_previous_action_value
        if total_cooperators_in_prev_round >= self.m:
            return Action.D
        else:
            other_cooperators_expected_this_round = history.opponent_actions[-1, :].sum()
            if other_cooperators_expected_this_round + 1 < self.m:
                return Action.D
            elif other_cooperators_expected_this_round + 1 == self.m:
                return Action.C

class Strategy_EXPLOITATIVE_445(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal gain by free-riding on the contributions
    of others, while selectively contributing only when its contribution is
    pivotal and individually profitable to achieve the 'k' bonus. It adapts its
    behavior based on the immediate past collective outcome and its own previous action.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Free-Rider strategy.

        Args:
            game_description (CollectiveRiskDescription): Description of the game
                                                         parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): Historical data from previous rounds. None for the first round.

        Returns:
            Action: The chosen action for the current round (Action.C or Action.D).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        num_opp_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        N_c_t_minus_1 = int(num_opp_cooperators_prev_round) + (1 if my_action_prev_round_is_C else 0)
        if N_c_t_minus_1 >= min_cooperators_m:
            return Action.D
        elif my_action_prev_round_is_C:
            return Action.D
        elif N_c_t_minus_1 == min_cooperators_m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_446(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding
    whenever possible, and strategically cooperating only when its
    contribution is pivotal to secure a greater personal reward,
    while avoiding futile cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == self.r - 1:
            return Action.D
        previous_round_idx = current_round_idx - 1
        c_prev = int(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx, :])
        if c_prev >= self.m:
            return Action.D
        elif c_prev + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_447(BaseStrategy):
    """
    The Opportunistic Catalyst strategy defaults to defecting (free-riding) to maximize individual gain.
    It only acts as a temporary "catalyst" for cooperation when the collective project is on the
    verge of failure, but only under conditions where its cooperation is likely to be directly
    effective. Once the project is back on track, it immediately reverts to exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_of_committed_cooperation_remaining = 0
        self.consecutive_round_failures = 0
        self.FIX_COOPERATION_DURATION = 2
        self.FAILURE_STREAK_THRESHOLD = 1
        self.COOPERATION_IMPACT_WINDOW = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        if current_round_num == total_rounds - 1:
            return Action.D
        if current_round_num == 0:
            return Action.D
        num_cooperators_t_minus_1 = np.sum(history.opponent_actions[-1, :])
        total_cooperators_t_minus_1 = num_cooperators_t_minus_1 + (1 if history.my_actions[-1] else 0)
        if total_cooperators_t_minus_1 >= min_cooperators_m:
            self.consecutive_round_failures = 0
            self.rounds_of_committed_cooperation_remaining = 0
        else:
            self.consecutive_round_failures += 1
            if self.rounds_of_committed_cooperation_remaining > 0:
                self.rounds_of_committed_cooperation_remaining -= 1
            else:
                cooperators_needed = min_cooperators_m - total_cooperators_t_minus_1
                if self.consecutive_round_failures >= self.FAILURE_STREAK_THRESHOLD and cooperators_needed <= self.COOPERATION_IMPACT_WINDOW:
                    self.rounds_of_committed_cooperation_remaining = self.FIX_COOPERATION_DURATION
        if self.rounds_of_committed_cooperation_remaining > 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_448(BaseStrategy):
    """
    The Calculated Cooperator strategy prioritizes free-riding by defecting whenever the collective reward 'k' is secured.
    It strategically intervenes with cooperation only under specific, high-leverage conditions to nudge the group
    towards meeting the 'm' threshold, specifically when its single contribution would complete the threshold.
    It avoids being a "sucker" by reverting to defection if its cooperation in the previous round did not lead to the
    threshold being met. In the final round, it always defects due to lack of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        num_cooperators_opponents_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_round_was_C = history.my_actions[prev_round_idx]
        num_cooperators_prev_round = num_cooperators_opponents_prev_round + (1 if my_action_prev_round_was_C else 0)
        if num_cooperators_prev_round >= self.m:
            return Action.D
        elif my_action_prev_round_was_C:
            return Action.D
        else:
            cooperators_needed_to_meet_m = self.m - num_cooperators_prev_round
            if cooperators_needed_to_meet_m == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_449(BaseStrategy):
    """
    The Conditional Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize total personal payoff by prioritizing
    defection (free-riding) but employs strategic, conditional cooperation
    to ensure the collective reward 'k' is achieved, especially when its own
    contribution is critical. It adapts based on observed collective
    cooperation levels and leverages the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        C_prev = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        m = self.game_description.m
        if C_prev >= m:
            return Action.D
        elif C_prev + 1 < m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_450(BaseStrategy):
    """
    Greedy Adaptive Free-Rider (GAFR) Strategy for Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding on the cooperation
    of others, only contributing when absolutely necessary and highly impactful
    to secure the public good reward. It adapts based on the outcome of the
    immediately preceding round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        prev_round_index_in_history = state.round_number - 1
        c_prev = np.sum(history.opponent_actions[prev_round_index_in_history, :]) + history.my_actions[prev_round_index_in_history]
        if c_prev >= self.m_threshold:
            return Action.D
        elif c_prev == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_451(BaseStrategy):
    """
    The "Adaptive Threshold Exploiter" strategy aims to maximize its own total payoff by strategically cooperating only when it is either necessary to unlock the collective reward or to establish a cooperative environment that can later be exploited, otherwise defaulting to defection. It prioritizes free-riding whenever the collective good is sufficiently provided by others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialises the Adaptive Threshold Exploiter strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing historical actions and payoffs for
                     this player and opponents. None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_number == 0:
            return Action.C
        elif current_round_number == total_rounds - 1:
            return Action.D
        else:
            N_others_C_prev = np.sum(history.opponent_actions[-1, :])
            if N_others_C_prev >= m_threshold:
                return Action.D
            elif N_others_C_prev == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_452(BaseStrategy):
    """
    Adaptive Opportunistic Defector (AOD) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its individual payoff by defecting whenever possible,
    only cooperating when its contribution is critically needed to meet the 'm' threshold
    and that cooperation yields a higher individual payoff (due to k > 1).
    It exploits the willingness of others to cooperate and avoids futile contributions.

    Key principles:
    - Last round defection for immediate individual gain.
    - Adaptive estimation of other players' cooperation based on the previous round.
    - Conditional cooperation only when its contribution is decisive and profitable.
    - Free-riding and avoidance of futile sacrifice in all other scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        estimated_nc_other: int
        if current_round_1_indexed == 1:
            estimated_nc_other = 0
        else:
            prev_round_history_index = state.round_number - 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_history_index, :])
            estimated_nc_other = opponent_cooperators_prev_round
            estimated_nc_other = max(0, estimated_nc_other)
        if estimated_nc_other == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_453(BaseStrategy):
    """
    This strategy aims to opportunistically free-ride whenever possible, provide just enough
    cooperation to meet the collective bonus threshold, and avoid prolonged sacrifice if
    cooperation efforts are futile.

    It prioritizes free-riding when the collective threshold (m) is met with a surplus.
    It reluctantly cooperates when the threshold is barely met to maintain the collective
    benefit. It cooperates in the first round to signal willingness, and always defects
    in the final round. A "despair" mechanism causes it to defect after 3 consecutive
    rounds where the collective threshold was not met, cutting losses and guaranteeing
    a private payoff of 1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_consecutive_fail_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == r - 1:
            return Action.D
        else:
            my_prev_action_val = 1 if history.my_actions[current_round - 1] else 0
            opponent_prev_cooperators = np.sum(history.opponent_actions[current_round - 1, :])
            C_prev = my_prev_action_val + opponent_prev_cooperators
            if C_prev < m:
                self.my_consecutive_fail_rounds += 1
            else:
                self.my_consecutive_fail_rounds = 0
            if C_prev >= m:
                if C_prev > m:
                    return Action.D
                else:
                    return Action.C
            elif self.my_consecutive_fail_rounds >= 3:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_454(BaseStrategy):
    """
    Calculated Opportunist (CO-v2) strategy for the Collective Risk Dilemma.
    This strategy balances initial probing, aggressive free-riding, and calculated
    minimal cooperation to secure the collective reward when it's critically at stake
    and our contribution is pivotal. It prioritizes individual payoff maximization.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.r - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        prev_round_idx = state.round_number - 1
        my_last_action_was_C = history.my_actions[prev_round_idx]
        prev_opponent_cooperators_count = np.sum(history.opponent_actions[prev_round_idx, :])
        prev_total_cooperators_count = prev_opponent_cooperators_count + (1 if my_last_action_was_C else 0)
        was_threshold_met_prev = prev_total_cooperators_count >= self.m
        if was_threshold_met_prev:
            cooperators_without_me_prev = prev_total_cooperators_count - (1 if my_last_action_was_C else 0)
            if cooperators_without_me_prev >= self.m:
                return Action.D
            else:
                return Action.D
        else:
            cooperators_needed = self.m - prev_total_cooperators_count
            if cooperators_needed == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_455(BaseStrategy):
    """
    Adaptive Back-and-Forth for k:
    This strategy aims to exploit the Collective Risk Dilemma by strategically free-riding
    while ensuring the collective benefit 'k' is realized. It adapts its behavior based
    on whether the cooperation threshold 'm' was met in the previous round and its own
    last action. It explicitly handles first and last rounds for optimal exploitation
    and initiation of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._my_last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        if current_round_num == 0:
            last_round_cooperators = 0
        else:
            cooperators_from_my_last_action = int(history.my_actions[-1])
            cooperators_from_opponents = np.sum(history.opponent_actions[-1, :])
            last_round_cooperators = cooperators_from_my_last_action + cooperators_from_opponents
        my_current_action = Action.D
        if current_round_num == total_rounds - 1:
            my_current_action = Action.D
        elif current_round_num == 0:
            my_current_action = Action.C
        elif last_round_cooperators >= min_cooperators_needed:
            my_current_action = Action.D
        elif self._my_last_action == Action.C:
            my_current_action = Action.D
        elif self._my_last_action == Action.D:
            my_current_action = Action.C
        else:
            my_current_action = Action.C
        self._my_last_action = my_current_action
        return my_current_action

class Strategy_EXPLOITATIVE_456(BaseStrategy):
    """
    Adaptive Free-Rider with Collective Benefit Assurance for the Collective Risk Dilemma.

    This strategy aims to exploit free-riding opportunities while ensuring the collective
    benefit `k` is met when the player's action is individually rational and decisive
    in reaching the `m` cooperator threshold.

    Decision Rules per Round (1-indexed):
    1. Last Round (`r`): Always Defect. Maximizes immediate payoff as there are no future consequences.
    2. First Round (`1`): Always Defect. Probes for free-riding opportunities and gathers information.
    3. Intermediate Rounds (`1 < t < r`):
       - If `m` cooperators were met in the previous round:
         - If `m` cooperators would still be met without my previous cooperation: Defect (free-ride).
         - If my previous cooperation was crucial to meet `m`: Cooperate (ensure `k`, since `k > 1`).
       - If `m` cooperators were NOT met in the previous round:
         - If my cooperation alone would reach `m`: Cooperate (ensure `k`, since `k > 1`).
         - If my cooperation alone is NOT enough to reach `m`: Defect (avoid wasted cooperation).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_round_is_C = history.my_actions[prev_round_idx]
        num_cooperators_prev_round_opponents = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev_round_total = num_cooperators_prev_round_opponents + (1 if my_action_prev_round_is_C else 0)
        if num_cooperators_prev_round_total >= self.m:
            cooperators_excluding_me_prev = num_cooperators_prev_round_total - (1 if my_action_prev_round_is_C else 0)
            if cooperators_excluding_me_prev >= self.m:
                return Action.D
            else:
                return Action.C
        elif num_cooperators_prev_round_total == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_457(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by primarily free-riding on the
    cooperation of others. It only contributes (cooperates) when its contribution
    is both essential to trigger the collective reward 'k' AND doing so yields a
    higher personal payoff than defecting. It prioritizes self-interest in every
    decision, adapting to the observed behavior of opponents from the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.D
        if current_round_zero_indexed == self.n_rounds - 1:
            return Action.D
        num_other_cooperators_prev = np.sum(history.opponent_actions[current_round_zero_indexed - 1, :])
        if num_other_cooperators_prev >= self.m:
            return Action.D
        elif num_other_cooperators_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_458(BaseStrategy):
    """
    The "Opportunistic Nudger" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by defaulting to free-riding
    (defecting) and only cooperating when its contribution is critically needed
    to secure or maintain the 'k' bonus for the group. It adapts its behavior
    based on the outcome of the immediate preceding round. It defects in both
    the first and last rounds due to specific rationales.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == r - 1:
            return Action.D
        elif current_round_0_indexed == 0:
            return Action.D
        else:
            my_action_prev_round_bool = history.my_actions[-1]
            opponent_actions_prev_round_bool = history.opponent_actions[-1, :]
            num_opponent_cooperators_prev_round = np.sum(opponent_actions_prev_round_bool)
            num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_bool else 0)
            threshold_met_prev_round = num_cooperators_prev_round >= m
            if threshold_met_prev_round:
                if num_cooperators_prev_round > m:
                    return Action.D
                elif my_action_prev_round_bool == False:
                    return Action.D
                else:
                    return Action.C
            else:
                cooperators_needed = m - num_cooperators_prev_round
                if cooperators_needed <= 1:
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_459(BaseStrategy):
    """
    Adaptive Free-Rider with Re-Initialization strategy for the Collective Risk Dilemma.

    This strategy is designed to be exploitative, prioritizing individual payoff by
    defaulting to defection (free-riding). However, it adapts to prevent the system
    from collapsing into a low-payoff state (universal defection), which would be
    suboptimal for this player too.

    The strategy operates in three main modes:
    1.  **Free-Riding:** The default mode where the player defects if the collective
        threshold `m` was met in the previous round, or if their cooperation alone
        cannot make the collective meet `m`. This maximizes immediate private gain.
    2.  **Pivotal Cooperation:** If the number of cooperators in the previous round
        was `m-1`, this strategy cooperates. Its cooperation alone is pivotal to
        reach the `m` threshold, thereby securing the collective reward `k` (which
        is greater than 1). Cooperating to receive `k` is better than defecting to
        receive `1` in this specific scenario.
    3.  **Re-initiation:** If the collective consistently fails to meet the `m`
        threshold for `REINITIATION_TRIGGER_THRESHOLD` consecutive rounds, the
        strategy enters a temporary cooperation phase (`REINITIATION_DURATION` rounds).
        This is a strategic investment to "prime the pump" and encourage others to
        restore cooperation, allowing the strategy to return to free-riding in the
        long term for higher payoffs.

    Special handling for the first and last rounds ensures maximum immediate payoff,
    as there's no history to learn from or future rounds to influence, respectively.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.REINITIATION_TRIGGER_THRESHOLD = 3
        self.REINITIATION_DURATION = min(self.game_description.m, 3)
        self.consecutive_fail_rounds = 0
        self.cooperation_reinitiation_phase = False
        self.reinitiation_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == n_rounds - 1:
            return Action.D
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        my_action_prev_was_C = history.my_actions[-1]
        N_C_prev = prev_round_opponent_cooperators + (1 if my_action_prev_was_C else 0)
        if N_C_prev < m:
            self.consecutive_fail_rounds += 1
        else:
            self.consecutive_fail_rounds = 0
        if self.cooperation_reinitiation_phase:
            if self.reinitiation_rounds_left > 0:
                self.reinitiation_rounds_left -= 1
                return Action.C
            else:
                self.cooperation_reinitiation_phase = False
        if self.consecutive_fail_rounds >= self.REINITIATION_TRIGGER_THRESHOLD:
            self.cooperation_reinitiation_phase = True
            self.reinitiation_rounds_left = self.REINITIATION_DURATION - 1
            return Action.C
        if N_C_prev >= m:
            return Action.D
        elif N_C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_460(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual total payoff by being highly reactive
    to the previous round's outcome, seeking to free-ride when possible, but also
    strategically contributing to restore cooperation when necessary and viable.
    It incorporates an end-game effect.

    Decision Rules:
    1.  Round t=1 (First Round): Cooperate (C) to probe and establish a baseline.
    2.  Rounds 1 < t < r (Mid-Game Rounds):
        -   If C_prev >= m (Threshold met last round): Defect (D) to free-ride.
        -   If C_prev < m (Threshold not met last round):
            -   If (C_prev + 1) < m: Defect (D) to cut losses, as own cooperation is futile.
            -   Else ((C_prev + 1) >= m): Cooperate (C) to strategically restore cooperation.
    3.  Round t=r (Last Round): Defect (D) to maximize immediate payoff, as no future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.C
        prev_round_index = current_round_number - 1
        my_prev_cooperation = history.my_actions[prev_round_index]
        opponent_prev_cooperations = np.sum(history.opponent_actions[prev_round_index, :])
        C_prev = int(my_prev_cooperation) + int(opponent_prev_cooperations)
        if C_prev >= self.m:
            return Action.D
        elif C_prev + 1 < self.m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_461(BaseStrategy):
    """
    The "Adaptive Threshold Exploiter (ATE)" strategy is designed to maximize individual payoff in the Collective Risk Dilemma
    by strategically free-riding while making calculated contributions to ensure the collective reward remains accessible.
    It adapts its behavior based on the observed level of cooperation in previous rounds and its own past actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.game_description.n_rounds - 1:
            return Action.D
        my_action_prev_round_was_C = history.my_actions[-1]
        opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + int(my_action_prev_round_was_C)
        if total_cooperators_prev_round >= self.game_description.m + 1:
            return Action.D
        if total_cooperators_prev_round < self.game_description.m - 1:
            return Action.D
        if total_cooperators_prev_round == self.game_description.m - 1:
            return Action.C
        if total_cooperators_prev_round == self.game_description.m:
            if not my_action_prev_round_was_C:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_462(BaseStrategy):
    """
    Adaptive Exploiter with Terminal Defection (AETD) strategy.

    This strategy balances the desire to free-ride with the need to maintain the
    collective benefit of meeting the cooperation threshold 'm'. It observes past
    outcomes and adjusts its own contribution, always with an eye toward maximizing
    its own total payoff.

    Core Principles:
    1. Prioritize '1+k': Aim for the highest possible payoff by defecting when 'k' is secured.
    2. Ensure 'k': Act to restore the 'k' reward if the group consistently fails to meet 'm'.
    3. Avoid being a "Sucker": Do not cooperate indefinitely if it yields no collective benefit.
    4. Terminal Exploitation: Defect in the final round for maximum immediate gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        if current_round_t == 1:
            return Action.C
        my_previous_action_bool = history.my_actions[state.round_number - 1]
        num_cooperators_previous_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        num_cooperators_previous_round += 1 if my_previous_action_bool else 0
        if num_cooperators_previous_round >= self.game_description.m:
            return Action.D
        elif my_previous_action_bool:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_463(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to maximize its own total payoff over 'r' rounds.
    It primarily free-rides on the cooperation of others but critically contributes when its cooperation
    is essential to secure the collective reward 'k' (given k > 1). It adapts its behavior based on
    the observed total number of cooperators in the immediate previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        num_cooperators_opponents_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_was_cooperation = history.my_actions[-1]
        n_c_prev = num_cooperators_opponents_prev + (1 if my_action_prev_was_cooperation else 0)
        if n_c_prev >= m_threshold:
            return Action.D
        elif n_c_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_464(BaseStrategy):
    """
    Reactive Free-Rider with Minimum Sustenance (RFR-MS) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by free-riding on others' cooperation
    and only contributing when its cooperation is critical to securing the collective reward 'k',
    and when that contribution is individually beneficial. It adapts its behavior based on
    the observed cooperation levels from the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.D
        elif current_round_zero_indexed == self.n_rounds - 1:
            return Action.D
        else:
            last_num_cooperators_excluding_self = np.sum(history.opponent_actions[-1, :])
            if last_num_cooperators_excluding_self >= self.m_threshold:
                return Action.D
            elif last_num_cooperators_excluding_self == self.m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_465(BaseStrategy):
    """
    The Opportunistic Prober-FreeRider (OPFR) strategy aims to maximize individual payoff
    by free-riding while being adaptive. It starts by probing for cooperation in the first round.
    In intermediate rounds, it primarily defects, but will cooperate if its contribution is
    critical to meet the cooperation threshold ('m') and secure the 'k' reward. It also
    self-corrects by cooperating if its previous defection caused the group to miss 'k'
    when it was pivotal. In the final 'L' rounds, it unconditionally defects due to endgame
    unraveling.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.L = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        chosen_action: Action
        if t == 1:
            chosen_action = Action.C
        elif t > self.r - self.L:
            chosen_action = Action.D
        else:
            C_observed_prev_opponents = np.sum(history.opponent_actions[-1, :])
            my_action_prev_bool = history.my_actions[-1]
            my_action_prev_enum = Action.C if my_action_prev_bool else Action.D
            C_observed_prev_total = C_observed_prev_opponents + int(my_action_prev_bool)
            if my_action_prev_enum == Action.D and C_observed_prev_total == self.m - 1:
                chosen_action = Action.C
            elif C_observed_prev_total >= self.m:
                chosen_action = Action.D
            elif C_observed_prev_total == self.m - 1:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        return chosen_action

class Strategy_EXPLOITATIVE_466(BaseStrategy):
    """
    Exploitative K-Rescuer strategy for the Collective Risk Dilemma.

    Core Principle: Free-ride whenever the collective reward 'k' is being
    successfully delivered by others. If 'k' is consistently missed,
    temporarily contribute for one round to try and restore it. If that
    temporary contribution doesn't work, revert to free-riding to avoid
    being a "sucker." Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if current_round_t == total_rounds:
            return Action.D
        if current_round_t == 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_idx]
        n_c_prev_opponents = np.sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev = n_c_prev_opponents + int(my_action_prev)
        if total_cooperators_prev >= min_cooperators:
            return Action.D
        elif my_action_prev == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_467(BaseStrategy):
    """
    Adaptive Exploiter with Calculated Sacrifice strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by free-riding on others' cooperation.
    It defects in the first and last rounds. In intermediate rounds, it defects if the
    collective reward (k) was achieved in the previous round. If the collective reward
    was not achieved, it makes a "calculated sacrifice" by cooperating only if its
    cooperation is precisely what is needed to meet the threshold 'm', assuming others
    who cooperated previously continue to do so. Otherwise, it defects to avoid futile
    sacrifice.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_zero_indexed == 0:
            return Action.D
        if current_round_zero_indexed == total_rounds - 1:
            return Action.D
        other_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        num_cooperators_prev_round = other_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if num_cooperators_prev_round >= m_threshold:
            return Action.D
        if other_cooperators_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_468(BaseStrategy):
    """
    This strategy, named "Conditional Free-Rider with Probe," is designed to be highly
    exploitative by prioritizing individual payoff maximization, adapting to observed
    opponent behavior, and leveraging the game's structure, especially the end-game effect.

    Core Philosophy:
    The strategy aims to free-ride on the cooperation of others as much as possible,
    contributing only when its contribution is individually necessary and beneficial
    to achieve the collective reward `k`. It probes for cooperation initially and
    ensures it does not become a "sucker" by cooperating when the threshold is
    unlikely to be met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game's parameters.
        No mutable state is required as decisions are based solely on the current
        round and the immediate past history.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Contains the current round number (0-indexed).
            history (PlayerHistory | None): Contains records of past actions and payoffs
                                            for this player and opponents. None for round 0.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        elif current_round_0_indexed == n_rounds - 1:
            return Action.D
        else:
            previous_round_0_indexed = current_round_0_indexed - 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_0_indexed, :])
            my_action_prev_round_was_C = history.my_actions[previous_round_0_indexed]
            num_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
            if num_cooperators_prev_round >= m_threshold:
                return Action.D
            elif num_cooperators_prev_round == m_threshold - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_469(BaseStrategy):
    """
    The "Opportunistic Catalyst" strategy is designed to be highly exploitative in the
    Collective Risk Dilemma. It prioritizes individual payoff maximization through
    free-riding while adaptively contributing only when strategically necessary
    to reactivate the public good for future exploitation or to secure an immediate,
    higher payoff. It does not rely on shared norms or specific opponent types,
    adapting instead to observed collective behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_last_round: int = 0
        self.consecutive_failures: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number > 0:
            prev_round_idx = state.round_number - 1
            my_prev_action_was_C = history.my_actions[prev_round_idx]
            prev_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
            total_cooperators_prev_round = (1 if my_prev_action_was_C else 0) + prev_opponent_cooperators
            self.cooperators_last_round = total_cooperators_prev_round
            if self.cooperators_last_round < m:
                self.consecutive_failures += 1
            else:
                self.consecutive_failures = 0
        if state.round_number == 0:
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        if self.cooperators_last_round >= m:
            return Action.D
        elif self.cooperators_last_round == m - 1:
            return Action.C
        elif self.consecutive_failures >= 2 and self.cooperators_last_round > 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_471(BaseStrategy):
    """
    Calculated Opportunist strategy for the Collective Risk Dilemma.
    This strategy aims to free-ride whenever possible, defecting by default.
    It only cooperates under specific, calculated conditions:
    1. The 'k' bonus is sufficiently valuable (`k > 2`).
    2. The collective threshold 'm' has been missed for `D_ROUNDS_TOLERANCE`
       consecutive rounds, indicating a persistent problem.
    3. There are enough remaining rounds to potentially benefit from future free-riding
       after a temporary cooperation phase.
    When these conditions are met, it triggers a temporary cooperation phase
    (`TRIGGER_COOPERATE_DURATION` rounds) to try and kickstart collective success,
    then reverts to free-riding. It always defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_needed = game_description.m
        self.k_reward_factor = game_description.k
        self.D_ROUNDS_TOLERANCE = max(3, self.n_players - self.m_needed + 1)
        self.TRIGGER_COOPERATE_DURATION = 1
        self.consecutive_failures = 0
        self.is_in_trigger_phase = False
        self.trigger_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_action_prev_is_C = 1 if history.my_actions[prev_round_idx] else 0
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev = my_action_prev_is_C + opponent_cooperators_prev
        was_successful_prev = num_cooperators_prev >= self.m_needed
        if self.is_in_trigger_phase:
            if self.trigger_rounds_left > 0:
                self.trigger_rounds_left -= 1
                return Action.C
            else:
                self.is_in_trigger_phase = False
                self.consecutive_failures = 0
                return Action.D
        elif was_successful_prev:
            self.consecutive_failures = 0
            return Action.D
        else:
            self.consecutive_failures += 1
            cond_k_valuable = self.k_reward_factor > 2
            cond_failures_tolerated = self.consecutive_failures >= self.D_ROUNDS_TOLERANCE
            future_rounds_after_current = self.n_rounds - current_round_1_indexed
            cond_enough_rounds_left = future_rounds_after_current > self.TRIGGER_COOPERATE_DURATION
            if cond_k_valuable and cond_failures_tolerated and cond_enough_rounds_left:
                self.is_in_trigger_phase = True
                self.trigger_rounds_left = self.TRIGGER_COOPERATE_DURATION
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_472(BaseStrategy):
    """
    An Adaptive Free-Rider with Conditional Enforcement strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by free-riding whenever possible,
    but strategically contributes to ensure the collective 'k' bonus is achieved,
    especially when its contribution is critical. It avoids being a "sucker" by
    not cooperating when its individual contribution is unlikely to make a difference.

    Decision Rules per Round:
    1. First Round (state.round_number == 0): Cooperate (C) to initiate cooperation and gather data.
    2. Last Round (state.round_number == self.n_rounds - 1): Defect (D) to maximize immediate payoff,
       as there are no future consequences.
    3. Intermediate Rounds:
        a. Calculate C_prev: The total number of cooperators in the previous round.
        b. If C_prev >= self.m (threshold met previously): Defect (D) to free-ride.
        c. If C_prev == self.m - 1 (one short of threshold): Cooperate (C) to be the critical contributor.
        d. If C_prev < self.m - 1 (far from threshold): Defect (D) to avoid being a "sucker."
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_prev_round >= self.m:
            return Action.D
        elif cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_473(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily defecting (free-riding).
    It only cooperates strategically in a very specific scenario: when its contribution
    is critical to meet the collective threshold 'm', and doing so yields a higher
    personal payoff ('k') than defecting ('1').

    Decision Rules (for current 0-indexed round `state.round_number`):
    1.  If the current round is the final round (state.round_number == self.game_description.n_rounds - 1):
        Defect ('D'). There are no future interactions to influence, so the optimal
        strategy is to maximize immediate individual payoff. Defecting guarantees
        a payoff of 1 (if threshold not met) or 1+k (if met), both better than 0 or k
        respectively, if one were to cooperate.

    2.  If the current round is the first round (state.round_number == 0):
        Defect ('D'). With no prior history, the most exploitative starting point is to
        defect. This serves as an initial probe: if enough other players cooperate, this
        strategy immediately free-rides for 1+k. If not, it still secures a payoff of 1.

    3.  For all intermediate rounds (0 < state.round_number < self.game_description.n_rounds - 1):
        a.  Determine `num_others_C_prev`: the number of cooperators among *other players*
            in the immediately preceding round (state.round_number - 1).

        b.  Scenario A: The 'm' threshold was met by others (or would have been met
            even if this player defected) in the previous round.
            (i.e., `num_others_C_prev >= self.game_description.m`)
            Decision: Defect ('D'). Other players are providing enough cooperation.
            This player can confidently free-ride, securing 1+k.

        c.  Scenario B: This player's cooperation is potentially critical to meet the 'm' threshold.
            (i.e., `num_others_C_prev == self.game_description.m - 1`)
            Decision: Cooperate ('C'). If this player cooperates, the 'm' threshold will be met.
            Potential payoff for cooperating is 'k'. Potential payoff for defecting is '1'.
            Since `k > 1` (as per game specification), 'k' is strictly greater than '1',
            making cooperation the self-interested choice in this specific situation.

        d.  Scenario C: This player's cooperation is insufficient to meet the 'm' threshold.
            (i.e., `num_others_C_prev < self.game_description.m - 1`)
            Decision: Defect ('D'). Even if this player cooperates, the total number of
            cooperators (`num_others_C_prev + 1`) will still be less than 'm'. The
            threshold will not be met. Cooperating would yield 0, while defecting
            guarantees 1. Maximize individual payoff by avoiding a wasted contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        if state.round_number == total_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.D
        num_others_C_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        if num_others_C_prev >= m_threshold:
            return Action.D
        elif num_others_C_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_474(BaseStrategy):
    """
    Opportunistic K-Enabler strategy for the Collective Risk Dilemma.
    This strategy aims to free-ride whenever possible but strategically cooperates
    to ensure the collective bonus 'k' is achieved or maintained. It adapts its
    behavior based on observed cooperation levels and endgame effects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            round_idx_prev = state.round_number - 1
            own_action_prev_is_C = history.my_actions[round_idx_prev]
            num_own_cooperated_prev = 1 if own_action_prev_is_C else 0
            opp_cooperators_prev = np.sum(history.opponent_actions[round_idx_prev, :])
            total_cooperators_prev = opp_cooperators_prev + num_own_cooperated_prev
            if total_cooperators_prev >= self.m_threshold:
                cooperators_excluding_self_prev = opp_cooperators_prev
                if cooperators_excluding_self_prev >= self.m_threshold:
                    return Action.D
                else:
                    return Action.C
            elif total_cooperators_prev <= math.floor((self.m_threshold - 1) / 2):
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_475(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own payoff by strategically leveraging 
    the collective actions of other players. It prioritizes free-riding when the collective good is 
    already secured, and only cooperates when its contribution is both necessary and directly 
    beneficial to achieving the collective reward (k).

    Decision Rules:
    1. Round 1: Defect (D) to probe opponent behavior without cost.
    2. Last Round: Defect (D) as there are no future consequences.
    3. Middle Rounds:
       a. If threshold (m) was met in the previous round: Defect (D) to free-ride.
       b. If threshold was NOT met in the previous round:
          i. If this player's cooperation is decisive (i.e., exactly m-1 cooperators previously): Cooperate (C) to secure the 'k' bonus (since k > 1).
          ii. Otherwise (less than m-1 cooperators previously): Defect (D) to avoid wasted effort, as cooperation alone cannot meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        current_round_t_0_indexed = state.round_number
        if current_round_t_0_indexed == 0:
            return Action.D
        if current_round_t_0_indexed == n_rounds - 1:
            return Action.D
        my_action_prev_round_is_C = history.my_actions[-1]
        num_opponent_cooperators_prev_round = history.opponent_actions[-1, :].sum()
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_is_C)
        if num_total_cooperators_prev_round >= m:
            return Action.D
        elif num_total_cooperators_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_476(BaseStrategy):
    """
    Opportunistic Defector strategy for the Collective Risk Dilemma.

    This strategy aims to free-ride aggressively when the collective benefit seems assured,
    but is willing to cooperate strategically if its contribution is critical and can turn
    a collective failure (N_C < m) into a success (N_C >= m). It prioritizes maximizing
    individual payoff by achieving 1+k (defecting when k is met), then k (cooperating to meet k),
    then 1 (defecting when k is missed and my cooperation is futile), avoiding 0.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        my_action_prev = history.my_actions[prev_round_index]
        opponent_actions_prev = history.opponent_actions[prev_round_index, :]
        N_C_others_prev = np.sum(opponent_actions_prev)
        N_C_prev = N_C_others_prev + (1 if my_action_prev else 0)
        if N_C_prev >= self.m:
            if N_C_others_prev >= self.m:
                return Action.D
            else:
                return Action.D
        elif N_C_prev + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_478(BaseStrategy):
    """
    The Opportunistic Threshold-Follower (OTF) strategy aims to maximize its
    individual payoff in the Collective Risk Dilemma. It selectively free-rides
    when possible, only contributing when its cooperation is critical to
    achieving the collective reward, and strictly defecting when its
    contribution would be futile or when no future consequences exist.

    Decision Rules:
    1. Round 1 (Initial Probe): Defect.
    2. Intermediate Rounds (Round 2 to r-1):
        - If other players met or exceeded the threshold 'm' in the previous round: Defect (free-ride).
        - If other players were exactly 'm-1' cooperators in the previous round: Cooperate (critical contribution for k > 1 payoff).
        - If other players were less than 'm-1' cooperators in the previous round: Defect (contribution is futile).
    3. Last Round (r): Defect (end-game exploitation).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward_factor = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        cooperators_from_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if cooperators_from_others_prev_round >= m_threshold:
            return Action.D
        elif cooperators_from_others_prev_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_480(BaseStrategy):
    """
    Stabilizing Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by securing the collective
    reward (k) and then exploiting the system by defecting whenever possible
    without risking the collapse of cooperation. It contributes to cooperation
    when the collective project is failing or at risk, and defects otherwise.

    Decision Rules:
    1. Round 1 (state.round_number == 0): Cooperate (C) to help establish the 'k' reward.
    2. Last Round (state.round_number == total_rounds - 1): Defect (D) for pure exploitation,
       as there are no future consequences.
    3. Intermediate Rounds:
        a. If `C_prev < m` (Collective Failure in previous round): Cooperate (C) to stabilize
           the system and re-establish the potential for 'k'.
        b. If `C_prev >= m` (Collective Success in previous round):
            i. If `C_prev - 1 >= m` (Sufficient Slack for Defection): Defect (D). My defection
               is unlikely to cause the system to drop below the 'm' threshold.
            ii. If `C_prev - 1 < m` (Insufficient Slack - Risky Defection):
                - If I cooperated in the previous round (`my_action_prev == C`): Cooperate (C).
                  My defection would likely cause a collective failure.
                - If I defected in the previous round (`my_action_prev == D`): Defect (D).
                  I was already free-riding, and the system still succeeded.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        cooperators_opponents_prev = np.sum(history.opponent_actions[prev_round_index, :])
        my_action_prev_is_C = history.my_actions[prev_round_index]
        C_prev = cooperators_opponents_prev + (1 if my_action_prev_is_C else 0)
        min_cooperators_m = self.game_description.m
        if C_prev < min_cooperators_m:
            return Action.C
        elif C_prev - 1 >= min_cooperators_m:
            return Action.D
        elif my_action_prev_is_C:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_481(BaseStrategy):
    """
    Adaptive Collective Aversion (ACA) strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly exploitative by maximizing individual gain
    while leveraging the collective good. It focuses on free-riding whenever possible
    and strategically contributing only when necessary to restore the collective benefit,
    or to initiate it, based on the outcome of the immediately preceding round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        prev_C_count = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if current_round_0_indexed < self.n_rounds - 1:
            if prev_C_count >= self.m:
                return Action.D
            else:
                return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            if prev_C_count >= self.m:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_482(BaseStrategy):
    """
    Implements the Adaptive Free-Rider with Strategic Intervention strategy for the Collective Risk Dilemma.
    Prioritizes defection (free-riding) when the collective threshold 'm' is met.
    If the threshold is not met, strategically cooperates only when one's own cooperation
    is the minimum necessary to unlock the collective reward 'k'. Otherwise, defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_0_indexed = state.round_number
        current_round_1_indexed = state.round_number + 1
        my_action = Action.D
        if current_round_1_indexed == 1:
            my_action = Action.D
        elif current_round_1_indexed == r:
            my_action = Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[current_round_0_indexed - 1, :])
            my_action_prev_round_is_cooperate = history.my_actions[current_round_0_indexed - 1]
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_is_cooperate)
            if total_cooperators_prev_round >= m:
                my_action = Action.D
            elif total_cooperators_prev_round == m - 1:
                my_action = Action.C
            else:
                my_action = Action.D
        return my_action

class Strategy_EXPLOITATIVE_483(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma called 'Adaptive Shirk-and-Punish'.

    This strategy aims to maximize its own payoff by primarily free-riding on the cooperation
    of other players. It will only cooperate under specific conditions: to strategically
    "invest" when its cooperation is critical for the collective reward, or to "punish"
    widespread defection by refusing to contribute when its efforts would be futile.
    The default stance is to defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_1_indexed_round = state.round_number + 1
        if current_1_indexed_round == 1:
            return Action.D
        if current_1_indexed_round == n_rounds:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1])
        my_cooperation_prev = int(history.my_actions[state.round_number - 1])
        N_C_prev = num_opponent_cooperators_prev + my_cooperation_prev
        if N_C_prev >= m_threshold:
            return Action.D
        else:
            deficit = m_threshold - N_C_prev
            if deficit <= 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_484(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy aims to maximize individual payoff by strategically
    leveraging the collective good (k) while minimizing personal contribution. It adapts 
    its behavior based on observed actions, free-riding when possible and only cooperating
    when its contribution is essential to secure the 'k' reward for itself.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_1_indexed == n_rounds:
            return Action.D
        if state.round_number == 0:
            return Action.D
        C_prev = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        my_action_prev_bool = history.my_actions[-1]
        if C_prev >= m_threshold:
            if my_action_prev_bool == True:
                if C_prev - 1 < m_threshold:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        else:
            others_C_prev = np.sum(history.opponent_actions[-1, :])
            if others_C_prev + 1 >= m_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_485(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy aims to maximize individual payoff by exploiting
    others' willingness to cooperate. It primarily defects to free-ride,
    but strategically cooperates only when its contribution is pivotal to reaching
    the 'm' cooperator threshold and directly beneficial (k > 1 ensures this).
    It always defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        c_others_prev = np.sum(previous_round_opponent_actions)
        if c_others_prev >= self.m:
            return Action.D
        elif c_others_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_486(BaseStrategy):
    """
    The Adaptive Free-Rider with Strategic Intervention (AFRSI) strategy.
    This strategy is highly self-interested, aiming to maximize personal payoff by free-riding
    on others' contributions when the collective reward (k) is guaranteed. It strategically
    cooperates only when its contribution is critical to achieving or re-establishing the k reward,
    ensuring it still benefits from the collective outcome. It avoids being a "sucker" by not
    cooperating when its individual effort is clearly insufficient to achieve the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        num_cooperators_opponents_prev = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev = num_cooperators_opponents_prev + (1 if my_action_prev_bool else 0)
        if num_cooperators_prev >= self.m_threshold:
            cooperators_if_i_had_defected = num_cooperators_prev - (1 if my_action_prev_bool else 0)
            if cooperators_if_i_had_defected >= self.m_threshold:
                return Action.D
            else:
                return Action.C
        elif num_cooperators_prev + 1 >= self.m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_487(BaseStrategy):
    """
    The Opportunistic Threshold Enabler (OTE) strategy is highly exploitative and adaptive.
    It free-rides whenever possible, contributing only when its individual cooperation
    can directly secure a greater personal payoff (k > 2) by enabling the collective reward.
    It defects in the first and last rounds and when the project is "hopeless" or already successful.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the OTE strategy with game parameters.
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the OTE strategy rules.
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        num_opponent_cooperators_prev = sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_was_C = history.my_actions[state.round_number - 1]
        N_C_prev = num_opponent_cooperators_prev + (1 if my_action_prev_was_C else 0)
        if N_C_prev >= self.m:
            return Action.D
        elif N_C_prev == self.m - 1:
            if self.k > 2.0:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_488(BaseStrategy):
    """
    Proactive Threshold Exploiter (PTE) Strategy.

    This strategy is designed to maximize individual payoff in the Collective Risk Dilemma.
    It strategically initiates cooperation in the first round to test the waters,
    then free-rides on established collective benefits by defecting. If cooperation
    fails, it reacts by defecting to avoid being a "sucker", but will proactively
    cooperate if its single contribution can critically secure the collective reward (k > 1).
    In the final round, it always defects as there are no future rounds to influence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        if state.round_number == total_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_prev_action_bool = history.my_actions[state.round_number - 1]
        my_prev_action_enum = Action.C if my_prev_action_bool else Action.D
        opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_opponents_prev_round = np.sum(opponent_actions_prev_round)
        num_cooperators_prev_round = num_cooperators_opponents_prev_round + (1 if my_prev_action_bool else 0)
        if num_cooperators_prev_round >= min_cooperators:
            return Action.D
        elif my_prev_action_enum == Action.C:
            return Action.D
        elif num_cooperators_prev_round == min_cooperators - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_489(BaseStrategy):
    """
    The Conditional Free-Rider with Threshold Enforcement strategy.

    This strategy aims to maximize individual gain by free-riding whenever possible,
    while strategically contributing to ensure the collective reward 'k' is met.
    It adapts to opponent behavior and game state without assuming inherent cooperation
    or pre-arranged coordination.

    Core Principles:
    1. Prioritize 'k': The 'k' bonus is crucial. The strategy aims to ensure 'm'
       cooperators are met to secure 'k', but only when necessary.
    2. Free-ride ruthlessly: Once 'k' is secured (or appears likely to be secured
       by others), the strategy defects to gain the private endowment '+1'.
    3. Punish/Incentivize: If 'k' is not met, the strategy employs conditional
       actions to either "punish" the group's failure (by defecting after previous
       cooperation) or "incentivize" a return to 'k'-securing behavior (by
       cooperating after previous defection).
    4. Terminal exploitation: In the final round, the strategy defects to
       maximize immediate payoff, disregarding future interactions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == total_rounds - 1:
            action_to_take = Action.D
            self.my_last_action = action_to_take
            return action_to_take
        if current_round_0_indexed == 0:
            action_to_take = Action.C
            self.my_last_action = action_to_take
            return action_to_take
        cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_last_round >= m_threshold:
            action_to_take = Action.D
        elif self.my_last_action == Action.C:
            action_to_take = Action.D
        else:
            action_to_take = Action.C
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_490(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by primarily defecting
    and free-riding on others' contributions. It strategically cooperates
    only when its contribution is strictly pivotal to reaching the 'm'
    cooperation threshold, thereby securing the 'k' bonus for the collective
    (and thus for itself). It defaults to defection in the first and last rounds
    and when cooperation from others is either sufficient or too low to be
    influenced by a single contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m_threshold = self.game_description.m
        total_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        prev_my_action_was_C = history.my_actions[-1]
        prev_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        c_count_prev = prev_opponent_cooperators + (1 if prev_my_action_was_C else 0)
        if c_count_prev >= m_threshold:
            return Action.D
        elif prev_my_action_was_C:
            return Action.D
        elif c_count_prev == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_491(BaseStrategy):
    """
    Adaptive Exploiter with Failure Threshold (AEFT) strategy for the Collective Risk Dilemma.

    This strategy aims to consistently achieve the highest possible payoff (1+k) by free-riding
    on others' cooperation. If collective cooperation falters and leads to low payoffs,
    it will strategically invest in cooperation to "nudge" the system back to success,
    but only if such investment hasn't proven consistently futile. It prioritizes its
    own long-term gain over collective welfare and will cease unproductive cooperation
    to avoid sustained losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._consecutive_failures_with_my_C = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == r:
            return Action.D
        if current_round_1_indexed == 1:
            self._consecutive_failures_with_my_C = 0
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        my_prev_action_bool = history.my_actions[previous_round_0_indexed]
        my_prev_action_enum = Action.C if my_prev_action_bool else Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[previous_round_0_indexed, :]) + my_prev_action_bool
        if cooperators_prev_round >= m:
            self._consecutive_failures_with_my_C = 0
        elif my_prev_action_enum == Action.C:
            self._consecutive_failures_with_my_C += 1
        else:
            self._consecutive_failures_with_my_C = 0
        action_for_current_round = Action.D
        if cooperators_prev_round >= m:
            action_for_current_round = Action.D
        elif self._consecutive_failures_with_my_C >= 2:
            action_for_current_round = Action.D
        else:
            action_for_current_round = Action.C
        return action_for_current_round

class Strategy_EXPLOITATIVE_492(BaseStrategy):
    """
    Adaptive Free-Rider with Failure-Response strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by free-riding on the cooperation of others
    whenever possible, while also ensuring the collective reward 'k' is achieved if its
    cooperation is pivotal and profitable. It adapts to opponent behavior and incorporates
    mechanisms to prevent endless self-sacrifice in the face of widespread defection.

    Key principles:
    1.  Exploit Free-Riding Opportunities: Defect if enough other players are cooperating to
        meet the threshold 'm', maximizing individual payoff (1+k).
    2.  Pivotal Cooperation: Cooperate if its contribution is essential to reach the 'm'
        threshold (since k > 1, this is profitable).
    3.  Initial Probing/Signaling: Cooperate for the first two rounds to signal willingness
        and gather initial information on opponent behavior.
    4.  Punish Persistent Defection: If cooperation repeatedly fails to meet the threshold
        (3 consecutive failures), the strategy switches to permanent defection to avoid
        continuous unrewarded contributions.
    5.  End-Game Rationality: In the final round, with no future consequences, the strategy
        always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.consecutive_failure_rounds: int = 0
        self.given_up_on_cooperation: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.r:
            return Action.D
        if self.given_up_on_cooperation:
            return Action.D
        if current_round_t <= 2:
            return Action.C
        prev_round_index = state.round_number - 1
        my_action_last_round = history.my_actions[prev_round_index]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[prev_round_index, :])
        num_cooperators_last_round = int(my_action_last_round) + opponent_cooperators_last_round
        if num_cooperators_last_round < self.m:
            self.consecutive_failure_rounds += 1
        else:
            self.consecutive_failure_rounds = 0
        if self.consecutive_failure_rounds >= 3:
            self.given_up_on_cooperation = True
            return Action.D
        cooperators_if_i_defected_last_round = num_cooperators_last_round
        if my_action_last_round == True:
            cooperators_if_i_defected_last_round -= 1
        if cooperators_if_i_defected_last_round >= self.m:
            return Action.D
        if cooperators_if_i_defected_last_round == self.m - 1 and my_action_last_round == True:
            return Action.C
        if num_cooperators_last_round < self.m:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_493(BaseStrategy):
    """
    The "Calculated Leverage" strategy aims to maximize personal payoff in the Collective Risk Dilemma
    by free-riding on others' cooperation. It only cooperates as a minimal, calculated intervention
    when its cooperation is strictly necessary and personally beneficial to achieve the collective
    reward (k > 1). It is ruthlessly self-interested.

    Decision Rules:
    1. Round 1 (Initial Probe): Defect (D)
       - Rationale: Tests the waters by defecting, hoping others cooperate to meet the 'm' threshold for a 1+k payoff.
    2. Last Round (r): Defect (D)
       - Rationale: In the final round, there are no future interactions. Defecting (1 or 1+k) always yields
                  a strictly higher payoff than cooperating (0 or k), given k > 1.
    3. Intermediate Rounds (1 < t < r):
       - If the group succeeded last round (N_C_last >= m): Defect (D)
         - Rationale: The collective threshold was met without my contribution; continue free-riding for 1+k.
       - If the group failed last round (N_C_last < m):
         - Calculate `shortfall = m - N_C_last`.
         - If `shortfall == 1`: Cooperate (C)
           - Rationale: My cooperation is exactly what's needed to reach 'm'. If I cooperate, my payoff is k.
                      If I defect, the project fails and my payoff is 1. Since k > 1, cooperating is selfishly
                      beneficial (k > 1).
         - If `shortfall > 1`: Defect (D)
           - Rationale: My single cooperation is insufficient to bring about collective success. It would result
                      in a payoff of 0 (cooperating when project fails) instead of 1 (defecting when project fails).
                      Defect to secure 1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        my_action_prev_round = history.my_actions[state.round_number - 1]
        opponent_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        N_C_last = int(my_action_prev_round) + opponent_cooperators_prev_round
        if N_C_last >= m:
            return Action.D
        else:
            shortfall = m - N_C_last
            if shortfall == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_494(BaseStrategy):
    """
    Adaptive Free-Rider with Calculated Contribution (AFRCC) Strategy.

    This strategy aims to maximize individual payoff by free-riding when the
    collective good is secured, contributing only when necessary to achieve
    the collective benefit, and withdrawing contributions when they are clearly
    ineffective. It adapts its behavior based on the observed collective outcome
    of the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_was_C_prev_round = history.my_actions[-1]
        C_prev = cooperators_from_opponents_prev_round + (1 if my_action_was_C_prev_round else 0)
        if C_prev >= self.m:
            return Action.D
        else:
            potential_C_this_round_if_I_cooperate = C_prev + 1
            if potential_C_this_round_if_I_cooperate >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_495(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the Collective Risk Dilemma.
    This strategy aims to free-ride as much as possible, benefitting from others'
    contributions while minimizing its own. It strategically cooperates only when
    its contribution is pivotal to achieving the collective reward 'k', or when
    it needs to "re-seed" cooperation to enable future free-riding opportunities
    if the group's overall cooperation rate is low and there are sufficient
    remaining rounds to benefit from this investment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.num_successful_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        action_to_take = Action.D
        if current_round_0_indexed > 0:
            cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if cooperators_in_prev_round >= self.m:
                self.num_successful_rounds += 1
        if current_round_0_indexed == self.r - 1:
            action_to_take = Action.D
        elif current_round_0_indexed == 0:
            action_to_take = Action.D
        else:
            prev_cooperators = cooperators_in_prev_round
            if prev_cooperators >= self.m:
                action_to_take = Action.D
            elif prev_cooperators == self.m - 1:
                action_to_take = Action.C
            else:
                num_completed_rounds = current_round_0_indexed
                success_rate = self.num_successful_rounds / num_completed_rounds
                rounds_remaining = self.r - (current_round_0_indexed + 1)
                if success_rate < 0.5 and rounds_remaining > self.r / 3:
                    action_to_take = Action.C
                else:
                    action_to_take = Action.D
        return action_to_take

class Strategy_EXPLOITATIVE_496(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.
    This strategy aims to maximize personal payoff by exploiting opportunities to free-ride,
    while strategically contributing when it is critical to achieve a substantial collective reward 'k'.
    It adapts to the observed level of cooperation in the group and accounts for the game's termination.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.K_pusher_threshold = 2.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        min_cooperators_m = self.game_description.m
        reward_factor_k = self.game_description.k
        if current_round_1_indexed == total_rounds:
            return Action.D
        elif reward_factor_k < self.K_pusher_threshold:
            return Action.D
        elif state.round_number == 0:
            return Action.C
        else:
            prev_my_action_is_c = int(history.my_actions[-1])
            prev_opponent_cooperators_count = np.sum(history.opponent_actions[-1, :])
            total_cooperators_prev_round = prev_my_action_is_c + prev_opponent_cooperators_count
            if total_cooperators_prev_round >= min_cooperators_m:
                return Action.D
            elif total_cooperators_prev_round == min_cooperators_m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_497(BaseStrategy):
    """
    Adaptive Trigger and Free-Ride strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily free-riding on
    others' cooperation. It cooperates only when its action is pivotal to
    reach the collective reward threshold 'm' and doing so is strictly
    individually beneficial (i.e., payoff 'k' > 1). It defaults to defection
    in the first and last rounds to secure immediate gains or avoid risk, and
    adapts its behavior in intermediate rounds based on the previous round's
    collective cooperation level.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        min_cooperators_needed = self.game_description.m
        reward_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        cooperators_prev_opponents = np.sum(history.opponent_actions[-1, :])
        cooperators_prev_me = 1 if history.my_actions[-1] else 0
        N_c_prev = cooperators_prev_opponents + cooperators_prev_me
        if N_c_prev >= min_cooperators_needed:
            return Action.D
        elif N_c_prev == min_cooperators_needed - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_498(BaseStrategy):
    """
    Adaptive Critical Contributor (ACC) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its own total payoff by primarily defecting,
    but will strategically cooperate only when its contribution is critically
    necessary and individually beneficial to unlock the collective reward.
    It adapts to observed opponent behavior and considers the game's finite horizon,
    implementing self-interested conditional cooperation and backward induction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        my_last_action_was_C = history.my_actions[-1]
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        n_cooperators_last_round = num_opponent_cooperators_last_round + (1 if my_last_action_was_C else 0)
        if n_cooperators_last_round >= m_threshold:
            if not my_last_action_was_C:
                return Action.D
            elif n_cooperators_last_round > m_threshold:
                return Action.D
            else:
                return Action.C
        elif n_cooperators_last_round == m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_499(BaseStrategy):
    """
    The "Adaptive Opportunistic Free-Rider" strategy aims to maximize its own cumulative payoff.
    It cooperates in the first round as a probe. In intermediate rounds, it defects if the
    collective reward threshold (m) was met in the previous round (free-riding). If the
    threshold was not met, it cooperates only if its contribution is exactly sufficient
    to meet the threshold (i.e., m-1 cooperators were present). In the last round, it always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        k_reward = self.game_description.k
        current_round_index = state.round_number
        if current_round_index == 0:
            return Action.C
        if current_round_index == n_rounds - 1:
            return Action.D
        cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        cooperators_from_me_prev_round = 1 if my_action_prev_round_is_cooperate else 0
        c_count_prev = cooperators_from_opponents_prev_round + cooperators_from_me_prev_round
        was_threshold_met_prev_round = c_count_prev >= m_threshold
        if was_threshold_met_prev_round:
            return Action.D
        else:
            cooperators_needed = m_threshold - c_count_prev
            if cooperators_needed == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_500(BaseStrategy):
    """
    Adaptive Conditional Free-Rider (ACFR) strategy for the Collective Risk Dilemma.
    This strategy is designed to be highly self-interested. It prioritizes securing
    the collective reward 'k' only when its own contribution is crucial to achieving it;
    otherwise, it aggressively free-rides. It avoids "wasted" cooperation where its
    single action cannot secure the reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        else:
            c_others_prev_round = np.sum(history.opponent_actions[-1, :])
            if c_others_prev_round >= m:
                return Action.D
            elif c_others_prev_round == m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_501(BaseStrategy):
    """
    Opportunistic Pacer strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by primarily free-riding
    on the cooperation of others. It strategically cooperates only when:
    1. Its action is pivotal to reaching the collective reward threshold `m`
       (yielding a personal payoff of `k`, which is greater than `1` from defecting
       without the threshold met).
    2. As a calculated "investment" or "reset" (a "kickstart") to encourage
       others to cooperate, after a defined period of collective failures
       or personal defection, creating future opportunities to free-ride.

    It is designed to avoid consistent zero payoffs and strives for `1+k` whenever possible.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.rounds_since_success = 0
        self.my_streak_C = 0
        self.my_streak_D = 0
        self.cooperation_kickstart_threshold = max(2, self.r // 5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        if current_0_indexed_round == 0:
            action_to_take = Action.D
            self.my_streak_D = 1
            self.my_streak_C = 0
            return action_to_take
        my_action_in_last_round_bool = history.my_actions[current_0_indexed_round - 1]
        opponent_actions_prev_bool = history.opponent_actions[current_0_indexed_round - 1, :]
        total_C_opponents_prev = np.sum(opponent_actions_prev_bool)
        total_C_prev = total_C_opponents_prev + (1 if my_action_in_last_round_bool else 0)
        if total_C_prev >= self.m:
            self.rounds_since_success = 0
        else:
            self.rounds_since_success += 1
        if my_action_in_last_round_bool:
            self.my_streak_C += 1
            self.my_streak_D = 0
        else:
            self.my_streak_D += 1
            self.my_streak_C = 0
        action_to_take = Action.D
        if current_0_indexed_round == self.r - 1:
            action_to_take = Action.D
        elif total_C_prev >= self.m:
            action_to_take = Action.D
        else:
            cooperators_excluding_me_prev = total_C_opponents_prev
            if cooperators_excluding_me_prev == self.m - 1:
                action_to_take = Action.C
            elif self.rounds_since_success >= self.cooperation_kickstart_threshold or self.my_streak_D >= self.cooperation_kickstart_threshold:
                action_to_take = Action.C
        return action_to_take

class Strategy_EXPLOITATIVE_502(BaseStrategy):
    """
    The "Adaptive Pothole-Fixer" (APF) strategy for the Collective Risk Dilemma.
    It aims to maximize its total payoff by strategically cooperating when its
    contribution is pivotal to secure a collective reward, and free-riding
    (defecting) whenever possible.
    
    The strategy makes an initial investment in Round 1, then adaptively
    exploits successful cooperation or strategically contributes when its
    single action can tip the group towards meeting the threshold. In the
    final round, it always defects to maximize immediate payoff without
    future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing the game parameters
                              (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for all
                     players, or None for the very first round.

        Returns:
            An Action enum: Action.C for Cooperate, Action.D for Defect.
        """
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        current_0_indexed_round = state.round_number
        if current_0_indexed_round == 0:
            return Action.C
        if current_0_indexed_round == n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_was_C_prev_round = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_was_C_prev_round else 0)
        if total_cooperators_prev_round >= m_threshold:
            return Action.D
        elif total_cooperators_prev_round == m_threshold - 1:
            return Action.C
        elif total_cooperators_prev_round < m_threshold - 1:
            return Action.D

class Strategy_EXPLOITATIVE_503(BaseStrategy):
    """
    Adaptive Exploitative Free-Rider (AEFR) strategy for the Collective Risk Dilemma.

    Core Principle: This strategy aims to maximize individual payoff by consistently
    attempting to free-ride. It defaults to defection, but is prepared to selectively
    cooperate when its contribution is critically needed to secure the collective reward (k),
    or when an initial probe suggests sufficient cooperation from others. It fully exploits
    the endgame by defecting. The strategy adapts based on the previous round's outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs, or None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_num = state.round_number
        if current_round_num == 0:
            return Action.D
        if current_round_num == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if num_cooperators_prev_round >= self.m_threshold:
            return Action.D
        elif num_cooperators_prev_round == self.m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_504(BaseStrategy):
    """
    Adaptive Collapse-Averse Free-Rider (ACAFR) strategy for the Collective Risk Dilemma.

    Core Philosophy: The ACAFR strategy aims to maximize its own total payoff by leveraging
    the public good nature of the 'k' reward. It primarily seeks to free-ride, enjoying the
    benefits of cooperation from others without contributing itself. However, it is designed
    to strategically cooperate when its contribution is critical to achieving the 'm'
    cooperator threshold (specifically when C_prev == m-2, based on the provided pseudocode),
    thereby preventing a collective loss of the 'k' reward and securing at least 'k' for itself.
    It also incorporates backward induction for the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for the player and opponents.
                     It is None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        cooperators_prev_round_opponents = np.sum(history.opponent_actions[previous_round_index, :])
        cooperators_prev_round_my_action = history.my_actions[previous_round_index]
        C_prev = cooperators_prev_round_opponents + cooperators_prev_round_my_action
        if C_prev >= m:
            return Action.D
        if C_prev == m - 1:
            return Action.D
        if C_prev == m - 2:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_505(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize its total payoff by strategically leveraging
    group cooperation. It adapts to observed historical cooperation levels,
    cooperating when it's pivotal or sets a positive signal, and defecting
    to free-ride when the collective threshold is met or to avoid the
    "sucker's payoff" when cooperation is too low.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        m = self.game_description.m
        n_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        previous_round_index = state.round_number - 1
        my_last_action_was_coop = history.my_actions[previous_round_index]
        opponents_last_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
        last_round_cooperators = int(my_last_action_was_coop) + opponents_last_cooperators
        if current_round_1_indexed == n_rounds:
            return Action.D
        elif last_round_cooperators >= m:
            return Action.D
        elif last_round_cooperators == m - 1:
            return Action.C
        elif last_round_cooperators >= math.floor(m / 2):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_506(BaseStrategy):
    """
    The "Adaptive Conditional Exploiter" strategy is designed to maximize individual payoff by intelligently
    free-riding when possible and only contributing when its own action is strategically crucial to
    securing the collective reward. It avoids futile contributions and leverages the end-game effect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        min_cooperators = self.game_description.m
        reward_factor = self.game_description.k
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        my_action_prev_round_is_C = history.my_actions[previous_round_0_indexed]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        if num_cooperators_prev_round >= min_cooperators:
            return Action.D
        elif num_cooperators_prev_round == min_cooperators - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_507(BaseStrategy):
    """
    Adaptive Tipping Point Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by prioritizing free-riding.
    It adapts its behavior based on the observed actions of other players in the
    previous round.

    Core Principles:
    1. Prioritize Free-Riding: If enough other players are cooperating to meet
       the 'm' threshold, defect to gain the maximum individual payoff (1+k).
    2. Strategic Contribution: Only cooperate if your contribution is the
       "tipping point" that secures the collective reward 'k'. This ensures
       you receive 'k' (which is >1) instead of '1' (if the threshold isn't met
       and you defect) or '0' (if you cooperated needlessly and threshold isn't met).
    3. Avoid Being a Sucker: If your cooperation alone is insufficient to meet
       the 'm' threshold, defect to avoid a payoff of '0' and guarantee '1'.

    It starts with cooperation in the first round to signal willingness and
    establish a baseline for subsequent adaptive decisions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players,
                              n_rounds, m, k).
        """
        self.game_description = game_description
        self.my_last_action_taken: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the game state and history.

        Args:
            state: An object containing the current round number.
            history: An object containing the player's and opponents' actions
                     and payoffs from previous rounds, or None for the first round.

        Returns:
            An Action (C for Cooperate, D for Defect).
        """
        m = self.game_description.m
        action_for_this_round: Action
        if state.round_number == 0:
            action_for_this_round = Action.C
        else:
            N_C_others_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
            if N_C_others_prev_round >= m:
                action_for_this_round = Action.D
            elif N_C_others_prev_round == m - 1:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        self.my_last_action_taken = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_508(BaseStrategy):
    """
    Strict Conditional Free-Rider (SCFR) Strategy for Collective Risk Dilemma.

    This strategy aims to maximize its own payoff by generally defecting,
    only cooperating when its contribution is pivotal to reaching the 'm'
    cooperation threshold, thereby securing a personal gain 'k' that would
    otherwise be missed. It avoids cooperation in the first and last rounds
    and when its contribution is not critical.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.n_rounds:
            return Action.D
        if current_round_t == 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if num_total_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_509(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by adaptively free-riding
    or strategically cooperating based on the previous round's outcome. It's
    designed to be ruthlessly self-interested, leveraging the cooperation of
    others while minimizing its own contributions when possible.

    Core Principles:
    1.  Prioritize self-preservation: Never contribute if it's likely to be wasted.
    2.  Exploit surplus cooperation: Free-ride when others contribute more than enough.
    3.  Strategic Cooperation: Only cooperate when your contribution is *essential*
        to secure the collective benefit (k), and receiving k is better than 1 (always true since k > 1).
    4.  No Future Obligation: Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the Collective Risk Dilemma's game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, m (minimum cooperators needed),
                              and k (reward if threshold met factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Free-Rider strategy rules.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs
                     for the player and opponents. This will be None for the very
                     first round (round_number == 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_action_prev_round_was_C = history.my_actions[-1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        prev_round_total_cooperators = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        m = self.game_description.m
        if prev_round_total_cooperators > m:
            return Action.D
        elif prev_round_total_cooperators == m:
            if not my_action_prev_round_was_C:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_510(BaseStrategy):
    """
    The Calculated Opportunist strategy is highly exploitative, prioritizing individual payoff
    by free-riding whenever possible. It only contributes to the collective good when its
    cooperation is pivotal and individually advantageous, leading to a better outcome than defecting.
    It bases decisions solely on game parameters and the total number of cooperators observed in
    the immediately preceding round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        n_c_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        m = self.game_description.m
        k = self.game_description.k
        if n_c_prev >= m:
            return Action.D
        else:
            cooperators_needed = m - n_c_prev
            if cooperators_needed == 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_511(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize personal payoff by primarily defecting,
    but strategically cooperating when its contribution is pivotal to achieving the collective
    reward 'k'. It adapts to opponent behaviors, free-riding when possible, and avoiding
    wasted cooperation when it's futile.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        previous_round_index = state.round_number - 1
        my_action_prev_round_was_C = history.my_actions[previous_round_index]
        opponent_actions_prev_round = history.opponent_actions[previous_round_index, :]
        num_cooperators_prev_round_others = np.sum(opponent_actions_prev_round)
        num_cooperators_prev_round_total = num_cooperators_prev_round_others + (1 if my_action_prev_round_was_C else 0)
        if num_cooperators_prev_round_total >= self.m:
            return Action.D
        elif num_cooperators_prev_round_others >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_512(BaseStrategy):
    """
    Stabilizing Exploiter strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by selectively cooperating to
    ensure the collective reward 'k' is available, while primarily prioritizing
    defection (free-riding) whenever the 'm' threshold is being met by others.

    It dynamically adjusts its behavior based on the outcome of the previous round
    (whether the collective threshold 'm' was met) and its own action in that
    previous round. This allows it to exploit situations of collective success
    while also making strategic "investments" to prevent prolonged collective failure
    and restore conditions favorable for future free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_is_C = history.my_actions[-1]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_is_C)
        if num_cooperators_prev_round >= self.m:
            return Action.D
        elif not my_action_prev_is_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_398(BaseStrategy):
    """
    Adaptive Collective Steward (ACS) strategy for the Collective Risk Dilemma.

    This strategy aims to secure the collective reward 'k' by reacting to the
    group's prior performance. It balances proactive cooperation with a pragmatic
    response to collective failure.

    Decision Rules:
    - Round 1: Cooperate (C) as an initial probe and signal of intent.
    - Intermediate Rounds (2 to r-1):
        - If the collective cooperation threshold (m) was met in the previous round,
          continue to Cooperate (C) to maintain success.
        - If the collective cooperation threshold (m) was *not* met in the previous round,
          Defect (D) to signal dissatisfaction, conserve resources, and potentially
          reset group dynamics.
    - Final Round (r): Defect (D) due to the absence of future consequences
      (backward induction).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = int(my_prev_action_was_C) + num_opponent_cooperators_prev
        if total_cooperators_prev_round >= self.m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_308(BaseStrategy):
    """
    Calculated Free-Rider with System Maintenance (CFR-SM) strategy.
    Aims to maximize personal payoff by opportunistically free-riding,
    while strategically contributing to maintain the collective benefit (k bonus).
    This strategy balances individual gain with the necessity of preserving the common good.
    """
    THRESHOLD_DEFECT_LIMIT: int
    MAX_FAIL_TOLERANCE: int
    CLOSE_TO_M_THRESHOLD: int

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the CFR-SM strategy with game-specific parameters
        and its internal heuristic values.

        Args:
            game_description (CollectiveRiskDescription): An object containing
                                                         game parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.THRESHOLD_DEFECT_LIMIT = 2
        self.MAX_FAIL_TOLERANCE = 3
        self.CLOSE_TO_M_THRESHOLD = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the game state, historical actions, and internal strategy rules.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): An object containing past actions and payoffs
                                            of this player and opponents. None for round 0.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        rounds_since_m_met = 0
        consecutive_defect_if_m_met = 0
        if state.round_number > 0 and history is not None:
            for r_idx in range(state.round_number - 1, -1, -1):
                prev_my_action = history.my_actions[r_idx]
                prev_my_action_val = 1 if prev_my_action else 0
                prev_C_total = np.sum(history.opponent_actions[r_idx, :]) + prev_my_action_val
                if prev_C_total < m:
                    rounds_since_m_met += 1
                    consecutive_defect_if_m_met = 0
                else:
                    rounds_since_m_met = 0
                    if not prev_my_action:
                        consecutive_defect_if_m_met += 1
                    else:
                        consecutive_defect_if_m_met = 0
                        break
        action_to_take_enum = None
        if state.round_number == 0:
            action_to_take_enum = Action.C
        elif state.round_number == r - 1:
            last_round_my_action_val = 1 if history.my_actions[-1] else 0
            last_round_C_total = np.sum(history.opponent_actions[-1, :]) + last_round_my_action_val
            if last_round_C_total >= m:
                action_to_take_enum = Action.D
            else:
                needed_cooperators = m - last_round_C_total
                if needed_cooperators == 1:
                    action_to_take_enum = Action.C
                else:
                    action_to_take_enum = Action.D
        else:
            last_round_my_action_val = 1 if history.my_actions[-1] else 0
            last_round_C_total = np.sum(history.opponent_actions[-1, :]) + last_round_my_action_val
            if last_round_C_total >= m:
                if last_round_C_total > m and consecutive_defect_if_m_met < self.THRESHOLD_DEFECT_LIMIT:
                    action_to_take_enum = Action.D
                else:
                    action_to_take_enum = Action.C
            else:
                needed_cooperators = m - last_round_C_total
                if needed_cooperators == 1:
                    action_to_take_enum = Action.C
                elif rounds_since_m_met >= self.MAX_FAIL_TOLERANCE:
                    action_to_take_enum = Action.D
                elif last_round_C_total >= m - self.CLOSE_TO_M_THRESHOLD:
                    action_to_take_enum = Action.C
                else:
                    action_to_take_enum = Action.D
        return action_to_take_enum

class Strategy_EXPLOITATIVE_432(BaseStrategy):
    """
    The Adaptive Threshold Exploit (ATE) strategy is designed to maximize individual payoff
    by strategically exploiting collective cooperation when it exists, and defensively
    protecting private endowment when it doesn't. It adapts to opponent behavior and game state,
    prioritizing the highest possible individual payoff (1+k) by defecting, while strategically
    cooperating only when it is critically necessary and individually beneficial to secure
    the collective reward (k).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveThresholdExploit strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as n_players,
                              n_rounds, m (minimum cooperators needed), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Threshold Exploit strategy.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the current player
                     and opponents. It is None for the very first round (round_number = 0).

        Returns:
            An Action enum: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_actions_prev = history.opponent_actions[-1, :]
        N_C_others_prev = int(np.sum(opponent_actions_prev))
        N_C_prev = N_C_others_prev + (1 if my_action_prev else 0)
        if N_C_prev >= m:
            return Action.D
        elif my_action_prev:
            return Action.D
        elif N_C_others_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_470(BaseStrategy):
    """
    The "Conditional Free-Rider" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by primarily free-riding on the
    cooperation of others, only contributing when it's critically necessary to unlock
    the collective reward. It diligently avoids scenarios where its cooperation would
    be futile or unnecessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        prev_round_history_index = current_round_0_indexed - 1
        num_cooperators_prev_round_opponents = np.sum(history.opponent_actions[prev_round_history_index, :])
        my_action_prev_round_bool = history.my_actions[prev_round_history_index]
        my_action_prev_round_int = int(my_action_prev_round_bool)
        N_C_prev = num_cooperators_prev_round_opponents + my_action_prev_round_int
        if N_C_prev >= self.m:
            return Action.D
        elif N_C_prev == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_477(BaseStrategy):
    """
    Adaptive Exploiter with Strategic Pumping: Defaults to free-riding (Defecting) but is willing to
    temporarily "pump" cooperation when the collective benefit (k) is consistently missed, always
    reverting to free-riding as soon as the benefit is secured or its own investment becomes too costly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FREE_RIDE_FAIL_THRESHOLD = 2
        self.MAX_PUMPING_ROUNDS = 2
        self.rounds_since_k_met = 0
        self.my_pumping_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0:
            prev_round_total_cooperators = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
            k_bonus_achieved_prev_round = prev_round_total_cooperators >= self.game_description.m
            if k_bonus_achieved_prev_round:
                self.rounds_since_k_met = 0
                self.my_pumping_streak = 0
            else:
                self.rounds_since_k_met += 1
                if history.my_actions[-1] == True:
                    self.my_pumping_streak += 1
                else:
                    self.my_pumping_streak = 0
        current_action = Action.D
        if state.round_number == 0:
            current_action = Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            current_action = Action.D
        elif self.rounds_since_k_met == 0:
            current_action = Action.D
        elif self.my_pumping_streak > 0 and self.my_pumping_streak < self.MAX_PUMPING_ROUNDS:
            current_action = Action.C
        elif self.rounds_since_k_met >= self.FREE_RIDE_FAIL_THRESHOLD:
            current_action = Action.C
        return current_action

class Strategy_EXPLOITATIVE_479(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by primarily defecting.
    It only cooperates when its contribution is decisive for securing the
    collective reward 'k', specifically when exactly 'm-1' cooperators were
    observed in the previous round and 'k' was not achieved.
    It defects in the first and last rounds due to lack of information or
    future consequences, respectively, and also defects if 'k' was already
    achieved by others in the previous round (free-riding).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Free-Rider strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, m (minimum cooperators), and k (reward factor).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for the player and opponents.
                     It is None for the very first round (round_number = 0).

        Returns:
            Action.C for Cooperate or Action.D for Defect.
        """
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == 0:
            return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        my_prev_action = history.my_actions[state.round_number - 1]
        opponent_prev_cooperators = sum(history.opponent_actions[state.round_number - 1, :])
        C_prev = int(my_prev_action) + opponent_prev_cooperators
        if C_prev >= m:
            return Action.D
        elif C_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_114(BaseStrategy):
    """
    The "ID-Based Adaptive Contributor" strategy for the Collective Risk Dilemma.

    This strategy leverages player IDs and adaptive responses to foster cooperation
    and ensure the collective reward 'k' is consistently met. It establishes an
    initial ID-based division of labor, where players with lower IDs are designated
    as primary cooperators. The strategy then adaptively responds to the group's
    performance in the previous round to either maintain this equilibrium or
    dynamically increase cooperation when the collective goal is at risk.

    Key features:
    - Initial ID-based role assignment: Players with `my_ID <= m_threshold` start as
      cooperators, others as defectors. This aims for an optimal `m` cooperators.
    - Consistency: Designated cooperators (`my_ID <= m_threshold`) generally maintain
      cooperation.
    - Efficiency-seeking: Designated defectors (`my_ID > m_threshold`) free-ride when
      the threshold `m` was met, maximizing individual payoffs.
    - Adaptive recovery: If the threshold `m` was *not* met in the previous round,
      all players (including designated defectors) temporarily switch to cooperation
      to "pull" the group back to a cooperative state.
    - Robustness: Defects in the final round, anticipating rational self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_reward = game_description.k
        self.my_ID: int = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            if self.my_ID <= self.m_threshold:
                return Action.C
            else:
                return Action.D
        prev_round_idx = current_round_0_indexed - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + (1 if history.my_actions[prev_round_idx] else 0)
        if num_cooperators_prev_round >= self.m_threshold:
            if self.my_ID <= self.m_threshold:
                return Action.C
            else:
                return Action.D
        elif self.my_ID <= self.m_threshold:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_449(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the Collective Risk Dilemma.
    This strategy adapts its behavior based on observed collective cooperation
    in previous rounds, transitioning between COOPERATE, PUNISH, and RECONCILE phases.
    """

    class Phase(Enum):
        COOPERATE = 1
        PUNISH = 2
        RECONCILE = 3
    DEFAULT_PUNISH_ROUNDS = 3

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.PUNISH_DURATION = max(1, min(self.DEFAULT_PUNISH_ROUNDS, self.game_description.n_rounds - 1))
        self.current_phase: Strategy.Phase = self.Phase.COOPERATE
        self.rounds_in_punishment: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.current_phase = self.Phase.COOPERATE
            self.rounds_in_punishment = 0
            return Action.C
        num_cooperators_prev_round = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if self.current_phase == self.Phase.COOPERATE:
            if num_cooperators_prev_round < self.game_description.m:
                self.current_phase = self.Phase.PUNISH
                self.rounds_in_punishment = 1
        elif self.current_phase == self.Phase.PUNISH:
            if self.rounds_in_punishment < self.PUNISH_DURATION:
                self.rounds_in_punishment += 1
            else:
                self.current_phase = self.Phase.RECONCILE
                self.rounds_in_punishment = 0
        elif self.current_phase == self.Phase.RECONCILE:
            if num_cooperators_prev_round < self.game_description.m:
                self.current_phase = self.Phase.PUNISH
                self.rounds_in_punishment = 1
            else:
                self.current_phase = self.Phase.COOPERATE
        if self.current_phase == self.Phase.COOPERATE or self.current_phase == self.Phase.RECONCILE:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_101(BaseStrategy):
    """
    The Adaptive Free-Rider strategy is an exploitative approach for the Collective Risk Dilemma.
    It prioritizes free-riding by defecting, aiming for the '1+k' payoff.
    However, it adaptively invests minimal cooperation (a 'kickstart') if the group consistently fails
    to meet the cooperation threshold 'm' for a specified number of rounds, as this failure
    denies the strategy the 'k' reward. In the final round, it always defects to maximize terminal payoff.
    """

    class Mode(Enum):
        FREE_RIDE = 1
        KICKSTART_COOPERATION = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.DEFECTION_STREAK_THRESHOLD = 2
        self.ROUNDS_TO_COOPERATE_IF_FAILED = 1
        self.consecutive_failures = 0
        self.cooperation_countdown = 0
        self.my_current_mode = self.Mode.FREE_RIDE

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        n_rounds_total = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if current_round_0_indexed == 0:
            self.consecutive_failures = 0
            self.cooperation_countdown = 0
            self.my_current_mode = self.Mode.FREE_RIDE
            return Action.D
        opponents_cooperated_prev = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        my_action_prev_was_C = history.my_actions[current_round_0_indexed - 1]
        C_prev = opponents_cooperated_prev + (1 if my_action_prev_was_C else 0)
        if C_prev >= m_threshold:
            self.consecutive_failures = 0
            if self.my_current_mode == self.Mode.KICKSTART_COOPERATION:
                self.my_current_mode = self.Mode.FREE_RIDE
                self.cooperation_countdown = 0
        else:
            self.consecutive_failures += 1
            if self.my_current_mode == self.Mode.KICKSTART_COOPERATION:
                self.cooperation_countdown -= 1
                if self.cooperation_countdown <= 0:
                    self.my_current_mode = self.Mode.FREE_RIDE
                    self.cooperation_countdown = 0
        if self.my_current_mode == self.Mode.FREE_RIDE and self.consecutive_failures >= self.DEFECTION_STREAK_THRESHOLD:
            self.my_current_mode = self.Mode.KICKSTART_COOPERATION
            self.cooperation_countdown = self.ROUNDS_TO_COOPERATE_IF_FAILED
        if current_round_1_indexed == n_rounds_total:
            return Action.D
        if self.my_current_mode == self.Mode.FREE_RIDE:
            return Action.D
        else:
            return Action.C
