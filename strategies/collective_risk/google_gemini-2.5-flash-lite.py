"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: Client
- Model: gemini-2.5-flash-lite
- Game: CollectiveRiskDescription
"""

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState
import numpy as np
from numpy.typing import NDArray
import math
import random



class Strategy_COOPERATIVE_1(BaseStrategy):
    """
    Cooperative Risk Aversion Strategy (CRAS): Aims to achieve the higher collective
    reward by cooperating only when conditions suggest it's safe and likely to succeed.
    It prioritizes avoiding the lowest possible payoff while enabling high collective reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.observed_cooperators_last_round = 0
        self.previously_threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        cooperators_in_prev_round = 0
        if history.round_number > 0:
            if history.my_actions[history.round_number - 1]:
                cooperators_in_prev_round += 1
            cooperators_in_prev_round += np.sum(history.opponent_actions[history.round_number - 1, :])
            self.observed_cooperators_last_round = cooperators_in_prev_round
            if not self.previously_threshold_met:
                if cooperators_in_prev_round >= self.m:
                    self.previously_threshold_met = True
        else:
            self.observed_cooperators_last_round = 0
            self.previously_threshold_met = False
        initial_cooperation_warmup = min(5, self.r - 1)
        if initial_cooperation_warmup <= 0:
            initial_cooperation_warmup = 1
        if current_round <= initial_cooperation_warmup:
            if self.observed_cooperators_last_round + 1 >= self.m:
                return Action.C
            else:
                return Action.D
        elif self.observed_cooperators_last_round > self.n - self.m:
            return Action.C
        elif self.observed_cooperators_last_round >= self.m - 1 and self.previously_threshold_met:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_2(BaseStrategy):
    """
    Cautious Threshold Guardian:
    This strategy aims to foster cooperation in the Collective Risk Dilemma by
    initially promoting cooperation, then adapting based on the observed success
    of meeting the cooperation threshold. It cooperates when success seems likely,
    retaliates mildly against failed cooperation when it also cooperated, and
    defaults to defection when cooperation is consistently unsuccessful or in the
    final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action = None
        self.consecutive_failures = 0
        self.initial_cooperation_rounds = 5
        self.failure_threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if state.round_number == r - 1:
            return Action.D
        if history is None:
            self.my_last_action = Action.C
            self.consecutive_failures = 0
            return Action.C
        current_round = state.round_number
        my_actions = history.my_actions
        opponent_actions = history.opponent_actions
        cooperators_prev_round = 0
        if current_round > 0:
            cooperators_prev_round = np.sum(opponent_actions[current_round - 1, :]) + (1 if my_actions[current_round - 1] == Action.C else 0)
        threshold_met_prev_round = cooperators_prev_round >= m
        if not threshold_met_prev_round:
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
        if threshold_met_prev_round:
            self.my_last_action = Action.C
            return Action.C
        elif current_round < self.initial_cooperation_rounds:
            self.my_last_action = Action.C
            return Action.C
        if self.consecutive_failures >= self.failure_threshold:
            self.my_last_action = Action.D
            return Action.D
        if self.my_last_action == Action.C and (not threshold_met_prev_round):
            self.my_last_action = Action.D
            return Action.D
        estimated_cooperators_for_threshold = cooperators_prev_round - (1 if self.my_last_action == Action.C else 0)
        if estimated_cooperators_for_threshold + 1 >= m:
            self.my_last_action = Action.C
            return Action.C
        self.my_last_action = Action.D
        return Action.D

class Strategy_COOPERATIVE_3(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) Strategy.

    This strategy aims to foster cooperation by incentivizing players to contribute
    to the collective good, while also adapting to perceived defecting behavior.
    It starts cooperatively and only defects when the group's cooperation falls
    short of the required threshold or when the game is about to end.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATC strategy.

        Args:
            game_description: Description of the game parameters.
        """
        self.game_description = game_description
        self.cooperation_confidence = 1.0
        self.punishment_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state.
            history: The history of actions and payoffs from previous rounds.
                     None if it's the first round.

        Returns:
            Action: The action to take (Action.C or Action.D).
        """
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        if state.round_number == r:
            return Action.D
        my_actions_prev = self.convert_to_bool_array(history.my_actions)
        opponent_actions_prev = self.convert_to_bool_array(history.opponent_actions)
        cooperation_count_t_minus_1 = np.sum(my_actions_prev[-1]) + np.sum(opponent_actions_prev[-1, :])
        if cooperation_count_t_minus_1 >= m:
            self.cooperation_confidence = min(1.0, self.cooperation_confidence + 0.1)
            self.punishment_level = max(0, self.punishment_level - 1)
        else:
            self.cooperation_confidence = max(0.0, self.cooperation_confidence - 0.2)
            self.punishment_level = self.punishment_level + 1
        confidence_threshold = m / n
        punishment_threshold = 2
        if self.cooperation_confidence > confidence_threshold and self.punishment_level <= punishment_threshold:
            return Action.C
        else:
            return Action.D

    def convert_to_bool_array(self, actions_or_payoffs) -> NDArray[np.bool_]:
        """
        Helper to ensure we are working with boolean arrays.
        Assumes input is either boolean or numeric that can be cast to boolean.
        """
        if isinstance(actions_or_payoffs, np.ndarray):
            return actions_or_payoffs.astype(np.bool_)
        return np.array(actions_or_payoffs, dtype=np.bool_)

class Strategy_COOPERATIVE_4(BaseStrategy):
    """
    Adaptive Threshold Cooperat (ATC) strategy.

    This strategy aims to foster cooperation by demonstrating a commitment to the
    community project while simultaneously protecting itself from exploitation. It
    adapts its behavior based on the observed success of cooperation and the
    proximity to the critical threshold of `m`.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_belief = 1.0
        self.alpha = 0.8
        self.buffer = 1
        self.my_player_index = -1

    def _get_my_index(self, history: PlayerHistory) -> int:
        """Determines the player's index based on history structure."""
        if self.my_player_index == -1:
            if history.opponent_actions.shape[1] == self.n - 1:
                self.my_player_index = self.n - 1
            else:
                self.my_player_index = 0
        return self.my_player_index

    def _count_cooperators_in_round(self, round_idx: int, history: PlayerHistory) -> int:
        """Counts the number of cooperators in a given round."""
        if round_idx < 0 or round_idx >= history.round_number:
            return 0
        my_action = self.game_description.n_players - 1 if self.my_player_index == -1 and history.opponent_actions.shape[1] == self.n - 1 else self._get_my_index(history)
        cooperators = 0
        if history.opponent_actions.shape[1] == self.n - 1:
            cooperators += np.sum(history.opponent_actions[round_idx, :])
            if round_idx < len(history.my_actions):
                if history.my_actions[round_idx]:
                    cooperators += 1
        else:
            if round_idx < len(history.my_actions):
                if history.my_actions[round_idx]:
                    cooperators += 1
            for opp_idx in range(history.opponent_actions.shape[1]):
                if history.opponent_actions[round_idx, opp_idx]:
                    cooperators += 1
        return cooperators

    def _calculate_prob_m_met(self, observed_cooperators: int) -> float:
        """
        Estimates the probability of reaching the threshold 'm' given observed cooperators.
        This is a simplified heuristic, not a precise binomial calculation.
        """
        if observed_cooperators >= self.m:
            return 1.0
        elif observed_cooperators < self.m - self.buffer:
            return 0.0
        else:
            return min(1.0, max(0.0, (observed_cooperators - (self.m - 1 - self.buffer)) / (self.buffer + 1.0)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            self.cooperation_belief = 1.0
            return Action.C
        previous_round_idx = current_round - 1
        cooperators_observed_t_minus_1 = self._count_cooperators_in_round(previous_round_idx, history)
        prob_m_met_in_t = self._calculate_prob_m_met(cooperators_observed_t_minus_1)
        self.cooperation_belief = self.alpha * self.cooperation_belief + (1 - self.alpha) * prob_m_met_in_t
        if current_round == self.r - 1:
            return Action.D
        num_cooperators_in_prev_round = self._count_cooperators_in_round(current_round - 1, history)
        if self.cooperation_belief >= self.m / self.n and num_cooperators_in_prev_round >= self.m - 1:
            return Action.C
        elif num_cooperators_in_prev_round < self.m - 1 and self.cooperation_belief < self.m / self.n:
            return Action.D
        elif random.random() < self.cooperation_belief:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_5(BaseStrategy):
    """
    Adaptive Grateful Defector strategy for the Collective Risk Dilemma.

    This strategy is cooperative by default, rewards successful cooperation,
    and retaliates strategically against exploitation. It adapts its
    cooperation target rate based on observed outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_target_rate = self.m / self.n + 0.05
        self.my_last_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if history is None:
            self.my_last_action = Action.C
            return Action.C
        n_cooperators_prev_round = np.sum(history.opponent_actions[current_round_number - 1, :])
        observed_coop_rate_prev = n_cooperators_prev_round / self.n
        threshold_met_prev = n_cooperators_prev_round >= self.m
        if threshold_met_prev:
            self.cooperation_target_rate = min(1.0, self.cooperation_target_rate + 0.05)
        else:
            self.cooperation_target_rate = max(self.m / self.n, self.cooperation_target_rate - 0.05)
        if current_round_number == self.r:
            self.my_last_action = Action.D
            return Action.D
        current_action = Action.D
        if observed_coop_rate_prev >= self.cooperation_target_rate:
            current_action = Action.C
        elif n_cooperators_prev_round >= self.m - 1:
            current_action = Action.C
        self.my_last_action = current_action
        return current_action

class Strategy_COOPERATIVE_6(BaseStrategy):
    """
    Adaptive Threshold Seeker: A cooperative strategy for the Collective Risk Dilemma.
    This strategy aims to achieve cooperation by monitoring the group's collective
    behavior and adjusting its own actions. It balances the desire for the high
    reward of successful cooperation with the risk of being exploited by defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.my_last_action = Action.C
            return Action.C
        if state.round_number == self.r - 1:
            self.my_last_action = Action.C
            return Action.C
        previous_cooperators = 0
        if history.my_actions is not None and len(history.my_actions) > 0:
            if history.my_actions[state.round_number - 1] == Action.C:
                previous_cooperators += 1
            if history.opponent_actions is not None and history.opponent_actions.shape[0] > state.round_number - 1:
                previous_cooperators += np.sum(history.opponent_actions[state.round_number - 1, :])
        i_cooperated_last_round = self.my_last_action == Action.C
        decision = Action.D
        if previous_cooperators >= self.m:
            if not i_cooperated_last_round:
                decision = Action.C
            else:
                decision = Action.C
        elif previous_cooperators == self.m - 1:
            if i_cooperated_last_round:
                decision = Action.C
            else:
                decision = Action.C
        else:
            decision = Action.D
        self.my_last_action = decision
        return decision

class Strategy_COOPERATIVE_7(BaseStrategy):
    """
    Adaptive Reciprocity Strategy (ARS) for the Collective Risk Dilemma.

    This strategy aims to foster cooperation through conditional cooperation and
    reciprocity, while incorporating forgiveness to prevent indefinite punishment
    cycles. It starts by cooperating, then cooperates if the group has historically
    shown a propensity to meet the cooperation threshold, or if the immediate past
    round met the threshold. If cooperation falters, it defects for a limited period
    (forgiveness_threshold) before reassessing.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.recent_defection_streak = 0
        self.forgiveness_threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.recent_defection_streak = 0
            return Action.C
        current_round_number = history.round_number
        cooperators_last_round = 0
        if history.opponent_actions.shape[0] > 0:
            cooperators_last_round = np.sum(history.opponent_actions[-1, :])
            if history.my_actions[-1] == Action.C.value:
                cooperators_last_round += 1
        all_past_cooperators = []
        if history.opponent_actions.shape[0] > 0:
            all_past_cooperators.extend(np.sum(history.opponent_actions, axis=1))
            my_past_cooperation = np.array(history.my_actions, dtype=np.int64)
            all_past_cooperators = np.array(all_past_cooperators) + my_past_cooperation
        if len(all_past_cooperators) > 0:
            cooperators_average_past = np.mean(all_past_cooperators)
        else:
            cooperators_average_past = 0
        if cooperators_last_round >= self.m and self.recent_defection_streak > 0:
            self.recent_defection_streak = 0
            return Action.C
        if cooperators_last_round >= self.m or (cooperators_average_past >= self.m - 1 and self.recent_defection_streak < self.forgiveness_threshold):
            self.recent_defection_streak = 0
            return Action.C
        else:
            self.recent_defection_streak += 1
            return Action.D

class Strategy_COOPERATIVE_8(BaseStrategy):
    """
    Reciprocal Escalation and De-escalation strategy for the Collective Risk Dilemma.
    This strategy aims to foster cooperation by starting cooperatively,
    rewarding collective success, and conditionally defecting only when cooperation
    is demonstrably unsustainable or when individual defection becomes strategically optimal
    to achieve a threshold. It provides a "second chance" for the group if it cooperated
    but the threshold wasn't met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None:
            self.my_last_action = Action.C
            return Action.C
        last_round_idx = history.round_number - 1
        cooperators_from_me = 1 if self.my_last_action == Action.C else 0
        cooperators_from_opponents = np.sum(history.opponent_actions[last_round_idx, :])
        total_cooperators_last_round = cooperators_from_me + cooperators_from_opponents
        if total_cooperators_last_round >= m:
            self.my_last_action = Action.C
            return Action.C
        elif self.my_last_action == Action.C:
            self.my_last_action = Action.C
            return Action.C
        elif total_cooperators_last_round + 1 == m:
            self.my_last_action = Action.C
            return Action.C
        else:
            self.my_last_action = Action.D
            return Action.D

class Strategy_COOPERATIVE_9(BaseStrategy):
    """
    Cooperative Risk-Averse Strategy (CRAS)

    CRAS is a cooperative strategy that aims to achieve the collective reward
    in the Collective Risk Dilemma by contributing to the community project.
    It is pragmatically cautious, adjusting its risk tolerance based on past
    round outcomes. It cooperates when it perceives a sufficient probability
    of reaching the cooperation threshold, otherwise it defects to avoid losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self._cooperation_count_history = []
        self._my_cooperation_history = []
        self._successful_cooperation_rounds = 0
        self._risk_tolerance = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        self._update_internal_history(history)
        if current_round == self.n_rounds:
            historical_success_rate = self._successful_cooperation_rounds / len(self._cooperation_count_history) if self._cooperation_count_history else 0
            if historical_success_rate > 0.8:
                return Action.C
            else:
                return Action.D
        historical_success_rate = self._successful_cooperation_rounds / len(self._cooperation_count_history) if self._cooperation_count_history else 0
        estimated_cooperation_needed = self.m
        cooperated_last_round = self._my_cooperation_history[-1]
        last_round_cooperators = self._cooperation_count_history[-1]
        if historical_success_rate > 0.6:
            self._risk_tolerance = min(0.9, self._risk_tolerance * 1.1)
        elif historical_success_rate < 0.4:
            self._risk_tolerance = max(0.1, self._risk_tolerance * 0.8)
        if cooperated_last_round:
            if last_round_cooperators >= self.m:
                estimated_cooperation_needed = max(0, self.m - 1)
            elif last_round_cooperators >= self.m - 1:
                estimated_cooperation_needed = max(0, self.m - 1)
            else:
                estimated_cooperation_needed = self.m
        elif historical_success_rate > 0.5:
            estimated_cooperation_needed = max(0, self.m - 1)
        else:
            estimated_cooperation_needed = self.m
        if estimated_cooperation_needed <= 1 and self._risk_tolerance > 0.5:
            return Action.C
        else:
            return Action.D

    def _update_internal_history(self, history: PlayerHistory):
        """Updates internal state variables based on the provided PlayerHistory."""
        if not self._cooperation_count_history:
            my_action_last_round = history.my_actions[0]
            opponent_actions_last_round = history.opponent_actions[0]
            self._my_cooperation_history.append(my_action_last_round)
            current_cooperators = np.sum(opponent_actions_last_round)
            if my_action_last_round:
                current_cooperators += 1
            self._cooperation_count_history.append(current_cooperators)
            if current_cooperators >= self.m:
                self._successful_cooperation_rounds += 1
        else:
            my_action_last_round = history.my_actions[-1]
            opponent_actions_last_round = history.opponent_actions[-1]
            self._my_cooperation_history.append(my_action_last_round)
            current_cooperators = np.sum(opponent_actions_last_round)
            if my_action_last_round:
                current_cooperators += 1
            self._cooperation_count_history.append(current_cooperators)
            if current_cooperators >= self.m:
                self._successful_cooperation_rounds += 1

class Strategy_COOPERATIVE_10(BaseStrategy):
    """
    Adaptive Threshold Maintainer strategy for the Collective Risk Dilemma.
    This strategy cooperates if the observed cooperation level in the previous
    round meets or exceeds a dynamically adjusted target threshold. The target
    threshold adapts to maintain cooperation when successful and becomes more
    demanding when cooperation fails to meet the minimum requirement 'm'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.strategy_target_coop_pct = game_description.m / game_description.n_players
        self.first_round_cooperated = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        if history is None:
            self.first_round_cooperated = True
            return Action.C
        previous_round_actions = history.my_actions[-1]
        all_players_previous_round_actions = np.append(history.my_actions[-1], history.opponent_actions[-1])
        cooperators_in_previous_round = np.sum(all_players_previous_round_actions)
        observed_coop_pct_prev = cooperators_in_previous_round / n
        current_round_target_coop_pct = 0.0
        if observed_coop_pct_prev >= m / n:
            current_round_target_coop_pct = max(m / n, self.strategy_target_coop_pct * 0.95)
        else:
            current_round_target_coop_pct = max(m / n, self.strategy_target_coop_pct * 1.1)
        self.strategy_target_coop_pct = current_round_target_coop_pct
        if observed_coop_pct_prev >= current_round_target_coop_pct:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_11(BaseStrategy):
    """
    Adaptive Threshold Cooperation strategy for the Collective Risk Dilemma.
    This strategy aims to be maximally cooperative while remaining robust
    to potential defectors. It leverages the repeated nature of the game
    and the observed history to make informed decisions. The core idea is to
    conditionally cooperate, starting with a cooperative stance and adjusting
    based on the collective behavior of the group.

    Decision Logic (for rounds t > 1):
    1. Calculate observed cooperation rate: R_observed = C_{t-1} / n
    2. If R_observed < (m/n) * 0.8: Defect (D)
    3. If R_observed >= (m/n) * 1.2: Cooperate (C)
    4. Otherwise (buffer zone): Mirror the action taken in round t-1.

    Edge Cases:
    - First Round (t = 1): Cooperate (C)
    - Last Round (t = r): Defect (D)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if history is None:
            self.my_last_action = Action.C
            return Action.C
        if current_round_num == self.r:
            return Action.D
        my_previous_action_bool = history.my_actions[-1]
        opponent_actions_in_previous_round = history.opponent_actions[-1, :]
        cooperators_in_previous_round = 0
        if my_previous_action_bool:
            cooperators_in_previous_round += 1
        cooperators_in_previous_round += np.sum(opponent_actions_in_previous_round)
        observed_cooperation_rate = cooperators_in_previous_round / self.n
        min_cooperation_fraction = self.m / self.n
        cooperation_trigger_rate = min(1.0, min_cooperation_fraction * 1.2)
        defection_trigger_rate = min_cooperation_fraction * 0.8
        if observed_cooperation_rate < defection_trigger_rate:
            self.my_last_action = Action.D
            return Action.D
        elif observed_cooperation_rate >= cooperation_trigger_rate:
            self.my_last_action = Action.C
            return Action.C
        else:
            return self.my_last_action

class Strategy_COOPERATIVE_12(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy.

    This strategy is cooperative by default, aiming for the group benefit,
    but will defect if the cooperation threshold is not met in the previous
    round. It also defects in the last round to maximize individual payoff
    in a finitely repeated game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds:
            return Action.D
        if history is None:
            return Action.C
        previous_round_index = current_round_number - 1
        my_action_prev = history.my_actions[previous_round_index]
        opponent_actions_prev = history.opponent_actions[previous_round_index, :]
        cooperators_in_previous_round = int(my_action_prev)
        cooperators_in_previous_round += np.sum(opponent_actions_prev)
        if cooperators_in_previous_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_13(BaseStrategy):
    """
    The "Cautious Cooperative Threshold" (CCT) Strategy.
    This strategy cooperates if the previous round was close to meeting the
    cooperation threshold (m), and defects otherwise. It adapts its threshold
    based on the group's success. It defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_gap = self.game_description.m - 1
        self.initial_coop_gap = self.game_description.m - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == r:
            return Action.D
        if history is None:
            self.coop_gap = self.initial_coop_gap
            return Action.C
        previous_round_idx = state.round_number - 1
        total_cooperators_last_round = np.sum(history.opponent_actions[previous_round_idx, :]) + history.my_actions[previous_round_idx]
        if total_cooperators_last_round >= m:
            self.coop_gap = max(1, self.coop_gap - 1)
        elif total_cooperators_last_round < m - 1:
            self.coop_gap = min(n - 1, self.coop_gap + 1)
        other_cooperators_last_round = np.sum(history.opponent_actions[previous_round_idx, :])
        if history.my_actions[previous_round_idx]:
            other_cooperators_last_round -= 1
        if other_cooperators_last_round >= self.coop_gap:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_14(BaseStrategy):
    """
    Threshold Tit-for-Tat with Generous Forgiveness (TTGF) for the
    Collective Risk Dilemma.

    This strategy aims to be cooperative by default but retaliates against
    sustained underperformance by the group. It also exhibits forgiveness,
    returning to cooperation after a period of consistent group cooperation.
    The decision-making is based on observed cooperation rates in the
    previous round relative to predefined thresholds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.T_C = self.m / self.n_players
        self.T_D = (self.m - 1) / self.n_players
        self.P_persist = max(2, math.ceil(self.n_players / 3))
        self.F_persist = max(2, math.ceil(self.n_players / 3))
        self.delta = 0.05
        self.defect_streak = 0
        self.cooperate_streak = 0
        self.last_cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.last_cooperation_rate = 1.0
            return Action.C
        prev_round_idx = state.round_number - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + history.my_actions[prev_round_idx]
        cooperation_rate_prev_round = num_cooperators_prev_round / self.n_players
        if self.last_cooperation_rate is not None:
            if self.last_cooperation_rate < self.T_D:
                self.defect_streak += 1
                self.cooperate_streak = 0
            elif self.last_cooperation_rate >= self.T_C:
                self.cooperate_streak += 1
                self.defect_streak = 0
            elif self.last_cooperation_rate >= self.T_C - self.delta:
                self.cooperate_streak += 1
                self.defect_streak = 0
            else:
                self.defect_streak += 1
                self.cooperate_streak = 0
        self.last_cooperation_rate = cooperation_rate_prev_round
        if self.defect_streak >= self.P_persist:
            return Action.D
        elif self.cooperate_streak >= self.F_persist:
            return Action.C
        elif cooperation_rate_prev_round >= self.T_C:
            return Action.C
        elif cooperation_rate_prev_round < self.T_D:
            return Action.D
        elif cooperation_rate_prev_round >= self.T_C - self.delta:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_15(BaseStrategy):
    """
    Adaptive Cooperative Threshold Seeker (ACTS) strategy.
    ACTS aims to maximize long-term collective payoff by being conditionally cooperative.
    It prioritizes achieving the cooperative reward ($k$) by monitoring collective behavior
    and adjusting its actions based on the likelihood of meeting the cooperation threshold ($m$).
    It attempts to cooperate when the threshold is likely to be met and defects when it's clear
    cooperation will fail or when it perceives exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperators_in_round = np.zeros(self.n_rounds, dtype=int)
        self.total_cooperators_in_history = 0
        self.number_of_players_observed = 0
        self.exploited_defects = 0
        self.failed_cooperations = 0
        self.defensive_defection_counter = 0
        self.relaxed_threshold_counter = 0
        self.learning_phase_end = max(3, math.floor(self.n_rounds * 0.1))
        self.early_coop_threshold = max(1, self.m - 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds:
            return Action.D
        if self.defensive_defection_counter > 0:
            self.defensive_defection_counter -= 1
            return Action.D
        current_rac_threshold = 0.6
        if self.relaxed_threshold_counter > 0:
            self.relaxed_threshold_counter -= 1
            current_rac_threshold = 0.5
        if history is not None:
            num_coop_last_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
            self.cooperators_in_round[current_round - 1] = num_coop_last_round
            if num_coop_last_round >= self.m:
                if not history.my_actions[-1] and num_coop_last_round >= self.m:
                    self.exploited_defects += 1
            elif history.my_actions[-1] and num_coop_last_round < self.m:
                self.failed_cooperations += 1
            self.total_cooperators_in_history += num_coop_last_round
            self.number_of_players_observed += self.n_players
        if self.failed_cooperations > 3 and self.exploited_defects == 0:
            self.defensive_defection_counter = 5
            return Action.D
        if self.exploited_defects > 2 and self.failed_cooperations == 0:
            self.relaxed_threshold_counter = 5
        if current_round == 0:
            return Action.C
        if current_round < self.learning_phase_end:
            if current_round < self.m and current_round >= self.early_coop_threshold:
                return Action.C
            else:
                rac_window = min(current_round, 5)
                if rac_window > 0:
                    sum_coop_last_window = np.sum(self.cooperators_in_round[current_round - rac_window:current_round])
                    rac = sum_coop_last_window / (rac_window * self.n_players)
                else:
                    rac = 0
                if rac > 0.5:
                    return Action.C
                else:
                    return Action.D
        else:
            rac_window = min(current_round, 5)
            sum_coop_last_window = np.sum(self.cooperators_in_round[current_round - rac_window:current_round])
            rac = sum_coop_last_window / (rac_window * self.n_players)
            if self.number_of_players_observed > 0:
                lac = self.total_cooperators_in_history / self.number_of_players_observed
            else:
                lac = 0
            num_coop_last_round = self.cooperators_in_round[current_round - 1]
            if num_coop_last_round >= self.m:
                prev_round_coop_rate = num_coop_last_round / self.n_players
                if prev_round_coop_rate > self.m / self.n_players + 0.1:
                    return Action.C
                elif lac > 0.5:
                    return Action.C
                else:
                    return Action.D
            elif rac > current_rac_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_16(BaseStrategy):
    """
    Adaptive Threshold Cooperative (ATC) strategy for the Collective Risk Dilemma.

    This strategy cooperates in the first round. In subsequent rounds, it
    cooperates if the previous round met or exceeded the cooperation threshold (m/n).
    If the previous round fell short, it defects unless it's the last round,
    in which case it cooperates to maximize potential payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        n_rounds = self.game_description.n_rounds
        if history is None:
            return Action.C
        current_round_number = state.round_number
        n_remaining = n_rounds - current_round_number
        last_completed_round_index = history.round_number - 1
        my_action_prev = history.my_actions[last_completed_round_index]
        opponent_actions_prev = history.opponent_actions[last_completed_round_index, :]
        num_cooperators_prev_round = int(my_action_prev) + np.sum(opponent_actions_prev)
        coop_rate_prev = num_cooperators_prev_round / n_players
        coop_threshold_proportion = m / n_players
        if coop_rate_prev >= coop_threshold_proportion:
            return Action.C
        elif n_remaining > 1:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_17(BaseStrategy):
    """
    Strategy: "Conditional Reciprocity with Threshold Vigilance"

    This strategy aims to be cooperative by default, but it vigilantly monitors
    the group's performance and adjusts its behavior to safeguard against
    defection and to capitalize on collective success.

    Core Principles:
    1. Cooperative Default: Assume cooperation is best when the threshold is met.
    2. Reciprocity: Reward cooperation with cooperation, and punish defection with defection.
    3. Threshold Vigilance: Be aware of past cooperation counts and proximity to the threshold.
    4. Risk Aversion (Early Stages): Err on the side of caution in initial rounds.
    5. Exploiting Opportunity (Later Stages): Become more aggressive in contributing
       as confidence grows and the group demonstrates willingness to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters and internal state.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.threshold_reached_ever = False
        self.cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: PlayerHistory object containing past actions and payoffs.
                     None if it's the first round.

        Returns:
            The chosen Action (Action.C or Action.D).
        """
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if not self.threshold_reached_ever:
            for r in range(current_round_number):
                coop_count_this_round = np.sum(history.opponent_actions[r, :]) + (1 if history.my_actions[r] else 0)
                if coop_count_this_round >= self.m:
                    self.threshold_reached_ever = True
                    break
        cooperators_this_round = np.sum(history.opponent_actions[current_round_number - 1, :]) + (1 if history.my_actions[current_round_number - 1] else 0)
        self.cooperators_last_round = cooperators_this_round
        if self.threshold_reached_ever:
            return Action.C
        elif self.cooperators_last_round >= self.m:
            return Action.C
        elif self.cooperators_last_round >= self.m - 2 and self.m > 2:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_18(BaseStrategy):
    """
    Cautious Collective Trust (CCT) strategy for the Collective Risk Dilemma.
    This strategy aims to foster cooperation by initially trusting others,
    but quickly adapting to observed behavior. It prioritizes achieving the
    cooperation threshold (m) without being overly exploitative when cooperation fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the CCT strategy.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperator_counts_history = []
        self.my_actions_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past game data, or None for the first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        my_last_action = history.my_actions[-1]
        if current_round == self.r:
            if num_cooperators_prev_round >= self.m:
                return Action.C
            if num_cooperators_prev_round + 1 == self.m:
                return Action.C
            else:
                return Action.D
        total_cooperators_all_past_rounds = sum(history.my_actions)
        for i in range(current_round):
            total_cooperators_all_past_rounds += np.sum(history.opponent_actions[i, :])
        num_completed_rounds = current_round
        avg_cooperators_all_past_rounds = total_cooperators_all_past_rounds / num_completed_rounds if num_completed_rounds > 0 else 0
        if num_cooperators_prev_round >= self.m:
            return Action.C
        avg_opponent_coop_rate = (avg_cooperators_all_past_rounds - (1 if my_last_action else 0)) / max(1, num_completed_rounds)
        if avg_opponent_coop_rate + (1 if my_last_action else 0) >= (self.m - 1) / (self.n - 1) and num_cooperators_prev_round >= self.m - 1:
            return Action.C
        if avg_opponent_coop_rate >= (self.m - 1) / (self.n - 1) and num_cooperators_prev_round >= self.m - 1 and (not my_last_action):
            return Action.C
        if avg_opponent_coop_rate < (self.m - 2) / (self.n - 1) or (num_cooperators_prev_round < self.m and my_last_action):
            return Action.D
        return Action.D

class Strategy_COOPERATIVE_19(BaseStrategy):
    """
    Adaptive Risk Aversion with Tit-for-Tat strategy for the Collective Risk Dilemma.

    This strategy prioritizes cooperation, monitors group behavior, punishes defection
    when the cooperation threshold is not met, and forgives if cooperation resumes.
    It also incorporates specific logic for the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = history.round_number
        my_previous_action = self.decode_action(history.my_actions[-1])
        if current_round == self.r:
            return Action.D
        previous_round_cooperators = np.sum(history.opponent_actions[-1, :])
        if my_previous_action == Action.C:
            previous_round_cooperators += 1
        if previous_round_cooperators >= self.m:
            return Action.C
        elif my_previous_action == Action.C:
            if previous_round_cooperators >= self.m - 1:
                return Action.C
            else:
                return Action.D
        elif previous_round_cooperators >= self.m - 1:
            return Action.C
        else:
            return Action.D

    def decode_action(self, encoded_action: np.bool_) -> Action:
        """Decodes the boolean action representation to Action enum."""
        return Action.C if encoded_action else Action.D

class Strategy_COOPERATIVE_20(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Monitoring.

    This strategy aims to foster cooperation by monitoring collective behavior.
    It leans towards cooperation early on, but will defect if cooperation
    consistently fails to meet the threshold or if it's being exploited.
    It also employs end-game defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_actions_history = []
        self.cooperators_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        if len(self.my_actions_history) == 0:
            return Action.C
        last_round_cooperators = self.cooperators_history[-1]
        last_round_defectors = self.n - last_round_cooperators
        if last_round_cooperators >= self.m:
            return Action.C
        if last_round_cooperators >= 1:
            return Action.C
        if last_round_cooperators == 0:
            return Action.D
        if last_round_defectors >= self.n - self.m + 1:
            return Action.D
        return Action.D

    def _update_history(self, my_action: Action, cooperators_this_round: int):
        """Helper to update internal history."""
        self.my_actions_history.append(my_action)
        self.cooperators_history.append(cooperators_this_round)

    def update_state_after_round(self, my_action_taken: Action, opponents_actions_this_round: NDArray[np.bool_], my_payoff_this_round: float, opponents_payoffs_this_round: NDArray[np.float64]):
        """
        Update internal state with results of the last round.
        This method is called by the framework *after* __call__ has been executed for the round.
        """
        cooperators_this_round = np.sum(opponents_actions_this_round) + (1 if my_action_taken == Action.C else 0)
        self._update_history(my_action_taken, cooperators_this_round)

class Strategy_COOPERATIVE_21(BaseStrategy):
    """
    The "Cautious Reciprocator with Adaptive Threshold" strategy for the
    Collective Risk Dilemma.

    This strategy aims to achieve the collective reward by cooperating,
    but it adapts its behavior based on the observed cooperation levels of
    other players. It reciprocates successful cooperation, punishes prolonged
    lack of cooperation by defecting, and uses an adaptive threshold to
    become more forgiving if others show some effort towards meeting the
    cooperation requirement.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.previous_cooperators_count = 0
        self.dynamic_min_cooperation_threshold = max(1, math.floor(self.m - (self.m - 1) / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.dynamic_min_cooperation_threshold = max(1, math.floor(self.m - (self.m - 1) / 2))
            return Action.C
        current_round = history.round_number
        actual_cooperators_last_round = np.sum(history.my_actions[current_round - 1])
        actual_cooperators_last_round += np.sum(history.opponent_actions[current_round - 1, :])
        self.previous_cooperators_count = actual_cooperators_last_round
        if self.previous_cooperators_count < self.m:
            if self.previous_cooperators_count >= self.dynamic_min_cooperation_threshold:
                self.dynamic_min_cooperation_threshold = max(1, self.dynamic_min_cooperation_threshold - 1)
        else:
            self.dynamic_min_cooperation_threshold = self.m
        if self.previous_cooperators_count >= self.m:
            return Action.C
        elif self.previous_cooperators_count >= self.dynamic_min_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_22(BaseStrategy):
    """
    Adaptive Threshold Responder: A strategy for the Collective Risk Dilemma that
    prioritizes cooperation but dynamically adjusts its behavior based on observed
    cooperation success in previous rounds. It aims to achieve collective rewards
    while mitigating the risk of being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.8
        self.beta = 2.0
        self.gamma = 1.5
        self.min_cooperation_prob = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        if history is None:
            cooperation_probability = self.alpha
        else:
            my_last_action_cooperated = self.game_description.n_rounds == 0 or (self.game_description.n_rounds == 1 and self.game_description.n_players > 1)
            my_actions_list = []
            if hasattr(history, 'my_actions'):
                my_actions_list = history.my_actions
            if len(my_actions_list) > 0 and my_actions_list[-1]:
                num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + 1
            else:
                num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            cooperation_rate_prev_round = num_cooperators_prev_round / n
            just_below_threshold_target = (m - 1) / n
            significantly_below_threshold_target = (m - 2) / n
            if cooperation_rate_prev_round >= just_below_threshold_target:
                adjustment = self.beta * (cooperation_rate_prev_round - just_below_threshold_target)
                cooperation_probability = self.alpha * min(1.0, 1.0 + adjustment)
            elif cooperation_rate_prev_round >= significantly_below_threshold_target:
                adjustment = self.beta * (cooperation_rate_prev_round - significantly_below_threshold_target)
                cooperation_probability = self.alpha * min(1.0, 1.0 + adjustment)
            else:
                reduction = self.gamma * (significantly_below_threshold_target - cooperation_rate_prev_round)
                cooperation_probability = max(self.min_cooperation_prob, 1.0 - reduction)
        cooperation_probability = max(0.0, min(1.0, cooperation_probability))
        if random.random() <= cooperation_probability:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_23(BaseStrategy):
    """
    "Conditional Reward-Seeking" strategy for the Collective Risk Dilemma.

    This strategy aims to consistently push towards cooperation but intelligently
    scales back its commitment when faced with widespread defection, prioritizing
    survival and maximizing potential gains when conditions are favorable.

    It adjusts its probability of cooperating based on the observed success rate
    of meeting the cooperation threshold in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players,
                              n_rounds, m, and k.
        """
        self.game_description = game_description
        self.base_cooperation_probability = max(0.5, self.game_description.m / self.game_description.n_players)
        self.successful_rounds_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None if it's the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        current_round = state.round_number
        n_players = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if current_round == r:
            return Action.D
        if history is None:
            return Action.C
        own_cooperators_prev_round = 1 if self.get_my_action_in_round(history, current_round - 1) == Action.C else 0
        opponents_cooperators_prev_round = 0
        if hasattr(history, 'opponent_actions') and history.opponent_actions.size > 0:
            opponents_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
        num_cooperators_prev_round = own_cooperators_prev_round + opponents_cooperators_prev_round
        if num_cooperators_prev_round >= m:
            self.successful_rounds_count += 1
        if current_round > 1:
            success_rate = self.successful_rounds_count / current_round
        else:
            success_rate = 0.0
        cooperation_probability = self.base_cooperation_probability
        if success_rate > 0.75:
            cooperation_probability = self.base_cooperation_probability + 0.1 * (1 - self.base_cooperation_probability)
        elif success_rate < 0.25:
            if num_cooperators_prev_round >= m - 2:
                cooperation_probability = self.base_cooperation_probability
            else:
                cooperation_probability = self.base_cooperation_probability * 0.75
        cooperation_probability = max(0.0, min(1.0, cooperation_probability))
        if random.random() < cooperation_probability:
            return Action.C
        else:
            return Action.D

    def get_my_action_in_round(self, history: PlayerHistory, round_idx: int) -> Action:
        """
        Retrieves the player's action in a specific historical round.

        Args:
            history: The PlayerHistory object.
            round_idx: The 0-indexed round number.

        Returns:
            The Action taken by the player in that round.
        """
        if 0 <= round_idx < len(history.my_actions):
            return Action.C if history.my_actions[round_idx] else Action.D
        return Action.D

class Strategy_COOPERATIVE_24(BaseStrategy):
    """
    The "Threshold Sentinel" strategy.

    This strategy cooperates if there's a reasonable expectation that the
    cooperation threshold (m) will be met, based on the previous round's
    cooperation count. It starts by cooperating in the first round and
    adapts its behavior based on historical data. It includes specific
    logic for the final round and to prevent being the sole defector
    when it would prevent the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperators_in_previous_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.cooperators_in_previous_round = 0
            return Action.C
        if current_round > 0:
            if current_round == 1:
                my_coop_prev = self.my_action_to_bool(history.my_actions[0])
                opp_coops_prev = np.sum(history.opponent_actions[0, :])
                self.cooperators_in_previous_round = int(my_coop_prev) + int(opp_coops_prev)
            else:
                my_coop_prev = self.my_action_to_bool(history.my_actions[current_round - 1])
                opp_coops_prev = np.sum(history.opponent_actions[current_round - 1, :])
                self.cooperators_in_previous_round = int(my_coop_prev) + int(opp_coops_prev)
        else:
            self.cooperators_in_previous_round = 0
        would_defect_alone_prevent_threshold = self.cooperators_in_previous_round == self.m - 1 and self.n - self.cooperators_in_previous_round == 1
        should_cooperate = self.cooperators_in_previous_round >= self.m - 1 and (current_round < self.r or not would_defect_alone_prevent_threshold)
        should_defect = self.cooperators_in_previous_round < self.m - 1 or (current_round == self.r and self.cooperators_in_previous_round < self.m)
        if should_cooperate:
            return Action.C
        elif should_defect:
            return Action.D
        else:
            return Action.C

    def my_action_to_bool(self, action: Action) -> int:
        """Converts Action enum to its boolean/integer representation."""
        return 1 if action == Action.C else 0

class Strategy_COOPERATIVE_25(BaseStrategy):
    """
    Adaptive Threshold Weaver: This strategy aims to maximize individual and
    collective payoffs by adaptively responding to the observed cooperation
    levels, with a strong bias towards cooperation when the collective is
    close to meeting the threshold. It learns from history and promotes
    cooperation without explicit communication.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_count_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.cooperation_count_prev_round = 0
            return Action.C
        self.cooperation_count_prev_round = int(np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :]))
        if current_round == self.r:
            return Action.D
        cooperation_momentum = 'Neutral/Approaching'
        if self.cooperation_count_prev_round >= self.m:
            cooperation_momentum = 'Positive'
        elif self.cooperation_count_prev_round <= self.m - 3:
            cooperation_momentum = 'Negative'
        if cooperation_momentum == 'Positive':
            return Action.C
        elif cooperation_momentum == 'Neutral/Approaching':
            return Action.C
        elif cooperation_momentum == 'Negative':
            if self.m - 1 > self.cooperation_count_prev_round:
                return Action.D
            else:
                return Action.D

class Strategy_COOPERATIVE_26(BaseStrategy):
    """
    The "Threshold Weaver" strategy aims to establish and maintain cooperation by
    dynamically adjusting its behavior based on observed cooperation levels in
    previous rounds. It prioritizes achieving the collective benefit while
    minimizing the risk of being exploited.

    - Initial Cooperation: Always starts by cooperating to give cooperation a chance.
    - Dynamic Threshold: Maintains an internal 'expected cooperation level' (threshold).
      This threshold increases if cooperation was successful and decreases if it failed.
    - Conditional Cooperation: Cooperates if the previous round's cooperation count
      met or exceeded the current threshold.
    - Adaptive Punishment: Defects if the previous round's cooperation count fell
      below the threshold, becoming more cautious.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_threshold = 0.0
        self.initialized = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.current_threshold = self.m + math.floor((self.n - self.m) / 2)
            self.initialized = True
            return Action.C
        if not self.initialized:
            self.current_threshold = self.m + math.floor((self.n - self.m) / 2)
            self.initialized = True
        num_cooperators_last_round = 0
        if self.m > 0:
            if history.my_actions[history.round_number - 1]:
                num_cooperators_last_round += 1
            if self.n > 1:
                num_cooperators_last_round += np.sum(history.opponent_actions[history.round_number - 1, :])
        if num_cooperators_last_round >= self.current_threshold:
            self.current_threshold = min(self.n, self.current_threshold + 1)
            return Action.C
        else:
            self.current_threshold = max(self.m - 1, self.current_threshold - 1)
            return Action.D

class Strategy_COOPERATIVE_27(BaseStrategy):
    """
    Cooperative Adaptive Strategy (CAS) for the Collective Risk Dilemma.
    This strategy aims to foster cooperation by taking calculated risks,
    rewarding observed cooperation, and punishing defection, while remaining
    flexible to the evolving game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action = None
        self.observed_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round = state.round_number
        if history is None:
            self.my_last_action = Action.C
            return Action.C
        self.observed_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] == Action.C else 0)
        self.my_last_action = history.my_actions[-1]
        if current_round == r:
            return Action.D
        coop_ratio_prev_round = self.observed_cooperators_prev_round / n
        high_confidence_threshold = m / n + 0.15
        moderate_confidence_lower_bound = m / n
        moderate_confidence_upper_bound = m / n + 0.15
        if coop_ratio_prev_round >= moderate_confidence_upper_bound:
            self.my_last_action = Action.C
            return Action.C
        elif coop_ratio_prev_round >= moderate_confidence_lower_bound:
            if self.my_last_action == Action.C:
                self.my_last_action = Action.C
                return Action.C
            else:
                self.my_last_action = Action.C
                return Action.C
        elif self.my_last_action == Action.C:
            self.my_last_action = Action.D
            return Action.D
        else:
            self.my_last_action = Action.D
            return Action.D

class Strategy_COOPERATIVE_28(BaseStrategy):
    """
    The "Adaptive Threshold Seeker" strategy for the Collective Risk Dilemma.
    This strategy aims to achieve cooperation by continuously assessing the
    collective risk and adapting its behavior to encourage the group to reach
    the cooperation threshold (m). It prioritizes achieving the higher payoff
    associated with cooperation while being resilient to exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description
        self.observed_cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: PlayerHistory object containing past actions and payoffs.
                     None for the first round.

        Returns:
            Action.C for Cooperate, Action.D for Defect.
        """
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == r:
            return Action.D
        if hasattr(history, 'my_actions') and history.my_actions is not None and (len(history.my_actions) > 0):
            if hasattr(history, 'opponent_actions') and history.opponent_actions is not None and (history.opponent_actions.shape[0] > 0):
                cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            else:
                cooperators_in_prev_round = history.my_actions[-1]
        else:
            cooperators_in_prev_round = 0
        effective_cooperation_percentage = cooperators_in_prev_round / n
        base_threshold_percentage = m / n
        cooperation_confidence_threshold = max(base_threshold_percentage, base_threshold_percentage * 1.1)
        if effective_cooperation_percentage >= cooperation_confidence_threshold:
            return Action.C
        elif effective_cooperation_percentage >= base_threshold_percentage:
            return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_29(BaseStrategy):
    """
    Conditional Reciprocity with Risk Aversion (CRRA) Strategy.

    This strategy aims to foster cooperation in the Collective Risk Dilemma
    by conditionally cooperating based on observed group behavior and
    personal risk assessment. It initiates cooperation, rewards sufficient
    group cooperation, and defects to protect itself when cooperation is
    unlikely to meet the threshold. It includes a limited defection phase
    to signal dissatisfaction while remaining open to rejoining cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_actions_history = []
        self.n_defect_punishment_rounds = max(2, math.ceil(self.r / 5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if current_round_num == 0:
            self.my_actions_history.append(True)
            return Action.C
        last_round_idx = current_round_num - 1
        num_cooperators_last_round = np.sum(history.opponent_actions[last_round_idx, :]) + (1 if self.my_actions_history[-1] else 0)
        prop_cooperators_last_round = num_cooperators_last_round / self.n
        recent_def_count = 0
        for action_bool in reversed(self.my_actions_history[-self.n_defect_punishment_rounds:]):
            if not action_bool:
                recent_def_count += 1
            else:
                break
        if len(self.my_actions_history) >= self.n_defect_punishment_rounds and recent_def_count == self.n_defect_punishment_rounds:
            if prop_cooperators_last_round * self.n >= self.m:
                self.my_actions_history.append(True)
                return Action.C
            else:
                self.my_actions_history.append(False)
                return Action.D
        elif prop_cooperators_last_round * self.n >= self.m:
            self.my_actions_history.append(True)
            return Action.C
        else:
            self.my_actions_history.append(False)
            return Action.D

class Strategy_COOPERATIVE_30(BaseStrategy):
    """
    The "Cautious Optimist" strategy for the Collective Risk Dilemma.

    This strategy aims to achieve the collective reward by cooperating when the
    probability of success is high, while protecting itself when cooperation is
    unlikely to succeed. It learns from past rounds to adjust its behavior.

    Core Principles:
    1. Conditional Cooperation: Cooperate only when there's a reasonable expectation
       that the threshold 'm' will be met.
    2. Adaptive Threshold: Dynamically adjust the perceived likelihood of others
       cooperating based on historical data.
    3. End-of-Game Consideration: Slightly more cautious in the final round to
       secure accumulated payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.buffer_factor = 0.2
        self.moving_average_window = 5

    def _calculate_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Calculates the smoothed cooperation rate based on historical data.
        Uses a simple moving average over the last 'moving_average_window' rounds.
        """
        n_past_rounds = history.round_number
        if n_past_rounds == 0:
            return 0.0
        window_size = min(n_past_rounds, self.moving_average_window)
        recent_opponent_actions = history.opponent_actions[-window_size:]
        total_cooperators_in_window = np.sum(recent_opponent_actions)
        total_actions_in_window = window_size * self.game_description.n_players
        if total_actions_in_window == 0:
            return 0.0
        return total_cooperators_in_window / total_actions_in_window

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs for all players.
                     None if it's the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        current_round_number = state.round_number
        cooperation_threshold_needed = m
        buffer = self.buffer_factor * cooperation_threshold_needed
        if history is None:
            if m <= n / 2:
                return Action.C
            else:
                return Action.D
        cooperation_rate = self._calculate_cooperation_rate(history)
        expected_cooperators = cooperation_rate * n
        if expected_cooperators >= cooperation_threshold_needed - buffer:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_31(BaseStrategy):
    """
    The Adaptive Reciprocator strategy for the Collective Risk Dilemma.
    This strategy aims to promote cooperation by starting cooperatively,
    and then adapting its cooperation threshold based on the group's
    observed cooperativeness in previous rounds. It also includes
    specific logic for the final round to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.cooperation_threshold = self.m / self.n
        self.defection_threshold = self.m / self.n
        self.adjustment_factor_increase = 1.02
        self.adjustment_factor_decrease = 0.98
        self.significant_cooperation_margin = 0.15
        self.last_strategy_action = None

    def _calculate_gcr(self, opponent_actions_in_round: NDArray[np.bool_]) -> float:
        """Calculates the Group Cooperation Ratio for a given round's opponent actions."""
        num_cooperators = np.sum(opponent_actions_in_round)
        return num_cooperators / self.n

    def _update_thresholds(self, history: PlayerHistory):
        """
        Updates the cooperation and defection thresholds based on the history.
        This method should be called after the current round's decision is made
        but before the next round's decision is calculated.
        """
        if history.round_number < 2:
            return
        gcr_prev_prev_round = self._calculate_gcr(history.opponent_actions[-2, :])
        gcr_last_round = self._calculate_gcr(history.opponent_actions[-1, :])
        if self.m / self.n <= gcr_last_round < self.cooperation_threshold:
            self.cooperation_threshold *= self.adjustment_factor_decrease
            self.defection_threshold *= self.adjustment_factor_decrease
        elif gcr_last_round >= self.cooperation_threshold + self.significant_cooperation_margin:
            self.cooperation_threshold *= self.adjustment_factor_increase
            self.defection_threshold *= self.adjustment_factor_increase
        elif gcr_last_round < self.m / self.n and self.last_strategy_action == Action.D:
            self.defection_threshold *= self.adjustment_factor_decrease
        elif gcr_last_round < self.m / self.n and self.last_strategy_action == Action.C:
            self.defection_threshold *= self.adjustment_factor_increase
        self.cooperation_threshold = max(self.m / self.n, min(1.0, self.cooperation_threshold))
        self.defection_threshold = max(self.m / self.n, min(1.0, self.defection_threshold))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.
        """
        current_round = state.round_number
        if history is None:
            self.last_strategy_action = Action.C
            return Action.C
        self._update_thresholds(history)
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        gcr_prev_round = num_cooperators_prev_round / self.n
        if current_round == self.r - 1:
            if gcr_prev_round >= self.cooperation_threshold:
                self.last_strategy_action = Action.C
                return Action.C
            else:
                self.last_strategy_action = Action.D
                return Action.D
        if gcr_prev_round >= self.cooperation_threshold:
            self.last_strategy_action = Action.C
            return Action.C
        elif gcr_prev_round < self.defection_threshold:
            self.last_strategy_action = Action.D
            return Action.D
        else:
            self.last_strategy_action = Action.C
            return Action.C

class Strategy_COOPERATIVE_32(BaseStrategy):
    """
    Adaptive Threshold Seeker: Cooperates initially and continues to cooperate
    if the previous round was at or one player short of the cooperation threshold (m).
    Defects if the previous round was two or more players short of the threshold.
    Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == r - 1:
            return Action.D
        previous_round_idx = current_round_number - 1
        own_cooperators_prev_round = 1 if history.my_actions[previous_round_idx] else 0
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        num_cooperators_previous_round = own_cooperators_prev_round + opponent_cooperators_prev_round
        if num_cooperators_previous_round >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_33(BaseStrategy):
    """
    Adaptive Threshold Cooperative (ATC) strategy.
    This strategy aims to be cooperative by default, but it's also pragmatic
    and responsive to the collective actions of others. It focuses on reaching
    the cooperation threshold to unlock the higher payoffs for everyone, while
    also protecting itself if cooperation fails.

    Decision Logic:
    - First Round (history is None): Cooperate (C) to initiate.
    - Subsequent Rounds (using history):
        - Count cooperators in the previous round.
        - If previous cooperators >= m: Cooperate (C) to reinforce success.
        - If previous cooperators < m:
            - If it's not the last round: Cooperate (C) to persist and give the group
              another chance to reach the threshold.
            - If it is the last round: Defect (D) to secure private endowment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        else:
            current_round_num = state.round_number + 1
            if history.round_number == 0:
                return Action.C
            my_action_last_round = Action.C if history.my_actions[-1] else Action.D
            opponent_actions_last_round = history.opponent_actions[-1, :]
            num_cooperated_last_round = 0
            if my_action_last_round == Action.C:
                num_cooperated_last_round += 1
            num_cooperated_last_round += np.sum(opponent_actions_last_round)
            if num_cooperated_last_round >= self.m:
                return Action.C
            elif current_round_num < self.r:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_34(BaseStrategy):
    """
    "Cautious Cooperation" strategy for the Collective Risk Dilemma.

    This strategy aims to cooperate when it's likely to meet the cooperation
    threshold 'm', especially if the reward 'k' is high. It adapts based on
    past cooperation rates and avoids being exploited by defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperators_per_round = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            if self.m <= self.n / 2.0:
                return Action.C
            else:
                return Action.D
        prev_round_my_action = history.my_actions[current_round - 1]
        prev_round_opponent_actions = history.opponent_actions[current_round - 1, :]
        num_cooperators_prev_round = np.sum(prev_round_opponent_actions) + (1 if prev_round_my_action else 0)
        self.cooperators_per_round.append(num_cooperators_prev_round)
        if current_round == self.r - 1:
            if not self.cooperators_per_round:
                return Action.D
            avg_coop_rate_past = np.mean(self.cooperators_per_round)
            last_completed_round_coop_count = self.cooperators_per_round[-1]
            if avg_coop_rate_past + 1 >= self.m and last_completed_round_coop_count > 0:
                return Action.C
            else:
                return Action.D
        if not self.cooperators_per_round:
            return Action.D
        avg_coop_rate_past = np.mean(self.cooperators_per_round)
        last_completed_round_coop_count = self.cooperators_per_round[-1]
        if avg_coop_rate_past + 1 >= self.m and last_completed_round_coop_count > 0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_35(BaseStrategy):
    """
    Adaptive Reward Seeker strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by rewarding successful outcomes
    and cautiously defecting when cooperation fails. It adapts its probability
    of cooperation based on past round results, with a special handling for
    the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.initial_cooperation_probability = 0.5
        self.delta_increase_success = 0.1
        self.delta_increase_near_miss = 0.05
        self.delta_decrease_failure = 0.08
        self.threshold_for_guaranteed_cooperation = 0.9
        self.threshold_for_guaranteed_defection = 0.1
        self.cooperation_probability = self.initial_cooperation_probability

    def _count_cooperators(self, actions: NDArray[np.bool_]) -> int:
        """Counts the number of True (Cooperate) actions in an array."""
        return int(np.sum(actions))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round = state.round_number
        if current_round == r - 1:
            return Action.D
        if history is None:
            self.cooperation_probability = self.initial_cooperation_probability
            if self.cooperation_probability >= self.threshold_for_guaranteed_cooperation:
                return Action.C
            elif self.cooperation_probability <= self.threshold_for_guaranteed_defection:
                return Action.D
            else:
                return Action.C if random.random() < self.cooperation_probability else Action.D
        previous_round_actions_self = history.my_actions[current_round - 1]
        previous_round_actions_opponents = history.opponent_actions[current_round - 1, :]
        all_players_actions_prev_round = np.concatenate(([previous_round_actions_self], previous_round_actions_opponents))
        cooperators_in_previous_round = self._count_cooperators(all_players_actions_prev_round)
        if cooperators_in_previous_round >= m:
            self.cooperation_probability = min(1.0, self.cooperation_probability + self.delta_increase_success)
        elif cooperators_in_previous_round == m - 1:
            self.cooperation_probability = min(1.0, self.cooperation_probability + self.delta_increase_near_miss)
        else:
            self.cooperation_probability = max(0.0, self.cooperation_probability - self.delta_decrease_failure)
        if self.cooperation_probability >= self.threshold_for_guaranteed_cooperation:
            return Action.C
        elif self.cooperation_probability <= self.threshold_for_guaranteed_defection:
            return Action.D
        else:
            return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COOPERATIVE_36(BaseStrategy):
    """
    The Adaptive Safety Net strategy for the Collective Risk Dilemma.

    This strategy aims to be a proactive cooperator, prioritizing collective
    success. It dynamically adjusts its tolerance for cooperation based on
    past round outcomes, aiming for robustness against exploitation and
    ensuring a reasonable outcome even if cooperation falters.

    Key Features:
    - Proactive Cooperation: Defaults to cooperating to initiate collective action.
    - Dynamic Tolerance: Adjusts willingness to cooperate based on success rate.
    - Safety Net: Maintains a minimum tolerance to avoid immediate defection
                   upon minor failures.
    - End-Game Strategy: Defects in the final round if cooperation failed
                         in the penultimate round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveSafetyNetStrategy.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.current_round_num = 0
        self.cooperation_tolerance = float(self.m)
        self.total_successful_rounds = 0
        self.tolerance_increase_factor = 0.05
        self.tolerance_decrease_factor = 0.02
        self.min_cooperation_tolerance = float(self.m) / 2.0
        self.cooperators_per_round = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including round number.
            history: History of past actions and payoffs, or None if it's the first round.

        Returns:
            Action.C if cooperating, Action.D if defecting.
        """
        self.current_round_num = state.round_number
        if history is None:
            return Action.C
        my_prev_actions = history.my_actions.tolist()
        opponent_prev_actions = history.opponent_actions.tolist()
        last_round_cooperators = sum((1 for action in opponent_prev_actions[-1] if action)) + (1 if my_prev_actions[-1] else 0)
        self.cooperators_per_round.append(last_round_cooperators)
        was_successful = last_round_cooperators >= self.m
        if was_successful:
            self.total_successful_rounds += 1
            increase_amount = self.m * self.tolerance_increase_factor
            self.cooperation_tolerance = min(float(self.m), self.cooperation_tolerance + increase_amount)
        else:
            decrease_amount = self.m * self.tolerance_decrease_factor
            self.cooperation_tolerance = max(self.min_cooperation_tolerance, self.cooperation_tolerance - decrease_amount)
        if self.current_round_num == self.r:
            if not was_successful:
                return Action.D
            else:
                return Action.C
        if was_successful:
            return Action.C
        elif last_round_cooperators >= self.cooperation_tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_37(BaseStrategy):
    """
    The "Resilient Rewarder" strategy for the Collective Risk Dilemma.

    This strategy aims to be a reliable cooperator while simultaneously rewarding
    the group's collective success and being judicious about risk. It is designed
    to incentivize others to cooperate by demonstrating a willingness to contribute
    when the group is close to the cooperation threshold, and adapting based on
    collective outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.player_id = None
        self.action_map = {True: Action.C, False: Action.D}
        self.reverse_action_map = {Action.C: True, Action.D: False}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.player_id = 0
            return Action.C
        if self.player_id is None:
            if len(history.my_actions) > 0:
                pass
            else:
                self.player_id = 0
        my_last_action_bool = history.my_actions[-1]
        prev_round_actions_bool = history.opponent_actions[-1]
        cooperators_last_round = np.sum(prev_round_actions_bool) + (1 if my_last_action_bool else 0)
        my_decision_last_round = my_last_action_bool
        if current_round == self.n_rounds:
            return Action.C
        if cooperators_last_round >= self.m:
            return Action.C
        elif cooperators_last_round == self.m - 1:
            return Action.C
        elif cooperators_last_round < self.m - 1 and my_decision_last_round:
            return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_38(BaseStrategy):
    """
    The Cautious Cooperator strategy aims to maximize collective payoff while
    being resilient to defections. It learns from past behavior, prioritizing
    cooperation when the threshold is likely to be met, but defecting strategically
    to protect itself when cooperation seems improbable or during the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Cautious Cooperator strategy.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.prob_cooperate_uncertain_zone_high = 0.7
        self.prob_cooperate_uncertain_zone_low = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round.

        Returns:
            Action.C if the player decides to cooperate, Action.D if they decide to defect.
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        previous_round_my_action = 1 if history.my_actions[current_round - 1] else 0
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[current_round - 1, :])
        cooperators_last_round = previous_round_my_action + previous_round_opponent_cooperators
        needed_cooperators = self.m - cooperators_last_round
        if needed_cooperators == 1:
            return Action.C
        if needed_cooperators <= 0:
            return Action.C
        if cooperators_last_round <= self.m - 2:
            return Action.D
        probability_to_cooperate = 0.0
        if needed_cooperators == 2:
            probability_to_cooperate = self.prob_cooperate_uncertain_zone_high
        elif needed_cooperators > 2 and cooperators_last_round > 0:
            probability_to_cooperate = self.prob_cooperate_uncertain_zone_low
        if random.random() < probability_to_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_39(BaseStrategy):
    """
    Adaptive Threshold Cooperate (ATC) Strategy.

    This strategy aims to maximize payoffs by cooperating when there's a high
    probability of meeting the cooperation threshold 'm', and defecting otherwise.
    It dynamically adjusts its cooperation probability based on past observed
    cooperation rates and a meta-adaptive baseline. It's designed to be
    cooperative when beneficial but also robust against exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATC strategy.

        Args:
            game_description: A CollectiveRiskDescription object containing game parameters.
        """
        self.game_description = game_description
        self.initial_cooperation_prob_t1 = max(0.0, min(1.0, self.game_description.m / self.game_description.n_players + 0.1))
        self.adaptation_factor_up = 0.5
        self.adaptation_factor_down = 0.7
        self.baseline_cooperation_prob = max(0.0, min(1.0, self.game_description.m / self.game_description.n_players))
        self.meta_adaptation_window = 10
        self.meta_adaptation_increase = 0.01
        self.meta_adaptation_decrease = 0.02
        self.high_payoff_threshold = 1.5
        self.low_payoff_threshold = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round.

        Returns:
            Action: The action to take (Action.C or Action.D).
        """
        current_round = state.round_number
        if current_round == self.game_description.n_rounds:
            return Action.D
        if history is None:
            if random.random() < self.initial_cooperation_prob_t1:
                return Action.C
            else:
                return Action.D
        n_players = self.game_description.n_players
        m = self.game_description.m
        my_last_action = 1 if history.my_actions[current_round - 1] else 0
        opponent_cooperators_last_round = int(np.sum(history.opponent_actions[current_round - 1, :]))
        n_cooperators_prev_round = my_last_action + opponent_cooperators_last_round
        n_cooperators_ratio_prev_round = n_cooperators_prev_round / n_players
        if len(history.my_payoffs) >= self.meta_adaptation_window:
            recent_payoffs = history.my_payoffs[-self.meta_adaptation_window:]
            avg_payoff_recent = np.mean(recent_payoffs)
            if avg_payoff_recent > self.high_payoff_threshold:
                self.baseline_cooperation_prob = min(1.0, self.baseline_cooperation_prob + self.meta_adaptation_increase)
            elif avg_payoff_recent <= self.low_payoff_threshold:
                self.baseline_cooperation_prob = max(0.0, self.baseline_cooperation_prob - self.meta_adaptation_decrease)
        target_cooperation_ratio = m / n_players
        current_cooperation_prob = self.baseline_cooperation_prob
        if n_cooperators_ratio_prev_round >= target_cooperation_ratio:
            adjustment = (n_cooperators_ratio_prev_round - target_cooperation_ratio) * self.adaptation_factor_up
            current_cooperation_prob = min(1.0, current_cooperation_prob + adjustment)
        else:
            adjustment = (target_cooperation_ratio - n_cooperators_ratio_prev_round) * self.adaptation_factor_down
            current_cooperation_prob = max(0.0, current_cooperation_prob - adjustment)
        if random.random() < current_cooperation_prob:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_40(BaseStrategy):
    """
    Conditional Trust Tit-for-Tat strategy for the Collective Risk Dilemma.
    This strategy initially cooperates, and then cooperates if the cooperation
    threshold was met in the previous round. If the threshold was not met,
    it defects until the threshold is met again.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing the game's parameters
                              (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.in_punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs from previous rounds.
                     None for the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        if history is None:
            self.in_punishment_phase = False
            return Action.C
        m_threshold = self.game_description.m
        last_round_index = history.round_number - 1
        my_last_action = Action.C if history.my_actions[last_round_index] else Action.D
        cooperators_last_round = 0
        if my_last_action == Action.C:
            cooperators_last_round += 1
        if history.opponent_actions.shape[1] > 0:
            cooperators_last_round += np.sum(history.opponent_actions[last_round_index, :])
        if cooperators_last_round < m_threshold:
            self.in_punishment_phase = True
        else:
            self.in_punishment_phase = False
        if self.in_punishment_phase:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_41(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC) strategy.

    This strategy aims to promote cooperation in the Collective Risk Dilemma.
    It cooperates in the first round and then monitors the level of cooperation
    in the previous round. If the cooperation threshold (m) was met, it continues
    to cooperate. If the threshold was not met, it defects to protect its
    individual resources.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        else:
            if history.round_number == 0:
                return Action.C
            previous_round_index = history.round_number - 1
            my_previous_action = 1 if history.my_actions[previous_round_index] else 0
            cooperators_in_previous_round = my_previous_action + np.sum(history.opponent_actions[previous_round_index, :])
            if cooperators_in_previous_round >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_42(BaseStrategy):
    """
    The "Cautious Collaborator" strategy for the Collective Risk Dilemma.

    This strategy aims to balance cooperation and self-interest by adapting to
    the collective outcome of previous rounds. It initiates cooperation,
    rewards successful group efforts, and punishes collective failures.

    Key principles:
    - Tit-for-Tat with a Graded Response and Threshold Focus.
    - Cooperate in the first round to initiate the possibility of high payoffs.
    - Defect in the last round as there is no future incentive for cooperation.
    - If the cooperation threshold 'm' was met in the previous round:
        - Cooperate if the player themselves cooperated previously.
        - Defect if the player themselves defected previously (punishing individual defection within success).
    - If the cooperation threshold 'm' was NOT met in the previous round:
        - Defect, regardless of the player's own previous action (punishing collective failure).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Cautious Collaborator strategy.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on game history.

        Args:
            state: The current game state, including the round number.
            history: Historical data of actions and payoffs, or None if it's the first round.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == self.r:
            return Action.D
        previous_round_idx = current_round_number - 1
        my_previous_action_int = int(history.my_actions[previous_round_idx])
        cooperation_count_previous_round = np.sum(history.opponent_actions[previous_round_idx, :])
        total_cooperators_previous_round = cooperation_count_previous_round + my_previous_action_int
        if total_cooperators_previous_round >= self.m:
            if my_previous_action_int == Action.C.value:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_43(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) Strategy.

    ATC dynamically adjusts player behavior based on observed cooperation levels
    and game parameters to promote cooperation while being pragmatically cautious.
    It aims to hit the cooperation threshold (m) when achievable or necessary,
    but defects if cooperation appears to be failing or the risk of exploitation is high.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.risk_sensitivity = 0.5
        self.coop_incentive = 0.7
        self.last_round_caution = 0.7
        self.target_coop_rate = self.m / self.n
        self.near_failure_rate = (self.m - 1) / self.n if self.m > 1 else 0.0

    def _calculate_defect_risk(self, prob_coop: float) -> float:
        """
        Calculates a risk factor for defecting.
        A higher return value means higher risk of being exploited if cooperating.
        Returns 0.0 if threshold is met or exceeded.
        """
        if prob_coop >= self.target_coop_rate:
            return 0.0
        if prob_coop < self.near_failure_rate:
            risk = 1.0 - prob_coop / self.near_failure_rate
            return max(0.0, risk)
        else:
            if self.target_coop_rate == self.near_failure_rate:
                return 1.0 if prob_coop < self.target_coop_rate else 0.0
            risk = 1.0 - (prob_coop - self.near_failure_rate) / (self.target_coop_rate - self.near_failure_rate)
            return max(0.0, risk)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        my_actions_prev = history.my_actions
        opp_actions_prev = history.opponent_actions
        num_cooperators_prev = np.sum(my_actions_prev[-1]) + np.sum(opp_actions_prev[-1])
        prob_coop_prev = num_cooperators_prev / self.n
        if current_round == self.r - 1:
            last_round_caution_factor = 1.0 - self.last_round_caution
            if num_cooperators_prev >= self.m:
                defect_risk = self._calculate_defect_risk(prob_coop_prev)
                coop_prob = (1.0 - defect_risk * self.risk_sensitivity) * last_round_caution_factor
                if random.random() < coop_prob:
                    return Action.C
                else:
                    return Action.D
            elif prob_coop_prev < self.near_failure_rate:
                defect_prob = 1.0 - prob_coop_prev / self.target_coop_rate * self.coop_incentive
                defect_prob = max(0.0, defect_prob)
                if random.random() < defect_prob * last_round_caution_factor:
                    return Action.D
                else:
                    return Action.C
            else:
                coop_prob = prob_coop_prev / self.target_coop_rate * self.coop_incentive
                if random.random() < coop_prob * last_round_caution_factor:
                    return Action.C
                else:
                    return Action.D
        defect_risk = self._calculate_defect_risk(prob_coop_prev)
        if num_cooperators_prev >= self.m:
            coop_prob = 1.0 - defect_risk * self.risk_sensitivity
            if random.random() < coop_prob:
                return Action.C
            else:
                return Action.D
        elif prob_coop_prev < self.near_failure_rate:
            defect_prob = 1.0 - prob_coop_prev / self.target_coop_rate * self.coop_incentive
            defect_prob = max(0.0, defect_prob)
            if random.random() < defect_prob:
                return Action.D
            else:
                return Action.C
        else:
            coop_prob = prob_coop_prev / self.target_coop_rate * self.coop_incentive
            if random.random() < coop_prob:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_44(BaseStrategy):
    """
    The "Cautious Benefactor" strategy for the Collective Risk Dilemma.
    This strategy aims to balance collective reward with self-preservation.
    It cooperates when the observed behavior suggests the cooperation threshold 'm'
    is likely to be met, and defects otherwise. It adapts its belief about group
    cooperation based on recent outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Cautious Benefactor strategy.

        Args:
            game_description: The description of the Collective Risk Dilemma game.
        """
        self.game_description = game_description
        self.cooperative_score = float(game_description.n_players)
        self.alpha = 0.1
        self.beta = 0.2

    def _update_cooperative_score(self, completed_rounds_actions: NDArray[np.bool_], current_round_num: int):
        """
        Updates the internal cooperative_score based on observed actions.

        Args:
            completed_rounds_actions: A numpy array containing the actions of all players
                                      in the completed rounds. Shape: [round, player].
            current_round_num: The current round number (1-indexed).
        """
        if current_round_num == 0:
            return
        last_round_actions = completed_rounds_actions[-1, :]
        cooperators_in_last_round = np.sum(last_round_actions)
        m = self.game_description.m
        n = self.game_description.n_players
        if cooperators_in_last_round >= m:
            self.cooperative_score += (n - self.cooperative_score) * self.alpha
        else:
            self.cooperative_score -= self.cooperative_score * self.beta
        if cooperators_in_last_round < m / 2:
            self.cooperative_score = max(0.0, self.cooperative_score - (m - cooperators_in_last_round))
        self.cooperative_score = min(float(n), max(0.0, self.cooperative_score))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs from previous rounds.
                     None for the first round.

        Returns:
            The action to take: Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_number = state.round_number
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            self.cooperative_score = float(n)
            return Action.C
        all_past_actions = np.hstack((history.my_actions[:, np.newaxis], history.opponent_actions))
        self._update_cooperative_score(all_past_actions, current_round_number)
        if current_round_number == r:
            if self.cooperative_score >= m:
                return Action.C
            else:
                return Action.D
        if self.cooperative_score >= m:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_45(BaseStrategy):
    """
    The "Threshold Tracker" strategy is designed to be a robust cooperative player
    in the Collective Risk Dilemma. It monitors past cooperation levels to decide
    whether to cooperate or defect. It prioritizes cooperation when the collective
    effort is close to or has historically met the cooperation threshold 'm',
    while defecting to secure its own endowment when collective effort is insufficient.
    It has specific rules for the first and last rounds to adapt to the game's progression.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round_num = state.round_number
        if history is None or current_round_num == 0:
            return Action.C
        completed_rounds = current_round_num
        cooperators_last_round = 0
        if completed_rounds > 0:
            my_action_last_round = self.game_description.n_players
            if len(history.my_actions) > 0:
                my_action_last_round = history.my_actions[completed_rounds - 1]
            opponent_actions_last_round = np.zeros(n - 1, dtype=np.bool_)
            if completed_rounds > 0 and history.opponent_actions.shape[0] >= completed_rounds:
                opponent_actions_last_round = history.opponent_actions[completed_rounds - 1, :]
            cooperators_last_round = int(my_action_last_round) + int(np.sum(opponent_actions_last_round))
        total_cooperators_previous_rounds = 0
        if completed_rounds > 0:
            total_cooperators_previous_rounds += np.sum(history.my_actions[:completed_rounds])
            if completed_rounds > 0 and history.opponent_actions.shape[0] >= completed_rounds:
                total_cooperators_previous_rounds += np.sum(history.opponent_actions[:completed_rounds, :])
        average_cooperation_rate = 0
        if completed_rounds > 0:
            average_cooperation_rate = total_cooperators_previous_rounds / (completed_rounds * n)
        cooperative_threshold_decision = (m - 1) / n
        if current_round_num == r - 1:
            if average_cooperation_rate >= cooperative_threshold_decision:
                return Action.C
            elif cooperators_last_round >= m - 1 and cooperators_last_round < m:
                return Action.C
            else:
                return Action.D
        if average_cooperation_rate >= cooperative_threshold_decision:
            return Action.C
        if cooperators_last_round >= m - 1 and cooperators_last_round < m:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_46(BaseStrategy):
    """
    Cooperative Strategy: "Reciprocal Tit-for-Tat with Escalation"

    This strategy aims to foster cooperation by signaling a willingness to contribute,
    while defensively reacting to perceived defection. It balances the desire for the
    high payoff from collective cooperation with the risk of being exploited.

    Decision Rule for Round `t` (where `t > 1`):
    1. Cooperate if the collective cooperation in the previous round was sufficient
       to meet the threshold 'm', AND the player was part of that successful cooperation.
    2. Otherwise, defect.

    Round 1 Handling: Default Action is Cooperate (C).
    Last Round Handling: Decision rule remains the same.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        prev_round_idx = history.round_number - 1
        cooperators_in_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + (1 if history.my_actions[prev_round_idx] else 0)
        i_cooperated_in_prev_round = history.my_actions[prev_round_idx]
        if cooperators_in_prev_round >= self.m and i_cooperated_in_prev_round:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_47(BaseStrategy):
    """
    The "Cautious Benefactor" strategy.
    This strategy cooperates initially and continues to cooperate as long as
    the collective threshold 'm' is met or if cooperation has failed for
    a limited number of consecutive rounds. It defects if cooperation
    has consistently failed beyond a certain threshold, to avoid individual ruin.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.defection_punishment_counter = 0
        self.max_defection_punishment_counter = self.n_players - self.m + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_number = history.round_number
        previous_round_cooperators = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if current_round_number == self.n_rounds:
            if previous_round_cooperators >= self.m:
                return Action.C
            else:
                return Action.D
        if previous_round_cooperators >= self.m:
            self.defection_punishment_counter = 0
            return Action.C
        elif self.defection_punishment_counter < self.max_defection_punishment_counter:
            self.defection_punishment_counter += 1
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_48(BaseStrategy):
    """
    The "Threshold Trust" strategy is a cooperative strategy for the Collective Risk Dilemma.
    It aims to foster cooperation by dynamically assessing the likelihood of successful
    cooperation based on past outcomes. It starts by cooperating, then adjusts its
    cooperation probability based on whether the cooperation threshold was met and
    whether it contributed to it. It defects in the last round to maximize final payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.initial_cooperation_probability = self.game_description.m / self.game_description.n_players
        self.adjustment_factor_positive = 0.1
        self.adjustment_factor_slight = 0.05
        self.adjustment_factor_negative = 0.15
        self.adjustment_factor_mild = 0.05
        self.threshold_to_cooperate = 0.6
        self.cooperation_probability = self.initial_cooperation_probability
        self.my_player_index = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.my_player_index == -1:
            if history is not None:
                self.my_player_index = len(history.my_actions)
                pass
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.C
        last_round_cooperator_count = np.sum(history.opponent_actions[-1, :])
        my_action_last_round = history.my_actions[-1]
        total_cooperators_last_round = last_round_cooperator_count + (1 if my_action_last_round else 0)
        if current_round == self.game_description.n_rounds - 1:
            return Action.D
        if total_cooperators_last_round >= self.game_description.m:
            if my_action_last_round:
                self.cooperation_probability += self.adjustment_factor_positive
            else:
                self.cooperation_probability += self.adjustment_factor_slight
        elif my_action_last_round:
            self.cooperation_probability -= self.adjustment_factor_negative
        else:
            self.cooperation_probability -= self.adjustment_factor_mild
        self.cooperation_probability = max(0.0, min(1.0, self.cooperation_probability))
        if self.cooperation_probability >= self.threshold_to_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_49(BaseStrategy):
    """
    Strategy: "Conditional Commitment with Gradual Trust"

    This strategy is cooperative by default but cautiously so. It gradually builds
    trust based on observed behavior in previous rounds. It commits to cooperation
    as long as the collective is leaning towards it, but pulls back if defection
    becomes too prevalent. It introduces a "gray zone" to prevent immediate
    collapse and has specific logic for the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.cooperate_threshold = self.m / self.n
        self.defect_threshold = (self.m - 1) / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     If None, it signifies the first round (round_number = 0).

        Returns:
            Action.C if the strategy decides to Cooperate, Action.D if it decides to Defect.
        """
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == self.r - 1:
            return Action.D
        previous_round_cooperators_count = np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        observed_coop_rate_prev_round = previous_round_cooperators_count / self.n
        if observed_coop_rate_prev_round >= self.cooperate_threshold:
            return Action.C
        elif observed_coop_rate_prev_round <= self.defect_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_50(BaseStrategy):
    """
    Conditional Escalation with Retaliation strategy for the Collective Risk Dilemma.

    This strategy aims to achieve the highest possible cumulative payoff by
    fostering cooperation when beneficial and safe, and retaliating against
    widespread defection. It is conditionally cooperative and adaptive.

    Decision Logic:
    - Round 1: Always Cooperate (C).
    - Subsequent Rounds:
        - If the cooperation threshold (m) was met in the previous round: Cooperate (C).
        - If the threshold was not met, but this player cooperated and was the marginal
          cooperator (i.e., threshold would have been met if they had defected): Cooperate (C).
        - If the threshold was not met and this player defected in the previous round: Defect (D).
        - If the threshold was not met and this player cooperated but was not the marginal
          cooperator (i.e., the threshold was still missed despite their cooperation): Defect (D).
    - Last Round: Always Defect (D) to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        my_action_prev_round = history.my_actions[current_round - 1]
        cooperators_prev_round_total = np.sum(history.opponent_actions[current_round - 1, :])
        if my_action_prev_round:
            cooperators_prev_round_total += 1
        if cooperators_prev_round_total >= self.m:
            return Action.C
        else:
            if my_action_prev_round and cooperators_prev_round_total + 1 >= self.m:
                cooperators_excluding_self_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
                if my_action_prev_round and cooperators_excluding_self_prev_round + 1 >= self.m and (cooperators_excluding_self_prev_round < self.m):
                    return Action.C
            if not my_action_prev_round:
                return Action.D
            if my_action_prev_round:
                cooperators_excluding_self_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
                if cooperators_excluding_self_prev_round < self.m and cooperators_excluding_self_prev_round + 1 < self.m:
                    return Action.D
            if my_action_prev_round:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_51(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) Strategy.

    This strategy adapts its cooperation tendency based on the observed success
    rate of the group in reaching the cooperation threshold (m). It starts with
    a cautious yet potentially optimistic approach in the first round, and
    adjusts its probability of cooperating based on whether the threshold was
    met in the previous round. It also defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_willingness = 0.0
        self.previous_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None:
            if self.k > 1.5 and self.n > self.m * 1.5:
                self.cooperation_willingness = 1.0
            else:
                self.cooperation_willingness = 0.5
            if random.random() < self.cooperation_willingness:
                self.previous_action = Action.C
                return Action.C
            else:
                self.previous_action = Action.D
                return Action.D
        my_last_action = self.previous_action.value
        if self.n > 1:
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        else:
            opponent_cooperators_prev_round = 0
        cooperators_prev_round = opponent_cooperators_prev_round + my_last_action
        cooperation_percentage_prev_round = cooperators_prev_round / self.n
        target_cooperation_percentage = self.m / self.n
        if cooperation_percentage_prev_round >= target_cooperation_percentage:
            self.cooperation_willingness = min(1.0, self.cooperation_willingness + 0.1)
        else:
            self.cooperation_willingness = max(0.0, self.cooperation_willingness - 0.2)
        if random.random() < self.cooperation_willingness:
            self.previous_action = Action.C
            return Action.C
        else:
            self.previous_action = Action.D
            return Action.D

class Strategy_COOPERATIVE_52(BaseStrategy):
    """
    "Conditional Reciprocity with Trust Building" strategy for the Collective Risk Dilemma.

    This strategy prioritizes cooperation by defaulting to it, especially in early
    rounds and when the cooperation threshold has been met. It retaliates against
    widespread defection but offers a path back to cooperation through persistence
    if the player has been a consistent cooperator. The endgame focuses on
    maximizing immediate payoff based on the likelihood of success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.initial_trust_building_period = max(2, math.floor(self.n_rounds / 10))
        self.my_cooperation_count = 0
        self.last_round_cooperation_met = False
        self.last_round_observed_cooperation_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is not None and current_round > 0:
            previous_round_actions = history.opponent_actions[current_round - 1]
            num_cooperators_last_round = np.sum(previous_round_actions) + (1 if history.my_actions[current_round - 1] else 0)
            self.last_round_cooperation_met = num_cooperators_last_round >= self.m
            self.last_round_observed_cooperation_rate = num_cooperators_last_round / self.n_players
            if history.my_actions[current_round - 1]:
                self.my_cooperation_count += 1
        if current_round < self.initial_trust_building_period:
            return Action.C
        elif current_round == self.n_rounds:
            if self.last_round_cooperation_met or self.last_round_observed_cooperation_rate > (self.m - 1) / self.n_players:
                return Action.C
            else:
                return Action.D
        elif self.last_round_cooperation_met:
            return Action.C
        elif self.last_round_observed_cooperation_rate < self.m / self.n_players:
            return Action.D
        else:
            rounds_played_so_far = current_round
            my_cooperation_proportion = self.my_cooperation_count / rounds_played_so_far if rounds_played_so_far > 0 else 1.0
            if my_cooperation_proportion > 0.5:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_53(BaseStrategy):
    """
    The Threshold Guardian strategy is a cooperative and adaptive approach to the
    Collective Risk Dilemma. It defaults to cooperation, but monitors the observed
    cooperation levels from the previous round. It is willing to cooperate
    even if one person short of the threshold (m-1 cooperators observed) to
    push for collective reward. If cooperation levels are too low, it defects
    to conserve resources and signal dissatisfaction.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.observed_cooperators_last_round = 0
        self.my_contribution_last_round = None
        self.cooperation_opportunity_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.observed_cooperators_last_round = 0
            self.my_contribution_last_round = None
            self.cooperation_opportunity_last_round = False
            return Action.C
        current_round_number = state.round_number
        if current_round_number > 0:
            last_round_idx = current_round_number - 1
            if history.opponent_actions.shape[0] > last_round_idx:
                observed_cooperators_this_round = np.sum(history.opponent_actions[last_round_idx, :])
                if self.my_contribution_last_round == Action.C:
                    observed_cooperators_this_round += 1
                self.observed_cooperators_last_round = observed_cooperators_this_round
                self.my_contribution_last_round = history.my_actions[last_round_idx]
                if observed_cooperators_this_round >= self.game_description.m:
                    self.cooperation_opportunity_last_round = True
                else:
                    self.cooperation_opportunity_last_round = False
            else:
                self.observed_cooperators_last_round = 0
                self.my_contribution_last_round = None
                self.cooperation_opportunity_last_round = False
        else:
            self.observed_cooperators_last_round = 0
            self.my_contribution_last_round = None
            self.cooperation_opportunity_last_round = False
        my_action = Action.C
        if self.cooperation_opportunity_last_round:
            my_action = Action.C
        elif self.observed_cooperators_last_round >= self.game_description.m - 1:
            my_action = Action.C
        else:
            my_action = Action.D
        return my_action

class Strategy_COOPERATIVE_54(BaseStrategy):
    """
    "Reciprocity with Threshold Enforcement" strategy for the Collective Risk Dilemma.
    This strategy cooperates by default, retaliates against widespread defection
    that prevents the threshold from being met, and rewards sustained cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.lookback_period = min(5, self.r)
        self.cooperation_buffer = 1

    def _count_persistent_cooperators(self, history: PlayerHistory) -> int:
        """
        Counts players who have consistently cooperated for the lookback period,
        and also cooperated in the *immediately preceding* round.
        """
        if history.round_number < self.lookback_period:
            return 0
        recent_rounds_actions = history.opponent_actions[history.round_number - self.lookback_period:history.round_number]
        last_round_actions = history.opponent_actions[history.round_number - 1]
        persistent_count = 0
        for player_idx in range(self.n - 1):
            player_recent_history = recent_rounds_actions[:, player_idx]
            if np.all(player_recent_history):
                if last_round_actions[player_idx]:
                    persistent_count += 1
        return persistent_count

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        previous_round_actions = history.opponent_actions[current_round - 1]
        cooperators_last_round = np.sum(previous_round_actions)
        if cooperators_last_round >= self.m:
            persistent_cooperators = self._count_persistent_cooperators(history)
            if persistent_cooperators >= self.m:
                return Action.C
            else:
                return Action.C
        else:
            defectors_last_round = self.n - cooperators_last_round
            if defectors_last_round > self.n - self.m + self.cooperation_buffer:
                return Action.D
            else:
                return Action.C

class Strategy_COOPERATIVE_55(BaseStrategy):
    """
    Adaptive Threshold Cooperation strategy for the Collective Risk Dilemma.

    This strategy prioritizes cooperation to achieve the collective reward but
    adapts its behavior based on historical outcomes. It defaults to cooperation,
    punishes failed cooperative attempts by defecting temporarily, and continues
    cooperation when the threshold is met. It always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_actions_history = []
        self.cooperator_counts_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is not None:
            previous_round_my_action = self.my_actions_history[-1]
            previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            total_cooperators_previous_round = previous_round_opponent_cooperators + (1 if previous_round_my_action == Action.C else 0)
            self.cooperator_counts_history.append(total_cooperators_previous_round)
        else:
            pass
        if current_round == r:
            return Action.D
        if current_round == 0:
            self.my_actions_history.append(Action.C)
            return Action.C
        previous_round_action = self.my_actions_history[-1]
        previous_round_cooperators = self.cooperator_counts_history[-1]
        if previous_round_cooperators >= m:
            current_action = Action.C
        elif previous_round_action == Action.C:
            current_action = Action.D
        else:
            current_action = Action.D
        self.my_actions_history.append(current_action)
        return current_action

class Strategy_COOPERATIVE_56(BaseStrategy):
    """
    The Adaptive Reciprocity Strategy (ARS) aims to achieve high cumulative payoffs
    by balancing individual gain with collective success. It is proactive in
    initiating cooperation, retaliatory yet forgiving, and prioritizes reaching
    the cooperation threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        prev_round_idx = current_round - 1
        all_prev_actions = np.append([history.my_actions[prev_round_idx]], history.opponent_actions[prev_round_idx, :])
        cooperators_last_round = np.sum(all_prev_actions)
        defectors_last_round = self.n - cooperators_last_round
        my_last_payoff = history.my_payoffs[prev_round_idx]
        my_last_action_was_cooperate = history.my_actions[prev_round_idx]
        player_i_cooperated_last_round = my_last_action_was_cooperate
        threshold_met_last_round = cooperators_last_round >= self.m
        if threshold_met_last_round:
            return Action.C
        elif not player_i_cooperated_last_round:
            if defectors_last_round <= self.n - self.m:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_57(BaseStrategy):
    """
    Adaptive Risk-Averse Cooperation strategy for the Collective Risk Dilemma.
    This strategy cooperates in the first round and then assesses the likelihood
    of reaching the cooperation threshold (m) based on historical cooperation.
    It cooperates if the threshold is likely to be met and there's positive momentum,
    defects if the threshold is unlikely or momentum is lost, and always defects
    in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.ever_cooperated = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None:
            return Action.C
        num_cooperators_prev_round = int(np.sum(history.opponent_actions[-1, :]))
        if history.my_actions[-1] == Action.C:
            self.ever_cooperated[0] = True
            pass
        for player_idx in range(n - 1):
            if history.opponent_actions[-1, player_idx] == Action.C:
                self.ever_cooperated[player_idx + 1] = True
        estimated_potential_cooperators = 0
        if history.my_actions[-1] == Action.C:
            estimated_potential_cooperators += 1
        for player_idx in range(n - 1):
            if history.opponent_actions[-1, player_idx] == Action.C:
                estimated_potential_cooperators += 1
        if current_round > 0 and history.my_actions[current_round - 1] == Action.C:
            self.ever_cooperated[0] = True
        if current_round > 0:
            for player_idx in range(n - 1):
                if history.opponent_actions[current_round - 1, player_idx] == Action.C:
                    self.ever_cooperated[player_idx + 1] = True
        estimated_potential_cooperators = 0
        if self.ever_cooperated[0]:
            estimated_potential_cooperators += 1
        for player_idx in range(n - 1):
            if self.ever_cooperated[player_idx + 1]:
                estimated_potential_cooperators += 1
        cooperate_decision = False
        if current_round < r:
            if estimated_potential_cooperators + 1 >= m:
                if num_cooperators_prev_round >= m - 1:
                    cooperate_decision = True
        if cooperate_decision:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_58(BaseStrategy):
    """
    Conditional Hope Strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by being a conditional cooperator.
    It attempts to identify and leverage opportunities for collective gain while
    minimizing the risk of being exploited by defectors. It cooperates initially,
    and then adapts based on the collective behavior observed.

    Decision Rules:
    1. Initial Rounds: Cooperate to build trust and gather information.
    2. Mid-Rounds: Adapt based on past success, cooperation trend, and proximity to the threshold 'm'.
    3. Final Rounds: Assess historical success rate and cooperate only if consistently successful, otherwise defect to secure private endowment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.initial_phase_rounds = min(math.floor(self.r / 3), 5)
        self.final_phase_rounds = min(math.floor(self.r / 3), 5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if history is None:
            return Action.C
        num_prev_rounds = history.round_number
        my_actions = history.my_actions
        num_cooperators_all_rounds = np.sum(history.opponent_actions, axis=1) + my_actions.astype(int)
        if current_round_num < self.initial_phase_rounds:
            return Action.C
        if current_round_num >= self.r - self.final_phase_rounds:
            if num_prev_rounds > 0:
                rounds_to_analyze_count = min(num_prev_rounds, self.final_phase_rounds)
                start_index = num_prev_rounds - rounds_to_analyze_count
                successful_rounds_in_final_phase = 0
                for i in range(start_index, num_prev_rounds):
                    if num_cooperators_all_rounds[i] >= self.m:
                        successful_rounds_in_final_phase += 1
                avg_success_rate_last_few = successful_rounds_in_final_phase / rounds_to_analyze_count
                if avg_success_rate_last_few >= 0.7:
                    return Action.C
                else:
                    return Action.D
            else:
                pass
        if num_prev_rounds >= 1:
            n_c_t_minus_1 = num_cooperators_all_rounds[-1]
            was_successful_t_minus_1 = n_c_t_minus_1 >= self.m
            if was_successful_t_minus_1:
                return Action.C
            else:
                cooperation_trend_up = False
                proximity_to_m = False
                if num_prev_rounds >= 2:
                    n_c_t_minus_2 = num_cooperators_all_rounds[-2]
                    cooperation_trend_up = n_c_t_minus_1 > n_c_t_minus_2
                proximity_to_m = n_c_t_minus_1 >= self.m - 2
                if cooperation_trend_up or proximity_to_m:
                    return Action.C
                else:
                    return Action.D
        return Action.C

class Strategy_COOPERATIVE_59(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Monitoring Strategy.

    This strategy aims to cooperate when the collective threshold 'm' is likely
    to be met, and defect otherwise. It uses historical data to inform its
    decisions, with special handling for the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperate_threshold_prob = 0.7
        self.recent_history_window = 5

    def _count_cooperators_in_round(self, round_idx: int, history: PlayerHistory) -> int:
        """Counts the total number of cooperators in a given round."""
        my_cooperation = 1 if self.my_actions[round_idx] else 0
        opponent_cooperation = np.sum(history.opponent_actions[round_idx, :])
        return my_cooperation + opponent_cooperation

    def _calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        """Calculates the average cooperation rate over a recent window of rounds."""
        if history is None or history.round_number == 0:
            return 0.0
        start_round = max(0, history.round_number - self.recent_history_window)
        total_cooperators = 0
        total_decisions = 0
        total_cooperators += np.sum(self.my_actions[start_round:])
        total_decisions += history.round_number - start_round
        if history.opponent_actions.shape[0] > 0:
            total_cooperators += np.sum(history.opponent_actions[start_round:, :])
            total_decisions += (history.round_number - start_round) * self.n_players_minus_one
        if total_decisions == 0:
            return 0.0
        return total_cooperators / total_decisions

    @property
    def my_actions(self):
        return self.history.my_actions if self.history else np.array([], dtype=np.bool_)

    @property
    def n_players_minus_one(self):
        return self.n_players - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.history = history
        if history is None:
            return Action.C
        if state.round_number == self.n_rounds:
            return Action.D
        last_round_idx = state.round_number - 1
        num_cooperators_last_round = self._count_cooperators_in_round(last_round_idx, history)
        if num_cooperators_last_round >= self.m:
            return Action.C
        else:
            avg_cooperation_rate = self._calculate_average_cooperation_rate(history)
            if num_cooperators_last_round >= self.m - 1 and avg_cooperation_rate >= 0.5:
                return Action.C
            elif avg_cooperation_rate >= self.cooperate_threshold_prob:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_60(BaseStrategy):
    """
    Threshold-Adaptive Cooperation strategy for the Collective Risk Dilemma.

    This strategy aims to adapt its cooperation level based on the observed
    success of achieving the cooperation threshold in previous rounds.
    It starts cooperatively and adjusts its probability of cooperating
    to encourage collective success while mitigating risks of exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
            if cooperators_last_round >= self.m:
                return Action.C
            else:
                return Action.D
        else:
            cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
            observed_cooperation_rate = cooperators_prev_round / self.n
            target_cooperation_ratio = self.m / self.n
            if observed_cooperation_rate >= target_cooperation_ratio:
                p_cooperate = 1.0
            else:
                p_cooperate = (target_cooperation_ratio + (1 - observed_cooperation_rate)) / 2.0
                p_cooperate = max(0.0, min(1.0, p_cooperate))
            if random.random() < p_cooperate:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_61(BaseStrategy):
    """
    The "Cautious Threshold Seeker" strategy.
    This strategy aims to achieve cooperation by conditionally cooperating.
    It starts with a cooperative inclination, but is prepared to defect if
    cooperation appears to be failing or if it's the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == r:
            return Action.D
        if history is None:
            return Action.C
        cooperators_in_previous_round = 0
        if history.round_number > 0:
            my_last_action = Action.C if history.my_actions[-1] else Action.D
            cooperators_in_previous_round = np.sum(history.opponent_actions[-1, :])
            if my_last_action == Action.C:
                cooperators_in_previous_round += 1
        if cooperators_in_previous_round >= m:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_62(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for the Collective Risk Dilemma.

    ATC aims to facilitate cooperation by leveraging the collective reward (k)
    while mitigating the risk of individual defection. It dynamically adjusts
    its cooperation threshold based on observed player behavior, aiming to
    achieve the minimum cooperative threshold (m) without exposing itself to
    excessive exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATC strategy.

        Args:
            game_description: The game's parameters.
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs from previous rounds.
                     None if it's the first round.

        Returns:
            The action to take: Action.C (Cooperate) or Action.D (Defect).
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        my_actions_prev = self.history_to_bool_array(history.my_actions)
        opponent_actions_prev = self.history_to_bool_array(history.opponent_actions)
        all_actions_prev = np.copy(opponent_actions_prev[-1])
        my_last_action_bool = my_actions_prev[-1]
        num_cooperators_previous_round = np.sum(opponent_actions_prev[-1]) + my_last_action_bool
        if num_cooperators_previous_round >= self.m - 1:
            return Action.C
        if num_cooperators_previous_round == self.m - 2 and my_last_action_bool:
            return Action.C
        if current_round == self.n_rounds - 1 and num_cooperators_previous_round < self.m:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        return Action.D

    def history_to_bool_array(self, actions_or_payoffs: NDArray) -> NDArray[np.bool_]:
        """Converts an array of booleans to boolean type."""
        return actions_or_payoffs.astype(np.bool_)

class Strategy_COOPERATIVE_63(BaseStrategy):
    """
    Cooperative Threshold Strategy (CTS) for the Collective Risk Dilemma.
    This strategy aims to initiate and sustain cooperation by leading when
    conditions are favorable and punishing defection by withdrawing cooperation.
    It adapts based on the observed cooperation rate in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if history is None:
            return Action.C
        if current_round_t == self.r - 1:
            return Action.D
        previous_round_index = current_round_t - 1
        my_cooperated_previous = self.game_description.m > 0 and history.my_actions[previous_round_index]
        num_opponent_cooperators_previous = np.sum(history.opponent_actions[previous_round_index, :])
        num_cooperators_previous_round = num_opponent_cooperators_previous + my_cooperated_previous
        if num_cooperators_previous_round >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_64(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) Strategy.

    This strategy aims to maximize long-term payoff in the Collective Risk Dilemma
    by balancing individual gain and collective reward. It starts cooperatively
    and adapts its threshold for cooperation based on the observed success of
    previous rounds. It defects if the group consistently fails to meet the
    cooperation threshold and in the final round to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATC strategy.

        Args:
            game_description: An object containing game parameters (n, m, r, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_proportion = self.m / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: Player history from previous rounds. None if it's the first round.

        Returns:
            Action.C for Cooperate, Action.D for Defect.
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        previous_round_index = current_round - 1
        my_previous_action = history.my_actions[previous_round_index]
        opponent_previous_actions = history.opponent_actions[previous_round_index, :]
        cooperators_in_previous_round = int(my_previous_action) + np.sum(opponent_previous_actions)
        proportion_cooperators = cooperators_in_previous_round / self.n
        if proportion_cooperators >= self.cooperation_threshold_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_65(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) Strategy.
    This strategy dynamically adjusts its cooperation threshold based on past cooperation
    successes and failures, while incorporating a forgiveness mechanism. It starts
    cooperatively and becomes more cautious if cooperation falters, and more
    optimistic if it succeeds. It also includes special logic for the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATC strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.coop_boost_factor = 0.05
        self.caution_increment_factor = 0.03
        self.forgiveness_window = 2
        self.forgiveness_threshold_multiplier = 0.9
        self.dynamic_cooperation_threshold = self.m / self.n
        self.forgiveness_counter = 0
        self.total_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A history of past actions and payoffs for all players.
                     None for the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        current_round_number = state.round_number
        if history is None:
            return Action.C
        my_prev_action = int(history.my_actions[-1])
        opp_prev_actions = history.opponent_actions[-1]
        cooperators_in_prev_round = my_prev_action + int(np.sum(opp_prev_actions))
        observed_cooperation_rate = cooperators_in_prev_round / self.n
        if current_round_number == self.r - 1:
            guaranteed_min_total_payoff = self.r * 1.0
            if self.total_payoff < guaranteed_min_total_payoff - 1.0:
                return Action.D
            else:
                return Action.C
        previous_round_met_m = cooperators_in_prev_round >= self.m
        if previous_round_met_m:
            boost_amount = self.coop_boost_factor * self.n / self.n
            self.dynamic_cooperation_threshold = max(self.m / self.n * 0.9, self.dynamic_cooperation_threshold - boost_amount)
        else:
            caution_amount = self.caution_increment_factor * self.n / self.n
            self.dynamic_cooperation_threshold = min(1.0, self.dynamic_cooperation_threshold + caution_amount)
        if not previous_round_met_m:
            self.forgiveness_counter += 1
        else:
            self.forgiveness_counter = 0
        cooperate = False
        if observed_cooperation_rate >= self.dynamic_cooperation_threshold:
            cooperate = True
            if self.forgiveness_counter > 0:
                self.forgiveness_counter = 0
        elif self.forgiveness_counter > 0 and self.forgiveness_counter <= self.forgiveness_window:
            if observed_cooperation_rate >= self.m / self.n * self.forgiveness_threshold_multiplier:
                cooperate = True
        if cooperate:
            chosen_action = Action.C
        else:
            if self.forgiveness_counter == 0:
                self.forgiveness_counter = 1
            chosen_action = Action.D
        return chosen_action

class Strategy_COOPERATIVE_66(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Reward and Punish"

    This strategy aims to foster cooperation by rewarding past cooperation and punishing defection,
    while acknowledging the inherent risk of the dilemma. It's designed to be robust and adaptive,
    adjusting its behavior based on the observed cooperation levels in previous rounds.

    Decision Rules:
    - Round 1: Always Cooperate (C).
    - If cooperation threshold (m) was met or exceeded in the previous round: Cooperate (C) with probability 1.
    - If cooperation threshold was not met:
        - If only 1 player was missing for cooperation (deficit = 1): Cooperate (C) with probability (n - m + 1) / n.
        - If more than 1 player was missing (deficit > 1): Cooperate (C) with probability (number of cooperators last round) / (n - 1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.cooperation_threshold_met_last_round = False
        self.number_of_cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.cooperation_threshold_met_last_round = False
            self.number_of_cooperators_last_round = 0
            return Action.C
        last_round_index = history.round_number - 1
        cooperators_this_round = 0
        my_last_action = Action.C if history.my_actions[last_round_index] else Action.D
        cooperators_this_round += 1 if my_last_action == Action.C else 0
        if self.n > 1:
            opponent_actions_last_round = history.opponent_actions[last_round_index, :]
            cooperators_this_round += np.sum(opponent_actions_last_round)
        self.number_of_cooperators_last_round = cooperators_this_round
        if cooperators_this_round >= self.m:
            self.cooperation_threshold_met_last_round = True
        else:
            self.cooperation_threshold_met_last_round = False
        if self.cooperation_threshold_met_last_round:
            return Action.C
        else:
            cooperation_deficit = self.m - self.number_of_cooperators_last_round
            if cooperation_deficit == 1:
                probability_to_cooperate = (self.n - self.m + 1.0) / self.n
                if random.random() < probability_to_cooperate:
                    return Action.C
                else:
                    return Action.D
            elif self.n - 1 > 0:
                probability_to_cooperate = self.number_of_cooperators_last_round / (self.n - 1.0)
                if random.random() < probability_to_cooperate:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_COOPERATIVE_67(BaseStrategy):
    """
    The "Threshold Seeker" strategy.
    This strategy aims to achieve the cooperative outcome by cooperating
    when its contribution is necessary to meet the threshold, or when
    a strong trend towards cooperation is observed. It defects to protect
    itself from exploitation when cooperation seems unlikely or is not
    reciprocated.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0 or current_round == 1:
            return Action.C
        if history is None:
            return Action.C
        previous_round_idx = current_round - 1
        my_prev_action = 1 if self.my_actions_from_history(history)[previous_round_idx] else 0
        opponent_prev_actions = self.opponent_actions_from_history(history)[previous_round_idx]
        num_cooperators_previous_round = np.sum(opponent_prev_actions) + my_prev_action
        if current_round == self.r - 1:
            if num_cooperators_previous_round >= self.m:
                return Action.C
            else:
                return Action.D
        if self.m - 1 <= num_cooperators_previous_round < self.m:
            return Action.C
        if num_cooperators_previous_round >= self.m and num_cooperators_previous_round / self.n >= 0.5:
            return Action.C
        if num_cooperators_previous_round < self.m - 1:
            return Action.D
        num_defectors_previous_round = self.n - num_cooperators_previous_round
        if num_cooperators_previous_round < self.m and num_defectors_previous_round / self.n >= 0.5:
            return Action.D
        return Action.D

    def my_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Helper to combine my previous actions with history."""
        return history.my_actions

    def opponent_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Helper to access opponent actions from history."""
        return history.opponent_actions

class Strategy_COOPERATIVE_68(BaseStrategy):
    """
    Adaptive Reciprocity Strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by mirroring past group behavior,
    rewarding successful cooperation, and defecting conditionally when cooperation fails.
    It starts by cooperating and continues to do so if the cooperation threshold
    was met in the previous round. If the threshold was not met, it checks if it
    cooperated in the previous round. If it did and the threshold failed, it defects
    as a punishment for the lack of collective success. If it defected in the
    previous round (or if the threshold was met in the previous round), it resumes
    cooperating.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        previous_round_index = history.round_number - 1
        my_previous_action = self.get_my_action(history, previous_round_index)
        num_cooperators_in_previous_round = int(np.sum(history.opponent_actions[previous_round_index, :]))
        if my_previous_action == Action.C:
            num_cooperators_in_previous_round += 1
        if num_cooperators_in_previous_round >= self.m:
            return Action.C
        elif my_previous_action == Action.C:
            return Action.D
        else:
            return Action.C

    def get_my_action(self, history: PlayerHistory, round_idx: int) -> Action:
        """Helper to get own action from history."""
        if history.my_actions[round_idx]:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_69(BaseStrategy):
    """
    Implements the "Conditional Reciprocity with Threshold Awareness" strategy
    for the Collective Risk Dilemma. This strategy cooperates in the first round,
    and in subsequent rounds, it cooperates if the previous round's cooperation
    count met or was close to the threshold 'm', especially if there are more
    rounds remaining. It defects if cooperation consistently fails to meet the
    threshold or in the last round if the threshold was not met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if history is None or current_round_t == 0:
            return Action.C
        previous_round_index = current_round_t - 1
        cooperators_t_minus_1 = np.sum(history.opponent_actions[previous_round_index, :])
        if history.my_actions[previous_round_index]:
            cooperators_t_minus_1 += 1
        if cooperators_t_minus_1 >= self.m:
            return Action.C
        elif cooperators_t_minus_1 >= self.m - 1 and current_round_t < self.n_rounds - 1:
            return Action.C
        elif cooperators_t_minus_1 >= self.m - 2 and current_round_t < self.n_rounds - 1 and (self.m <= self.n_players / 2):
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_70(BaseStrategy):
    """
    The "Conditional Reciprocity and Threshold Awareness" (CRTA) Strategy.

    This strategy aims to achieve cooperation by rewarding it when others cooperate
    and defecting when cooperation seems unlikely or is exploited. It's based on
    observing the collective behavior and adjusting its own actions accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        prev_round_idx = current_round - 1
        cooperators_last_round = (1 if history.my_actions[prev_round_idx] else 0) + np.sum(history.opponent_actions[prev_round_idx, :])
        my_last_action_was_C = history.my_actions[prev_round_idx]
        if cooperators_last_round >= self.m - 1:
            return Action.C
        if my_last_action_was_C and cooperators_last_round >= self.m:
            return Action.C
        if cooperators_last_round < self.m - 1 and my_last_action_was_C:
            return Action.D
        if current_round == self.r and cooperators_last_round < self.m:
            return Action.D
        return Action.D

class Strategy_COOPERATIVE_71(BaseStrategy):
    """
    The Threshold Sentinel strategy aims to foster cooperation by acting as a vigilant guardian
    of the collective benefit. It prioritizes achieving the cooperation threshold while minimizing
    the risk of individual loss. It adaptively estimates the probability of reaching the threshold
    and cooperates if it believes the chance is high enough.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.CONFIDENCE_THRESHOLD = 0.75
        self.MOMENTUM_FACTOR = 0.1
        self.COOPERATION_BOOST = 0.05
        self.CAUTION_FACTOR = 0.05
        self.my_previous_action = None
        self.round_history = []

    def _calculate_ecp(self, state: GameState, history: PlayerHistory | None) -> float:
        """Calculates the Expected Cooperation Probability (ECP)."""
        current_round_num = state.round_number
        if current_round_num == 0:
            return max(0.5, self.m / self.n)
        else:
            num_cooperators_prev = np.sum(history.my_actions[:current_round_num])
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[current_round_num - 1, :])
            total_cooperators_prev = num_cooperators_prev + num_opponent_cooperators_prev
            baseline_ecp = total_cooperators_prev / self.n
            ecp_adjusted = baseline_ecp
            if current_round_num >= 2:
                num_cooperators_prev_prev = np.sum(history.my_actions[:current_round_num - 1])
                num_opponent_cooperators_prev_prev = np.sum(history.opponent_actions[current_round_num - 2, :])
                total_cooperators_prev_prev = num_cooperators_prev_prev + num_opponent_cooperators_prev_prev
                delta_cooperators = total_cooperators_prev - total_cooperators_prev_prev
                ecp_adjusted = baseline_ecp + self.MOMENTUM_FACTOR * (delta_cooperators / self.n)
            ecp_adjusted = max(0.0, min(1.0, ecp_adjusted))
            ecp_final = ecp_adjusted
            if self.my_previous_action == Action.C:
                if total_cooperators_prev >= self.m:
                    ecp_final = ecp_adjusted + self.COOPERATION_BOOST
                else:
                    ecp_final = ecp_adjusted - self.CAUTION_FACTOR
            return max(0.0, min(1.0, ecp_final))

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        if state.round_number == self.r - 1:
            return Action.D
        ecp = self._calculate_ecp(state, history)
        if ecp > self.CONFIDENCE_THRESHOLD:
            self.my_previous_action = Action.C
            return Action.C
        else:
            self.my_previous_action = Action.D
            return Action.D

class Strategy_COOPERATIVE_72(BaseStrategy):
    """
    Adaptive Threshold Cautious Cooperate (ATCC) Strategy.
    This strategy aims to maximize collective payoff by promoting cooperation
    when the cooperation threshold is met, but defects to avoid exploitation
    when it is not met or in the final round. It cooperates in the first round
    to initiate cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATCC strategy.

        Args:
            game_description: A dictionary containing game parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players.
                     None if it's the first round.

        Returns:
            Action.C to cooperate, or Action.D to defect.
        """
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        previous_round_index = current_round - 1
        if previous_round_index < 0:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        if num_cooperators_prev_round >= m:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_73(BaseStrategy):
    """
    Adaptive Cooperation Thresholding (ACT) Strategy for Collective Risk Dilemma.

    This strategy aims to be cooperative by default, but pragmatically adjusts
    its cooperation threshold based on historical performance and proximity to
    the game's end. It prioritizes achieving collective benefits while
    mitigating personal risk from exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) in the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players.
                     None if this is the first round.

        Returns:
            Action.C if cooperating, Action.D if defecting.
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        cooperators_last_round = 0
        if history.my_actions[-1]:
            cooperators_last_round += 1
        cooperators_last_round += np.sum(history.opponent_actions[-1, :])
        if current_round == self.n_rounds - 1:
            return Action.D
        if cooperators_last_round >= self.m:
            return Action.C
        if current_round < self.n_rounds - 2:
            if cooperators_last_round >= self.m - 2:
                return Action.C
        return Action.D

class Strategy_COOPERATIVE_74(BaseStrategy):
    """
    The "Threshold Guardian" strategy.
    Cooperates in the first round to initiate collective action.
    In subsequent rounds, it cooperates if and only if the threshold 'm' was met
    in the previous round. Otherwise, it defects to protect itself from exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        else:
            if not history.my_actions.size:
                return Action.D
            if history.opponent_actions.shape[1] > 0:
                n_cooperators_previous_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            else:
                n_cooperators_previous_round = history.my_actions[-1]
            if n_cooperators_previous_round >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_75(BaseStrategy):
    """
    "Adaptive Trust & Deterrence" Strategy for Collective Risk Dilemma.

    This strategy aims to foster cooperation by being willing to cooperate when
    it seems beneficial, but also to signal displeasure and potentially deter
    future defection when cooperation is exploited. It learns from past rounds
    and adjusts its behavior.

    Core Philosophy:
    The strategy believes in the power of collective action. It's willing to take
    the initial risk to achieve the higher rewards of cooperation. However, it's
    not naive. If others don't reciprocate, it will adjust its strategy to
    protect its own interests while still leaving the door open for future
    cooperation.

    Decision Logic:
    1.  Initial Trust (Round 1): Always Cooperate (C).
    2.  Adaptive Cooperation Threshold (Round t > 1):
        - High Trust: Cooperate (C).
        - Moderate Trust: Cooperate (C) if previous round was close to m
          (>= m-1 cooperators). Otherwise, Defect (D).
        - Low Trust: Defect (D).
    3.  Deterrence Mechanism: Trust Level decreases if player cooperates and
        m is not met.
    4.  Final Round (Round r): Acts based on accumulated Trust Level.

    Internal State:
    - TrustLevel: A dynamic internal variable representing the strategy's belief
      in the likelihood of successful cooperation. Initialized and updated.
    - LearningRate: Controls how much each round affects TrustLevel.
    - Threshold_High, Threshold_Low: Define boundaries for TrustLevel scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.trust_level = 0.5
        self.learning_rate = 0.1
        self.threshold_high = 0.75
        self.threshold_low = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_number = state.round_number
        previous_round_index = current_round_number - 1
        if current_round_number > 0:
            observed_cooperation_count_prev = 0
            if self.n > 1:
                observed_cooperation_count_prev = np.sum(history.opponent_actions[previous_round_index, :])
            if history.my_actions[previous_round_index]:
                observed_cooperation_count_prev += 1
            observed_cooperation_rate_prev = observed_cooperation_count_prev / self.n
            collective_success = observed_cooperation_count_prev >= self.m
            if collective_success:
                self.trust_level += self.learning_rate * (1 - self.trust_level)
            else:
                self.trust_level -= self.learning_rate * self.trust_level
                if history.my_actions[previous_round_index] == Action.C.value:
                    self.trust_level -= self.learning_rate * 0.5
            self.trust_level = max(0.0, min(1.0, self.trust_level))
        observed_cooperation_rate_prev_for_decision = 0
        if current_round_number > 0:
            observed_cooperation_count_prev_for_decision = 0
            if self.n > 1:
                observed_cooperation_count_prev_for_decision = np.sum(history.opponent_actions[previous_round_index, :])
            if history.my_actions[previous_round_index]:
                observed_cooperation_count_prev_for_decision += 1
            observed_cooperation_rate_prev_for_decision = observed_cooperation_count_prev_for_decision / self.n
        if self.trust_level > self.threshold_high:
            return Action.C
        elif self.trust_level >= self.threshold_low:
            if observed_cooperation_rate_prev_for_decision >= (self.m - 1) / self.n:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_76(BaseStrategy):
    """
    The "Threshold Guardian" strategy for the Collective Risk Dilemma.
    This strategy aims to cooperate when the collective is close to or has met
    the cooperation threshold 'm'. It learns from past performance, reducing
    its tendency to cooperate if the threshold is consistently missed, and
    increasing it if cooperation is successful. It defects in the last round
    to maximize its own final payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_tendency = 1.0
        self.alpha = 0.3
        self.beta = 0.07

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.r:
            return Action.D
        if history is None:
            self.cooperation_tendency = 1.0
            return Action.C
        current_round = state.round_number
        prev_round_idx = current_round - 1
        my_cooperated_last_round = self.get_action_from_bool(history.my_actions[prev_round_idx]) == Action.C
        num_opponents = self.n - 1
        if num_opponents > 0:
            opponent_cooperators_last_round = np.sum(history.opponent_actions[prev_round_idx, :] == Action.C.value)
        else:
            opponent_cooperators_last_round = 0
        cooperators_last_round = (1 if my_cooperated_last_round else 0) + opponent_cooperators_last_round
        if cooperators_last_round >= self.m:
            my_action = Action.C
        else:
            my_action = Action.D
        cooperation_deficit = 0
        if cooperators_last_round < self.m:
            cooperation_deficit = self.m - cooperators_last_round
            reduction_factor = self.alpha * (cooperation_deficit / self.n)
            self.cooperation_tendency = max(0.0, self.cooperation_tendency * (1.0 - reduction_factor))
        else:
            increase_factor = self.beta * (1.0 - self.cooperation_tendency)
            self.cooperation_tendency = min(1.0, self.cooperation_tendency + increase_factor)
        return my_action

    def get_action_from_bool(self, action_bool: bool) -> Action:
        """Helper to convert boolean action encoding to Action enum."""
        return Action.C if action_bool else Action.D

class Strategy_COOPERATIVE_77(BaseStrategy):
    """
    The "Threshold Sentinel" strategy for the Collective Risk Dilemma.
    This strategy cooperates if the number of cooperators in the previous round
    was sufficient to suggest the cooperation threshold (m) is likely to be met,
    otherwise it defects. It cooperates in the first round and defects in the last.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None:
            return Action.C
        last_round_index = history.round_number - 1
        my_cooperators_last_round = int(history.my_actions[last_round_index])
        opponent_cooperators_last_round = 0
        if self.n > 1:
            if self.n == 2:
                opponent_cooperators_last_round = int(history.opponent_actions[last_round_index, 0])
            else:
                opponent_cooperators_last_round = np.sum(history.opponent_actions[last_round_index, :])
        total_cooperators_last_round = my_cooperators_last_round + opponent_cooperators_last_round
        if total_cooperators_last_round >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_78(BaseStrategy):
    """
    Cooperative Strategy: Conditional Cooperation with Threshold Re-evaluation.
    This strategy starts cooperatively, continues to cooperate if the threshold
    was met in the previous round, and defects only if the number of defectors
    in the previous round was overwhelming (i.e., >= m). In the last round,
    it always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if current_round == r:
            return Action.D
        if history is None:
            return Action.C
        previous_round_actions = history.my_actions[-1]
        cooperators_in_previous_round = np.sum(previous_round_actions)
        defectors_in_previous_round = n - cooperators_in_previous_round
        if cooperators_in_previous_round >= m:
            return Action.C
        elif defectors_in_previous_round >= m:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_79(BaseStrategy):
    """
    "Trust but Verify with Escalating Commitment" strategy for the Collective Risk Dilemma.
    This strategy initiates cooperation, monitors group behavior, and adjusts its
    cooperation level based on historical success in meeting the cooperation threshold.
    It aims to balance the pursuit of collective reward with individual risk management.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if current_round == r:
            return Action.D
        if history is None:
            return Action.C
        if len(self.cooperation_history) < current_round:
            cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            self.cooperation_history.append(int(cooperators_last_round))
        cooperators_in_previous_round = self.cooperation_history[-1] if self.cooperation_history else 0
        N_for_average = max(3, math.ceil(r / 4))
        start_index = max(0, len(self.cooperation_history) - N_for_average)
        recent_cooperation_data = self.cooperation_history[start_index:]
        average_cooperators_last_N_rounds = 0.0
        if recent_cooperation_data:
            average_cooperators_last_N_rounds = np.mean(recent_cooperation_data)
        if cooperators_in_previous_round >= m or average_cooperators_last_N_rounds >= m:
            return Action.C
        if cooperators_in_previous_round >= m - 1 and average_cooperators_last_N_rounds >= m - 1:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_80(BaseStrategy):
    """
    The "Threshold Guardian" strategy.

    This strategy aims to foster cooperation by being a conditional cooperator.
    It initiates cooperation and continues to cooperate as long as there's a
    reasonable expectation of meeting the cooperation threshold. It punishes
    persistent defection by defecting itself, prioritizing self-preservation
    when the collective outcome is unlikely. It also prioritizes defecting in
    the final round to maximize individual gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_cooperation_history = []
        self.cooperators_count_threshold_met = 0
        self.defection_streak = 0
        self.last_observed_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            self.my_cooperation_history.append(True)
            self.last_observed_cooperators = 0
            self.cooperators_count_threshold_met = 0
            self.defection_streak = 0
            return Action.C
        if state.round_number == r:
            return Action.D
        my_action_last_round = self.my_cooperation_history[-1]
        if len(history.opponent_actions) > 0:
            observed_cooperators_in_round_t_minus_1 = np.sum(history.opponent_actions[-1, :])
        else:
            observed_cooperators_in_round_t_minus_1 = 0
        total_cooperators_last_round = observed_cooperators_in_round_t_minus_1 + (1 if my_action_last_round else 0)
        if total_cooperators_last_round >= m:
            self.cooperators_count_threshold_met += 1
            self.defection_streak = 0
        else:
            self.defection_streak += 1
        self.last_observed_cooperators = observed_cooperators_in_round_t_minus_1
        needed_to_reach_m = max(0, m - total_cooperators_last_round)
        if needed_to_reach_m == 0 and self.cooperators_count_threshold_met > 0:
            self.my_cooperation_history.append(True)
            return Action.C
        if needed_to_reach_m > 0 and self.defection_streak < 3:
            self.my_cooperation_history.append(True)
            return Action.C
        if needed_to_reach_m > 0 and self.defection_streak >= 3:
            self.my_cooperation_history.append(False)
            return Action.D
        if total_cooperators_last_round >= m:
            self.my_cooperation_history.append(True)
            return Action.C
        elif self.defection_streak < 3:
            self.my_cooperation_history.append(True)
            return Action.C
        else:
            self.my_cooperation_history.append(False)
            return Action.D

class Strategy_COOPERATIVE_81(BaseStrategy):
    """
    Threshold-Sensitive Reciprocity: Cooperates if enough others are likely to,
    otherwise defects. Starts cooperative, punishes sustained defection,
    and always defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.successful_cooperation_streak = 0
        self.defection_punishment_counter = 0
        self.last_round_cooperators_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if current_round_num == self.n_rounds:
            return Action.D
        if history is None:
            self.successful_cooperation_streak = 1
            self.defection_punishment_counter = 0
            self.last_round_cooperators_count = self.n_players
            return Action.C
        previous_round_num = current_round_num - 1
        if previous_round_num >= 0:
            actual_cooperators_prev_round = 0
            if previous_round_num < len(history.my_actions):
                actual_cooperators_prev_round += history.my_actions[previous_round_num]
                if previous_round_num < history.opponent_actions.shape[0]:
                    actual_cooperators_prev_round += np.sum(history.opponent_actions[previous_round_num, :])
            self.last_round_cooperators_count = actual_cooperators_prev_round
            if actual_cooperators_prev_round >= self.m:
                self.successful_cooperation_streak += 1
                self.defection_punishment_counter = max(0, self.defection_punishment_counter - 1)
            else:
                self.successful_cooperation_streak = 0
                self.defection_punishment_counter += 1
        else:
            self.last_round_cooperators_count = 0
            self.successful_cooperation_streak = 0
            self.defection_punishment_counter += 1
        cooperation_likely_to_succeed = self.last_round_cooperators_count >= self.m - 1
        punishment_threshold_not_met = self.defection_punishment_counter <= math.ceil(self.n_players / 2)
        if cooperation_likely_to_succeed and punishment_threshold_not_met:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_82(BaseStrategy):
    """
    The "Threshold Watcher" strategy is a cooperative and adaptive strategy
    for the Collective Risk Dilemma. It starts by cooperating and then
    adjusts its behavior based on the observed cooperation rate in the
    previous round, aiming to maximize its long-term payoff by cooperating
    when the collective is likely to succeed and defecting when it's not.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            return Action.C
        current_round_number = history.round_number
        previous_round_cooperators = 0
        if current_round_number > 0:
            previous_round_index = current_round_number - 1
            my_previous_action = 1 if self.game_description.n_players > 0 and history.my_actions[previous_round_index] else 0
            previous_round_cooperators += my_previous_action
            if history.opponent_actions.shape[0] > previous_round_index:
                previous_round_cooperators += np.sum(history.opponent_actions[previous_round_index, :])
        else:
            return Action.C
        previous_ocr = previous_round_cooperators / n
        if state.round_number == r:
            if previous_ocr >= m / n:
                return Action.C
            else:
                return Action.D
        elif previous_ocr >= m / n * (1 + self.epsilon):
            return Action.C
        elif previous_ocr >= (m - 1) / n:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_83(BaseStrategy):
    """
    Cautious Reciprocator strategy for the Collective Risk Dilemma.
    Aims to be cooperative when beneficial and safe, adapting to observed behavior,
    and protecting itself from exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if history is None:
            return Action.C
        n_c_prev = 0
        if history.round_number > 0:
            if history.my_actions[-1] == Action.C.value:
                n_c_prev += 1
            n_c_prev += np.sum(history.opponent_actions[-1, :] == Action.C.value)
        my_last_action_was_C = False
        if history.round_number > 0:
            my_last_action_was_C = history.my_actions[-1] == Action.C.value
        action_to_play = Action.D
        if current_round_t == 0:
            action_to_play = Action.C
        elif current_round_t >= self.r - 2:
            if n_c_prev >= self.m - 1:
                action_to_play = Action.C
        elif n_c_prev >= self.m:
            action_to_play = Action.C
        elif n_c_prev >= self.m - 1:
            action_to_play = Action.C
        elif n_c_prev < self.m - 1:
            if my_last_action_was_C and n_c_prev < self.m:
                action_to_play = Action.D
            else:
                action_to_play = Action.D
        return action_to_play

class Strategy_COOPERATIVE_84(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for the Collective Risk Dilemma.

    This strategy attempts to foster cooperation by mirroring the game's reward
    structure. It adapts to observed cooperation levels, striving to reach the
    cooperative threshold (m) when likely, and defecting when cooperation
    is likely to fail, minimizing individual loss.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_count_history = []
        self.early_phase_end_round = max(5, math.ceil(self.n_rounds / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            if self.m <= self.n_players * 0.5 and self.m > 1:
                return Action.C
            else:
                return Action.D
        last_round_cooperation_count = 0
        if self.cooperation_count_history:
            last_round_cooperation_count = self.cooperation_count_history[-1]
        if current_round == self.n_rounds:
            return Action.D
        if current_round < self.early_phase_end_round:
            if not self.cooperation_count_history:
                if self.m <= self.n_players * 0.5 and self.m > 1:
                    return Action.C
                else:
                    return Action.D
            average_cooperation_rate = np.mean(self.cooperation_count_history)
            num_past_rounds = len(self.cooperation_count_history)
            predicted_cooperation = last_round_cooperation_count + (self.n_players - num_past_rounds) * average_cooperation_rate
            if predicted_cooperation >= self.m:
                return Action.C
            else:
                return Action.D
        elif last_round_cooperation_count >= self.m:
            return Action.C
        elif last_round_cooperation_count == self.m - 1:
            return Action.C
        else:
            return Action.D

    def _record_round_outcome(self, current_round_cooperation_count: int):
        """Helper to record the cooperation count for the current round."""
        self.cooperation_count_history.append(current_round_cooperation_count)

class Strategy_COOPERATIVE_85(BaseStrategy):
    """
    The "Threshold Tracker" strategy for the Collective Risk Dilemma.
    This strategy aims to achieve cooperation by monitoring the observed
    cooperative rate and adjusting its own actions to incentivize or maintain
    the cooperative outcome. It cooperates by default in the first round,
    and then dynamically decides to cooperate or defect based on the
    number of cooperators in the previous round relative to the threshold 'm'.
    It's designed to be robust and adapt to group behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_action_last_round = None
        self.observed_cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on the game state and history.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs up to the previous round.
                     None if it's the first round.

        Returns:
            The action (Action.C or Action.D) for the current round.
        """
        current_round = state.round_number
        if history is None:
            self.my_action_last_round = Action.C.value
            return Action.C
        self.my_action_last_round = history.my_actions[current_round - 1]
        self.observed_cooperators_last_round = sum(history.opponent_actions[current_round - 1, :]) + self.my_action_last_round
        if self.observed_cooperators_last_round >= self.m:
            self.my_action_last_round = Action.C.value
            return Action.C
        if self.observed_cooperators_last_round + 1 >= self.m:
            self.my_action_last_round = Action.C.value
            return Action.C
        if self.observed_cooperators_last_round < self.m - 1:
            self.my_action_last_round = Action.D.value
            return Action.D
        self.my_action_last_round = Action.C.value
        return Action.C

class Strategy_COOPERATIVE_86(BaseStrategy):
    """
    "Commit to Cooperative Threshold" strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by defaulting to cooperation,
    but it pragmatically defects if the collective cooperation threshold 'm'
    is unlikely to be met, thereby avoiding wasted contributions and ensuring
    a minimum payoff. It has specific logic for the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs if this is not the first round,
                     otherwise None.

        Returns:
            Action.C if the player chooses to cooperate, Action.D otherwise.
        """
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == self.r:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round_number - 1, :]
        n_c_observed = np.sum(previous_round_actions)
        required_others_for_cooperation = self.m - 1
        if n_c_observed >= required_others_for_cooperation:
            return Action.C
        elif n_c_observed + 1 < self.m:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_87(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy.

    This strategy aims to maximize total payoff by fostering cooperation while
    remaining robust to defection and ensuring a reasonable individual payoff.
    It maintains a dynamic cooperation threshold that adjusts based on the
    success of past cooperation attempts.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.dynamic_threshold_T = self.m + 1
        self.cooperation_success_score_S = 0.0
        self.last_round_cooperation_count = 0
        self.cooperation_reward = 1.0
        self.defection_penalty = 2.0
        self.threshold_adjustment_step = 1.0
        self.end_game_buffer_period = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds:
            return Action.D
        if current_round_number > self.n_rounds - self.end_game_buffer_period:
            return Action.D
        if history is None:
            self.dynamic_threshold_T = self.m + 1
            self.cooperation_success_score_S = 0.0
            self.last_round_cooperation_count = 0
            return Action.C
        current_cooperation_count = self.last_round_cooperation_count
        if current_cooperation_count >= self.m:
            self.cooperation_success_score_S += self.cooperation_reward
        else:
            self.cooperation_success_score_S -= self.defection_penalty
        if self.cooperation_success_score_S > 0:
            if current_cooperation_count >= self.m:
                self.dynamic_threshold_T = max(self.m, self.dynamic_threshold_T - self.threshold_adjustment_step)
            else:
                self.dynamic_threshold_T = min(self.n_players, self.dynamic_threshold_T + self.threshold_adjustment_step)
        else:
            self.dynamic_threshold_T = min(self.n_players, self.dynamic_threshold_T + self.threshold_adjustment_step)
        self.dynamic_threshold_T = max(self.m, min(self.n_players, self.dynamic_threshold_T))
        if current_cooperation_count >= self.dynamic_threshold_T:
            return Action.C
        elif current_cooperation_count >= self.m and self.cooperation_success_score_S > 0:
            return Action.C
        else:
            return Action.D

    def update_state_after_round(self, state: GameState, history: PlayerHistory) -> None:
        """
        This method is called by the simulator after each round to update the strategy's state.
        It's crucial for strategies that maintain state across rounds.
        """
        last_round_cooperators = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        self.last_round_cooperation_count = last_round_cooperators

class Strategy_COOPERATIVE_88(BaseStrategy):
    """
    Adaptive Threshold Cooperate (ATC) Strategy for the Collective Risk Dilemma.

    This strategy aims to promote cooperation by dynamically adjusting its decision-making
    based on observed cooperation rates, the proximity to the cooperation threshold,
    and past game outcomes. It seeks to balance cooperative intent with a cautious
    approach to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.initial_cooperation_rounds = max(2, math.floor(self.r / 4))
        self.sqrt_n_floor = math.floor(math.sqrt(self.n))
        self.recent_failure_window_size = 2 * math.ceil(self.r / 5)

    def _count_cooperators(self, actions: NDArray[np.bool_]) -> int:
        """Counts the number of True values (cooperators) in a boolean array."""
        return np.sum(actions)

    def _count_past_successes(self, history: PlayerHistory) -> int:
        """Counts how many past rounds met the cooperation threshold."""
        success_count = 0
        for round_idx in range(history.round_number):
            round_cooperators = self._count_cooperators(history.opponent_actions[round_idx, :]) + (1 if self.my_actions[round_idx] else 0)
            if round_cooperators >= self.m:
                success_count += 1
        return success_count

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        self.my_actions = history.my_actions if history else np.array([], dtype=bool)
        if history is None or history.round_number == 0:
            if self.m <= self.n / 2:
                return Action.C
            else:
                return Action.D
        previous_round_idx = history.round_number - 1
        observed_cooperators_prev = self._count_cooperators(history.opponent_actions[previous_round_idx, :]) + (1 if self.my_actions[previous_round_idx] else 0)
        total_cooperation_achieved_in_past = self._count_past_successes(history)
        num_past_rounds = history.round_number
        cooperate = False
        if num_past_rounds <= self.initial_cooperation_rounds:
            if observed_cooperators_prev >= self.m - self.sqrt_n_floor:
                cooperate = True
        if observed_cooperators_prev >= self.m - 1:
            cooperate = True
        if total_cooperation_achieved_in_past > 0 and observed_cooperators_prev >= self.m - 2:
            cooperate = True
        if num_past_rounds <= self.initial_cooperation_rounds:
            if observed_cooperators_prev < self.m - self.sqrt_n_floor:
                return Action.D
        recent_failure_count = 0
        if num_past_rounds >= self.recent_failure_window_size:
            for i in range(num_past_rounds - self.recent_failure_window_size, num_past_rounds):
                round_cooperators = self._count_cooperators(history.opponent_actions[i, :]) + (1 if self.my_actions[i] else 0)
                if round_cooperators < self.m:
                    recent_failure_count += 1
            if recent_failure_count == self.recent_failure_window_size and observed_cooperators_prev < self.m:
                return Action.D
        if self.k > self.n / self.m and observed_cooperators_prev < self.m - 1:
            return Action.D
        if cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_89(BaseStrategy):
    """
    The "Cautious Optimizer" strategy for the Collective Risk Dilemma.
    This strategy prioritizes reaching the cooperation threshold while
    mitigating the risk of exploitation. It learns from past rounds and
    adjusts its cooperation level based on observed success and the
    necessity of its own contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.base_CTT = 0.5
        self.learning_rate = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        if history is None or history.round_number == 0:
            return Action.C
        current_round = history.round_number
        previous_round_index = current_round - 1
        observed_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
        if history.my_actions[previous_round_index]:
            observed_cooperators += 1
        observed_cooperation_rate = observed_cooperators / n
        last_round_success = observed_cooperators >= m
        potential_cooperators_if_i_cooperate = observed_cooperators + 1
        players_needed_for_threshold = max(0, m - potential_cooperators_if_i_cooperate)
        cns = players_needed_for_threshold / (n - 1) if n - 1 > 0 else 0
        current_CTT = self.base_CTT
        if last_round_success:
            current_CTT = self.base_CTT + self.learning_rate * (1 - observed_cooperation_rate)
        else:
            current_CTT = self.base_CTT - self.learning_rate * observed_cooperation_rate
        current_CTT = max(0.0, min(1.0, current_CTT))
        if cns <= current_CTT:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_91(BaseStrategy):
    """
    Adaptive Risk Aversion (ARA) strategy for the Collective Risk Dilemma.

    ARA aims to maximize its long-term payoff by cooperatively contributing
    when there's a reasonable chance of achieving the cooperative outcome (m
    or more cooperators), while protecting itself from exploitation when
    cooperation seems unlikely or has failed. It learns from past rounds
    to inform future decisions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.lookback_window = 3
        self.cooperation_momentum_threshold = (self.m - 1) / self.n
        self.risk_aversion_threshold_avg_factor = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = history.round_number
        total_cooperators_last_round = np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        my_coop_count_so_far = np.sum(history.my_actions)
        proportion_cooperated_last_round = total_cooperators_last_round / self.n
        if current_round < self.lookback_window:
            recent_rounds_cooperators = []
            for r in range(current_round):
                coop_in_round = np.sum(history.my_actions[r]) + np.sum(history.opponent_actions[r, :])
                recent_rounds_cooperators.append(coop_in_round)
            if not recent_rounds_cooperators:
                average_cooperators_last_w_rounds = 0
            else:
                average_cooperators_last_w_rounds = np.mean(recent_rounds_cooperators)
        else:
            recent_rounds_cooperators = []
            for r in range(current_round - self.lookback_window, current_round):
                coop_in_round = np.sum(history.my_actions[r]) + np.sum(history.opponent_actions[r, :])
                recent_rounds_cooperators.append(coop_in_round)
            average_cooperators_last_w_rounds = np.mean(recent_rounds_cooperators)
        if current_round == self.r - 1:
            if total_cooperators_last_round >= self.m:
                return Action.C
            else:
                return Action.D
        if total_cooperators_last_round >= self.m:
            if my_coop_count_so_far >= current_round * 0.5:
                return Action.C
            else:
                return Action.D
        elif total_cooperators_last_round >= self.m - 1:
            if average_cooperators_last_w_rounds >= self.m * self.risk_aversion_threshold_avg_factor and proportion_cooperated_last_round >= self.cooperation_momentum_threshold:
                return Action.C
            else:
                return Action.D
        else:
            my_defection_rate = (current_round - my_coop_count_so_far) / current_round
            if my_defection_rate > 0.7 and proportion_cooperated_last_round >= (self.m - 2) / self.n:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_92(BaseStrategy):
    """
    Conditional Cooperation with Escalation Response (CCER).
    This strategy starts cooperatively, then adapts based on the observed
    level of cooperation in the previous round. It continues to cooperate
    if the threshold 'm' is met or nearly met, but defects if cooperation
    significantly fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy.

        Args:
            game_description: An object containing game parameters.
        """
        self.game_description = game_description
        self.penalty_threshold = min(self.game_description.m - 1, 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on game history.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs from previous rounds.
                     None if this is the first round.

        Returns:
            Action.C if cooperating, Action.D if defecting.
        """
        if history is None:
            return Action.C
        previous_round_index = history.round_number - 1
        my_previous_action = Action.C if history.my_actions[previous_round_index] else Action.D
        num_opponent_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
        total_cooperators_previous_round = num_opponent_cooperators + (1 if my_previous_action == Action.C else 0)
        m = self.game_description.m
        if total_cooperators_previous_round >= m:
            return Action.C
        elif m > total_cooperators_previous_round >= m - self.penalty_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_93(BaseStrategy):
    """
    Adaptive Conditional Cooperation with Escalation strategy for the Collective Risk Dilemma.

    This strategy initially cooperates to signal good faith. In subsequent rounds,
    it cooperates if the cooperation threshold was met or nearly met in the previous round.
    If the threshold was significantly missed, it defects to punish exploitation and
    encourage a shift towards cooperation. The buffer `m - x` allows for some tolerance
    before switching to defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing the game's parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.BUFFER_X = 2
        self.PROLONGED_DEFECTION_THRESHOLD = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the game state and history.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs from previous rounds.
                     None if it's the first round.

        Returns:
            Action.C to cooperate or Action.D to defect.
        """
        if history is None:
            return Action.C
        n_players = self.game_description.n_players
        m = self.game_description.m
        if history.opponent_actions.shape[1] == 0:
            last_round_cooperation_count = 0
        else:
            cooperators_self = 1 if history.my_actions[-1] else 0
            cooperators_opponents = np.sum(history.opponent_actions[-1, :])
            last_round_cooperation_count = cooperators_self + cooperators_opponents
        if last_round_cooperation_count >= m:
            return Action.C
        elif last_round_cooperation_count >= m - self.BUFFER_X:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_94(BaseStrategy):
    """
    Adaptive Threshold Cooperate strategy for the Collective Risk Dilemma.

    This strategy aims to achieve the collective reward by coordinating cooperation
    when it's likely to succeed, while defecting to safeguard its own payoff
    when cooperation seems unlikely or risky. It observes the history of play
    to adapt its behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs from previous rounds,
                     or None if it's the first round.

        Returns:
            Action: The chosen action (Action.C or Action.D).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1]
        my_previous_action = history.my_actions[current_round - 1]
        cooperators_in_previous_round = np.sum(previous_round_actions) + int(my_previous_action)
        observation_phase_end_idx = min(r, m + 1) - 1
        if current_round <= observation_phase_end_idx:
            if cooperators_in_previous_round >= m:
                return Action.C
            elif cooperators_in_previous_round == m - 1:
                return Action.C
            else:
                return Action.D
        elif cooperators_in_previous_round >= m:
            return Action.C
        elif cooperators_in_previous_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_95(BaseStrategy):
    """
    Conditional Reciprocity with Risk Aversion Strategy.

    This strategy prioritizes cooperation, especially when near the threshold,
    but employs conditional reciprocity to avoid exploitation. It cooperates
    in the first round, continues to cooperate if the threshold was met,
    and uses temporary defection to signal dissatisfaction if the threshold
    was not met and either the player defected last round, or cooperated
    but the group was far from the threshold. It strongly leans towards
    cooperation when the group is just one cooperator away from the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = state.round_number
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        if num_cooperators_prev_round < self.m and (not my_action_prev_round_is_cooperate):
            return Action.D
        if num_cooperators_prev_round < self.m and num_cooperators_prev_round >= self.m - 1:
            return Action.C
        if num_cooperators_prev_round >= self.m:
            return Action.C
        if num_cooperators_prev_round < self.m and num_cooperators_prev_round < self.m - 1 and my_action_prev_round_is_cooperate:
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_96(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Punishment and Reward Discounting (ATTPRD)
    This strategy cooperates by default, especially in the first round.
    In subsequent rounds, it observes the collective cooperation rate from the previous round.
    If the cooperation threshold 'm' was met or exceeded, it continues to cooperate.
    If the threshold was not met, it defects to punish collective failure.
    In the final round, it always defects as there is no future incentive.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if history is None:
            return Action.C
        previous_round_my_action = self.game_description.m
        if self.n > 1:
            if history.opponent_actions.shape[0] > 0:
                previous_round_opponent_actions = history.opponent_actions[-1, :]
                cooperators_in_previous_round = np.sum(previous_round_opponent_actions) + (1 if history.my_actions[-1] else 0)
            else:
                cooperators_in_previous_round = 1 if history.my_actions[-1] else 0
        else:
            cooperators_in_previous_round = 1 if history.my_actions[-1] else 0
        if current_round_number == self.r - 1:
            return Action.D
        if cooperators_in_previous_round >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_97(BaseStrategy):
    """
    Threshold Tit-for-Tat strategy for the Collective Risk Dilemma.

    This strategy cooperates on the first round. In subsequent rounds, it cooperates
    if the cooperation threshold was met or exceeded in the previous round.
    If the threshold was not met, it defects to punish the group and incentivize
    future cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        else:
            last_round_index = history.round_number - 1
            opponent_cooperators_last_round = np.sum(history.opponent_actions[last_round_index, :])
            my_last_action = Action.C if history.my_actions[last_round_index] else Action.D
            total_cooperators_last_round = opponent_cooperators_last_round
            if my_last_action == Action.C:
                total_cooperators_last_round += 1
            if total_cooperators_last_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_98(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy.

    This strategy aims to foster cooperation by dynamically adjusting a cooperation
    threshold based on observed outcomes of previous rounds. It starts cautiously
    and becomes more optimistic with successful cooperation, while becoming more
    cautious when cooperation fails to meet the threshold. It defects in the
    final round due to the lack of future consequences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATC strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.initial_cct_offset = 0.05
        self.delta = 0.05
        self.gamma = 0.1
        self.cct = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: Player history from previous rounds. None if it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round = state.round_number
        if current_round == r:
            return Action.D
        if history is None:
            self.cct = m / n + self.initial_cct_offset
            return Action.C
        num_my_cooperators_last_round = 1 if history.my_actions[current_round - 1] == Action.C else 0
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[current_round - 1, :])
        num_cooperators_last_round = num_my_cooperators_last_round + num_opponent_cooperators_last_round
        observed_cooperation_level = num_cooperators_last_round / n
        if observed_cooperation_level >= self.cct:
            action = Action.C
        else:
            action = Action.D
        reward_achieved_last_round = num_cooperators_last_round >= m
        if reward_achieved_last_round:
            self.cct = max(0.0, self.cct - self.delta)
        else:
            self.cct = min(1.0, self.cct + self.gamma)
        return action

class Strategy_COOPERATIVE_99(BaseStrategy):
    """
    The "Cautious Rewarder" strategy.

    This strategy aims to be cooperative by signaling a willingness to cooperate
    and rewarding others who do the same, while also being cautious to avoid
    exploitation. It is designed to be robust against various player behaviors
    by dynamically adjusting its cooperation threshold based on past outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        my_last_action = self.get_my_last_action(history)
        opponent_last_actions = history.opponent_actions[current_round - 1, :]
        cooperators_t_minus_1 = int(my_last_action) + np.sum(opponent_last_actions)
        if cooperators_t_minus_1 >= self.m:
            return Action.C
        else:
            defectors_t_minus_1 = self.n - cooperators_t_minus_1
            if defectors_t_minus_1 > self.n - self.m:
                return Action.D
            else:
                return Action.C

    def get_my_last_action(self, history: PlayerHistory) -> bool:
        """Helper to get my last action, returning True for C, False for D."""
        if len(history.my_actions) == 0:
            return False
        return history.my_actions[-1]

class Strategy_COOPERATIVE_100(BaseStrategy):
    """
    Adaptive Reciprocity with Risk Aversion (ARRA) for the Collective Risk Dilemma.

    This strategy aims to maximize its own long-term payoff by fostering cooperation
    while safeguarding against exploitation. It is guided by the principle of
    reciprocity, but with a pragmatic acknowledgment of the risk involved in
    trusting others.

    Decision logic is based on observing the collective behavior of other players
    in previous rounds and assessing the likelihood of meeting the cooperation
    threshold (m).

    General Principle: Cooperate if the probability of meeting the threshold *with
    your cooperation* is sufficiently high. Defect if there is a significant risk
    that your cooperation will be wasted, or if others are consistently defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if current_round_num == self.r:
            return Action.D
        if history is None:
            if self.n - 1 >= self.m - 1:
                return Action.C
            else:
                return Action.D
        other_players_actions_prev_round = history.opponent_actions[-1, :]
        o_cooperators = np.sum(other_players_actions_prev_round)
        o_defectors = self.n - 1 - o_cooperators
        if o_cooperators >= self.m - 1:
            return Action.C
        if self.m > 2 and o_cooperators == self.m - 2:
            return Action.C
        if o_defectors >= self.n - self.m:
            return Action.D
        return Action.D

class Strategy_COOPERATIVE_101(BaseStrategy):
    """
    Cooperative Risk Aversion Strategy (CRAS)

    This strategy initially cooperates to establish a cooperative equilibrium.
    It adapts by cooperating if the threshold was met in the previous round,
    or if it was narrowly missed with few defectors.
    It defects if it was exploited (cooperated and threshold not met),
    or if defection was widespread in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = history.round_number
        prev_round_idx = current_round - 1
        if prev_round_idx < 0:
            return Action.C
        my_action_prev = Action.C if history.my_actions[prev_round_idx] else Action.D
        cooperators_prev = 0
        if my_action_prev == Action.C:
            cooperators_prev += 1
        if self.n > 1 and len(history.opponent_actions) > 0:
            cooperators_prev += np.sum(history.opponent_actions[prev_round_idx, :])
        defectors_prev = self.n - cooperators_prev
        if cooperators_prev >= self.m:
            return Action.C
        if cooperators_prev == self.m - 1 and defectors_prev < self.n / 2.0:
            return Action.C
        if my_action_prev == Action.C and cooperators_prev < self.m:
            return Action.D
        if defectors_prev >= self.n / 2.0:
            return Action.D
        return Action.D

class Strategy_COOPERATIVE_102(BaseStrategy):
    """
    The "Threshold Weaver" strategy for the Collective Risk Dilemma.
    This strategy aims to weave a pattern of cooperation that reliably hits the 'm' threshold,
    adapting to past outcomes. It prioritizes collective success while minimizing
    individual risk by estimating opponent behavior and adjusting its cooperation level.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.rounds_played = 0
        self.cooperation_successes = 0
        self.my_cooperation_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            if self.m <= math.floor(self.n / 2) + 1:
                self.my_cooperation_history.append(1)
                return Action.C
            else:
                self.my_cooperation_history.append(0)
                return Action.D
        else:
            self.rounds_played = history.round_number
            self.my_cooperation_history = history.my_actions.tolist()
            self.cooperation_successes = 0
            for r_idx in range(history.round_number):
                coop_in_round = 0
                if history.my_actions[r_idx]:
                    coop_in_round += 1
                coop_in_round += np.sum(history.opponent_actions[r_idx, :])
                if coop_in_round >= self.m:
                    self.cooperation_successes += 1
            avg_other_coop_rate = 0.0
            total_other_players_who_played = history.round_number * (self.n - 1)
            if total_other_players_who_played > 0:
                total_cooperators_excluding_self = 0
                for r_idx in range(history.round_number):
                    total_cooperators_excluding_self += np.sum(history.opponent_actions[r_idx, :])
                avg_other_coop_rate = total_cooperators_excluding_self / total_other_players_who_played
            estimated_other_cooperators = math.floor(avg_other_coop_rate * (self.n - 1))
            historical_success_rate = self.cooperation_successes / history.round_number if history.round_number > 0 else 0.0
            cooperation_buffer = 0
            if historical_success_rate > 0.75:
                cooperation_buffer = 1
            elif historical_success_rate > 0.4:
                cooperation_buffer = 1
            if estimated_other_cooperators + cooperation_buffer >= self.m - 1:
                self.my_cooperation_history.append(1)
                return Action.C
            else:
                self.my_cooperation_history.append(0)
                return Action.D

class Strategy_COOPERATIVE_103(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC):
    This strategy aims to maximize long-term cumulative payoff in the Collective Risk Dilemma.
    It cooperates in the first round to gather data. In subsequent rounds, it cooperates
    if the previous round was successful, or if the previous round was very close to meeting
    the cooperation threshold (m-1 cooperators). If the previous round fell significantly
    short of the threshold, it defects to protect its endowment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Adaptive Threshold Cooperation strategy.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game (e.g., current round number).
            history: A PlayerHistory object containing past actions and payoffs.
                     If None, it's the first round.

        Returns:
            Action: The action to take (Action.C or Action.D).
        """
        n = self.game_description.n_players
        m = self.game_description.m
        if history is None:
            return Action.C
        previous_round_index = -1
        cooperation_count_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
        my_cooperation_prev_round = history.my_actions[previous_round_index]
        total_cooperators_prev_round = cooperation_count_prev_round + my_cooperation_prev_round
        threshold_met_prev_round = total_cooperators_prev_round >= m
        if threshold_met_prev_round:
            return Action.C
        elif total_cooperators_prev_round >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_104(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for the Collective Risk Dilemma.

    ATC cooperates by default in the first round and when historical cooperation
    has successfully met the threshold frequently. It adapts its cooperation
    threshold based on recent performance, becoming more cooperative if success
    rates are high and more cautious if cooperation frequently fails to meet the
    required threshold. It defects in the final round to maximize individual gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATC strategy.

        Args:
            game_description: An object containing the game's parameters.
        """
        self.game_description = game_description
        self.W = math.ceil(game_description.n_rounds / 4)
        self.P_coop_initial = game_description.m / game_description.n_players
        self.P_coop_adjustment_step = 0.01
        self.P_coop = self.P_coop_initial

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: Historical data from previous rounds, or None for the first round.

        Returns:
            The action to take: Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_number = state.round_number
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is None:
            self.P_coop = self.P_coop_initial
            return Action.C
        recent_history_start_round = max(0, current_round_number - self.W)
        cooperation_met_count = 0
        total_recent_rounds = 0
        for t in range(recent_history_start_round, current_round_number):
            num_cooperators_in_round_t = 1 if history.my_actions[t] else 0
            num_cooperators_in_round_t += np.sum(history.opponent_actions[t, :])
            if num_cooperators_in_round_t >= self.game_description.m:
                cooperation_met_count += 1
            total_recent_rounds += 1
        recent_cooperation_success_ratio = 0.0
        if total_recent_rounds > 0:
            recent_cooperation_success_ratio = cooperation_met_count / total_recent_rounds
        if recent_cooperation_success_ratio >= self.P_coop:
            self.P_coop = min(1.0, self.P_coop + self.P_coop_adjustment_step)
        else:
            self.P_coop = max(self.game_description.m / self.game_description.n_players, self.P_coop - self.P_coop_adjustment_step)
        if recent_cooperation_success_ratio >= self.P_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_105(BaseStrategy):
    """
    The "Threshold Weaver" strategy for the Collective Risk Dilemma.
    This strategy aims to foster cooperation by adapting to the group's behavior.
    It starts by cooperating, then cooperates if the threshold was met or nearly met
    in the previous round. Otherwise, it defects to protect against exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.C
        cooperators_last_round = np.sum(history.my_actions[current_round - 1]) + np.sum(history.opponent_actions[current_round - 1])
        if current_round == self.r - 1:
            if cooperators_last_round >= self.m:
                return Action.C
            else:
                return Action.D
        if cooperators_last_round >= self.m:
            return Action.C
        elif cooperators_last_round >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_106(BaseStrategy):
    """
    "Adaptive Threshold Seeker" strategy for the Collective Risk Dilemma.
    This strategy aims to foster cooperation by starting cooperatively,
    then dynamically adjusting based on observed cooperation levels.
    It prioritizes reaching the cooperation threshold 'm' while mitigating
    exploitation risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_trend_threshold_factor = 0.75
        self.smoothed_coop_ratio = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for this player and opponents.
                     None for the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        num_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        total_players_prev_round = self.n_players
        current_coop_ratio = num_cooperators_prev / total_players_prev_round
        alpha = 0.3
        if current_round == 1:
            self.smoothed_coop_ratio = current_coop_ratio
        else:
            self.smoothed_coop_ratio = alpha * current_coop_ratio + (1 - alpha) * self.smoothed_coop_ratio
        if num_cooperators_prev >= self.m:
            return Action.C
        if num_cooperators_prev == self.m - 1:
            return Action.C
        required_ratio = self.m / self.n_players
        if self.smoothed_coop_ratio > required_ratio * self.cooperation_trend_threshold_factor:
            if current_round < self.n_rounds:
                return Action.C
            else:
                return Action.C
        if current_round == self.n_rounds:
            return Action.D
        return Action.D

class Strategy_COOPERATIVE_107(BaseStrategy):
    """
    Adaptive Cooperative Threshold Seeker (ACTS).

    This strategy aims to foster cooperation by balancing the desire for collective
    rewards with the risk of individual defection. It adapts its behavior based on
    observed cooperation levels, aiming to push the group towards the cooperative
    threshold without being exploited.

    Core Principles:
    1. Cautious Cooperation: Initially, err on the side of caution but cooperate
       when the group shows signs of moving towards the threshold.
    2. Threshold Reinforcement: Once the threshold is met or nearly met,
       reinforce that behavior by continuing to cooperate.
    3. Punishment for Non-Cooperation: If cooperation falters significantly,
       reduce cooperation to minimize individual losses.
    4. Future-Oriented: Considers the repeated nature of the game.

    Decision Rule:
    - Cooperate (C) if the observed number of cooperators in the previous round
      was >= (m - 1).
    - Defect (D) if the observed number of cooperators in the previous round
      was < (m - 1).
    - For the first round, always cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D).

        Args:
            state: The current game state.
            history: Player history for previous rounds. None if it's the first round.

        Returns:
            Action.C or Action.D.
        """
        if history is None:
            return Action.C
        else:
            my_last_action = self.game_description.n_players
            if self.n > 1:
                my_last_action_is_cooperate = history.my_actions[-1]
                opponent_cooperators = np.sum(history.opponent_actions[-1, :])
                observed_cooperators_prev_round = opponent_cooperators + (1 if my_last_action_is_cooperate else 0)
            else:
                observed_cooperators_prev_round = 1 if history.my_actions[-1] else 0
            if observed_cooperators_prev_round >= self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_108(BaseStrategy):
    """
    "Cautious Reciprocity with Threshold Awareness" strategy for the Collective Risk Dilemma.
    This strategy aims to be cooperative by default, but adjusts its behavior based on
    the observed cooperation rate in the previous round relative to the required threshold (m)
    and the balance between cooperators and defectors.

    It cooperates in the first round and defects in the last round.
    In intermediate rounds, it cooperates if the previous round met the threshold 'm' AND
    the number of cooperators strictly exceeded the number of defectors. Otherwise, it defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None if it's the first (zeroth) round before any actions are taken.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None or current_round_number == 0:
            return Action.C
        prev_round_idx = history.round_number - 1
        my_prev_action_is_c = history.my_actions[prev_round_idx]
        prev_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        prev_cooperators_count = prev_opponent_cooperators + (1 if my_prev_action_is_c else 0)
        prev_defectors_count = self.n - prev_cooperators_count
        met_threshold = prev_cooperators_count >= self.m
        cooperation_trending_up = prev_cooperators_count > prev_defectors_count
        if met_threshold and cooperation_trending_up:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_109(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Cooperative Threshold Awareness (ATT-CTA).
    This strategy aims to foster cooperation by rewarding it and punishing defection,
    while remaining acutely aware of the collective risk and the parameters governing success.
    It balances a desire for cooperation with a pragmatic approach to ensure individual gains
    are not jeopardized by others' non-compliance.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.opponent_defection_streaks = {}
        self.m_adaptive = self.m
        self.cooperation_trend_memory = []
        self.last_cooperators_count = 0
        self.PUNISHMENT_STREAK_THRESHOLD = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.opponent_defection_streaks = {i: 0 for i in range(self.n - 1)}
            self.m_adaptive = self.m
            self.cooperation_trend_memory = []
            self.last_cooperators_count = 0
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        my_prev_action = self.my_action_in_prev_round(history)
        cooperators_prev_round = self.count_cooperators_prev_round(history)
        self.last_cooperators_count = cooperators_prev_round
        self.cooperation_trend_memory.append(cooperators_prev_round)
        if len(self.cooperation_trend_memory) > 5:
            self.cooperation_trend_memory.pop(0)
        if history.opponent_actions.shape[0] > 0:
            latest_opponent_actions = history.opponent_actions[-1, :]
            for i in range(self.n - 1):
                if latest_opponent_actions[i] == Action.D.value:
                    self.opponent_defection_streaks[i] = self.opponent_defection_streaks.get(i, 0) + 1
                else:
                    self.opponent_defection_streaks[i] = 0
        if cooperators_prev_round >= self.m:
            return Action.C
        if cooperators_prev_round < self.m and my_prev_action == Action.C:
            return Action.D
        if cooperators_prev_round < self.m and my_prev_action == Action.D:
            if len(self.cooperation_trend_memory) >= 2:
                current_coop_count = self.cooperation_trend_memory[-1]
                prev_coop_count = self.cooperation_trend_memory[-2]
                if current_coop_count > prev_coop_count:
                    self.m_adaptive = max(1, self.m_adaptive - 1)
                elif current_coop_count <= prev_coop_count:
                    self.m_adaptive = min(self.m, self.m_adaptive + 1)
            else:
                self.m_adaptive = self.m
            if cooperators_prev_round >= self.m_adaptive:
                return Action.C
            else:
                return Action.D
        for i in range(self.n - 1):
            if self.opponent_defection_streaks.get(i, 0) >= self.PUNISHMENT_STREAK_THRESHOLD:
                return Action.D
        return Action.D

    def my_action_in_prev_round(self, history: PlayerHistory) -> Action:
        """Helper to get my action in the most recent completed round."""
        if history.my_actions.size == 0:
            return Action.D
        return Action(history.my_actions[-1])

    def count_cooperators_prev_round(self, history: PlayerHistory) -> int:
        """Helper to count cooperators in the most recent completed round."""
        if history.opponent_actions.shape[0] == 0:
            return 0
        latest_opponent_actions = history.opponent_actions[-1, :]
        cooperators = np.sum(latest_opponent_actions)
        return cooperators

class Strategy_COOPERATIVE_110(BaseStrategy):
    """
    "Conditional Generosity with Vigilance": A cooperative strategy that aims for
    collective benefit by rewarding cooperation and cautiously punishing defection.
    It adapts based on the observed success of cooperation over a lookback window.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.lookback_window = 4
        self.cooperation_threshold_high = 0.8
        self.cooperation_threshold_low = 0.5
        self.my_actions_history = []
        self.observed_cooperators_per_round = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number = state.round_number
        if history is None:
            return Action.C
        num_past_rounds = history.round_number
        self.my_actions_history = list(history.my_actions)
        if len(self.observed_cooperators_per_round) < num_past_rounds:
            self.observed_cooperators_per_round = []
            for round_idx in range(num_past_rounds):
                cooperators_this_round = np.sum(history.opponent_actions[round_idx])
                self.observed_cooperators_per_round.append(cooperators_this_round)
        if len(self.observed_cooperators_per_round) < num_past_rounds:
            cooperators_this_round = np.sum(history.opponent_actions[num_past_rounds - 1])
            self.observed_cooperators_per_round.append(cooperators_this_round)
        if current_round_number == r:
            return Action.D
        recent_cooperation_history = self.observed_cooperators_per_round[max(0, num_past_rounds - self.lookback_window):]
        if not recent_cooperation_history:
            cooperation_rate_past_rounds = 0.0
        else:
            total_cooperators = np.sum(recent_cooperation_history)
            total_player_observations = len(recent_cooperation_history) * n
            cooperation_rate_past_rounds = total_cooperators / total_player_observations if total_player_observations > 0 else 0.0
        current_round_cooperation = 0
        if num_past_rounds > 0:
            current_round_cooperation = self.observed_cooperators_per_round[-1]
        if cooperation_rate_past_rounds >= self.cooperation_threshold_high:
            return Action.C
        if num_past_rounds > 0 and current_round_cooperation >= m:
            return Action.C
        if num_past_rounds > 0:
            consecutive_failures = 0
            checks_to_perform = min(3, num_past_rounds)
            for i in range(checks_to_perform):
                if self.observed_cooperators_per_round[-(i + 1)] < m:
                    consecutive_failures += 1
            if consecutive_failures >= 2 and cooperation_rate_past_rounds < self.cooperation_threshold_low:
                return Action.D
        if num_past_rounds > 0 and current_round_cooperation < m and (cooperation_rate_past_rounds < m / n):
            return Action.D
        if num_past_rounds > 0:
            return Action(self.my_actions_history[-1])
        else:
            return Action.C

class Strategy_COOPERATIVE_111(BaseStrategy):
    """
    Strategy: "Conditional Cooperate with Escalating Trust"

    This strategy aims to initiate cooperation, maintain it as long as the group's
    collective behavior warrants it, and adapt to circumstances. It builds trust
    by observing the success of cooperation and punishes defection by temporarily
    reverting to defection itself, with an eye towards re-establishing cooperation.
    An escalation mechanism is included to break cycles of mutual defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_escalation = 3
        self.my_action_t_minus_1 = Action.D
        self.consecutive_defections_by_others = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            self.my_action_t_minus_1 = Action.C
            return Action.C
        current_round = history.round_number
        my_action_prev_round = self.my_action_t_minus_1
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        num_cooperators_prev_round = np.sum(opponent_actions_prev_round == Action.C) + (1 if my_action_prev_round == Action.C else 0)
        M_cooperated_prev_round = num_cooperators_prev_round >= m
        decided_action = Action.D
        if M_cooperated_prev_round:
            self.consecutive_defections_by_others = 0
            if my_action_prev_round == Action.C:
                decided_action = Action.C
            else:
                decided_action = Action.C
        elif my_action_prev_round == Action.C:
            decided_action = Action.D
        else:
            all_others_defected = np.all(opponent_actions_prev_round == Action.D)
            if all_others_defected:
                self.consecutive_defections_by_others += 1
            else:
                self.consecutive_defections_by_others = 0
            if self.consecutive_defections_by_others < self.threshold_escalation:
                decided_action = Action.D
            else:
                decided_action = Action.C
        self.my_action_t_minus_1 = decided_action
        return decided_action

class Strategy_COOPERATIVE_112(BaseStrategy):
    """
    Adaptive Cooperative Risk Averter (ACRA) strategy for the Collective Risk Dilemma.

    ACRA is a conditionally cooperative strategy that aims to maximize payoffs by
    pursuing cooperation when it's likely to succeed, and defecting to protect
    its own endowment when cooperation is improbable or has repeatedly failed.
    It dynamically estimates the probability of meeting the cooperation threshold
    based on historical data, recent trends, and the number of cooperators in the
    previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_threshold_for_defect_decision = max(1, self.game_description.m - 2)
        self.trend_window = 3
        self.last_chance_cooperation_threshold = max(1, self.game_description.m - 1)
        self.initial_cooperation_rounds = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_num = state.round_number
        num_completed_rounds = history.round_number
        if current_round_num < self.initial_cooperation_rounds:
            return Action.C
        num_cooperators_prev_round = 0
        if num_completed_rounds > 0:
            prev_round_actions = np.concatenate([[self.game_description.n_players > 0 and history.my_actions[num_completed_rounds - 1]], history.opponent_actions[num_completed_rounds - 1, :]])
            num_cooperators_prev_round = np.sum(prev_round_actions)
        recent_cooperator_counts = []
        if num_completed_rounds >= self.trend_window:
            for i in range(num_completed_rounds - self.trend_window, num_completed_rounds):
                round_actions = np.concatenate([[self.game_description.n_players > 0 and history.my_actions[i]], history.opponent_actions[i, :]])
                recent_cooperator_counts.append(np.sum(round_actions))
        elif num_completed_rounds > 0:
            for i in range(num_completed_rounds):
                round_actions = np.concatenate([[self.game_description.n_players > 0 and history.my_actions[i]], history.opponent_actions[i, :]])
                recent_cooperator_counts.append(np.sum(round_actions))
        increasing_trend = False
        decreasing_trend = False
        if len(recent_cooperator_counts) >= 2:
            if recent_cooperator_counts[-1] > recent_cooperator_counts[-2]:
                increasing_trend = True
            elif recent_cooperator_counts[-1] < recent_cooperator_counts[-2]:
                decreasing_trend = True
        if current_round_num == r - 1:
            if num_cooperators_prev_round >= m:
                return Action.C
            elif num_cooperators_prev_round >= self.last_chance_cooperation_threshold:
                return Action.C
            else:
                return Action.D
        if num_cooperators_prev_round >= m:
            return Action.C
        if num_cooperators_prev_round >= self.cooperate_threshold_for_defect_decision:
            if increasing_trend:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_113(BaseStrategy):
    """
    Adaptive Threshold Responder strategy for the Collective Risk Dilemma.

    This strategy defaults to cooperation, but adjusts its behavior based on
    the observed cooperation rate of the group in previous rounds. It maintains
    a 'trust_level' that influences its decision-making. It aims to reach the
    cooperation threshold ('m') by rewarding good group behavior and punishing
    consistent failure to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.trust_level = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players. None for the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        previous_round_actions = history.opponent_actions[current_round - 1]
        cooperators_last_round = np.sum(previous_round_actions)
        observed_cooperation_rate = cooperators_last_round / self.n
        if observed_cooperation_rate >= self.m / self.n:
            self.trust_level = min(1.0, self.trust_level + 0.1)
        elif self.m > 2 and observed_cooperation_rate < (self.m - 1) / self.n:
            self.trust_level = max(0.0, self.trust_level - 0.2)
        if self.m == self.n:
            return Action.D
        if current_round < self.r - 1:
            if self.trust_level >= 0.5:
                return Action.C
            elif observed_cooperation_rate >= self.m / self.n:
                return Action.C
            else:
                return Action.D
        elif observed_cooperation_rate >= self.m / self.n:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_114(BaseStrategy):
    """
    Conditional Reciprocity with Minimum Commitment (CRMC) strategy.
    This strategy aims to be cooperative by initially cooperating,
    reciprocating observed cooperation, and attempting to push the group
    over the cooperation threshold when close. It defects in the last round
    and punishes sustained defection by eventually defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the CRMC strategy.

        Args:
            game_description: An object containing the game's parameters.
        """
        self.game_description = game_description
        self.initial_cooperation_rounds = max(1, min(5, math.floor(game_description.n_rounds / 10)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of play up to the previous round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_num = state.round_number
        if current_round_num == r:
            return Action.D
        if history is None:
            return Action.C
        previous_round_actions = history.opponent_actions[current_round_num - 1]
        my_action_prev_round = history.my_actions[current_round_num - 1]
        num_cooperators_prev_round = np.sum(previous_round_actions)
        if num_cooperators_prev_round >= m:
            return Action.C
        if current_round_num <= self.initial_cooperation_rounds:
            return Action.C
        if num_cooperators_prev_round >= m - 1 and my_action_prev_round:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_115(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Threshold Focus for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by rewarding collective success
    and punishing individual defections, with a keen eye on the crucial
    cooperation threshold 'm'. It adapts its behavior based on the observed
    success of the group in meeting the threshold in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        cooperators_in_previous_round = 0
        if self.game_description.n_rounds > 0 and len(history.my_actions) > previous_round_index:
            if history.my_actions[previous_round_index]:
                cooperators_in_previous_round += 1
        if self.game_description.n_rounds > 0 and len(history.opponent_actions) > previous_round_index:
            cooperators_in_previous_round += np.sum(history.opponent_actions[previous_round_index, :])
        if cooperators_in_previous_round >= self.m:
            return Action.C
        else:
            my_action_in_previous_round = False
            if self.game_description.n_rounds > 0 and len(history.my_actions) > previous_round_index:
                my_action_in_previous_round = history.my_actions[previous_round_index]
            if not my_action_in_previous_round:
                return Action.D
            else:
                return Action.C

class Strategy_COOPERATIVE_116(BaseStrategy):
    """
    "Trust and Verify" Strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by initially trusting the group,
    but rigorously verifying that trust through observed actions. It is
    inherently cooperative but pragmatic, seeking the collectively beneficial
    outcome while safeguarding against exploitation.

    Core Principles:
    1. Pro-Cooperation Default: Start by assuming others will cooperate.
    2. Vigilant Verification: Continuously monitor group behavior.
    3. Punish Defection, Reward Cooperation: Respond to observed defection
       with defection, and to sustained cooperation with cooperation.
    4. Forgiveness (with a Limit): Offer a path back to cooperation for
       defectors if the group demonstrates a strong commitment.
    5. Parameter Awareness: Adjust behavior based on game parameters (m, k).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.punishment_phase = False
        self.forgiveness_window_active = False
        self.punishment_start_round = -1
        self.punishment_threshold_rounds = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        total_cooperators_last_round = 0
        if history.round_number > 0:
            if history.my_actions[-1] == Action.C.value:
                total_cooperators_last_round += 1
            total_cooperators_last_round += np.sum(history.opponent_actions[-1, :])
        if state.round_number == self.r:
            return Action.D
        if self.forgiveness_window_active:
            if total_cooperators_last_round >= self.m and history.my_actions[-1] == Action.C.value:
                self.forgiveness_window_active = False
                self.punishment_phase = False
                return Action.C
            else:
                self.forgiveness_window_active = False
                self.punishment_phase = True
                self.punishment_start_round = state.round_number
                return Action.D
        if self.punishment_phase:
            if total_cooperators_last_round >= self.m and history.my_actions[-1] == Action.D.value:
                self.punishment_phase = False
                self.forgiveness_window_active = True
                return Action.C
            else:
                return Action.D
        if total_cooperators_last_round >= self.m:
            return Action.C
        if total_cooperators_last_round < self.m:
            if self.punishment_start_round == -1:
                self.punishment_start_round = state.round_number
            elif state.round_number - self.punishment_start_round >= self.punishment_threshold_rounds:
                self.punishment_phase = True
                self.punishment_start_round = state.round_number
                return Action.D
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_117(BaseStrategy):
    """
    Adaptive Threshold Cooperate (ATC) strategy for the Collective Risk Dilemma.
    This strategy aims to promote cooperation by adapting its behavior based on
    the observed cooperation rate in previous rounds. It cooperates if the
    threshold 'm' was met or is likely to be met. If the threshold was not met
    but was close, and the player cooperated, it "leads" by cooperating again.
    Otherwise, it defects to secure individual payoff. It always cooperates in
    the first round and defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        prev_round_idx = history.round_number - 1
        num_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :]) + history.my_actions[prev_round_idx]
        player_action_prev = Action.C if history.my_actions[prev_round_idx] else Action.D
        if num_cooperators_prev >= self.m:
            return Action.C
        if num_cooperators_prev < self.m and self.m - num_cooperators_prev <= self.n - self.m and (player_action_prev == Action.C):
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_118(BaseStrategy):
    """
    The "Adaptive Threshold Seeker" strategy for the Collective Risk Dilemma.

    This strategy aims to foster cooperation by prioritizing the achievement of the
    cooperation threshold `m`. It adapts its behavior based on historical rounds,
    initiating cooperation, conditionally cooperating when the threshold seems
    achievable, and employing a temporary punishment mechanism if cooperation fails
    significantly after the player attempted to cooperate. It always defects in the
    last round.

    Key features:
    - Always cooperates in the first round.
    - Always defects in the last round.
    - Explores cooperation in initial rounds with probability adjusted by previous cooperation.
    - In later rounds, makes decisions based on recent average cooperation rates and
      the number of cooperators in the previous round.
    - Implements a temporary punishment phase by increasing defection probability
      if the threshold is not met despite the player cooperating and the count
      being significantly below `m`.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.R_init = 3
        self.R_history = 5
        self.punishment_rounds = 2
        self.punishment_threshold_multiplier = 0.7
        self.punishment_active = False
        self.punishment_end_round = 0

    def _count_cooperators_in_round(self, actions: NDArray[np.bool_]) -> int:
        """Counts the number of cooperators (True values) in an action array."""
        return np.sum(actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action for the current round.

        Args:
            state: The current state of the game (round number).
            history: Historical data from previous rounds, or None if it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.r - 1:
            return Action.D
        if self.punishment_active and current_round_number < self.punishment_end_round:
            return Action.D
        C_prev = self._count_cooperators_in_round(history.opponent_actions[-1, :])
        if history.my_actions is not None:
            C_prev += history.my_actions[-1]
        num_rounds_played = history.round_number
        if num_rounds_played < self.R_init:
            if C_prev >= self.m - 1:
                return Action.C
            else:
                prob_cooperate = min(1.0, max(0.0, C_prev / (self.m - 1)))
                if random.random() < prob_cooperate:
                    return Action.C
                else:
                    return Action.D
        else:
            start_index = max(0, num_rounds_played - self.R_history)
            recent_rounds_coop_counts_sum = np.sum(history.my_actions[start_index:]) + np.sum(history.opponent_actions[start_index:, :])
            total_actions_in_history = (num_rounds_played - start_index) * self.n
            if total_actions_in_history == 0:
                P_cooperate_expected = 0
            else:
                P_cooperate_expected = recent_rounds_coop_counts_sum / total_actions_in_history
            expected_cooperators_based_on_avg = self.n * P_cooperate_expected
            if C_prev >= self.m - 1 and expected_cooperators_based_on_avg >= self.m - 1:
                return Action.C
            elif C_prev < self.m - 1 and expected_cooperators_based_on_avg < self.m - 1:
                return Action.D
            elif C_prev >= self.m - 1 and expected_cooperators_based_on_avg < self.m - 1:
                return Action.D
            elif C_prev < self.m - 1 and expected_cooperators_based_on_avg >= self.m - 1:
                return Action.C
            elif expected_cooperators_based_on_avg >= self.m - 1:
                return Action.C
            else:
                return Action.D
        if current_round_number > 0:
            completed_round_idx = current_round_number - 1
            if completed_round_idx >= 0:
                if history.my_actions is not None and history.opponent_actions is not None:
                    coop_count_in_prev_round = history.my_actions[completed_round_idx]
                    coop_count_in_prev_round += np.sum(history.opponent_actions[completed_round_idx, :])
                    player_cooperated_in_prev_round = history.my_actions[completed_round_idx]
                    if player_cooperated_in_prev_round and coop_count_in_prev_round < self.m and (coop_count_in_prev_round < self.m * self.punishment_threshold_multiplier):
                        self.punishment_active = True
                        self.punishment_end_round = current_round_number + self.punishment_rounds
        if current_round_number > 0:
            completed_round_idx = current_round_number - 1
            if completed_round_idx >= 0 and history.my_actions is not None and (history.opponent_actions is not None):
                coop_count_in_completed_round = history.my_actions[completed_round_idx]
                coop_count_in_completed_round += np.sum(history.opponent_actions[completed_round_idx, :])
                player_action_in_completed_round = history.my_actions[completed_round_idx]
                if player_action_in_completed_round and coop_count_in_completed_round < self.m and (coop_count_in_completed_round < self.m * self.punishment_threshold_multiplier):
                    self.punishment_active = True
                    self.punishment_end_round = current_round_number + self.punishment_rounds
        if self.punishment_active and current_round_number < self.punishment_end_round:
            return Action.D
        action_decision = Action.D
        if current_round_number == 0:
            action_decision = Action.C
        elif current_round_number == self.r - 1:
            action_decision = Action.D
        elif self.punishment_active and current_round_number < self.punishment_end_round:
            action_decision = Action.D
        else:
            C_prev = self._count_cooperators_in_round(history.opponent_actions[-1, :])
            if history.my_actions is not None:
                C_prev += history.my_actions[-1]
            num_rounds_played = history.round_number
            if num_rounds_played < self.R_init:
                if C_prev >= self.m - 1:
                    action_decision = Action.C
                else:
                    prob_cooperate = min(1.0, max(0.0, C_prev / (self.m - 1)))
                    if random.random() < prob_cooperate:
                        action_decision = Action.C
                    else:
                        action_decision = Action.D
            else:
                start_index = max(0, num_rounds_played - self.R_history)
                recent_rounds_coop_counts_sum = np.sum(history.my_actions[start_index:]) + np.sum(history.opponent_actions[start_index:, :])
                total_actions_in_history = (num_rounds_played - start_index) * self.n
                if total_actions_in_history == 0:
                    P_cooperate_expected = 0
                else:
                    P_cooperate_expected = recent_rounds_coop_counts_sum / total_actions_in_history
                expected_cooperators_based_on_avg = self.n * P_cooperate_expected
                if C_prev >= self.m - 1 and expected_cooperators_based_on_avg >= self.m - 1:
                    action_decision = Action.C
                elif C_prev < self.m - 1 and expected_cooperators_based_on_avg < self.m - 1:
                    action_decision = Action.D
                elif C_prev >= self.m - 1 and expected_cooperators_based_on_avg < self.m - 1:
                    action_decision = Action.D
                elif C_prev < self.m - 1 and expected_cooperators_based_on_avg >= self.m - 1:
                    action_decision = Action.C
                elif expected_cooperators_based_on_avg >= self.m - 1:
                    action_decision = Action.C
                else:
                    action_decision = Action.D
        if current_round_number > 0:
            completed_round_idx = current_round_number - 1
            if completed_round_idx >= 0 and history.my_actions is not None and (history.opponent_actions is not None):
                coop_count_in_completed_round = history.my_actions[completed_round_idx]
                coop_count_in_completed_round += np.sum(history.opponent_actions[completed_round_idx, :])
                player_action_in_completed_round = history.my_actions[completed_round_idx]
                if player_action_in_completed_round and coop_count_in_completed_round < self.m and (coop_count_in_completed_round < self.m * self.punishment_threshold_multiplier):
                    self.punishment_active = True
                    self.punishment_end_round = current_round_number + self.punishment_rounds
        return action_decision

class Strategy_COOPERATIVE_119(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Tit-for-Tat with Threshold Awareness"

    This strategy aims to foster cooperation in the Collective Risk Dilemma by:
    1. Always initiating cooperation in the first round.
    2. Continuing to cooperate if the previous round was successful (threshold m met).
    3. Cooperating if the previous round failed but the player's own cooperation
       would have been sufficient to meet the threshold ("last chance" cooperation).
    4. Defecting if the previous round failed and the player's cooperation was
       not enough to meet the threshold, or if the player defected in a failing round.
    5. Defecting in the final round to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        prev_round_idx = current_round - 1
        my_action_prev = history.my_actions[prev_round_idx]
        num_cooperators_prev = np.sum(history.my_actions[:current_round])
        my_cooperation_would_have_been_enough = num_cooperators_prev + (0 if my_action_prev else 1) >= self.m
        if num_cooperators_prev >= self.m:
            if my_action_prev:
                return Action.C
            else:
                return Action.C
        elif my_action_prev:
            if my_cooperation_would_have_been_enough:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_120(BaseStrategy):
    """
    The Cautious Cooperator strategy aims for cooperation to maximize collective benefit,
    but with clear safeguards to prevent exploitation. It prioritizes achieving the
    cooperative threshold while remaining flexible based on observed play.

    Decision Rules:
    - Round 1: Cooperate (C) - Initiate cooperation.
    - Last Round (t=r): Defect (D) - Exploit knowledge of no future rounds.
    - Intermediate Rounds (1 < t < r):
        - If m or more players cooperated in the previous round: Cooperate (C) - Continue successful cooperation.
        - If fewer than m players cooperated in the previous round: Defect (D) - Wait for stronger evidence of cooperation to avoid exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._cooperative_threshold_met_last_round = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.game_description.n_rounds:
            return Action.D
        if self._cooperative_threshold_met_last_round:
            return Action.C
        else:
            return Action.D

    def update_state(self, round_outcome: dict):
        """
        Updates the internal state based on the outcome of a round.

        Args:
            round_outcome (dict): A dictionary representing the outcome of the last played round.
                                  Expected keys: "round", "num_cooperators".
                                  "round" is 0-indexed for the completed round.
        """
        num_cooperators = round_outcome.get('num_cooperators', 0)
        m = self.game_description.m
        if num_cooperators >= m:
            self._cooperative_threshold_met_last_round = True
        else:
            self._cooperative_threshold_met_last_round = False

class Strategy_COOPERATIVE_121(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Reciprocity with Escalation"

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a strong bias towards cooperating, especially when the threshold is likely
    to be met. It incorporates learning from past round outcomes to adjust its behavior.

    Core Philosophy:
    The strategy believes that the community project is beneficial when successful (m or more cooperators).
    Therefore, it prioritizes contributing to that success. However, it also recognizes that
    free-riding (defecting when others cooperate) is detrimental. The strategy seeks to balance
    the desire for collective good with the need to deter exploitative behavior.

    Decision Rules:
    - Cooperate in the first round.
    - Cooperate in the last round.
    - In intermediate rounds:
        - If the previous round met the cooperation threshold (>= m cooperators), Cooperate.
        - If the previous round was close to the threshold (>= m-1 cooperators), Cooperate.
        - If the number of cooperators in the previous round was very low (<= m/2), Cooperate,
          assuming individual cooperation is pivotal.
        - Otherwise, if there was significant defection in the previous round (> n/2 defectors)
          and the threshold was not met, Defect.
        - Otherwise, default to Cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.1
        self.cooperation_score = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.cooperation_score = 0.0
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        my_prev_action = self.my_actions_to_bool(history.my_actions[prev_round_idx])
        opponent_prev_actions = history.opponent_actions[prev_round_idx, :]
        num_cooperators_last_round = int(my_prev_action) + np.sum(opponent_prev_actions)
        num_defectors_last_round = self.n_players - num_cooperators_last_round
        cooperation_proportion = num_cooperators_last_round / self.n_players
        self.cooperation_score = self.alpha * cooperation_proportion + (1 - self.alpha) * self.cooperation_score
        if num_cooperators_last_round >= self.m:
            return Action.C
        elif num_cooperators_last_round >= self.m - 1:
            return Action.C
        elif num_cooperators_last_round <= self.m / 2:
            return Action.C
        elif num_defectors_last_round > self.n_players / 2 and num_cooperators_last_round < self.m:
            return Action.D
        else:
            return Action.C

    def my_actions_to_bool(self, action_enum):
        """Helper to convert Action enum to boolean (True for C, False for D)."""
        return action_enum == Action.C

class Strategy_COOPERATIVE_122(BaseStrategy):
    """
    Threshold Reciprocity Strategy:
    Cooperates in early rounds to signal intent.
    In later rounds, cooperates if the previous round met the cooperation threshold,
    otherwise defects to protect endowment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing the game's parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players.
                     None for the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        current_round = state.round_number
        game_round = current_round + 1
        n = self.game_description.n_players
        m = self.game_description.m
        if game_round < m:
            return Action.C
        else:
            previous_round_index = history.round_number - 1
            my_last_action = Action.C if history.my_actions[previous_round_index] else Action.D
            opponents_last_actions = history.opponent_actions[previous_round_index, :]
            cooperators_last_round = int(my_last_action == Action.C) + int(np.sum(opponents_last_actions))
            if cooperators_last_round >= m:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_123(BaseStrategy):
    """
    The Conditional Opportunist strategy.
    This strategy aims to be cooperative by default, adjusting its behavior based on
    observed collective actions. It seeks to exploit opportunities for mutual gain
    while safeguarding against exploitation.

    Decision Rules:
    1. Default Action: Cooperate (C) - Assumes others are inclined towards cooperation.
    2. Conditional Defection Trigger: Risk of Wasted Cooperation.
       - If the observed cooperation rate in recent rounds has consistently fallen
         significantly short of the threshold 'm'.
       - If there's an indication of players "pulling the rug" (e.g., a sudden increase
         in defections when close to 'm').
    3. Handling Edge Cases:
       - First Round (t=1): Cooperate (C) - No historical data.
       - Last Round (t=r): Cooperate (C) - Commitment to cooperation.
       - Rounds where 'm' is easily met: Cooperate (C).
       - Rounds where 'm' is clearly impossible: Defect (D).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.observed_cooperators_per_round: list[int] = []
        self.recent_cooperation_rates: list[float] = []
        self.cooperation_threshold_ratio = self.m / self.n_players
        self.low_cooperation_trigger_ratio = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_number = state.round_number
        cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        self.observed_cooperators_per_round.append(int(cooperators_last_round))
        recent_rounds_data = self.observed_cooperators_per_round[-min(5, len(self.observed_cooperators_per_round)):]
        if recent_rounds_data:
            avg_recent_cooperators = np.mean(recent_rounds_data)
            avg_recent_cooperation_rate = avg_recent_cooperators / self.n_players
            self.recent_cooperation_rates.append(avg_recent_cooperation_rate)
        else:
            self.recent_cooperation_rates.append(0.0)
        if current_round_number == self.n_rounds - 1:
            return Action.C
        if cooperators_last_round >= self.m - 1:
            return Action.C
        if self.recent_cooperation_rates:
            moving_avg_coop_rate = np.mean(self.recent_cooperation_rates[-min(5, len(self.recent_cooperation_rates)):])
            if moving_avg_coop_rate < self.cooperation_threshold_ratio * self.low_cooperation_trigger_ratio:
                return Action.D
        return Action.C

class Strategy_COOPERATIVE_124(BaseStrategy):
    """
    Threshold Trust Tit-for-Tat Strategy:
    This strategy cooperates in the first round to initiate cooperation.
    In subsequent rounds, it cooperates if the group met the cooperation
    threshold (m cooperators) in the previous round, and defects otherwise.
    This approach aims to reinforce successful collective action and avoid
    exploitation when the group fails to reach the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        previous_round_idx = history.round_number - 1
        num_cooperators_previous_round = 0
        if self.game_description.n_players > 0:
            if history.my_actions[previous_round_idx] == Action.C:
                num_cooperators_previous_round += 1
        if history.opponent_actions.shape[1] > 0:
            num_cooperators_previous_round += np.sum(history.opponent_actions[previous_round_idx, :] == Action.C)
        if num_cooperators_previous_round >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_125(BaseStrategy):
    """
    The "Conditional Threshold Monitor" strategy.
    This strategy monitors collective cooperation levels and adapts its behavior.
    It initially explores, then cooperates if the threshold was met in the previous round,
    or if confidence in cooperation remains above a certain level. It defects if
    confidence drops too low or in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.confidence_level = 0.3
        self.confidence_increment = 0.1
        self.confidence_decrement = 0.15
        self.initial_rounds_to_explore = max(2, math.floor(self.n / 2))
        self.last_round_cooperators = 0
        self.total_defecters_last_round = 0
        self.my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if current_round < self.initial_rounds_to_explore:
            self.my_last_action = Action.C
            return Action.C
        if history is not None:
            self.last_round_cooperators = np.sum(history.my_actions[-1:] | history.opponent_actions[-1, :])
            self.total_defecters_last_round = self.n - self.last_round_cooperators
            self.my_last_action = Action.C if history.my_actions[-1] else Action.D
        if self.last_round_cooperators >= self.m:
            self.confidence_level = min(1.0, self.confidence_level + self.confidence_increment)
            action = Action.C
        elif self.confidence_level > 0.5:
            self.confidence_level = max(0.0, self.confidence_level - self.confidence_decrement / 2)
            action = Action.C
        else:
            self.confidence_level = max(0.0, self.confidence_level - self.confidence_decrement)
            action = Action.D
        if self.my_last_action == Action.D and self.total_defecters_last_round > self.n / 2 and (self.confidence_level <= 0.5):
            action = Action.D
        self.my_last_action = action
        return action

class Strategy_COOPERATIVE_126(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Monitoring (ACTM) strategy for the
    Collective Risk Dilemma.

    ACTM conditionally cooperates based on the perceived likelihood of reaching
    the cooperation threshold 'm', considering recent player behavior. It aims
    to achieve the collective reward while mitigating the risk of being the
    sole cooperator.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACTM strategy.

        Args:
            game_description: An object containing game parameters
                              (n_players, n_rounds, m, k).
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.w = max(3, math.ceil(self.n_rounds / 5))

    def _calculate_cooperation_rate(self, actions: NDArray[np.bool_]) -> float:
        """Calculates the proportion of cooperators in a given round's actions."""
        if len(actions) == 0:
            return 0.0
        return np.mean(actions)

    def _calculate_recent_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Calculates the average cooperation rate over the last 'w' rounds.

        Args:
            history: The PlayerHistory object containing past actions.

        Returns:
            The recent cooperation rate.
        """
        num_completed_rounds = history.round_number
        if num_completed_rounds == 0:
            return 0.0
        start_index = max(0, num_completed_rounds - self.w)
        recent_actions = history.my_actions[start_index:num_completed_rounds]
        if len(recent_actions) == 0:
            return 0.0
        total_cr = 0.0
        for i in range(len(recent_actions)):
            round_actions = np.append(history.opponent_actions[start_index + i, :], recent_actions[i])
            total_cr += self._calculate_cooperation_rate(round_actions)
        return total_cr / len(recent_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state (e.g., round number).
            history: The history of actions and payoffs up to the previous round.
                     If None, it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        recent_cr = self._calculate_recent_cooperation_rate(history)
        needed_from_others = self.m - 1
        if recent_cr >= needed_from_others / self.n_players:
            previous_round_cooperators = np.sum(history.opponent_actions[-1, :])
            if previous_round_cooperators >= needed_from_others or recent_cr > self.m / self.n_players + 0.05:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_127(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) Strategy for the Collective Risk Dilemma.

    This strategy aims to be cooperative by rewarding past cooperation,
    but is also robust by defecting if cooperation seems unlikely to succeed.

    Decision Rules:
    - Round 1: Cooperate (Explore)
    - Last Round (r): Defect (Maximize individual payoff)
    - Intermediate Rounds (1 < t < r):
        - Cooperate if (Number of cooperators in previous round) >= (m - 1)
        - Defect otherwise
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ATC strategy.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: Current game state, including the round number.
            history: History of past actions and payoffs, or None if it's the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        else:
            num_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if num_cooperators_previous_round >= self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_128(BaseStrategy):
    """
    Adaptive Reward Seeking strategy for the Collective Risk Dilemma.
    This strategy dynamically adjusts its probability of cooperation based on
    the observed success of cooperation in previous rounds. It aims to maximize
    individual payoff by seeking collective rewards when beneficial and safe,
    while protecting against exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_cooperation_probability = 0.6
        self.learning_rate = 0.05
        self.last_round_cooperation_success = 0
        self.previous_round_was_successful = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if state.round_number == r:
            return Action.D
        if history is None:
            self.last_round_cooperation_success = 0
            self.previous_round_was_successful = False
            if random.random() < self.current_cooperation_probability:
                return Action.C
            else:
                return Action.D
        num_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1] == Action.C.value:
            num_cooperators_in_prev_round += 1
        self.previous_round_was_successful = num_cooperators_in_prev_round >= m
        if self.previous_round_was_successful:
            self.current_cooperation_probability = min(1.0, self.current_cooperation_probability + self.learning_rate)
        else:
            self.current_cooperation_probability = max(0.0, self.current_cooperation_probability - self.learning_rate)
        if random.random() < self.current_cooperation_probability:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_1(BaseStrategy):
    """
    Aggressive Opportunist Strategy for the Collective Risk Dilemma.

    This strategy prioritizes exploiting the collective risk reward when achievable,
    but defaults to defection to maximize personal gain and avoid exploitation.
    It cooperates only when there's a strong perceived likelihood of meeting the
    cooperation threshold and the expected payoff is strictly greater than defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.aggression_factor = 0.7
        self.p_high = 0.8
        self.p_medium = 0.5
        self.p_low = 0.2
        self.p_baseline = 0.3
        self.opponent_states = [[] for _ in range(self.n - 1)]

    def _get_opponent_prob(self, opponent_idx: int, current_round: int) -> float:
        """
        Estimates the probability of an opponent cooperating based on their history.
        """
        if opponent_idx >= len(self.opponent_states):
            return self.p_baseline
        history = self.opponent_states[opponent_idx]
        if not history:
            return self.p_baseline
        last_action, last_threshold_met = history[-1]
        if last_action == Action.C.value:
            if last_threshold_met:
                return self.p_high
            else:
                return self.p_medium
        else:
            return self.p_low

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        if history is not None:
            num_completed_rounds = history.round_number
            if num_completed_rounds > 0:
                last_round_actions = history.opponent_actions[num_completed_rounds - 1, :]
                total_cooperators_last_round = np.sum(last_round_actions) + 1
                threshold_met_last_round = total_cooperators_last_round >= self.m
                for i in range(self.n - 1):
                    opponent_action_encoded = history.opponent_actions[num_completed_rounds - 1, i]
                    my_action_last_round = history.my_actions[num_completed_rounds - 1]
                    current_round_cooperators = np.sum(history.opponent_actions[num_completed_rounds - 1, :])
                    if history.my_actions[num_completed_rounds - 1] == Action.C.value:
                        current_round_cooperators += 1
                    threshold_was_met = current_round_cooperators >= self.m
                    self.opponent_states[i].append((opponent_action_encoded, threshold_was_met))
        sum_of_others_probs = 0.0
        for i in range(self.n - 1):
            sum_of_others_probs += self._get_opponent_prob(i, current_round)
        perceived_threshold_likelihood = sum_of_others_probs + 1.0
        cooperate_condition_met = perceived_threshold_likelihood >= self.m
        if cooperate_condition_met and self.k > 1:
            if sum_of_others_probs >= (self.m - 1) * self.aggression_factor and self.k > 1:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_2(BaseStrategy):
    """
    "The Threshold Predator" strategy for the Collective Risk Dilemma.

    This strategy is aggressive and adaptive. It aims to:
    1. Initiate cooperation in the first round to gather information.
    2. Defect in the last round to maximize immediate payoff.
    3. In intermediate rounds, defect if the previous round was close to
       or met the cooperation threshold (m), exploiting potential success.
    4. Cooperate if the previous round was significantly below the threshold,
       unless a punitive defection is warranted due to past exploitation.
    5. Punish players who have defected when the player cooperated in the past.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_cooperated_at_least_once = False
        self.others_exploited_me_in_past = False

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        current_round = state.round_number
        if history is None:
            self.my_cooperated_at_least_once = True
            return Action.C
        if current_round == self.n_rounds:
            return Action.D
        previous_round_actions = history.my_actions[current_round - 1]
        all_previous_actions = np.append(previous_round_actions, history.opponent_actions[current_round - 1, :])
        num_cooperators_prev = np.sum(all_previous_actions)
        if not self.my_cooperated_at_least_once and previous_round_actions:
            if num_cooperators_prev < self.m:
                opponent_actions_prev_round = history.opponent_actions[current_round - 1, :]
                if np.sum(opponent_actions_prev_round == Action.D.value) > 0:
                    self.others_exploited_me_in_past = True
        if num_cooperators_prev < self.m - 1:
            if self.others_exploited_me_in_past and num_cooperators_prev > 0:
                return Action.D
            else:
                self.my_cooperated_at_least_once = True
                return Action.C
        if num_cooperators_prev >= self.m - 1:
            return Action.D
        self.my_cooperated_at_least_once = True
        return Action.C

class Strategy_AGGRESSIVE_3(BaseStrategy):
    """
    The Opportunistic Predator strategy is designed to maximize individual gain
    by exploiting cooperative efforts of others. It defaults to defecting,
    only cooperating when it's demonstrably the best path to a higher payoff
    for itself, specifically when its cooperation is critical to unlock the
    collective reward. It becomes more reluctant to cooperate over time if
    cooperation fails to meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_tolerance = self.game_description.m - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if history is None:
            return Action.D
        cooperators_observed_prev_round = 0
        if current_round > 0:
            cooperators_observed_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
            if history.my_actions[current_round - 1]:
                cooperators_observed_prev_round += 1
        my_action_prev_round = Action.D
        if history.my_actions[current_round - 1]:
            my_action_prev_round = Action.C
        was_threshold_met_prev_round = cooperators_observed_prev_round >= m
        if not was_threshold_met_prev_round and my_action_prev_round == Action.D:
            self.cooperation_tolerance += 1
        elif was_threshold_met_prev_round and my_action_prev_round == Action.C:
            self.cooperation_tolerance -= 1
        self.cooperation_tolerance = max(m - 1, min(n - 1, self.cooperation_tolerance))
        my_coop_contribution = 1 if Action.C else 0
        if cooperators_observed_prev_round + 1 >= m:
            E_C = k
        else:
            E_C = 0
        if cooperators_observed_prev_round >= m:
            E_D = 1 + k
        else:
            E_D = 1
        if E_D > E_C:
            return Action.D
        elif E_C > E_D:
            if cooperators_observed_prev_round < self.cooperation_tolerance:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_4(BaseStrategy):
    """
    The Opportunistic Leverager strategy aims to maximize individual gain in the
    Collective Risk Dilemma. It strategically initiates cooperation, punishes
    non-compliance by defecting when individually profitable, and always seeks
    the highest immediate payoff. It defects in the final round to ensure
    maximum personal gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        observed_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :]) + history.my_actions[current_round - 1]
        if current_round == self.r - 1:
            return Action.D
        potential_payoff_cooperate = 0.0
        if observed_cooperators_prev_round + 1 >= self.m:
            potential_payoff_cooperate = self.k
        potential_payoff_defect = 1.0
        if observed_cooperators_prev_round >= self.m:
            potential_payoff_defect = 1.0 + self.k
        if potential_payoff_defect > potential_payoff_cooperate:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_5(BaseStrategy):
    """
    "The Enforcer" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by proactively cooperating
    to trigger rewards, but swiftly punishing defection and securing private
    endowments when cooperation is unlikely or near the end of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.observed_cooperators_prev_round = 0
        self.defect_retaliation_needed = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round > 0:
            total_cooperators_prev_round = sum(history.opponent_actions[current_round - 1, :]) + history.my_actions[current_round - 1]
            self.observed_cooperators_prev_round = total_cooperators_prev_round
            if history.my_actions[current_round - 1]:
                opponents_who_defected = np.sum(history.opponent_actions[current_round - 1, :] == False)
                if total_cooperators_prev_round < m and opponents_who_defected > 0:
                    self.defect_retaliation_needed = True
            else:
                self.defect_retaliation_needed = False
        if current_round == r - 1:
            return Action.D
        if self.defect_retaliation_needed:
            self.defect_retaliation_needed = False
            return Action.D
        if self.observed_cooperators_prev_round >= m:
            return Action.C
        if current_round < m and self.observed_cooperators_prev_round >= m - 1:
            return Action.C
        if current_round > r - m and self.observed_cooperators_prev_round >= m:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_7(BaseStrategy):
    """
    The Opportunist: An aggressive strategy for the Collective Risk Dilemma.
    This strategy defects by default, only cooperating when the potential reward (k)
    is substantial and the probability of success (reaching the cooperation threshold 'm')
    is very high based on historical data. It prioritizes individual gain and
    exploits near-successes of the collective.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_player_index = -1

    def _estimate_cooperation_probability(self, history: PlayerHistory) -> float:
        """
        Estimates the probability that *other* players will cooperate in the next round.
        This is a simplified estimation focusing on recent trends and overall averages.
        A more sophisticated approach would consider individual player behavior.
        """
        num_past_rounds = history.round_number
        if num_past_rounds == 0:
            return 0.5
        total_cooperations_by_others = 0
        num_other_players = self.n - 1
        if num_other_players <= 0:
            return 0.0
        for round_idx in range(num_past_rounds):
            cooperators_this_round = np.sum(history.opponent_actions[round_idx, :])
            total_cooperations_by_others += cooperators_this_round
        avg_historical_cooperation = total_cooperations_by_others / (num_other_players * num_past_rounds)
        recent_rounds_for_trend = min(3, num_past_rounds)
        recent_cooperation_counts = [np.sum(history.opponent_actions[i, :]) for i in range(num_past_rounds - recent_rounds_for_trend, num_past_rounds)]
        trend_factor = 1.0
        if recent_rounds_for_trend > 1:
            if all((recent_cooperation_counts[i] <= recent_cooperation_counts[i + 1] for i in range(len(recent_cooperation_counts) - 1))):
                trend_factor = 1.1
            elif all((recent_cooperation_counts[i] >= recent_cooperation_counts[i + 1] for i in range(len(recent_cooperation_counts) - 1))):
                trend_factor = 0.9
        last_round_cooperators = np.sum(history.opponent_actions[-1, :])
        proximity_boost = 1.0
        if last_round_cooperators >= self.m - 2 and last_round_cooperators < self.m:
            proximity_boost = 1.05
        elif last_round_cooperators < self.m - 2:
            proximity_boost = 0.95
        estimated_prob = avg_historical_cooperation * trend_factor * proximity_boost
        return max(0.0, min(1.0, estimated_prob))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) based on the current state and history.
        """
        if history is None:
            return Action.D
        num_past_rounds = history.round_number
        cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
        cpe = self._estimate_cooperation_probability(history)
        my_action_prev_round = Action.C if self.my_actions_history[-1] else Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if total_cooperators_prev_round == self.m - 1:
            if self.k > 2.0 and cpe > 0.7:
                return Action.C
        elif total_cooperators_prev_round <= self.m - 2:
            if cpe > 0.9 and self.k > 3.0:
                return Action.C
        return Action.D

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_actions_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.my_actions_history.append(False)
            return Action.D
        self.my_actions_history.append(history.my_actions[-1])
        num_past_rounds = history.round_number
        cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        cpe = self._estimate_cooperation_probability(history)
        if cooperators_in_prev_round == self.m - 1:
            if self.k > 2.0 and cpe > 0.7:
                self.my_actions_history.append(True)
                return Action.C
        elif cooperators_in_prev_round <= self.m - 2:
            if cpe > 0.9 and self.k > 3.0:
                self.my_actions_history.append(True)
                return Action.C
        self.my_actions_history.append(False)
        return Action.D

class Strategy_AGGRESSIVE_8(BaseStrategy):
    """
    Aggressive strategy that initiates cooperation when close to the threshold,
    but defects if the threshold is unlikely to be met or in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == r:
            return Action.D
        if history is None:
            return Action.C
        previous_round_index = state.round_number - 2
        my_cooperators_previous_round = 0
        if history.my_actions[previous_round_index]:
            my_cooperators_previous_round = 1
        observed_cooperators_previous_round = my_cooperators_previous_round + np.sum(history.opponent_actions[previous_round_index, :])
        if observed_cooperators_previous_round >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_9(BaseStrategy):
    """
    The Dominator strategy for the Collective Risk Dilemma.
    This strategy is aggressive and aims to maximize individual payoff by
    leveraging the threat of defection. It only cooperates when it's
    guaranteed to reach the cooperation threshold 'm' and its cooperation
    is essential for that. Otherwise, it defects to secure private gains
    and potentially benefit from others' cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == r:
            return Action.D
        num_cooperators_previous_round = 0
        if current_round > 1:
            num_cooperators_previous_round = np.sum(history.opponent_actions[current_round - 2, :])
        if history.my_actions[current_round - 2] == Action.C.value:
            num_cooperators_previous_round += 1
        if num_cooperators_previous_round == m - 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_10(BaseStrategy):
    """
    Aggressive Strategy: The "Calculated Risk-Taker"
    This strategy aims to maximize individual gain by cooperating only when
    the cooperation threshold (m) is likely to be met by others, and defecting
    otherwise. It is aggressive in its approach to information gathering
    (defecting in round 1) and in its exit strategy (defecting in the last round).
    It also includes a tweak for scenarios where the cooperation threshold is very low.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.player_id = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round - 1
        observed_others_cooperating_last_round = np.sum(history.opponent_actions[previous_round_idx, :])
        estimated_others_cooperating = observed_others_cooperating_last_round
        if self.m - 1 <= 1:
            required_others = max(0, self.m - 2)
            if estimated_others_cooperating >= required_others:
                return Action.C
            else:
                return Action.D
        required_others = self.m - 1
        if estimated_others_cooperating >= required_others:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_11(BaseStrategy):
    """
    Aggressive "Exploitative Risk-Taker" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by free-riding when possible,
    contributing only when it guarantees a win or is strategically essential.
    It dynamically assesses cooperation confidence in other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.HIGH_CONFIDENCE_THRESHOLD = 0.8
        self.CRITICAL_CONFIDENCE_THRESHOLD = 0.6
        self.HIGH_PROB_MULTIPLIER = 1.5
        self.cooperation_confidence = np.full(self.n_players, 0.5, dtype=np.float64)

    def _update_cooperation_confidence(self, opponent_idx: int, action: bool, current_round_idx: int):
        """
        Updates the cooperation confidence for a specific opponent based on their action.
        Uses a weighted average to give more importance to recent rounds.
        """
        if current_round_idx == 0:
            self.cooperation_confidence[opponent_idx] = float(action)
        else:
            contribution = float(action)
            num_completed_rounds = current_round_idx
            pass

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if history is not None:
            num_completed_rounds = history.round_number
            if num_completed_rounds > 0:
                last_round_idx = num_completed_rounds - 1
                if last_round_idx < len(history.my_actions):
                    for opp_idx in range(self.n_players):
                        if opp_idx < history.opponent_actions.shape[1]:
                            observed_action = history.opponent_actions[last_round_idx, opp_idx]
                            t_prev = last_round_idx
                            action_value = float(observed_action)
                            if t_prev == 0:
                                self.cooperation_confidence[opp_idx] = action_value
                            else:
                                self.cooperation_confidence[opp_idx] = (self.cooperation_confidence[opp_idx] * t_prev + action_value) / (t_prev + 1)
        num_potential_cooperators_high_conf = 0
        num_potential_cooperators_moderate_conf = 0
        expected_cooperators_from_others = 0.0
        for opp_idx in range(self.n_players):
            confidence = self.cooperation_confidence[opp_idx]
            if confidence >= self.HIGH_CONFIDENCE_THRESHOLD:
                num_potential_cooperators_high_conf += 1
            if confidence >= self.CRITICAL_CONFIDENCE_THRESHOLD:
                num_potential_cooperators_moderate_conf += 1
            expected_cooperators_from_others += confidence
        if num_potential_cooperators_high_conf >= self.m - 1:
            return Action.C
        if expected_cooperators_from_others + 1.0 >= self.m and num_potential_cooperators_moderate_conf < self.m - 1:
            return Action.C
        if expected_cooperators_from_others + 1.0 >= self.m * self.HIGH_PROB_MULTIPLIER:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_12(BaseStrategy):
    """
    The Opportunistic Predator strategy.

    This strategy is aggressive, aiming to maximize individual gain.
    It defects in the first and last rounds. In intermediate rounds,
    it cooperates only if the previous round saw sufficient cooperation
    to meet the threshold (m) AND the player personally benefited from
    that cooperation (i.e., received the reward k). Otherwise, it defects
    to punish perceived free-riding or uncertainty.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if current_round_num == 0:
            return Action.D
        if current_round_num == self.r - 1:
            return Action.D
        previous_round_idx = current_round_num - 1
        my_payoff_prev = history.my_payoffs[previous_round_idx]
        num_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :])
        if num_cooperators_prev >= self.m and my_payoff_prev > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_13(BaseStrategy):
    """
    The Opportunistic Enforcer strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by leveraging cooperation when
    it's beneficial and defecting to secure gains when cooperation falters. It's
    aggressive by taking calculated risks in the first round and being decisive
    in response to past round outcomes.

    Core Philosophy:
    - Exploit when possible: If the collective cooperated successfully, join in.
    - Punish defection: If others did not cooperate sufficiently, defect to avoid
      contributing to their gains without receiving the reward.
    - No implicit trust: Decisions are based on observed actions.

    Decision Logic:
    - Round 1: Cooperate (C) - A calculated gamble to initiate cooperation.
    - Last Round (r): Defect (D) - Pure self-interest as there are no future consequences.
    - Middle Rounds (1 < t < r):
        - If the cooperation threshold (m) was met in the previous round, Cooperate (C)
          to gain the reward opportunistically.
        - If the cooperation threshold (m) was NOT met in the previous round, Defect (D)
          to protect one's endowment and punish collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game description.

        Args:
            game_description: An object containing game parameters like n, r, m, k.
        """
        self.game_description = game_description
        self.cooperate_action = Action.C.value
        self.defect_action = Action.D.value

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on game state and history.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players.
                     If None, it signifies the first round.

        Returns:
            The action (Action.C or Action.D) to be taken in the current round.
        """
        current_round_number = state.round_number
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        previous_round_idx = current_round_number - 1
        my_previous_action = self.cooperate_action if history.my_actions[previous_round_idx] else self.defect_action
        opponent_cooperators_last_round = np.sum(history.opponent_actions[previous_round_idx, :])
        total_cooperators_last_round = my_previous_action + opponent_cooperators_last_round
        if current_round_number == r:
            return Action.D
        elif total_cooperators_last_round >= m:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_14(BaseStrategy):
    """
    Opportunistic Escalation: An aggressive strategy that maximizes personal gain
    by leveraging historical data to push for cooperative outcomes when favorable,
    and swiftly reverting to defection when cooperation fails or is threatened.
    It prioritizes its own payoff, aggressively seeks opportunities for higher rewards,
    and quickly punishes perceived failures of cooperation by others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.avg_coop_rate_overall = 0.0
        self.threshold_of_trust = 0.75
        self.my_history_actions = []
        self.num_cooperators_all_history = []
        self.num_cooperators_others_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.my_history_actions.append(Action.D)
            return Action.D
        my_action_prev = Action.D if not self.my_history_actions else self.my_history_actions[-1]
        num_cooperators_all_prev = 0
        num_cooperators_others_prev = 0
        if history and history.round_number > 0:
            my_action_prev = history.my_actions[current_round - 1]
            num_cooperators_all_prev = np.sum(history.opponent_actions[current_round - 1, :]) + (1 if my_action_prev else 0)
            num_cooperators_others_prev = np.sum(history.opponent_actions[current_round - 1, :])
            self.my_history_actions.append(my_action_prev)
            self.num_cooperators_all_history.append(num_cooperators_all_prev)
            self.num_cooperators_others_history.append(num_cooperators_others_prev)
        else:
            pass
        if self.num_cooperators_all_history:
            current_avg_coop_rate = self.num_cooperators_all_history[-1] / self.n
            self.avg_coop_rate_overall = (self.avg_coop_rate_overall * (current_round - 1) + current_avg_coop_rate) / current_round
            if current_avg_coop_rate > 0.5:
                adaptation_step = 0.02
                if current_avg_coop_rate > 0.8:
                    adaptation_step *= 1.5
                self.threshold_of_trust = max(0.5, self.threshold_of_trust - adaptation_step)
        if current_round == self.r:
            return Action.D
        prob_coop_next_if_i_join = (num_cooperators_all_prev + 1) / self.n
        prob_coop_by_others = num_cooperators_others_prev / self.n
        if my_action_prev == Action.C:
            if prob_coop_next_if_i_join > self.m / self.n * 1.1 and self.avg_coop_rate_overall > self.threshold_of_trust:
                return Action.C
            else:
                return Action.D
        elif prob_coop_next_if_i_join > self.m / self.n * 1.3 and self.avg_coop_rate_overall > self.threshold_of_trust * 0.9:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_15(BaseStrategy):
    """
    Aggressive Dominance Strategy (ADS) for the Collective Risk Dilemma.

    This strategy aims to maximize personal gain by leveraging the collective
    risk reward whenever possible. It defects strategically in early rounds
    to gauge group behavior and conserve endowment, then cooperates when the
    probability of triggering the reward becomes sufficiently high, based on
    the number of remaining rounds and a calculated Aggressive Cooperation Threshold (ACT).
    Once the reward threshold is met, it always cooperates to secure the bonus.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        current_round_idx = current_round_number - 1
        if history is None:
            return Action.D
        num_cooperators_in_prev_round_outcome = 0
        if current_round_idx > 0:
            num_cooperators_in_prev_round_outcome = np.sum(history.opponent_actions[current_round_idx - 1, :])
            if history.my_actions[current_round_idx - 1] == Action.C:
                num_cooperators_in_prev_round_outcome += 1
        elif current_round_idx == 0:
            num_cooperators_in_prev_round_outcome = np.sum(history.opponent_actions[0, :])
        if num_cooperators_in_prev_round_outcome >= self.m:
            return Action.C
        remaining_rounds_t = self.n_rounds - current_round_idx
        act_t = max(1, math.floor(self.m * (1 - remaining_rounds_t / self.n_rounds)))
        immediate_coop_gap = self.m - num_cooperators_in_prev_round_outcome
        if immediate_coop_gap <= act_t:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_16(BaseStrategy):
    """
    The Opportunistic Defector strategy.
    This strategy aims to maximize individual payoff by exploiting others' cooperation
    while minimizing personal risk. It defects by default, and cooperates only when
    its action is crucial to tipping the balance towards the reward, and only if
    the threshold was already met or is about to be met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            self.my_last_action = Action.D
            return Action.D
        last_round_index = current_round - 1
        observed_cooperators_prev_round = 0
        if self.my_last_action == Action.C:
            observed_cooperators_prev_round += 1
        if self.n > 1:
            observed_cooperators_prev_round += np.sum(history.opponent_actions[last_round_index, :])
        if observed_cooperators_prev_round == self.m - 1:
            self.my_last_action = Action.C
            return Action.C
        elif observed_cooperators_prev_round >= self.m:
            self.my_last_action = Action.D
            return Action.D
        else:
            self.my_last_action = Action.D
            return Action.D

class Strategy_AGGRESSIVE_17(BaseStrategy):
    """
    The Aggressive Dominance Strategy is designed to aggressively exploit the
    Collective Risk Dilemma by prioritizing personal gain while leveraging the
    cooperation of others when it is guaranteed and highly rewarding. It is
    adaptive, focusing on identifying opportunities for high individual payoffs
    and punishing deviations that threaten its own success.

    Core Philosophy: "Exploit the Herd, Punish the Free-Rider"

    Decision Rules:
    1. Guaranteed High Reward (Cooperate): Cooperate if it's certain at least m
       players will cooperate. Certainty is inferred from robust historical data.
    2. Strategic Defection for Maximum Gain (Defect): Defect if cooperation
       won't guarantee a payoff of at least 2, prioritizing the guaranteed private gain.
    3. Punitive Defection (Defect): Defect if player cooperated in a past round
       where fewer than m players cooperated.
    4. Initial Conservative Aggression (Defect): Defect in the first round to
       gather information.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.k = game_description.k
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def _estimate_future_cooperators(self, history: PlayerHistory) -> int:
        """
        Estimates the number of future cooperators based on historical data.
        This is a simplified heuristic: if the last round had enough cooperators
        and there's no clear trend of decreasing cooperation, assume robustness.
        A more sophisticated strategy would analyze individual player consistency.
        """
        if history.round_number == 0:
            return 0
        last_round_cooperators = np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1])
        if last_round_cooperators >= self.m:
            if history.round_number >= self.r - 1 and last_round_cooperators >= self.m:
                return self.m
            elif history.round_number < self.r - 1 and last_round_cooperators >= self.m:
                return self.m
        return 0

    def _player_cooperated_in_failed_round(self, history: PlayerHistory) -> bool:
        """
        Checks if player i cooperated in any previous round where fewer than m players cooperated.
        """
        for r_idx in range(history.round_number):
            my_action_in_round = history.my_actions[r_idx]
            opponent_actions_in_round = history.opponent_actions[r_idx]
            total_cooperators = np.sum(my_action_in_round) + np.sum(opponent_actions_in_round)
            if total_cooperators < self.m and my_action_in_round:
                return True
        return False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        estimated_cooperators = self._estimate_future_cooperators(history)
        punitive_action = self._player_cooperated_in_failed_round(history)
        if estimated_cooperators >= self.m:
            return Action.C
        if punitive_action:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_18(BaseStrategy):
    """
    The Reckless Gambler: Aggressively cooperates only when the threshold is
    guaranteed to be met with their own participation, otherwise defects.
    Starts by defecting in the first round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        if history is None:
            return Action.D
        cooperators_in_previous_round = np.sum(history.opponent_actions[-1, :])
        my_potential_cooperators = cooperators_in_previous_round + 1
        if my_potential_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_19(BaseStrategy):
    """
    Calculated Risk Maximizer strategy for the Collective Risk Dilemma.
    This strategy aims to maximize personal gain by being opportunistic and punitive.
    It cooperates when the collective reward is guaranteed or when it's one step away
    from triggering the reward. It defects aggressively when cooperation levels are
    too low, and always defects in the last round to maximize immediate payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing the game's parameters.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the game state and history.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1]
        my_previous_action = history.my_actions[current_round - 1]
        cooperators_prev_round = np.sum(previous_round_actions) + my_previous_action
        if cooperators_prev_round < self.m - 1:
            return Action.D
        elif cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.C

class Strategy_AGGRESSIVE_20(BaseStrategy):
    """
    The Sentinel of Self-Interest strategy for the Collective Risk Dilemma.

    This strategy is aggressive, defaulting to defection and only cooperating
    when there's strong historical evidence of collective success. It prioritizes
    maximizing individual payoff by exploiting the cooperation of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Sentinel of Self-Interest strategy.

        Args:
            game_description: An object containing the game's parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.aggressive_cooperation_ratio = 1.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: An object containing past actions and payoffs for this player
                     and their opponents. It will be None in the first round.

        Returns:
            Action.C if the strategy decides to cooperate, Action.D otherwise.
        """
        if history is None or state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        previous_round_completed = state.round_number - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_completed, :])
        if history.my_actions[previous_round_completed]:
            num_cooperators_prev_round += 1
        aggressive_coop_trigger = math.ceil(self.aggressive_cooperation_ratio * self.game_description.m)
        if num_cooperators_prev_round >= aggressive_coop_trigger:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_21(BaseStrategy):
    """
    "The Opportunistic Trigger" strategy for the Collective Risk Dilemma.
    This strategy defaults to defection and cooperates only when the reward is
    assured or the collective is on the verge of success, aiming to maximize
    individual gain while minimizing personal risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (C or D) for the current round based on game state and history.

        Args:
            state (GameState): Current round number.
            history (None | PlayerHistory): History of past actions and payoffs.
                                          None for the first round.

        Returns:
            Action: The chosen action, Action.C or Action.D.
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            return Action.D
        current_round_num = state.round_number
        coop_count_last_round = 0
        if current_round_num > 0:
            coop_count_last_round += 1 if history.my_actions[current_round_num - 1] else 0
            coop_count_last_round += np.sum(history.opponent_actions[current_round_num - 1, :])
        if coop_count_last_round >= m - 1:
            if coop_count_last_round >= m:
                return Action.C
            if current_round_num > 1:
                coop_count_two_rounds_ago = 0
                if current_round_num > 1:
                    coop_count_two_rounds_ago += 1 if history.my_actions[current_round_num - 2] else 0
                    coop_count_two_rounds_ago += np.sum(history.opponent_actions[current_round_num - 2, :])
                if coop_count_last_round < coop_count_two_rounds_ago and coop_count_last_round < m:
                    return Action.D
            return Action.C
        if current_round_num == r:
            if coop_count_last_round >= m - 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_22(BaseStrategy):
    """
    "The Leverage Play" - An aggressive strategy for the Collective Risk Dilemma.
    This strategy defects by default, prioritizing private gain. It cooperates
    only when its cooperation is the direct cause for meeting the cooperation
    threshold (m) and securing the collective reward, specifically when exactly
    m-1 other players have cooperated in the previous round. This ensures
    that its cooperation is strategically timed to maximize personal benefit
    while leveraging the actions of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the strategy's logic.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs from previous rounds.
                     None if it's the first round.

        Returns:
            Action.C (Cooperate) or Action.D (Defect).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round_number = state.round_number
        if history is None:
            return Action.D
        if current_round_number == r:
            return Action.D
        known_cooperators_in_prev_round = np.sum(history.opponent_actions[current_round_number - 1, :])
        if known_cooperators_in_prev_round == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_23(BaseStrategy):
    """
    The Aggressive Opportunistic Leverager strategy.
    This strategy defaults to defecting to secure the private endowment.
    It cooperates only when it predicts a high probability of reaching the cooperation threshold 'm',
    leveraging the collective reward 'k'. It uses a weighted combination of historical data
    and an aggressive posture modifier (APM) to make its predictions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.APM = 0.75
        self.RELIABILITY_THRESHOLD = 0.8
        self.own_previous_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round_number = state.round_number
        if current_round_number == 0:
            self.own_previous_action = Action.D
            return Action.D
        if current_round_number == r - 1:
            return Action.D
        total_cooperators_past = 0
        num_reliable_cooperators_past = 0
        player_cooperation_counts = np.zeros(n, dtype=int)
        if history is not None:
            num_past_rounds = history.round_number
            for t in range(num_past_rounds):
                round_cooperators = 0
                if history.my_actions[t] == Action.C:
                    round_cooperators += 1
                    player_cooperation_counts[0] += 1
                for i in range(n - 1):
                    if history.opponent_actions[t, i] == Action.C:
                        round_cooperators += 1
                        player_cooperation_counts[i + 1] += 1
                total_cooperators_past += round_cooperators
            if num_past_rounds > 0:
                HCR_t_minus_1 = total_cooperators_past / (n * num_past_rounds)
            else:
                HCR_t_minus_1 = 0
            for i in range(n):
                if player_cooperation_counts[i] / num_past_rounds >= self.RELIABILITY_THRESHOLD:
                    num_reliable_cooperators_past += 1
        else:
            HCR_t_minus_1 = 0
            num_reliable_cooperators_past = 0
        own_contribution = 1 if self.own_previous_action == Action.C else 0
        predicted_cooperators_float = (1 - self.APM) * HCR_t_minus_1 * n + self.APM * num_reliable_cooperators_past + own_contribution
        predicted_cooperators = math.floor(predicted_cooperators_float)
        if predicted_cooperators >= m:
            next_action = Action.C
        else:
            next_action = Action.D
        self.own_previous_action = next_action
        return next_action

class Strategy_AGGRESSIVE_24(BaseStrategy):
    """
    The "Opportunistic Leverager" strategy is an aggressive approach to the Collective Risk Dilemma.
    It defaults to defecting, aiming to secure personal gain. Cooperation is a rare, calculated
    exception, employed only when it's necessary to meet the cooperation threshold (m) and
    guarantees a higher individual payoff than defecting would in that specific round.
    It does not cooperate in the first or last round to maximize immediate or final gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        num_cooperators_previous_round = 0
        if history.round_number > 0:
            if history.my_actions[history.round_number - 1] == Action.C.value:
                num_cooperators_previous_round += 1
            num_cooperators_previous_round += np.sum(history.opponent_actions[history.round_number - 1, :])
        if num_cooperators_previous_round == self.m - 1:
            return Action.C
        elif num_cooperators_previous_round >= self.m:
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_25(BaseStrategy):
    """
    The Iron Fist strategy for the Collective Risk Dilemma.
    This strategy is aggressive, prioritizing self-interest by defaulting to defection.
    It cooperates only when it's strategically advantageous, specifically when:
    1. It estimates that its cooperation will be enough to meet the cooperation threshold (m).
    2. The potential payoff from cooperation (k) is sufficiently higher than the guaranteed payoff from defection (1).
    It dynamically estimates the likelihood of others cooperating based on past rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperators_last_round_estimate = 0
        self.last_round_my_action = Action.D

    def _estimate_cooperation_threshold(self, history: None | PlayerHistory) -> int:
        """
        Estimates the number of other players likely to cooperate in the current round.
        This is a simplified heuristic focusing on the previous round's observed cooperation.
        A more advanced version could incorporate recency weighting, trend analysis,
        and specific punishment for persistent defectors.
        """
        if history is None or history.round_number == 0:
            return 0
        cooperators_this_round = np.sum(history.opponent_actions[-1, :])
        estimated_ct = cooperators_this_round
        return int(estimated_ct)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.last_round_my_action = Action.D
            return Action.D
        estimated_other_cooperators = self._estimate_cooperation_threshold(history)
        if current_round == self.r - 1:
            if estimated_other_cooperators >= self.m - 1:
                self.last_round_my_action = Action.C
                return Action.C
            else:
                self.last_round_my_action = Action.D
                return Action.D
        if estimated_other_cooperators >= self.m - 1:
            self.last_round_my_action = Action.C
            return Action.C
        else:
            self.last_round_my_action = Action.D
            return Action.D

class Strategy_AGGRESSIVE_26(BaseStrategy):
    """
    The Opportunist Strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual gain by being a reluctant cooperator.
    It cooperates only when the probability of collective success is high AND
    the advantage of defecting is not significantly large. Otherwise, it defects
    to secure immediate individual payoffs. It is aggressive by defaulting to
    defection in uncertain or highly advantageous defection scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.initial_threshold_cooperate = 0.9
        self.dat_multiplier = 0.3
        self.decay_rate_threshold = 0.005
        self.min_threshold_cooperate = 0.6
        self.num_simulations_ccs = 5000
        self.threshold_cooperate = self.initial_threshold_cooperate

    def _estimate_opponent_cooperation_prob(self, history: PlayerHistory) -> dict[int, float]:
        """
        Estimates the probability of each opponent cooperating based on their history.
        Returns a dictionary mapping opponent index (0 to n-2) to their cooperation probability.
        """
        opponent_probs = {}
        n_completed_rounds = history.round_number
        for opp_idx in range(self.n - 1):
            coop_count = np.sum(history.opponent_actions[0:n_completed_rounds, opp_idx])
            if n_completed_rounds == 0:
                opponent_probs[opp_idx] = 0.5
            else:
                opponent_probs[opp_idx] = coop_count / n_completed_rounds
        return opponent_probs

    def _calculate_ccs(self, opponent_probs: dict[int, float]) -> float:
        """
        Calculates the Cooperation Confidence Score (CCS).
        CCS is the probability that at least (m-1) other players will cooperate.
        Uses simulation for approximation.
        """
        num_opponents = self.n - 1
        cooperating_opponents_needed = max(0, self.m - 1)
        if num_opponents == 0:
            return 1.0 if self.m <= 1 else 0.0
        if cooperating_opponents_needed > num_opponents:
            return 0.0
        successful_simulations = 0
        for _ in range(self.num_simulations_ccs):
            current_coop_count = 0
            for prob in opponent_probs.values():
                if random.random() < prob:
                    current_coop_count += 1
            if current_coop_count >= cooperating_opponents_needed:
                successful_simulations += 1
        return successful_simulations / self.num_simulations_ccs

    def _calculate_defection_advantage(self, ccs: float) -> float:
        """
        Calculates the expected payoff difference between defecting and cooperating.
        E[pi_D] - E[pi_C]
        """
        expected_payoff_cooperate = ccs * self.k
        expected_payoff_defect = ccs * (1 + self.k) + (1 - ccs) * 1
        return expected_payoff_defect - expected_payoff_cooperate

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        self.threshold_cooperate = max(self.min_threshold_cooperate, self.threshold_cooperate - self.decay_rate_threshold)
        opponent_probs = self._estimate_opponent_cooperation_prob(history)
        ccs = self._calculate_ccs(opponent_probs)
        defection_advantage = self._calculate_defection_advantage(ccs)
        dat = self.dat_multiplier * defection_advantage
        if ccs >= self.threshold_cooperate and defection_advantage <= dat:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_27(BaseStrategy):
    """
    The "Calculated Opportunist" strategy for the Collective Risk Dilemma.
    This strategy is aggressive, defaulting to defection and only cooperating
    when a strong signal of sufficient cooperation from others is observed.
    It prioritizes individual payoff by avoiding being a "sucker" and
    maximizing gains in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.aggression_multiplier = 1.2
        self.m = game_description.m
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        my_previous_action = self.my_action_to_bool(history.my_actions[current_round - 1])
        opponent_previous_actions = history.opponent_actions[current_round - 1, :]
        num_cooperators_last_round = np.sum(opponent_previous_actions) + my_previous_action
        required_cooperators_for_aggression = self.m * self.aggression_multiplier
        if num_cooperators_last_round >= required_cooperators_for_aggression:
            return Action.C
        else:
            return Action.D

    def my_action_to_bool(self, action: np.bool_) -> int:
        """Converts boolean action representation to integer (1 for C, 0 for D)."""
        return 1 if action else 0

class Strategy_AGGRESSIVE_28(BaseStrategy):
    """
    Aggressive strategy: "Calculated Betrayal".
    This strategy defects in round 1 and the last round.
    It cooperates if and only if the previous round had at least `m - 1` cooperators
    (excluding itself), meaning its cooperation will guarantee the threshold is met.
    Otherwise, it defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.n_rounds:
            return Action.D
        previous_round_idx = history.round_number - 1
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        my_cooperated_prev_round = history.my_actions[previous_round_idx]
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_cooperated_prev_round else 0)
        if total_cooperators_prev_round >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_29(BaseStrategy):
    """
    The "Aggressive Opportunist" strategy aims to maximize individual payoff
    in the Collective Risk Dilemma by pursuing the higher reward when cooperation
    is likely to succeed, and defecting when it's uncertain to minimize personal loss.
    It dynamically estimates the probability of cooperation success and adjusts its
    behavior based on past outcomes and game phase (early, mid, late rounds).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_confidence_threshold = 0.8
        self.late_round_high_confidence_threshold = 0.85
        self.second_to_last_round_threshold = self.cooperation_confidence_threshold - 0.1
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.recent_cooperation_trend_window = 3
        self.early_game_window = 3

    def _estimate_cooperation_probability(self, history: PlayerHistory) -> float:
        """
        Estimates the probability of m or more players cooperating in the current round
        based on historical data. This is a simplified heuristic.
        A more sophisticated implementation would use Bayesian updating, Markov models,
        or more complex trend analysis.
        """
        if history is None:
            return 0.0
        num_completed_rounds = history.round_number
        if num_completed_rounds == 0:
            return 0.0
        successful_cooperation_outcomes = 0
        for i in range(num_completed_rounds):
            cooperators_this_round = np.sum(history.my_actions[i]) + np.sum(history.opponent_actions[i, :])
            if cooperators_this_round >= self.m:
                successful_cooperation_outcomes += 1
        if num_completed_rounds < 5:
            prob = successful_cooperation_outcomes / num_completed_rounds
        else:
            recent_successful_coop = 0
            for i in range(max(0, num_completed_rounds - 5), num_completed_rounds):
                cooperators_this_round = np.sum(history.my_actions[i]) + np.sum(history.opponent_actions[i, :])
                if cooperators_this_round >= self.m:
                    recent_successful_coop += 1
            prob = recent_successful_coop / min(5, num_completed_rounds)
        recent_coop_proportion = 0.0
        if num_completed_rounds > 0:
            recent_actions_sum = np.sum(history.my_actions[-self.recent_cooperation_trend_window:]) + np.sum(history.opponent_actions[-self.recent_cooperation_trend_window:, :])
            total_actions_in_window = min(num_completed_rounds, self.recent_cooperation_trend_window) * self.n
            if total_actions_in_window > 0:
                recent_coop_proportion = recent_actions_sum / total_actions_in_window
        estimated_prob = prob * 0.7 + recent_coop_proportion * 0.3
        return min(1.0, max(0.0, estimated_prob))

    def _check_recent_trends(self, history: PlayerHistory) -> tuple[bool, bool]:
        """
        Checks recent trends for cooperation and defection.
        Returns (significant_cooperation_trend, significant_defection_trend).
        """
        num_completed_rounds = history.round_number
        if num_completed_rounds == 0:
            return (False, False)
        window_size = self.recent_cooperation_trend_window
        start_round = max(0, num_completed_rounds - window_size)
        recent_cooperators_count = 0
        total_players_in_window = 0
        for i in range(start_round, num_completed_rounds):
            if history.my_actions[i]:
                recent_cooperators_count += 1
            recent_cooperators_count += np.sum(history.opponent_actions[i, :])
            total_players_in_window += self.n
        recent_cooperation_rate = recent_cooperators_count / total_players_in_window if total_players_in_window > 0 else 0.0
        significant_cooperation_trend = recent_cooperation_rate > self.m * 0.8 / self.n
        significant_defection_trend = recent_cooperation_rate < 0.5
        return (significant_cooperation_trend, significant_defection_trend)

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            confidence_score = self._estimate_cooperation_probability(history)
            if confidence_score > self.late_round_high_confidence_threshold:
                return Action.C
            else:
                return Action.D
        if current_round == self.r - 2:
            confidence_score = self._estimate_cooperation_probability(history)
            if confidence_score > self.second_to_last_round_threshold:
                return Action.C
            else:
                return Action.D
        confidence_score = self._estimate_cooperation_probability(history)
        sig_coop_trend, sig_defect_trend = self._check_recent_trends(history)
        if confidence_score > self.cooperation_confidence_threshold:
            if sig_coop_trend:
                return Action.C
            elif confidence_score > self.cooperation_confidence_threshold + 0.1:
                return Action.C
            else:
                return Action.D
        elif sig_defect_trend:
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_30(BaseStrategy):
    """
    The "Opportunistic Punisher" strategy for the Collective Risk Dilemma.
    This strategy aggressively seeks the highest immediate payoff. It defects by default,
    only cooperating if its cooperation is the sole factor required to meet the minimum
    cooperator threshold (m) and thus secure the collective reward.
    It prioritizes self-preservation and exploits the system by defecting when the
    threshold is met and others have cooperated.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        prev_round_index = history.round_number - 1
        my_prev_action_is_cooperate = self.game_description.n_players > 1 and history.my_actions[prev_round_index]
        if self.n > 1:
            total_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :]) + (1 if my_prev_action_is_cooperate else 0)
        else:
            total_cooperators_prev_round = 1 if my_prev_action_is_cooperate else 0
        action_to_take = Action.D
        if total_cooperators_prev_round == self.m - 1:
            action_to_take = Action.C
        return action_to_take

class Strategy_AGGRESSIVE_31(BaseStrategy):
    """
    The Opportunistic Enforcer strategy.
    This strategy is designed for an aggressive player in the Collective Risk Dilemma.
    It prioritizes maximizing individual gain by adapting its behavior based on the
    likelihood of collective success, history of cooperation, and potential for personal gain.
    It probes initially, cooperates when the threshold is near, retaliates against
    defection when the threshold is missed, cooperates when the threshold has been met,
    and defaults to defecting otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.probing_rounds = min(math.floor(game_description.n_rounds / 3), 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_t = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            return Action.D
        num_cooperators_prev = np.sum(history.my_actions[round_t - 1] == Action.C.value)
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[round_t - 1] == Action.C.value)
        num_cooperators_prev += num_opponent_cooperators_prev
        num_defectors_prev = n - num_cooperators_prev
        if round_t <= self.probing_rounds:
            return Action.D
        if num_cooperators_prev >= m - 1 and round_t < r:
            return Action.C
        if num_cooperators_prev < m - 1 and num_defectors_prev > 0:
            return Action.D
        if num_cooperators_prev >= m:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_32(BaseStrategy):
    """
    The Opportunistic Enforcer: An aggressive strategy for the Collective Risk Dilemma.
    This strategy prioritizes maximizing individual payoff by defecting unless cooperation
    is demonstrably beneficial or critical for securing the bonus. It relies on observed
    cooperation counts and adapts based on the perceived likelihood of meeting the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_action_prev_round = Action.D
        self.observed_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: An object containing historical data of past actions and payoffs.
                     It's None for the first round (round_number 0).

        Returns:
            The action to take (Action.C or Action.D).
        """
        current_round = state.round_number
        if history is None:
            self.my_action_prev_round = Action.D
            self.observed_cooperators_prev_round = 0
            return Action.D
        if current_round > 0:
            prev_round_index = current_round - 1
            my_prev_action_bool = history.my_actions[prev_round_index]
            self.my_action_prev_round = Action.C if my_prev_action_bool else Action.D
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :])
            self.observed_cooperators_prev_round = opponent_cooperators_prev_round
            if self.my_action_prev_round == Action.C:
                self.observed_cooperators_prev_round += 1
        if current_round == self.r - 1:
            if self.observed_cooperators_prev_round >= self.m:
                return Action.C
            else:
                return Action.D
        projected_coop_if_C = self.observed_cooperators_prev_round
        if self.my_action_prev_round == Action.D:
            projected_coop_if_C += 1
        projected_coop_if_D = self.observed_cooperators_prev_round
        if self.my_action_prev_round == Action.C:
            projected_coop_if_D -= 1
        if projected_coop_if_C >= self.m and projected_coop_if_D < self.m:
            return Action.C
        if projected_coop_if_D >= self.m:
            return Action.D
        if projected_coop_if_C < self.m:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_33(BaseStrategy):
    """
    The "Calculated Gambler" strategy.
    Prioritizes defecting unless there's a clear and compelling reason to cooperate,
    driven by observed group behavior and the potential for a guaranteed reward.
    It's adaptive, learning from the environment and adjusting its behavior accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperators_last_round = 0
        self.round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_number = state.round_number
        if self.round_number == 0:
            return Action.D
        if self.round_number == self.r - 1:
            return Action.D
        if history is not None:
            self.cooperators_last_round = int(np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1])
            X = max(3, math.floor(self.r / 4))
            start_lookback_round = max(0, self.round_number - X)
            successful_rounds_in_window = 0
            my_cooperation_in_successful_rounds_in_window = 0
            for i in range(start_lookback_round, self.round_number):
                total_cooperators_in_round_i = int(np.sum(history.opponent_actions[i, :]) + history.my_actions[i])
                if total_cooperators_in_round_i >= self.m:
                    successful_rounds_in_window += 1
                    if history.my_actions[i]:
                        my_cooperation_in_successful_rounds_in_window += 1
            proportion_successful_rounds = successful_rounds_in_window / (self.round_number - start_lookback_round) if self.round_number - start_lookback_round > 0 else 0
            proportion_my_cooperation = my_cooperation_in_successful_rounds_in_window / successful_rounds_in_window if successful_rounds_in_window > 0 else 0
        else:
            proportion_successful_rounds = 0
            proportion_my_cooperation = 0
        action = Action.D
        if self.cooperators_last_round == self.m - 1:
            required_from_others = self.m - 1
            available_others = self.n - 1
            if available_others >= required_from_others:
                action = Action.C
        if proportion_successful_rounds > 0.75 and proportion_my_cooperation >= 0.5:
            action = Action.C
        if self.cooperators_last_round < self.m - 1:
            action = Action.D
        if self.round_number > self.r * 0.9 and self.round_number < self.r - 1:
            if action == Action.C:
                if random.random() < 0.2:
                    pass
                else:
                    action = Action.D
        return action

class Strategy_AGGRESSIVE_34(BaseStrategy):
    """
    The "Calculated Risk-Taker" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize personal gain by exploiting the threshold
    mechanism. It primarily defects but cooperates when its contribution
    is necessary to trigger the collective reward, or when the risk of not
    cooperating is too high.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            return Action.D
        current_round_idx = state.round_number
        if current_round_idx == r - 1:
            return Action.D
        num_cooperated_last_round = 0
        if current_round_idx > 0:
            num_cooperated_last_round = np.sum(history.opponent_actions[current_round_idx - 1, :])
        if num_cooperated_last_round >= m - 1:
            return Action.C
        elif num_cooperated_last_round == m - 2:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_35(BaseStrategy):
    """
    Calculated Betrayal strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by defecting initially,
    but cooperates strategically if the collective is on the verge of meeting
    the cooperation threshold, and capitalizes on established cooperation.
    It always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == r:
            return Action.D
        my_last_action = Action.C if history.my_actions[current_round - 1] else Action.D
        opponents_last_actions = history.opponent_actions[current_round - 1, :]
        cooperators_from_opponents = np.sum(opponents_last_actions)
        cooperators_in_previous_round = cooperators_from_opponents + (1 if my_last_action == Action.C else 0)
        if cooperators_in_previous_round >= m:
            return Action.C
        elif m - 1 <= cooperators_in_previous_round < m:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_36(BaseStrategy):
    """
    Aggressive "Opportunistic Strike" Strategy for Collective Risk Dilemma.

    This strategy defects by default, securing a payoff of 1. It only cooperates
    if, in the immediately preceding round, at least m-1 other players cooperated
    and the collective reward was successfully achieved. If cooperation fails in
    the preceding round, or if the trigger condition isn't met, it defects.
    It also defects in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters
                              (n_players, n_rounds, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None for the first round.

        Returns:
            The chosen action: Action.C (Cooperate) or Action.D (Defect).
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        previous_round_actions_opponent = history.opponent_actions[current_round - 1, :]
        previous_round_action_self = history.my_actions[current_round - 1]
        total_cooperators_last_round = np.sum(previous_round_actions_opponent) + previous_round_action_self
        was_last_round_successful = total_cooperators_last_round >= self.m
        other_cooperators_last_round = np.sum(previous_round_actions_opponent)
        if was_last_round_successful and other_cooperators_last_round >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_37(BaseStrategy):
    """
    Aggressive strategy for the Collective Risk Dilemma.
    Initiates cooperation only when conditions strongly favor it, otherwise defects.
    It's adaptive to opponent behavior and seeks to exploit opportunities for high payoffs
    while minimizing personal risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.learning_rate = 0.6
        self.estimated_others_cooperators = 0.0
        self.my_last_action_was_cooperate = False
        self.last_round_cooperators = 0
        self.second_last_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        current_round = state.round_number
        if history is None:
            self.estimated_others_cooperators = 0.0
            self.my_last_action_was_cooperate = False
            self.last_round_cooperators = 0
            self.second_last_round_cooperators = 0
            return Action.D
        actual_cooperators_last_round = 1 if history.my_actions[current_round - 1] else 0
        if n > 1:
            actual_cooperators_last_round += np.sum(history.opponent_actions[current_round - 1, :])
        self.second_last_round_cooperators = self.last_round_cooperators
        self.last_round_cooperators = actual_cooperators_last_round
        observed_others_in_last_round = actual_cooperators_last_round - (1 if history.my_actions[current_round - 1] else 0)
        self.estimated_others_cooperators = self.learning_rate * observed_others_in_last_round + (1 - self.learning_rate) * self.estimated_others_cooperators
        self.my_last_action_was_cooperate = history.my_actions[current_round - 1]
        if current_round == r - 1:
            return Action.D
        if self.last_round_cooperators == m - 1 and (not self.my_last_action_was_cooperate):
            return Action.C
        if current_round >= 2 and self.last_round_cooperators >= m and (self.second_last_round_cooperators >= m):
            return Action.C
        early_game_limit = min(r, 3)
        if current_round < early_game_limit:
            if self.estimated_others_cooperators + 1 >= m:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_38(BaseStrategy):
    """
    The "Opportunistic Dominator" strategy for the Collective Risk Dilemma.
    This strategy defects in the first and last rounds. In intermediate rounds,
    it cooperates if and only if the previous round had at least m-1 cooperators.
    This aims to exploit the collective reward only when its success is highly probable,
    while minimizing risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number > 0:
            previous_round_idx = current_round_number - 1
            if current_round_number == 1:
                coop_count_round0 = 0
                if history is not None and len(history.my_actions) > 0:
                    if history.my_actions[0] == Action.C.value:
                        coop_count_round0 += 1
                    coop_count_round0 += np.sum(history.opponent_actions[0, :])
                self.cooperators_last_round = coop_count_round0
            if self.cooperators_last_round >= self.m - 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_39(BaseStrategy):
    """
    The Decisive Defector strategy.
    This strategy defects by default, prioritizing individual payoff.
    It cooperates only when there's a strong signal of imminent success
    (near miss, sustained cooperation, or last chance in the final round).
    Aggression comes from its default defection and decisive cooperation when
    conditions are met, without relying on punishment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.w = max(2, math.floor(self.r / 4))

    def _count_cooperators_in_previous_round(self, history: PlayerHistory) -> int:
        """Counts the number of cooperators in the previous round."""
        if history is None or history.round_number == 0:
            return 0
        cooperators = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            cooperators += 1
        return cooperators

    def _check_sustained_cooperation(self, history: PlayerHistory) -> bool:
        """Checks if cooperation has been sustained for 'w' rounds and yielded the bonus."""
        if history is None or history.round_number < self.w:
            return False
        for i in range(history.round_number - self.w, history.round_number):
            if not history.my_actions[i]:
                return False
            if not np.isclose(history.my_payoffs[i], 1 - 0 + self.k):
                return False
        return True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if history is None:
            return Action.D
        num_cooperators_prev_round = self._count_cooperators_in_previous_round(history)
        my_previous_action = Action.C if history.my_actions[-1] else Action.D
        if num_cooperators_prev_round == self.m - 1 and my_previous_action == Action.C:
            return Action.C
        if self._check_sustained_cooperation(history):
            return Action.C
        if current_round_num == self.r - 1 and num_cooperators_prev_round < self.m - 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_40(BaseStrategy):
    """
    The Opportunist's Gambit: An aggressive strategy that prioritizes immediate
    individual payoff by defaulting to Defect, exploiting successful collective
    actions, and immediately switching to Defect if cooperation yields no
    individual gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            self.my_last_action = Action.D
            return Action.D
        prev_round_number = state.round_number - 1
        num_cooperators_prev_round = 0
        if prev_round_number >= 0:
            my_action_prev_round_bool = history.my_actions[prev_round_number]
            opponent_actions_prev_round = history.opponent_actions[prev_round_number, :]
            num_cooperators_prev_round = int(my_action_prev_round_bool) + np.sum(opponent_actions_prev_round)
        if num_cooperators_prev_round >= m:
            self.my_last_action = Action.D
            return Action.D
        else:
            self.my_last_action = Action.D
            return Action.D

class Strategy_AGGRESSIVE_41(BaseStrategy):
    """
    Opportunistic Defector strategy for the Collective Risk Dilemma.
    This strategy defects by default and cooperates only when it's demonstrably
    profitable and safe, specifically when its cooperation is the sole trigger
    for meeting the cooperation threshold or when the probability of meeting
    the threshold is very high based on the previous round's observations.
    It avoids cooperation in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_index = current_round - 1
        prev_round_cooperators = 0
        if prev_round_index < len(history.my_actions):
            if history.my_actions[prev_round_index]:
                prev_round_cooperators += 1
        if prev_round_index < history.opponent_actions.shape[0]:
            prev_round_cooperators += np.sum(history.opponent_actions[prev_round_index, :])
        else:
            pass
        if prev_round_cooperators == self.m - 1:
            return Action.C
        if prev_round_cooperators >= self.m - 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_42(BaseStrategy):
    """
    The "Calculated Opportunist" strategy for the Collective Risk Dilemma.
    This strategy cooperates if the previous round had at least m-1 cooperators,
    or in the first round if a certain aggressive condition is met.
    Otherwise, it defects. It always defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if current_round == 0:
            if self.n - self.m + 1 >= self.m:
                return Action.C
            else:
                return Action.D
        else:
            last_round_my_action = Action.C if history.my_actions[-1] else Action.D
            last_round_opponent_actions = history.opponent_actions[-1, :]
            cooperators_in_last_round = np.sum(last_round_opponent_actions)
            if last_round_my_action == Action.C:
                cooperators_in_last_round += 1
            if cooperators_in_last_round >= self.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_43(BaseStrategy):
    """
    The "Calculated Risk-Taker" strategy is an aggressive, self-interested
    approach to the Collective Risk Dilemma. It defaults to defection,
    only cooperating when there's a strong guarantee of both collective
    success and significant personal gain, with a specific aversion to
    cooperating in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        previous_round_idx = history.round_number - 1
        cooperators_in_previous_round = np.sum(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx, :])
        if cooperators_in_previous_round >= self.m - 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_44(BaseStrategy):
    """
    The Opportunistic Trigger strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual gain by cooperating when the
    threshold is likely to be met, but defects aggressively when the collective
    fails or the odds are unfavorable. It includes a punitive trigger for
    past exploitation and a purely self-interested stance in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_deficit_trigger = False

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds:
            return Action.D
        if history is None:
            if self.n_players >= self.m * 2:
                return Action.C
            else:
                return Action.D
        lookback_rounds = min(current_round, 3)
        for i in range(lookback_rounds):
            round_idx = current_round - 1 - i
            my_action_in_round = self.cooperate_to_bool(self.get_my_action_from_history(history, round_idx))
            if my_action_in_round:
                num_my_cooperators_in_round = 1 + np.sum(history.opponent_actions[round_idx, :])
                if num_my_cooperators_in_round < self.m:
                    self.cooperation_deficit_trigger = True
                    break
        if self.cooperation_deficit_trigger:
            return Action.D
        recent_rounds_indices = list(range(max(0, current_round - 3), current_round))
        if not recent_rounds_indices:
            avg_cooperators_recent = 0
        else:
            total_cooperators_recent = 0
            for idx in recent_rounds_indices:
                total_cooperators_recent += 1 + np.sum(history.opponent_actions[idx, :])
            avg_cooperators_recent = total_cooperators_recent / len(recent_rounds_indices)
        if avg_cooperators_recent >= self.m - 1:
            return Action.C
        else:
            return Action.D

    def cooperate_to_bool(self, action: Action) -> bool:
        """Converts Action enum to boolean (True for C, False for D)."""
        return action == Action.C

    def get_my_action_from_history(self, history: PlayerHistory, round_idx: int) -> Action:
        """Retrieves my action for a specific round from history."""
        return Action.C if history.my_actions[round_idx] else Action.D

class Strategy_AGGRESSIVE_45(BaseStrategy):
    """
    Calculated Opportunist: An aggressive strategy for the Collective Risk Dilemma.
    This strategy defects initially, then cooperates only if the threshold 'm' is
    likely to be met. It aggressively exploits successful rounds by defecting,
    and swiftly reverts to defection if cooperation seems unreliable or if it
    perceives exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_threshold = self.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        last_round_index = current_round - 1
        cooperators_in_previous_round = np.sum(history.opponent_actions[last_round_index, :])
        my_action_in_previous_round = history.my_actions[last_round_index]
        if current_round == self.n_rounds:
            if cooperators_in_previous_round + my_action_in_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D
        if cooperators_in_previous_round + my_action_in_previous_round >= self.cooperation_threshold:
            return Action.D
        if cooperators_in_previous_round + my_action_in_previous_round == self.cooperation_threshold - 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_46(BaseStrategy):
    """
    Aggressive Strategy: "The Threshold Enforcer"
    This strategy prioritizes maximizing individual payoff while strategically leveraging
    the collective reward. It cooperates only when it's demonstrably beneficial
    to trigger the reward, and does so decisively. It defects to secure individual gains
    if the collective fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        observed_cooperators_prev_round = 0
        if history is not None:
            if current_round > 0 and len(history.my_actions) > 0:
                prev_round_idx = current_round - 1
                if prev_round_idx < len(history.my_actions):
                    if history.my_actions[prev_round_idx]:
                        observed_cooperators_prev_round += 1
                    if prev_round_idx < len(history.opponent_actions):
                        observed_cooperators_prev_round += np.sum(history.opponent_actions[prev_round_idx, :])
        my_potential_cooperation_count = observed_cooperators_prev_round + 1
        if current_round == 0:
            if n <= m:
                return Action.C
            else:
                return Action.D
        if current_round == r - 1:
            if my_potential_cooperation_count >= m:
                return Action.C
            else:
                return Action.D
        if current_round == r - 2:
            observed_cooperators_two_rounds_ago = 0
            if current_round >= 1:
                prev_round_idx = current_round - 1
                if prev_round_idx < len(history.my_actions):
                    coop_count_prev_round = 0
                    if history.my_actions[prev_round_idx]:
                        coop_count_prev_round += 1
                    if prev_round_idx < len(history.opponent_actions):
                        coop_count_prev_round += np.sum(history.opponent_actions[prev_round_idx, :])
                    if coop_count_prev_round == m - 1:
                        return Action.C
        if my_potential_cooperation_count >= m:
            return Action.C
        if observed_cooperators_prev_round >= m:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_47(BaseStrategy):
    """
    Aggressive Strategy: "The Opportunistic Predator"

    This strategy aims to maximize individual payoff by defecting by default
    and cooperating only when the collective reward threshold is nearly met or
    has already been met, ensuring participation in a beneficial outcome without
    bearing the initial risk. It defects in the first and last rounds to
    prioritize immediate, risk-free gains.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 1:
            return Action.D
        if current_round == self.r:
            return Action.D
        previous_round_idx = current_round - 2
        if previous_round_idx < 0:
            return Action.D
        cooperators_previous_round = 0
        if history.my_actions[previous_round_idx]:
            cooperators_previous_round += 1
        cooperators_previous_round += np.sum(history.opponent_actions[previous_round_idx, :])
        cooperation_deficit = max(0, self.m - cooperators_previous_round)
        if cooperation_deficit == 0:
            return Action.C
        else:
            if self.n - self.m == 0:
                if cooperation_deficit == 1:
                    return Action.C
                else:
                    return Action.D
            small_deficit_threshold = (self.n - self.m) // 2
            if cooperation_deficit <= small_deficit_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_48(BaseStrategy):
    """
    The "Threshold Striker" strategy is designed to exploit the payoff structure
    of the Collective Risk Dilemma by being aggressively opportunistic. It prioritizes
    maximizing personal gain by leveraging the increased reward for collective
    cooperation, but only when the risk of individual defection is outweighed by
    the potential for a superior payout. It cooperates only when it can guarantee
    the threshold is met for its own benefit, or in rare cases, to salvage future
    potential gains.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            return Action.D
        current_round_number = state.round_number
        observed_cooperators_last_round = np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1])
        if current_round_number == r:
            if observed_cooperators_last_round >= m - 1:
                return Action.C
            else:
                return Action.D
        remaining_rounds = r - current_round_number
        if observed_cooperators_last_round >= m - 1 and remaining_rounds > 1:
            return Action.C
        if n - observed_cooperators_last_round >= m and remaining_rounds > 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_49(BaseStrategy):
    """
    Aggressive "Calculated Escalation" Strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by opportunistically
    escalating cooperation to trigger the reward when it's within reach,
    punishing defections, and securing baseline payoffs otherwise.
    It prioritizes self-gain while leveraging collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        num_cooperators_prev_round = 0
        if history.my_actions.size > 0:
            my_cooperation_prev = history.my_actions[-1]
            opp_cooperation_prev = history.opponent_actions[-1, :]
            num_cooperators_prev_round = int(np.sum(opp_cooperation_prev) + my_cooperation_prev)
        if current_round == self.r:
            if num_cooperators_prev_round >= self.m - 1:
                return Action.C
            else:
                return Action.D
        if num_cooperators_prev_round >= self.m - 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_50(BaseStrategy):
    """
    Aggressive Strategy: The Opportunistic Enforcer

    This strategy is built on the principle of opportunism with a punitive undertone.
    It aims to exploit situations where cooperation is likely to succeed, but quickly
    punishes perceived freeloading. The core idea is to be an "enforcer" – pushing
    for cooperation when it's beneficial, but decisively defecting when others are
    not pulling their weight.

    Decision Rules:
    - Round 1: Cooperate (C) to probe.
    - Subsequent Rounds:
        - Calculate Cooperation Status (CS) from the previous round.
        - Determine Enforcement Trigger based on my previous action.
        - Cooperate (C) if CS meets or exceeds the Enforcement Trigger AND I cooperated previously.
        - Defect (D) if CS meets or exceeds 'm' AND I defected previously (freeloading).
        - Defect (D) if CS is below the Enforcement Trigger AND I cooperated previously (waste of effort).
        - Defect (D) if CS is below 'm' AND I defected previously (group failed to cooperate).
    - Last Resort Defection: If I have cooperated for the last 3 rounds and the threshold 'm' has never been met,
      switch to permanent Defect (D).
    - Last Round: Always Defect (D) as there are no future rounds to incentivize cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_last_action = None
        self.last_resort_cooperation_streak = 0
        self.last_resort_threshold_met_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            self.my_last_action = Action.C
            self.last_resort_cooperation_streak = 1
            self.last_resort_threshold_met_streak = 0
            return Action.C
        cooperators_last_round = 0
        if self.my_last_action == Action.C:
            cooperators_last_round += 1
        if history.opponent_actions.size > 0:
            cooperators_last_round += np.sum(history.opponent_actions[-1, :])
        if state.round_number == r:
            return Action.D
        threshold_met_this_round = cooperators_last_round >= m
        if self.my_last_action == Action.C:
            self.last_resort_cooperation_streak += 1
            if threshold_met_this_round:
                self.last_resort_threshold_met_streak += 1
            else:
                self.last_resort_threshold_met_streak = 0
        else:
            self.last_resort_cooperation_streak = 0
            self.last_resort_threshold_met_streak = 0
        if self.last_resort_cooperation_streak >= 3 and self.last_resort_threshold_met_streak == 0:
            self.my_last_action = Action.D
            return Action.D
        enforcement_trigger = 0
        if self.my_last_action == Action.C:
            enforcement_trigger = m - 1
        else:
            enforcement_trigger = m
        if cooperators_last_round >= enforcement_trigger:
            if self.my_last_action == Action.C:
                self.my_last_action = Action.C
                return Action.C
            else:
                self.my_last_action = Action.D
                return Action.D
        else:
            self.my_last_action = Action.D
            return Action.D

class Strategy_AGGRESSIVE_51(BaseStrategy):
    """
    The Aggressive Dominance Strategy is a calculating opportunist. It prioritizes
    self-preservation and immediate individual gain by defaulting to Defect (D).
    It aggressively cooperates (C) only when the collective reward is practically
    guaranteed in the current round or as a final gambit in the last round.
    It does not attempt to build trust or signal intent, focusing solely on
    exploiting opportunities for guaranteed payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        my_action = Action.D
        if history is None:
            return Action.D
        num_opponents = self.n - 1
        previous_round_cooperators_count = 0
        if current_round > 1:
            previous_round_cooperators_count = np.sum(history.opponent_actions[current_round - 2, :])
        current_round_observed_cooperators_count = 0
        if len(history.opponent_actions) > current_round - 1:
            current_round_observed_cooperators_count = np.sum(history.opponent_actions[current_round - 1, :])
        potential_cooperators_this_round = current_round_observed_cooperators_count + 1
        if potential_cooperators_this_round >= self.m:
            my_action = Action.C
        if current_round == self.r and previous_round_cooperators_count == self.m - 1:
            if current_round > 2:
                round_before_previous_cooperators = np.sum(history.opponent_actions[current_round - 3, :])
                if round_before_previous_cooperators < self.m - 1 and previous_round_cooperators_count == self.m - 1:
                    pass
                else:
                    my_action = Action.C
            else:
                my_action = Action.C
        return my_action

class Strategy_AGGRESSIVE_52(BaseStrategy):
    """
    The Opportunistic Enforcer: A strategy that aims to maximize individual gain
    by exploiting the cooperative reward when it's likely to be achieved, and
    punishing defection otherwise. It actively seeks the higher payoff when
    cooperation is likely to succeed, punishes opponents for defection, and
    adapts its behavior based on observed history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.lookback_window = 3
        self.punishment_threshold = max(1, self.m - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        recent_cooperation_count = 0
        num_rounds_to_consider = min(self.lookback_window, current_round)
        for round_idx in range(current_round - num_rounds_to_consider, current_round):
            if history.my_actions[round_idx]:
                recent_cooperation_count += 1
            recent_cooperation_count += np.sum(history.opponent_actions[round_idx, :])
        average_cooperators_in_window = 0
        if num_rounds_to_consider > 0:
            average_cooperators_in_window = recent_cooperation_count / num_rounds_to_consider
        else:
            average_cooperators_in_window = 0
        estimated_total_cooperators = average_cooperators_in_window + 1
        recent_defections_others = 0
        if current_round > 0:
            last_round_idx = current_round - 1
            recent_defections_others = self.n - 1 - np.sum(history.opponent_actions[last_round_idx, :])
        if estimated_total_cooperators >= self.m:
            if recent_defections_others < self.punishment_threshold:
                return Action.C
        if recent_defections_others >= self.punishment_threshold:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_53(BaseStrategy):
    """
    Calculated Opportunist: An aggressive, conditional cooperator.
    This strategy aims to maximize individual payoff by being a necessary cooperator
    when the collective reward is achievable with its contribution, and defecting otherwise.
    It aggressively pushes for cooperation in the first round and defects in the last.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_cooperation_threshold = math.floor((self.n - self.m) / 2)

    def _get_num_cooperators_last_round(self, history: PlayerHistory) -> int:
        """Calculates the number of players who cooperated in the previous round."""
        if history.round_number == 0:
            return 0
        own_coop_last_round = 1 if history.my_actions[-1] else 0
        opp_coop_last_round = np.sum(history.opponent_actions[-1, :])
        return own_coop_last_round + opp_coop_last_round

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if current_round_num == self.r - 1:
            return Action.D
        if history is None:
            return Action.C
        n_cooperators_last_round = self._get_num_cooperators_last_round(history)
        if n_cooperators_last_round >= self.m:
            return Action.D
        else:
            cni = self.m - n_cooperators_last_round
            if cni <= self.my_cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_54(BaseStrategy):
    """
    Calculated Aggression strategy for the Collective Risk Dilemma.

    This strategy defaults to defecting to secure individual payoff. It cooperates
    only when the likelihood of reaching the cooperation threshold 'm' is high,
    or when it has successfully exploited a previous cooperative round by defecting.
    It is aggressive by prioritizing self-interest and only taking calculated risks
    when the reward is likely to be achieved.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.our_action_in_prev_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == self.r:
            return Action.D
        if history is None:
            self.our_action_in_prev_round = Action.D
            return Action.D
        num_cooperators_prev_round = 0
        if history.round_number > 0:
            our_cooperation_last_round = 1 if history.my_actions[history.round_number - 1] else 0
            num_cooperators_prev_round = np.sum(history.opponent_actions[history.round_number - 1, :]) + our_cooperation_last_round
        self.our_action_in_prev_round = history.my_actions[history.round_number - 1] if history.round_number > 0 else Action.D
        predicted_other_cooperators = 0.0
        if history.round_number > 1:
            lookback_rounds = min(5, history.round_number - 1)
            relevant_history_indices = list(range(history.round_number - lookback_rounds, history.round_number))
            total_other_cooperators_sum = 0
            num_valid_rounds = 0
            for i in relevant_history_indices:
                num_valid_rounds += 1
                cooperators_in_past_round = np.sum(history.opponent_actions[i, :])
                if history.my_actions[i]:
                    cooperators_in_past_round -= 1
                total_other_cooperators_sum += cooperators_in_past_round
            if num_valid_rounds > 0 and self.n > 1:
                avg_other_cooperation_rate = total_other_cooperators_sum / num_valid_rounds / (self.n - 1)
                predicted_other_cooperators = avg_other_cooperation_rate * (self.n - 1)
            elif self.n == 1:
                predicted_other_cooperators = 0
        num_others_cooperated_prev = num_cooperators_prev_round - (1 if self.our_action_in_prev_round else 0)
        if num_others_cooperated_prev + math.floor(predicted_other_cooperators) + 1 >= self.m:
            self.our_action_in_prev_round = Action.C
            return Action.C
        if num_cooperators_prev_round >= self.m and self.our_action_in_prev_round == Action.D:
            self.our_action_in_prev_round = Action.C
            return Action.C
        self.our_action_in_prev_round = Action.D
        return Action.D

class Strategy_AGGRESSIVE_55(BaseStrategy):
    """
    Aggressive "Calculated Betrayal" Strategy:
    Defects by default, cooperating only when the probability of success and reward
    is overwhelmingly high, and the risk of being the sole cooperator is minimal.
    It seeks to exploit the cooperation of others without taking significant risks itself,
    especially in the early and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.prob_cooperate_if_cooperated = 0.7
        self.prob_cooperate_if_defected = 0.1

    def _predict_anticipate_cooperators(self, history: PlayerHistory) -> int:
        """
        Estimates the number of players likely to cooperate in the next round
        based on their actions in the previous round.
        """
        if history.round_number == 0:
            return 0
        last_round_actions = np.concatenate((history.my_actions[-1:], history.opponent_actions[-1, :]))
        anticipated = 0
        for action_bool in last_round_actions:
            if action_bool:
                if random.random() < self.prob_cooperate_if_cooperated:
                    anticipated += 1
            elif random.random() < self.prob_cooperate_if_defected:
                anticipated += 1
        return anticipated

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        observed_cooperators_prev_round = 0
        if history.round_number > 0:
            observed_cooperators_prev_round = np.sum(np.concatenate((history.my_actions[-1:], history.opponent_actions[-1, :])))
        anticipated_cooperators = self._predict_anticipate_cooperators(history)
        if current_round == self.r - 1:
            return Action.D
        sufficient_cooperators_predicted = observed_cooperators_prev_round + anticipated_cooperators >= self.m
        high_reward_margin = self.k >= 2.0
        enough_others_will_cooperate = anticipated_cooperators >= self.m - 1
        if observed_cooperators_prev_round + anticipated_cooperators >= self.m and self.k >= 2.0 and (anticipated_cooperators >= self.m - 1):
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_56(BaseStrategy):
    """
    The Calculated Gambler strategy.

    This strategy is opportunistic and aggressive. It defects by default but
    is willing to cooperate if the number of cooperators in the previous round
    is sufficiently high, especially if the group is on the verge of meeting
    the cooperation threshold 'm'. The strategy adapts its aggression threshold
    based on the success of cooperation in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Calculated Gambler strategy.

        Args:
            game_description: An object containing game parameters (n, r, m, k).
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.current_round_num = 0
        self.aggression_threshold = max(self.m, self.n - math.floor(self.n / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: Historical data of actions and payoffs. None for the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        self.current_round_num = state.round_number
        if history is None:
            return Action.D
        num_cooperators_prev = 0
        if history.opponent_actions.shape[0] > 0:
            my_cooperation_prev = int(history.my_actions[history.round_number - 1])
            opponent_cooperators_prev = np.sum(history.opponent_actions[history.round_number - 1, :])
            num_cooperators_prev = my_cooperation_prev + opponent_cooperators_prev
        min_at = self.m - 1
        max_at = self.n - 1
        if num_cooperators_prev >= self.m:
            self.aggression_threshold = max(min_at, self.aggression_threshold - 1)
        else:
            self.aggression_threshold = min(max_at, self.aggression_threshold + 1)
        if num_cooperators_prev >= self.aggression_threshold and num_cooperators_prev >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_57(BaseStrategy):
    """
    The Opportunistic Enforcer strategy.

    This strategy is aggressive, aiming to exploit the cooperation of others.
    It cooperates only in the first round as an investment.
    If the cooperation threshold 'm' is met in the previous round, it defects to maximize individual gain.
    If the threshold is not met, it defects to punish collective failure,
    unless it was the one cooperating in the previous round, in which case it still defects
    after absorbing the cost of one failed cooperative attempt.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_idx = history.round_number
        prev_round_idx = current_round_idx - 1
        my_previous_action = Action.C if history.my_actions[prev_round_idx] else Action.D
        cooperators_in_previous_round = history.my_actions[prev_round_idx]
        cooperators_in_previous_round += np.sum(history.opponent_actions[prev_round_idx, :])
        if cooperators_in_previous_round >= self.m:
            return Action.D
        elif my_previous_action == Action.C:
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_58(BaseStrategy):
    """
    The Opportunistic Dominator strategy for the Collective Risk Dilemma.

    This strategy is aggressive, prioritizing individual gain while strategically
    leveraging opportunities for collective reward. It aims to maximize personal
    payoff by making decisive actions based on the perceived likelihood of
    reaching the cooperation threshold and the potential outcomes of cooperating
    versus defecting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.weights = np.array([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])

    def _count_cooperators(self, actions: NDArray[np.bool_]) -> int:
        """Counts the number of True values (cooperators) in an action array."""
        return np.sum(actions)

    def _calculate_csp(self, history: PlayerHistory) -> float:
        """
        Calculates a weighted average of the proportion of cooperators in past rounds
        to estimate the Cooperation Success Probability (CSP).
        """
        if history.round_number == 0:
            return 0.0
        num_past_rounds = history.round_number
        cooperator_counts = []
        for round_idx in range(num_past_rounds):
            round_actions = np.concatenate(([history.my_actions[round_idx]], history.opponent_actions[round_idx]))
            cooperator_counts.append(self._count_cooperators(round_actions))
        relevant_weights = self.weights[-min(num_past_rounds, len(self.weights)):][::-1]
        recent_cooperator_counts = np.array(cooperator_counts[-len(relevant_weights):])
        weighted_csp_sum = np.sum(recent_cooperator_counts * relevant_weights)
        weight_sum = np.sum(relevant_weights)
        if weight_sum > 0:
            return weighted_csp_sum / (self.n * weight_sum)
        else:
            return 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 1:
            return Action.D
        if current_round == self.r:
            return Action.D
        csp = self._calculate_csp(history)
        epsilon = 0.05
        if csp > self.m / self.n + epsilon:
            return Action.C
        if (self.m - 1) / self.n < csp <= self.m / self.n + epsilon and self.r - current_round > 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_59(BaseStrategy):
    """
    The Reckless Benefactor: Aggressively seeks high rewards through calculated defection,
    only cooperating when necessary and when the probability of success is high.
    Leans heavily towards free-riding, but will take risks if the payoff is almost guaranteed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_pressure = 1.0
        self.estimated_others_cooperation_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.estimated_others_cooperation_rate = 0.0
            self.cooperation_pressure = 1.0
            cooperation_gap = self.m - (self.n - 1)
            if cooperation_gap <= 0:
                return Action.D
            elif 1 < self.m:
                return Action.D
            else:
                return Action.C
        else:
            if current_round > 0:
                if self.n - 1 > 0:
                    actual_others_cooperation_count_prev_round = np.sum(history.opponent_actions[-1, :])
                    observed_others_cooperation_rate = actual_others_cooperation_count_prev_round / (self.n - 1)
                    self.estimated_others_cooperation_rate = observed_others_cooperation_rate * self.cooperation_pressure
                    self.estimated_others_cooperation_rate = max(0.0, min(1.0, self.estimated_others_cooperation_rate))
                else:
                    self.estimated_others_cooperation_rate = 0.0
                m_met_this_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0) >= self.m
                my_action_this_round = history.my_actions[-1]
                if my_action_this_round:
                    if m_met_this_round:
                        self.cooperation_pressure *= 1.05
                    else:
                        self.cooperation_pressure *= 0.95
                elif not m_met_this_round:
                    self.cooperation_pressure *= 1.02
            cooperation_gap = self.m - (self.n - 1)
            if cooperation_gap <= 0:
                return Action.D
            else:
                estimated_others_cooperating = math.floor(self.estimated_others_cooperation_rate * (self.n - 1))
                estimated_players_cooperating_if_i_cooperate = estimated_others_cooperating + 1
                if estimated_players_cooperating_if_i_cooperate >= self.m:
                    return Action.C
                else:
                    return Action.D

class Strategy_AGGRESSIVE_60(BaseStrategy):
    """
    The Aggressive Strategy: "The Opportunistic Enforcer".
    This strategy aims to maximize individual gain by leveraging the collective reward
    while minimizing personal risk. It's aggressive, pushing for cooperation when the
    reward is attainable and exploiting situations where others cooperate.
    It defects when cooperation is unlikely or personally disadvantageous.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def _count_cooperators(self, history: PlayerHistory, round_idx: int) -> int:
        """Counts the number of players (including self) who cooperated in a given round."""
        if round_idx < 0 or round_idx >= len(history.my_actions):
            return 0
        my_coop = 1 if history.my_actions[round_idx] else 0
        opp_coop = np.sum(history.opponent_actions[round_idx, :])
        return my_coop + opp_coop

    def _get_my_action(self, history: PlayerHistory, round_idx: int) -> Action:
        """Returns my action in a given round."""
        if round_idx < 0 or round_idx >= len(history.my_actions):
            return Action.D
        return Action.C if history.my_actions[round_idx] else Action.D

    def _calculate_cooperation_trend(self, history: PlayerHistory, current_round_idx: int) -> int:
        """
        Calculates a simple cooperation trend based on the last 2-3 rounds.
        Returns:
            1: Increasing cooperation
           -1: Decreasing cooperation
            0: Stable or mixed
        """
        if current_round_idx < 2:
            return 0
        cooperators_last = self._count_cooperators(history, current_round_idx - 1)
        cooperators_prev = self._count_cooperators(history, current_round_idx - 2)
        cooperators_prev_prev = 0
        if current_round_idx >= 3:
            cooperators_prev_prev = self._count_cooperators(history, current_round_idx - 3)
        increases = 0
        decreases = 0
        if cooperators_last > cooperators_prev:
            increases += 1
        elif cooperators_last < cooperators_prev:
            decreases += 1
        if current_round_idx >= 3:
            if cooperators_prev > cooperators_prev_prev:
                increases += 1
            elif cooperators_prev < cooperators_prev_prev:
                decreases += 1
        if increases > decreases:
            return 1
        elif decreases > increases:
            return -1
        else:
            return 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if history is None:
            return Action.D
        if current_round_t == self.r:
            observed_cooperators_last_round = self._count_cooperators(history, current_round_t - 1)
            if observed_cooperators_last_round >= self.m:
                return Action.C
            else:
                return Action.D
        observed_cooperators_last_round = self._count_cooperators(history, current_round_t - 1)
        my_action_last_round = self._get_my_action(history, current_round_t - 1)
        cus = max(0, observed_cooperators_last_round - (self.m - 1))
        ct = self._calculate_cooperation_trend(history, current_round_t)
        if cus > 0 and ct >= 0:
            if my_action_last_round == Action.C:
                return Action.C
            else:
                return Action.D
        elif cus == 1 and ct >= 0:
            if my_action_last_round == Action.C:
                return Action.C
            else:
                return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_61(BaseStrategy):
    """
    The Calculated Risk-Taker: Aggressively cooperates when success is probable,
    defects to minimize losses otherwise. Cooperates in the first round as an
    initial gamble. In subsequent rounds, cooperates if the previous round's
    cooperation level suggests the threshold 'm' is achievable with minimal
    additional cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        k = self.game_description.k
        if history is None:
            return Action.C
        current_round_number = history.round_number
        previous_round_index = current_round_number - 1
        total_cooperators_previous_round = np.sum(history.my_actions[previous_round_index]) + np.sum(history.opponent_actions[previous_round_index, :])
        if total_cooperators_previous_round >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_62(BaseStrategy):
    """
    The Opportunistic Enforcer strategy:
    This strategy aggressively pursues self-gain by defecting in the first and last rounds.
    In intermediate rounds, it cooperates only when the probability of meeting the
    cooperation threshold is high, especially if its own cooperation is critical.
    It also exploits situations where others have successfully met the threshold,
    benefiting from cooperation without taking the initial risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.opponent_cooperation_tendency: NDArray[np.float64] = np.full(self.n - 1, 0.5)
        self.last_round_cooperation_count = 0
        self.my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.my_last_action = Action.D
            self.last_round_cooperation_count = 0
            return Action.D
        my_action_last_round = self.my_last_action
        cooperators_in_last_round = np.sum(history.opponent_actions[-1, :]) + (1 if my_action_last_round == Action.C else 0)
        self.last_round_cooperation_count = cooperators_in_last_round
        if self.n > 1:
            recent_opponent_actions = history.opponent_actions[-1, :]
            for i in range(self.n - 1):
                opponent_played_c = recent_opponent_actions[i]
                self.opponent_cooperation_tendency[i] = self.opponent_cooperation_tendency[i] * 0.9 + (1 if opponent_played_c else 0) * 0.1
        if current_round == self.r - 1:
            self.my_last_action = Action.D
            return Action.D
        base_cpe_from_collective = self.last_round_cooperation_count / self.n
        avg_individual_tendency = np.mean(self.opponent_cooperation_tendency) if self.n > 1 else 0.0
        CPE = base_cpe_from_collective * 0.7 + avg_individual_tendency * 0.3
        cooperators_needed_if_i_cooperate = max(0, self.m - 1)
        cooperators_needed_if_i_defect = max(0, self.m)
        prob_threshold_met_if_i_cooperate = CPE
        prob_threshold_met_if_i_defect = CPE
        if self.last_round_cooperation_count >= self.m:
            self.my_last_action = Action.C
            return Action.C
        if CPE * self.n >= self.m - 1:
            self.my_last_action = Action.C
            return Action.C
        else:
            self.my_last_action = Action.D
            return Action.D

class Strategy_AGGRESSIVE_63(BaseStrategy):
    """
    "The Calculated Risk-Taker" strategy for the Collective Risk Dilemma.

    This strategy defaults to defecting, prioritizing individual endowment.
    It cooperates only when there is a high probability that the minimum
    number of cooperators (m) will be met or exceeded, calculated based on
    observed historical cooperation propensities of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.COOP_CONFIDENCE_THRESHOLD = 0.9
        self.COOP_THRESHOLD = 0.7
        self.DEFECT_THRESHOLD = 0.3
        self.PROPENSITY_UPDATE_DELTA = 0.05
        self.PROACTIVE_COOPERATE_THRESHOLD = 0.7
        self.opponent_propensities: dict[int, float] = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            for i in range(n - 1):
                self.opponent_propensities[i] = 0.5
            return Action.D
        if history.round_number > 0:
            previous_round_idx = history.round_number - 1
            for opponent_idx in range(n - 1):
                opp_action_last_round = history.opponent_actions[previous_round_idx, opponent_idx]
                current_propensity = self.opponent_propensities[opponent_idx]
                if opp_action_last_round:
                    new_propensity = min(1.0, current_propensity + self.PROPENSITY_UPDATE_DELTA)
                else:
                    new_propensity = max(0.0, current_propensity - self.PROPENSITY_UPDATE_DELTA)
                self.opponent_propensities[opponent_idx] = new_propensity
        strong_cooperators_count = 0
        for opponent_idx, propensity in self.opponent_propensities.items():
            if propensity >= self.COOP_THRESHOLD:
                strong_cooperators_count += 1
        cpe = 0.0
        if strong_cooperators_count >= m:
            cpe = 1.0
        elif strong_cooperators_count > 0:
            cpe = min(1.0, strong_cooperators_count / m * 0.8)
        else:
            cpe = 0.1
        proactive_cooperate = False
        if history.round_number > 0:
            previous_round_actions = history.opponent_actions[previous_round_idx, :]
            observed_cooperators_last_round = np.sum(previous_round_actions)
            if observed_cooperators_last_round == m - 1 and cpe >= self.PROACTIVE_COOPERATE_THRESHOLD:
                proactive_cooperate = True
        if proactive_cooperate:
            return Action.C
        elif cpe >= self.COOP_CONFIDENCE_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_64(BaseStrategy):
    """
    The Unyielding Opportunist strategy.

    This strategy is aggressive, aiming to exploit cooperation when advantageous
    and punish defection decisively. It prioritizes personal gain, especially
    in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the UnyieldingOpportunist strategy.

        Args:
            game_description: A dataclass containing the game parameters (n, r, m, k).
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round.

        Returns:
            Action.C if cooperating, Action.D if defecting.
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        previous_cooperators = np.sum(history.my_actions[-1] or history.opponent_actions[-1, :])
        if previous_cooperators < self.m:
            return Action.D
        if previous_cooperators >= self.m - 1 and self.n <= self.m * self.k:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_65(BaseStrategy):
    """
    Aggressive "Opportunistic Sentinel" Strategy for the Collective Risk Dilemma.
    This strategy aims to maximize expected payoff by exploiting opportunities for
    high rewards while remaining vigilant against exploitation. It adjusts its
    behavior based on the observed cooperation rate and its own past outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_player_index = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if history is None:
            return Action.D
        prev_round = current_round - 1
        my_prev_action = Action.D if not history.my_actions[prev_round] else Action.C
        num_prev_cooperators_others = np.sum(history.opponent_actions[prev_round, :])
        action = Action.D
        if current_round <= r / 3:
            if num_prev_cooperators_others >= m - 2:
                action = Action.C
        elif r / 3 < current_round <= 2 * r / 3:
            total_prev_cooperators = num_prev_cooperators_others + (1 if my_prev_action == Action.C else 0)
            threshold_met_prev = total_prev_cooperators >= m
            if my_prev_action == Action.C and threshold_met_prev:
                action = Action.C
            elif my_prev_action == Action.D and threshold_met_prev:
                if num_prev_cooperators_others + 1 >= m:
                    action = Action.C
        else:
            total_prev_cooperators = num_prev_cooperators_others + (1 if my_prev_action == Action.C else 0)
            threshold_met_prev = total_prev_cooperators >= m
            if my_prev_action == Action.C and threshold_met_prev:
                action = Action.C
            elif num_prev_cooperators_others >= m - 1:
                action = Action.C
        return action

class Strategy_AGGRESSIVE_66(BaseStrategy):
    """
    The Opportunistic Dominator strategy.
    This strategy aims to maximize individual payoff by cooperating only when
    it guarantees a significantly higher payoff than defecting, and defecting otherwise.
    It exploits successful cooperation and avoids risky ventures.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def _count_cooperators(self, actions_data: NDArray[np.bool_], current_round_idx: int) -> int:
        """Counts the number of cooperators in a given round."""
        if current_round_idx < 0 or current_round_idx >= actions_data.shape[0]:
            return 0
        return np.sum(actions_data[current_round_idx, :])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        player_id = -1
        if history is None:
            if self.n_players - self.m < 2:
                return Action.C
            else:
                return Action.D
        previous_round_idx = current_round - 1
        my_payoff_prev_round = history.my_payoffs[previous_round_idx]
        num_cooperators_prev_round = self._count_cooperators(history.opponent_actions, previous_round_idx) + (1 if history.my_actions[previous_round_idx] == Action.C else 0)
        if num_cooperators_prev_round >= self.m:
            if my_payoff_prev_round == 1.0:
                return Action.D
            else:
                return Action.C
        elif num_cooperators_prev_round == self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_67(BaseStrategy):
    """
    The "Opportunistic Enforcer" strategy for the Collective Risk Dilemma.

    This strategy is designed to be aggressive by prioritizing self-interest while
    simultaneously exploiting the cooperative reward when it's most beneficial and
    least risky. It operates on a principle of conditional enforcement and
    opportunistic exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round_num = history.round_number
        last_round_my_action = history.my_actions[current_round_num - 1]
        last_round_opponent_actions = history.opponent_actions[current_round_num - 1, :]
        current_cooperators_observed = sum(last_round_opponent_actions)
        if last_round_my_action:
            current_cooperators_observed += 1
        current_players_observed = self.n
        if current_cooperators_observed >= self.m:
            return Action.C
        if current_cooperators_observed < self.m:
            if current_cooperators_observed + 1 < self.m:
                return Action.D
        if current_cooperators_observed + 1 >= self.m:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_68(BaseStrategy):
    """
    The Opportunistic Enforcer strategy for the Collective Risk Dilemma.

    This strategy adopts an aggressive stance, prioritizing individual payoff.
    It cooperates only when the likelihood of reaching the cooperation threshold 'm'
    is very high, particularly if it has previously benefited from others'
    cooperation (free-riding). Otherwise, it defaults to defecting to preserve
    its endowment. It also defects in the very first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with the game description.

        Args:
            game_description: An object containing the game's parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_actions_history = []
        self.my_payoffs_history = []
        self.opponent_actions_history = []
        self.opponent_payoffs_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players up to the
                     previous round. None if it's the first round.

        Returns:
            The chosen Action (Action.C or Action.D).
        """
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        if history.round_number == 0:
            prev_round_index = 0
        else:
            prev_round_index = history.round_number - 1
        coop_count_prev = 0
        for i in range(self.n - 1):
            if history.opponent_actions[prev_round_index, i]:
                coop_count_prev += 1
        if history.my_actions[prev_round_index]:
            coop_count_prev += 1
        your_payoff_in_prev_round = history.my_payoffs[prev_round_index]
        you_cooperated_prev = history.my_actions[prev_round_index]
        threshold_met_prev = coop_count_prev >= self.m
        confidence_level = 0
        if coop_count_prev >= self.m:
            confidence_level = 2
        elif coop_count_prev == self.m - 1:
            confidence_level = 1
        elif coop_count_prev >= self.m - 2 and self.m > 2:
            confidence_level = 1
        elif coop_count_prev >= 1 and self.m == 2:
            confidence_level = 1
        else:
            confidence_level = 0
        if confidence_level == 2 and (not you_cooperated_prev) and threshold_met_prev:
            return Action.C
        elif confidence_level >= 1:
            if confidence_level == 1 and (not you_cooperated_prev) and (not threshold_met_prev):
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_69(BaseStrategy):
    """
    The Reckoning Strategy: An aggressive Collective Risk Dilemma strategy that
    defaults to defection but cooperates when there's a strong signal that
    collective success is likely and the reward can be secured. It adapts
    its cooperation threshold based on the criticality of 'm' relative to 'n'.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): Historical data of actions and payoffs
                                            if available (None for the first round).

        Returns:
            Action: The action to take ('C' for Cooperate, 'D' for Defect).
        """
        if history is None:
            return Action.D
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        my_cooperation_prev = 1 if history.my_actions[-1] else 0
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        cooperators_in_prev_round = my_cooperation_prev + opponent_cooperators_prev
        if self.m >= self.n - 2:
            if cooperators_in_prev_round >= self.m - 1:
                return Action.C
            else:
                return Action.D
        elif self.m > self.n / 2:
            if cooperators_in_prev_round >= self.m:
                return Action.C
            else:
                return Action.D
        elif cooperators_in_prev_round >= self.m + 1:
            return Action.C
        else:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_70(BaseStrategy):
    """
    Dominant Aggressor Strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal gain by cooperating only when
    the success of the collective project is highly probable, and defecting
    otherwise. It features an aggressive start and a purely selfish last round.
    The strategy adapts its risk tolerance based on observed cooperation rates
    and the progression of the game through a dynamic risk buffer.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.risk_buffer = float(self.n - self.m + 1)
        self.observed_cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None:
            self.observed_cooperators_last_round = 0
            return Action.D
        if history.round_number > 0:
            self.observed_cooperators_last_round = np.sum(history.opponent_actions[history.round_number - 1, :])
        cooperation_likely = self.observed_cooperators_last_round + self.risk_buffer >= self.m
        learning_rate_buffer_decay = 0.1
        learning_rate_buffer_increase_low_coop = 0.5
        self.risk_buffer = max(0.0, self.risk_buffer - learning_rate_buffer_decay)
        if self.observed_cooperators_last_round < self.m / 2.0:
            self.risk_buffer = min(float(self.n - self.m + 1), self.risk_buffer + learning_rate_buffer_increase_low_coop)
        self.risk_buffer = max(0.0, min(float(self.n - self.m + 1), self.risk_buffer))
        if cooperation_likely:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_71(BaseStrategy):
    """
    The "Calculated Risk-Taker" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by pushing for cooperation
    when the reward is high and the risk is manageable, while defecting ruthlessly
    when cooperation seems unlikely or costly. It's about being the catalyst for
    success when beneficial, and a smart survivor when not.

    Core Philosophy:
    - Leverage the high reward (k) for meeting the threshold (m).
    - Minimize personal cost by avoiding being the "sucker".
    - Adapt to observed behavior of others.
    - Aggressively pursue favorable cooperative outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.early_game_threshold = math.ceil(self.r / 2)
        self.very_late_game_threshold = math.ceil(self.r / 4)
        self.close_to_m_threshold_buffer = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if history is None:
            return Action.C
        completed_rounds = history.round_number
        total_cooperators_past = np.sum(history.my_actions[:completed_rounds])
        total_cooperators_past += np.sum(history.opponent_actions[:completed_rounds, :])
        total_actions_past = completed_rounds * self.n
        cooperation_rate_past_rounds = total_cooperators_past / total_actions_past if total_actions_past > 0 else 0.0
        own_successful_cooperations = 0
        for i in range(completed_rounds):
            if history.my_actions[i] == Action.C.value:
                cooperators_this_round = np.sum(history.opponent_actions[i, :]) + 1
                if cooperators_this_round >= self.m:
                    own_successful_cooperations += 1
        rounds_remaining = self.r - current_round_number
        if cooperation_rate_past_rounds >= (self.m - 1) / self.n and rounds_remaining > self.early_game_threshold:
            return Action.C
        if cooperation_rate_past_rounds < (self.m - 1) / self.n and rounds_remaining < self.very_late_game_threshold:
            return Action.D
        last_round_cooperators = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] == Action.C.value else 0)
        if cooperation_rate_past_rounds >= (self.m - 1) / self.n - self.close_to_m_threshold_buffer and rounds_remaining > 1 and (last_round_cooperators >= self.m - 1):
            return Action.C
        own_cooperation_frequency = np.sum(history.my_actions[:completed_rounds]) / completed_rounds if completed_rounds > 0 else 0.0
        if own_cooperation_frequency < 0.5 and rounds_remaining < self.early_game_threshold:
            return Action.D
        if cooperation_rate_past_rounds > (self.m - 1) / self.n - 0.1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_72(BaseStrategy):
    """
    The "Calculated Risk" strategy for the Collective Risk Dilemma.
    This aggressive strategy aims to maximize individual payoff by
    dynamically adjusting an "Aggression Threshold" based on the history
    of observed cooperation. It seeks to secure the reward 'k' when
    realistically achievable, while protecting against failure by
    adapting its cooperation level to the perceived behavior of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.current_aggression_threshold = 0.0
        self.opponent_cooperation_counts = np.zeros(self.n_players - 1, dtype=np.float64)
        self.my_player_index = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_number = state.round_number
        if history is None:
            self.my_player_index = 0
            self.opponent_cooperation_counts = np.zeros(self.n_players - 1, dtype=np.float64)
            if self.m <= 2:
                self.current_aggression_threshold = 0.0
            else:
                self.current_aggression_threshold = float(self.m - 2)
            if 0 >= self.current_aggression_threshold:
                return Action.C
            else:
                return Action.D
        last_round_idx = round_number - 1
        if round_number > 1:
            observed_cooperators_last_round = np.sum(history.opponent_actions[last_round_idx, :])
        else:
            observed_cooperators_last_round = 0.0
        expected_total_cooperators_if_I_cooperate = observed_cooperators_last_round + 1.0
        will_cooperate = expected_total_cooperators_if_I_cooperate >= self.m and observed_cooperators_last_round >= self.current_aggression_threshold
        my_action = Action.C if will_cooperate else Action.D
        actual_cooperators_last_round = observed_cooperators_last_round + (1.0 if my_action == Action.C else 0.0)
        threshold_met_last_round = actual_cooperators_last_round >= self.m
        if threshold_met_last_round:
            self.current_aggression_threshold += 0.5
            if self.current_aggression_threshold > self.n_players - 1:
                self.current_aggression_threshold = float(self.n_players - 1)
        elif my_action == Action.C:
            self.current_aggression_threshold -= 0.5
            if self.current_aggression_threshold < 0.0:
                self.current_aggression_threshold = 0.0
        else:
            self.current_aggression_threshold += 0.2
            if self.current_aggression_threshold > self.n_players - 1:
                self.current_aggression_threshold = float(self.n_players - 1)
        if self.current_aggression_threshold < 0.0:
            self.current_aggression_threshold = 0.0
        if self.current_aggression_threshold > self.n_players - 1:
            self.current_aggression_threshold = float(self.n_players - 1)
        return my_action

class Strategy_AGGRESSIVE_73(BaseStrategy):
    """
    The Opportunistic Predator strategy is designed to maximize individual gain.
    It defaults to defecting and only cooperates if it guarantees a personal payoff
    increase and is crucial for meeting the cooperation threshold 'm'.
    It exploits the cooperation of others when it's personally advantageous.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.threshold_history_start_round = max(2, math.floor(self.n / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == self.r - 1:
            return Action.D
        total_opponent_cooperations_prev = 0
        num_other_players = self.n - 1
        if current_round_idx >= self.threshold_history_start_round:
            if num_other_players > 0:
                total_opponent_cooperations_prev = np.sum(history.opponent_actions[:current_round_idx, :])
                average_opponent_cooperation_rate = total_opponent_cooperations_prev / (num_other_players * current_round_idx)
                predicted_cooperators_from_others = math.floor(average_opponent_cooperation_rate * num_other_players)
            else:
                predicted_cooperators_from_others = 0
            my_potential_cooperation_count = predicted_cooperators_from_others + 1
            if my_potential_cooperation_count >= self.m:
                if self.k > 1:
                    return Action.C
        return Action.D

class Strategy_AGGRESSIVE_74(BaseStrategy):
    """
    The Dominator strategy: Aggressively initiates cooperation and severely punishes defection.
    It's willing to lead the charge for high rewards but demands reciprocity,
    defecting against anyone who doesn't contribute when it does.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_actions_history = []
        self.opponent_actions_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == r:
            return Action.D
        if history is None:
            self.my_actions_history.append(True)
            return Action.C
        current_round_index = state.round_number - 1
        if len(self.my_actions_history) <= current_round_index:
            self.my_actions_history.append(history.my_actions[current_round_index])
        if len(self.opponent_actions_history) <= current_round_index:
            self.opponent_actions_history.append(history.opponent_actions[current_round_index, :])
        prev_round_my_action = self.my_actions_history[current_round_index]
        prev_round_opponent_actions = self.opponent_actions_history[current_round_index]
        num_cooperators_prev_round = int(np.sum(prev_round_opponent_actions) + (1 if prev_round_my_action else 0))
        if num_cooperators_prev_round >= m:
            self.my_actions_history.append(True)
            return Action.C
        if num_cooperators_prev_round >= m - 1:
            self.my_actions_history.append(True)
            return Action.C
        if prev_round_my_action:
            opponents_who_defected_indices = np.where(prev_round_opponent_actions == False)[0]
            if len(opponents_who_defected_indices) > 0:
                self.my_actions_history.append(False)
                return Action.D
        self.my_actions_history.append(False)
        return Action.D

class Strategy_AGGRESSIVE_75(BaseStrategy):
    """
    The "Threshold Crusher" strategy is an aggressive approach to the Collective Risk Dilemma.
    It aims to maximize individual gains by cooperating only when the cooperation threshold
    is either met or is one step away from being met, leveraging past rounds' data.
    It defects in the first and last rounds, and when the group is far from the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        previous_round_idx = history.round_number - 1
        num_cooperators_prev_round = 0
        if previous_round_idx >= 0:
            num_cooperators_prev_round += np.sum(history.opponent_actions[previous_round_idx, :])
            if history.my_actions[previous_round_idx]:
                num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= self.m:
            return Action.C
        elif num_cooperators_prev_round < self.m - 1:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_76(BaseStrategy):
    """
    Aggressive "Threshold Predator" strategy for the Collective Risk Dilemma.
    This strategy aims to cooperate only when its action is likely to push
    the group over the cooperation threshold 'm', thereby securing the reward 'k'.
    It defaults to defecting to guarantee a baseline payoff of 1 when the
    threshold is unlikely to be met or when the risk is too high.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.D
        previous_round_index = current_round - 1
        previous_cooperators = int(history.my_actions[previous_round_index]) + np.sum(history.opponent_actions[previous_round_index, :])
        cooperation_gap = self.m - previous_cooperators
        if cooperation_gap <= 0:
            return Action.D
        if cooperation_gap == 1:
            return Action.C
        if cooperation_gap > 1:
            return Action.D
        if current_round == self.n_rounds - 1:
            if cooperation_gap == 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_77(BaseStrategy):
    """
    The "Calculated Gamble" strategy for the Collective Risk Dilemma.

    This strategy aims to maximize personal gain by exploiting cooperative
    efforts of others. It cooperates only if it's highly probable that
    the cooperation threshold will be met due to others' actions. It defects
    aggressively to protect its endowment when cooperation seems unlikely or
    in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.threshold_for_cooperation = max(1, self.m - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds:
            return Action.D
        if history is None:
            return Action.D
        previous_round_actions = np.concatenate([[history.my_actions[-1]], history.opponent_actions[-1, :]])
        observed_cooperators_prev_round = np.sum(previous_round_actions)
        if observed_cooperators_prev_round >= self.threshold_for_cooperation:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_78(BaseStrategy):
    """
    The "Tipping Point" Challenger strategy.
    This strategy is aggressive, initially defecting to secure private gains.
    It cooperates only when it's highly probable that the collective
    cooperation threshold will be met, aiming to maximize payoff by exploiting
    the reward factor 'k'. It's conditional on observed cooperation from others
    and the proximity to the end of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.CTF_INITIAL = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        all_player_actions_last_round = history.opponent_actions[current_round - 1, :]
        my_action_last_round = Action.C if history.my_actions[current_round - 1] else Action.D
        other_players_cooperated_last_round = np.sum(all_player_actions_last_round)
        num_other_players = self.n - 1
        if num_other_players > 0:
            obs_coop_rate_t_minus_1 = other_players_cooperated_last_round / num_other_players
        else:
            obs_coop_rate_t_minus_1 = 0.0
        if current_round == self.r:
            potential_cooperators_if_i_cooperate = other_players_cooperated_last_round + 1
            if potential_cooperators_if_i_cooperate >= self.m:
                return Action.C
            else:
                return Action.D
        else:
            CTF_current = self.CTF_INITIAL
            target_coop_rate_for_action = CTF_current * (self.m / self.n)
            total_cooperators_if_i_cooperate = other_players_cooperated_last_round + 1
            if obs_coop_rate_t_minus_1 >= target_coop_rate_for_action and total_cooperators_if_i_cooperate >= self.m:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_79(BaseStrategy):
    """
    The "Calculated Opportunist" strategy for the Collective Risk Dilemma.
    This strategy defaults to defecting, but cooperates if the group is very
    close to the cooperation threshold or if the reward factor 'k' is high
    relative to the number of players needed to cooperate. It punishes free-riders
    and exploitative behavior by immediately reverting to defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        prev_round_index = history.round_number - 1
        num_cooperators_prev_round = np.sum(history.my_actions[prev_round_index]) + np.sum(history.opponent_actions[prev_round_index, :])
        num_defectors_prev_round = self.n - num_cooperators_prev_round
        my_prev_action = Action.C if history.my_actions[prev_round_index] else Action.D
        if my_prev_action == Action.C and num_cooperators_prev_round < self.m:
            return Action.D
        if num_cooperators_prev_round < self.m - 1 and num_defectors_prev_round > num_cooperators_prev_round:
            return Action.D
        if num_cooperators_prev_round >= self.m - 1:
            return Action.C
        if self.k > self.n - self.m:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_80(BaseStrategy):
    """
    Aggressive "Leapfrog" Strategy for Collective Risk Dilemma.

    This strategy aims to maximize personal payoff by being decisive and taking
    calculated risks. It cooperates proactively when success seems likely,
    defects opportunistically when cooperation is improbable, and makes a final
    push in the last round if success is within reach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        num_cooperators_last_round = sum(history.opponent_actions[current_round - 1, :])
        perceived_coop_likelihood = num_cooperators_last_round / n
        momentum_factor = 0
        if current_round > 1:
            prev_num_cooperators_last_round = sum(history.opponent_actions[current_round - 2, :])
            if num_cooperators_last_round > prev_num_cooperators_last_round:
                momentum_factor = 0.1
            elif num_cooperators_last_round < prev_num_cooperators_last_round:
                momentum_factor = -0.1
        perceived_coop_likelihood += momentum_factor
        early_phase_end = r - max(0, math.floor(r * 0.2))
        if current_round < early_phase_end and perceived_coop_likelihood > 0.7:
            return Action.C
        if m - num_cooperators_last_round <= 2 and current_round < r - 1:
            return Action.C
        if num_cooperators_last_round < m - 1 and current_round > 1:
            return Action.D
        if current_round == 2 and num_cooperators_last_round < m - 1:
            return Action.D
        if current_round == r:
            if num_cooperators_last_round >= m - 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_81(BaseStrategy):
    """
    Aggressive "Exploiter" Strategy for the Collective Risk Dilemma.

    This strategy prioritizes self-interest, aiming to free-ride on others'
    cooperation. It cooperates only when there's a clear indication of
    significant personal gain or to push a near-certain successful outcome
    over the edge, especially if it was previously a free-rider.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.weight_risk = 0.5
        self.weight_past_payoff = 2.0
        self.weight_loss_aversion = 1.0
        self.threshold_cooperate = 1.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == r:
            return Action.D
        if history is None:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev = Action.C if history.my_actions[prev_round_idx] else Action.D
        num_players_cooperated_prev = np.sum(history.opponent_actions[prev_round_idx, :]) + (1 if my_action_prev == Action.C else 0)
        opponent_actions_this_round = history.opponent_actions[prev_round_idx, :]
        num_opponents_cooperated_prev = num_players_cooperated_prev - (1 if my_action_prev == Action.C else 0)
        if n - 1 > 0:
            average_opponent_cooperation_rate = num_opponents_cooperated_prev / (n - 1)
        else:
            average_opponent_cooperation_rate = 0.0
        if m - 1 <= num_players_cooperated_prev <= n - 1 and my_action_prev == Action.D:
            return Action.C
        your_past_cooperation_cost = 1 if my_action_prev == Action.C else 0
        your_potential_loss_if_no_cooperation = 1.0
        exploitation_potential = self.weight_risk * your_past_cooperation_cost + self.weight_past_payoff * average_opponent_cooperation_rate - self.weight_loss_aversion * your_potential_loss_if_no_cooperation
        if exploitation_potential > self.threshold_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_82(BaseStrategy):
    """
    The Opportunistic Enforcer strategy is an aggressive, self-interested approach
    to the Collective Risk Dilemma. It prioritizes maximizing individual gain
    by cooperating only when there's a high probability of achieving the collective
    reward (m cooperators). It defects by default to avoid being a "sucker" and
    to guarantee a minimum payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            return Action.D
        current_round = history.round_number
        own_cooperated_prev_round = 1 if self.game_description.m > 1 and history.my_actions[current_round - 1] else 0
        opponent_cooperators_prev_round = int(np.sum(history.opponent_actions[current_round - 1, :]))
        observed_cooperators_prev_round = own_cooperated_prev_round + opponent_cooperators_prev_round
        potential_future_cooperators = 0
        expected_cooperators = observed_cooperators_prev_round + potential_future_cooperators
        if expected_cooperators >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_84(BaseStrategy):
    """
    Aggressive Opportunist: Maximizes personal gain by defecting by default,
    cooperating only when close to the threshold in early/mid-game or as a
    last resort in the final stages, and always defecting when the reward
    is guaranteed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        previous_round_actions = history.my_actions[:current_round]
        opponent_actions_prev_round = history.opponent_actions[current_round - 1, :]
        all_previous_actions = np.append(previous_round_actions[-1], opponent_actions_prev_round)
        num_cooperators_previous_round = np.sum(all_previous_actions)
        if current_round == self.r:
            return Action.D
        if current_round < self.r / 2:
            if num_cooperators_previous_round == self.m - 1:
                return Action.C
            elif num_cooperators_previous_round >= self.m:
                return Action.D
            else:
                return Action.D
        elif num_cooperators_previous_round == self.m - 1 and current_round == self.r - 1:
            return Action.C
        elif num_cooperators_previous_round >= self.m:
            return Action.D
        elif num_cooperators_previous_round == self.m - 1:
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_85(BaseStrategy):
    """
    The "Opportunistic Ascendancy" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by aggressively pursuing
    the higher reward (k) while minimizing risk. It leverages the repeated
    nature of the game to adapt to player behavior, but always prioritizes
    self-gain and dominance. It's an aggressive strategy in that it aims to
    trigger the reward as quickly and consistently as possible, pushing the
    boundaries of cooperation when it's most advantageous.

    Core Philosophy: "I will contribute to success if and only if it is
    demonstrably beneficial to me. If others fail to contribute sufficiently,
    I will capitalize on their inaction. If they succeed, I will join them and
    reap the rewards. I will never be the sole reason for failure, nor will
    I shy away from prosperity."
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.total_cooperators_in_history_sum = 0.0
        self.total_rounds_played = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.D
        cooperators_in_previous_round = 0
        if history.round_number > 0:
            last_round_idx = history.round_number - 1
            cooperators_in_previous_round = np.sum(history.opponent_actions[last_round_idx, :])
        if current_round > 0:
            self.total_cooperators_in_history_sum += cooperators_in_previous_round
            self.total_rounds_played = history.round_number
        proportion_of_cooperators_in_history = 0.0
        if self.total_rounds_played > 0 and self.n - 1 > 0:
            proportion_of_cooperators_in_history = self.total_cooperators_in_history_sum / ((self.n - 1) * self.total_rounds_played)
        if cooperators_in_previous_round >= self.m - 2 and current_round < self.r:
            return Action.C
        if cooperators_in_previous_round >= self.m - 1 and current_round < self.r:
            return Action.C
        if current_round > self.r / 4 and proportion_of_cooperators_in_history > 0.7 and (cooperators_in_previous_round >= self.m - 1):
            return Action.C
        if current_round == self.r:
            if cooperators_in_previous_round >= self.m - 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_86(BaseStrategy):
    """
    The Opportunistic Maverick strategy:
    Defects by default. Cooperates only if it guarantees a superior outcome
    (i.e., securing the reward k) or as a calculated gamble in the final round
    when on the cusp of success (m-1 cooperators previously).
    Free-rides if the reward is already guaranteed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        action = Action.D
        if history is None:
            return Action.D
        previous_round_cooperators = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if current_round < r:
            if previous_round_cooperators == m - 1:
                action = Action.C
        if current_round == r:
            if previous_round_cooperators == m - 1:
                action = Action.C
        if previous_round_cooperators >= m:
            action = Action.D
        return action

class Strategy_AGGRESSIVE_88(BaseStrategy):
    """
    The Leveraged Opportunist strategy for the Collective Risk Dilemma.
    This strategy is aggressive, defaulting to defection to maximize private payoff.
    It cooperates only when it's strategically advantageous and its contribution
    is essential to unlocking a higher payoff (guaranteed reward 'k'), ideally
    with minimal risk of being the sole contributor.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description (CollectiveRiskDescription): Object containing game parameters.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): History of past actions and payoffs,
                                             or None if it's the first round.

        Returns:
            Action: The action to take in the current round (Action.C or Action.D).
        """
        current_round_t = state.round_number
        if current_round_t == 0 or current_round_t == self.r - 1:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round_t - 1, :]
        num_prev_cooperators = int(np.sum(previous_round_actions))
        if num_prev_cooperators < self.m and num_prev_cooperators + 1 >= self.m:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_89(BaseStrategy):
    """
    Aggressive "Threshold Guardian" Strategy for the Collective Risk Dilemma.

    This strategy aims to proactively push for the collective reward by cooperating
    when there's a strong indication others will too. It exhibits risk tolerance by
    initiating cooperation near the threshold and punishes collective failure by
    defecting when confidence is low.

    Core Philosophy: "I will push for the collective reward, but only when I'm
    confident enough others are too. If the collective falters due to others'
    weakness, I will punish that failure by defecting. The reward is worth the
    calculated risk."
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.round_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_counter = state.round_number
        if history is None:
            return Action.D
        if self.round_counter == self.r:
            return Action.D
        num_previous_rounds = history.round_number
        last_round_my_action = history.my_actions[num_previous_rounds - 1]
        last_round_opponent_actions = history.opponent_actions[num_previous_rounds - 1, :]
        n_c_prev = sum(last_round_opponent_actions) + (1 if last_round_my_action else 0)
        n_d_prev = self.n - n_c_prev
        total_cooperators_so_far = sum(history.my_actions[:num_previous_rounds]) + np.sum(history.opponent_actions[:num_previous_rounds, :])
        avg_cooperators_per_player = total_cooperators_so_far / (num_previous_rounds * self.n)
        base_cm_t = avg_cooperators_per_player * self.n
        aggressive_adjustment = 0.0
        defensive_adjustment = 0.0
        if n_c_prev >= self.m and n_d_prev > 0:
            aggressive_adjustment = n_d_prev / self.n * (self.k - 1)
        elif n_c_prev < self.m:
            defensive_adjustment = -(self.n - n_c_prev) / self.n
        cm_t = base_cm_t + aggressive_adjustment + defensive_adjustment
        cm_t = max(0.0, min(float(self.n), cm_t))
        if cm_t >= self.m - 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_90(BaseStrategy):
    """
    Opportunistic Aggression strategy for the Collective Risk Dilemma.

    This strategy prioritizes self-interest and seeks to maximize personal gain
    by cooperating only when there's a high perceived probability of success
    and defecting otherwise. It adapts its cooperation probability based on
    past outcomes, penalizing its own failed cooperation attempts and
    leveraging observed opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_coop = 0.75
        self.smoothing_factor = 0.3
        self.my_history = []
        self.avg_opponent_coop_rate_smoothed = 0.5

    def _calculate_smoothed_opponent_coop_rate(self, history: PlayerHistory) -> float:
        """
        Calculates a smoothed average of how many opponents cooperated in recent rounds.
        """
        n = self.game_description.n_players
        if n <= 1:
            return 0.0
        lookback_rounds = min(history.round_number, 5)
        if lookback_rounds == 0:
            return self.avg_opponent_coop_rate_smoothed
        recent_opponent_coop_ratios = []
        for r in range(history.round_number - lookback_rounds, history.round_number):
            my_action_bool = history.my_actions[r]
            num_cooperators_this_round = np.sum(history.opponent_actions[r, :]) + (1 if my_action_bool else 0)
            num_others_cooperated = num_cooperators_this_round - (1 if my_action_bool else 0)
            recent_opponent_coop_ratios.append(num_others_cooperated / (n - 1))
        avg_recent_coop = np.mean(recent_opponent_coop_ratios)
        smoothed_rate = self.smoothing_factor * avg_recent_coop + (1 - self.smoothing_factor) * self.avg_opponent_coop_rate_smoothed
        return smoothed_rate

    def _update_player_state(self, history: PlayerHistory):
        """
        Updates internal state based on the latest round's outcome.
        """
        if history is None or history.round_number == 0:
            return
        current_round_idx = history.round_number - 1
        my_last_action_bool = history.my_actions[current_round_idx]
        my_last_payoff = history.my_payoffs[current_round_idx]
        num_cooperators_this_round = np.sum(history.opponent_actions[current_round_idx, :]) + (1 if my_last_action_bool else 0)
        self.my_history.append((my_last_action_bool, my_last_payoff, num_cooperators_this_round))
        self.avg_opponent_coop_rate_smoothed = self._calculate_smoothed_opponent_coop_rate(history)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        self._update_player_state(history)
        P_coop_t_estimate = self.avg_opponent_coop_rate_smoothed
        if self.my_history:
            my_last_action_bool, my_last_payoff, _ = self.my_history[-1]
            if my_last_action_bool:
                if my_last_payoff == 1.0:
                    P_coop_t_estimate *= 0.3
                elif my_last_payoff > 1.0:
                    P_coop_t_estimate *= 1.1
            elif my_last_payoff > 1.0:
                pass
        P_coop_t_estimate = max(0.0, min(1.0, P_coop_t_estimate))
        if P_coop_t_estimate >= self.threshold_coop:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_91(BaseStrategy):
    """
    Aggressive Strategy: The Opportunistic Enforcer.
    This strategy aims to maximize individual payoff by cooperating only when
    there's a strong perceived likelihood of meeting the cooperation threshold (m),
    and defecting otherwise. It exhibits proactive cooperation to secure rewards
    and punitive defection to avoid exploitation. It has specific logic for the
    first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        my_action_prev_round = self.my_actions[current_round - 1] if hasattr(self, 'my_actions') and self.my_actions.size > 0 else True
        opponent_actions_prev_round = self.opponent_actions[current_round - 1, :] if hasattr(self, 'opponent_actions') and self.opponent_actions.shape[0] > 0 else np.array([True] * (self.n - 1))
        all_actions_prev_round = np.append(opponent_actions_prev_round, my_action_prev_round)
        cooperators_previous_round = np.sum(all_actions_prev_round)
        cooperation_momentum = cooperators_previous_round / self.n
        if cooperation_momentum >= (self.m - 1.0) / self.n and cooperation_momentum > 0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_92(BaseStrategy):
    """
    Aggressive strategy for the Collective Risk Dilemma.
    Aims to maximize payoff by strategically cooperating to achieve the reward
    when conditions are favorable, and defecting to guarantee baseline payoff
    when cooperation is risky or failed. It punishes defection indirectly by
    not rewarding failed cooperative efforts and ruthlessly defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        rounds_remaining = self.r - current_round
        if history is None:
            return Action.C
        my_actions = history.my_actions
        opponent_actions = history.opponent_actions
        num_completed_rounds = history.round_number
        last_round_my_action = my_actions[-1] if num_completed_rounds > 0 else None
        last_round_opponent_actions = opponent_actions[-1, :] if num_completed_rounds > 0 else None
        cooperators_last_round = 0
        if last_round_my_action is not None:
            cooperators_last_round += last_round_my_action
        if last_round_opponent_actions is not None:
            cooperators_last_round += np.sum(last_round_opponent_actions)
        if rounds_remaining == 0:
            return Action.D
        if cooperators_last_round < self.m and last_round_my_action == True:
            return Action.D
        cooperation_momentum_high = False
        if cooperators_last_round >= self.m - 1:
            cooperation_momentum_high = True
        if num_completed_rounds >= 3:
            recent_opponent_cooperation_rates = np.mean(opponent_actions[-3:, :], axis=1)
            if np.mean(recent_opponent_cooperation_rates) > 0.75:
                cooperation_momentum_high = True
        elif num_completed_rounds > 0:
            if np.mean(opponent_actions[-num_completed_rounds:, :]) > 0.75:
                cooperation_momentum_high = True
        if cooperation_momentum_high:
            return Action.C
        if cooperators_last_round >= self.m and self.n - cooperators_last_round <= self.n - self.m:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_93(BaseStrategy):
    """
    The "Opportunistic Enforcer" strategy.
    This aggressive strategy prioritizes maximizing individual payoff by capitalizing
    on collective cooperation when the threshold is likely to be met, while defecting
    to secure a baseline payoff when cooperation is uncertain or unsuccessful.
    It does not initiate cooperation but waits for signals of collective success.
    It defects in the first and last rounds to maximize immediate personal gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        if history is not None:
            my_actions = history.my_actions
            opponent_actions = history.opponent_actions
            cooperators_this_round = np.sum(opponent_actions[current_round - 1, :]) + int(my_actions[current_round - 1])
        else:
            return Action.D
        should_cooperate_based_on_history = False
        if cooperators_this_round >= self.m:
            should_cooperate_based_on_history = True
        elif cooperators_this_round == self.m - 1 and my_actions[current_round - 1] == Action.C.value:
            should_cooperate_based_on_history = True
        if should_cooperate_based_on_history:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_94(BaseStrategy):
    """
    Aggressive "Dominant Defector" strategy for the Collective Risk Dilemma.
    This strategy prioritizes self-interest by defaulting to defecting,
    only cooperating when there's a high certainty of reward for oneself,
    leveraging the observed cooperation of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threat_meter = 0
        self.my_previous_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            self.threat_meter = 0
            self.my_previous_action = Action.D
            return Action.D
        current_round_number = state.round_number
        observed_cooperators_prev_round = 0
        if current_round_number > 0:
            observed_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            if self.my_previous_action == Action.C:
                observed_cooperators_prev_round += 1
        if observed_cooperators_prev_round >= m:
            self.threat_meter += 1
        else:
            self.threat_meter = max(-5, self.threat_meter - 1)
        current_action = Action.D
        if current_round_number == r:
            if observed_cooperators_prev_round == n - 1 and n - 1 >= m:
                current_action = Action.C
            else:
                current_action = Action.D
        elif observed_cooperators_prev_round < m - 1:
            current_action = Action.D
        elif observed_cooperators_prev_round == m - 1:
            if self.threat_meter > 3 and n - 1 - np.sum(history.opponent_actions[-1, :]) > 0:
                current_action = Action.C
            else:
                current_action = Action.D
        else:
            current_action = Action.D
        self.my_previous_action = current_action
        return current_action

class Strategy_AGGRESSIVE_95(BaseStrategy):
    """
    Aggressive 'Calculated Gamble' strategy for the Collective Risk Dilemma.

    This strategy defaults to defection, aiming to maximize individual payoff.
    It will only cooperate if:
    1. The number of observed cooperators in the previous round is very close
       to the minimum required threshold `m` (within a small buffer).
    2. There hasn't been a consistent pattern of widespread defection in recent history.
    3. Cooperating yields a higher expected payoff than defecting in the current round.

    It also has specific logic for the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.threshold_buffer = 1
        self.lookback_window = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == 1:
            return Action.D
        if current_round == self.r:
            return Action.D
        previous_round_idx = current_round - 2
        cooperators_in_prev_round_others = 0
        if previous_round_idx >= 0:
            cooperators_in_prev_round_others = np.sum(history.opponent_actions[previous_round_idx, :])
            if history.my_actions[previous_round_idx]:
                cooperators_in_prev_round_others += 1
        is_close_to_threshold = self.m - 1 - cooperators_in_prev_round_others <= self.threshold_buffer
        average_recent_cooperators = 0
        if current_round - 1 >= self.lookback_window:
            recent_cooperation_sum = 0
            for r_idx in range(current_round - 1 - self.lookback_window, current_round - 1):
                recent_cooperation_sum += np.sum(history.opponent_actions[r_idx, :])
                if history.my_actions[r_idx]:
                    recent_cooperation_sum += 1
            average_recent_cooperators = recent_cooperation_sum / self.lookback_window
        else:
            num_rounds_in_history = current_round - 1
            if num_rounds_in_history > 0:
                recent_cooperation_sum = 0
                for r_idx in range(num_rounds_in_history):
                    recent_cooperation_sum += np.sum(history.opponent_actions[r_idx, :])
                    if history.my_actions[r_idx]:
                        recent_cooperation_sum += 1
                average_recent_cooperators = recent_cooperation_sum / num_rounds_in_history
            else:
                average_recent_cooperators = self.m
        has_pattern_of_cooperation = average_recent_cooperators >= self.m - 1
        if is_close_to_threshold and has_pattern_of_cooperation:
            payoff_defect = 1.0
            expected_payoff_cooperate = self.k
            if expected_payoff_cooperate > payoff_defect:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_96(BaseStrategy):
    """
    Aggressive Leverage Strategy (ALS) for the Collective Risk Dilemma.
    This strategy defects by default and cooperates only if the previous round
    had at least m-1 cooperators, ensuring the threshold is met and the reward
    is secured with minimal individual risk. It avoids cooperating in the first
    and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players,
                              n_rounds, m, and k.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: Historical data of actions and payoffs for all players.
                     None for the first round.

        Returns:
            Action.C if the player decides to Cooperate, Action.D otherwise.
        """
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None:
            return Action.D
        previous_round_index = current_round_number - 1
        cooperators_last_round = np.sum(history.opponent_actions[previous_round_index, :])
        if history.my_actions[previous_round_index]:
            cooperators_last_round += 1
        if cooperators_last_round >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_97(BaseStrategy):
    """
    Aggressive Strategy: The "Calculated Opportunist"
    This strategy aims to maximize individual payoffs by exploiting the reward (k)
    when cooperation is likely to succeed, defecting when it's likely to fail,
    and retaliating against perceived exploitation. It's aggressive by proactively
    seeking high-value opportunities and decisively responding to unfair play.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_actions_history = []
        self.opponent_actions_history = []
        self.cooperation_success_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if history is None:
            self.my_actions_history.append(Action.C)
            return Action.C
        if not self.opponent_actions_history or len(self.opponent_actions_history) < current_round:
            self.opponent_actions_history.append(history.opponent_actions[current_round - 1, :].tolist())
            cooperators_last_round = sum(history.opponent_actions[current_round - 1, :]) + (1 if self.my_actions_history[-1] == Action.C else 0)
            self.cooperation_success_history.append(cooperators_last_round >= m)
        if current_round == r:
            self.my_actions_history.append(Action.D)
            return Action.D
        estimated_likely_cooperators = self._estimate_likely_cooperators(n, m, k, self.my_actions_history, self.opponent_actions_history, self.cooperation_success_history)
        if estimated_likely_cooperators >= m:
            self.my_actions_history.append(Action.C)
            return Action.C
        if estimated_likely_cooperators == m - 1:
            self.my_actions_history.append(Action.C)
            return Action.C
        if estimated_likely_cooperators < m - 1:
            self.my_actions_history.append(Action.D)
            return Action.D
        if not self.cooperation_success_history[-1] and self.my_actions_history[-1] == Action.C:
            num_others_defected_last_round = 0
            for action_val in self.opponent_actions_history[-1]:
                if action_val == Action.D.value:
                    num_others_defected_last_round += 1
            if num_others_defected_last_round > n - m:
                self.my_actions_history.append(Action.D)
                return Action.D
        self.my_actions_history.append(Action.D)
        return Action.D

    def _estimate_likely_cooperators(self, n, m, k, my_hist, others_hist, success_hist):
        """
        Heuristic function to estimate the number of players likely to cooperate.
        This is a simplified predictor based on recent history.
        """
        num_rounds_played = len(my_hist)
        if num_rounds_played == 0:
            return 1
        estimated_cooperators = 0
        last_round_success = success_hist[-1]
        last_round_my_action = my_hist[-1]
        last_round_opp_actions = others_hist[-1]
        num_opp_cooperated_last_round = sum((1 for action_val in last_round_opp_actions if action_val == Action.C.value))
        if last_round_success:
            estimated_cooperators = num_opp_cooperated_last_round + random.choice([0, 1])
        elif last_round_my_action == Action.C:
            num_others_defected_last_round = sum((1 for action_val in last_round_opp_actions if action_val == Action.D.value))
            if num_others_defected_last_round > n - m:
                estimated_cooperators = 0
            else:
                estimated_cooperators = random.choice([0, 1])
        else:
            estimated_cooperators = random.choice([0, 1])
        return min(n, estimated_cooperators)

class Strategy_AGGRESSIVE_98(BaseStrategy):
    """
    The Opportunistic Gambler strategy for the Collective Risk Dilemma.
    This strategy prioritizes personal gain by defecting by default,
    but cooperates if the probability of reaching the cooperation threshold
    is high enough, and its contribution is likely to be the marginal factor.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        if self.n > 1:
            self.p_coop_estimate = (self.m - 1) / (self.n - 1)
        else:
            self.p_coop_estimate = 0.5
        self.last_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 1 or current_round == self.r:
            return Action.D
        if self.k <= 1:
            return Action.D
        if history is not None:
            my_previous_action = 1 if history.my_actions[-1] else 0
            opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
            self.last_round_cooperators = opponent_cooperators_last_round + my_previous_action
            if self.last_round_cooperators >= self.m:
                self.p_coop_estimate = min(1.0, self.p_coop_estimate + 0.2 * (1.0 - self.p_coop_estimate))
            elif self.last_round_cooperators == 0:
                self.p_coop_estimate = max(0.01, self.p_coop_estimate - 0.1 * self.p_coop_estimate)
            else:
                self.p_coop_estimate = max(0.01, self.p_coop_estimate * 0.9)
        else:
            self.last_round_cooperators = 0
        self.p_coop_estimate = max(0.01, min(0.99, self.p_coop_estimate))
        expected_coop_value = self.p_coop_estimate * self.k
        guaranteed_defect_value = 1.0
        expected_reward_sufficient = expected_coop_value > guaranteed_defect_value
        needed_cooperators = self.m - self.last_round_cooperators
        if history is not None:
            my_prev_action_val = 1 if history.my_actions[-1] else 0
            opponent_prev_coop_count = np.sum(history.opponent_actions[-1, :])
            n_c_prev_excluding_me = opponent_prev_coop_count
        else:
            n_c_prev_excluding_me = 0
        potential_other_cooperators = self.n - 1
        marginal_contribution_critical = False
        if needed_cooperators > 0 and potential_other_cooperators > 0:
            numerator_marginal = self.m - 1 - n_c_prev_excluding_me
            denominator_marginal = self.n - 1 - n_c_prev_excluding_me
            if denominator_marginal > 0:
                marginal_threshold_p_coop = numerator_marginal / denominator_marginal
                if self.p_coop_estimate > marginal_threshold_p_coop:
                    marginal_contribution_critical = True
            elif numerator_marginal <= 0:
                pass
        if expected_reward_sufficient and marginal_contribution_critical:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_99(BaseStrategy):
    """
    Aggressive Strategy: "The Calculated Risk-Taker"

    This strategy aims to maximize individual gain by opportunistically
    defecting when the risk of cooperation is high or defection is clearly
    profitable, and cooperating when the reward of cooperation is highly
    probable and significantly exceeds the guaranteed payoff of defection.
    It adapts to the observed behavior of other players, especially reacting
    to "near-misses" in cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.smoothing_alpha = 0.3
        self.round_num = 0
        self.prev_round_coop_count = 0
        self.avg_coop_rate_others = 0.0

    def binomial_coefficient(self, n, k):
        if k < 0 or k > n:
            return 0
        if k == 0 or k == n:
            return 1
        if k > n // 2:
            k = n - k
        res = 1
        for i in range(k):
            res = res * (n - i) // (i + 1)
        return res

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_num = state.round_number
        if history is None:
            self.prev_round_coop_count = 0
            self.avg_coop_rate_others = 0.0
            return Action.C
        prev_round_actions = history.my_actions[self.round_num - 1]
        prev_round_opponents_actions = history.opponent_actions[self.round_num - 1]
        num_others_cooperated_prev_round = 0
        if self.n > 1:
            num_others_cooperated_prev_round = int(np.sum(prev_round_opponents_actions))
        coop_rate_others_prev = 0.0
        if self.n - 1 > 0:
            coop_rate_others_prev = num_others_cooperated_prev_round / (self.n - 1)
        elif self.n == 2:
            coop_rate_others_prev = float(num_others_cooperated_prev_round)
        self.avg_coop_rate_others = self.smoothing_alpha * coop_rate_others_prev + (1 - self.smoothing_alpha) * self.avg_coop_rate_others
        self.prev_round_coop_count = num_others_cooperated_prev_round + (1 if prev_round_actions else 0)
        if self.round_num == self.r:
            return Action.D
        grd_payoff = 1.0
        if self.prev_round_coop_count >= self.m:
            grd_payoff = 1.0 + self.k
        estimated_others_needed_if_I_coop = max(0, self.m - 1)
        p = self.avg_coop_rate_others
        prob_threshold_met_if_I_coop = 0.0
        if self.n - 1 >= estimated_others_needed_if_I_coop:
            for x in range(estimated_others_needed_if_I_coop, self.n):
                prob_x_cooperate = self.binomial_coefficient(self.n - 1, x) * p ** x * (1 - p) ** (self.n - 1 - x)
                prob_threshold_met_if_I_coop += prob_x_cooperate
        erc_payoff = prob_threshold_met_if_I_coop * self.k
        cooperate_decision = False
        if erc_payoff > grd_payoff:
            if self.prev_round_coop_count >= self.m - 1:
                cooperate_decision = True
        if cooperate_decision:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_100(BaseStrategy):
    """
    The "Leveraged Gambler" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by conditionally cooperating.
    It defects in the first round, then cooperates if the previous round saw
    at least m-1 cooperators, otherwise defects. It always defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.D
        if current_round == r - 1:
            return Action.D
        cooperators_last_round = np.sum(history.opponent_actions[-1]) + history.my_actions[-1]
        total_cooperators_previous_round = np.sum(history.my_actions[-1])
        if history.opponent_actions.shape[1] > 0:
            total_cooperators_previous_round += np.sum(history.opponent_actions[-1])
        if total_cooperators_previous_round >= m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_101(BaseStrategy):
    """
    "Calculated Gamble" Strategy for the Collective Risk Dilemma.

    This strategy aggressively seeks the group reward when the probability of success
    is high and there are sufficient future rounds to benefit from it. It prioritizes
    individual gain by defecting when cooperation is unlikely to succeed or when
    the game is nearing its end, thus avoiding potential losses. The strategy
    learns from past opponent behavior to estimate the likelihood of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.3
        self.opponent_coop_prob = np.full(self.n - 1, 0.5)
        self.opponent_coop_counts = np.zeros(self.n - 1)
        self.opponent_total_rounds = np.zeros(self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is not None:
            if history.round_number >= 1:
                self.opponent_total_rounds[:self.n - 1] += 1
                last_round_opponent_actions = history.opponent_actions[current_round - 1, :]
                for i in range(self.n - 1):
                    if last_round_opponent_actions[i]:
                        self.opponent_coop_counts[i] += 1
                for i in range(self.n - 1):
                    if self.opponent_total_rounds[i] > 0:
                        self.opponent_coop_prob[i] = self.opponent_coop_counts[i] / self.opponent_total_rounds[i]
        if history is None or current_round == 0:
            return Action.D
        n_remaining = self.r - current_round
        if n_remaining == 1:
            return Action.D
        observed_cooperators_last_round = 0
        if current_round >= 1:
            observed_cooperators_last_round = np.sum(history.opponent_actions[current_round - 1, :])
        expected_cooperators_from_others = np.sum(self.opponent_coop_prob)
        cooperation_trigger_for_others = self.alpha * (self.m - 1) / (self.n - 1)
        n_remaining = self.r - current_round
        if observed_cooperators_last_round >= self.m - 1:
            return Action.C
        if observed_cooperators_last_round >= self.m - 2 and n_remaining > self.r / 2:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_102(BaseStrategy):
    """
    The Opportunistic Enforcer strategy is an aggressive, adaptive strategy for
    the Collective Risk Dilemma. It prioritizes maximizing individual gain
    by cooperating when the probability of reaching the cooperation threshold
    is high and rewarding, and defecting to secure private gains otherwise.
    It is particularly decisive in the first and last rounds, and punishes
    perceived free-riding in intermediate rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.coop_buffer = 0.15
        self.previous_cooperation_count = 0

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        current_round_num = state.round_number
        if history is None:
            self.previous_cooperation_count = 0
            return Action.C
        if history.round_number > 0:
            my_last_action = Action.C if history.my_actions[-1] else Action.D
            coop_in_last_round = np.sum(history.opponent_actions[-1, :])
            if my_last_action == Action.C:
                coop_in_last_round += 1
            self.previous_cooperation_count = coop_in_last_round
        else:
            self.previous_cooperation_count = 0
        if current_round_num == self.r - 1:
            return Action.D
        observed_coop_proportion = self.previous_cooperation_count / self.n
        coop_threshold_ratio = self.m / self.n
        if observed_coop_proportion >= coop_threshold_ratio:
            return Action.C
        elif observed_coop_proportion >= coop_threshold_ratio - self.coop_buffer:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_103(BaseStrategy):
    """
    The Unyielding Opportunist strategy.
    This strategy aims to maximize individual payoff by leveraging the collective reward
    when possible, but prioritizing self-interest and exploiting opportunities aggressively
    when cooperation is risky or when others are cooperating.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        current_round = state.round_number
        if history is None:
            if m > n / 2:
                return Action.C
            else:
                return Action.D
        my_actions_prev = [Action.C if action else Action.D for action in history.my_actions]
        opponent_actions_prev = []
        for i in range(n - 1):
            opponent_actions_prev.append([Action.C if action else Action.D for action in history.opponent_actions[:, i]])
        total_cooperators_in_last_round = 0
        if history.my_actions.size > 0:
            total_cooperators_in_last_round += int(history.my_actions[-1])
            if history.opponent_actions.shape[0] > 0:
                total_cooperators_in_last_round += np.sum(history.opponent_actions[-1, :])
        if current_round == r - 1:
            return Action.D
        if total_cooperators_in_last_round > m - 1:
            return Action.D
        if current_round < math.floor(r / 3):
            if total_cooperators_in_last_round >= m - 1:
                return Action.C
        if total_cooperators_in_last_round == m - 1:
            return Action.C
        if total_cooperators_in_last_round < m - 1:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_104(BaseStrategy):
    """
    The "Calculated Dominance" strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by strategically cooperating
    to trigger the reward when beneficial, and defecting ruthlessly otherwise.
    It prioritizes self-interest, leveraging history and game parameters to
    dictate outcomes and deter exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.game_description = game_description

    def _count_cooperators_in_round(self, round_idx: int, history: PlayerHistory) -> int:
        """Helper to count total cooperators in a given round."""
        if history is None or round_idx >= history.round_number:
            return 0
        my_coop = int(history.my_actions[round_idx])
        opponent_coop = np.sum(history.opponent_actions[round_idx, :])
        return my_coop + opponent_coop

    def _get_recent_cooperation_trend(self, history: PlayerHistory, num_rounds: int=3) -> float:
        """Calculates the average cooperation rate over the last `num_rounds`."""
        if history is None or history.round_number < num_rounds:
            return 0.5
        total_cooperation_sum = 0
        for i in range(1, min(num_rounds, history.round_number) + 1):
            round_idx = history.round_number - i
            coop_count = self._count_cooperators_in_round(round_idx, history)
            total_cooperation_sum += coop_count / self.n
        return total_cooperation_sum / min(num_rounds, history.round_number)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            if self.m > self.n / 2 and self.k > 2.5:
                pass
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_idx = current_round - 1
        cooperators_prev_round = self._count_cooperators_in_round(prev_round_idx, history)
        my_action_prev_round = history.my_actions[prev_round_idx]
        recent_cooperation_trend = self._get_recent_cooperation_trend(history)
        cooperate_conditions_met = cooperators_prev_round >= self.m - 1 and self.k > 1.5 and (recent_cooperation_trend >= 0.5)
        if cooperate_conditions_met:
            if my_action_prev_round and cooperators_prev_round < self.m:
                if self.m - cooperators_prev_round == 1 and self.k > 1.8:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_105(BaseStrategy):
    """
    The "Calculated Gamble" Aggressive Strategy.

    This strategy is designed to be aggressive by prioritizing personal gain while
    leveraging the collective reward when the odds are in its favor. It is adaptive
    and learns from the history of the game, but its core driver is a calculated
    risk-taking approach. It cooperates only when there's a high probability of
    reaching the cooperation threshold, and defects otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.act = 0.75
        self.total_cooperators_observed = 0
        self.last_cooperation_round = -1
        self.my_actions_history = []
        self.opponent_actions_history = []

    def _update_state(self, history: PlayerHistory):
        """Updates internal state based on the provided history."""
        if not history:
            return
        current_round_idx = history.round_number - 1
        prev_round_idx = current_round_idx - 1
        n_c_prev = 0
        if prev_round_idx >= 0:
            n_c_prev = sum(self.my_actions_history[prev_round_idx]) + sum(self.opponent_actions_history[prev_round_idx])
            for action in self.my_actions_history[:current_round_idx]:
                if action:
                    self.total_cooperators_observed += 1
            for opp_actions_round in self.opponent_actions_history[:current_round_idx]:
                for action in opp_actions_round:
                    if action:
                        self.total_cooperators_observed += 1
            threshold_met_prev = n_c_prev >= self.m
            we_cooperated_prev = self.my_actions_history[prev_round_idx]
            if threshold_met_prev:
                if we_cooperated_prev:
                    self.act = max(0.5, self.act - 0.05)
                else:
                    self.act = min(0.95, self.act + 0.07)
            elif we_cooperated_prev:
                self.act = min(0.95, self.act + 0.1)
            if threshold_met_prev and we_cooperated_prev:
                self.last_cooperation_round = prev_round_idx + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.my_actions_history.append(Action.D.value)
            self.act = 0.75
            self.total_cooperators_observed = 0
            self.last_cooperation_round = -1
            return Action.D
        if len(self.my_actions_history) < current_round:
            self._update_state(history)
        if len(self.my_actions_history) == current_round:
            self.my_actions_history.append(self.my_actions_history[-1])
        if len(self.opponent_actions_history) < current_round:
            self.opponent_actions_history.append(history.opponent_actions[current_round - 1].tolist())
        if current_round == self.r:
            return Action.D
        estimated_potential_other_cooperators = 0
        if current_round > 1:
            if self.total_cooperators_observed > 0 or self.last_cooperation_round != -1:
                estimated_potential_other_cooperators = math.floor(0.6 * (self.n - 1))
            else:
                estimated_potential_other_cooperators = math.floor(0.2 * (self.n - 1))
        else:
            estimated_potential_other_cooperators = math.floor(0.2 * (self.n - 1))
        cooperate_is_viable = False
        potential_cooperators_needed_from_others = max(0, self.m - 1)
        can_reach_m_with_us = estimated_potential_other_cooperators >= potential_cooperators_needed_from_others
        act_threshold_for_cooperation = 0.7
        act_is_favorable = self.act < act_threshold_for_cooperation
        penultimate_round_risk = True
        if current_round == self.r - 1 and self.act >= 0.85:
            penultimate_round_risk = False
        if can_reach_m_with_us and act_is_favorable and penultimate_round_risk:
            cooperate_is_viable = True
        if cooperate_is_viable:
            chosen_action = Action.C
        else:
            chosen_action = Action.D
        self.my_actions_history.append(chosen_action.value)
        return chosen_action

class Strategy_AGGRESSIVE_106(BaseStrategy):
    """
    The "Calculated Gambler" strategy for the Collective Risk Dilemma.
    This strategy aggressively seeks to exploit opportunities for cooperation when
    the threshold `m` is likely to be met, or when its own cooperation is the
    marginal factor to achieve success. It defaults to defection to secure a
    private payoff when uncertainty is high.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.player_cooperation_counts = {}
        self.total_rounds_played = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            self.total_rounds_played = 1
            for i in range(n):
                self.player_cooperation_counts[i] = 0
            return Action.D
        if self.total_rounds_played > 0:
            prev_round_idx = self.total_rounds_played - 1
            if history.my_actions[prev_round_idx]:
                self.player_cooperation_counts[self.get_my_player_index(history)] += 1
            for opp_idx in range(n - 1):
                pass
        current_round_number = state.round_number
        prev_round_idx = current_round_number - 1
        cooperators_prev_round = 0
        if prev_round_idx >= 0:
            if history.my_actions[prev_round_idx]:
                cooperators_prev_round += 1
            cooperators_prev_round += np.sum(history.opponent_actions[prev_round_idx, :])
        condition_one = cooperators_prev_round == m - 1
        condition_two = False
        if cooperators_prev_round >= m:
            reliable_cooperators_count = 0
            my_player_idx = self.get_my_player_index(history)
            all_player_cooperation_counts = {idx: 0 for idx in range(n)}
            all_player_total_rounds = {idx: 0 for idx in range(n)}
            for r_idx in range(prev_round_idx + 1):
                if history.my_actions[r_idx]:
                    all_player_cooperation_counts[my_player_idx] += 1
                all_player_total_rounds[my_player_idx] += 1
                for opp_hist_idx, opp_player_idx in enumerate(self.get_opponent_player_indices(history)):
                    if history.opponent_actions[r_idx, opp_hist_idx]:
                        all_player_cooperation_counts[opp_player_idx] += 1
                    all_player_total_rounds[opp_player_idx] += 1
            historically_cooperative_players_needed = m - 1
            for p_idx in range(n):
                if p_idx == my_player_idx:
                    continue
                cooperation_rate = 0.0
                if all_player_total_rounds[p_idx] > 0:
                    cooperation_rate = all_player_cooperation_counts[p_idx] / all_player_total_rounds[p_idx]
                if cooperation_rate >= 0.7:
                    reliable_cooperators_count += 1
            if reliable_cooperators_count >= historically_cooperative_players_needed:
                condition_two = True
        if condition_one or condition_two:
            self.total_rounds_played += 1
            return Action.C
        else:
            self.total_rounds_played += 1
            return Action.D

    def get_my_player_index(self, history: PlayerHistory) -> int:
        """
        Attempts to infer the player's own index in the game.
        This is a heuristic and assumes a consistent order of players.
        If this is the first round and history is None, this function won't be called.
        In later rounds, if opponent_actions is non-empty, we can infer.
        """
        if not hasattr(self, '_my_player_index'):
            n = self.game_description.n_players
            self._my_player_index = 0
            if history and history.opponent_actions.shape[1] > 0:
                pass
        return self._my_player_index

    def get_opponent_player_indices(self, history: PlayerHistory) -> list[int]:
        """
        Returns a list of player indices for the opponents.
        This also relies on the assumption of player index consistency.
        """
        n = self.game_description.n_players
        my_idx = self.get_my_player_index(history)
        opponent_indices = []
        for i in range(n):
            if i != my_idx:
                opponent_indices.append(i)
        return opponent_indices

class Strategy_AGGRESSIVE_107(BaseStrategy):
    """
    The Opportunistic Sentinel strategy embodies an aggressive mindset by
    prioritizing personal gain while vigilantly monitoring the collective for
    opportunities to exploit or contribute strategically for a larger reward.
    It is designed to be adaptive, learning from past interactions and adjusting
    its behavior accordingly. The core principle is to punish defection decisively
    and capitalize on cooperation when it's beneficial and likely to succeed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.player_cooperation_history = None

    def _count_cooperators_in_round(self, history: PlayerHistory, round_idx: int) -> int:
        """Counts the total number of cooperators in a given round."""
        my_cooperation = 1 if history.my_actions[round_idx] else 0
        opponent_cooperation = np.sum(history.opponent_actions[round_idx, :])
        return my_cooperation + opponent_cooperation

    def _predict_cooperation_for_next_round(self, history: PlayerHistory) -> float:
        """
        Predicts the number of cooperators in the *next* round.
        This is the core adaptive element and a simplified heuristic.
        """
        if history.round_number == 0:
            return 0.0
        last_round_cooperators = self._count_cooperators_in_round(history, history.round_number - 1)
        predicted_coop = last_round_cooperators
        if last_round_cooperators < self.m:
            for i in range(self.n - 1):
                if not history.opponent_actions[history.round_number - 1, i] and last_round_cooperators < self.m:
                    predicted_coop -= 0.75
        if history.round_number >= 2:
            second_last_round_cooperators = self._count_cooperators_in_round(history, history.round_number - 2)
            if last_round_cooperators > second_last_round_cooperators and last_round_cooperators < self.m:
                predicted_coop += 0.3
        return max(0.0, min(float(self.n), predicted_coop))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            last_round_cooperators = self._count_cooperators_in_round(history, current_round - 1)
            if last_round_cooperators >= self.m:
                return Action.C
            elif last_round_cooperators + 1 >= self.m:
                return Action.C
            else:
                return Action.D
        previous_round_cooperators = self._count_cooperators_in_round(history, current_round - 1)
        predicted_cooperators_for_this_round = self._predict_cooperation_for_next_round(history)
        if predicted_cooperators_for_this_round >= self.m:
            return Action.D
        if predicted_cooperators_for_this_round + 1 >= self.m:
            confidence_threshold = 0.5
            if self.k > 2.5:
                confidence_threshold = 0.4
            if predicted_cooperators_for_this_round > self.m - 2:
                confidence_threshold = 0.3
            if predicted_cooperators_for_this_round + 1 >= self.m:
                if predicted_cooperators_for_this_round >= self.m - 2 or self.k > 1.5:
                    return Action.C
        return Action.D

class Strategy_AGGRESSIVE_108(BaseStrategy):
    """
    Calculated Opportunist strategy for the Collective Risk Dilemma.
    This strategy is aggressive, prioritizing self-interest. It defects by default,
    but will cooperate strategically to exploit proximity to the cooperation threshold
    or to maintain the reward mechanism when it's beneficial.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        my_last_action_bool = history.my_actions[-1]
        my_last_action = Action.C if my_last_action_bool else Action.D
        cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + (1 if my_last_action_bool else 0)
        if cooperators_last_round >= self.m and my_last_action == Action.C:
            return Action.C
        if cooperators_last_round >= self.m - 1 and cooperators_last_round < self.m:
            return Action.C
        if cooperators_last_round < self.m - 1:
            return Action.D
        if cooperators_last_round >= self.m and my_last_action == Action.D:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_109(BaseStrategy):
    """
    The Leveraged Risk-Taker: An aggressive strategy for the Collective Risk Dilemma.

    This strategy aims to maximize individual payoff by strategically cooperating.
    It leverages the cooperative reward by only cooperating when there's a high
    probability of meeting the cooperation threshold 'm'. It punishes defection
    by raising its cooperation threshold and rewards successful collective action
    by lowering its threshold, becoming more likely to join. It also adapts its
    behavior towards the end of the game, making a calculated gamble in the
    final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_cooperation_threshold = max(1, self.n - self.m)
        self.previous_round_my_action = None
        self.previous_round_opponent_actions = None
        self.previous_round_cooperators_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.previous_round_my_action = None
            self.previous_round_opponent_actions = None
            self.previous_round_cooperators_count = 0
            return Action.D
        else:
            self.previous_round_my_action = history.my_actions[current_round - 1]
            self.previous_round_opponent_actions = history.opponent_actions[current_round - 1]
            cooperators_this_round_bool = np.concatenate([[self.previous_round_my_action], self.previous_round_opponent_actions])
            self.previous_round_cooperators_count = np.sum(cooperators_this_round_bool)
        observed_total_cooperation_rate_prev_round = self.previous_round_cooperators_count / self.n if self.n > 0 else 0
        if current_round == self.r - 1:
            if self.previous_round_cooperators_count >= self.m - 1:
                return Action.C
            else:
                return Action.D
        else:
            other_cooperators_prev_round = self.previous_round_cooperators_count - (1 if self.previous_round_my_action else 0)
            if other_cooperators_prev_round >= self.my_cooperation_threshold:
                if current_round / self.r < 0.8:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        m_was_met_prev_round = self.previous_round_cooperators_count >= self.m
        if m_was_met_prev_round:
            self.my_cooperation_threshold = max(1, self.my_cooperation_threshold - 1)
        else:
            self.my_cooperation_threshold += 1
        return Action.D

class Strategy_AGGRESSIVE_110(BaseStrategy):
    """
    Aggressive "Opportunistic Exploiter" Strategy for the Collective Risk Dilemma.

    This strategy prioritizes defecting, especially when cooperation is likely to succeed
    due to others' efforts. It aims to maximize individual gain by securing the
    highest possible payoff (1 + k) by defecting when the cooperation threshold is met.
    Cooperation is only considered if it's a near-certain path to guaranteed personal
    gain, which is rarely the case in a simultaneous-move game without explicit signaling.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description
        self.cooperation_success_history = []
        self.total_rounds = game_description.n_rounds
        self.min_cooperators = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on historical data.

        Args:
            state: The current game state, including the round number.
            history: Historical data from previous rounds. None for the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        current_round_number = state.round_number
        if history is None:
            return Action.D
        if current_round_number == self.total_rounds:
            return Action.D
        if not self.cooperation_success_history and current_round_number > 0:
            for r in range(current_round_number):
                cooperators_in_round = sum(history.opponent_actions[r, :])
                if history.my_actions[r]:
                    cooperators_in_round += 1
                self.cooperation_success_history.append(cooperators_in_round >= self.min_cooperators)
        successful_rounds = sum(self.cooperation_success_history)
        total_past_rounds = len(self.cooperation_success_history)
        csr = 0.0
        if total_past_rounds > 0:
            csr = successful_rounds / total_past_rounds
        if csr > 0.7:
            return Action.D
        elif csr > 0.3:
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_111(BaseStrategy):
    """
    The Opportunistic Dominator strategy aims to maximize individual gain by
    defaulting to defection. It only cooperates when the reward is guaranteed
    and substantial, or as a calculated move in the final round to secure
    maximum payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            return Action.D
        current_round_number = state.round_number
        previous_round_my_action = 1 if self.game_description.n_rounds > 1 and current_round_number > 0 else 0
        if history.my_actions is not None and len(history.my_actions) > 0:
            previous_round_my_action = history.my_actions[current_round_number - 1]
        previous_round_opponents_actions = np.zeros(n - 1, dtype=bool)
        if history.opponent_actions is not None and len(history.opponent_actions) > 0 and (history.opponent_actions.shape[0] > 0):
            previous_round_opponents_actions = history.opponent_actions[current_round_number - 1, :]
        cooperators_last_round = np.sum(previous_round_opponents_actions) + previous_round_my_action
        if current_round_number == r - 1:
            if cooperators_last_round >= m - 1:
                return Action.C
            else:
                return Action.D
        elif cooperators_last_round >= m - 1 and cooperators_last_round + 1 >= m:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_112(BaseStrategy):
    """
    The Opportunistic Gambler: Aggressively seeks the cooperation bonus
    when the probability of success is high or when the reward 'k' is
    exceptionally high, but retreats to defect and secure individual
    payoff when cooperation is unlikely. It employs a dynamic assessment
    of Cooperation Success Probability (CSP) and adapts its thresholds and
    aggressiveness based on the game parameters. It always defects in the
    final round to maximize immediate personal gain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.initial_csp_guess = self.game_description.m / self.game_description.n_players
        self.recent_rounds_weight = 0.7
        self.cooperate_threshold_buffer = 0.1
        self.defect_threshold_buffer = 0.1
        self.high_k_threshold = 2.0
        self.high_k_aggression_boost = 1.2
        self.high_k_prob_boost = 1.2
        self.last_round_cooperation_success = False
        self.my_last_action = Action.D

    def _update_csp(self, history: PlayerHistory) -> float:
        """
        Estimates the Cooperation Success Probability (CSP) based on historical data.
        A simple empirical probability blended with a lean towards recent trends
        and adjusted by my own last action.
        """
        num_completed_rounds = history.round_number
        if num_completed_rounds == 0:
            return self.initial_csp_guess
        successful_cooperation_rounds = 0
        for r in range(num_completed_rounds):
            cooperators_in_round = 1 + np.sum(history.opponent_actions[r, :])
            if cooperators_in_round >= self.game_description.m:
                successful_cooperation_rounds += 1
        empirical_probability = successful_cooperation_rounds / num_completed_rounds
        historical_weight = 1.0 - self.recent_rounds_weight
        weighted_csp = empirical_probability * historical_weight + self.last_round_cooperation_success * self.recent_rounds_weight
        if self.my_last_action == Action.C and self.last_round_cooperation_success:
            weighted_csp = min(1.0, weighted_csp + 0.05)
        elif self.my_last_action == Action.D and (not self.last_round_cooperation_success):
            weighted_csp = max(0.0, weighted_csp - 0.05)
        return weighted_csp

    def _get_thresholds_and_aggression_multiplier(self):
        """
        Calculates the cooperation and defection thresholds and the aggression multiplier,
        adjusting them based on the 'k' parameter.
        """
        base_cooperate_threshold = self.game_description.m / self.game_description.n_players
        base_defect_threshold = self.game_description.m / self.game_description.n_players - self.defect_threshold_buffer
        aggression_multiplier = 1.0
        current_threshold_cooperate = base_cooperate_threshold + self.cooperate_threshold_buffer
        current_threshold_defect = max(0.0, base_defect_threshold)
        if self.game_description.k > self.high_k_threshold:
            current_threshold_cooperate = min(1.0, base_cooperate_threshold * self.high_k_aggression_boost)
            current_threshold_defect = max(0.0, base_defect_threshold * self.high_k_aggression_boost)
            aggression_multiplier = self.high_k_prob_boost
        return (current_threshold_cooperate, current_threshold_defect, aggression_multiplier)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if history is None:
            csp = self.initial_csp_guess
            threshold_cooperate, threshold_defect, aggression_multiplier = self._get_thresholds_and_aggression_multiplier()
            if csp < threshold_defect:
                self.my_last_action = Action.D
                return Action.D
            elif csp > threshold_cooperate:
                self.my_last_action = Action.C
                return Action.C
            else:
                probability_of_cooperating = (csp - threshold_defect) / (threshold_cooperate - threshold_defect)
                adjusted_probability_of_cooperating = min(1.0, probability_of_cooperating * aggression_multiplier)
                if random.random() < adjusted_probability_of_cooperating:
                    self.my_last_action = Action.C
                    return Action.C
                else:
                    self.my_last_action = Action.D
                    return Action.D
        num_completed_rounds = history.round_number
        if num_completed_rounds > 0:
            my_last_action_bool = history.my_actions[-1]
            cooperators_last_round = 1 + np.sum(history.opponent_actions[-1, :])
            self.last_round_cooperation_success = cooperators_last_round >= self.game_description.m
            self.my_last_action = Action.C if my_last_action_bool else Action.D
        csp = self._update_csp(history)
        threshold_cooperate, threshold_defect, aggression_multiplier = self._get_thresholds_and_aggression_multiplier()
        if csp < threshold_defect:
            self.my_last_action = Action.D
            return Action.D
        elif csp > threshold_cooperate:
            self.my_last_action = Action.C
            return Action.C
        else:
            probability_of_cooperating = (csp - threshold_defect) / (threshold_cooperate - threshold_defect)
            adjusted_probability_of_cooperating = min(1.0, probability_of_cooperating * aggression_multiplier)
            if random.random() < adjusted_probability_of_cooperating:
                self.my_last_action = Action.C
                return Action.C
            else:
                self.my_last_action = Action.D
                return Action.D

class Strategy_AGGRESSIVE_113(BaseStrategy):
    """
    The Opportunist strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual gain by exploiting cooperation
    from others while minimizing personal risk. It defects by default,
    cooperating only in specific, calculated scenarios to ensure future
    benefit or to reinforce fragile but potentially profitable cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n_players = self.n
        m = self.m
        num_cooperators_prev = (1 if history.my_actions[-1] == Action.C else 0) + np.sum(history.opponent_actions[-1, :] == Action.C)
        my_action_prev = Action(history.my_actions[-1])
        if num_cooperators_prev < m:
            return Action.D
        else:
            reinforcement_threshold = m + math.ceil((n_players - m) / 2.0)
            if (num_cooperators_prev == m or num_cooperators_prev < reinforcement_threshold) and my_action_prev == Action.C:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_114(BaseStrategy):
    """
    Aggressive strategy that defects by default and cooperates only when
    the collective success is highly probable and lucrative. It aims to
    be a "smart defector" or "opportunistic exploiter."

    The strategy dynamically adjusts a cooperation probability threshold based
    on historical cooperation rates. It cooperates if the estimated probability
    of reaching the cooperation threshold 'm' (based on others' past behavior)
    exceeds the current threshold, and the reward 'k' is sufficiently high.
    It defects otherwise, prioritizing individual guaranteed payoff.

    Key Features:
    - Defaults to defection.
    - Cooperates only when payoff is almost guaranteed and substantial.
    - Learns from historical cooperation rates of others.
    - Explicitly defects in the first and last rounds.
    - Adaptively lowers threshold when past cooperation was successful or free-riding occurred.
    - Increases threshold when cooperation failed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.cooperation_probability_threshold = self.m / self.n
        self.history_log = []
        self.last_round_cooperated = False
        self.last_round_threshold_met = False
        self.last_round_my_action_bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None:
            self.last_round_cooperated = False
            self.last_round_threshold_met = False
            self.last_round_my_action_bool = False
            return Action.D
        if current_round_number > 1:
            self.last_round_my_action_bool = history.my_actions[current_round_number - 1]
            self.last_round_cooperated = self.last_round_my_action_bool
            num_cooperators_last_round = sum(history.opponent_actions[current_round_number - 1, :]) + (1 if self.last_round_my_action_bool else 0)
            self.last_round_threshold_met = num_cooperators_last_round >= self.m
        num_rounds_played = current_round_number
        total_cooperators_in_history_others = 0
        if num_rounds_played > 0:
            total_cooperators_in_history_others = np.sum(history.opponent_actions)
        num_other_players_total_actions = (self.n - 1) * num_rounds_played
        historical_cooperation_rate_others = total_cooperators_in_history_others / num_other_players_total_actions if num_other_players_total_actions > 0 else 0.0
        base_threshold_ratio = self.m / self.n
        adjustment_factor_history = 0.5
        adjustment_factor_last_round_success = 0.3
        adjustment_factor_last_round_failure = 0.1
        cooperation_history_impact = historical_cooperation_rate_others - base_threshold_ratio
        adjusted_threshold = base_threshold_ratio - cooperation_history_impact * adjustment_factor_history
        if self.last_round_threshold_met:
            adjusted_threshold -= adjustment_factor_last_round_success
        else:
            adjusted_threshold += adjustment_factor_last_round_failure
        self.cooperation_probability_threshold = max(0.1, min(0.9, adjusted_threshold))
        if historical_cooperation_rate_others >= self.cooperation_probability_threshold and self.k >= 2.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_115(BaseStrategy):
    """
    Aggressive strategy: "Calculated Opportunist".
    This strategy defaults to defecting to secure private endowment.
    It cooperates only when there is strong evidence from the previous round
    that enough other players will cooperate to meet the threshold `m`.
    It's "aggressive" because it's willing to be the catalyst for cooperation
    when the opportunity is present, taking a calculated risk for the higher reward `k`.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if history is None or current_round_number == 0:
            return Action.D
        last_round_idx = current_round_number - 1
        opponent_actions_last_round = history.opponent_actions[last_round_idx, :]
        observed_cooperators_last_round = np.sum(opponent_actions_last_round)
        if observed_cooperators_last_round >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_116(BaseStrategy):
    """
    Aggressive Leverage Strategy for Collective Risk Dilemma.

    This strategy aims to maximize individual gain by cooperating only when
    there's a high probability of success, acting as the "linchpin" for
    collective reward, and defecting otherwise to secure private endowment.
    It defaults to defection in the first and last rounds, and when uncertainty
    is high.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        cooperators_last_round = np.sum(history.opponent_actions[current_round - 1, :])
        my_action_last_round = history.my_actions[current_round - 1]
        predicted_future_cooperators = 0
        for i in range(self.n - 1):
            opponent_coop_count = np.sum(history.opponent_actions[:, i])
            if opponent_coop_count > current_round * 0.5:
                predicted_future_cooperators += 1
        if cooperators_last_round == self.m - 1 and predicted_future_cooperators >= self.m - 1:
            return Action.C
        if cooperators_last_round >= self.m - 1:
            if my_action_last_round and cooperators_last_round >= self.m:
                if predicted_future_cooperators >= self.m - 1:
                    return Action.C
            elif cooperators_last_round == self.m - 1:
                if predicted_future_cooperators >= self.m - 1:
                    return Action.C
        return Action.D

class Strategy_AGGRESSIVE_117(BaseStrategy):
    """
    The Calculated Risk-Taker strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual payoff by aggressively
    seeking to meet the cooperation threshold when beneficial, and defecting
    when cooperation is unlikely or to exploit successful cooperation in the
    final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_index = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.my_index == -1:
            pass
        current_round = state.round_number
        if history is None:
            return Action.C
        prev_round_num = current_round - 1
        prev_my_action_val = history.my_actions[prev_round_num]
        prev_my_action = Action.C if prev_my_action_val else Action.D
        all_prev_actions_bool = history.opponent_actions[prev_round_num, :]
        all_prev_actions_full = np.append(all_prev_actions_bool, prev_my_action_val)
        prev_cooperators_count = np.sum(all_prev_actions_full)
        if current_round == self.r:
            if prev_cooperators_count >= self.m:
                return Action.D
            else:
                return Action.D
        if prev_cooperators_count < self.m:
            cooperation_deficit = self.m - prev_cooperators_count
            if cooperation_deficit <= 1:
                if prev_my_action == Action.D:
                    return Action.C
                else:
                    return Action.C
            else:
                return Action.D
        else:
            exploiters_in_prev_round = self.n - prev_cooperators_count
            if exploiters_in_prev_round >= 1:
                return Action.C
            else:
                return Action.C

class Strategy_AGGRESSIVE_118(BaseStrategy):
    """
    The Calculated Risk-Taker strategy is designed for opportunistic exploitation
    and controlled risk-taking in the Collective Risk Dilemma. It prioritizes
    defecting to secure individual payoffs unless cooperation is guaranteed to
    yield a reward or is the last possible moment to secure the reward profitably.

    Core Logic:
    - Default to Defect ('D') in most situations.
    - Cooperate ('C') only when:
        1. The reward is guaranteed (m or more cooperators in the previous round).
        2. It's the last opportunity to contribute and secure the reward (m-1
           cooperators in the previous round, and defected last round).
    - Special handling for the first and last rounds (always defect).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        previous_round_cooperators = int(np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1])
        my_previous_action = Action.C if history.my_actions[-1] else Action.D
        if previous_round_cooperators >= self.m:
            return Action.C
        if previous_round_cooperators == self.m - 1 and my_previous_action == Action.D:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_119(BaseStrategy):
    """
    Calculated Aggression Strategy:
    This strategy aims to maximize individual payoff by being a conditional
    cooperator. It defaults to defection, only cooperating when there's a
    strong indication that cooperation will be successful and rewarded,
    thereby leveraging collective success without taking on excessive initial risk.
    It never cooperates in the first or last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.3
        self.momentum = 0.0
        self.cooperator_count_history = []
        self.my_actions_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.my_actions_history.append(Action.D)
            return Action.D
        if current_round == self.r - 1:
            self.my_actions_history.append(Action.D)
            return Action.D
        num_completed_rounds = history.round_number
        if num_completed_rounds > 0:
            opponent_cooperators_last_round = np.sum(history.opponent_actions[num_completed_rounds - 1, :])
            my_cooperator_last_round = 1 if self.my_actions_history[num_completed_rounds - 1] == Action.C else 0
            last_round_cooperators = opponent_cooperators_last_round + my_cooperator_last_round
            self.cooperator_count_history.append(last_round_cooperators)
        else:
            last_round_cooperators = 0
            self.cooperator_count_history.append(last_round_cooperators)
        if len(self.cooperator_count_history) > 1:
            previous_momentum = (1 - self.alpha) * self.momentum + self.alpha * self.cooperator_count_history[-2]
            self.momentum = previous_momentum
        else:
            self.momentum = self.alpha * last_round_cooperators
        N_C_prev = last_round_cooperators
        if N_C_prev >= self.m and self.momentum > self.m:
            self.my_actions_history.append(Action.C)
            return Action.C
        if N_C_prev == self.m - 1 and self.momentum > self.m * 1.2:
            self.my_actions_history.append(Action.C)
            return Action.C
        self.my_actions_history.append(Action.D)
        return Action.D

class Strategy_AGGRESSIVE_120(BaseStrategy):
    """
    The "Opportunistic Catalyst" strategy for the Collective Risk Dilemma.
    This strategy prioritizes self-interest, defects in the first and last rounds,
    and cooperates in intermediate rounds only when a sufficiently high proportion
    of other players have cooperated in the previous round. It dynamically adjusts
    its cooperation threshold based on the success of the collective in meeting
    the cooperation threshold in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.base_act_multiplier = 1.5
        self.act_adjustment_factor = 0.1
        self.current_act = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None or history.round_number == 0:
            ctr = self.m / self.n
            self.current_act = min(1.0, ctr * self.base_act_multiplier)
            return Action.D
        my_last_action_bool = history.my_actions[-1]
        previous_round_actions_bool = history.opponent_actions[-1]
        num_cooperators_prev_round = np.sum(previous_round_actions_bool)
        num_cooperators_excluding_me = num_cooperators_prev_round
        if my_last_action_bool:
            num_cooperators_excluding_me -= 1
        num_other_players = self.n - 1
        if num_other_players == 0:
            ocr_t_minus_1 = 0.0
        else:
            ocr_t_minus_1 = num_cooperators_excluding_me / num_other_players
        ctr = self.m / self.n
        if num_cooperators_prev_round >= self.m:
            self.current_act = max(0.0, self.current_act - self.act_adjustment_factor)
        else:
            self.current_act = min(1.0, self.current_act + self.act_adjustment_factor)
        self.current_act = max(0.0, min(1.0, self.current_act))
        if ocr_t_minus_1 >= self.current_act:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_121(BaseStrategy):
    """
    The Opportunist strategy aims to maximize individual gain by assessing the likelihood
    of the cooperation threshold being met. It defects by default unless there's a
    strong, calculated indication that cooperation will be beneficial and rewarding.

    Core Philosophy: Aggression means prioritizing the highest possible individual payoff
    in each round. It defects if it guarantees a higher payoff or if the risk of
    failure for cooperation is too high. It cooperates strategically when it perceives
    a high probability of achieving the collective reward.

    Decision Logic:
    - Cooperate (C) if the Cooperation Threshold Index (CTI) is above the Aggression Threshold.
    - Defect (D) otherwise.

    CTI Calculation:
    - Based on average cooperation in a recent observation window.
    - Adjusts dynamically based on proximity to the end of the game.

    Edge Cases:
    - First Round: Defect (no data to assess risk).
    - Last Round: Defect (no future rounds to incentivize cooperation).
    - Near End: Cooperate only if CTI is very high.

    Aggression Threshold:
    - Set relatively high ((n - m + 1) / n) to favor defection unless conditions are very favorable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Opportunist strategy.

        Args:
            game_description: An instance of CollectiveRiskDescription containing game parameters.
        """
        self.game_description = game_description
        self.observation_window = min(game_description.n_rounds - 1, 5)
        self.aggression_threshold = (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players
        self.all_actions_history = []

    def _count_cooperators_in_round(self, round_idx: int, actions_in_round: NDArray[np.bool_]) -> int:
        """Counts the number of cooperators in a given round's actions."""
        return np.sum(actions_in_round)

    def _calculate_cti(self, history: PlayerHistory) -> float:
        """
        Calculates the Cooperation Threshold Index (CTI) based on historical data.

        The CTI is an estimation of the likelihood that the minimum number of
        cooperators ('m') will be reached in the current round.

        Args:
            history: The PlayerHistory object containing past game states.

        Returns:
            The calculated CTI as a float.
        """
        current_round_num = history.round_number
        recent_cooperators_counts = []
        actual_observation_window = min(self.observation_window, current_round_num)
        if actual_observation_window == 0:
            return 0.0
        for r in range(current_round_num - actual_observation_window, current_round_num):
            my_coops = 1 if history.my_actions[r] else 0
            opponent_coops = np.sum(history.opponent_actions[r, :])
            recent_cooperators_counts.append(my_coops + opponent_coops)
        avg_cooperators = np.mean(recent_cooperators_counts)
        cti_base = (avg_cooperators + (1 if avg_cooperators >= self.game_description.m else 0)) / self.game_description.n_players
        proximity_factor = 1.0
        if current_round_num > self.game_description.n_rounds - self.observation_window:
            proximity_factor = (current_round_num - (self.game_description.n_rounds - self.observation_window)) / actual_observation_window
            cti = cti_base * proximity_factor * 1.2
        else:
            cti = cti_base * (current_round_num / self.game_description.n_rounds)
        cti = max(0.0, min(1.0, cti))
        return cti

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current GameState, including the round number.
            history: PlayerHistory object containing past game actions and payoffs.
                     None for the first round.

        Returns:
            The chosen Action (Action.C or Action.D).
        """
        if history is None:
            return Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        cti = self._calculate_cti(history)
        if cti > self.aggression_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_122(BaseStrategy):
    """
    "The Ruthless Opportunist" strategy for the Collective Risk Dilemma.
    This strategy aggressively seeks the higher payoff (when m or more cooperate)
    while ensuring minimal loss if the cooperation threshold isn't met. It prioritizes
    calculated risk-taking and punishes defections, aiming to exploit situations
    where cooperation is met with betrayal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        prev_round_index = current_round - 1
        my_prev_action = Action.C if history.my_actions[prev_round_index] else Action.D
        num_cooperators_prev = np.sum(history.opponent_actions[prev_round_index, :]) + (1 if my_prev_action == Action.C else 0)
        if num_cooperators_prev >= self.m and my_prev_action == Action.C:
            return Action.C
        if (num_cooperators_prev == self.m - 1 or num_cooperators_prev == self.m - 2) and my_prev_action == Action.C:
            return Action.C
        recent_rounds_start_index = max(0, current_round - 3)
        recent_rounds_indices = list(range(recent_rounds_start_index, current_round))
        if recent_rounds_indices:
            recent_my_actions = history.my_actions[recent_rounds_indices]
            recent_opponent_actions = history.opponent_actions[recent_rounds_indices, :]
            cooperation_count_recent = 0
            my_cooperation_count_recent = 0
            total_players_cooperated_in_recent = 0
            for round_idx in recent_rounds_indices:
                cooperators_in_this_round = np.sum(history.opponent_actions[round_idx, :]) + (1 if history.my_actions[round_idx] else 0)
                total_players_cooperated_in_recent += cooperators_in_this_round
                if history.my_actions[round_idx]:
                    my_cooperation_count_recent += 1
            avg_cooperators_recent = total_players_cooperated_in_recent / len(recent_rounds_indices)
            if avg_cooperators_recent >= self.m - 1 and my_cooperation_count_recent >= math.ceil(len(recent_rounds_indices) / 2):
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_123(BaseStrategy):
    """
    Aggressive "Dominance Threshold" Strategy for the Collective Risk Dilemma.

    This strategy aims to aggressively leverage the reward `k` by pushing for
    cooperation when it's strategically advantageous, while also protecting
    itself from exploitation. It cooperates to trigger the reward when it's
    the deciding factor or when a critical mass is assured, and defects to
    secure individual payoff when the risk of falling below the cooperation
    threshold is high or to punish free-riders.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.out_of_reach_buffer = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round = state.round_number
        prev_round_idx = current_round - 1
        cooperators_in_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        if history.my_actions[prev_round_idx]:
            cooperators_in_prev_round += 1
        my_prev_payoff = history.my_payoffs[prev_round_idx]
        if cooperators_in_prev_round + 1 == self.m and my_prev_payoff < 1 + self.k:
            return Action.C
        if cooperators_in_prev_round >= self.m - 1 and my_prev_payoff < 1 + self.k:
            return Action.C
        if cooperators_in_prev_round < max(1, self.m - self.out_of_reach_buffer):
            return Action.D
        if cooperators_in_prev_round >= self.m and my_prev_payoff == 1:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_124(BaseStrategy):
    """
    The Opportunistic Dominator strategy is an aggressive approach to the Collective Risk Dilemma.
    It prioritizes personal gain by opportunistically exploiting successful collective outcomes
    and minimizing losses when cooperation fails. It adapts its cooperation threshold dynamically
    and defects in the final round to maximize immediate payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m
        self.previous_round_was_success = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number == r:
            return Action.D
        if history is None:
            self.cooperation_threshold = m
            self.previous_round_was_success = False
            return Action.C
        observed_cooperators_previous_round = 0
        if history.opponent_actions.shape[0] > 0:
            observed_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        my_previous_action_was_c = history.my_actions[-1] if len(history.my_actions) > 0 else False
        total_cooperators_in_prev_round = observed_cooperators_previous_round + (1 if my_previous_action_was_c else 0)
        if self.previous_round_was_success:
            self.cooperation_threshold += 1
        else:
            self.cooperation_threshold = max(1, self.cooperation_threshold - 1)
        self.previous_round_was_success = total_cooperators_in_prev_round >= m
        if observed_cooperators_previous_round + 1 >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_125(BaseStrategy):
    """
    The "Threshold Predator" strategy is an aggressive approach to the Collective Risk Dilemma.
    It prioritizes self-preservation by defecting in the initial and final rounds,
    and opportunistically cooperates only when the reward threshold is imminent due to
    the actions of other players. This strategy aims to maximize individual payoff
    by leveraging others' cooperation without taking early risks.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the Threshold Predator strategy.

        Args:
            game_description: An object containing the game's parameters (n, r, m, k).
        """
        self.game_description = game_description
        self.current_round_num = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: An object containing the history of actions and payoffs for all players.
                     If None, it's the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m_threshold = self.game_description.m
        if history is None:
            self.current_round_num = 1
            return Action.D
        self.current_round_num = history.round_number + 1
        if self.current_round_num == n_rounds:
            return Action.D
        previous_round_actions = history.opponent_actions[history.round_number - 1, :]
        cooperators_last_round = np.sum(previous_round_actions)
        if cooperators_last_round >= m_threshold - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_126(BaseStrategy):
    """
    The Threshold Predator strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual gain by strategically exploiting
    the collective risk mechanism. It prioritizes defecting to secure immediate
    private gain unless a critical mass of cooperation is guaranteed, at which
    point it will "predate" on the collective reward. The core idea is to be a
    free-rider as much as possible, but to swiftly join the cooperative effort
    when it's a near certainty of success, thereby capturing the amplified reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.coop_momentum = 0.0
        self.action_mapping = {0: Action.D, 1: Action.C}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            if (self.n_players - 1) * 0.5 >= self.m - 1:
                return Action.C
            else:
                return Action.D
        if current_round > 0:
            previous_round_idx = current_round - 1
            observed_opponent_actions_prev = history.opponent_actions[previous_round_idx, :]
            observed_cooperators_prev = np.sum(observed_opponent_actions_prev)
            observed_defectors_prev = self.n_players - 1 - observed_cooperators_prev
            if observed_cooperators_prev > observed_defectors_prev:
                self.coop_momentum = min(1.0, self.coop_momentum + 0.1)
            elif observed_defectors_prev > observed_cooperators_prev:
                self.coop_momentum = max(-1.0, self.coop_momentum - 0.1)
        estimated_others_coop_probability = 0.5 + self.coop_momentum * 0.5
        expected_cooperators_from_others = (self.n_players - 1) * estimated_others_coop_probability
        if expected_cooperators_from_others + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_127(BaseStrategy):
    """
    The Opportunistic Dominator: An aggressive strategy that defaults to defection,
    only cooperating when there's strong evidence (m-1 other cooperators in the
    previous round) that the threshold will be met. It prioritizes personal gain
    and exploits any emergent cooperation from other players.

    Rationale:
    - Maximize individual payoff.
    - Default to defection to secure baseline payoff (1).
    - Cooperate only when success is highly probable (m-1 other cooperators in the previous round).
    - No cooperation in the first or last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        if current_round > 0:
            previous_round_idx = current_round - 1
            num_other_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
            if num_other_cooperators_prev_round >= self.m - 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_128(BaseStrategy):
    """
    Calculated Opportunism strategy for the Collective Risk Dilemma.

    This strategy defaults to defecting, reflecting an aggressive stance. It
    will only cooperate if specific conditions are met, indicating a high
    probability of securing the collective reward (k) while still having
    a chance for private gain or if it's the decisive factor to meet the
    threshold. It avoids cooperation in the first and last rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def _count_cooperators(self, actions: NDArray[np.bool_]) -> int:
        """Counts the number of True (Cooperate) values in a boolean array."""
        return np.sum(actions)

    def _calculate_confidence_in_others(self, history: PlayerHistory) -> float:
        """
        Calculates confidence in others cooperating to reach the threshold.

        Confidence is based on the number of cooperators in the previous round
        and the trend of cooperation.
        """
        if history.round_number == 0:
            return 0.0
        prev_round_actions = history.my_actions[history.round_number - 1]
        c_prev = self._count_cooperators(prev_round_actions)
        d_prev = self.n - c_prev
        strong_cooperation_signal = c_prev >= self.m - 1
        cooperation_trend = 0
        if history.round_number > 1:
            prev_prev_round_actions = history.my_actions[history.round_number - 2]
            c_prev_prev = self._count_cooperators(prev_prev_round_actions)
            if c_prev > c_prev_prev:
                cooperation_trend = 1
            elif c_prev < c_prev_prev:
                cooperation_trend = -1
        if strong_cooperation_signal and cooperation_trend > 0 or c_prev >= self.m:
            return 1.0
        else:
            return 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_actions = history.my_actions[history.round_number - 1]
        c_prev = self._count_cooperators(prev_round_actions)
        d_prev = self.n - c_prev
        would_meet_threshold_if_i_cooperate = c_prev + 1 >= self.m
        private_gain_possible_if_i_defect = d_prev < self.n - self.m
        strong_cooperation_signal = c_prev >= self.m - 1
        confidence_in_others_cooperating = self._calculate_confidence_in_others(history)
        if would_meet_threshold_if_i_cooperate and private_gain_possible_if_i_defect and strong_cooperation_signal and (confidence_in_others_cooperating > 0):
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_90(BaseStrategy):
    """
    Cooperative Strategy for the Collective Risk Dilemma.
    This strategy aims to maximize individual and collective long-term payoff by
    fostering cooperation when it's beneficial and protecting against exploitation.
    It's adaptive, learning from past rounds and adjusting behavior accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.trust_scores = {i: 0.0 for i in range(self.n)}
        self.defect_when_close_counts = {i: 0 for i in range(self.n)}
        self.cooperation_buffer = 2
        self.min_reliable_cooperators_ratio = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        prev_round_idx = current_round - 1
        cooperation_count_prev_round = 0
        for i in range(self.n):
            if i == 0:
                pass
            else:
                opponent_idx = i - 1
                if opponent_idx < history.opponent_actions.shape[1]:
                    if history.opponent_actions[prev_round_idx, opponent_idx]:
                        self.trust_scores[i] += 1.0
                        self.defect_when_close_counts[i] = 0
                    else:
                        self.trust_scores[i] -= 0.5
        my_cooperation_prev_round = history.my_actions[prev_round_idx]
        cooperation_count_prev_round = int(my_cooperation_prev_round) + int(np.sum(history.opponent_actions[prev_round_idx, :]))
        threshold_met_prev_round = cooperation_count_prev_round >= self.m
        if not threshold_met_prev_round and cooperation_count_prev_round >= self.m - self.cooperation_buffer:
            for i in range(self.n):
                if i == 0:
                    if not my_cooperation_prev_round:
                        self.defect_when_close_counts[i] += 1
                else:
                    opponent_idx = i - 1
                    if opponent_idx < history.opponent_actions.shape[1]:
                        if not history.opponent_actions[prev_round_idx, opponent_idx]:
                            self.defect_when_close_counts[i] += 1
        threshold_met_prev_round = cooperation_count_prev_round >= self.m
        if threshold_met_prev_round:
            return Action.C
        elif cooperation_count_prev_round >= self.m - self.cooperation_buffer:
            reliable_cooperators_count = 0
            for i in range(1, self.n):
                opponent_idx = i - 1
                if opponent_idx < history.opponent_actions.shape[1]:
                    if self.trust_scores[i] > 0 and self.defect_when_close_counts[i] < 2:
                        reliable_cooperators_count += 1
            if reliable_cooperators_count + 1 >= self.m:
                return Action.C
            else:
                return Action.D
        else:
            highly_reliable_cooperators_count = 0
            for i in range(1, self.n):
                opponent_idx = i - 1
                if opponent_idx < history.opponent_actions.shape[1]:
                    if self.trust_scores[i] > 2 and self.defect_when_close_counts[i] == 0:
                        highly_reliable_cooperators_count += 1
            if highly_reliable_cooperators_count >= math.ceil(self.m * self.min_reliable_cooperators_ratio):
                return Action.C
            else:
                return Action.D
        if current_round == self.r and (not threshold_met_prev_round):
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_6(BaseStrategy):
    """
    Aggressive "Exploitative Opportunist" Strategy for Collective Risk Dilemma.

    This strategy primarily defects to secure the base payoff of 1. It only
    cooperates under very specific conditions:
    1. In the last round, if cooperation is necessary to achieve the reward threshold (m).
    2. When the reward is guaranteed or highly probable by other players,
       it will still defect if defecting yields a higher immediate payoff (1+k vs 0+k).
       This highlights its exploitative nature.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.has_cooperated_ever = [False] * self.n
        self.observed_cooperators_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is not None:
            for player_idx in range(self.n):
                if not self.has_cooperated_ever[player_idx]:
                    opponent_col_idx = player_idx if player_idx < self.n - 1 else player_idx - 1
                    if player_idx == self.n - 1:
                        for round_idx in range(history.round_number):
                            if player_idx < self.n - 1:
                                if history.opponent_actions[round_idx, player_idx]:
                                    self.has_cooperated_ever[player_idx] = True
                                    break
            self.observed_cooperators_count = 0
            for i in range(self.n):
                if i != self.n - 1:
                    if self.has_cooperated_ever[i]:
                        self.observed_cooperators_count += 1
        else:
            self.has_cooperated_ever = [False] * self.n
            self.observed_cooperators_count = 0
        action = Action.D
        if current_round == self.r - 1:
            if self.observed_cooperators_count < self.m - 1:
                action = Action.C
        elif self.observed_cooperators_count >= self.m - 1:
            action = Action.D
        return action

class Strategy_AGGRESSIVE_83(BaseStrategy):
    """
    The Opportunistic Driver: An aggressive, adaptive strategy for the Collective Risk Dilemma.
    This strategy prioritizes maximizing individual payoff by defecting when the probability
    of achieving the cooperation reward is low, and cooperating only when the collective
    reward is almost guaranteed or when a strong incentive exists to do so. It's designed
    to push the boundaries of cooperation, forcing others to carry the burden or risk
    losing out on the reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.k_factor_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        n_c_prev = np.sum(history.opponent_actions[current_round - 1, :]) + (1 if history.my_actions[current_round - 1] else 0)
        if n_c_prev >= self.m:
            return Action.C
        cooperation_pressure = max(0, self.m - n_c_prev)
        n_other_players = self.n - 1
        if cooperation_pressure > 0 and cooperation_pressure <= self.k_factor_threshold:
            if cooperation_pressure == 1:
                return Action.C
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_87(BaseStrategy):
    """
    The Opportunistic Enforcer strategy for the Collective Risk Dilemma.
    This strategy is aggressive, prioritizing self-payoff by defecting by default.
    It cooperates only when it can confidently predict the reward threshold will be met,
    or when its own cooperation is the sole factor in achieving the reward.
    It punishes defection with immediate and consistent retaliation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.opponent_history = {}
        self.my_actions_history = []
        self.current_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round = state.round_number
        if history is None:
            self.my_actions_history.append(False)
            return Action.D
        if self.current_round > 0:
            if len(self.my_actions_history) < self.current_round:
                self.my_actions_history.append(history.my_actions[self.current_round - 1])
            for i in range(self.n - 1):
                if i not in self.opponent_history:
                    self.opponent_history[i] = []
                self.opponent_history[i].append(history.opponent_actions[self.current_round - 1, i])
        if self.current_round == self.r:
            self.my_actions_history.append(False)
            return Action.D
        cooperation_scores = {}
        for i in range(self.n - 1):
            if i in self.opponent_history and self.opponent_history[i]:
                cooperations = sum(self.opponent_history[i])
                total_past_rounds = len(self.opponent_history[i])
                cooperation_scores[i] = cooperations / total_past_rounds
            else:
                cooperation_scores[i] = 0.0
        likely_cooperators_count = 0
        for i in range(self.n - 1):
            if cooperation_scores.get(i, 0.0) > 0.5:
                likely_cooperators_count += 1
        remaining_rounds_ratio = (self.r - self.current_round) / self.r
        potential_other_cooperators = self.n - 1 - likely_cooperators_count
        buffer = math.floor(potential_other_cooperators * remaining_rounds_ratio)
        predicted_cooperators = likely_cooperators_count + buffer
        consistent_defectors = 0
        for i in range(self.n - 1):
            if cooperation_scores.get(i, 0.0) < 0.1:
                consistent_defectors += 1
        if predicted_cooperators + 1 >= self.m and consistent_defectors < self.m - 1:
            highly_reliable_cooperators = 0
            for i in range(self.n - 1):
                if cooperation_scores.get(i, 0.0) > 0.9:
                    highly_reliable_cooperators += 1
            cooperate_decision = False
            if highly_reliable_cooperators >= self.m - 1:
                cooperate_decision = True
            elif predicted_cooperators + 1 >= self.m and highly_reliable_cooperators + 1 >= self.m:
                cooperate_decision = True
            elif predicted_cooperators + 1 == self.m and highly_reliable_cooperators < self.m - 1:
                cooperate_decision = True
            if cooperate_decision:
                self.my_actions_history.append(True)
                return Action.C
            else:
                self.my_actions_history.append(False)
                return Action.D
        else:
            self.my_actions_history.append(False)
            return Action.D