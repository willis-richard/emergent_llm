"""
Strategy descriptions for CollectiveRiskDescription.

Generated with:
- Provider: Client
- Model: gemini-2.5-flash-lite
"""


description_COOPERATIVE_1 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust:

## Cooperative Risk Aversion Strategy (CRAS)

**Core Principle:** CRAS aims to achieve the higher collective reward by cooperating, but only when the conditions suggest it's safe and likely to succeed. It prioritizes avoiding the lowest possible payoff (defecting when enough others cooperate) while still enabling the possibility of high collective reward.

**1. Decision Rules:**

*   **Initial Cooperation Threshold:** In the first few rounds, CRAS will adopt a cautious approach. It will cooperate only if the number of observed cooperators in the *previous round* plus 1 (representing its own potential cooperation) is greater than or equal to the cooperation threshold `m`. This helps to build momentum for cooperation without immediately risking a low payoff.

    *   **Pseudocode Snippet:**
        ```
        if current_round <= initial_cooperation_warmup:
            if (observed_cooperators_last_round + 1) >= m:
                play C
            else:
                play D
        ```
        *Note: `initial_cooperation_warmup` is a small, predefined number of initial rounds (e.g., 3-5) to allow for observation. This value could even be a function of `n` and `m`.*

*   **Adaptive Cooperation Threshold:** After the initial warmup phase, CRAS's decision becomes more dynamic. It will cooperate if:
    *   The number of observed cooperators in the *previous round* is strictly greater than `n - m`. This means that if it cooperates, the group will *certainly* reach the cooperation threshold `m`. This is a risk-averse way to ensure its own cooperation contributes to a successful collective outcome.
    *   Alternatively, if the number of observed cooperators in the *previous round* is greater than or equal to `m - 1`, and it has previously observed at least one round where the threshold was met, CRAS will cooperate. This implies a trust has been built. The additional condition of having previously seen the threshold met encourages cooperation when there's evidence it can be sustained.

    *   **Pseudocode Snippet:**
        ```
        if current_round > initial_cooperation_warmup:
            if observed_cooperators_last_round > (n - m):
                play C  // Guaranteed success if I cooperate
            elif observed_cooperators_last_round >= (m - 1) and previously_threshold_met:
                play C  // Trust based on past success
            else:
                play D
        ```

*   **Default to Defection:** If none of the above conditions are met, CRAS will defect. This is the safest option when the likelihood of reaching the cooperation threshold is uncertain, preventing the player from receiving the lower payoff when others defect.

**2. Handling Edge Cases:**

*   **First Round:** In the very first round, CRAS will **always defect**. This is the most conservative strategy as there is no history to learn from. Defecting ensures a guaranteed payoff of 1 and avoids any risk of contributing to a failed cooperation attempt.

    *   **Pseudocode Snippet:**
        ```
        if current_round == 1:
            play D
        ```

*   **Last Round:** In the final round, the incentive structure changes slightly as there are no future rounds to influence. However, CRAS's core decision rules still apply based on the history of the *penultimate* round. The strategy remains the same: cooperate if the conditions suggest a high probability of success and a good individual payoff, otherwise defect. There's no explicit "punishment" or "reward" mechanism for the last round beyond what the payoff calculation dictates.

*   **Dynamic Tracking:** CRAS will maintain the following state variables:
    *   `observed_cooperators_last_round`: The count of players who played 'C' in the immediately preceding round.
    *   `previously_threshold_met`: A boolean flag, set to `True` if there has been at least one round in the history where `m` or more players cooperated. This flag is updated after each round's outcome is known.

**3. Being Cooperative:**

CRAS is inherently cooperative by its design:

*   **Goal-Oriented:** Its primary objective is to enable the higher collective payoff (achieved when `m` or more players cooperate). It actively seeks to be in a position to cooperate when it's strategically sound.
*   **Risk Mitigation:** It doesn't blindly cooperate. It cooperates when it's confident that its contribution will be part of a successful collective effort. This prevents it from being exploited by defectors who might benefit from its cooperation without contributing themselves, thus paving the way for future cooperation.
*   **Building Trust (Implicitly):** The `previously_threshold_met` condition is a subtle way of building trust. Once cooperation has proven successful, CRAS becomes more inclined to participate again, encouraging others to do the same.
*   **Responsiveness:** It adapts its behavior based on the observed actions of others, making it responsive to the evolving game dynamics.

**Overall Strategy Flow (Pseudocode):**

```
// State variables initialized before the game starts
observed_cooperators_last_round = 0
previously_threshold_met = False
initial_cooperation_warmup = min(5, r - 1) // A small number, but not more than r-1

function DecideAction(current_round, historical_actions_per_round):
    // History is available as a list of lists, where each inner list is a player's action in a round.
    // We need to extract our own actions and others' actions.
    // For simplicity, let's assume we can directly access the counts of cooperators from the previous round.

    if current_round == 1:
        return D
    
    // Update counts from the previous round (t-1)
    if current_round > 1:
        cooperators_in_prev_round = 0
        for player_actions_in_prev_round in historical_actions_per_round[current_round - 1]:
            if player_actions_in_prev_round == C:
                cooperators_in_prev_round += 1
        observed_cooperators_last_round = cooperators_in_prev_round

        // Update the threshold met flag if it wasn't already
        if not previously_threshold_met:
            if observed_cooperators_last_round >= m:
                previously_threshold_met = True

    // Decision logic
    if current_round <= initial_cooperation_warmup:
        // Cautious cooperation in warmup phase
        if (observed_cooperators_last_round + 1) >= m:
            return C
        else:
            return D
    else:
        // Adaptive cooperation after warmup
        if observed_cooperators_last_round > (n - m):
            return C  // Guaranteed success if I cooperate
        elif observed_cooperators_last_round >= (m - 1) and previously_threshold_met:
            return C  // Trust based on past success
        else:
            return D

// After each round, update state:
function UpdateState(current_round, actions_in_current_round):
    cooperators_in_current_round = 0
    for action in actions_in_current_round:
        if action == C:
            cooperators_in_current_round += 1
    
    // This will be used in the *next* round's decision
    observed_cooperators_last_round = cooperators_in_current_round

    if not previously_threshold_met:
        if cooperators_in_current_round >= m:
            previously_threshold_met = True
```

This strategy balances the desire for high collective rewards with the need for self-preservation and robustness against exploitative behaviors. It learns from the history of the game and adapts its cooperative tendencies, making it a strong contender in a varied tournament.
'''

description_COOPERATIVE_2 = '''
This strategy is designed to promote cooperation in the Collective Risk Dilemma while remaining resilient to defection. It aims to achieve a high total payoff by rewarding successful cooperation and punishing deviations, but it prioritizes ensuring a minimum level of cooperation to unlock the collective reward.

## Cooperative Strategy: "Cautious Threshold Guardian"

This strategy is built on the principle of *reciprocity with a clear threshold of trust*. It aims to signal a willingness to cooperate, but it's also designed to protect against exploitation by defectors. The core idea is to attempt cooperation when it seems likely to succeed and to retaliate if cooperation fails due to insufficient participation.

### 1. Decision Rules

The core of the strategy revolves around predicting the likelihood of meeting the cooperation threshold `m` and adjusting behavior accordingly.

*   **Cooperate (C) if:**
    *   **Initial Cooperation Phase:** In the early rounds (or until a certain threshold of rounds has passed), the strategy will err on the side of cooperation to try and establish a cooperative norm and to test if others are also willing to cooperate.
    *   **High Probability of Success:** Based on the observed behavior of other players in previous rounds, if the strategy estimates that `m` or more players are likely to cooperate in the current round, it will cooperate. This estimation will be based on the proportion of cooperators in previous rounds.
    *   **"Forgiveness" Window:** If the cooperation threshold was *just* missed in the previous round (i.e., `m-1` cooperators), and the strategy believes there's still a strong chance of success in the current round, it will cooperate. This allows for minor dips in cooperation without immediately abandoning the cooperative endeavor.

*   **Defect (D) if:**
    *   **Low Probability of Success:** If the observed behavior of other players in previous rounds strongly suggests that `m` players will *not* cooperate, the strategy will defect to secure its private endowment.
    *   **Recent History of Failure:** If the cooperation threshold `m` has been missed for a significant number of consecutive rounds, indicating persistent defection or a lack of coordinated cooperation, the strategy will switch to a more defensive posture and defect.
    *   **Retaliation for Exploitation:** If the strategy cooperated in the previous round, but the threshold `m` was still not met (meaning the strategy contributed to the failed project), it will defect in the current round as a form of mild retaliation.

### 2. Handling Edge Cases

*   **First Round (`t=1`):**
    *   **Decision:** Cooperate (C).
    *   **Rationale:** In the absence of any history, the strategy must initiate cooperation. This signals a cooperative intent and allows the strategy to gather initial data on other players' behavior. It's a necessary gamble to potentially unlock the collective reward from the outset.

*   **Last Round (`t=r`):**
    *   **Decision:** Defect (D).
    *   **Rationale:** In the final round, there is no future to incentivize cooperation. The only goal is to maximize the individual payoff. Since cooperation offers no future reward and carries the risk of getting only `(1-1) + k` while others defect and get `1 + k` (or simply `1` if `m` is not met), defection is the dominant strategy for self-interest. This is a standard element of finite repeated games; cooperation unravels at the end.

*   **Near the End of the Game (e.g., `t = r-1`):**
    *   **Decision:** The decision becomes more nuanced and depends on the observed history. If the game has been largely cooperative, the strategy might still cooperate to aim for the highest possible reward. However, if there's been significant defection, the strategy will lean towards defection in these late rounds to secure its payoff, anticipating that others might do the same.

### 3. Being Cooperative

The core of this strategy is to *enable* cooperation when feasible and to signal a willingness to contribute to the collective good.

*   **Intentionality:** The strategy's initial and ongoing attempts to cooperate, especially when the threshold seems achievable, are direct manifestations of its cooperative intent. It actively tries to build a successful collective outcome.
*   **Adaptive Reciprocity:** It employs a form of *conditional cooperation*. It cooperates when it believes its cooperation, combined with others', will be sufficient to trigger the reward. It then retaliates *proportionally* when cooperation fails due to insufficient participation, not out of malice, but to discourage future exploitation and to avoid being the sole contributor to a failed project.
*   **Focus on Threshold:** The emphasis on `m` ensures that the strategy only invests in cooperation when it's likely to be successful and benefit the group. This prevents it from being a "sucker" who always cooperates even when it's clearly not leading to a collective reward.
*   **Long-Term Payoff:** By aiming to achieve the collective reward `k` in as many rounds as possible, the strategy prioritizes a higher total payoff for itself and, by extension, for the group if others adopt similar cooperative strategies.

---

## Pseudocode Representation:

```pseudocode
Strategy: CautiousThresholdGuardian

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

State Variables (maintained across rounds):
  history_actions: List of lists, where history_actions[t][i] is player i's action in round t
  num_cooperators_last_round: Number of players who cooperated in the previous round
  my_action_last_round: My action in the previous round
  consecutive_failed_cooperation: Count of consecutive rounds where m was not met

Function GetAction(current_round, player_index, all_player_actions_history, my_action_history):
  // Initialize state if this is the first round
  If current_round == 1:
    num_cooperators_last_round = 0
    my_action_last_round = None
    consecutive_failed_cooperation = 0
    history_actions = [] // Initialize history

  // --- Handle Edge Cases ---
  If current_round == r:
    Return D // Defect in the last round

  // --- Adaptive Cooperation Logic ---
  // Estimate likelihood of reaching threshold 'm' in this round.
  // This is a simplified estimation. A more sophisticated approach would involve
  // player-specific beliefs. Here, we use the average success rate.
  cooperation_success_rate = (sum of 1s in last 5 rounds of num_cooperators_history) / min(current_round - 1, 5) // Look at last 5 rounds, or fewer if game just started

  estimated_cooperators_this_round = 0
  If current_round > 1:
    // Simple heuristic: Assume others' behavior persists.
    // If more than 'm' cooperated last round, likely to again.
    // If significantly less than 'm', likely to fail again.
    // This part can be complex. For simplicity, let's base it on recent history.
    if num_cooperators_last_round >= m:
      estimated_cooperators_this_round = num_cooperators_last_round // Assume persistence
    elif num_cooperators_last_round >= m - 1:
      estimated_cooperators_this_round = m - 1 // Assume borderline success
    else:
      estimated_cooperators_this_round = num_cooperators_last_round // Assume low participation

  // --- Decision Rules ---

  // Rule 1: Cooperate in the first few rounds to establish cooperation
  // Let's say first 5 rounds, or if it has been very successful so far.
  If current_round <= 5 AND cooperation_success_rate > 0.5: // Example: Early rounds or evidence of prior success
    Return C

  // Rule 2: Cooperate if there's a high probability of success (including borderline cases)
  // Heuristic: If estimated cooperators + 1 (for self) is >= m
  If estimated_cooperators_this_round + 1 >= m:
    Return C

  // Rule 3: Retaliate if cooperation failed last round *and* I cooperated
  If my_action_last_round == C AND num_cooperators_last_round < m:
    Return D // Punish failed cooperation

  // Rule 4: Defect if cooperation has consistently failed
  If consecutive_failed_cooperation >= 3: // Example: 3 consecutive failures
    Return D

  // Rule 5: Default to Defect if no clear signal for Cooperation
  Return D


  // --- Update State Variables for Next Round ---
  // This part would happen *after* the action is chosen and payoffs are calculated.
  // For simplicity, let's assume it's handled by the game engine.
  // If not, you'd need to:
  //   - Record all players' actions for the current round in history_actions.
  //   - Calculate num_cooperators_last_round based on history_actions for the current_round.
  //   - Update my_action_last_round.
  //   - Update consecutive_failed_cooperation based on whether m was met.

```

**Explanation of Pseudocode Logic:**

*   **Initialization:** Sets up the basic state for the first round.
*   **Edge Cases:** Explicitly handles the last round (always defect).
*   **Cooperation Estimation:** The `cooperation_success_rate` and `estimated_cooperators_this_round` are simplified heuristics. A more robust strategy might involve tracking individual players' past actions and building a probabilistic model of their future behavior.
*   **Decision Rules:**
    *   **Early Cooperation:** Encourages initial cooperation in the first few rounds to test the waters.
    *   **High Probability:** Cooperates if it believes `m` will be met (self included).
    *   **Retaliation:** If the strategy cooperated, but the threshold was still missed, it defects in the next round. This acts as a mild punishment and a signal that the strategy won't be exploited.
    *   **Consistent Failure:** If cooperation has failed repeatedly, the strategy switches to a defensive stance.
    *   **Default:** If none of the above conditions are met, it defaults to defecting to ensure its own endowment.

This "Cautious Threshold Guardian" strategy aims to be a balanced and effective approach in the Collective Risk Dilemma. It prioritizes cooperation when conditions are favorable, adapts to the actions of others, and protects itself from exploitation.
'''

description_COOPERATIVE_3 = '''
## Cooperative Strategy: Adaptive Threshold Cooperation (ATC)

This strategy aims to foster cooperation by incentivizing players to contribute to the collective good, while also adapting to perceived defecting behavior. The core idea is to start cooperatively and only defect when the group's cooperation falls short of the required threshold, indicating a need to protect one's own endowment. The strategy is designed to be robust by progressively punishing defection and rewarding cooperation.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in round `t` depends on two primary factors:

*   **The current state of cooperation in the previous round (t-1):** Specifically, the number of players who cooperated in the previous round.
*   **A dynamic "cooperation threshold confidence" parameter:** This parameter reflects how confident we are that the group will meet the cooperation threshold `m`.

Let's define some internal state variables for each player using this strategy:

*   `cooperation_count_t_minus_1`: The number of players who cooperated in round `t-1`.
*   `cooperation_confidence`: A value between 0 and 1 representing our confidence that the group will achieve `m` cooperators. Initially, this is high.
*   `punishment_level`: An integer indicating the severity of our response to perceived defection. Initially 0.

**Decision Logic for Round `t`:**

1.  **If `t == 1` (First Round):**
    *   **Decision: Cooperate (C)**
    *   **Rationale:** In the absence of any history, assume the best and initiate cooperation. This sets a positive precedent.

2.  **If `t > 1` (Subsequent Rounds):**
    *   Calculate `cooperation_count_t_minus_1` based on the observed actions of all players in round `t-1`.
    *   **Update `cooperation_confidence`:**
        *   If `cooperation_count_t_minus_1 >= m`: The group succeeded. Increase `cooperation_confidence` (e.g., `cooperation_confidence = min(1.0, cooperation_confidence + 0.1)`).
        *   If `cooperation_count_t_minus_1 < m`: The group failed. Decrease `cooperation_confidence` (e.g., `cooperation_confidence = max(0.0, cooperation_confidence - 0.2)`).
    *   **Update `punishment_level`:**
        *   If `cooperation_count_t_minus_1 >= m`: The group succeeded. Decrease `punishment_level` (e.g., `punishment_level = max(0, punishment_level - 1)`). This indicates a reduction in the need for punitive measures.
        *   If `cooperation_count_t_minus_1 < m`: The group failed. Increase `punishment_level` (e.g., `punishment_level = punishment_level + 1`). This signals a more cautious approach due to perceived defection.

    *   **Determine Decision:**
        *   **If `cooperation_confidence` is high enough and `punishment_level` is low:**
            *   **Decision: Cooperate (C)**
            *   **Condition:** `cooperation_confidence > (m / n)` AND `punishment_level <= 2` (This is a heuristic, could be tuned).
            *   **Rationale:** We are confident that the group will meet the threshold, and there's no strong indication of widespread defection. Contributing is the best path to maximize collective payoff.

        *   **If `cooperation_confidence` is low or `punishment_level` is high:**
            *   **Decision: Defect (D)**
            *   **Condition:** Otherwise.
            *   **Rationale:** The risk of not meeting the threshold is high, and/or there's been a pattern of insufficient cooperation. Protecting one's own endowment becomes the priority.

3.  **If `t == r` (Last Round):**
    *   **Decision: Defect (D)**
    *   **Rationale:** Since there are no future rounds to incentivize cooperation, the optimal strategy in the final round is always to defect and secure the private payoff of 1. This is a standard game-theoretic outcome in the last round of repeated games.

### 3. Being Cooperative

This strategy is inherently cooperative in its *intent* and *initial actions*:

*   **Initiation of Cooperation:** The strategy always starts by cooperating, assuming good faith and hoping to establish a cooperative equilibrium.
*   **Reward for Success:** It explicitly rewards collective success by increasing `cooperation_confidence` and decreasing `punishment_level`. This reinforces positive behavior.
*   **Conditional Punishment:** Defection is not a default; it's a *response* to insufficient cooperation. The strategy doesn't seek to punish arbitrarily but to deter free-riding and encourage the group to reach the cooperative threshold.
*   **Adaptability:** The strategy learns from past rounds and adjusts its behavior, aiming to find a sustainable cooperative outcome. It's not rigid and can adapt to different group dynamics.

### Pseudocode

```python
# Internal state for each player using this strategy
cooperation_confidence = 1.0  # Start optimistic
punishment_level = 0          # Start with no punishment

def decide_action(current_round, num_players, min_cooperators, reward_factor, observed_actions_previous_round):
    """
    Decides whether to Cooperate (C) or Defect (D) for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        num_players (int): Total number of players (n).
        min_cooperators (int): Minimum cooperators needed (m).
        reward_factor (float): Reward multiplier (k).
        observed_actions_previous_round (list): A list of actions ('C' or 'D') of all players in the previous round.
                                                If current_round is 1, this will be None or empty.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """
    global cooperation_confidence, punishment_level # Access and modify global state

    # --- Handle Edge Cases ---
    if current_round == 1:
        return 'C' # Always cooperate in the first round

    if current_round == num_rounds: # Assuming num_rounds is accessible globally or passed in
        return 'D' # Always defect in the last round

    # --- Update Internal State (for rounds > 1 and not the last round) ---
    cooperation_count_t_minus_1 = observed_actions_previous_round.count('C')

    if cooperation_count_t_minus_1 >= min_cooperators:
        # Group succeeded: increase confidence, decrease punishment
        cooperation_confidence = min(1.0, cooperation_confidence + 0.1)
        punishment_level = max(0, punishment_level - 1)
    else:
        # Group failed: decrease confidence, increase punishment
        cooperation_confidence = max(0.0, cooperation_confidence - 0.2)
        punishment_level = punishment_level + 1

    # --- Decision Logic ---
    # Threshold for confidence: a simple heuristic, can be tuned
    confidence_threshold = min_cooperators / num_players
    # Heuristic for acceptable punishment level
    punishment_threshold = 2

    if cooperation_confidence > confidence_threshold and punishment_level <= punishment_threshold:
        return 'C' # Cooperate if confident and punishment is low
    else:
        return 'D' # Defect if not confident or punishment is high

```

**Explanation of Heuristics and Adaptability:**

*   **`cooperation_confidence` Update:** The specific increment/decrement values (0.1, 0.2) are tunable parameters. A larger decrement for failure makes the strategy more sensitive to defection.
*   **`punishment_level` Update:** Similarly, the increment/decrement values are tunable.
*   **`confidence_threshold`:** Using `m / n` as a baseline confidence threshold makes sense because if our confidence is below the objective probability of success (assuming random actions), we should be more cautious.
*   **`punishment_threshold`:** The value of `2` for `punishment_level` is a heuristic. It means that if we experience two consecutive rounds of failure (or one very bad round and then another), we will start defecting. This allows for some tolerance but punishes persistent failure.
*   **Robustness:**
    *   Against **consistent cooperators**: The strategy will remain cooperative, maximizing everyone's payoff.
    *   Against **tit-for-tat-like defectors**: If they defect, our `cooperation_confidence` will drop and `punishment_level` will rise, leading us to defect in subsequent rounds, effectively mirroring their behavior after a few rounds.
    *   Against **always defectors**: We will start cooperating, then quickly recognize the failure to meet `m`, our confidence will drop, and punishment will rise, leading us to defect as well.
    *   Against **random players**: The strategy will adapt based on the observed outcomes. If random actions lead to meeting `m`, we'll continue cooperating. If not, we'll shift to defecting.

This Adaptive Threshold Cooperation (ATC) strategy provides a framework for cooperative behavior that is responsive to group dynamics and aims to achieve the higher collective payoffs offered by the game's cooperative equilibrium, while prudently protecting oneself when cooperation falters.
'''

description_COOPERATIVE_4 = '''
## Cooperative Strategy: The "Adaptive Threshold Cooperat" (ATC)

This strategy aims to foster cooperation by demonstrating a commitment to the community project while simultaneously protecting itself from exploitation. It adapts its behavior based on the observed success of cooperation and the proximity to the critical threshold of `m`.

### Core Principles:

1.  **Initial Trust, but with Safeguards:** Begin with a cooperative stance to encourage others, but be prepared to defect if cooperation falters.
2.  **Threshold Awareness:** Actively monitor the number of cooperators and adjust behavior as the threshold `m` is approached.
3.  **Punish Defection, Reward Cooperation:** Respond to observed defection by reducing cooperation, and reward consistent cooperation with continued commitment.
4.  **Forgiveness (with Limits):** Allow for some deviation from cooperation, but if defection becomes too prevalent, a more defensive stance will be adopted.
5.  **Endgame Consideration:** Adjust strategy in the final round to maximize individual payoff, given the completed history.

### Strategy Components and Decision Rules:

The ATC strategy can be broken down into the following components:

#### 1. Initial Round Behavior:

*   **Rule:** In the first round (t=1), always cooperate.
    *   **Rationale:** This signals a willingness to engage in the collective project and sets a positive tone. It's a low-cost way to explore the potential for cooperation.

#### 2. Dynamic Cooperation Threshold Adjustment:

The strategy maintains an internal "cooperation belief" about the likelihood of the community reaching the `m` threshold. This belief is updated based on historical observations.

*   **Variable:** `cooperation_belief` (float, 0.0 to 1.0) - Represents the estimated probability that `m` or more players will cooperate in the current round.
*   **Initialization:** `cooperation_belief = 1.0` (since we start with the belief that cooperation is possible).
*   **Update Rule (after each round t > 1):**
    *   Let `cooperators_observed_t` be the number of players observed to cooperate in round `t`.
    *   Calculate `probability_of_m_met_in_t` = (number of ways to choose `m-1` cooperators from `cooperators_observed_t-1` other players) / (number of ways to choose `m-1` players from `n-1` other players). This is a simplified binomial probability estimate.
    *   `cooperation_belief` = `alpha` * `cooperation_belief` + (1 - `alpha`) * `probability_of_m_met_in_t`
        *   `alpha` (float, e.g., 0.8): A decay factor, giving more weight to past beliefs but allowing for adaptation.

#### 3. Cooperation Decision Rule (for round t > 1):

The decision to cooperate or defect in round `t` depends on the current `cooperation_belief` and the proximity to the `m` threshold.

*   **If `t == r` (Last Round):**
    *   **Decision:** Defect.
    *   **Rationale:** In the final round, there is no future incentive to cooperate. The primary goal becomes maximizing individual payoff by keeping the endowment, as the reward from the community project is no longer relevant for future rounds.

*   **If `t < r` (Not the Last Round):**
    *   **Decision:**
        *   **If `cooperation_belief` >= `m / n` AND `cooperators_observed_t-1` >= `m - 1` (i.e., the threshold was met or very nearly met in the previous round, and we believe it's likely to be met again):** Cooperate.
        *   **Else if `cooperators_observed_t-1` < `m - 1` (i.e., cooperation failed in the previous round and we are far from the threshold):** Defect.
        *   **Else (in the "grey zone" where cooperation is uncertain or borderline):**
            *   **If `cooperators_observed_t-1` is "close" to `m-1` (e.g., `m-1 - num_observed_cooperators_t-1 < buffer`):** Cooperate with probability `cooperation_belief`. This means, if `cooperation_belief` is 0.7, we cooperate 70% of the time.
            *   **Else:** Defect.

    *   **Rationale:** This rule is adaptive. It cooperates when there's a strong indication that the threshold will be met, thus securing the `k` reward. It defects when cooperation is clearly failing to protect against individual loss. The "grey zone" allows for a calculated risk-taking approach, using the `cooperation_belief` to guide probabilistic cooperation.

#### 4. Defection Response and Forgiveness:

*   **Rule:** If, in any round `t-1`, `m` or more players *did not* cooperate (meaning the threshold was NOT met), and `m` or more players *did* cooperate in the *previous* round (`t-2`), then our `cooperation_belief` is significantly reduced, and we become more hesitant to cooperate in the subsequent round.
    *   **Rationale:** This mechanism punishes a sudden collapse in cooperation. It signals that the strategy will not continue to bear the cost of cooperation if others renege on their commitment.

*   **Forgiveness (limited):** The `cooperation_belief` update mechanism provides a degree of forgiveness. If cooperation rebounds after a failure, the `cooperation_belief` will naturally rise again. However, sustained defection will lead to a prolonged period of distrust.

#### 5. History Tracking:

The strategy needs to track the following historical data:

*   `actions_history`: A list of (player_id, action) tuples for each round.
*   `num_cooperators_history`: A list of the number of cooperators in each round.
*   `cooperation_belief`: The internal belief variable.
*   `t`: The current round number.

### Pseudocode:

```pseudocode
// Game Parameters
n: integer (number of players)
r: integer (number of rounds)
m: integer (minimum cooperators needed)
k: float (reward factor)

// Strategy Variables
cooperation_belief: float = 1.0
actions_history: list of tuples = []
num_cooperators_history: list of integers = []
t: integer = 1 // Current round

// Parameters for adaptation
alpha: float = 0.8 // Decay factor for cooperation_belief
buffer: integer = 1 // Sensitivity buffer for threshold
```

```pseudocode
// --- Decision Function for Player i in Round t ---
function decide_action(t, n, m, k, actions_history, num_cooperators_history):

    // Handle Last Round
    if t == r:
        return D

    // Initialize cooperation_belief for first round if not already set
    if t == 1:
        cooperation_belief = 1.0 // Or a slightly lower value like 0.9 to be slightly cautious
    else:
        // Update cooperation_belief based on previous round's outcome
        previous_round_cooperators = num_cooperators_history[t-2] // Assuming 0-indexed history
        probability_of_m_met_in_previous_round = calculate_prob_m_met(previous_round_cooperators, n, m) // Needs a binomial probability function
        cooperation_belief = alpha * cooperation_belief + (1 - alpha) * probability_of_m_met_in_previous_round

    // Determine action based on cooperation_belief and threshold
    current_round_cooperators = get_observed_cooperators(t-1, actions_history) // Function to count cooperators from history

    if cooperation_belief >= (m / n) and current_round_cooperators >= (m - 1):
        return C
    else if current_round_cooperators < (m - 1) and cooperation_belief < (m / n): // Significantly below threshold
        return D
    else: // "Grey Zone" - uncertain
        // Use cooperation_belief as a probability to decide
        if random_float() < cooperation_belief:
            return C
        else:
            return D

// --- Helper Functions ---

function calculate_prob_m_met(observed_cooperators, n, m):
    // This is a simplified placeholder. A proper implementation would use binomial distribution.
    // For example, probability of reaching m if 'observed_cooperators' is the number of players
    // we know for sure will cooperate, and we assume others cooperate with probability 'observed_cooperators / (n-1)'
    if observed_cooperators >= m:
        return 1.0
    elif observed_cooperators < m - buffer: // Too far to reach
        return 0.0
    else:
        // A more sophisticated calculation would be needed here, e.g., using binomial distribution
        // based on the assumption that remaining players cooperate with some probability.
        // For simplicity, we can return a value reflecting proximity.
        return min(1.0, max(0.0, (observed_cooperators - (m - 1 - buffer)) / (buffer + 1.0)))


function get_observed_cooperators(round_num, actions_history):
    count = 0
    for player_action in actions_history[round_num]: // Assuming actions_history is structured by round
        if player_action.action == C:
            count += 1
    return count

// --- Main Loop (simulated) ---
// In each round, the AI would:
// 1. Call decide_action() to get its action.
// 2. Record its action and observe other players' actions.
// 3. Update num_cooperators_history and actions_history.
// 4. Increment t.
```

### Strategy Alignment with Cooperative Mindset:

1.  **Initial Cooperation:** The strategy starts by cooperating, demonstrating a willingness to contribute to the collective good.
2.  **Conditional Cooperation:** Cooperation is maintained as long as there's a reasonable expectation of the collective goal being met (`m` cooperators). This aligns with the idea of "cooperating when it's mutually beneficial."
3.  **Responsiveness to Others:** The strategy reacts to the behavior of other players. It rewards observed cooperation with continued cooperation and punishes sustained defection by becoming more defensive.
4.  **Commitment to the Threshold:** The core of the strategy is focused on achieving the `m` threshold, which is the condition for collective benefit. This directly supports the cooperative objective of the game.
5.  **Adaptive Nature:** By updating its `cooperation_belief`, the strategy doesn't rigidly stick to a plan but adapts to the unfolding reality of the game, which is a hallmark of intelligent and cooperative behavior.

This strategy is designed to be robust because it doesn't rely on specific knowledge of opponents' strategies. It learns from the aggregate behavior and makes decisions based on probabilities and observed outcomes, aiming to find a sustainable level of cooperation that maximizes its own long-term payoff while contributing to the potential for collective success.
'''

description_COOPERATIVE_5 = '''
This is a fascinating variant of the Prisoner's Dilemma, where cooperation provides a collective benefit but also exposes individuals to greater risk if the threshold isn't met. The key challenge is to incentivize cooperation without being exploited.

Here's a cooperative strategy designed to be adaptive and robust, focusing on building trust through observed cooperation while minimizing individual risk.

## Cooperative Strategy: "Conditional Reciprocity with Threshold Vigilance"

This strategy aims to foster cooperation by rewarding it and punishing defection, but with a strong emphasis on ensuring the collective reward is achievable. It's designed to learn and adapt to the prevailing behavior of other players.

### Core Principles:

1.  **Reward Cooperation:** When cooperation leads to success (threshold met), generously reward it.
2.  **Punish Defection:** When defection leads to failure (threshold not met), punish it by defecting in the next round.
3.  **Threshold Vigilance:** Prioritize ensuring the "m" threshold is met. This is crucial for maximizing collective payoffs and avoiding individual losses.
4.  **Adaptive Learning:** Continuously monitor the observed cooperation rate and adjust the strategy's own cooperativeness accordingly.
5.  **Last Round Caution:** In the final round, the incentive for long-term reciprocity disappears, so the strategy becomes more self-interested.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on a few factors:

*   **Observed Cooperation Rate (`obs_coop_rate_t-1`):** The proportion of players who cooperated in the *previous* round (`t-1`).
*   **My Cooperation History (`my_coop_t-1`):** Whether I cooperated in the previous round (`t-1`).
*   **Proximity to Threshold (`m`):** How close the observed cooperation rate is to the minimum required for reward.

**Decision Logic for Player `i` in Round `t`:**

Let `num_cooperators_t-1` be the number of players who cooperated in round `t-1`.
`obs_coop_rate_t-1 = num_cooperators_t-1 / n`

*   **If `t == r` (Last Round):**
    *   **Defect (D):** This strategy becomes purely self-interested in the last round. The future benefits of cooperation are gone, so it's best to secure the private endowment.

*   **If `t < r` (Not the Last Round):**
    *   **Consider Cooperation if:**
        *   The observed cooperation rate in the previous round was high enough to *suggest* others are cooperating or are likely to cooperate to meet the threshold: `num_cooperators_t-1 >= m - 1`. (We only need one more cooperator besides ourselves).
        *   OR if the observed cooperation rate in the previous round was already successful (`num_cooperators_t-1 >= m`) AND I cooperated in the previous round. This signals a willingness to maintain successful cooperation.

    *   **Defect if:**
        *   The observed cooperation rate in the previous round was too low to realistically meet the threshold *even if I cooperate*: `num_cooperators_t-1 < m - 2`. (Even if I and everyone else who defected in `t-1` now cooperate, we still won't reach `m`). This is a strong signal that cooperation is not viable.
        *   OR if the observed cooperation rate in the previous round was *just* below the threshold (`num_cooperators_t-1 == m - 1`) AND I defected in the previous round. This is a tit-for-tat punishment for failing to contribute when it was critical.
        *   OR if the observed cooperation rate in the previous round was *just* above the threshold (`num_cooperators_t-1 == m`) BUT I defected in the previous round. This punishes my own previous defection even though the group succeeded.

    *   **Default to Cooperate (with nuance):**
        *   If none of the above strong conditions for defection are met, and the conditions for considering cooperation are also not strongly met, the strategy will *lean towards cooperation* if it believes it can nudge the group towards success. This means if `m` is relatively achievable (e.g., `obs_coop_rate_t-1 * n >= m - 2`), it might choose to cooperate to test the waters and encourage others.

**Simplified Decision Rule Summary:**

Let `num_cooperators_t-1` be the count of cooperators in round `t-1`.

*   **If `t == r`:**
    *   `Action = D`

*   **If `t < r`:**
    *   **Defect if:**
        *   `num_cooperators_t-1 < m - 2` (Too few cooperators, unlikely to ever reach `m`)
        *   `num_cooperators_t-1 == m - 1` AND `my_coop_t-1 == D` (Punish own defection when `m` was narrowly missed)
        *   `num_cooperators_t-1 == m` AND `my_coop_t-1 == D` (Punish own defection when `m` was met, but I didn't contribute)
    *   **Cooperate if:**
        *   `num_cooperators_t-1 >= m - 1` (High chance of reaching `m` or already reached `m`, especially if I cooperated)
        *   OR if `num_cooperators_t-1 >= m - 2` and `my_coop_t-1 == C` (Maintain cooperation if successful or nearly successful)
    *   **Default (Cautious Cooperation):** If the above are not definitive, and the group is *close enough* to `m` to make cooperation a worthwhile gamble (e.g., `num_cooperators_t-1 >= m - 3` and I cooperated last round), cooperate. Otherwise, defect.

This logic needs refinement for practical implementation. Let's use a more structured approach.

---

### **Strategy: "Adaptive Grateful Defector"**

This strategy is designed to be:
*   **Cooperative by Default:** Assumes good intentions and tries to foster cooperation.
*   **Grateful:** Rewards successful cooperation.
*   **Retaliatory:** Punishes exploitation, but only strategically.
*   **Adaptive:** Adjusts based on observed behavior.

**1. Decision Rules (Natural Language & Pseudocode):**

The core idea is to maintain a "cooperation threshold" internally. If the observed cooperation rate is *at or above* this threshold, we cooperate. If it drops below, we defect until the observed rate recovers.

*   **Initialization (Round 0):**
    *   Set an initial internal `cooperation_target_rate`. This should be slightly *above* `m/n` to encourage exceeding the threshold, but not so high as to be unachievable early on. A good starting point could be `m/n + epsilon` (e.g., `m/n + 0.05` or `(m+1)/n` if `m < n-1`).
    *   `my_last_action = C` (Start optimistically)

*   **For Round `t` (where `t > 0`):**

    *   **Step 1: Analyze Previous Round (`t-1`)**
        *   Let `N_cooperators_t-1` be the number of players who cooperated in round `t-1`.
        *   Calculate `observed_coop_rate_t-1 = N_cooperators_t-1 / n`.
        *   Determine if the threshold `m` was met: `threshold_met_t-1 = (N_cooperators_t-1 >= m)`.

    *   **Step 2: Update Internal `cooperation_target_rate` (Adaptation)**
        *   If `threshold_met_t-1` was `True`:
            *   We are being rewarded for cooperation. Increase the `cooperation_target_rate` slightly. This signals appreciation and aims to maintain success.
            *   `cooperation_target_rate = min(1.0, cooperation_target_rate + 0.05)` (Cap at 1.0)
        *   If `threshold_met_t-1` was `False`:
            *   Cooperation was not enough. Decrease the `cooperation_target_rate` slightly. This makes it easier for us to cooperate in the future, as we're less demanding.
            *   `cooperation_target_rate = max(m/n, cooperation_target_rate - 0.05)` (Floor at `m/n` to ensure we don't aim too low)

    *   **Step 3: Determine Current Action**

        *   **Special Case: Last Round (`t == r`)**
            *   `current_action = D` (Self-interest takes over, no future benefit from cooperation)

        *   **General Case (`t < r`)**
            *   **If `observed_coop_rate_t-1 >= cooperation_target_rate`:**
                *   The group is behaving cooperatively enough to meet our current target.
                *   `current_action = C`
            *   **Else (`observed_coop_rate_t-1 < cooperation_target_rate`):**
                *   The group is not cooperating enough. We need to signal dissatisfaction or wait for better behavior.
                *   However, consider one last push if we were close:
                    *   If `N_cooperators_t-1 >= m - 1` (meaning only one more cooperator was needed, and it might have been us):
                        *   `current_action = C` (A final attempt to reach the threshold).
                    *   Else:
                        *   `current_action = D` (Defect to signal, and to protect ourselves from likely failure).

    *   **Step 4: Record `my_last_action` for the next round.**
        *   `my_last_action = current_action`

**Pseudocode:**

```python
# Parameters
n = ... # number of players
r = ... # number of rounds
m = ... # minimum cooperators needed
k = ... # reward factor

# State variables (internal to the AI)
cooperation_target_rate = (m / n) + 0.05 # Initial target, slightly above threshold
my_last_action = 'C' # Assume cooperation initially

# History (for observed actions) - In a tournament, this would be provided externally
# For this strategy description, we assume we know N_cooperators_t-1

def decide_action(current_round, N_cooperators_prev_round):
    global cooperation_target_rate
    global my_last_action

    # --- Handle Last Round ---
    if current_round == r:
        return 'D'

    # --- Analyze Previous Round ---
    observed_coop_rate_prev = N_cooperators_prev_round / n
    threshold_met_prev = (N_cooperators_prev_round >= m)

    # --- Update Internal Cooperation Target (Adaptation) ---
    if threshold_met_prev:
        # Group succeeded, reward cooperation by increasing our expectation
        cooperation_target_rate = min(1.0, cooperation_target_rate + 0.05)
    else:
        # Group failed, lower our expectation to make cooperation easier
        cooperation_target_rate = max(m / n, cooperation_target_rate - 0.05)

    # --- Determine Current Action ---
    current_action = 'D' # Default to defecting

    if observed_coop_rate_prev >= cooperation_target_rate:
        # Observed behavior meets or exceeds our target, so we cooperate
        current_action = 'C'
    else:
        # Observed behavior is below our target.
        # Make one final push if we were close to the threshold last round.
        if N_cooperators_prev_round >= m - 1:
            current_action = 'C' # Last ditch effort if only one more cooperator was needed
        # Else (observed_coop_rate_prev < cooperation_target_rate AND N_cooperators_prev_round < m - 1):
            # Stay defecting as the target isn't met and we weren't close enough for a push.

    # --- Record my action for next round ---
    my_last_action = current_action
    return current_action

# Example usage (in a tournament loop):
# N_cooperators_prev_round = get_observed_cooperators_last_round()
# action_this_round = decide_action(t, N_cooperators_prev_round)
```

---

### 2. Handling Edge Cases:

*   **First Round (`r = 1`):**
    *   The problem statement says `r > 1`. If `r=1`, this is a one-shot game. The strategy described above doesn't have a "previous round" to analyze.
    *   **For `t = 1`:** We need an initial action. The strategy starts with `my_last_action = 'C'` and an initial `cooperation_target_rate` based on `m/n`. The `decide_action` function would need a slight modification for `t=1` where `N_cooperators_prev_round` is effectively 0 (or we have no prior information).
    *   **Revised `decide_action` for `t=1`:**
        *   If `t == 1`:
            *   If `m/n < 0.5` (threshold is less than half the players): Cooperate `C` to test the waters and show willingness.
            *   If `m/n >= 0.5`: Defect `D`. If more than half the players need to cooperate, it's too risky to be the sole cooperator without evidence.
            *   This initial decision can be seen as a heuristic. Another approach is to always cooperate in the first round to signal good intent, regardless of `m/n`. Let's adopt this simpler, more cooperative start.
            *   **For `t = 1` (Revised): `current_action = 'C'`**

*   **Last Round (`t = r`):**
    *   As explicitly stated in the rules: The strategy defects (`D`). There's no incentive to maintain good behavior for future rounds.

*   **Few Players (`n` is small):**
    *   The logic scales naturally. If `n=2` and `m=2`, players need to cooperate. The strategy will attempt to cooperate if `cooperation_target_rate` is met, which means the other player must have cooperated. If the other player defects, `N_cooperators_prev_round` will be 1, `observed_coop_rate_prev` will be 0.5. If `cooperation_target_rate` is still high, we defect. If `m=2`, `m/n = 1`. `cooperation_target_rate` starts at `1.05`. It will only cooperate if the other player *always* cooperates, ensuring `N_cooperators_prev_round` is `n`. This correctly models the high-stakes nature of small groups needing full cooperation.

*   **High `m` (e.g., `m = n-1`):**
    *   Requires almost universal cooperation. The strategy will be very cautious. It will only cooperate if `N_cooperators_prev_round` is `n-1` (meaning the other `n-1` players cooperated) or if `N_cooperators_prev_round` is `n` (everyone cooperated). It will defect if `N_cooperators_prev_round` is `n-2` or less, as the threshold is too unlikely to be met. The `cooperation_target_rate` will likely stay high, demanding strong evidence of cooperation from others.

*   **Low `m` (e.g., `m = 2`):**
    *   Requires minimal cooperation. The strategy will be more willing to cooperate, especially early on, to try and build momentum. The `cooperation_target_rate` will be lower, and the condition `N_cooperators_prev_round >= m - 1` becomes easier to meet.

---

### 3. Being Cooperative:

*   **Default to Cooperation:** The strategy starts by cooperating in the first round (`t=1`), signaling a desire for mutual benefit.
*   **Conditional Reciprocity (Grateful Defector):** It rewards cooperation by increasing its `cooperation_target_rate` when the threshold is met. This signifies: "When you cooperate and we succeed, I'll be more inclined to cooperate in the future."
*   **Adaptive Threshold:** By adjusting the `cooperation_target_rate`, the strategy aims to align itself with the group's behavior. If the group is generally cooperative, it becomes more cooperative. If the group is not, it becomes more cautious but still tries to facilitate cooperation when it's close to success.
*   **Last Ditch Effort:** The rule `if N_cooperators_prev_round >= m - 1: current_action = 'C'` is crucial for cooperation. It means that even if our internal `cooperation_target_rate` hasn't been met by others, if the group was *just* one cooperator short, we will still try to be that cooperator to achieve the collective reward. This is a strong cooperative signal.
*   **Punishment is Restorative:** Defection is not purely punitive; it's a signal. By lowering the `cooperation_target_rate` when cooperation fails, the strategy makes it easier for *itself* to cooperate again, aiming to bring the group back to a cooperative state. It doesn't get stuck in endless defection loops unless the observed behavior is consistently poor and unrecoverable.

This strategy aims to be robust by not blindly cooperating, but by basing its decisions on observed history and actively seeking conditions where mutual cooperation is most likely to succeed and be rewarded. It prioritizes the collective good when achievable but protects itself from exploitation when necessary.
'''

description_COOPERATIVE_6 = '''
Here's a design for a cooperative strategy for the Collective Risk Dilemma game, focusing on adaptiveness, robustness, and a clear cooperative orientation.

## Cooperative Strategy: "Adaptive Threshold Seeker"

This strategy aims to achieve cooperation by actively monitoring the group's collective behavior and adjusting its own actions accordingly. It balances the desire for the high reward of successful cooperation with the risk of being exploited by defectors.

### Core Philosophy:

The "Adaptive Threshold Seeker" is fundamentally cooperative. It believes that the group's greatest success comes from meeting the cooperation threshold. However, it recognizes that individual cooperation can be costly if not reciprocated. Therefore, it seeks to identify and leverage periods of collective commitment while mitigating the risks of unilateral cooperation.

### 1. Decision Rules:

The core of the strategy lies in a dynamic assessment of the group's cooperation level and its impact on the player's expected payoff.

**A. Cooperation Threshold Monitoring:**

*   **Count Previous Cooperators:** In each round `t > 1`, the strategy will count the number of players who cooperated in the *previous* round (`t-1`). Let's call this `previous_cooperators`.
*   **Assess Group Performance:** Based on `previous_cooperators`, the strategy makes a decision for the current round:
    *   **If `previous_cooperators` >= m - 1 (and I cooperated last round):** If the previous round was *close* to meeting the threshold (meaning at least `m-1` players cooperated), and *I was one of them*, then there is a high probability that the group can achieve the threshold in the current round. In this case, the strategy will **Cooperate (C)**. The rationale is that collective effort is being demonstrated, and my cooperation is likely to tip the balance.
    *   **If `previous_cooperators` >= m (and I defected last round):** If the previous round *successfully* met the threshold (`m` or more cooperators), but *I defected*, then it's a strong signal that others are willing to bear the risk. The strategy will **Cooperate (C)** to share in the reward and contribute to future success.
    *   **If `previous_cooperators` < m - 1:** If the previous round was far from meeting the threshold, or if there were only a few cooperators, it suggests that the group is not reliably coordinating. In this scenario, to avoid being exploited, the strategy will **Defect (D)**. This acts as a self-preservation mechanism.
    *   **If `previous_cooperators` == m - 1 (and I defected last round):** This is a critical borderline case. The group *almost* made it, but I didn't contribute to the success. This strategy leans towards **Cooperate (C)**. The idea is to "nudge" the group over the threshold and signal a commitment to cooperation. This is a slight bias towards cooperation to encourage group success.

**B. Payoff-Driven Re-evaluation (Optional but recommended for robustness):**

While the primary decision is based on past cooperation counts, a more advanced version could incorporate a payoff-based adjustment.

*   **Monitor Personal Payoff Trend:** If the strategy consistently receives low payoffs (i.e., mostly individual payouts without the `k` reward) over several rounds, even when attempting to cooperate, it might indicate a persistently uncooperative environment. In such cases, the strategy might shift towards more frequent defections to protect its accumulated score, until evidence of group-level cooperation reappears. This is a secondary adjustment, not the primary driver.

### 2. Handling Edge Cases:

**A. The First Round (t=1):**

*   **Initial Action: Cooperate (C)**
    *   **Rationale:** In the absence of any history, the most cooperative stance is to initiate cooperation. This signals a willingness to work towards the common good and sets a positive tone. It also offers the possibility of immediately achieving the `k` reward if enough other players also cooperate. This is a calculated risk to unlock higher payoffs for everyone.

**B. The Last Round (t=r):**

*   **Final Action: Cooperate (C)**
    *   **Rationale:** Even in the last round, the strategy maintains its cooperative orientation. There is no future round to be exploited in, and cooperating now ensures that if the threshold is met, the player receives the `k` reward. Furthermore, it leaves a positive final impression, which could be relevant in a tournament setting if final scores are used for tie-breaking or reputation. It reinforces the cooperative intent throughout the game.

**C. Low Number of Players (n close to m):**

*   The strategy remains the same. The `m` threshold is relative to `n`. If `n=4` and `m=3`, then `m-1=2`. The logic of counting previous cooperators and adjusting still holds.

**D. High Number of Players (n much larger than m):**

*   The strategy remains the same. The `m` threshold still dictates the target. The adaptive nature of the strategy is designed to handle varying group sizes and cooperation densities.

### 3. Be Cooperative:

*   **Intent:** The strategy's primary goal is to maximize the *collective* payoff by reaching the cooperation threshold `m`. It believes that sustained cooperation yields the highest rewards for all.
*   **Bias:** The strategy has a slight bias towards cooperation, particularly in borderline cases (e.g., `m-1` cooperators and the strategy defected). This is to encourage the group to overcome the hurdle and reap the benefits.
*   **Learning and Adaptation:** The strategy actively learns from the group's past behavior. If the group demonstrates a pattern of cooperation, it will reinforce its own cooperative actions. If the group consistently fails to cooperate, it will temporarily revert to defection to protect itself, but it will remain vigilant for any signs of renewed collective effort.
*   **No Exploitation:** The strategy is designed *not* to exploit others. It doesn't aim to "free-ride" on the efforts of others by defecting when the threshold is likely to be met. Its defections are primarily for self-preservation when cooperation appears unlikely.

### Pseudocode Representation:

```python
class AdaptiveThresholdSeeker:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.history = [] # Stores actions of all players in previous rounds
        self.my_last_action = None # To track my own last action

    def choose_action(self, current_round):
        # --- Handle First Round ---
        if current_round == 1:
            self.my_last_action = 'C'
            self.history.append([self.my_last_action] * self.n) # Placeholder for others' actions
            return 'C'

        # --- Handle Last Round ---
        if current_round == self.r:
            self.my_last_action = 'C'
            return 'C'

        # --- Decision Rules for Intermediate Rounds ---
        previous_round_actions = self.history[-1] # Actions from round t-1
        previous_cooperators = sum(1 for action in previous_round_actions if action == 'C')

        # Check if I cooperated in the previous round
        i_cooperated_last_round = (self.my_last_action == 'C')

        # Decision logic
        if previous_cooperators >= self.m:
            # Threshold was met last round
            if not i_cooperated_last_round:
                # I defected, others succeeded. Now I cooperate.
                decision = 'C'
            else:
                # I cooperated and threshold was met. Continue cooperating.
                decision = 'C'
        elif previous_cooperators == self.m - 1:
            # Almost met the threshold
            if i_cooperated_last_round:
                # I cooperated and we almost made it. Keep cooperating.
                decision = 'C'
            else:
                # I defected, we almost made it. Lean towards cooperation to push it.
                decision = 'C'
        else: # previous_cooperators < m - 1
            # Far from threshold. Defect for self-preservation.
            decision = 'D'

        self.my_last_action = decision
        return decision

    def record_history(self, all_players_actions_this_round):
        # This method would be called *after* all players have chosen their actions.
        # It records the actions of all players for the current round.
        self.history.append(all_players_actions_this_round)

# Example Usage (Conceptual):
# strategy = AdaptiveThresholdSeeker(n=6, r=10, m=3, k=2)
# for round_num in range(1, 11):
#     my_action = strategy.choose_action(round_num)
#     # ... (play the round, get actions of other players)
#     # other_players_actions = [...]
#     # all_actions_this_round = [my_action] + other_players_actions
#     # strategy.record_history(all_actions_this_round)
```

This strategy is designed to be robust because it doesn't rely on predicting specific opponent behaviors but rather on observing and reacting to aggregated group behavior. Its cooperative nature is embedded in its default actions and its preference for shared success over individual gain, while incorporating self-preservation.
'''

description_COOPERATIVE_7 = '''
This is a fascinating strategic dilemma! The core challenge lies in balancing individual gain (defecting when others cooperate) with collective benefit (cooperating to reach the threshold and earn the reward). Given the repeated nature of the game and the perfect information, a strategy that focuses on **reciprocity and conditional cooperation** would be most effective.

Here's a design for a cooperative strategy, which I'll call the **Adaptive Reciprocity Strategy (ARS)**.

## Adaptive Reciprocity Strategy (ARS)

The ARS aims to foster cooperation by rewarding it and punishing defection, but with a degree of forgiveness to prevent long-term cycles of mutual defection. It adapts its behavior based on the observed cooperation levels and the game's history.

### Core Principles:

1.  **Initial Cooperation:** Start by signaling a willingness to cooperate. This is crucial to initiate the potential for collective benefit.
2.  **Conditional Cooperation:** Cooperate if the conditions suggest that cooperation is likely to be rewarded. Specifically, if enough players have cooperated in the past to make reaching the threshold viable.
3.  **Punishment for Defection:** Defect if previous rounds have demonstrated a lack of cooperation from the group, making individual defection the more rational choice for self-preservation.
4.  **Forgiveness:** Do not hold grudges indefinitely. If a period of defection occurs, but then signals of renewed cooperation appear, be willing to return to cooperation.
5.  **Parameter Awareness:** The strategy explicitly uses `n`, `m`, and `k` to inform its decisions.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in the current round will depend on the player's belief about the group's propensity to cooperate, influenced by past observations.

**Key Metrics to Track:**

*   `cooperators_last_round`: The number of players who cooperated in the immediately preceding round.
*   `cooperators_average_past`: The average number of cooperators across all previous rounds.
*   `recent_defection_streak`: A counter for consecutive rounds where the player defected due to insufficient group cooperation. This represents a "punishment" phase.

**Decision Logic (for round `t > 1`):**

1.  **If `t == 1` (First Round):**
    *   **Action:** Cooperate (C)
    *   **Rationale:** To signal good faith and attempt to initiate a cooperative equilibrium. This is the default starting point for ARS.

2.  **If `t > 1`:**
    *   **Scenario A: Sufficient recent cooperation observed.**
        *   **Condition:** If `cooperators_last_round >= m` OR (`cooperators_average_past >= m - 1` AND `recent_defection_streak < forgiveness_threshold`)
        *   **Action:** Cooperate (C)
        *   **Rationale:** The group has demonstrated its ability or willingness to meet the cooperation threshold. It's beneficial to continue cooperating to secure the reward. The `recent_defection_streak` condition adds a layer of forgiveness, allowing a return to cooperation even if the average is slightly below `m` if the current trend is positive and the player hasn't been defecting for too long.

    *   **Scenario B: Insufficient recent cooperation observed and no signs of recovery.**
        *   **Condition:** If `cooperators_last_round < m` AND (`cooperators_average_past < m - 1` OR `recent_defection_streak >= forgiveness_threshold`)
        *   **Action:** Defect (D)
        *   **Rationale:** The group is not consistently cooperating, and the past average is also low. Continuing to cooperate would likely lead to a lower payoff (1 + 0 = 1) compared to defecting (1 + 0 = 1 for defectors, potentially 0+0=0 for cooperators if threshold is not met). This is the "punishment" phase. Increment `recent_defection_streak`.

    *   **Scenario C: Acknowledging a shift after punishment.**
        *   **Condition:** If `cooperators_last_round >= m` AND `recent_defection_streak > 0` (meaning we were in a punishment phase but now the group has met the threshold)
        *   **Action:** Cooperate (C)
        *   **Rationale:** The group has successfully reached the cooperation threshold, indicating a potential shift. We return to cooperating, resetting `recent_defection_streak` to 0. This is the forgiveness aspect.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):** As stated above, always Cooperate (C). This is the initial signal of good intent.
*   **Last Round (`t = r`):** The strategy remains the same as the logic for `t > 1`. However, the player's objective in the last round is purely to maximize their *own* final payoff. The ARS will naturally do this based on the observed cooperation in round `r-1`. If `cooperators_last_round >= m` in round `r-1`, the ARS will cooperate in round `r`. If not, it will defect in round `r`. This is a rational consequence of the repeated game ending.
*   **Low `k` values (close to 1):** If `k` is only slightly greater than 1, the reward for cooperation is small. The ARS might become more risk-averse and defect more readily if cooperation isn't guaranteed. The current logic handles this indirectly: a low `k` makes the payoff difference between the cooperative and non-cooperative outcome smaller, thus making defection more tempting if there's any doubt.
*   **High `k` values:** A high `k` significantly amplifies the benefit of cooperation. The ARS will be more inclined to trust and cooperate if there's a chance of reaching the threshold, as the reward is substantial.

**Parameter for Forgiveness:**

*   `forgiveness_threshold`: This is a crucial parameter for the ARS. It determines how many consecutive rounds the ARS will defect before it becomes harder to return to cooperation, even if there are some positive signals. A good starting point might be `floor(n / m)` or `r / 2`. Let's set it as a conceptual parameter that can be tuned. For example, `forgiveness_threshold = 3` rounds.

### 3. Being Cooperative

The ARS is fundamentally cooperative:

*   **Initiation:** It always starts by attempting to cooperate, aiming to build trust and unlock collective benefits.
*   **Responsiveness:** It rewards cooperation by continuing to cooperate, thereby reinforcing the cooperative norm.
*   **Non-Aggression:** It only defects as a response to observed group behavior, not proactively. It's a defensive measure to prevent being exploited.
*   **Reconciliation:** It is designed to return to cooperation once the group demonstrates a renewed commitment to the cooperative goal. It doesn't seek to punish indefinitely.

### Pseudocode Representation:

```pseudocode
// Game Parameters
n // number of players
m // minimum cooperators needed
k // reward if threshold met factor
r // number of rounds

// Strategy State Variables (for each player using this strategy)
current_round = 1
cooperators_last_round = 0
cooperators_history = [] // list of counts for each past round
recent_defection_streak = 0
forgiveness_threshold = 3 // Example value, can be tuned

// Decision Function for player i in round t
FUNCTION DecideAction(t, actions_of_others_in_previous_round_if_available):
    IF t == 1 THEN
        // First round: Always cooperate to signal intent
        RETURN "C"
    ELSE
        // Calculate average cooperators from history
        IF length(cooperators_history) > 0 THEN
            cooperators_average_past = average(cooperators_history)
        ELSE
            cooperators_average_past = 0 // Should not happen if t > 1

        // Decision Logic
        IF cooperators_last_round >= m THEN
            // Group successfully met threshold last round - cooperate and reset streak
            recent_defection_streak = 0
            RETURN "C"
        ELSE IF cooperators_average_past >= m - 1 AND recent_defection_streak < forgiveness_threshold THEN
            // Group is close to threshold or has a positive recent trend and we are not in a deep punishment phase
            RETURN "C"
        ELSE IF cooperators_last_round >= m AND recent_defection_streak > 0 THEN
            // Returned to cooperation after a defection streak
            recent_defection_streak = 0
            RETURN "C"
        ELSE
            // Group is not cooperating enough - defect and increment streak
            recent_defection_streak = recent_defection_streak + 1
            RETURN "D"
        END IF
    END IF
END FUNCTION

// Update State after each round (called by the tournament system)
PROCEDURE UpdateState(t, my_action, actions_of_all_players_in_round_t):
    cooperators_last_round = count_cooperators(actions_of_all_players_in_round_t)
    cooperators_history.append(cooperators_last_round)
    current_round = t + 1
END PROCEDURE

// Helper function to count cooperators
FUNCTION count_cooperators(actions_list):
    count = 0
    FOR EACH action IN actions_list:
        IF action == "C" THEN
            count = count + 1
        END IF
    END FOR
    RETURN count
END FUNCTION
```

**Adaptability and Robustness:**

*   **Adaptive:** The strategy directly reacts to the observed behavior of other players. It doesn't rely on fixed patterns. The `cooperators_average_past` and `cooperators_last_round` provide dynamic feedback.
*   **Robust:**
    *   **To low cooperators:** If the group consistently defects, the ARS will also defect, minimizing losses.
    *   **To high cooperators:** If the group consistently cooperates, the ARS will also cooperate, maximizing gains.
    *   **To fluctuating behavior:** The `forgiveness_threshold` allows the strategy to recover from periods of defection if the group shows signs of returning to cooperation. This makes it robust to opponents who might oscillate in their behavior.
    *   **To single defectors:** The ARS focuses on the aggregate number of cooperators, making it less susceptible to being thrown off by a single rogue player.

This strategy offers a good balance between pursuing cooperation and protecting against exploitation, making it a strong contender in a repeated game with imperfect coordination.
'''

description_COOPERATIVE_8 = '''
This is a fascinating collective risk dilemma with the added complexity of repeated interactions and the absence of communication. The core challenge is to incentivize cooperation without being exploited by defectors. My proposed cooperative strategy aims to achieve this by being adaptive, conditional, and focused on long-term collective benefit.

## Cooperative Strategy: "Reciprocal Escalation and De-escalation"

This strategy is based on the principle of reciprocity, but with a twist to handle the threshold-based reward. It aims to signal a commitment to cooperation while also protecting against exploitation. The core idea is to start cooperatively, observe the collective behavior, and adjust cooperation levels accordingly, aiming to reach the cooperation threshold whenever possible.

### 1. Decision Rules

**Core Logic:**
My strategy will primarily cooperate, but will only defect if it becomes clear that cooperation is not sustainable due to a lack of sufficient collective effort, or if it's strategically necessary to punish defection and encourage future cooperation.

**Specific Rules:**

*   **Initial Round (Round 1):**
    *   **Cooperate (C).** This is the default starting point for a cooperative strategy. It signals a willingness to contribute to the collective good and aims to trigger the reward.

*   **Subsequent Rounds (t > 1):**
    *   **Track Collective Cooperation:** For each round `t-1`, I will count the total number of players who cooperated. Let this be `C_(t-1)`.
    *   **Assess Past Round Outcome:**
        *   **If `C_(t-1) >= m` (Threshold Met):**
            *   **Cooperate (C).** This indicates that the collective effort was sufficient to achieve the reward. Continuing to cooperate reinforces this successful outcome and maximizes future potential rewards.
        *   **If `C_(t-1) < m` (Threshold Not Met):**
            *   **Analyze the "Gap":** How far were we from the threshold? `Gap = m - C_(t-1)`.
            *   **Analyze Your Own Past Action:** Did I cooperate or defect in round `t-1`? Let this be `Action_(t-1)`.
            *   **Decision Logic:**
                *   **If `Action_(t-1) == C` (I cooperated last round):**
                    *   **Cooperate (C).** This means I contributed to the effort, and the collective still failed. I will continue to cooperate for one more round, hoping others will also increase their cooperation to meet the threshold. This represents a "second chance" for the group.
                *   **If `Action_(t-1) == D` (I defected last round):**
                    *   **This scenario should ideally not happen if the strategy is consistently cooperative. However, if for some reason it did (e.g., an earlier, less robust version of this strategy, or a response to extreme past exploitation), we would need a fallback.** For this strategy, assuming it started cooperatively and is generally cooperative, this means we are dealing with a situation where I defected in the *previous* round because the threshold *was not met*, and I *still* defected. This is unlikely for this strategy's design. Let's refine the logic for the case where the threshold was *not met*.

    *   **Revised Logic for `C_(t-1) < m`:**
        *   **If `Action_(t-1) == C` (I cooperated last round):**
            *   **Cooperate (C).** This shows I'm willing to bear the cost if others are also trying.
        *   **If `Action_(t-1) == D` (I defected last round because threshold was not met):**
            *   **Check for Escalation Opportunity:** Is my cooperation now *needed* to cross the threshold? If `C_(t-1) + 1 == m`, meaning my cooperation would exactly meet the threshold, then I should **Cooperate (C)**. This is a strategic cooperation to *achieve* the reward.
            *   **Otherwise (If my cooperation alone wouldn't meet the threshold):**
                *   **Defect (D).** If the threshold wasn't met, and I defected last round (because it wasn't met), and my cooperation *alone* still wouldn't get us there, then it's not strategically beneficial for me to cooperate. Others are not stepping up enough, and I'm bearing the cost without the reward.

*   **Last Round (Round r):**
    *   The decision logic remains the same as for subsequent rounds. However, the long-term implications of defection are less relevant. The primary goal is to maximize my own payoff in this final round.
    *   **Revised Last Round Logic:**
        *   **Calculate Expected Payoff:** For each possible collective action (based on my choice C or D and expected actions of others), I can calculate my payoff.
        *   **If `C_(t-1) >= m`:**
            *   **Cooperate (C).** The reward is guaranteed from the previous round's success.
        *   **If `C_(t-1) < m`:**
            *   **If `Action_(t-1) == C`:**
                *   **Cooperate (C).** I've shown willingness, and it's the last chance to get the reward for everyone.
            *   **If `Action_(t-1) == D`:**
                *   **If `C_(t-1) + 1 == m`:**
                    *   **Cooperate (C).** This is the only way to get the reward for everyone in the final round.
                *   **Otherwise (If `C_(t-1) + 1 < m`):**
                    *   **Defect (D).** If the threshold cannot be met even with my cooperation, and I've already defected, there's no benefit to cooperating.

**Summary of Decision Rules (Simplified for Clarity):**

1.  **Round 1:** Cooperate (C).
2.  **For Rounds t > 1 and t < r:**
    *   If threshold `m` was met in round `t-1` (i.e., `C_(t-1) >= m`), **Cooperate (C)**.
    *   If threshold `m` was NOT met in round `t-1` (`C_(t-1) < m`):
        *   If I cooperated in round `t-1` (`Action_(t-1) == C`), **Cooperate (C)**.
        *   If I defected in round `t-1` (`Action_(t-1) == D`):
            *   If my cooperation would now meet the threshold (i.e., `C_(t-1) + 1 == m`), **Cooperate (C)**.
            *   Otherwise, **Defect (D)**.
3.  **Round r (Last Round):**
    *   If threshold `m` was met in round `t-1` (`C_(t-1) >= m`), **Cooperate (C)**.
    *   If threshold `m` was NOT met in round `t-1` (`C_(t-1) < m`):
        *   If I cooperated in round `t-1` (`Action_(t-1) == C`), **Cooperate (C)**.
        *   If I defected in round `t-1` (`Action_(t-1) == D`):
            *   If my cooperation would now meet the threshold (i.e., `C_(t-1) + 1 == m`), **Cooperate (C)**.
            *   Otherwise, **Defect (D)**.

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated, the strategy always starts by cooperating to signal good intent and to try and trigger the reward immediately. This is crucial for establishing a cooperative norm.
*   **Last Round (t=r):** The logic adapts slightly as outlined above. While the cooperative spirit remains, the focus shifts to maximizing individual payoff since there are no future rounds to influence. However, the strategy still attempts to achieve the reward if it's still possible and beneficial (i.e., my cooperation would tip the balance).
*   **Low `m` values (e.g., `m=2`, `m=3`):** The strategy will be more likely to achieve the threshold, and thus more likely to continue cooperating. This is good.
*   **High `m` values (close to `n`):** The threshold is harder to meet. The strategy will experience more rounds where `C_(t-1) < m`. The adaptive nature will then come into play, responding to others' behavior. If others consistently fail to cooperate, the strategy will eventually defect to avoid consistent losses.
*   **When `m = n`:** This is a special case where `m` cannot be strictly less than `n` according to the parameter definition (`1 < m < n`). If `m` were `n`, it would require everyone to cooperate. The strategy would cooperate, and if anyone defects, it would be left with a payoff of 1. The strategy would likely defect in the next round if it saw the threshold wouldn't be met.
*   **When `m = 2`:** This is the smallest possible threshold (given `m > 1`). The strategy will heavily favor cooperation, as it only takes one other player to cooperate with me to meet the threshold.

### 3. Being Cooperative

This strategy is fundamentally cooperative for several reasons:

*   **Default to Cooperation:** It always starts by cooperating.
*   **Proactive Reward Seeking:** It actively tries to achieve the cooperation threshold to generate the collective reward (`k`), benefiting all participants.
*   **Conditional Cooperation, Not Punishment-Driven:** While it can defect, it doesn't do so purely out of spite. Defection is a response to the *lack* of successful collective cooperation, particularly when the strategy's own cooperation is insufficient to tip the scales. It aims to avoid being a "sucker."
*   **"Second Chance" Mechanism:** When the threshold isn't met, and the strategy *did* cooperate, it gives the group one more chance by cooperating again. This shows a commitment to the cooperative ideal, even when facing initial failure.
*   **Strategic Cooperation to Achieve Threshold:** The rule where "if my cooperation alone would meet the threshold, I cooperate" is a crucial aspect of cooperative strategy. It's a calculated move to *enable* the collective benefit when it's within reach.

### Pseudocode

```
// Strategy: Reciprocal Escalation and De-escalation

// State variables for player i:
// my_last_action: The action player i took in the previous round (C or D)
// total_cooperators_last_round: The count of players who cooperated in the previous round

function decide_action(round_number, n, m, k, history):
    // history is a list of dictionaries, where each dictionary represents a round
    // and contains 'actions': a list of actions taken by all players {player_id: action}

    if round_number == 1:
        my_last_action = C
        return C // Always cooperate in the first round

    // Get information from the last round
    last_round_info = history[round_number - 2] // history is 0-indexed
    actions_last_round = last_round_info['actions']

    total_cooperators_last_round = 0
    for player_action in actions_last_round.values():
        if player_action == C:
            total_cooperators_last_round += 1

    my_action_last_round = actions_last_round[my_player_id] // Assume my_player_id is available

    // Decision Logic for rounds > 1
    if total_cooperators_last_round >= m:
        // Threshold was met, continue cooperating to maintain the reward
        my_last_action = C
        return C
    else:
        // Threshold was not met
        if my_action_last_round == C:
            // I cooperated, and the threshold wasn't met. Give it one more chance.
            my_last_action = C
            return C
        else: // my_action_last_round == D
            // I defected. Will my cooperation now meet the threshold?
            if total_cooperators_last_round + 1 == m:
                // Yes, my cooperation would exactly meet the threshold. Cooperate strategically.
                my_last_action = C
                return C
            else:
                // No, my cooperation wouldn't be enough. Defect.
                my_last_action = D
                return D

// Note: The 'my_player_id' would need to be known by the strategy.
// 'history' is assumed to be structured as a list of rounds,
// and each round's actions are a dictionary mapping player IDs to their actions.
// For the last round (round_number == r), the logic follows the same rules.
```

This strategy is designed to be robust because it doesn't rely on specific patterns of play from others. It reacts to the aggregate outcome and its own previous contribution, aiming to balance the desire for collective reward with the need for self-preservation against consistent defection. It is adaptive because it adjusts its behavior based on the observed success or failure of the group in meeting the cooperation threshold.
'''

description_COOPERATIVE_9 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, focusing on adaptability and robustness without relying on pre-existing communication or coordination:

## Cooperative Risk-Averse Strategy (CRAS)

The core idea behind CRAS is to be cooperatively inclined but pragmatically cautious. It aims to achieve the collective reward by contributing to the community project, but it won't blindly cooperate if the risk of being the sole cooperator (or one of very few) is too high. It learns from past rounds to adjust its risk tolerance.

### 1. Decision Rules

CRAS's decision in each round is based on two primary factors: the **cooperation threshold (m)** and the **historical success rate of cooperation**.

**General Rule:**

*   **Cooperate (C) if:** The perceived probability of reaching the cooperation threshold (m) in the current round is sufficiently high, and the cost of being a lone cooperator is manageable.
*   **Defect (D) if:** The perceived probability of reaching the cooperation threshold (m) is low, or if past experiences suggest that cooperation is consistently punished.

**Detailed Decision Logic:**

Let's define a few key variables that CRAS will track:

*   `cooperation_count_history`: A list storing the number of cooperators in each previous round.
*   `my_cooperation_history`: A list storing whether player i cooperated (1) or defected (0) in each previous round.
*   `successful_cooperation_rounds`: A counter for rounds where the threshold `m` was met or exceeded.
*   `total_cooperation_rounds`: A counter for all rounds played.
*   `risk_tolerance`: A dynamic value that influences the decision to cooperate. This will be adjusted based on past outcomes.

**Decision Rule for Round `t` (where `t > 1`):**

1.  **Assess Historical Cooperation Success:** Calculate the proportion of past rounds where the cooperation threshold `m` was met.
    `historical_success_rate = successful_cooperation_rounds / total_cooperation_rounds`

2.  **Estimate Current Round Cooperation:** This is the trickiest part without communication. CRAS will estimate the likelihood of others cooperating by looking at their past behavior *relative to its own actions*.
    *   **If CRAS cooperated in the previous round:** Observe how many others cooperated *when CRAS cooperated*.
    *   **If CRAS defected in the previous round:** Observe how many others defected *when CRAS defected*.

    Let's simplify this for now by assuming a basic estimation: **If cooperation has been successful in the past (high `historical_success_rate`), CRAS will be more inclined to cooperate.**

3.  **Dynamic Risk Tolerance Adjustment:**
    *   **If `historical_success_rate` is high and CRAS's past cooperations were generally rewarded (i.e., `m` was met when it cooperated):** Increase `risk_tolerance`.
    *   **If `historical_success_rate` is low, or CRAS's past cooperations were often met with few other cooperators (leading to a low individual payoff):** Decrease `risk_tolerance`.

    A simple way to implement `risk_tolerance` is to make it a multiplier that influences the perceived "cost" of defecting. A higher `risk_tolerance` means CRAS is more willing to risk cooperating.

4.  **Cooperation Threshold Calculation (for current round):**
    CRAS will aim to contribute if it believes there's a reasonable chance of reaching `m`. This probability is informed by `historical_success_rate` and `risk_tolerance`.

    Let's introduce a **"cooperation incentive threshold"**: `cooperation_incentive = m / n`. If CRAS believes the *average* cooperation rate in the current round will be `cooperation_incentive` or higher, it will lean towards cooperating.

5.  **The Decision:**

    *   **If `historical_success_rate` is above a certain baseline (e.g., 0.5) AND `risk_tolerance` is sufficiently high:**
        *   **Consider cooperating.** If the estimated number of cooperators based on past trends (including CRAS's own past cooperation) suggests a high likelihood of reaching `m`, then **Cooperate (C)**.
    *   **Otherwise:**
        *   **Defect (D)**. This is the safer option when uncertainty is high or past cooperation has been punished.

**Refined Decision Logic for estimating current cooperation:**

Instead of a vague "estimated number of cooperators," CRAS can employ a more concrete rule:

*   **If `historical_success_rate` is high:**
    *   **If CRAS cooperated in the previous round and `m` was met or exceeded:** **Cooperate (C)**. This is a strong signal that cooperation is working.
    *   **If CRAS cooperated in the previous round but `m` was *not* met:** **Cooperate (C)** if the number of *other* cooperators was at least `m-1`. This means CRAS was "close" and can try again. If fewer than `m-1` others cooperated, then **Defect (D)**, as the risk of being alone is too high.
    *   **If CRAS defected in the previous round:** Look at the average number of cooperators in rounds where CRAS defected. If this average is close to `m`, **Cooperate (C)**. If it's significantly less than `m`, **Defect (D)**.

*   **If `historical_success_rate` is low:**
    *   **If CRAS cooperated in the previous round and `m` was not met:** **Defect (D)**. This signals a need for caution.
    *   **If CRAS defected in the previous round:** **Defect (D)**. Maintain the status quo of safety.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision Rule:** Since there's no history, CRAS must adopt a baseline strategy. To be cooperative, it will initially **Cooperate (C)**. This is a gamble to kickstart cooperation, but it's a calculated one. The rationale is that the first round sets a precedent, and the reward for collective success (`k`) is significant. If everyone defects, the payoff is only 1. If even a few cooperate, there's a chance for a higher payoff.
    *   **Rationale for Cooperating:** This is the most "cooperative" move in the absence of information. It signals a willingness to contribute.

*   **Last Round (t=r):**
    *   **Decision Rule:** In the final round, there's no incentive for future cooperation. The immediate payoff is the only concern. CRAS will analyze the history to determine the most likely outcome for *itself*.
        *   If `m` or more players are *very likely* to cooperate (based on overwhelming historical data of successful cooperation), then **Cooperate (C)** to secure the `k` reward.
        *   Otherwise, **Defect (D)** to guarantee its private endowment of 1.
    *   **Rationale:** In the absence of future rounds, the forward-looking reciprocity aspect of cooperation disappears. The decision becomes purely about maximizing individual payoff in that single round. CRAS will err on the side of caution (defecting) if there's any doubt about reaching the threshold.

*   **When `m` is very close to `n` (e.g., `m = n-1`):**
    *   CRAS becomes significantly more cautious. It will only cooperate if it is highly confident that at least `m` players will also cooperate. This might mean requiring a very high `historical_success_rate` and observing a strong trend of cooperation from other players in previous rounds.

*   **When `m` is close to 2 (i.e., `m=2`):**
    *   CRAS is more willing to take a risk. It will cooperate more readily if it believes there's a reasonable chance of finding just one other cooperator. The `historical_success_rate` threshold for cooperating will be lower.

### 3. Being Cooperative

CRAS embodies a cooperative mindset through its core principles:

*   **Proactive Cooperation:** In the absence of information (first round), it chooses to cooperate, signaling a willingness to contribute to the collective good.
*   **Learning and Adaptation:** It learns from past outcomes. If cooperation leads to success (`m` reached), it reinforces its cooperative tendency. If it leads to punishment (low individual payoff due to insufficient cooperators), it becomes more cautious but not necessarily spiteful.
*   **Goal Alignment:** The ultimate goal is to reach the cooperation threshold `m` to unlock the collective reward `k`. CRAS's decisions are always geared towards this shared objective, while balancing individual risk.
*   **Fairness:** While not explicitly programmed with fairness metrics, by aiming for collective success and adapting its risk based on observed outcomes, it indirectly promotes a scenario where everyone can benefit. It avoids being a "free rider" by attempting to cooperate when conditions are favorable.
*   **Resilience:** CRAS is designed to be robust to a range of opponent behaviors. If others are consistently defecting, CRAS will eventually reduce its own cooperation to avoid consistently suffering losses. If others are cooperating, CRAS will reciprocate and increase its cooperation to achieve the group benefit.

### Pseudocode for CRAS

```pseudocode
// Global variables to be maintained across rounds
global cooperation_count_history = [] // List of number of cooperators in each round
global my_cooperation_history = []    // List of player's actions (1 for C, 0 for D)
global successful_cooperation_rounds = 0
global total_cooperation_rounds = 0
global risk_tolerance = 0.5          // Initial risk tolerance (can be tuned)

// Parameters
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

function decide_action(current_round):
    // Handle first round
    if current_round == 1:
        return "C" // Cooperate in the first round to set a positive precedent

    // Handle last round
    if current_round == r:
        // Analyze history for last-round decision
        // This is a simplified logic: if historical success is very high, cooperate
        if total_cooperation_rounds > 0 and (successful_cooperation_rounds / total_cooperation_rounds) > 0.8:
            return "C"
        else:
            return "D"

    // --- Adaptive Decision Logic for intermediate rounds ---

    // Calculate historical success rate
    historical_success_rate = 0
    if total_cooperation_rounds > 0:
        historical_success_rate = successful_cooperation_rounds / total_cooperation_rounds

    // Estimate likelihood of reaching threshold 'm'
    estimated_cooperation_needed = m
    if historical_success_rate > 0.6: // If cooperation has been generally successful
        // Look at how many others cooperated when I cooperated in past successful rounds
        // (This requires more detailed history tracking of specific player actions,
        // but for simplicity, we use historical_success_rate as a proxy for others'
        // tendency to cooperate)
        if my_cooperation_history[-1] == 1: // If I cooperated last round
            // If last round was successful, assume others will continue
            if cooperation_count_history[-1] >= m:
                estimated_cooperation_needed = max(0, m - 1) // Need one less
            else: // Last round was not successful
                // If at least m-1 others cooperated, I should try again
                if cooperation_count_history[-1] >= m - 1:
                    estimated_cooperation_needed = max(0, m - 1)
                else:
                    estimated_cooperation_needed = m // Assume I'll need to contribute fully
        else: // I defected last round
            // If historical success is high, consider cooperating
            estimated_cooperation_needed = max(0, m - 1)

    elif historical_success_rate < 0.4: // If cooperation has been generally unsuccessful
        // Reduce risk tolerance, become more cautious
        risk_tolerance = max(0.1, risk_tolerance * 0.8) // Decrease risk tolerance
        if my_cooperation_history[-1] == 1: // If I cooperated and it failed
            return "D" // Defect to avoid further loss
        else: // I defected and it still failed
            return "D" // Stay safe

    else: // Moderate historical success
        risk_tolerance = min(0.9, risk_tolerance * 1.1) // Slightly increase risk tolerance

    // Decision based on estimated cooperation needed and risk tolerance
    // If the remaining cooperators needed is low (due to high historical success or my previous cooperation)
    // AND my risk tolerance is high enough, then cooperate.
    if estimated_cooperation_needed <= 1 and risk_tolerance > 0.6:
        return "C"
    else:
        return "D"

// --- To be called after each round ---
function update_history(my_action, other_players_actions):
    total_cooperation_rounds += 1
    my_cooperation_history.append(my_action)

    current_cooperators = 0
    if my_action == "C":
        current_cooperators += 1
    for action in other_players_actions:
        if action == "C":
            current_cooperators += 1

    cooperation_count_history.append(current_cooperators)

    if current_cooperators >= m:
        successful_cooperation_rounds += 1

```
'''

description_COOPERATIVE_10 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust, relying solely on game parameters and observed history.

## Cooperative Strategy: "Adaptive Threshold Maintainer"

This strategy aims to achieve collective cooperation by actively contributing to the "community project" as long as the observed cooperation level is sufficiently high and seems to be sustainably met. It prioritizes avoiding the worst-case scenario (everyone defecting) while trying to secure the collective reward.

### Core Principle:

**Cooperate if the current cooperation level (as a fraction of players) is at or above a dynamically adjusted target threshold. Otherwise, defect.**

The target threshold will adapt based on past performance, specifically aiming to prevent the game from falling below the minimum cooperation requirement `m`.

### 1. Decision Rules:

**For each round `t` (from 1 to `r`):**

**Calculate the observed cooperation percentage in the previous round `t-1`:**

*   Let `C_{t-1}` be the number of players who cooperated in round `t-1`.
*   Let `ObservedCoopPct_{t-1} = C_{t-1} / n`.

**Determine the current round's cooperation target threshold `TargetCoopPct_t`:**

*   **Initialization (Round 1):** `TargetCoopPct_1` is set to a slightly optimistic but safe value. A good starting point is `m/n`. This signals an intent to cooperate if the minimum is met.
*   **Adaptation (Round t > 1):**
    *   If `ObservedCoopPct_{t-1} >= m/n` (the minimum cooperation was met or exceeded in the previous round):
        *   The strategy aims to maintain or slightly increase cooperation.
        *   `TargetCoopPct_t = max(m/n, TargetCoopPct_{t-1} * 0.95)`  (Slightly reduce the target to encourage more players to join in, but don't go below the absolute minimum. The 0.95 factor is a learning rate, encouraging convergence.)
    *   If `ObservedCoopPct_{t-1} < m/n` (the minimum cooperation was NOT met in the previous round):
        *   The strategy must be more cautious to avoid repeated failure.
        *   `TargetCoopPct_t = max(m/n, TargetCoopPct_{t-1} * 1.10)` (Increase the target threshold to demand a higher level of observed cooperation before contributing. The 1.10 factor is an increasing rate to signal that cooperation is needed.)

**My Decision for Round `t`:**

*   If `ObservedCoopPct_{t-1} >= TargetCoopPct_t`: **Cooperate (Play C)**
*   If `ObservedCoopPct_{t-1} < TargetCoopPct_t`: **Defect (Play D)**

**Explanation of the Adaptation Logic:**

*   When cooperation is successful (`ObservedCoopPct_{t-1} >= m/n`), we slightly *lower* the target threshold. This is because if the collective managed to meet `m`, it means the current observed cooperation level is likely sufficient. Lowering the target slightly encourages *us* to cooperate if the observed level is a bit lower than what we *aimed* for last round but still above the absolute minimum, and importantly, it makes it easier for *other* players to see our cooperation as a signal to join in. We are essentially saying, "We contributed, and it worked; let's keep this going, even if the observed cooperation dips a little from the absolute peak." The `max(m/n, ...)` ensures we never ask for less than the absolute requirement to trigger the reward.
*   When cooperation fails (`ObservedCoopPct_{t-1} < m/n`), we *increase* the target threshold. This is a defensive move. If the collective couldn't reach `m` last round, it means the observed cooperation level was too low. We will only rejoin cooperation if we see a *higher* level of cooperation from others in the next round. This is a signal that we require more confidence from the group before taking on the risk ourselves. The `max(m/n, ...)` here ensures we are still aiming for a state where `m` *can* be achieved.

### 2. Handling Edge Cases:

*   **Round 1:**
    *   In the first round, there is no history.
    *   **Decision Rule:** **Cooperate (Play C)**.
    *   **Rationale:** To signal a cooperative intent from the outset and attempt to kickstart cooperation. This is the most cooperative action and aligns with the goal of achieving the collective reward. If `m=n`, this is the only way to trigger the reward. Even if `m<n`, initiating cooperation is a strong signal.
    *   **Initialization:** `TargetCoopPct_1 = m/n`.

*   **Last Round (`r`):**
    *   The strategy treats the last round the same as any other round based on the *final* observed cooperation history. There is no future to consider for reciprocal behavior.
    *   **Decision Rule:** Apply the standard decision rules based on `ObservedCoopPct_{r-1}` and `TargetCoopPct_r`.
    *   **Rationale:** The adaptation mechanism is based on past performance. Since the game ends after round `r`, the decision in round `r` is purely reactive to what happened in round `r-1`.

*   **`m = n`:**
    *   If `m=n`, cooperation is only rewarded if *all* players cooperate.
    *   **Decision Rule:** The strategy will cooperate as long as the observed cooperation rate in the previous round is 100% (`ObservedCoopPct_{t-1} = 1`). If it drops below 100%, the `TargetCoopPct_t` will increase, and the strategy will defect until it sees 100% cooperation again.
    *   **Rationale:** This is a strict reciprocity. The strategy cooperates only when it sees full cooperation from others, incentivizing everyone to reach the 100% mark.

*   **`m` close to `n`:**
    *   When `m` is very close to `n`, the game becomes more precarious. The strategy's adaptation will become more sensitive. If cooperation slightly falters, it will be slower to rejoin.
    *   **Rationale:** The strategy's increased `TargetCoopPct_t` when cooperation fails is more pronounced and has a greater impact when `m/n` is high.

### 3. Cooperative Mindset:

This strategy is inherently cooperative because:

*   **Proactive Cooperation:** It starts by cooperating in the first round, setting a cooperative tone.
*   **Reward Seeking:** It actively tries to achieve the state where `m` or more players cooperate, aiming for the collective reward `k`.
*   **Adaptive Reciprocity:** It cooperates when it observes sufficient cooperation from others and scales back only when the group fails to meet the threshold, not out of spite, but as a strategic adjustment to encourage a return to successful cooperation.
*   **Risk Aversion (for the group):** By aiming to meet `m`, it seeks to avoid the worst-case scenario where no one cooperates and everyone receives only `1`.
*   **Parameter-Driven:** Its decisions are solely based on the game's rules and observed history, not on assumptions about specific opponents' predictable patterns or fixed strategies. This makes it robust.

### Pseudocode:

```
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Strategy State
my_cooperation_history = [] // List of my actions (C or D)
opponent_cooperation_counts_history = [] // List of counts of opponent cooperations in each round

// Strategy Functions
function decide_action(round_number, all_previous_actions):
  // all_previous_actions is a list of lists, where each inner list is the actions of all players in a round.
  // Example: [[C, D, C], [C, C, C]] for 3 players and 2 previous rounds.

  // --- Handle Round 1 ---
  if round_number == 1:
    // In the first round, always cooperate to signal intent.
    return 'C'

  // --- Calculate Observed Cooperation from Previous Round ---
  previous_round_actions = all_previous_actions[round_number - 2] // 0-indexed for history
  cooperators_in_previous_round = count_cooperators(previous_round_actions)
  observed_coop_pct_prev = cooperators_in_previous_round / n

  // --- Determine Target Cooperation Threshold ---
  // Initialize target_coop_pct for the current round based on the previous round's performance and the current target.
  // We need to store the target_coop_pct_t for each round for adaptation.
  // For simplicity, let's assume we have a global variable or pass it around.
  // Let's say current_target_coop_pct holds the target for THIS round.

  if round_number == 2: // First adaptation round
      target_coop_pct = m / n // Initial target
  else:
      // Re-calculate target_coop_pct based on previous round's actual outcome and previous target.
      // This requires storing the target_coop_pct used in the *previous* round.
      // Let's refine: We need a variable for the target threshold that *was used* in the previous round.
      // Let's assume 'previous_target_coop_pct' is available.

      if observed_coop_pct_prev >= m / n:
          // Met or exceeded minimum, slightly reduce target to encourage continuity.
          // Ensure we don't drop below the absolute minimum requirement.
          target_coop_pct = max(m / n, previous_target_coop_pct * 0.95)
      else:
          // Did not meet minimum, increase target to signal need for more cooperation.
          target_coop_pct = max(m / n, previous_target_coop_pct * 1.10)

  // Store the target_coop_pct for the *current* round so it can be used for adaptation in the *next* round.
  // In a real implementation, this would be stored in a persistent strategy state.
  current_target_coop_pct = target_coop_pct

  // --- Make Decision ---
  if observed_coop_pct_prev >= current_target_coop_pct:
    return 'C'
  else:
    return 'D'

// Helper function (assumes 'C' means cooperate)
function count_cooperators(actions_list):
  count = 0
  for action in actions_list:
    if action == 'C':
      count += 1
  return count

// --- Strategy State Management ---
// The strategy needs to maintain its own target_coop_pct over rounds.
// Let's say 'strategy_target_coop_pct' is a persistent variable.

function initialize_strategy_state():
  strategy_target_coop_pct = m / n // For round 1

function get_action(round_number, all_previous_actions, game_parameters):
  n = game_parameters['n']
  m = game_parameters['m']

  if round_number == 1:
    // Round 1: Always cooperate.
    // Initialize strategy state for future rounds.
    // The target for round 2 will be m/n.
    strategy_target_coop_pct = m / n
    return 'C'

  // Get actions from the previous round (r-1). History is 0-indexed.
  // all_previous_actions contains actions for rounds 1 to round_number - 1.
  // So, the last element is for round_number - 1.
  previous_round_actions = all_previous_actions[-1]
  cooperators_in_previous_round = count_cooperators(previous_round_actions)
  observed_coop_pct_prev = cooperators_in_previous_round / n

  // Determine the target threshold for the *current* round based on the *previous* round's outcome
  // and the target threshold that was *active* in the previous round.
  current_round_target_coop_pct = 0
  if observed_coop_pct_prev >= m / n:
    // Met or exceeded minimum, slightly reduce target.
    current_round_target_coop_pct = max(m / n, strategy_target_coop_pct * 0.95)
  else:
    // Did not meet minimum, increase target.
    current_round_target_coop_pct = max(m / n, strategy_target_coop_pct * 1.10)

  // Update the strategy's persistent target for the *next* round's calculation.
  strategy_target_coop_pct = current_round_target_coop_pct

  // Make the decision for the current round.
  if observed_coop_pct_prev >= current_round_target_coop_pct:
    return 'C'
  else:
    return 'D'
```
'''

description_COOPERATIVE_11 = '''
## Cooperative Strategy: "Adaptive Threshold Cooperation"

This strategy aims to be maximally cooperative while remaining robust to potential defectors. It leverages the repeated nature of the game and the observed history to make informed decisions. The core idea is to *conditionally cooperate*, starting with a cooperative stance and adjusting based on the collective behavior of the group.

### 1. Decision Rules

The strategy's decision in any given round (except the first) is based on the *observed cooperation rate* in the previous round. Let $C_{t-1}$ be the number of players who cooperated in round $t-1$, and $N$ be the total number of players $n$. The observed cooperation rate is $\frac{C_{t-1}}{N}$.

*   **If the observed cooperation rate in the previous round is sufficiently high:** Cooperate.
*   **If the observed cooperation rate in the previous round is too low:** Defect.
*   **A "buffer zone" exists:** To account for noise and the possibility of accidental defections, we don't immediately switch to defection if the cooperation rate dips slightly. We only defect if the cooperation rate falls below a certain threshold, and we only resume cooperation if it significantly rebounds.

Let's define some thresholds based on the game parameters:

*   **Cooperation Threshold ($T_{coop}$):** This is the minimum observed cooperation rate required to *maintain* cooperation in the current round. A sensible value would be slightly above the minimum required for success, acknowledging that sometimes one player defecting might cause others to defect due to uncertainty. We can set this as a fraction of the required minimum cooperators ($m$) relative to the total players ($n$).
    *   $T_{coop} = \frac{m}{n} + \text{a small buffer}$
    *   **Practical implementation:** We will use a percentage of the required cooperators. Let's say we need the observed cooperation rate to be at least **1.2 times the minimum required fraction of cooperators (m/n)**, but capped at 1 (meaning if m/n is already very high, we don't require *more* than 100% cooperation).
    *   Specifically, if $\frac{C_{t-1}}{n} \ge \min(1.0, \frac{m}{n} \times 1.2)$, we *consider* cooperating.

*   **Defection Threshold ($T_{defect}$):** This is the observed cooperation rate below which we will definitely defect in the current round. This threshold should be lower than $T_{coop}$ to create a hysteresis effect, preventing oscillation between cooperation and defection.
    *   **Practical implementation:** We will require the observed cooperation rate to be at least **0.8 times the minimum required fraction of cooperators (m/n)** for us to resume cooperation. If it falls below this, we defect.
    *   Specifically, if $\frac{C_{t-1}}{n} < \frac{m}{n} \times 0.8$, we *will* defect.

**Core Decision Logic (for rounds t > 1):**

1.  **Calculate observed cooperation rate:** $R_{observed} = \frac{C_{t-1}}{n}$ (where $C_{t-1}$ is the count of cooperators from the previous round).
2.  **Check for Defection:** If $R_{observed} < \frac{m}{n} \times 0.8$, then **Defect (D)**.
3.  **Check for Cooperation:** If $R_{observed} \ge \frac{m}{n} \times 1.2$, then **Cooperate (C)**.
4.  **Buffer Zone (Sticking with previous action):** If neither of the above conditions is met (i.e., $\frac{m}{n} \times 0.8 \le R_{observed} < \frac{m}{n} \times 1.2$), then **mirror the action taken in round t-1**. This is crucial for stability and avoiding immediate reactions to slight dips.

**Rationale for thresholds:**
*   The buffer zone around the target $\frac{m}{n}$ allows the strategy to tolerate temporary dips in cooperation without immediately retaliating.
*   The higher threshold for cooperation ensures that we only resume full cooperation when there's strong evidence of collective success.
*   The lower threshold for defection ensures that we quickly disengage if cooperation is clearly failing.

### 2. Handling Edge Cases

*   **First Round (t = 1):**
    *   In the absence of any historical data, the most cooperative and trusting approach is to **Cooperate (C)**. This signals a willingness to try for the collective good from the outset. This is the most "cooperative" starting point.

*   **Last Round (t = r):**
    *   In the final round, there is no future to incentivize cooperation. The rational decision for any player is to maximize their immediate payoff. Since defecting always yields at least the private endowment (1) and potentially more if others cooperate, the individually rational choice is to **Defect (D)**. This strategy aligns with this by also defecting in the last round.

*   **Near-misses in cooperation:** The buffer zone logic described above addresses this. If the number of cooperators is just below $m$, but not critically low, we don't immediately punish. We stick with our previous action.

*   **Guaranteed success (m=n):** If $m=n$, then everyone must cooperate for success.
    *   In this case, our thresholds $\frac{m}{n} \times 0.8$ and $\frac{m}{n} \times 1.2$ become $0.8$ and $1.2$.
    *   Our logic becomes:
        *   If $R_{observed} < 0.8$, **Defect**.
        *   If $R_{observed} \ge 1.2$ (which is impossible as $R_{observed} \le 1.0$), then **Cooperate**.
        *   Buffer zone: $0.8 \le R_{observed} < 1.2$. In this range, mirror previous action.
    *   This means we start by cooperating. If even one person defects ($R_{observed} < 0.8$), we will defect in the next round. If everyone cooperates ($R_{observed} = 1.0$), we stay in the buffer zone and mirror cooperation. This effectively becomes a Tit-for-Tat-like strategy that is forgiving only if cooperation is overwhelmingly successful.

*   **Guaranteed failure (m > n or m=1):** The problem statement defines $1 < m < n$, so these extreme cases are not possible.

### 3. Cooperative Mindset Alignment

This strategy is designed with a strong cooperative inclination:

*   **Starts cooperative:** It defaults to cooperation in the first round, demonstrating good faith.
*   **Forgiving:** It doesn't immediately punish small deviations from the ideal cooperation level. The buffer zone allows for recovery and doesn't create a cycle of retaliation for minor missteps.
*   **Rewards sustained cooperation:** It only fully commits to cooperation when the observed cooperation rate is sufficiently high, aiming to exploit the mutual benefit of the community project.
*   **Adapts to group behavior:** It's not rigid. It learns from the collective actions of others and adjusts its own behavior accordingly. If others consistently defect, it will eventually follow suit to protect its own payoff, but it doesn't initiate defection.
*   **Avoids unnecessary defection:** It only defects when it believes cooperation is genuinely failing or when it's strategically rational in the final round.

### Pseudocode

```python
# Game Parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward factor

# Strategy State
history = []  # List of tuples: (player_i_action, other_players_actions_in_round)
my_last_action = None

def decide_action(current_round, history):
    """
    Determines the action for player i in the current_round.

    Args:
        current_round (int): The current round number (1-indexed).
        history (list): A list of past rounds' outcomes.
                        Each element is a tuple: (my_action, [opponent1_action, ..., opponent_n-1_action])

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    if current_round == 1:
        # First round: Start by cooperating to show good faith.
        my_last_action = 'C'
        return 'C'

    if current_round == r:
        # Last round: Rational to defect to maximize individual payoff.
        return 'D'

    # Calculate observed cooperation rate from the previous round
    previous_round_data = history[-1] # Get data from the most recent completed round
    my_previous_action = previous_round_data[0]
    opponent_actions_in_previous_round = previous_round_data[1]

    cooperators_in_previous_round = 0
    if my_previous_action == 'C':
        cooperators_in_previous_round += 1
    cooperators_in_previous_round += opponent_actions_in_previous_round.count('C')

    observed_cooperation_rate = cooperators_in_previous_round / n

    # Define thresholds based on game parameters
    min_cooperation_fraction = m / n
    cooperation_trigger_rate = min_cooperation_fraction * 1.2
    # Ensure trigger rate doesn't exceed 1.0
    cooperation_trigger_rate = min(1.0, cooperation_trigger_rate)

    defection_trigger_rate = min_cooperation_fraction * 0.8

    # Decision logic
    if observed_cooperation_rate < defection_trigger_rate:
        # Cooperation has significantly failed, defect to protect payoff.
        my_last_action = 'D'
        return 'D'
    elif observed_cooperation_rate >= cooperation_trigger_rate:
        # Cooperation is strong, continue cooperating.
        my_last_action = 'C'
        return 'C'
    else:
        # In the buffer zone, mirror the action from the previous round for stability.
        # If this is the first round after a period of uncertainty, my_last_action
        # would have been set in the previous iteration of this decision logic.
        return my_last_action

# Example usage (within a tournament simulation)
# player_action = decide_action(current_round_number, game_history)
```

This strategy aims to find a sweet spot between being a naive cooperator and a purely retaliatory player. It prioritizes collective success but has built-in safeguards against exploitation and a clear understanding of game-theoretic incentives in the final round.
'''

description_COOPERATIVE_12 = '''
This strategy, dubbed "Adaptive Threshold Cooperation" (ATC), is designed to be cooperative by default, but sufficiently adaptive to punish defection and reward cooperation. It aims to leverage the repeated nature of the game and the common knowledge of parameters to foster a stable cooperative equilibrium.

## Adaptive Threshold Cooperation (ATC) Strategy

**Core Philosophy:** ATC operates on the principle of fostering cooperation by initially aiming for the group benefit, but vigilantly monitoring the collective actions. It aims to be a "good citizen" as long as the group reciprocates, but will shift to a more self-interested stance if cooperation fails or is exploited.

### 1. Decision Rules

ATC's decision in any given round, *t*, is determined by the observed collective behavior in previous rounds, specifically the number of players who cooperated.

**General Rule (for rounds t > 1):**

*   **If the number of cooperators in the previous round (t-1) was greater than or equal to *m*:**
    *   **Cooperate (C).** This signifies that the group successfully met the cooperation threshold, and ATC rewards this collective success by continuing to cooperate.

*   **If the number of cooperators in the previous round (t-1) was less than *m*:**
    *   **Defect (D).** This signals that the group failed to achieve the cooperation threshold. ATC will defect to preserve its own endowment, as the community project did not yield its benefits. Furthermore, this serves as a punishment for the insufficient cooperation observed, incentivizing future cooperation.

### 2. Handling Edge Cases

**Round 1:**

*   **Decision: Cooperate (C).** In the absence of any historical data, ATC initiates with a cooperative gesture. This is the most conducive action for potentially achieving the beneficial outcome of meeting the cooperation threshold from the outset. It's an optimistic start, assuming others might also be inclined to cooperate.

**Last Round (Round *r*):**

*   **Decision: Defect (D).** This is a crucial modification for the final round. In a finitely repeated game, the last round often incentivizes players to defect to maximize their individual payoff in that final interaction, as there are no future rounds to influence. To prevent exploitation and ensure ATC doesn't leave its endowment on the table, it defects in the last round, regardless of past history. This is a standard deviation from pure cooperation in the final stage of finitely repeated games.

**Specifics on Counting Cooperators:**

*   ATC needs to count the number of players who played 'C' in the *previous* round to make its decision for the *current* round.
*   This count should include its own action in the previous round if it cooperated.

### 3. Be Cooperative

ATC is fundamentally cooperative by its initial action and its default behavior in non-punishment phases.

*   **Initial Cooperation:** By choosing 'C' in the first round, ATC signals a willingness to invest in the collective good and attempt to establish a cooperative norm.
*   **Reward for Cooperation:** ATC's decision to 'C' when the threshold is met in the previous round directly rewards collective success and reinforces the beneficial outcome of cooperation.
*   **Punishment as a Reversion:** The shift to 'D' when the threshold is not met is not gratuitous aggression, but a measured response to the failure of the collective to achieve the desired outcome. It's a temporary measure designed to steer the group back towards cooperation by making defection individually more profitable when cooperation fails. This form of conditional cooperation is a well-established mechanism for fostering cooperation in repeated games.

---

### Pseudocode Representation

```
function ATC_strategy(player_id, current_round, game_parameters, history):
    n = game_parameters['n']
    r = game_parameters['r']
    m = game_parameters['m']
    k = game_parameters['k']

    # history is a list of lists, where history[t-1] is a list of actions taken by all players in round t-1
    # e.g., history[t-1] = [action_player_1, action_player_2, ..., action_player_n]

    # Handle the last round
    if current_round == r:
        return 'D' # Defect in the last round

    # Handle the first round
    if current_round == 1:
        return 'C' # Cooperate in the first round

    # For all other rounds (t > 1 and t < r)
    previous_round_actions = history[current_round - 1] # Actions from the *previous* round

    cooperators_in_previous_round = 0
    for action in previous_round_actions:
        if action == 'C':
            cooperators_in_previous_round += 1

    # Decision rule based on previous round's cooperation
    if cooperators_in_previous_round >= m:
        return 'C' # Cooperate if threshold was met
    else:
        return 'D' # Defect if threshold was not met

```

---

**Robustness and Adaptability:**

*   **Adaptive:** The strategy explicitly adapts its behavior based on the observed outcomes of previous rounds. It's not a fixed strategy.
*   **Robust to Opponent Behavior:**
    *   **Pure Cooperators:** If all players cooperate, ATC will continue to cooperate indefinitely (until the last round).
    *   **Pure Defectors:** If all players defect, ATC will defect after the first round. This prevents it from being exploited.
    *   **Mixed Strategies:** ATC's success hinges on whether a sufficient number of players (at least *m*) also adopt cooperative or conditionally cooperative strategies. If the group can collectively reach the threshold, ATC reinforces that behavior. If not, it defaults to self-preservation.
    *   **"Grim Trigger" like:** The strategy exhibits a component similar to a "grim trigger" in that a single round of insufficient cooperation leads to a period of defection. However, unlike a true grim trigger which might defect forever, ATC can revert to cooperation if the group *again* meets the threshold. This makes it less unforgiving and more conducive to re-establishing cooperation.

This strategy is designed to create a stable environment where cooperation can emerge and be sustained, while also protecting the player from outright exploitation.
'''

description_COOPERATIVE_13 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, emphasizing adaptation and robustness:

## Cooperative Strategy: The "Cautious Cooperative Threshold" (CCT) Strategy

This strategy aims to maximize long-term collective benefit by conditionally cooperating, aiming to meet the cooperation threshold ($m$) without being exploited. It's designed to be adaptive to the observed behavior of other players and robust to a variety of strategies.

### Core Philosophy:

The CCT strategy believes in the power of cooperation for shared gain. However, it acknowledges the risk of being exploited by defectors. Therefore, it adopts a "cautious cooperative" stance:

*   **Default to cooperation:** If there's a reasonable chance of reaching the threshold, cooperate.
*   **Observe and adapt:** Continuously monitor the behavior of others. If cooperation is consistently failing to meet the threshold, or if defectors are prevalent, adjust the strategy accordingly.
*   **Punish defection indirectly:** While direct punishment isn't possible in this game structure, the strategy can indirectly "punish" by reducing its own cooperation when the group consistently fails to achieve the collective reward. This makes defecting less attractive in the long run.

### 1. Decision Rules:

The decision to cooperate or defect in any given round is based on a dynamic assessment of the group's past performance and the player's own risk tolerance.

**Primary Decision Rule:**

*   **If (Number of previous cooperators in the last round) >= (m - 1):**
    *   **Play C (Cooperate).**
    *   *Reasoning:* This indicates that the group was very close to meeting the threshold in the previous round. By cooperating now, you increase the probability of tipping the scales and achieving the collective reward for everyone.

*   **Else (If Number of previous cooperators in the last round) < (m - 1):**
    *   **Play D (Defect).**
    *   *Reasoning:* The group has fallen significantly short of the cooperation threshold in the last round. Cooperating again in this situation, without clear signs of others' commitment, risks wasted endowments and doesn't contribute to the collective benefit. This is a signal to re-evaluate the group's cooperative intentions.

**Secondary Decision Rule (Adaptive Threshold Adjustment):**

To make the strategy more robust, we introduce an adaptive threshold. This allows the strategy to become more or less demanding of cooperation from others based on observed success.

*   **Initial "Cooperation Gap":** Set an initial parameter, `CoopGap = m - 1`. This represents the minimum number of *other* players you ideally want to see cooperate to justify your own cooperation.
*   **Dynamic `CoopGap` Adjustment:**
    *   **If (Number of cooperators in the last round) >= m:**
        *   `CoopGap = max(1, CoopGap - 1)` (Slightly decrease the `CoopGap`, meaning you'll be more willing to cooperate even if slightly fewer others cooperate next round. This rewards successful cooperation.)
    *   **If (Number of cooperators in the last round) < m - 1:**
        *   `CoopGap = min(n - 1, CoopGap + 1)` (Slightly increase the `CoopGap`, meaning you'll require more other players to cooperate before you'll cooperate next round. This punishes consistent failure to meet the threshold.)

**Revised Decision Rule using `CoopGap`:**

*   **If (Number of previous cooperators in the last round, excluding self) >= `CoopGap`:**
    *   **Play C (Cooperate).**
*   **Else:**
    *   **Play D (Defect).**

**Rationale for the `CoopGap`:**

*   **Learning and Adaptation:** This dynamic `CoopGap` allows the strategy to learn about the overall cooperativeness of the group. If the group consistently succeeds in reaching the threshold, the CCT becomes more trusting and more likely to cooperate. If the group consistently fails, it becomes more cautious.
*   **Robustness to Defection:** If a significant number of players are defecting, the `CoopGap` will increase, making the CCT less likely to cooperate unnecessarily. This prevents it from being a consistent target for exploitation.
*   **Encouraging Cooperation:** When the group is *almost* there, the CCT's willingness to cooperate (when `CoopGap` is low) can be the decisive factor in reaching the threshold.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Play C (Cooperate).**
    *   *Reasoning:* In the absence of any historical data, the CCT assumes a baseline of cooperative intent. It makes an initial cooperative move to signal willingness and to see if others will reciprocate, aiming to kickstart the collective reward mechanism. The `CoopGap` is initialized to `m - 1`.

*   **Last Round (t=r):**
    *   **Play D (Defect).**
    *   *Reasoning:* In the final round, there are no future consequences for defection. The primary goal shifts from long-term collective gain to maximizing individual payoff in the last opportunity. Since there's no future to build trust or encourage cooperation, defecting is the individually rational choice in a single-round game. This is a standard Nash Equilibrium consideration for finitely repeated games.

*   **Rounds Just Before the Last (t=r-1):**
    *   The strategy follows the primary and adaptive decision rules. It cooperates if the previous round was close to the threshold, otherwise defects. The CCT does *not* preemptively defect in round `r-1` just because round `r` is the last. It plays the game on its merits until the very end.

*   **Very Small `n` or `m`:**
    *   The strategy is designed to be general. However, in extreme cases (e.g., `m=2`, `n=3`), the dynamics become more sensitive. The `CoopGap` adjustment logic handles these by bounding `CoopGap` between 1 and `n-1`.

### 3. Being Cooperative:

The CCT strategy is fundamentally cooperative in its intent. It *prefers* to cooperate and aims to achieve the collective reward whenever there's a reasonable probability of success.

*   **Proactive Cooperation:** It initiates cooperation in the first round to demonstrate its cooperative stance and set a positive example.
*   **Conditional Reciprocity:** It reciprocates cooperation when it appears beneficial and when others are showing signs of commitment. The `CoopGap` mechanism ensures that the CCT doesn't get exploited by consistently defecting players.
*   **Long-Term Focus (until the last round):** The strategy's decision-making process is geared towards maximizing the total payoff over *all* rounds, not just maximizing payoff in a single round (except for the very last round). This aligns with a cooperative, forward-looking approach.
*   **No Malice:** The CCT does not actively seek to punish or harm other players. Its "punishment" is a consequence of observed behavior and a defensive mechanism to protect its own long-term interests and the potential for future cooperation if the group dynamics shift.

### Pseudocode:

```python
# Game Parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward if threshold met factor

# Strategy State Variables
coop_gap = m - 1  # Initial cooperation threshold for other players
last_round_cooperators = 0 # Track cooperators in the previous round

def decide_action(current_round, player_id, history):
    """
    Determines the action (C or D) for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        player_id (int): The ID of the current player.
        history (list of lists): A list of lists, where history[t][p] is the action
                                  of player p in round t (0-indexed, 'C' or 'D').

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """
    global coop_gap, last_round_cooperators

    # --- Handle Edge Cases ---
    if current_round == r:
        # Last round: Always defect to maximize individual payoff.
        return 'D'

    if current_round == 1:
        # First round: Always cooperate to initiate.
        coop_gap = m - 1 # Reset coop_gap for a fresh start
        return 'C'

    # --- Calculate Cooperation in the Last Round ---
    # The history is 0-indexed for rounds, and might not include the current round yet.
    # So, we look at history[current_round - 2] for the actions of the previous round.
    # Note: history is assumed to be populated with previous rounds' actions.
    if current_round > 1:
        previous_round_actions = history[current_round - 2] # Get actions from t-1
        # Count cooperators from *other* players in the previous round.
        # We assume we don't have our own action for the current round in history yet.
        # So we count all cooperators, and if we cooperated, we'd subtract 1 from the count
        # if we wanted to know how many *others* cooperated.
        # However, the logic for coop_gap adjustment compares against *total* cooperators,
        # and the decision logic compares against *our own* coop_gap.

        # For simplicity, let's get total cooperators in the last round.
        # If history structure is different, adjust this part.
        # Assuming history is a list of rounds, and each round is a list of player actions.
        total_cooperators_last_round = previous_round_actions.count('C')

        # --- Adaptive Threshold Adjustment ---
        # Adjust coop_gap based on last round's outcome
        if total_cooperators_last_round >= m:
            # Success: Reduce the required number of other cooperators slightly.
            coop_gap = max(1, coop_gap - 1)
        elif total_cooperators_last_round < m - 1:
            # Significant Failure: Increase the required number of other cooperators.
            coop_gap = min(n - 1, coop_gap + 1)
        # If total_cooperators_last_round is between m-1 and m (inclusive of m-1, exclusive of m),
        # coop_gap remains unchanged.

    # --- Primary Decision Rule (using adjusted coop_gap) ---
    # We need to know how many *other* players cooperated in the last round.
    # If our own action in the last round was 'C', we subtract 1 from the total count.
    # We need to access our *own* previous action.
    # Let's assume history[current_round - 2][player_id - 1] gives our previous action.
    # (Adjust indexing based on actual history implementation).

    # For this pseudocode, let's re-evaluate: The decision rule should be
    # "If (Number of previous cooperators in the last round, excluding self) >= coop_gap".
    # This requires knowing our own action in the last round.
    # Let's assume we have access to `my_previous_action`.

    # Re-calculating last_round_cooperators based on total and knowing our action.
    # This is a conceptual detail, the implementation might access history differently.
    # For the pseudocode, we'll simplify the decision logic to make it clearer.

    # Simplified Decision Logic:
    # If total cooperators last round was >= m-1, we cooperate.
    # This is a strong signal that others are trying.
    if total_cooperators_last_round >= m - 1:
        return 'C'
    else:
        # If less than m-1 cooperated, it's a clear sign of insufficient cooperation.
        # We defect to conserve our endowment.
        return 'D'


# --- Notes on History and State Management ---
# - The 'history' parameter needs to be managed externally and passed in.
# - The 'global' keyword for coop_gap and last_round_cooperators is a simplification.
#   In a real implementation, these would be part of an object or passed as state.
# - The calculation of 'total_cooperators_last_round' and 'my_previous_action'
#   depends heavily on how the 'history' data structure is implemented.
#   The pseudocode assumes history[round_index][player_index].

# A more refined approach to the decision rule, using the dynamic coop_gap:
def decide_action_refined(current_round, player_id, history, my_previous_action):
    global coop_gap

    if current_round == r:
        return 'D'

    if current_round == 1:
        coop_gap = m - 1
        return 'C'

    # Get actions from the previous round (t-1)
    # history[current_round - 2] refers to the list of actions for round t-1.
    previous_round_actions = history[current_round - 2]
    total_cooperators_last_round = previous_round_actions.count('C')

    # Adaptive Threshold Adjustment
    if total_cooperators_last_round >= m:
        coop_gap = max(1, coop_gap - 1)
    elif total_cooperators_last_round < m - 1:
        coop_gap = min(n - 1, coop_gap + 1)

    # Decision based on adjusted coop_gap
    # Number of *other* players who cooperated in the last round.
    other_cooperators_last_round = total_cooperators_last_round
    if my_previous_action == 'C':
        other_cooperators_last_round -= 1

    if other_cooperators_last_round >= coop_gap:
        return 'C'
    else:
        return 'D'

```
'''

description_COOPERATIVE_14 = '''
Okay, let's design a cooperative strategy for the Collective Risk Dilemma. The core challenge here is to incentivize cooperation when individual defection can be tempting, especially if others are cooperating. Since we can't communicate and the game is repeated, we can use past behavior to inform future decisions.

Our strategy will be called **"Threshold Tit-for-Tat with Generous Forgiveness" (TTGF)**.

**Core Philosophy:**

Our strategy aims to be cooperative by default, but it will retaliate against defection. However, it will also be forgiving to encourage a return to cooperation, recognizing that occasional mistakes or attempts to test the waters might occur. The key is to learn from the group's collective behavior.

**1. Decision Rules:**

The decision to cooperate (C) or defect (D) for player *i* in round *t* will be based on the observed cooperation rate in the previous round (*t-1*) and a set of pre-defined thresholds.

*   **Default Action:** Cooperate. We start by assuming cooperation is the best path forward for the group.

*   **Condition for Defection:** Player *i* will defect in round *t* if the observed cooperation rate in round *t-1* falls *below* a certain threshold, and this low cooperation has persisted for a specified number of rounds.

*   **Condition for Returning to Cooperation:** Player *i* will return to cooperating in round *t* if the observed cooperation rate in round *t-1* *meets or exceeds* a certain threshold, and it has been above a "punishment threshold" for a certain number of rounds.

**Defining Thresholds and Parameters for TTGF:**

To make this concrete, let's introduce some parameters to our strategy. These parameters are derived from the game's parameters ($n, m, r, k$) and can be adjusted.

*   **Cooperation Target Threshold ($T_C$):** This is the minimum desired cooperation rate for the group to be considered "cooperating well." A reasonable starting point is slightly above the minimum required to trigger the reward: $T_C = m/n$. However, to be more robust, we might set this slightly higher, say $T_C = m/n + \epsilon$, where $\epsilon$ is a small value.

*   **Defection Trigger Threshold ($T_D$):** This is the cooperation rate below which we consider the group to be "in trouble." We should defect if the group consistently fails to meet the minimum cooperation requirement ($m$). So, $T_D = (m-1)/n$. If the cooperation rate falls *below* this, it's a strong signal of potential collapse.

*   **Punishment Persistence ($P_{persist}$):** The number of consecutive rounds the cooperation rate must be *below* $T_D$ before we switch to a sustained defection mode. This prevents immediate retaliation for a single bad round. A reasonable value could be related to $n$ or $r$, e.g., $P_{persist} = \lceil n/3 \rceil$ or $P_{persist} = \lceil r/5 \rceil$.

*   **Forgiveness Persistence ($F_{persist}$):** The number of consecutive rounds the cooperation rate must be *at or above* $T_C$ after a period of defection before we forgive and return to cooperation. This allows the group to prove it's committed to cooperating again. A similar value to $P_{persist}$ could be used, e.g., $F_{persist} = \lceil n/3 \rceil$ or $F_{persist} = \lceil r/5 \rceil$.

*   **Cooperation Signal Threshold ($T_S$):** A threshold that indicates a strong, positive cooperative signal from the group, allowing for faster return to cooperation. This could be a rate significantly above $T_C$, e.g., $T_S = (m + \lfloor n/2 \rfloor) / n$.

**Decision Logic in Detail:**

Let $N_{C, t-1}$ be the number of players who cooperated in round $t-1$.
Let $C_{rate, t-1} = N_{C, t-1} / n$ be the cooperation rate in round $t-1$.

We will maintain two counters for player *i*:
*   `defect_streak`: Number of consecutive rounds where $C_{rate, t-1} < T_D$.
*   `cooperate_streak`: Number of consecutive rounds where $C_{rate, t-1} \ge T_C$.

**Player *i*'s action in round *t*:**

1.  **If it's the first round (t=1):** Cooperate (C). We always start cooperatively.

2.  **If `defect_streak` >= $P_{persist}$:**
    *   Defect (D). We have observed sustained underperformance and are entering a punitive mode.

3.  **Else if `cooperate_streak` >= $F_{persist}$ (and `defect_streak` < $P_{persist}$):**
    *   Cooperate (C). We have seen enough consistent cooperation to forgive and return to our default cooperative stance.

4.  **Else (default case, not in sustained defection or sustained cooperation):**
    *   **If $C_{rate, t-1} \ge T_C$:** Cooperate (C). The group is performing at an acceptable level.
    *   **Else if $C_{rate, t-1} < T_D$:** Defect (D). The group has fallen below a critical threshold.
    *   **Else (i.e., $T_D \le C_{rate, t-1} < T_C$):**
        *   This is a tricky intermediate zone. To be more robust, we can introduce a slight bias towards cooperation if it's not too far below $T_C$.
        *   If $C_{rate, t-1} \ge T_C - \delta$ (where $\delta$ is a small tolerance, e.g., $0.05$ or related to $1/n$): Cooperate (C). The slight dip is within acceptable tolerance.
        *   Else: Defect (D). The dip is significant enough to warrant caution.

**Updating Streaks:**

After determining the action for round *t*, we update the streaks based on the observed $C_{rate, t-1}$ (or $C_{rate, t}$ for the next round's calculation):

*   If $C_{rate, t-1} < T_D$: Increment `defect_streak`. Reset `cooperate_streak` to 0.
*   If $C_{rate, t-1} \ge T_C$: Increment `cooperate_streak`. Reset `defect_streak` to 0.
*   If $T_D \le C_{rate, t-1} < T_C$:
    *   If $C_{rate, t-1} \ge T_C - \delta$: Increment `cooperate_streak`. Reset `defect_streak` to 0.
    *   Else: Increment `defect_streak`. Reset `cooperate_streak` to 0.

**2. Handling Edge Cases:**

*   **First Round (t=1):** As stated, always Cooperate (C). This sets a positive initial tone.

*   **Last Round (t=r):** The strategy doesn't change its decision logic for the last round. It will still evaluate the previous round's cooperation rate and apply its rules. However, it's important to note that in the absence of future rounds, the incentive to cooperate from a strategic perspective *might* change for other players (e.g., they might defect in the last round knowing there are no future consequences). Our strategy, being history-dependent, will continue to play based on the history, but it won't "anticipate" this last-round defection by others unless the history already shows it. It's a form of "open-ended" strategy applied to a finite horizon.

*   **No Cooperation in Previous Round ($C_{rate, t-1} = 0$):** This will trigger `defect_streak` to increase rapidly. If $m > 1$, this will almost certainly lead to defection if it persists.

*   **Full Cooperation in Previous Round ($C_{rate, t-1} = 1$):** This will boost `cooperate_streak` and reinforce our cooperative stance.

**3. Cooperativeness:**

*   **Default Cooperative Stance:** The strategy starts by cooperating and prefers to cooperate as long as the group's behavior is not significantly detrimental.
*   **Tit-for-Tat with Generosity:** It retaliates against underperformance but not immediately for a single lapse. It requires a persistent pattern of defection ($P_{persist}$ rounds) before it fully switches to defection. Crucially, it's also generous in returning to cooperation, requiring only a sustained period of good behavior ($F_{persist}$ rounds) to forgive past transgressions.
*   **Adaptive to Group Behavior:** The strategy's thresholds are directly tied to the game's parameters ($m, n$), making it inherently adaptive to the group size and the required cooperation level. The persistence parameters ($P_{persist}, F_{persist}$) can be tuned to adjust how quickly it reacts to changes.
*   **No Exploitation:** The strategy does not seek to exploit other players. It only defects when it perceives a risk to the collective good or as a response to persistent negative behavior. It aims to foster a stable cooperative equilibrium.

**Pseudocode for Player *i*'s Strategy:**

```python
# Game Parameters (provided externally)
n = ...
r = ...
m = ...
k = ...

# Strategy Parameters (derived from game parameters)
# Example values, can be tuned
T_C = m / n  # Cooperation Target Threshold
T_D = (m - 1) / n # Defection Trigger Threshold
P_persist = max(2, ceil(n / 3)) # Punishment Persistence
F_persist = max(2, ceil(n / 3)) # Forgiveness Persistence
delta = 0.05 # Tolerance for intermediate cooperation rate

# State Variables for player i
defect_streak = 0
cooperate_streak = 0

def choose_action(round_number, history):
    """
    Determines player i's action based on game history.

    Args:
        round_number: The current round number (1-indexed).
        history: A list of lists, where history[t-1] is a list of actions
                 played by all n players in round t-1.
                 e.g., [[C, D, C], [D, D, C]] for 2 rounds with n=3.

    Returns:
        'C' or 'D'
    """

    if round_number == 1:
        # Always cooperate in the first round
        return 'C'

    # Calculate cooperation rate in the previous round
    prev_round_actions = history[-1]
    num_cooperators_prev_round = prev_round_actions.count('C')
    cooperation_rate_prev_round = num_cooperators_prev_round / n

    # Update streaks based on previous round's cooperation rate
    if cooperation_rate_prev_round < T_D:
        defect_streak += 1
        cooperate_streak = 0
    elif cooperation_rate_prev_round >= T_C:
        cooperate_streak += 1
        defect_streak = 0
    else: # T_D <= cooperation_rate_prev_round < T_C
        if cooperation_rate_prev_round >= T_C - delta:
            cooperate_streak += 1
            defect_streak = 0
        else:
            defect_streak += 1
            cooperate_streak = 0

    # Decision Logic
    if defect_streak >= P_persist:
        # Sustained underperformance: enter punitive mode
        return 'D'
    elif cooperate_streak >= F_persist:
        # Sustained good performance after punishment: forgive and return to cooperate
        return 'C'
    elif cooperation_rate_prev_round >= T_C:
        # Acceptable cooperation rate
        return 'C'
    elif cooperation_rate_prev_round < T_D:
        # Below critical threshold
        return 'D'
    else: # T_D <= cooperation_rate_prev_round < T_C and not in sustained defection/cooperation
        if cooperation_rate_prev_round >= T_C - delta:
            # Slightly below target, but within tolerance
            return 'C'
        else:
            # Significant dip, but not yet sustained punishment
            return 'D'

# Note: The 'history' object would be managed by the tournament system,
# providing the actions of all players from previous rounds.
# The strategy needs access to 'n' for cooperation rate calculation.
```

This TTGF strategy provides a robust, cooperative, and adaptive approach to the Collective Risk Dilemma by leveraging historical group behavior to guide future decisions, balancing the need for cooperation with a mechanism for deterrence and forgiveness.
'''

description_COOPERATIVE_15 = '''
This strategy, named "Adaptive Cooperative Threshold Seeker" (ACTS), aims to maximize long-term collective payoff while remaining robust to a variety of player behaviors in the Collective Risk Dilemma. It prioritizes achieving the cooperative reward ($k$) by carefully monitoring the collective behavior and adjusting its own actions accordingly.

The core idea is to be conditionally cooperative. ACTS will attempt to cooperate when it believes the threshold ($m$) is likely to be met, and defect when it's clear cooperation will fail or when it perceives others are exploiting its cooperation.

---

## Adaptive Cooperative Threshold Seeker (ACTS) Strategy

### 1. Decision Rules

ACTS's decision in any given round $t$ depends on its assessment of the likelihood of meeting the cooperation threshold ($m$) in the *current* round, informed by the history of the game.

**Key Variables Tracked:**

*   **`cooperators_in_round[t]`**: The number of players who cooperated in round $t$.
*   **`my_actions[t]`**: Player i's action (C or D) in round $t$.
*   **`total_cooperators_in_history`**: Cumulative number of players who cooperated across all past rounds.
*   **`number_of_players_observed`**: Total number of player actions observed across all past rounds.

**Decision Logic (for round $t > 1$):**

1.  **Initial Hesitation/Learning Phase (First few rounds, e.g., `t` ≤ `max(3, r/10)`):**
    *   **If `t` < `m`:** Cooperate (C). The assumption here is that in the early stages, and especially when $m$ is relatively high, the best way to signal intent and explore the possibility of cooperation is to be cooperative. This also helps establish a baseline for others' reactions.
    *   **If `t` ≥ `m`:** Play based on the previous round's outcome and a rolling average of cooperation.
        *   Calculate the **rolling average cooperation rate (RAC)** over the last `min(t-1, 5)` rounds. This captures recent trends.
        *   `RAC = (sum of cooperators in last 5 rounds) / (5 * n)`
        *   **If `RAC` > `0.5` (or some moderate threshold):** Cooperate (C). This indicates a general trend towards cooperation.
        *   **Else:** Defect (D). This suggests cooperation might be failing.

2.  **Established Phase (Rounds `t` > `max(3, r/10)`):**
    *   **Scenario A: Threshold was met in the previous round (`cooperators_in_round[t-1]` ≥ `m`)**
        *   Calculate the **cooperation rate in the previous round (`prev_round_coop_rate = cooperators_in_round[t-1] / n`)**.
        *   **If `prev_round_coop_rate` is high (e.g., > `(m/n) + 0.1`)**: Cooperate (C). This suggests strong, sustained cooperation that is likely to continue. The `+ 0.1` buffer is to ensure we're not just barely meeting the threshold.
        *   **Else (if the previous round barely met the threshold or was close to failing)**: Assess the overall trend.
            *   Calculate the **long-term average cooperation rate (LAC)**: `LAC = total_cooperators_in_history / number_of_players_observed`.
            *   **If `LAC` > `0.5` (or a similar moderate threshold indicating a generally cooperative environment):** Cooperate (C). This suggests that even with recent fluctuations, the overall environment is conducive to cooperation.
            *   **Else:** Defect (D). This indicates a decline or an inherently non-cooperative environment, and we should protect our own endowment.

    *   **Scenario B: Threshold was NOT met in the previous round (`cooperators_in_round[t-1]` < `m`)**
        *   Calculate the **rolling average cooperation rate (RAC)** over the last `min(t-1, 5)` rounds.
        *   **If `RAC` > `0.6` (a higher threshold than before, indicating a strong positive trend is emerging or re-emerging):** Cooperate (C). This suggests that a critical mass of cooperation might be building, and one more cooperator could tip the balance.
        *   **Else:** Defect (D). If the threshold wasn't met and there isn't a strong positive trend, defecting is the safer option to avoid being exploited.

    *   **Scenario C: Exploitation Detection (Applies to all rounds after the first few):**
        *   Keep track of the number of times we defected while the threshold *was* met in that round (`exploited_defects`).
        *   Keep track of the number of times we cooperated while the threshold *was not* met in that round (`failed_cooperations`).
        *   **If `failed_cooperations` > `(some_small_number, e.g., 3)` and `exploited_defects` == `0`:** This suggests we are consistently cooperating and failing to reach the threshold, without others defecting when we do cooperate. This is a signal of being consistently "taken advantage of."
            *   Switch to a **more aggressive defection strategy** for a few rounds (e.g., defect for the next 5 rounds, then revert to the above logic).
        *   **If `exploited_defects` > `(some_small_number, e.g., 2)` and `failed_cooperations` == `0`:** This suggests we are defecting and the threshold is being met anyway. This is a sign that others are cooperating and we might be missing out on collective gains or acting overly cautiously.
            *   Reduce the threshold for cooperation slightly for a few rounds (e.g., lower the RAC threshold by `0.1` for the next 5 rounds).

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   Always Cooperate (C). This is a bold opening move to establish a cooperative intent and see how others react. It's crucial for initiating any potential for collective gain.

*   **Last Round (t=r):**
    *   Always Defect (D). In the final round, there is no future incentive for cooperation. The optimal strategy is to maximize individual payoff by keeping the endowment, regardless of others' actions. This is a standard assumption in game theory for finite, known-length repeated games.

*   **Early Rounds (t <= `max(3, r/10)`):**
    *   As described in Decision Rule 1, a more lenient cooperation policy is adopted to explore the cooperative landscape.

*   **Threshold `m` is very high (close to `n`):**
    *   The strategy naturally adapts. The thresholds for RAC and LAC will need to be met by a larger proportion of players, making cooperation harder to achieve. The strategy will be more inclined to defect if a high number of cooperators is not observed.

*   **Threshold `m` is low (close to 1, but > 1):**
    *   The strategy will be more inclined to cooperate, as the threshold is easier to meet. The reliance on RAC and LAC will still guide decisions, but the overall probability of cooperating will be higher.

*   **Reward `k` is very high:**
    *   The strategy's inherent goal is to achieve the high reward. The adaptive nature will mean it is more willing to risk cooperating if the potential for a high reward exists, provided there's a reasonable chance of success.

### 3. Being Cooperative

The ACTS strategy is fundamentally cooperative in its intent.

*   **Proactive Cooperation:** It initiates cooperation in the first round and attempts to cooperate when there's a reasonable chance of meeting the threshold and achieving the collective reward ($k$).
*   **Adaptive Thresholding:** Instead of fixed cooperation, it uses dynamic thresholds (rolling averages, long-term averages) to assess the *likelihood* of success, reflecting a desire to achieve the collective good when it's feasible.
*   **Resilience, Not Naivete:** While cooperative, it's not naive. It includes mechanisms to detect and respond to exploitative behavior (by defecting defensively) and to adjust when its cooperative efforts are consistently failing to yield benefits. This resilience ensures that the cooperative spirit is not punished and can be sustained.
*   **Focus on Collective Gain:** The primary driver for cooperation is the potential for a higher payoff for *all* players when the threshold is met. The strategy aims to engineer the conditions for this collective gain.

---

### Pseudocode Representation

```pseudocode
// --- Global state variables ---
var cooperators_in_round = array of size r, initialized to 0
var my_actions = array of size r, initialized to null
var total_cooperators_in_history = 0
var number_of_players_observed = 0
var exploited_defects = 0 // Rounds where I defected AND threshold was met
var failed_cooperations = 0 // Rounds where I cooperated AND threshold was NOT met
var defensive_defection_counter = 0 // Counter for aggressive defection phase
var relaxed_threshold_counter = 0 // Counter for temporarily lowering coop threshold

// --- Strategy function for round t ---
function ACTS_Strategy(t, n, m, k, observed_actions_previous_rounds):
    // --- Handle last round ---
    if t == r:
        return D

    // --- Handle defensive defection phase ---
    if defensive_defection_counter > 0:
        defensive_defection_counter = defensive_defection_counter - 1
        return D

    // --- Handle relaxed threshold phase ---
    if relaxed_threshold_counter > 0:
        relaxed_threshold_counter = relaxed_threshold_counter - 1
        // Temporarily lower cooperation threshold by 0.1
        temp_rac_threshold = 0.5 // Default for relaxed phase
    else:
        temp_rac_threshold = 0.6 // Default for relaxed phase


    // --- Update history and observed counts ---
    // Assume observed_actions_previous_rounds contains actions of all n players for rounds 1 to t-1
    if t > 1:
        num_coop_last_round = count_cooperators(observed_actions_previous_rounds[t-1])
        cooperators_in_round[t-1] = num_coop_last_round
        if num_coop_last_round >= m:
            // Check if I defected last round but threshold was met
            if my_actions[t-1] == D and num_coop_last_round >= m:
                exploited_defects = exploited_defects + 1
        else:
            // Check if I cooperated last round but threshold was not met
            if my_actions[t-1] == C and num_coop_last_round < m:
                failed_cooperations = failed_cooperations + 1

        total_cooperators_in_history = total_cooperators_in_history + num_coop_last_round
        number_of_players_observed = number_of_players_observed + n

    // --- Detection of Exploitation ---
    // If we've consistently cooperated and failed to reach m, and haven't seen others defecting when we did
    if failed_cooperations > 3 and exploited_defects == 0:
        defensive_defection_counter = 5 // Defect for next 5 rounds
        return D // Immediate defection

    // If we've defected and threshold was met without our help, and we haven't failed cooperations
    if exploited_defects > 2 and failed_cooperations == 0:
        relaxed_threshold_counter = 5 // Lower threshold for next 5 rounds
        // Continue to standard logic, but with lowered threshold


    // --- Decision Logic ---
    // First round
    if t == 1:
        my_actions[t] = C
        return C

    // Initial Learning Phase (e.g., first 10% or 3 rounds, whichever is larger)
    learning_phase_end = max(3, floor(r * 0.1))
    if t <= learning_phase_end:
        if t < m: // Early on, be cooperative to signal intent
            my_actions[t] = C
            return C
        else: // After t >= m in learning phase, start looking at recent history
            // Calculate RAC over last 5 rounds
            rac_window = min(t - 1, 5)
            if rac_window > 0:
                sum_coop_last_window = sum(cooperators_in_round[t-rac_window : t-1])
                rac = sum_coop_last_window / (rac_window * n)
            else:
                rac = 0 // No history yet

            if rac > 0.5: // Moderate threshold
                my_actions[t] = C
                return C
            else:
                my_actions[t] = D
                return D

    // Established Phase
    else:
        // Calculate RAC over last 5 rounds
        rac_window = min(t - 1, 5)
        if rac_window > 0:
            sum_coop_last_window = sum(cooperators_in_round[t-rac_window : t-1])
            rac = sum_coop_last_window / (rac_window * n)
        else:
            rac = 0 // Should not happen in established phase unless t=1

        // Calculate LAC
        if number_of_players_observed > 0:
            lac = total_cooperators_in_history / number_of_players_observed
        else:
            lac = 0 // Should not happen

        num_coop_last_round = cooperators_in_round[t-1] // Already calculated for history update

        // Scenario A: Threshold was met in the previous round
        if num_coop_last_round >= m:
            prev_round_coop_rate = num_coop_last_round / n
            if prev_round_coop_rate > (m/n) + 0.1: // High certainty of sustained cooperation
                my_actions[t] = C
                return C
            else: // Barely met or close to failing
                if lac > 0.5: // Check long-term trend
                    my_actions[t] = C
                    return C
                else:
                    my_actions[t] = D
                    return D

        // Scenario B: Threshold was NOT met in the previous round
        else:
            if rac > temp_rac_threshold: // Use potentially relaxed threshold
                my_actions[t] = C
                return C
            else:
                my_actions[t] = D
                return D

// Helper function to count cooperators in an action list
function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == C:
            count = count + 1
    return count
```
'''

description_COOPERATIVE_16 = '''
This is a fascinating game that blends elements of public goods provision with a social dilemma. The core challenge lies in coordinating enough players to reach the cooperation threshold ('m') while avoiding being exploited by defectors.

Here's a cooperative strategy designed to be adaptive and robust, focusing on the parameters and observed history:

## Strategy: Adaptive Threshold Cooperative (ATC)

The ATC strategy aims to foster cooperation by observing the collective behavior and adjusting its own actions accordingly. It prioritizes reaching the cooperative threshold, but with a crucial "safety net" to prevent exploitation.

### 1. Decision Rules

The ATC strategy's decision in any given round is based on two primary factors:

*   **The observed cooperation rate in the previous round:** This is the most direct indicator of how well the group is coordinating.
*   **The number of rounds remaining:** This influences how much risk the strategy is willing to take.

**Decision Logic:**

Let `coop_rate_prev` be the proportion of players who cooperated in the *immediately preceding* round.
Let `n_remaining` be the number of rounds left to play (including the current round).

*   **If `coop_rate_prev` >= `m / n` (i.e., the previous round met or exceeded the cooperation threshold):**
    *   **Cooperate (C).**
    *   **Reasoning:** The group has demonstrated its ability to achieve the cooperative outcome. Continuing to cooperate reinforces this behavior and maximizes the chance of continued collective benefit.

*   **If `coop_rate_prev` < `m / n` (i.e., the previous round fell short of the cooperation threshold):**
    *   **If `n_remaining` > 1 (i.e., not the last round):**
        *   **Defect (D).**
        *   **Reasoning:** The group failed to cooperate. In this situation, defecting is the safer choice. Cooperating when others defect leads to a payoff of `(1 - 1) = 0` for the cooperator, which is worse than defecting and getting `(1 - 0) = 1`. This pause in cooperation acts as a deterrent, signaling that the benefits of cooperation are contingent on achieving the threshold. The strategy will re-evaluate in the next round.
    *   **If `n_remaining` == 1 (i.e., the last round):**
        *   **Cooperate (C).**
        *   **Reasoning:** In the final round, there are no future rounds to recover from a failed cooperative attempt. The best outcome for the individual, regardless of others' actions, is to ensure the threshold is met *if possible* to maximize payoff for that round, or at least to contribute to a potentially higher payoff for the group. If `m-1` other players cooperate, the ATC player cooperating would get `(1-1) + k = k`. If the ATC player defects and `m-1` others cooperate, they'd get `(1-0) + k = 1+k`. If fewer than `m-1` others cooperate, defecting gives `1` and cooperating gives `0`. However, the "cooperative" imperative is strongest here, aiming for the highest possible group outcome in the final moment.

### 2. Handling Edge Cases

*   **First Round:**
    *   **Decision:** **Cooperate (C).**
    *   **Reasoning:** In the absence of any historical data, the most "cooperative" stance is to initiate cooperation. This sets a positive example and provides the initial data for future rounds. It's a calculated risk to see if others will follow.

*   **Last Round:**
    *   **Decision:** **Cooperate (C).** (As detailed in the decision rules above).
    *   **Reasoning:** The ATC strategy prioritizes maximizing potential payoff in the final round, and this is achieved by ensuring the cooperation threshold is met if feasible. The risk of being exploited is mitigated by the fact that there are no future rounds to be exploited in.

*   **Rounds where `m` is very close to `n` (e.g., `m = n-1`):**
    *   The strategy inherently handles this. The threshold `m/n` will be high, meaning the group needs to cooperate significantly. If the previous round's cooperation rate doesn't meet this high bar, the strategy will defect, waiting for better collective action.

*   **Rounds where `m` is very close to 1 (e.g., `m = 2` for `n > 2`):**
    *   The strategy also handles this. The threshold `m/n` will be low. If cooperation is observed, the strategy will continue to cooperate. If not, it will defect until the threshold is met again.

### 3. Being Cooperative

The ATC strategy is fundamentally cooperative for the following reasons:

*   **Initiation of Cooperation:** It always starts by cooperating, demonstrating a willingness to engage in collective action.
*   **Reinforcement of Cooperation:** It rewards observed successful cooperation by continuing to cooperate, thereby reinforcing the positive behavior of the group.
*   **Strategic Patience:** When cooperation falters, it defect s not out of malice, but as a strategic pause. This pause aims to deter free-riding and signal that cooperation is the path to collective benefit. It's a cooperative "timeout" rather than a permanent defection.
*   **Commitment to Collective Good:** The strategy's ultimate goal is to reach the `m` threshold, which benefits all players (albeit defectors more in the short term if they exploit a successful cooperative effort). By aiming for this threshold, it aligns with the game's intended cooperative outcome.
*   **Adaptive Nature:** It learns from the group's behavior and adapts, rather than adhering to a rigid, potentially exploitable, pattern. This adaptability is key to long-term cooperative success in a dynamic environment.

### Pseudocode Representation:

```
// Global variables to track history
let history_cooperation_counts = [] // List of number of cooperators in each round
let n_players = n
let coop_threshold = m
let reward_factor = k
let total_rounds = r

function decide_action(current_round_number):
    // Handle first round
    if current_round_number == 1:
        return "C"

    // Get data from the previous round
    let num_cooperators_prev_round = history_cooperation_counts[current_round_number - 2] // Index for previous round
    let coop_rate_prev = num_cooperators_prev_round / n_players

    // Calculate rounds remaining
    let n_remaining = total_rounds - current_round_number + 1

    // Decision logic
    if coop_rate_prev >= (coop_threshold / n_players):
        // Previous round met or exceeded threshold, continue cooperating
        return "C"
    else:
        // Previous round fell short of threshold
        if n_remaining > 1:
            // Not the last round, defect to encourage better future coordination
            return "D"
        else:
            // Last round, cooperate to maximize potential payoff for this round
            return "C"

// Example of how history might be updated (this happens after actions are revealed)
function update_history(number_of_cooperators_this_round):
    history_cooperation_counts.append(number_of_cooperators_this_round)

```

**Key Strengths of ATC:**

*   **Resilience:** It's designed to recover from periods of low cooperation without permanently abandoning the cooperative ideal.
*   **Forward-Looking:** The consideration of remaining rounds makes its decisions strategic, not just reactive.
*   **Parameter-Aware:** The decision to cooperate or defect is directly tied to the game's parameters (`m` and `n`).
*   **Simplicity:** The logic is understandable and implementable.

This strategy aims to be a reliable and cooperative participant, fostering positive outcomes when possible while protecting itself from exploitation when the group fails to coordinate.
'''

description_COOPERATIVE_17 = '''
This is a fascinating Collective Risk Dilemma! Designing a robust cooperative strategy in the absence of communication, especially in a repeated game, requires a delicate balance of risk aversion, learning, and a commitment to cooperation when it's most beneficial.

Here's a cooperative strategy designed to be adaptive and robust:

## Strategy: "Conditional Reciprocity with Threshold Vigilance"

This strategy aims to be cooperative by default, but it also vigilantly monitors the group's performance and adjusts its behavior to safeguard against defection and to capitalize on collective success.

### Core Principles:

1.  **Cooperative Default:** Assume cooperation is the best path to maximizing collective payoff when the threshold is met.
2.  **Reciprocity:** Reward cooperation with cooperation, and punish defection with defection.
3.  **Threshold Vigilance:** Be acutely aware of the number of cooperators in *past* rounds and how close the group came to reaching the threshold.
4.  **Risk Aversion (Early Stages):** In the initial rounds, when information is scarce, err on the side of caution to avoid being exploited.
5.  **Exploiting Opportunity (Later Stages):** As confidence grows and the group demonstrates a willingness to cooperate, become more aggressive in contributing to reach the threshold.

### Decision Rules:

The decision to Cooperate (C) or Defect (D) for player `i` in round `t` will depend on a few factors:

1.  **Initial Rounds (Learning Phase):**
    *   **Decision:** Cooperate (C).
    *   **Rationale:** In the early rounds, we have no history to inform our decisions. Assuming cooperation is the best strategy to *try* and reach the threshold and unlock the reward is the most cooperative stance. This also provides valuable data about the group's behavior.

2.  **Subsequent Rounds (Adaptive Phase):**
    Let:
    *   `C_t-1` be the number of players who cooperated in round `t-1`.
    *   `Total_Cooperators_History` be the total number of players who have cooperated across all *previous* rounds (a cumulative measure of cooperation).
    *   `Threshold_Reached_Ever` be a boolean flag, true if the threshold `m` has been met in at least one previous round.

    *   **If `Threshold_Reached_Ever` is TRUE:**
        *   **Decision:** Cooperate (C).
        *   **Rationale:** The group has proven it can achieve the cooperative outcome. This signals a higher probability of future cooperation and a desire for the reward. By cooperating, we reinforce this behavior.

    *   **If `Threshold_Reached_Ever` is FALSE:**
        *   **Consider `C_t-1` and `m`:**
            *   **If `C_t-1 >= m`:**
                *   **Decision:** Cooperate (C).
                *   **Rationale:** The threshold was met in the *immediate* previous round. This is a strong signal of collective success and willingness to cooperate. We should continue to foster this.
            *   **If `C_t-1 < m` but `C_t-1 >= m - 2` (i.e., close to the threshold):**
                *   **Decision:** Cooperate (C).
                *   **Rationale:** The group is *almost* there. To tip the scales and unlock the reward, a bit more commitment from a few players is needed. This is a calculated risk to achieve a high payoff for everyone.
            *   **If `C_t-1 < m - 2` (i.e., far from the threshold):**
                *   **Decision:** Defect (D).
                *   **Rationale:** The group has consistently failed to reach the threshold, and there's no strong indication that the current state is leading to cooperation. Continuing to cooperate in this scenario might lead to repeated exploitation (others defecting while you cooperate, yielding you a payoff of 0). This is a cautious response to a persistent lack of collective success.

3.  **The Last Round (`t = r`):**
    *   **Decision:** Cooperate (C).
    *   **Rationale:** In the final round, there's no future to consider. The optimal strategy is to maximize the immediate payoff for this round. If the group can still reach the threshold, cooperating is the best way to secure the `k` reward. If the group cannot reach the threshold, the payoff is `1-c_i`. Cooperating yields `1-1=0` (if the threshold isn't met) or `1-1+k=k` (if the threshold is met). Defecting yields `1-0=1`. Therefore, if `k > 1`, cooperation is always better or equal in the last round if the threshold is *potentially* reachable. Even if the threshold is unlikely to be met, the payoff from C is 0, and from D is 1. *However*, given the *cooperative* design principle, even in the last round, we will stick to the general adaptive phase rules above. If `Threshold_Reached_Ever` is true, or if `C_t-1 >= m-2`, we cooperate. Otherwise, we defect. This prioritizes a cooperative signal even at the cost of a potentially higher individual payoff in the last round if collective failure is certain. *Self-correction: The initial rationale for always cooperating in the last round is flawed if collective failure is certain. The adaptive phase rules are more robust here.*

### Handling Edge Cases:

*   **First Round (`t = 1`):** As stated, always Cooperate (C). This is the learning phase.
*   **Last Round (`t = r`):** Apply the "Adaptive Phase" rules described above. The game is still a repeated game, and past behavior influences the current decision.
*   **Threshold `m` is very high (close to `n`):** The strategy will naturally become more cautious. If `m = n`, it's highly unlikely the threshold will ever be met if even one player defects. The strategy will lean towards defecting more often in the adaptive phase if cooperation isn't demonstrated early.
*   **Threshold `m` is very low (close to 1):** The strategy will be more inclined to cooperate, as it's easier to reach the threshold.

### Pseudocode Representation:

```pseudocode
// Strategy: Conditional Reciprocity with Threshold Vigilance

// Game Parameters (available to the strategy)
let n be the number of players
let r be the number of rounds
let m be the minimum cooperators needed
let k be the reward if threshold met factor

// State Variables (maintained by the strategy)
let current_round = 1
let Threshold_Reached_Ever = false
let C_last_round = 0 // Number of cooperators in the previous round

function decide_action():
    // --- Learning Phase ---
    if current_round == 1:
        action = C // Cooperate in the first round
        log_cooperation(1) // Record that this player cooperated
        return action

    // --- Adaptive Phase ---
    else:
        if Threshold_Reached_Ever:
            action = C // Threshold has been met before, reinforce cooperation
        else:
            // Check how close we were in the last round
            if C_last_round >= m:
                action = C // Threshold was met last round, reinforce
            else if C_last_round >= m - 2 and m > 2: // Need to be careful if m is very small (e.g., m=2)
                action = C // Close to the threshold, make an effort
            else:
                action = D // Consistently far from threshold, defect to protect oneself

        log_cooperation(1 if action == C else 0) // Record player's action for this round
        return action

// --- Helper functions to update state based on round outcome ---
function update_strategy_state(round_results):
    // round_results is a list of actions for the previous round (e.g., [C, D, C, ...])
    let cooperators_this_round = count_cooperators(round_results)

    C_last_round = cooperators_this_round

    if cooperators_this_round >= m:
        Threshold_Reached_Ever = true

    current_round = current_round + 1

// --- Internal logging (for strategy's own memory) ---
// In a real implementation, this would be a data structure storing historical actions.
// For this description, we're simplifying to the key state variables.
function log_cooperation(cooperated_value):
    // This function would update the strategy's internal memory,
    // e.g., add cooperators_this_round to a history list,
    // or update Total_Cooperators_History.
    // For simplicity in this pseudocode, we focus on the immediate needs.
    pass // Placeholder

```

### Explanation of Cooperativeness:

This strategy is inherently cooperative because:

*   **It prioritizes achieving the collective reward (`k`)**: By cooperating by default and taking calculated risks to reach the threshold, it aims to maximize *shared* gains.
*   **It exhibits conditional cooperation**: It doesn't blindly cooperate. It learns from the group's behavior. If the group consistently fails to cooperate, it shifts to self-preservation (defecting) to avoid being a "sucker." This is a pragmatic form of cooperation that aims to foster a sustainable cooperative environment, rather than being exploited to death.
*   **It is reciprocal**: It rewards observed cooperation with its own cooperation. This is a fundamental element of cooperative game theory.
*   **It has a strong incentive to cooperate when the threshold is near**: The decision to cooperate when `C_last_round >= m - 2` is a direct demonstration of its willingness to contribute to a group success that benefits everyone.

This strategy aims to be a "good citizen" in the tournament, fostering cooperation when possible, but also exhibiting resilience and self-interest when collective action fails to materialize. It balances the pursuit of mutual benefit with the need to protect itself from exploitation.
'''

description_COOPERATIVE_18 = '''
This is a fascinating challenge! Designing a cooperative strategy for the Collective Risk Dilemma requires balancing the incentive to defect for personal gain with the potential for a significantly higher collective reward. The key is to be cautiously optimistic, adapt to the environment, and prioritize cooperation when it's most likely to be beneficial.

Here's a strategy designed to be cooperative, adaptive, and robust:

## Strategy: "Cautious Collective Trust" (CCT)

This strategy aims to foster cooperation by initially trusting others, but quickly adapting to observed behavior. It prioritizes achieving the cooperation threshold ($m$) without being overly exploitative when cooperation fails.

### Core Principles:

1.  **Initial Trust:** Start with an assumption that others might cooperate.
2.  **Adaptive Learning:** Continuously monitor the observed cooperation rate.
3.  **Threshold-Driven Cooperation:** Cooperate when the probability of reaching the threshold is high.
4.  **Punishment for Defection (Subtle):** Reduce cooperation if sustained defection is observed, but not to a degree that guarantees failure for everyone.
5.  **Reward for Cooperation:** Increase cooperation when others reciprocate.
6.  **Endgame Consideration:** Adjust behavior in the final round to maximize individual payoff, while still acknowledging the history.

### 1. Decision Rules:

The decision to cooperate or defect in round $t$ (where $t$ ranges from 1 to $r$) depends on a combination of factors:

*   **Number of Cooperators in the Previous Round ($C_{t-1}$):** The actual count of players who cooperated in the immediately preceding round.
*   **My Previous Action ($A_{i, t-1}$):** Whether I cooperated or defected in the previous round.
*   **Observed Cooperation Rate in Previous Rounds:** A moving average or weighted average of cooperation across previous rounds.
*   **Round Number ($t$):** To handle initial and final round dynamics.

**Decision Logic for Player $i$ in Round $t$:**

Let $N_C(t-1)$ be the number of players who cooperated in round $t-1$.

*   **If $t = 1$ (First Round):**
    *   **Cooperate (C).**
    *   *Rationale:* Initiate cooperation. The first round is crucial for setting a cooperative tone. With no history, it's best to be optimistic.

*   **If $t > 1$ and $N_C(t-1) \ge m$:**
    *   **Cooperate (C).**
    *   *Rationale:* If the threshold was met in the previous round, it's highly beneficial for everyone to continue cooperating. The reward ($k$) outweighs the individual cost of cooperating.

*   **If $t > 1$ and $N_C(t-1) < m$ (Threshold not met in previous round):**
    *   This is where adaptiveness comes in. We need to assess the likelihood of reaching the threshold.
    *   **Calculate a "Trust Score" ($TS_t$):** This score reflects my confidence in others cooperating enough to meet the threshold. A simple approach:
        *   Let $\overline{C_{hist}} = \frac{1}{t-1} \sum_{j=1}^{t-1} N_C(j)$ be the average number of cooperators in *all* previous rounds.
        *   A more sophisticated approach might use a weighted average, giving more weight to recent rounds. For simplicity, let's stick to the average for now.
        *   $TS_t = \overline{C_{hist}} / (n-1)$ (normalize by the maximum possible number of *other* cooperators). This gives a fraction representing the average cooperation level of *others* in the past.

    *   **Decision Rule:**
        *   **If $TS_t \times (n-1) + 1 \ge m$ AND $N_C(t-1) \ge m-1$ (i.e., *I* am the only one potentially needed to tip the balance) AND I cooperated last round ($A_{i, t-1} == C$):**
            *   **Cooperate (C).**
            *   *Rationale:* If the average historical cooperation, combined with my own recent cooperation, suggests we are very close to or have just missed the threshold, and if I am needed as the *last piece of the puzzle* to reach $m$, I will cooperate again, especially if I cooperated last round and it didn't result in a failed round. This shows continued commitment.
        *   **If $TS_t \times (n-1) \ge m - 1$ (i.e., others' *average* behavior is close to $m-1$) AND $N_C(t-1) \ge m-1$ AND I defected last round ($A_{i, t-1} == D$):**
            *   **Cooperate (C).**
            *   *Rationale:* If others are consistently cooperating at a high rate (averaging close to $m-1$), and I defected last round (which likely contributed to failure), I should try cooperating again to see if their high cooperation can be sustained and perhaps achieve the threshold this round.
        *   **If $TS_t \times (n-1) < m - 1$ (i.e., others' *average* behavior is significantly below $m-1$) OR $N_C(t-1) < m - 1$ AND my previous action was C:**
            *   **Defect (D).**
            *   *Rationale:* If historical data suggests a low probability of reaching the threshold, or if previous rounds have consistently failed to reach it despite my cooperation, it's time to protect my individual endowment. This is a signal that the collective is not cooperating effectively.
        *   **Default (if none of the above apply, or if I defected last round and the situation is ambiguous):**
            *   **Defect (D).**
            *   *Rationale:* If the situation doesn't strongly indicate cooperation is likely to be successful, or if I have been exploited (i.e., I cooperated and the threshold was not met), it's safer to defect.

*   **If $t = r$ (Last Round):**
    *   **Analyze the current state of $N_C(t-1)$ and $TS_t$.**
    *   **If $N_C(t-1) \ge m$:**
        *   **Cooperate (C).**
        *   *Rationale:* The reward is guaranteed and the game is over. No reason not to cooperate.
    *   **If $N_C(t-1) < m$:**
        *   **Check if $N_C(t-1) + 1 == m$ (i.e., if *my* cooperation would exactly meet the threshold).**
            *   **If yes:** **Cooperate (C).**
            *   *Rationale:* This is the last chance to secure the collective reward. It's a calculated risk that might pay off if others also cooperated to get close.
        *   **If no (i.e., even if I cooperate, the threshold won't be met):**
            *   **Defect (D).**
            *   *Rationale:* Since the threshold cannot be met with my cooperation alone, and there are no future rounds to influence, the optimal strategy for the last round is to secure my private endowment.
    *   *Note:* The endgame logic is crucial. It shifts from influencing future rounds to maximizing current payoff.

### 2. Handling Edge Cases:

*   **First Round ($t=1$):** As stated above, always **Cooperate (C)**. This is an investment in future cooperation.

*   **Last Round ($t=r$):** The logic is detailed in the decision rules above. It prioritizes achieving the reward if possible, otherwise maximizing individual gain.

*   **Low $m$ values (e.g., $m=2$):** The strategy should still work. With a low $m$, cooperation is easier to achieve, and the "Trust Score" will likely rise faster, encouraging more cooperation.

*   **High $m$ values (e.g., $m \approx n$):** The strategy becomes more conservative. It will be harder to reach the threshold, so the "Trust Score" will likely remain low, leading to more defections as the strategy protects itself.

*   **Very high $k$ values:** The strategy will be more willing to take risks and cooperate, as the reward for success is so substantial.

*   **Very low $k$ values:** The strategy will be more risk-averse and defect more readily, as the reward for cooperation is less compelling.

### 3. Being Cooperative:

The strategy is fundamentally cooperative in its intent:

*   **Initiation of Cooperation:** It starts by cooperating, signaling a willingness to build trust and achieve collective success.
*   **Reciprocity:** It rewards observed cooperation by increasing its own propensity to cooperate. If others are playing cooperatively and getting close to the threshold, CCT will try to meet them.
*   **Threshold Focus:** It is primarily driven by the desire to reach the collective threshold ($m$), which is the definition of collective success in this game. It only defects when it strongly believes cooperation is unlikely to succeed, or when it's no longer advantageous (last round).
*   **Adaptive but Not Exploitative:** While it learns and adapts, it avoids a purely tit-for-tat or retaliatory approach that could lead to cycles of defection. Instead, it uses observed behavior to inform future cooperative attempts, not just to punish. It tries to "pull" others towards cooperation rather than just reacting to defection.

### Pseudocode Representation:

```python
def decide_action(player_id, current_round, n, m, k, history):
    """
    Determines the action (C or D) for player i in the current_round.

    Args:
        player_id (int): The ID of the player (not strictly needed for this strategy).
        current_round (int): The current round number (1-indexed).
        n (int): Total number of players.
        m (int): Minimum cooperators needed.
        k (float): Reward factor.
        history (list of dict): List of past round results. Each dict has:
            'actions': list of actions for all players in that round (e.g., ['C', 'D', ...])
            'payoffs': list of payoffs for all players in that round

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    if current_round == 1:
        return 'C'

    # Analyze the previous round's outcome
    prev_round_data = history[-1]
    num_cooperators_prev_round = prev_round_data['actions'].count('C')
    my_last_action = prev_round_data['actions'][player_id] # Assuming player_id is index

    # Analyze historical cooperation
    total_cooperators_history = 0
    num_past_rounds = len(history)
    for round_data in history:
        total_cooperators_history += round_data['actions'].count('C')

    # Calculate average cooperators in *all* past rounds
    avg_cooperators_all_past_rounds = total_cooperators_history / num_past_rounds if num_past_rounds > 0 else 0

    # --- Decision Logic ---

    # Case 1: Threshold was met in the previous round
    if num_cooperators_prev_round >= m:
        return 'C'

    # Case 2: Last round - pure self-interest after checking threshold
    if current_round == r: # Assuming 'r' is accessible or passed as param
        if num_cooperators_prev_round >= m:
            return 'C'
        if num_cooperators_prev_round + 1 == m: # My cooperation would meet it
            return 'C'
        else: # Cannot meet threshold even with my cooperation
            return 'D'

    # Case 3: Threshold NOT met in previous round (adaptive logic)

    # Trust Score (normalized average cooperation of others across all past rounds)
    # For simplicity, using average of ALL past rounds. A more complex version could use recent history.
    # We consider the average number of *other* players who cooperated.
    # If avg_cooperators_all_past_rounds is 5, and n=6, then on average 5 people cooperated.
    trust_score_others = avg_cooperators_all_past_rounds

    # Condition: Am I crucial? Do others show good promise?
    # If average cooperation rate of others + my last action would meet m
    # Or if others are consistently cooperating near m-1

    # High promise based on average and my prior cooperation
    if (trust_score_others + (1 if my_last_action == 'C' else 0)) >= (m - 1) and num_cooperators_prev_round >= m - 1 :
        return 'C'

    # High promise based on average and I defected, but others are high
    if (trust_score_others >= m - 1) and num_cooperators_prev_round >= m - 1 and my_last_action == 'D':
         return 'C'


    # Low promise: If average cooperation is significantly low, or if my cooperation didn't help.
    if trust_score_others < m - 2 or num_cooperators_prev_round < m - 1 and my_last_action == 'C':
        return 'D'

    # Default to Defect if conditions aren't strongly pushing for cooperation
    return 'D'

```

This strategy, "Cautious Collective Trust," aims to be a robust and cooperative player. It starts with good faith, learns from experience, and makes calculated decisions to maximize the chances of collective success while protecting itself from exploitation.
'''

description_COOPERATIVE_19 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust in a tournament setting. This strategy emphasizes the long-term benefits of cooperation while mitigating the risks of exploitation.

## Cooperative Strategy: "Adaptive Risk Aversion with Tit-for-Tat"

This strategy is built on the principle of "cooperate until defected upon, then retaliate, but forgive if cooperation resumes." It adapts to the observed behavior of the group, aiming to maintain cooperation when beneficial and protect itself when others defect.

### Core Principles:

1.  **Prioritize Cooperation:** The default action is to cooperate, as achieving the threshold `m` yields a higher payoff for everyone.
2.  **Monitor Group Behavior:** Track the number of cooperators in previous rounds to assess the group's willingness to cooperate.
3.  **Punish Defection:** If the group fails to meet the cooperation threshold due to defections, temporarily defect to signal disapproval and discourage future defections.
4.  **Forgive and Rejoin:** If the group returns to cooperation after a period of defection, rejoin the cooperative effort.
5.  **Strategic Patience:** Understand that deviations from cooperation can occur and don't necessarily signal a permanent shift.

### 1. Decision Rules:

The decision for player `i` in round `t` is based on the observed behavior in round `t-1` and the game parameters.

Let:
*   `C_t-1` be the number of players who cooperated in round `t-1`.
*   `MyAction_t-1` be player `i`'s action in round `t-1`.
*   `MyPayoff_t-1` be player `i`'s payoff in round `t-1`.

**For Round `t` (where `t > 1`):**

*   **If `C_t-1 >= m` (Group cooperated successfully in the previous round):**
    *   **And if `MyAction_t-1 == C`:** **Cooperate (C)**. Continue the cooperative path, as it is beneficial.
    *   **And if `MyAction_t-1 == D`:** **Cooperate (C)**. This scenario is unlikely if the group succeeded, but if it did, it means my defection didn't prevent success. I'll rejoin cooperation to benefit from the reward.

*   **If `C_t-1 < m` (Group failed to cooperate in the previous round):**
    *   **And if `MyAction_t-1 == C`:** This means I cooperated, but the group still failed. This suggests a critical number of other players defected.
        *   **If `C_t-1 >= m - 1` (i.e., only one cooperator was missing from the threshold):** **Cooperate (C)**. There's a high chance that one more cooperator would have tipped the balance. I'll try again.
        *   **If `C_t-1 < m - 1` (i.e., multiple cooperators were missing):** **Defect (D)**. My cooperation was not enough to reach the threshold, and the group's behavior indicates a significant propensity for defection. I need to protect my endowment and signal that this level of defection is unacceptable.
    *   **And if `MyAction_t-1 == D`:** This means I defected, and the group still failed. This confirms the group's general tendency towards defection.
        *   **If `C_t-1 >= m - 1`:** **Cooperate (C)**. This is a crucial signal. If the group *almost* met the threshold while I defected, it implies that a significant number of others *did* cooperate. This is an opportunity to re-establish cooperation. I will cooperate to try and pull the group back to success.
        *   **If `C_t-1 < m - 1`:** **Defect (D)**. The group is consistently failing to cooperate, and my defection is aligned with this trend. I will continue to defect to safeguard my endowment.

### 2. Handling Edge Cases:

*   **Round 1 (`t = 1`):**
    *   **Decision Rule:** **Cooperate (C)**.
    *   **Rationale:** In the absence of any history, the most cooperative and optimistic approach is to initiate cooperation. This maximizes the chance of achieving the reward from the very first round. It's a signal of goodwill.

*   **Last Round (`t = r`):**
    *   **Decision Rule:** **Defect (D)**.
    *   **Rationale:** In the final round, there are no future rounds to incentivize cooperation. The payoff is purely deterministic. To maximize individual gain in the final round, a player should always defect, as there's no risk of future retaliation or benefit from future cooperation. This is a necessary adaptation to the finite nature of the game.

*   **Low `k` values (e.g., `k` close to 1):** If `k` is only slightly greater than 1, the reward for cooperation is marginal compared to the guaranteed payoff of defection. In such cases, the strategy should lean more towards defection, as the risk-reward calculation becomes less favorable for cooperation. *However, since the prompt asks for a cooperative strategy, this extreme case might lead to a more aggressive "defect if reward is not significantly high" rule, but for a truly cooperative strategy, we'll stick to the core rules above.*

*   **Very High `m` (e.g., `m` close to `n`):** If `m` is very close to `n`, it becomes extremely difficult for any player to guarantee reaching the threshold alone. The strategy will naturally become more cautious and might default to defection more often if historical data suggests low cooperation. The current rules implicitly handle this by assessing `C_t-1`.

### 3. Being Cooperative:

*   **Intent:** The strategy's primary intention is to foster and sustain cooperation whenever it is mutually beneficial. It prioritizes the collective good as long as it doesn't lead to consistent exploitation.
*   **Initiation:** It starts by cooperating, offering the benefit of the doubt.
*   **Reciprocity:** It mirrors the group's cooperative behavior. If the group cooperates, it cooperates. If the group defects consistently, it defects in response.
*   **Forgiveness:** It is willing to return to cooperation even after periods of defection, provided there's evidence of a shift back towards the cooperative norm. This "forgiveness" is key to re-establishing collective benefit.
*   **Adaptive Nature:** The strategy doesn't rigidly adhere to a fixed pattern. It learns from the history of the game and adapts its behavior, making it robust against a variety of player types, including those who defect strategically, randomly, or are simply indifferent.

### Pseudocode:

```
Strategy_AdaptiveRiskAversion_TitForTat(game_parameters, game_history)

  n = game_parameters.n
  m = game_parameters.m
  r = game_parameters.r
  k = game_parameters.k

  current_round = game_history.current_round
  previous_round_actions = game_history.get_actions_for_round(current_round - 1) # List of actions of all players in t-1
  my_previous_action = game_history.get_my_action(current_round - 1)

  # Handle first round
  if current_round == 1:
    return C

  # Handle last round
  if current_round == r:
    return D

  # Calculate number of cooperators in previous round
  cooperators_in_previous_round = count_cooperators(previous_round_actions)

  # Decision logic for rounds t > 1 and t < r
  if cooperators_in_previous_round >= m:
    # Group cooperated successfully
    return C
  else:
    # Group failed to cooperate
    if my_previous_action == C:
      # I cooperated, but group failed
      if cooperators_in_previous_round >= m - 1:
        # Only one cooperator missing, try again
        return C
      else:
        # Multiple cooperators missing, signal dissatisfaction
        return D
    else: # my_previous_action == D
      # I defected, and group failed
      if cooperators_in_previous_round >= m - 1:
        # Group almost met threshold, re-establish cooperation
        return C
      else:
        # Group consistently failing, continue defection
        return D

# Helper function to count cooperators from a list of actions
function count_cooperators(actions_list):
  count = 0
  for action in actions_list:
    if action == C:
      count = count + 1
  return count
```

This strategy aims to be a "good citizen" of the game, striving for mutually beneficial outcomes while having a clear and consistent response to ensure its own survival and discourage persistent defection. Its adaptability to group behavior, coupled with a basic tit-for-tat mechanism and specific handling of the first and last rounds, makes it a robust candidate for a cooperative strategy.
'''

description_COOPERATIVE_20 = '''
Here's a design for a cooperative strategy for the Collective Risk Dilemma, focusing on adaptiveness and robustness without relying on pre-established communication or norms.

## Strategy: "Adaptive Cooperation with Threshold Monitoring"

This strategy aims to foster cooperation by monitoring the collective behavior and adjusting its own actions accordingly, while always trying to contribute to reaching the cooperative threshold. It prioritizes achieving the higher reward when possible but also guards against being exploited.

### Core Principles:

1.  **Initial Tentative Cooperation:** In the early stages, the strategy will lean towards cooperation to signal willingness and to test the waters.
2.  **Threshold Monitoring:** The primary driver for cooperation is the observed number of cooperators in previous rounds. The strategy aims to ensure the *minimum* required number of cooperators (`m`) is met or exceeded.
3.  **Defection as a Response to Failure:** If cooperation consistently fails to reach the threshold, the strategy will reduce its cooperation to protect its individual endowment.
4.  **Exploitation Aversion:** While aiming for cooperation, the strategy must avoid being the sole cooperator if others are defecting, as this leads to a poor outcome for the individual.
5.  **End-Game Adjustment:** The strategy acknowledges the unique incentives in the final round and adjusts its behavior to maximize its own payoff in that context.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on the observed history of cooperation. Let `N_C(t-1)` be the number of players who cooperated in round `t-1`.

*   **In Round `t` (where `t > 1`):**
    *   **If `N_C(t-1) >= m`:** This means the collective succeeded in reaching the cooperative threshold in the previous round. The strategy will **Cooperate (C)** in round `t`. This reinforces successful cooperation and aims to continue reaping the rewards.
    *   **If `m > N_C(t-1) >= 1`:** The cooperative threshold was not met, but at least one other player cooperated. The strategy will **Cooperate (C)** in round `t`. This shows continued commitment to cooperation and signals to others that cooperation is still being attempted. The hope is that this sustained effort might tip the balance.
    *   **If `N_C(t-1) == 0`:** No one cooperated in the previous round. The strategy will **Defect (D)** in round `t`. This is a protective measure, as continuing to cooperate alone would be self-defeating.
    *   **If `1 <= N_C(t-1) < m` AND (the number of defectors in the previous round is `n - N_C(t-1) >= n - m + 1`)** (i.e., a significant majority defected, even if some cooperated): The strategy will **Defect (D)** in round `t`. This is a more nuanced defection, triggered when cooperation is significantly outnumbered, even if not entirely absent. This prevents being the only cooperator in a strong defection environment.

*   **In Round `t` (where `t == 1` - First Round):**
    *   **Cooperate (C)**. The strategy begins by signaling its willingness to cooperate, hoping to initiate a cooperative norm or at least gather information on other players' inclinations.

*   **In Round `t` (where `t == r` - Last Round):**
    *   **Defect (D)**. In the final round, there is no future to incentivize cooperation. The optimal strategy for an individual player is to defect, as this guarantees them a payoff of 1 (their endowment) plus the reward `k` if the threshold is met (which they don't need to contribute to achieving, as there's no future consequence). This is a standard unravelling in finite repeated games.

### 2. Handling Edge Cases:

*   **First Round (`t=1`):** Always Cooperate (C). This is the initial signal of intent.
*   **Last Round (`t=r`):** Always Defect (D). This is the standard rational response in a finitely repeated game when the future is no longer a factor.
*   **Early Rounds (e.g., first `m-1` rounds if `m` is large):** The strategy will continue to Cooperate (C) as long as the threshold hasn't been met, but it will be observing closely. If, after a few rounds, cooperation still consistently fails to reach `m` and the number of defectors is overwhelmingly large (as per the nuanced defection rule above), it might switch to Defect (D) sooner to protect itself.
*   **Sudden Shifts in Opponent Behavior:** The strategy is adaptive. If a group that was cooperating suddenly shifts to defecting, the strategy will detect this via `N_C(t-1)` and adjust its behavior accordingly, likely switching to Defect (D) to avoid exploitation.

### 3. Being Cooperative:

The "Adaptive Cooperation with Threshold Monitoring" strategy is fundamentally cooperative in its intent:

*   **Proactive Cooperation:** It starts by cooperating, actively trying to initiate or maintain a cooperative environment.
*   **Enabling Collective Benefit:** Its decision to cooperate is heavily influenced by the *potential* for collective success (reaching `m` cooperators). It wants to contribute to that success because the reward `k` for the collective is significant.
*   **Resilience in Cooperation:** Even if the threshold isn't met in the previous round, if there was *some* cooperation, it will continue to cooperate for a while longer, giving the group another chance to reach the threshold. This demonstrates a commitment beyond just immediate self-interest.
*   **Avoiding Exploitation, Not Seeking It:** Defection is primarily a defensive mechanism. It's a response to the group's inability to cooperate effectively or to clear signals of widespread defection, which would otherwise lead to a worse individual outcome. The strategy doesn't aim to "trick" others into cooperating so it can defect; it aims to participate in cooperation if it's viable.

### Pseudocode:

```pseudocode
Strategy: AdaptiveCooperationWithThresholdMonitoring

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

Variables:
  cooperation_history = []  // List to store number of cooperators in each past round
  my_actions = []           // List to store my own past actions

Function DecideAction(current_round):
  // Handle the last round: always defect
  if current_round == r:
    return D

  // Handle the first round: always cooperate
  if current_round == 1:
    return C

  // For rounds between the first and last:
  // Get the number of cooperators from the previous round
  if len(cooperation_history) == 0: // Should not happen for current_round > 1
      last_round_cooperators = 0
  else:
      last_round_cooperators = cooperation_history[-1]

  // Get the total number of players who defected in the last round
  if len(cooperation_history) == 0: // Should not happen for current_round > 1
      last_round_defectors = 0
  else:
      last_round_defectors = n - last_round_cooperators

  // Decision Logic:

  // If threshold was met or exceeded in the last round, cooperate
  if last_round_cooperators >= m:
    return C

  // If some players cooperated (but not enough), still cooperate to encourage
  if last_round_cooperators >= 1:
    return C

  // If no one cooperated in the last round, defect to protect endowment
  if last_round_cooperators == 0:
    return D

  // More nuanced defection: if cooperation is significantly outnumbered
  // This condition is technically covered by the above, but can be made explicit:
  // if 1 <= last_round_cooperators < m and last_round_defectors >= (n - m + 1):
  //   return D

  // Default (should be covered, but as a fallback for safety)
  return D

Function RecordHistory(action_taken, number_of_cooperators_this_round):
  my_actions.append(action_taken)
  cooperation_history.append(number_of_cooperators_this_round)

// --- Tournament Execution Loop (Conceptual) ---
// For each round t from 1 to r:
//   my_decision = DecideAction(t)
//   Send my_decision to game engine
//   Receive actual_actions_of_all_players_this_round from game engine
//   Calculate number_of_cooperators_this_round = count(action == C for action in actual_actions_of_all_players_this_round)
//   RecordHistory(my_decision, number_of_cooperators_this_round)
```

This strategy balances the desire for collective gain with the necessity of individual protection, adapting dynamically to the unfolding cooperation landscape.
'''

description_COOPERATIVE_21 = '''
This is a fascinating challenge! Designing a cooperative strategy in a repeated Collective Risk Dilemma without communication requires balancing the desire to achieve the collective reward with the inherent incentive to defect and secure a private payoff, especially when others might be doing the same.

Here's a cooperative strategy designed to be adaptive and robust, focusing on achieving the collective reward while minimizing individual risk:

## Strategy: The "Cautious Reciprocator with Adaptive Threshold"

This strategy is designed to be cooperative by aiming for the collective reward, but it's also cautious. It learns from the past and adjusts its behavior based on the observed cooperation levels, aiming to trigger the reward while protecting itself from being exploited.

### Core Principles:

1.  **Aim for Cooperation:** The primary goal is to reach the cooperation threshold ($m$) to secure the collective reward ($k$).
2.  **Observe and Learn:** The strategy will track the number of cooperators in previous rounds.
3.  **Reciprocate Cooperation:** Reward cooperation with continued cooperation.
4.  **Punish Defection (Mildly):** Respond to widespread defection with a period of defecting to avoid being exploited, but not indefinitely.
5.  **Adaptive Threshold Adjustment:** Slightly lower the perceived "required" cooperation from others if the group consistently falls short but shows some effort.

### 1. Decision Rules:

**General Rule (Rounds t > 1):**

*   **If** the number of cooperators in the *previous* round ($C_{t-1}$) was **greater than or equal to** $m$:
    *   **Play C (Cooperate).** This is a direct reciprocation of successful cooperation.
*   **Else if** the number of cooperators in the *previous* round ($C_{t-1}$) was **less than** $m$, but **greater than or equal to** a dynamically adjusted "minimum observed cooperation" threshold ($M'_{t-1}$):
    *   **Play C (Cooperate).** This indicates some effort from others, and we are willing to continue trying to reach the threshold, especially if $C_{t-1}$ is getting close to $m$.
*   **Else (i.e., $C_{t-1}$ is significantly below $m$ and below $M'_{t-1}$):**
    *   **Play D (Defect).** This is a defensive measure to avoid being the sole cooperator or one of very few cooperators, thus securing a private payoff of 1.

**Dynamic Minimum Observed Cooperation Threshold ($M'_t$):**

This threshold helps the strategy be more forgiving and adaptive.

*   **Initialize:** In round 1, this isn't used. After round 1, initialize $M'_1 = m - \lfloor (m-1)/2 \rfloor$. This is roughly half of the *minimum* required cooperators minus one, aiming to be forgiving if at least some players (but not enough) cooperate.
*   **Update Rule:** After round $t-1$:
    *   **If** the number of cooperators in round $t-1$ ($C_{t-1}$) was **less than** $m$ AND **greater than or equal to** $M'_{t-1}$:
        *   $M'_t = \max(1, M'_{t-1} - 1)$. This means we are slightly more forgiving and willing to cooperate if the previous round had some cooperation, even if it wasn't enough. We reduce the "minimum observed cooperation" threshold by 1, but it cannot go below 1.
    *   **Else if** the number of cooperators in round $t-1$ ($C_{t-1}$) was **less than** $M'_{t-1}$:
        *   $M'_t = M'_{t-1}$. The threshold remains the same, as there was insufficient cooperation to warrant being more forgiving.
    *   **Else (i.e., $C_{t-1} \ge m$):**
        *   $M'_t = m$. If the threshold was met, we reset the dynamic threshold to the actual requirement, as the current level of cooperation is sufficient.

### 2. Handling Edge Cases:

*   **Round 1 (Initial Round):**
    *   **Default Action:** **Play C (Cooperate).** In the very first round, there is no history. The strategy takes an initial cooperative stance to *try* and trigger the reward. This is a gamble, but a necessary one for a cooperative strategy to initiate cooperation.
*   **Last Round (Round r):**
    *   **Rule:** The same general decision rules apply based on the history of rounds $1$ to $r-1$. The strategy still aims to cooperate if the prior round was cooperative or showed sufficient effort. There is no special rule for the last round because the payoff is cumulative. If cooperation was achieved in previous rounds, the benefit is already accrued. If not, the last round is an opportunity to try one last time based on the established pattern.

### 3. Being Cooperative:

*   **Intent:** The strategy's core design is to facilitate the achievement of the collective reward ($k$). It prioritizes reaching the threshold ($m$) over purely maximizing its individual payoff in the short term.
*   **Reciprocity:** It actively rewards observed cooperation, creating a positive feedback loop.
*   **Forgiveness (Adaptive Threshold):** The $M'_t$ mechanism makes the strategy somewhat forgiving. It doesn't immediately abandon cooperation if the group narrowly misses the threshold. It's willing to try again as long as there's a reasonable level of cooperation.
*   **Robustness:** By defecting when cooperation levels are critically low, it protects itself from being exploited by purely selfish players who would always defect and benefit from others' cooperation. This ensures the strategy can survive and continue to participate constructively.

---

### Pseudocode for "Cautious Reciprocator with Adaptive Threshold":

```
// Strategy State Variables
// (These would be persistent across rounds for a given player)
integer previous_cooperators_count = 0
float dynamic_min_cooperation_threshold = 0

// Game Parameters (known to all)
integer n // number of players
integer r // number of rounds
integer m // minimum cooperators needed
float k // reward if threshold met factor

// Strategy Function for Round 't'
function decide_action(t, historical_actions_per_round)
    // historical_actions_per_round is a list of lists, where each inner list
    // contains the actions of all players in a past round.
    // The count of 'C' in the last round (t-1) can be derived from this.

    // --- Initialization (Round 1) ---
    if t == 1 then
        // Initialize state for the first round
        previous_cooperators_count = 0 // No history yet
        // Initialize dynamic threshold: m - floor((m-1)/2)
        // Example: if m=3, M' = 3 - floor(2/2) = 3 - 1 = 2
        // Example: if m=4, M' = 4 - floor(3/2) = 4 - 1 = 3
        dynamic_min_cooperation_threshold = max(1, m - floor((m - 1) / 2))
        return C // Default to cooperate in the first round

    // --- Update State based on Previous Round (t-1) ---
    // Calculate the actual number of cooperators in round t-1
    if t > 1 then
        // Get actions from round t-1 from historical_actions_per_round
        actions_round_t_minus_1 = historical_actions_per_round[t-1] // Assuming 1-indexed rounds
        actual_cooperators_round_t_minus_1 = count_cooperators(actions_round_t_minus_1)
        previous_cooperators_count = actual_cooperators_round_t_minus_1

        // Update dynamic_min_cooperation_threshold
        if actual_cooperators_round_t_minus_1 < m then
            if actual_cooperators_round_t_minus_1 >= dynamic_min_cooperation_threshold then
                // If we were close to the dynamic threshold, become more forgiving
                dynamic_min_cooperation_threshold = max(1, dynamic_min_cooperation_threshold - 1)
            // Else: actual_cooperators_round_t_minus_1 < dynamic_min_cooperation_threshold
            //     dynamic_min_cooperation_threshold remains the same
        else // actual_cooperators_round_t_minus_1 >= m
            // If cooperation was met, reset dynamic threshold to the target 'm'
            dynamic_min_cooperation_threshold = m
        end if
    end if

    // --- Decision Logic (General Rule) ---
    if previous_cooperators_count >= m then
        // Reciprocate successful cooperation
        return C
    else if previous_cooperators_count >= dynamic_min_cooperation_threshold then
        // Cooperate if previous round showed enough effort, even if threshold not met
        // This also covers the case where dynamic_min_cooperation_threshold is m
        return C
    else
        // Defect if cooperation levels are too low to be worth the risk
        return D
    end if
end function

// Helper function to count cooperators
function count_cooperators(actions_list)
    count = 0
    for each action in actions_list
        if action == C then
            count = count + 1
        end if
    end for
    return count
end function
```

This strategy is designed to be a strong contender in a tournament by embodying cooperative intent while maintaining a pragmatic and adaptive approach to avoid exploitation. It prioritizes reaching the common good, but only when there's a reasonable signal that others are also contributing to it.
'''

description_COOPERATIVE_22 = '''
Here's a strategy designed for the Collective Risk Dilemma, prioritizing cooperation while being adaptive and robust. The core idea is to balance the desire for the collective reward with the inherent risk of being exploited.

## Strategy: The "Adaptive Threshold Responder"

This strategy aims to be cooperative by generally favoring cooperation, but it dynamically adjusts its behavior based on the observed success of cooperation in previous rounds. It's designed to avoid being a "sucker" while still trying to achieve the higher payoffs that come with successful collective action.

### 1. Decision Rules

The strategy's decision in any given round is a probabilistic one, influenced by the recent history of cooperation and the likelihood of reaching the cooperation threshold.

**Core Logic:**

*   **Initial Tendency:** Start with a strong inclination to cooperate. The primary goal is to establish a cooperative environment.
*   **Learning from Experience:** Observe the proportion of players who cooperated in the *previous* round.
*   **Adjusting Cooperation Probability:** If the previous round's cooperation rate was high enough to suggest the threshold is achievable, increase the probability of cooperating. If it was low, decrease the probability.
*   **Risk Mitigation:** To prevent being exploited when cooperation fails, the probability of defecting increases as the observed cooperation rate falls below a certain point.
*   **Threshold-Focused Cooperation:** The strategy explicitly tries to "hit" the threshold, not just cooperate blindly.

**Specific Rules:**

Let $P_i(t)$ be the probability that player $i$ cooperates in round $t$.
Let $N_C(t-1)$ be the number of players who cooperated in round $t-1$.
Let $C_{rate}(t-1) = N_C(t-1) / n$ be the cooperation rate in round $t-1$.

1.  **In round $t$ (for $t > 1$):**
    *   Calculate the cooperation rate from the previous round: $C_{rate}(t-1) = N_C(t-1) / n$.
    *   **If $C_{rate}(t-1) \ge (m-1)/n$ (i.e., the previous round was very close to or met the threshold):**
        *   Set $P_i(t) = \alpha \cdot \min(1, 1 + \beta \cdot (C_{rate}(t-1) - (m-1)/n))$
        *   Here, $\alpha$ is a base cooperation probability (e.g., 0.8), and $\beta$ is a learning rate (e.g., 2.0). This means if the previous round was very successful, we'll cooperate with high probability. If it was *just* below the threshold, we'll increase our probability to try and push it over.
    *   **If $(m-2)/n \le C_{rate}(t-1) < (m-1)/n$ (i.e., the previous round was moderately close to the threshold):**
        *   Set $P_i(t) = \alpha \cdot \min(1, 1 + \beta \cdot (C_{rate}(t-1) - (m-2)/n))$
        *   We still favor cooperation, but the boost from the previous round's success is less pronounced.
    *   **If $C_{rate}(t-1) < (m-2)/n$ (i.e., the previous round was significantly below the threshold):**
        *   Set $P_i(t) = \max(0.1, 1 - \gamma \cdot ((m-2)/n - C_{rate}(t-1)))$
        *   Here, $\gamma$ is a risk aversion factor (e.g., 1.5). This means if cooperation is consistently failing, we significantly reduce our cooperation probability, but we never go to zero to avoid missing out on potential future cooperation if others change their behavior. The minimum cooperation probability is set to a small value (e.g., 0.1) to remain somewhat cooperative.

2.  **In round $t$ (for $t = 1$ - First Round):**
    *   Set $P_i(1) = \alpha$ (e.g., 0.8). We start with a high probability of cooperation.

3.  **Action Selection:**
    *   Generate a random number $u$ between 0 and 1.
    *   If $u \le P_i(t)$, play 'C'.
    *   Otherwise, play 'D'.

**Rationale for Parameter Choices:**

*   $\alpha$ (Base Cooperation Probability): This sets the default cooperative stance. A value of 0.8 suggests a strong preference for cooperation.
*   $\beta$ (Learning Rate for Success): This determines how strongly we escalate cooperation when the threshold is being met or exceeded. A higher $\beta$ makes us more responsive to positive reinforcement.
*   $\gamma$ (Risk Aversion Factor for Failure): This determines how much we reduce cooperation when it's consistently failing. A higher $\gamma$ makes us more cautious about being exploited.
*   Minimum Cooperation Probability (e.g., 0.1): This is a crucial "safety net" ensuring we don't completely abandon cooperation, allowing us to adapt if others suddenly become more cooperative.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   As stated above, we begin with a high probability of cooperation ($P_i(1) = \alpha$). This is to initiate a cooperative environment and give the game a chance to reach positive outcomes early. There's no history to learn from, so we rely on our predefined "cooperative baseline."

*   **Last Round (t=r):**
    *   The strategy *does not* change its behavior in the last round. In repeated games, future payoffs can influence current decisions (e.g., trigger strategies). However, since there is no future beyond round $r$, and the game is identical in each round, there's no inherent incentive to "defect in the last round" to maximize immediate payoff unless the strategy's logic already dictates it. Our current logic adapts based on past success, not future expectations. Therefore, the decision rules for $t>1$ apply directly to $t=r$. This ensures consistency and avoids a sudden shift in behavior that could be exploited by others.

*   **Very Few Players (n close to m):**
    *   When $n$ is only slightly larger than $m$, the stakes of individual decisions are very high. Our probabilistic approach naturally handles this. If $m=n-1$, then $m/n$ is close to 1. If $m=2$, and $n=3$, then $m/n = 2/3$. The strategy will adjust its cooperation probability based on the observed $C_{rate}(t-1)$ relative to $(m-1)/n$. The learning rates $\beta$ and $\gamma$ will still apply.

*   **Very High Reward Factor (k):**
    *   A high $k$ makes cooperation significantly more attractive. The strategy's learning mechanism will naturally respond to this. If cooperation frequently leads to the threshold being met (because $k$ is so attractive), the $C_{rate}(t-1)$ will tend to be high, increasing $P_i(t)$.

### 3. Being Cooperative

This strategy is fundamentally cooperative for several reasons:

*   **High Initial Cooperation Probability:** It starts by defaulting to cooperation, signaling a willingness to contribute to the collective good.
*   **Learning and Adaptation for Cooperation:** The strategy actively *rewards* successful cooperation by increasing its own likelihood of cooperating. It aims to sustain and amplify successful collective efforts.
*   **Threshold-Oriented Cooperation:** It's not just about cooperating blindly; it's about coordinating to meet the *specific* threshold required for the collective reward. This is a more sophisticated form of cooperation than simply always cooperating.
*   **Gradual Reduction in Cooperation:** When cooperation fails repeatedly, the strategy reduces its cooperation probability *gradually*. It doesn't abruptly switch to defection. This provides a buffer for others who might be slow to adapt or for unexpected shifts in the game dynamics. It also leaves a "door open" for a return to higher cooperation if conditions improve.
*   **No Exploitation of Others' Cooperation:** The strategy doesn't have a mechanism to "punish" others or deliberately defect to exploit their cooperation. Its defection is a response to the *lack* of collective success, not a tactic to exploit individual cooperators.

### Pseudocode

```
// Strategy: Adaptive Threshold Responder

// Parameters for the strategy (can be tuned)
BASE_COOPERATION_PROBABILITY = 0.8  // alpha
SUCCESS_LEARNING_RATE = 2.0       // beta
FAILURE_RISK_AVERSION_FACTOR = 1.5 // gamma
MIN_COOPERATION_PROB = 0.1        // Minimum probability of cooperating

function decide_action(round_number, history, n, m, k):
  // history is a list of lists, where history[t-1] contains actions of all players in round t-1

  if round_number == 1:
    // First round: start with high cooperation
    cooperation_probability = BASE_COOPERATION_PROBABILITY
  else:
    // Get actions from previous round
    previous_round_actions = history[round_number - 2] // history is 0-indexed for rounds

    // Count number of cooperators in the previous round
    num_cooperators_prev_round = 0
    for action in previous_round_actions:
      if action == 'C':
        num_cooperators_prev_round = num_cooperators_prev_round + 1

    cooperation_rate_prev_round = num_cooperators_prev_round / n

    // Calculate the threshold probabilities for reference
    threshold_met_target = m / n
    just_below_threshold_target = (m - 1) / n
    significantly_below_threshold_target = (m - 2) / n

    if cooperation_rate_prev_round >= just_below_threshold_target:
      // Previous round was very close to or met the threshold
      // Increase cooperation based on how far above/below the 'just below' target we were
      cooperation_probability = BASE_COOPERATION_PROBABILITY * min(1.0, 1.0 + SUCCESS_LEARNING_RATE * (cooperation_rate_prev_round - just_below_threshold_target))
    elif cooperation_rate_prev_round >= significantly_below_threshold_target:
      // Previous round was moderately close to the threshold
      // Increase cooperation, but less strongly than if it was 'just below'
      cooperation_probability = BASE_COOPERATION_PROBABILITY * min(1.0, 1.0 + SUCCESS_LEARNING_RATE * (cooperation_rate_prev_round - significantly_below_threshold_target))
    else:
      // Previous round was significantly below the threshold
      // Reduce cooperation probability, but not to zero
      cooperation_probability = max(MIN_COOPERATION_PROB, 1.0 - FAILURE_RISK_AVERSION_FACTOR * (significantly_below_threshold_target - cooperation_rate_prev_round))

  // Ensure probability is within [0, 1] bounds
  cooperation_probability = max(0.0, min(1.0, cooperation_probability))

  // Decide action based on probability
  random_number = random.uniform(0, 1)
  if random_number <= cooperation_probability:
    return 'C'
  else:
    return 'D'

// Example of how history might be structured:
// history = [ ['C', 'D', 'C', 'D', 'C', 'D'],  // actions in round 1
//             ['C', 'C', 'D', 'D', 'C', 'D']   // actions in round 2
//           ]
// For round 3, we'd look at history[1] (round 2's actions).
```
'''

description_COOPERATIVE_23 = '''
This is an intriguing collective risk dilemma where cooperation is rewarded, but only if a sufficient number of players take the risk. The core challenge is to incentivize cooperation when individual defection is tempting, especially when others are defecting. Given the constraints of no communication and perfect information, a successful cooperative strategy needs to be self-enforcing and learn from past outcomes.

Here's a design for a cooperative strategy, focusing on adaptiveness and robustness:

## Strategy Name: "Conditional Reward-Seeking"

This strategy aims to consistently push towards cooperation, but intelligently scales back its commitment when faced with widespread defection, prioritizing survival and maximizing potential gains when conditions are favorable.

### 1. Decision Rules

The core of the "Conditional Reward-Seeking" strategy is to maintain a level of cooperation, but to adjust this level based on the observed success of collective cooperation in previous rounds.

**Key Concepts:**

*   **Cooperation Threshold Adjustment:** We will track the success of reaching the cooperation threshold (`m`) in previous rounds. If we consistently fail to meet `m`, our individual willingness to cooperate will decrease. Conversely, if `m` is frequently met, we become more confident in cooperating.
*   **Risk Tolerance:** The strategy has a base level of cooperation. This risk tolerance will be adjusted based on past outcomes.

**Decision Logic (for round `t`):**

1.  **Initial Cooperation Signal:** Define a "cooperation signal" based on the observed success rate of reaching `m` in previous rounds. Let `S_t` be the proportion of rounds `1` to `t-1` where `m` or more players cooperated.
    *   If `S_t` is high (e.g., > 0.75), we are in a "cooperative environment" and are more inclined to cooperate.
    *   If `S_t` is low (e.g., < 0.25), we are in a "risky environment" and are more cautious about cooperating.
    *   Intermediate `S_t` values represent a balanced environment.

2.  **Personal Cooperation Decision:**
    *   **If `t == 1` (First Round):**
        *   Cooperate (Play 'C'). This is the initial default to signal a desire for cooperation and to gather information.
    *   **If `t > 1`:**
        *   **Calculate `m_needed`:** The number of cooperators from the *previous* round (`t-1`) needed to *trigger* the reward. Let `C_{t-1}` be the number of players who cooperated in round `t-1`.
        *   **Determine `cooperation_probability`:**
            *   **Base Cooperation Probability (`p_base`):** This is a tunable parameter. A good starting point might be related to `m/n`, representing a fair share of cooperation. For instance, `p_base = max(0.5, m/n)`. This ensures we at least try to cooperate at a significant level.
            *   **Adaptive Adjustment:**
                *   If `C_{t-1} >= m`: The previous round was a success. We feel more confident in cooperating.
                    *   `cooperation_probability = p_base + 0.1 * (1 - p_base)` (Increase probability, but don't necessarily go to 1.0 immediately).
                *   If `C_{t-1} < m` and `C_{t-1} >= m - 2` (meaning it was close to succeeding): We are in a "near miss" situation.
                    *   `cooperation_probability = p_base` (Maintain current probability, signaling persistence).
                *   If `C_{t-1} < m - 2` (meaning a significant failure): We need to be more cautious.
                    *   `cooperation_probability = p_base * 0.75` (Reduce probability, but not to zero).
        *   **Randomized Decision:**
            *   Generate a random number `rand_num` between 0 and 1.
            *   If `rand_num < cooperation_probability`: Play 'C'.
            *   Else: Play 'D'.

**Rationale for `p_base` and Adaptive Adjustment:**

*   `p_base`: This is our intrinsic inclination to cooperate. Setting it to at least 0.5 and considering `m/n` ensures we are not inherently a defector and have a reasonable expectation of cooperation.
*   Adaptive Adjustment: This is crucial for robustness.
    *   **Reward for Success:** When `m` is met, we increase our cooperation probability. This signals that we are willing to reciprocate success and increase the chances of future success.
    *   **Caution for Failure:** When `m` is not met, we reduce our cooperation probability. This prevents us from being exploited by a group that consistently defects, preserving our endowment in those rounds. The reduction is not absolute, allowing us to remain open to cooperation if others begin to cooperate. The "near miss" condition allows us to be slightly more forgiving when the threshold was almost met.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):** Always cooperate ('C'). This is a crucial signal of intent. It allows us to gather initial data on opponent behavior. If everyone defects, we'll learn that quickly. If some cooperate, we can start to adapt our strategy for round 2.
*   **Last Round (`t = r`):** In the very last round, the incentive for future cooperation disappears. This is where the "end-game problem" of repeated games arises.
    *   **Decision:** For the last round, we will defect ('D').
    *   **Rationale:** Since there are no future rounds to incentivize cooperation, and the reward `k` can no longer be leveraged for future benefit, the optimal strategy for a rational player in the final round is to defect. This is a standard game-theoretic outcome for the last round of a finitely repeated game. Our strategy explicitly accounts for this by reverting to defection.
*   **Insufficient Players (`m > n` or `m <= 1`):** The problem statement specifies `1 < m < n`. If these parameters were invalid, we would ideally error or use a default strategy. Assuming valid parameters, this is not a concern.

### 3. Cooperative Mindset Alignment

This strategy is fundamentally cooperative in its design:

*   **Default to Cooperation:** The first round is always a cooperative act.
*   **Reward Cooperation:** When collective cooperation is successful (`>= m`), our strategy becomes *more* cooperative, increasing its probability of playing 'C'. This directly reinforces successful cooperative behavior.
*   **Adaptive but Not Punitive:** When collective cooperation fails, the strategy becomes more cautious, but it doesn't simply retaliate by defecting 100% of the time. It reduces its probability of cooperation, leaving room for others to change their behavior and for the strategy to re-engage if conditions improve. This is a form of "conditional cooperation" rather than pure punishment.
*   **Maximizing Collective Benefit (Implicitly):** By aiming to consistently push towards meeting the threshold `m`, the strategy is implicitly trying to achieve the higher payoff for everyone. The adaptive nature is to ensure that this pursuit of collective benefit is not at the cost of individual ruin when others are not cooperating.

### Pseudocode

```pseudocode
// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor (not directly used in decision, but influences its value)

// State variables
cooperators_last_round = 0
successful_rounds_count = 0 // Count of rounds where m cooperators were met

// Tunable parameter
base_cooperation_probability = max(0.5, m / n) // Initial inclination to cooperate

// Strategy function for player i
function decide_action(current_round, history):
    // history is a list of past actions (e.g., [[player1_r1, player2_r1, ...], [player1_r2, ...]])
    // We'll assume we can derive the number of cooperators from history.
    // Let's simplify and assume we have direct access to relevant stats from previous rounds.

    // --- Decision Logic ---

    // Edge Case: Last Round
    if current_round == r:
        return 'D' // Defect in the final round

    // First Round
    if current_round == 1:
        return 'C' // Cooperate to initiate

    // --- Adaptive Cooperation ---

    // Determine the number of cooperators in the previous round (t-1)
    // For implementation, this would be derived from the history.
    // Let's assume we have this value:
    // Example: If history is [[C, D, C], [D, D, C]], and current_round is 3,
    // then cooperators_last_round would be 2 (from [C, D, C]).
    num_cooperators_prev_round = count_cooperators(history, current_round - 1)

    // Update successful rounds count
    if num_cooperators_prev_round >= m:
        successful_rounds_count += 1

    // Calculate the success rate of reaching the threshold
    success_rate = successful_rounds_count / (current_round - 1)

    // Determine the adjusted cooperation probability
    cooperation_probability = base_cooperation_probability

    if success_rate > 0.75: // Highly cooperative environment
        cooperation_probability = base_cooperation_probability + 0.1 * (1 - base_cooperation_probability)
    elif success_rate < 0.25: // Risky/uncooperative environment
        // Adjust based on how close we were in the previous round
        if num_cooperators_prev_round >= m - 2: // Near miss
            // Stay neutral, but don't boost too much
            cooperation_probability = base_cooperation_probability
        else: // Significant failure
            cooperation_probability = base_cooperation_probability * 0.75
    // If success_rate is between 0.25 and 0.75, keep base_cooperation_probability

    // Ensure probability stays within [0, 1] (though logic should prevent this)
    cooperation_probability = max(0.0, min(1.0, cooperation_probability))

    // Randomized Decision
    rand_num = random_uniform(0, 1)
    if rand_num < cooperation_probability:
        return 'C'
    else:
        return 'D'

// Helper function to count cooperators in a given round from history
function count_cooperators(history, round_number):
    if round_number <= 0 or round_number > len(history):
        return 0 // Or handle error
    
    round_actions = history[round_number - 1]
    count = 0
    for action in round_actions:
        if action == 'C':
            count += 1
    return count

```

**Key Considerations for Implementation:**

*   **`history` Structure:** The `history` would need to store the actions of all players in previous rounds.
*   **`success_rate` Calculation:** It's crucial to divide by `(current_round - 1)` to avoid division by zero for the first round and to ensure the average is over *completed* rounds.
*   **Tuning `base_cooperation_probability`:** This is the most sensitive parameter. Starting with `max(0.5, m/n)` is a reasonable heuristic. Extensive testing in a tournament environment might reveal optimal values.
*   **Thresholds for `success_rate`:** The 0.75 and 0.25 thresholds for adjusting cooperation are also tunable. They represent how strongly the strategy reacts to observed success or failure.

This "Conditional Reward-Seeking" strategy aims to be a robust and inherently cooperative player in the Collective Risk Dilemma, balancing the desire for collective gain with intelligent self-preservation against defection.
'''

description_COOPERATIVE_24 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust within the given constraints. This strategy aims to maximize collective well-being by encouraging cooperation while safeguarding against exploitation.

## Cooperative Strategy: The "Threshold Sentinel"

This strategy is named "Threshold Sentinel" because it actively monitors the collective behavior and aims to ensure the cooperation threshold is met, while being prepared to defect if cooperation falters. It's fundamentally a **tit-for-tat** like approach, but adapted to a collective threshold rather than pairwise interaction.

### 1. Decision Rules:

The core of the Threshold Sentinel strategy is to cooperate if and only if there's a reasonable expectation that the cooperation threshold (`m`) will be met. This expectation is derived from past round performance.

**For any given round `t` (where `t > 1`):**

*   **Cooperate (C) if:**
    *   The number of cooperators in the previous round (`t-1`) was greater than or equal to `m-1`.
    *   **AND** if it's not the final round (`r`) and the total number of players (`n`) is large enough that your defection alone cannot prevent the threshold from being met in the current round. This is a subtle but important addition to prevent being the sole defector when `m=n` and the count was `n-1` in the previous round. (Essentially, `n - (cooperators_t-1) > 1` or `cooperators_t-1 < n-1`).

*   **Defect (D) if:**
    *   The number of cooperators in the previous round (`t-1`) was less than `m-1`.
    *   **OR** if it is the final round (`r`) and the number of cooperators in the previous round (`t-1`) was less than `m`. (This is a defensive measure to secure your own payoff in the last round if cooperation seems unlikely).

### 2. Handling Edge Cases:

*   **First Round (t = 1):**
    *   In the first round, there is no historical data. To initiate cooperation and signal good faith, the Threshold Sentinel will **Cooperate (C)**. This is a calculated risk. The hope is that other players will also adopt a cooperative stance, or that the subsequent rounds will reveal a pattern of cooperation that can be built upon.

*   **Last Round (t = r):**
    *   The strategy becomes more cautious in the last round. As mentioned in the decision rules, if the number of cooperators in round `r-1` was less than `m`, the Threshold Sentinel will **Defect (D)**. This is because there's no future round to "punish" defection or reward cooperation, so it's best to secure the private endowment if the collective reward is unlikely. If cooperation in `r-1` met or exceeded `m`, the strategy will **Cooperate (C)**.

*   **Threshold (`m`) being close to `n`:**
    *   If `m` is very close to `n` (e.g., `m = n-1` or `m = n`), the strategy's decision to cooperate becomes more sensitive to the actions of a few players. The condition "if it's not the final round and the total number of players (`n`) is large enough that your defection alone cannot prevent the threshold from being met" becomes critical. If your defection would be the *only* thing preventing the threshold from being met in the *next* round (given the history), and it's not the final round, you might still cooperate to encourage others. However, if `m` is very high and the cooperation rate has been consistently low, the strategy will lean towards defection.

### 3. Being Cooperative:

The Threshold Sentinel is fundamentally a cooperative strategy with a strong incentive to see the collective reward achieved.

*   **Proactive Cooperation:** It starts by cooperating in the first round, setting a positive example.
*   **Conditional Cooperation:** It rewards past cooperation by continuing to cooperate, aiming to stabilize and reinforce positive behavior.
*   **Threshold-Focused:** Its primary goal is to reach the `m` threshold, thereby benefiting all players. It actively "leans in" towards cooperation as long as there's a reasonable chance of success.
*   **Risk Aversion (Adaptive):** While cooperative, it's not naive. It adapts its behavior based on observed outcomes. If the collective consistently fails to meet the threshold, it recognizes that continued cooperation from its side might be futile and detrimental to its own payoff, hence the shift towards defection. This is a pragmatic form of cooperation that seeks to create a sustainable cooperative environment rather than being exploited.

### Pseudocode Representation:

```python
# Initialize historical data
cooperators_in_previous_round = 0
current_round = 1

def decide_action(game_parameters, history):
    n = game_parameters['n']
    r = game_parameters['r']
    m = game_parameters['m']
    k = game_parameters['k']

    global current_round
    global cooperators_in_previous_round

    # --- Round 1 ---
    if current_round == 1:
        cooperators_in_previous_round = 0 # Reset for clarity
        current_round += 1
        return 'C' # Start cooperatively

    # --- Subsequent Rounds ---
    # Calculate cooperators from history (assuming history provides this info)
    # For this strategy, we only need the count from the immediate previous round.
    # In a real implementation, history would be a list of past actions/outcomes.
    # For this pseudocode, we'll assume cooperators_in_previous_round is correctly updated.

    # Condition for cooperation:
    # 1. Previous round was at least m-1 cooperators
    # 2. AND (It's not the last round OR your defection alone wouldn't prevent the threshold)
    can_you_alone_prevent_threshold = (n - cooperators_in_previous_round) == 1 and m > cooperators_in_previous_round

    should_cooperate = (cooperators_in_previous_round >= m - 1) and \
                       (current_round < r or not can_you_alone_prevent_threshold)

    # Condition for defection:
    # 1. Previous round was less than m-1 cooperators
    # 2. OR (It is the last round AND previous round was less than m)
    should_defect = (cooperators_in_previous_round < m - 1) or \
                    (current_round == r and cooperators_in_previous_round < m)

    # Decision
    if should_cooperate:
        current_round += 1
        return 'C'
    elif should_defect:
        current_round += 1
        return 'D'
    else:
        # Fallback: If neither explicit condition is met, default to cooperation
        # This scenario is unlikely with the defined logic but good for robustness.
        # It means cooperators_in_previous_round >= m-1 but can_you_alone_prevent_threshold is true AND current_round < r
        # In this case, we are the only one whose defection would break the threshold, so we still cooperate.
        current_round += 1
        return 'C'

# --- How to update cooperators_in_previous_round for the next iteration ---
# After the current round's actions are revealed, count the number of 'C's.
# For the next call to decide_action, set:
# cooperators_in_previous_round = count_of_cooperators_in_this_round
```

**Rationale for Robustness and Adaptiveness:**

*   **Adaptiveness:** The strategy dynamically adjusts its behavior based on the observed success of cooperation in the *previous* round. This allows it to react to changing dynamics in the game.
*   **Robustness to Opponent Behavior:**
    *   **Against pure cooperators:** The strategy will cooperate alongside them, ensuring the threshold is met and maximizing mutual gain.
    *   **Against pure defectors:** If enough other players also defect, the `cooperators_in_previous_round` will drop below `m-1`, causing the Threshold Sentinel to also defect, thus protecting its endowment.
    *   **Against mixed strategies:** The strategy is designed to "follow the leader" in a sense. If the collective leans towards cooperation (meeting `m-1`), it will reciprocate. If it leans towards defection, it will eventually follow suit.
    *   **Against exploitation:** The careful handling of the last round and the condition about individual impact on the threshold prevents the strategy from being a "free rider" in the final round or being the sole cooperative player when it's clearly futile.

This "Threshold Sentinel" strategy provides a clear, parameter-driven, and history-dependent decision-making process that aligns with the cooperative spirit of the game while incorporating essential safeguards.
'''

description_COOPERATIVE_25 = '''
## Cooperative Strategy: The "Adaptive Threshold Weaver"

This strategy aims to maximize individual and collective payoffs by adaptively responding to the observed cooperation levels, with a strong bias towards cooperation when the collective is close to meeting the threshold. It's designed to be robust, learn from history, and promote cooperation without explicit communication.

**Core Philosophy:** My strategy is rooted in the belief that sustained cooperation, even with the inherent risks, is the most likely path to higher collective and individual rewards in the long run. I will prioritize contributing to the collective good when there's a reasonable chance of success, while still protecting myself from exploitation.

---

### 1. Decision Rules

My decision to Cooperate (C) or Defect (D) in any given round will be based on the following factors:

**1.1. The "Cooperation Momentum" Metric:**

This metric represents the observed trend in cooperation from the previous round and informs my expectation for the current round.

*   **`cooperation_count_prev_round`**: The number of players who cooperated in the *immediately preceding* round.
*   **`cooperation_momentum`**:
    *   If `cooperation_count_prev_round >= m`: The momentum is **Positive**. The group successfully met the threshold.
    *   If `cooperation_count_prev_round < m` AND `cooperation_count_prev_round > m - 2`: The momentum is **Neutral/Approaching**. The group narrowly missed the threshold, or is getting close. This indicates a potential for success with a small increase in cooperation.
    *   If `cooperation_count_prev_round <= m - 3`: The momentum is **Negative**. The group is significantly short of the threshold.

**1.2. My Decision Logic:**

*   **If `cooperation_momentum` is Positive:**
    *   **Decision:** **Cooperate (C)**.
    *   **Rationale:** The group has demonstrated its ability to achieve the cooperative outcome. By cooperating, I reinforce this positive behavior and signal my commitment to collective success. This is the most beneficial scenario for all, and I aim to maintain it.

*   **If `cooperation_momentum` is Neutral/Approaching:**
    *   **Decision:** **Cooperate (C)**.
    *   **Rationale:** The group is on the cusp of success. My cooperation here could be the tipping point that shifts the collective outcome from failure to success. The potential reward (k) far outweighs the risk of losing my endowment (1) if others also cooperate. Even if others defect, my loss is limited to my endowment.

*   **If `cooperation_momentum` is Negative:**
    *   **Decision:** **Defect (D)**.
    *   **Rationale:** If the group has consistently failed to reach the cooperation threshold, and is significantly far off, it suggests a lack of widespread commitment to cooperation. In such a scenario, defecting is a more prudent strategy to preserve my own endowment. I will not be the sole cooperator in a likely failing endeavor.

**1.3. Minimum Cooperation Threshold Guard:**

This is a secondary check to ensure I don't become a lone cooperator in a round where cooperation is clearly not happening.

*   **`my_cooperation_count`**: The number of players I *expect* to cooperate in the current round, including myself. This is estimated based on `cooperation_count_prev_round`.
*   **If `cooperation_momentum` is Negative (as per 1.2):**
    *   **If `m - 1 > cooperation_count_prev_round`:** (Meaning even if I cooperate, we still wouldn't reach `m`)
        *   **Decision:** **Defect (D)**.
        *   **Rationale:** I will not be the only one contributing to a project that is highly unlikely to meet its threshold. This prevents me from being exploited.

**Summary of Decision Rules:**

| `cooperation_momentum`      | `m - 1 > cooperation_count_prev_round` (if momentum is Negative) | Decision |
| :-------------------------- | :--------------------------------------------------------------- | :------- |
| Positive                    | N/A                                                              | C        |
| Neutral/Approaching         | N/A                                                              | C        |
| Negative                    | False                                                            | D        |
| Negative                    | True                                                             | D        |

---

### 2. Handling Edge Cases

**2.1. First Round (t=1):**

*   **Decision:** **Cooperate (C)**.
*   **Rationale:** In the absence of any historical data, the optimal approach is to initiate cooperation. This sets a positive tone and provides the best chance for early success. It also gives other players an opportunity to reciprocate. This is a bet on the best-case scenario.

**2.2. Last Round (t=r):**

*   **Decision:** **Defect (D)**.
*   **Rationale:** In the final round, there are no future consequences for my actions. The primary objective shifts from building long-term trust and cooperation to maximizing my immediate payoff. Since there's no incentive for reciprocal cooperation in future rounds, defecting is the rational choice to secure my individual endowment.

**2.3. Scenario: Very Low `m` (e.g., `m=2` with `n > 2`)**

*   My strategy remains the same. However, I acknowledge that with a low `m`, it becomes easier for a small group to exploit others by cooperating amongst themselves and defecting when the threshold is not met. My "Neutral/Approaching" and "Negative" momentum thresholds are designed to be somewhat forgiving, but in extreme cases of low `m`, my strategy might still be susceptible to exploitation if many players adopt a purely opportunistic mindset.

**2.4. Scenario: Very High `m` (e.g., `m` close to `n`)**

*   My strategy adapts. When `m` is high, the "Neutral/Approaching" and "Negative" momentum thresholds will be triggered more frequently. This means I will be more likely to defect if the group doesn't have a very strong showing of cooperation. This is a necessary adaptation to avoid contributing to a project with a very low probability of success.

---

### 3. Being Cooperative

My strategy is inherently cooperative in spirit:

*   **Proactive Cooperation:** I initiate cooperation in the first round, demonstrating a willingness to contribute to the collective good.
*   **Reciprocal Reinforcement:** I continue to cooperate when the group has successfully met the threshold (`Positive` momentum), reinforcing successful cooperative behavior.
*   **Risk-Averse Cooperation:** I cooperate when the group is close to the threshold (`Neutral/Approaching` momentum). This shows a willingness to take a calculated risk for the benefit of all, understanding that my contribution could be the catalyst for success.
*   **Fairness and Sustainability:** My "Negative" momentum rule is not about punishment, but about self-preservation and ensuring that my cooperation isn't consistently exploited. It aims to foster a dynamic where cooperation is only pursued when there's a realistic chance of collective reward.

---

### Pseudocode Representation

```
Strategy: Adaptive Threshold Weaver

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

State Variables (per player):
  current_round: integer (starts at 1)
  cooperation_count_prev_round: integer (initialized to 0)

Function DecideAction():
  // Edge Case: First Round
  if current_round == 1:
    return C

  // Edge Case: Last Round
  if current_round == r:
    return D

  // Calculate Cooperation Momentum
  cooperation_momentum = "Neutral/Approaching" // Default
  if cooperation_count_prev_round >= m:
    cooperation_momentum = "Positive"
  elif cooperation_count_prev_round <= m - 3:
    cooperation_momentum = "Negative"

  // Decision Logic
  if cooperation_momentum == "Positive":
    return C
  elif cooperation_momentum == "Neutral/Approaching":
    return C
  elif cooperation_momentum == "Negative":
    // Minimum Cooperation Threshold Guard
    if (m - 1) > cooperation_count_prev_round:
      return D // Don't be the only one if it's highly unlikely to succeed
    else:
      return D // Consistently failing, protect own endowment

// At the end of each round, update state variables:
// cooperation_count_prev_round = number of players who played C in the just-finished round
// current_round = current_round + 1
```

This strategy is designed to be a dynamic and responsive participant, balancing the desire for collective gain with the necessity of individual prudence. It aims to weave a path towards cooperation by adapting to the unfolding history of the game.
'''

description_COOPERATIVE_26 = '''
This is a fascinating strategic challenge! The core tension lies between the individual incentive to defect (to secure the private payoff of 1 even if cooperation fails) and the collective benefit of cooperation (achieving a higher payoff of 2 for everyone if the threshold is met). Designing a cooperative strategy that is adaptive and robust requires a careful balance.

Here's a proposed cooperative strategy, focusing on principles of conditional cooperation and learning from past outcomes, while adhering to the "no communication" constraint.

## Cooperative Strategy: "Threshold Weaver"

The "Threshold Weaver" strategy aims to establish and maintain cooperation by dynamically adjusting its behavior based on observed cooperation levels in previous rounds. It prioritizes achieving the collective benefit while minimizing the risk of being exploited.

### 1. Decision Rules

The core of the strategy revolves around a **dynamic cooperation threshold**. This threshold represents the minimum number of players the strategy *expects* to cooperate in the current round. If the observed cooperation in the *previous* round meets or exceeds this dynamic threshold, the strategy will cooperate in the current round. Otherwise, it will defect.

Here's how it works in detail:

**a) Initial Cooperation Threshold:**
*   In the very first round (round 1), the strategy has no historical data. To encourage initial cooperation and signal a willingness to cooperate, it will initially set a slightly higher expectation of cooperation than the minimum required to trigger the reward.
    *   **Decision Rule (Round 1):** Cooperate (C).
    *   **Rationale:** This acts as an initial "generous" move to try and establish a cooperative norm.

**b) Dynamic Threshold Adjustment:**
*   After the first round, the strategy will observe the actions of all players in the previous round. Let $C_{t-1}$ be the number of players who cooperated in round $t-1$.
*   The strategy will then adjust its internal dynamic cooperation threshold for the *next* round ($t$).

    *   **If $C_{t-1} \ge m$ (Cooperation succeeded in the previous round):**
        *   The strategy will increase its expectation of cooperation for the next round. It wants to see continued cooperation if the collective reward was achieved.
        *   **Dynamic Threshold for round $t$: Increase** (e.g., increase by a small increment, but cap it at a value close to $n$, or potentially maintain it if it was already high). A simple rule could be `current_threshold = min(n, previous_threshold + 1)`.
        *   **Decision Rule (Round $t$):** Cooperate (C).
        *   **Rationale:** The collective project was successful, so the strategy reinforces this by cooperating again, expecting others to do the same.

    *   **If $C_{t-1} < m$ (Cooperation failed in the previous round):**
        *   The strategy needs to be cautious. It will lower its expectation of cooperation, as the collective effort was insufficient.
        *   **Dynamic Threshold for round $t$: Decrease** (e.g., decrease by a small increment, but ensure it doesn't fall below $m-1$, as we still need to *try* to reach $m$). A simple rule could be `current_threshold = max(m - 1, previous_threshold - 1)`.
        *   **Decision Rule (Round $t$):** Defect (D).
        *   **Rationale:** The collective effort was insufficient. Defecting protects the strategy from the negative payoff of contributing to a failed project and signals that the required level of cooperation was not met.

**c) Core Decision Logic (for rounds t > 1):**
*   Let $C_{t-1}$ be the number of cooperators in round $t-1$.
*   Let $T_{t-1}$ be the strategy's internal "target" or "expected cooperation level" from round $t-1$. (This starts high and adjusts).

    *   **If $C_{t-1} \ge T_{t-1}$:**
        *   **Decision Rule (Round $t$):** Cooperate (C).
        *   **Rationale:** Others are cooperating at or above the expected level, so join in.

    *   **If $C_{t-1} < T_{t-1}$:**
        *   **Decision Rule (Round $t$):** Defect (D).
        *   **Rationale:** Others are not cooperating at the expected level, so defect to protect oneself.

**d) Re-evaluation of Threshold:**
*   The strategy needs to constantly re-evaluate its *internal expectation* ($T_t$).
    *   If cooperation succeeded ($C_{t-1} \ge m$), the strategy will aim for *at least* that level of cooperation in the future, or even a bit higher, as the reward was obtained.
    *   If cooperation failed ($C_{t-1} < m$), the strategy will lower its expectation for the next round, becoming more cautious.

Let's refine the dynamic threshold adjustment with a specific mechanism:

**Refined Dynamic Threshold (T):**

*   **Initialize $T_1 = m + \lfloor (n-m)/2 \rfloor$ (or some value slightly above m but not too high).** This is the initial expectation for Round 1.
*   **For round $t > 1$:**
    *   Observe $C_{t-1}$ (number of cooperators in the previous round).
    *   **If $C_{t-1} \ge m$ (Cooperation was successful):**
        *   We want to encourage continued cooperation. The new target should reflect this success.
        *   **Decision Rule (Round $t$):** Cooperate (C).
        *   **Update $T_t = \min(n, T_{t-1} + 1)$.** (Slightly increase the expected cooperation level).
    *   **If $C_{t-1} < m$ (Cooperation failed):**
        *   We need to be more cautious. The target cooperation level must be lower.
        *   **Decision Rule (Round $t$):** Defect (D).
        *   **Update $T_t = \max(m - 1, T_{t-1} - 1)$.** (Slightly decrease the expected cooperation level, but ensure it's still plausible).

**Summary of Decision Rules:**

*   **Round 1:** Cooperate (C). Set $T_1 = m + \lfloor (n-m)/2 \rfloor$.
*   **Round $t > 1$:**
    *   Observe $C_{t-1}$ (number of cooperators in round $t-1$).
    *   **If $C_{t-1} \ge T_{t-1}$:**
        *   **Play C.**
        *   **Update $T_t = \min(n, T_{t-1} + 1)$.** (If we achieved our target or exceeded it, aim a little higher).
    *   **If $C_{t-1} < T_{t-1}$:**
        *   **Play D.**
        *   **Update $T_t = \max(m - 1, T_{t-1} - 1)$.** (If we fell short, become more cautious and lower expectations).

**This strategy is adaptive because:**
*   It continuously adjusts its "cooperation expectation" ($T_t$) based on historical outcomes ($C_{t-1}$).
*   If cooperation is consistently successful, it will aim for higher levels of cooperation.
*   If cooperation consistently fails, it will become more cautious and defect more often.

**This strategy is robust because:**
*   It starts with a slightly ambitious but reasonable target, willing to take an initial cooperative risk.
*   It has a mechanism to "punish" defection (by defecting itself) but also to "reward" cooperation (by cooperating again and aiming higher).
*   It avoids being a pure altruist by defecting when the observed cooperation falls below its dynamically adjusted threshold, protecting itself from exploitation.
*   It does not require explicit communication or shared norms, relying solely on observed actions and game parameters.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** Cooperate (C).
    *   **Initial Threshold:** Set $T_1 = m + \lfloor (n-m)/2 \rfloor$. This is a reasonable starting point – higher than the minimum $m$ to encourage others, but not so high as to be immediately unattainable. A simpler starting point could also be $T_1 = m + 1$.
    *   **Rationale:** To initiate cooperation and signal good faith.

*   **Last Round (t=r):**
    *   The "Threshold Weaver" strategy, as described, is inherently myopic in its decision-making for each round based *only* on the previous round's outcome. In a true *finite* repeated game without explicit forward-looking punishment/reward mechanisms beyond the current round, the last round often collapses to a one-shot game's logic.
    *   However, the "Threshold Weaver" strategy's *dynamic threshold adjustment* already implicitly incorporates a form of learning and adaptation. For the last round, the strategy will simply apply its current decision rule based on round $r-1$'s outcome and its threshold $T_{r-1}$.
    *   **Decision (Round r):** Based on $C_{r-1}$ and $T_{r-1}$ using the standard rule.
        *   If $C_{r-1} \ge T_{r-1}$, play C.
        *   If $C_{r-1} < T_{r-1}$, play D.
    *   **No Special "Last Round" Rule:** The strategy doesn't need a distinct "last round" rule because its adaptive nature means it's always playing based on the most recent history. The dynamic threshold $T_r$ would simply not be used for future rounds.

*   **When $m$ is close to $n$ or 2:**
    *   If $m$ is very close to $n$ (e.g., $m = n-1$), cooperation is very difficult to achieve. The strategy will likely defect frequently after initial failed attempts.
    *   If $m=2$ (and $n>2$), cooperation is relatively easy. The strategy will be more inclined to cooperate and will aim for higher thresholds. The initial $T_1$ and subsequent adjustments will adapt.

*   **When $k$ is very high:**
    *   A very high $k$ increases the incentive for cooperation. The strategy will still operate on the observed cooperation levels relative to its dynamic threshold, but the overall incentive structure of the game will encourage more cooperation from all players, potentially leading the "Threshold Weaver" to cooperate more consistently.

### 3. Be Cooperative

This strategy is designed to be fundamentally cooperative in its *intent*.

*   **Initial Cooperation:** It always starts by cooperating to give cooperation a chance.
*   **Reinforcement of Success:** When cooperation leads to the collective reward ($C_{t-1} \ge m$), it reinforces this by cooperating and *aiming higher*. This is a proactive way to build a more stable cooperative environment.
*   **Conditional Cooperation:** It cooperates when it perceives a reasonable level of cooperation from others, aligning its actions with a shared successful outcome.
*   **Adaptive Punishment:** Its "punishment" (defecting) is conditional and temporary. It defects not out of malice, but to protect itself and signal that the cooperative conditions were not met. It is always ready to return to cooperation if the observed behavior shifts.

**Pseudocode:**

```python
class ThresholdWeaver:
    def __init__(self, n, m, r, k):
        self.n = n
        self.m = m
        self.r = r
        self.k = k
        self.current_threshold = 0 # Internal expectation of cooperation
        self.last_round_cooperators = 0

    def decide_action(self, round_num):
        if round_num == 1:
            # Round 1: Initial cooperative move
            self.current_threshold = self.m + (self.n - self.m) // 2 # Initial expectation
            return 'C'
        else:
            # Rounds t > 1: Adjust threshold based on previous round
            if self.last_round_cooperators >= self.current_threshold:
                # Met or exceeded expectation: Cooperate and aim slightly higher
                self.current_threshold = min(self.n, self.current_threshold + 1)
                return 'C'
            else:
                # Fell short of expectation: Defect and become more cautious
                self.current_threshold = max(self.m - 1, self.current_threshold - 1)
                return 'D'

    def update_history(self, num_cooperators_last_round):
        self.last_round_cooperators = num_cooperators_last_round

# Example Usage (within a tournament simulation loop)
# For each player i:
#   strategy_i = ThresholdWeaver(n, m, r, k)
#
# For round_num from 1 to r:
#   actions_this_round = []
#   for player_i:
#     actions_this_round.append(player_i.decide_action(round_num))
#
#   # Determine num_cooperators_this_round from actions_this_round
#   num_cooperators_this_round = actions_this_round.count('C')
#
#   # Update history for all players
#   for player_i:
#     player_i.update_history(num_cooperators_this_round)
#
#   # Calculate payoffs based on actions_this_round and game rules
```

This "Threshold Weaver" strategy aims to be a resilient and progressively cooperative player, balancing the desire for collective gain with the necessity of self-preservation in a game of imperfect trust.
'''

description_COOPERATIVE_27 = '''
Here's a strategy designed for the Collective Risk Dilemma, emphasizing cooperation while being adaptive and robust.

## Cooperative Adaptive Strategy (CAS)

This strategy aims to foster cooperation by taking calculated risks, rewarding observed cooperation, and punishing defection, all while remaining flexible to the evolving game dynamics.

### Core Philosophy:

The strategy operates on the principle of **reciprocity with a tolerance for risk**. It believes that sustained cooperation is the most beneficial outcome for all players in the long run, but it recognizes that this requires a certain level of trust and a mechanism to discourage exploitation.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round is based on the observed behavior of other players in previous rounds, specifically focusing on the **proportion of cooperators** and a measure of **risk tolerance**.

Let's define:

*   **`coop_ratio_prev_round`**: The proportion of players who cooperated in the *previous* round.
*   **`my_last_action`**: The action taken by this player in the *previous* round.
*   **`current_round`**: The current round number (1 to r).

**Decision Logic:**

1.  **If `current_round` == 1 (First Round):**
    *   **Cooperate (C)**. The initial action is to signal a willingness to cooperate and to try and trigger the mutually beneficial outcome. This sets a cooperative tone.

2.  **If `current_round` > 1:**
    *   **Calculate `coop_ratio_prev_round`**: This is the total number of cooperators in the previous round divided by `n`.
    *   **Determine Threshold for Cooperation**: We need to decide if the observed cooperation is sufficient to warrant continued cooperation. This threshold will be dynamic and influenced by `m`, `k`, and past behavior.

        *   **High Confidence Cooperation:** If `coop_ratio_prev_round` is **significantly above `m/n`** (e.g., `coop_ratio_prev_round >= m/n + 0.15` or a similar tunable parameter that represents a buffer), this indicates strong collective commitment. In this case:
            *   **Cooperate (C)**.

        *   **Moderate Confidence Cooperation:** If `coop_ratio_prev_round` is **at or slightly above `m/n`** (e.g., `m/n <= coop_ratio_prev_round < m/n + 0.15`). This is where the adaptive element becomes crucial. We need to consider past individual behavior and the reward factor.

            *   **If `my_last_action` was C:**
                *   **Cooperate (C)**. We maintain our cooperative stance, rewarding their past cooperation and hoping to encourage it.
            *   **If `my_last_action` was D:**
                *   **Cooperate (C)**. Even if we defected last round, if the overall cooperation is near the threshold, we still attempt to cooperate. This is a crucial element for recovering from potential cycles of defection and can encourage others to be more forgiving.

        *   **Low Confidence Cooperation (Below Threshold):** If `coop_ratio_prev_round` is **below `m/n`**. This indicates that the collective is not meeting the cooperation requirement.

            *   **If `my_last_action` was C:**
                *   **Defect (D)**. This is a signal of disappointment and a "punishment" for insufficient collective cooperation. We are no longer willing to bear the cost of cooperation if others are not.
            *   **If `my_last_action` was D:**
                *   **Defect (D)**. If the group has consistently failed to cooperate, and we defected last round, we continue to defect.

    *   **Special Case: Last Round (`current_round` == r):**
        *   **Defect (D)**. In the final round, there is no future to build trust or reciprocity. The incentive to cooperate disappears, and the rational choice is to maximize individual payoff by keeping the endowment. This is a standard consideration in repeated game theory where the future is finite.

### 2. Handling Edge Cases:

*   **First Round (r=1):** As stated in the decision rules, **always Cooperate (C)**. This is the initial signal of good faith.
*   **Last Round (r=r):** As stated in the decision rules, **always Defect (D)**. This is the rational disincentive to cooperate in the absence of future interactions.
*   **Insufficient Players (n < m):** The game design prevents this by `m < n`.
*   **No Cooperation in a Round:** If `coop_ratio_prev_round` drops significantly below `m/n`, the strategy will transition to defecting. This is a self-protective mechanism.

### 3. Being Cooperative:

*   **Initial Cooperative Stance:** The strategy begins by cooperating, demonstrating a willingness to engage in the mutually beneficial outcome.
*   **Rewarding Cooperation:** The strategy is more likely to cooperate if it observed cooperation in the previous round, especially if the overall cooperation level is near the threshold. This reinforces cooperative behavior.
*   **Forgiving Behavior (within limits):** If the overall cooperation is close to `m/n`, the strategy will still attempt to cooperate even if it defected in the prior round. This allows for recovery from situations where a player might have defected due to miscalculation or lack of trust, and it can encourage others to return to cooperation.
*   **Focus on Collective Outcome:** The decision-making process explicitly considers the `coop_ratio_prev_round`, indicating that the strategy is evaluating the collective success rather than solely individual past actions.

### Pseudocode:

```pseudocode
// Strategy: Cooperative Adaptive Strategy (CAS)

// Parameters:
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward if threshold met factor (not directly used in decision logic but influences the game's incentives)

// State Variables (maintained across rounds):
// my_last_action: Player's action in the previous round (initially None)
// observed_cooperators_prev_round: Count of players who cooperated in the previous round (initially 0)

// Function: decide_action(current_round, n, m, k, observed_cooperators_prev_round, my_last_action)

// --- Decision Rules ---

if current_round == 1:
    return "C" // Always cooperate in the first round

if current_round == r:
    return "D" // Always defect in the last round

// Calculate cooperation ratio from the previous round
coop_ratio_prev_round = observed_cooperators_prev_round / n

// Define tunable thresholds (can be adjusted for performance)
high_confidence_threshold = m/n + 0.15
moderate_confidence_lower_bound = m/n
moderate_confidence_upper_bound = m/n + 0.15

if coop_ratio_prev_round >= moderate_confidence_upper_bound:
    // High confidence in collective cooperation
    return "C"

else if coop_ratio_prev_round >= moderate_confidence_lower_bound:
    // Moderate confidence - consider own previous action
    if my_last_action == "C":
        return "C" // Reward past cooperation
    else: // my_last_action == "D"
        return "C" // Attempt to recover cooperation

else: // coop_ratio_prev_round < moderate_confidence_lower_bound
    // Low confidence in collective cooperation
    if my_last_action == "C":
        return "D" // Punish insufficient cooperation
    else: // my_last_action == "D"
        return "D" // Continue defection if group is not cooperating

// --- Update State for Next Round ---
// This part would be handled by the tournament system after the decision
// For clarity, let's assume the system updates:
// - my_last_action = chosen_action_for_this_round
// - observed_cooperators_prev_round = count of cooperators in this round

// End Function
```

### Robustness and Adaptability:

*   **Adaptive Thresholds:** The decision to cooperate or defect is not binary based on `m/n`. It uses a range and considers the *degree* of cooperation.
*   **Response to Group Behavior:** The strategy directly reacts to the collective performance (`coop_ratio_prev_round`), making it responsive to how the group as a whole is behaving.
*   **Reciprocity with Forgiveness:** By cooperating even after a defection if the group is close to `m/n`, the strategy attempts to break cycles of mutual defection.
*   **Clear Punishment:** If cooperation consistently falters, the strategy will switch to defecting, protecting itself from being exploited.
*   **No Hardcoded Patterns:** The strategy doesn't rely on specific cooperation schedules or patterns, making it adaptable to a wide variety of opponent strategies. It learns from history.

This strategy aims to be a strong cooperative player, incentivizing others to cooperate while having a clear mechanism to disincentivize exploitation. Its success will depend on how well other players respond to its cooperative overtures and punishments.
'''

description_COOPERATIVE_28 = '''
Here's a design for a cooperative strategy for the Collective Risk Dilemma, focusing on adaptability, robustness, and a clear cooperative intent.

## Cooperative Strategy: The "Adaptive Threshold Seeker"

This strategy aims to achieve cooperation by continuously assessing the collective risk and adapting its behavior to encourage the group to reach the cooperation threshold (m). It prioritizes achieving the higher payoff associated with cooperation while being resilient to exploitation.

### Core Principle: Risk Aversion and Reward Maximization

The strategy fundamentally believes that sustained cooperation is the optimal path to maximize long-term payoffs, given the significant reward multiplier 'k' when the threshold is met. However, it recognizes that outright cooperation in every round can be exploited if others consistently defect. Therefore, it seeks to *induce* cooperation by demonstrating a willingness to cooperate, but only when the group appears likely to succeed.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect in any given round (t > 1) is based on two primary factors:

*   **Observed Cooperation Rate in Previous Rounds:** The strategy tracks the proportion of players who cooperated in past rounds.
*   **The "Momentum" Towards Threshold:** The strategy looks for signs that the group is trending towards meeting the 'm' threshold.

Here's a breakdown of the decision logic:

**For Round t (where t > 1):**

*   **Calculate the "Effective Cooperation Percentage" (ECP):** This is the percentage of players who cooperated in the *previous* round (t-1).
    *   `ECP = (Number of players who cooperated in round t-1) / n`

*   **Define a "Confidence Zone" around the threshold:** We introduce a buffer to avoid overreacting to small fluctuations. Let's define a **Cooperation Confidence Threshold (CCT)**. A good starting point for CCT could be `m / n`. However, to be more adaptive, we can adjust this. A reasonable CCT for our decision rule is:
    *   `CCT = (m / n) * 1.1` (This means we're looking for at least 10% *more* than the minimum required cooperation percentage to feel confident).
    *   **Important Note:** The CCT should never be lower than `m/n`. If `(m/n) * 1.1 < m/n`, we use `m/n`.

*   **Decision Logic:**
    1.  **If `ECP >= CCT`:**  This indicates a strong likelihood that the group will meet or exceed the cooperation threshold in the current round.
        *   **Action:** **Cooperate (C)**. The strategy is willing to invest in the community project when the collective effort is clearly sufficient.
    2.  **If `ECP < CCT` AND `ECP >= (m / n)`:** The group is close to the threshold, but not yet confidently there. There's a risk that if this strategy defects, the group might fall below 'm'.
        *   **Action:** **Observe and Defect (D)**. In this delicate state, the strategy plays it safe. It keeps its endowment, hoping that others' cooperation will push the group over the threshold. This is a calculated risk to avoid being the sole cooperator in a failing cooperative attempt, but it also aims to not be the one tipping the balance away from cooperation.
    3.  **If `ECP < (m / n)`:** The group is significantly below the cooperation threshold. The probability of reaching 'm' is low, and continuing to cooperate would likely lead to a lower payoff than defecting.
        *   **Action:** **Defect (D)**. The strategy retreats to a guaranteed private payoff, preserving its resources until the group shows a stronger commitment to cooperation.

### 2. Handling Edge Cases

*   **Round 1 (Initial Round):**
    *   In the very first round, there is no history. The strategy needs an initial stance.
    *   **Decision:** **Cooperate (C)**. This is the most "cooperative" move to initiate. It signals a willingness to try for the mutually beneficial outcome. It assumes that in the absence of negative information, a proactive cooperative stance is the best way to establish a cooperative norm.
    *   **Rationale:** This altruistic first move is a gamble, but it sets a positive tone. If all players adopt similar initial cooperative strategies, the threshold is immediately met. If others defect, the strategy will then react based on the observed (low) cooperation rate in the subsequent rounds.

*   **Last Round (Round r):**
    *   The repeated nature of the game often leads to a "shadow of the future" effect, encouraging cooperation. However, in the final round, there's no future to incentivize.
    *   **Decision:** **Defect (D)**.
    *   **Rationale:** In the last round, there is no incentive to cooperate to build future cooperation. The strategy will simply aim to maximize its individual payoff based on the actions of others in that final round. This is a standard assumption in game theory for finitely repeated games with a clear end. The payoff from cooperation is only realized if the threshold is met, and there's no future benefit to be gained by achieving it in the last round.

*   **Threshold 'm' is 1 or n:** The problem statement specifies `1 < m < n`. This avoids trivial cases where cooperation is always guaranteed or impossible based on the number of players.

### 3. Being Cooperative: Alignment with Cooperative Mindset

This strategy is inherently cooperative in its **intent and long-term goal**:

*   **Prioritizes Collective Success:** The strategy's primary objective is to reach the cooperation threshold 'm' because it leads to a higher payoff for *everyone* when successful (`k > 1`).
*   **Adaptive and Forgiving (to a degree):** It doesn't immediately punish a single round of low cooperation. Instead, it gradually withdraws cooperation if the trend is consistently downwards, but it's always ready to re-engage if the group shows renewed commitment.
*   **Risk-Averse Towards Exploitation:** While cooperative in spirit, it's not naive. It protects itself from being the sole cooperator if the group is not moving towards the mutually beneficial outcome. This self-preservation is crucial for its long-term ability to participate and influence the group.
*   **Initiates Cooperation:** The first-round action is a deliberate act of initiating cooperation, demonstrating a belief in the possibility and desirability of a cooperative outcome.

### Pseudocode Representation

```
Strategy: Adaptive Threshold Seeker

Parameters:
  n: total number of players
  r: total number of rounds
  m: minimum cooperators needed
  k: reward factor (k > 1)

Variables:
  current_round: integer
  observed_cooperators_last_round: integer (initially 0)
  cooperation_confidence_threshold: float (calculated dynamically)

Initialization (Before Round 1):
  current_round = 1
  observed_cooperators_last_round = 0

Decision Function (For round t):
  If t == 1:
    Action = Cooperate (C)
    // For Round 1, we don't update observed_cooperators_last_round until after the round completes.
    // The initial value is a placeholder and not used for decision-making.
  Else If t == r:
    Action = Defect (D)
  Else: // t is between 2 and r-1
    // Calculate Cooperation Confidence Threshold (CCT)
    base_threshold_percentage = m / n
    cooperation_confidence_threshold = max(base_threshold_percentage, base_threshold_percentage * 1.1) // Ensure CCT is at least m/n

    // Calculate Effective Cooperation Percentage (ECP) based on previous round's outcome
    effective_cooperation_percentage = observed_cooperators_last_round / n

    If effective_cooperation_percentage >= cooperation_confidence_threshold:
      Action = Cooperate (C)
    Else If effective_cooperation_percentage >= base_threshold_percentage: // Close to threshold, but not confident
      Action = Defect (D)
    Else: // Significantly below threshold
      Action = Defect (D)

  Return Action

Update State (After Round t completes):
  If t < r:
    observed_cooperators_last_round = (number of players who played C in round t)
  current_round = t + 1

```

### Robustness and Adaptability

*   **Adaptability:** The strategy dynamically adjusts its decision threshold (`CCT`) and its cooperation behavior based on the actual observed actions of the group in the preceding round. It doesn't rely on fixed past patterns.
*   **Robustness:**
    *   **Against pure defectors:** If opponents consistently defect, the `ECP` will remain low, and this strategy will also defect, guaranteeing a payoff of 1.
    *   **Against exploitative cooperators:** If a few players cooperate and many defect, the `ECP` might be between `m/n` and `CCT`. In this scenario, the strategy defects, avoiding being the sole cooperator and receiving a payoff of 1 instead of potentially 2 (if it cooperated and the threshold was still not met).
    *   **Against a generally cooperative group:** If the group consistently cooperates or gets close to the threshold, the strategy will respond by cooperating, aiming for the higher payoff of `2`.
    *   **Against unpredictable behavior:** The reliance on the *previous round's* outcome makes it sensitive to immediate changes in group behavior, allowing it to adapt relatively quickly.

This "Adaptive Threshold Seeker" strategy aims to strike a balance between being a reliable cooperator and a cautious player, ultimately seeking the mutually beneficial outcome of collective success in the Collective Risk Dilemma.
'''

description_COOPERATIVE_29 = '''
This is a fascinating collective risk dilemma! The core challenge is balancing the individual incentive to defect (to guarantee a payoff of 1) against the collective benefit of cooperation (which can yield a payoff of 2 or 3 for cooperators, and 3 for defectors when the threshold is met). The repeated nature of the game and perfect information open the door for adaptive strategies.

My designed cooperative strategy, which I'll call **"Conditional Reciprocity with Risk Aversion" (CRRA)**, aims to achieve high cumulative payoffs by fostering cooperation when it's likely to succeed, while safeguarding against exploitation.

## Conditional Reciprocity with Risk Aversion (CRRA) Strategy

**Core Philosophy:** The CRRA strategy believes in the power of cooperation to achieve mutually beneficial outcomes. However, it recognizes that cooperation requires trust, and trust must be earned and maintained. It will cooperate when the conditions suggest a high probability of reaching the cooperation threshold, but will defect to protect itself if cooperation appears unlikely or if others are consistently defecting.

### 1. Decision Rules

The decision for player `i` in round `t` depends on the observed actions in previous rounds, specifically the proportion of cooperators and the player's own perceived risk.

**Key Metrics to Track:**

*   `N_coop_t-1`: The number of players who cooperated in the previous round (`t-1`).
*   `P_coop_t-1`: The proportion of players who cooperated in the previous round (`t-1`), calculated as `N_coop_t-1 / n`.
*   `My_cooperation_history`: A list of player `i`'s own past actions.
*   `N_defect_streak`: A counter for consecutive rounds where player `i` defected.

**Decision Logic for Player `i` in Round `t`:**

*   **If `t == 1` (First Round):**
    *   **Cooperate (C)**.
        *   *Rationale:* In the absence of any historical data, the strategy initiates cooperation to signal its cooperative intent and to explore the possibility of collective success. This also aligns with the "Be cooperative" requirement.

*   **If `t > 1` (Subsequent Rounds):**
    *   **Calculate the "Cooperation Likelihood Score" (CLS):**
        *   `CLS = P_coop_t-1 + (1 if My_cooperation_history[-1] == 'C' else 0) / n`
        *   *Rationale:* The CLS is a heuristic to estimate the probability of reaching the threshold `m`. It considers the overall cooperation rate from the previous round and adds a small bonus if *I* cooperated in the last round, indicating my continued commitment. This simple augmentation helps slightly bias towards cooperation if I personally sustained it.

    *   **Decision:**
        *   **If `CLS * n >= m` AND `My_cooperation_history[-1] != 'D'`:**
            *   **Cooperate (C)**
            *   *Rationale:* If the estimated likelihood of reaching the threshold is high, and I have not just defected in the previous round (to avoid being overly exploited), I will cooperate. This maintains the possibility of achieving the higher payoffs.

        *   **If `CLS * n < m` AND `My_cooperation_history[-1] == 'C'`:**
            *   **Defect (D)**
            *   *Rationale:* If the estimated likelihood of reaching the threshold is low, and I *did* cooperate in the previous round, it indicates my cooperation might have been in vain. To prevent being exploited (i.e., getting a payoff of 1 when others defected and the threshold wasn't met), I will defect.

        *   **If `My_cooperation_history[-1] == 'D'`:**
            *   **Defect (D)**
            *   *Rationale:* If I defected in the previous round, it was likely a response to perceived low cooperation or exploitation. I will continue to defect for a few more rounds to observe the group's behavior and ensure my defection is not leading to a persistent failure to reach `m`. This is a "cool-down" period for defection.

        *   **If `CLS * n >= m` AND `My_cooperation_history[-1] == 'D'`:**
            *   **Cooperate (C)**
            *   *Rationale:* Even if I defected last round, if the overall group cooperation is still strong enough to likely meet `m`, I will re-initiate cooperation. This is an attempt to rejoin the cooperative fold.

        *   **Otherwise (default):**
            *   **Defect (D)**
            *   *Rationale:* In ambiguous situations, or when the other conditions don't clearly favor cooperation, the strategy defaults to defecting to secure the guaranteed payoff of 1, especially if the risk of not reaching `m` is high.

**Refinement of the Defection Streak:**
The "If `My_cooperation_history[-1] == 'D'`" rule needs a little more nuance. Instead of defaulting to defect indefinitely, I introduce a limited defection period.

*   Let `Defect_Punishment_Rounds` be a parameter, perhaps set to `max(2, r // 5)`. This means I'll defect for at least 2 rounds or 20% of the total rounds, whichever is greater, after a personal defection, to gather information and signal displeasure.

**Revised Decision Logic:**

1.  **If `t == 1`:** Cooperate (C).
2.  **If `t > 1`:**
    *   Calculate `P_coop_t-1`.
    *   **If `My_cooperation_history` contains `D` for the last `Defect_Punishment_Rounds`:**
        *   **If `P_coop_t-1 * n >= m`:** Cooperate (C) - Rejoining cooperation.
        *   **Else:** Defect (D) - Continuing to punish/observe.
    *   **Else (I haven't defected recently or my defection period is over):**
        *   **If `P_coop_t-1 * n >= m`:** Cooperate (C) - High likelihood of success.
        *   **Else (`P_coop_t-1 * n < m`):** Defect (D) - Low likelihood of success, protect individual payoff.

This revised logic is more sophisticated. It allows for a temporary "punishment" phase after defection but still has a mechanism to return to cooperation if the group's overall cooperation is sufficient.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):** As stated, always Cooperate (C). This is the critical signal of cooperative intent and an attempt to kickstart collective success.
*   **Last Round (`t = r`):**
    *   **If the cumulative payoff of cooperating in the last round (assuming `m` is met) would be higher than defecting in the last round, and there's a reasonable chance `m` will be met:** Cooperate (C).
    *   **Otherwise:** Defect (D).
    *   *Rationale:* In the final round, there's no future to build reputation or enforce reciprocity. The decision becomes a purely payoff-maximizing one for the last round. However, since the strategy is generally cooperative, it will lean towards cooperation if the immediate outcome is still favorable.
*   **`m` close to `n`:** When `m` is very high (e.g., `m = n-1`), cooperation becomes very risky. The strategy will be more inclined to defect earlier if the number of cooperators is not consistently high.
*   **`m` close to 1:** This parameter is invalid per the rules (`1 < m < n`), but if it were allowed, the strategy would heavily favor cooperation.
*   **Low `k`:** If `k` is only slightly greater than 1, the reward for cooperation is marginal. The strategy might become more cautious and defect more readily if the threshold isn't consistently met.
*   **High `k`:** A high `k` makes cooperation significantly more attractive. The strategy will be more willing to take risks to achieve the higher payoffs.

### 3. Being Cooperative

The CRRA strategy is inherently cooperative in its design and intent for several reasons:

*   **Initiation of Cooperation:** It always starts by cooperating, actively trying to build a cooperative environment.
*   **Conditional Cooperation:** It cooperates when the conditions are favorable and cooperation is likely to benefit everyone. It doesn't defect out of malice, but out of a calculated assessment of risk and potential reward.
*   **Rejoining Cooperation:** The strategy includes mechanisms to return to cooperation even after periods of defection, provided the group demonstrates sufficient cooperative effort. This shows a willingness to forgive and rebuild trust.
*   **Focus on Collective Success:** The core logic is driven by the desire to reach the cooperation threshold `m`, which unlocks the higher payoffs for all players.

### Pseudocode for CRRA Strategy

```python
# Parameters (assume these are globally available)
# n: number of players
# r: number of rounds
# m: minimum cooperators needed
# k: reward factor

# Internal state variables for player i
my_cooperation_history = [] # Stores 'C' or 'D' for player i's past actions
last_round_actions = []     # Stores actions of ALL players from the previous round
n_defect_punishment_rounds = max(2, r // 5) # Number of rounds to punish after defecting

def decide_action(current_round, all_previous_actions):
    """
    Decides whether to Cooperate (C) or Defect (D) for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        all_previous_actions (list of lists): A list where each inner list
                                               contains the actions of all players
                                               in a previous round.
                                               e.g., [['C', 'D', ...], ['C', 'C', ...]]

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    global my_cooperation_history, n_defect_punishment_rounds

    # --- Round 1: Always Cooperate ---
    if current_round == 1:
        my_cooperation_history.append('C')
        return 'C'

    # --- Subsequent Rounds ---

    # Extract actions from the previous round
    if not all_previous_actions: # Should not happen after round 1
        my_cooperation_history.append('C') # Fallback
        return 'C'
    
    last_round_actions_this_round = all_previous_actions[-1]
    num_cooperators_last_round = last_round_actions_this_round.count('C')
    prop_cooperators_last_round = num_cooperators_last_round / n

    # Check recent defection history for player i
    recent_def_count = 0
    for action in reversed(my_cooperation_history[-n_defect_punishment_rounds:]):
        if action == 'D':
            recent_def_count += 1
        else:
            break # Stop counting if a cooperation is found

    # --- Decision Logic ---

    # Case 1: Player i defected recently and is still in punishment phase
    if recent_def_count >= n_defect_punishment_rounds and len(my_cooperation_history) >= n_defect_punishment_rounds:
        if prop_cooperators_last_round * n >= m:
            # Group cooperation is sufficient, rejoin
            my_cooperation_history.append('C')
            return 'C'
        else:
            # Group cooperation is insufficient, continue observing/punishing
            my_cooperation_history.append('D')
            return 'D'

    # Case 2: Player i did not defect recently, or punishment phase is over
    else:
        if prop_cooperators_last_round * n >= m:
            # Likelihood of reaching threshold is high
            my_cooperation_history.append('C')
            return 'C'
        else:
            # Likelihood of reaching threshold is low, defect to secure individual payoff
            my_cooperation_history.append('D')
            return 'D'

    # Fallback for any unhandled scenario (should ideally not be reached)
    my_cooperation_history.append('D')
    return 'D'

```

This strategy aims to be robust by not relying on predictable behavior from others, but instead by reacting to observed patterns and prioritizing long-term cumulative payoff through adaptive cooperation. It embodies the spirit of conditional cooperation, aiming for the best collective outcome when possible, but wisely protecting individual interests when necessary.
'''

description_COOPERATIVE_30 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust while adhering to the specified constraints.

## Cooperative Strategy: "Cautious Optimist"

This strategy aims to achieve the collective reward by cooperating when the probability of success is high, while protecting itself when cooperation is unlikely to succeed. It learns from past rounds to adjust its behavior.

### Core Principles:

1.  **Conditional Cooperation:** Cooperate only when there's a reasonable expectation that the threshold *m* will be met.
2.  **Adaptive Threshold:** Dynamically adjust the perceived likelihood of others cooperating based on historical data.
3.  **Punishment & Reward:** Implicitly "punishes" defection by withholding cooperation when the threshold is likely unmet, and "rewards" successful cooperation by participating.
4.  **End-of-Game Consideration:** Slightly more cautious in the final round to secure accumulated payoffs.

### 1. Decision Rules:

The core of the "Cautious Optimist" strategy is to estimate the probability of reaching the cooperation threshold *m* in the current round. This estimation is based on the proportion of players who cooperated in previous rounds.

Let:
*   `N_cooperators_t-1` be the number of players who cooperated in round `t-1`.
*   `total_players` be `n`.
*   `cooperation_rate_history` be a smoothed average of `N_cooperators_t-1 / total_players` over the last few rounds (e.g., last 3-5 rounds, or all past rounds if fewer than that). We'll use a simple moving average.
*   `cooperation_threshold_factor` be a parameter that determines how confident we need to be. A value of 0.8 means we want to be reasonably sure that at least 80% of the players needed for cooperation will cooperate.

**Decision Logic for Player *i* in Round *t*:**

1.  **Initial Rounds (Early in the game, e.g., first few rounds):**
    *   If `n` is large and `m` is relatively small compared to `n`, it might be beneficial to attempt cooperation to establish a norm.
    *   However, to remain cautious, we will rely on the historical data. If there's no history, we will default to a slightly risk-averse stance.
    *   **Rule:** In the first few rounds (e.g., first `min(r/4, 5)` rounds), if the estimated likelihood of reaching *m* (based on a conservative assumption or a very short history) is above a certain confidence level (e.g., 60%), cooperate. Otherwise, defect.

2.  **Established Rounds (Most of the game):**
    *   Calculate the `cooperation_rate_history` based on past rounds.
    *   Estimate the expected number of cooperators in the current round: `expected_cooperators = cooperation_rate_history * total_players`.
    *   **Decision:**
        *   If `expected_cooperators >= m - buffer`, then **Cooperate (C)**.
        *   If `expected_cooperators < m - buffer`, then **Defect (D)**.

    *   **`buffer`:** This is a critical parameter for robustness. It introduces a safety margin. A buffer of `m * 0.2` (20% of the threshold) would mean we need to be reasonably confident that `m * 0.8` players will cooperate. This accounts for noise, potential defectors, and the risk of misinterpreting past behavior.

3.  **Last Round (Round *r*):**
    *   In the final round, there's no future to incentivize cooperation. The goal is to maximize the accumulated payoff.
    *   **Rule:** Defect (D). This is a pure self-preservation strategy in the face of the end of the game. *Correction: This is not always the most cooperative. A more cooperative approach in the last round is to still try to achieve the reward if the probability is high, as it benefits everyone. Let's refine this.*

    *   **Revised Rule for Last Round (*r*):**
        *   Calculate `cooperation_rate_history` based on rounds 1 to `r-1`.
        *   Estimate `expected_cooperators` in round `r`.
        *   **Decision:**
            *   If `expected_cooperators >= m - buffer`, then **Cooperate (C)**. (Still trying to achieve the collective good one last time).
            *   If `expected_cooperators < m - buffer`, then **Defect (D)**.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   Since there's no history, we must make an assumption. A conservative approach is to start by defecting. This protects against being exploited early on. However, to be *cooperative*, we need to try and initiate cooperation.
    *   **Revised First Round Rule:** In the absence of history, assume that a small but significant fraction of players will cooperate. Let's say, we *hope* for `m` players to cooperate. We will cooperate if `m <= n/2` (a rough heuristic that cooperation is plausible) or if `m` is very close to `n`. Otherwise, we defect in the first round. This is a compromise between extreme caution and initiating cooperation.
    *   *Alternative for stronger cooperation:* Cooperate in the first round if `m <= n/2` (meaning it's not an impossible task for more than half the players to cooperate) and `k` is significantly large relative to the private payoff.

*   **Last Round (t=r):**
    *   As discussed above, the strategy attempts to cooperate if the historical data suggests a high likelihood of success, even in the last round. This is to fulfill the "cooperative" aspect of the strategy and aim for the collective reward one last time.

*   **Insufficient Data for Moving Average:**
    *   If the number of past rounds is less than the window size for the moving average, use all available past rounds for the calculation.
    *   For the very first round, no history is available. We'll use a predefined heuristic (as described in the "First Round" section).

### 3. Cooperativeness:

The "Cautious Optimist" strategy is designed to be cooperative through:

*   **Proactive Cooperation Attempts:** It will initiate cooperation when it has a reasonable belief that others will do the same, rather than waiting for others to prove themselves first. This is seen in the initial rounds and the last round decisions.
*   **Long-Term Gain Focus:** By aiming for the `k` reward, it prioritizes the collective benefit over immediate personal gains (keeping the endowment).
*   **Learning and Adapting for Collective Success:** The strategy uses historical data to learn about the group's behavior. It cooperates more when the group has shown a tendency to cooperate and defects when it predicts failure, thus avoiding contributing to a failed project and encouraging future cooperation.
*   **Parameter Tuning (Implicit):** The choice of `buffer` directly influences the level of risk taken. A smaller buffer means more aggressive cooperation, while a larger buffer means more caution.

### Pseudocode:

```pseudocode
// Game Parameters
n: total_players
r: total_rounds
m: cooperation_threshold
k: reward_factor
buffer_factor = 0.2 // e.g., 20% of m

// Strategy State
history = [] // List of dictionaries, each containing {'player_actions': [action_1, ..., action_n], 'round_payoffs': [...]}
round_number = 1

// Helper function to calculate cooperation rate from history
function calculate_cooperation_rate(history, total_players):
    if length(history) == 0:
        return 0.0
    
    // Simple Moving Average over last 5 rounds (or all if fewer)
    window_size = min(length(history), 5)
    recent_rounds = history[length(history) - window_size : length(history)]
    
    total_cooperators_in_window = 0
    for round_data in recent_rounds:
        for action in round_data['player_actions']:
            if action == 'C':
                total_cooperators_in_window += 1
    
    return total_cooperators_in_window / (window_size * total_players)

// Main Decision Function for player i in round t
function decide_action(n, m, k, r, round_number, history):
    
    cooperation_threshold_needed = m
    buffer = buffer_factor * cooperation_threshold_needed

    if round_number == 1:
        // First round heuristic: Cooperate if m is not too high relative to n, or if k is very rewarding.
        // A simple heuristic: cooperate if m <= n/2 (plausible for more than half to cooperate)
        if m <= n / 2: // This heuristic could be adjusted
            return 'C'
        else:
            return 'D'
            
    else if round_number == r:
        // Last round: Still attempt cooperation if likely to succeed, but more cautious.
        cooperation_rate = calculate_cooperation_rate(history, n)
        expected_cooperators = cooperation_rate * n
        
        if expected_cooperators >= cooperation_threshold_needed - buffer:
            return 'C'
        else:
            return 'D'
            
    else: // Established rounds (2 to r-1)
        cooperation_rate = calculate_cooperation_rate(history, n)
        expected_cooperators = cooperation_rate * n
        
        if expected_cooperators >= cooperation_threshold_needed - buffer:
            return 'C'
        else:
            return 'D'

// Example of how history might be updated after each round
// after_round_t(round_t_data):
//     history.append(round_t_data)
//     round_number += 1
```

**Explanation of the Pseudocode and Heuristics:**

*   **`buffer_factor`**: This is a tunable parameter. A `0.2` means we are willing to cooperate if we believe at least `80%` of the required cooperators (`m`) will show up. This provides a safety net.
*   **First Round Heuristic (`m <= n/2`)**: This is a simplified rule. The idea is that if the threshold requires less than half of the players to cooperate, it's a more achievable goal, and thus worth attempting cooperation early to signal cooperative intent. If `m` is a large majority, it's much harder to coordinate, so defecting might be wiser initially until more information is gathered.
*   **`calculate_cooperation_rate`**: Uses a simple moving average. This is a common technique to smooth out short-term fluctuations in behavior and capture longer-term trends. The window size (`min(length(history), 5)`) means it looks at the last 5 rounds, but if fewer than 5 rounds have occurred, it uses all of them.
*   **Decision Logic:** The core logic (`expected_cooperators >= cooperation_threshold_needed - buffer`) translates to: "If the number of cooperators we *expect* to see is at least the required threshold minus our buffer for error, then cooperate."

This strategy is cooperative because it actively seeks the common good (`k` reward) by taking calculated risks. It's adaptive because it learns from history. It's robust because the `buffer` accounts for uncertainty and potential defectors, and it has specific logic for the beginning and end of the game.
'''

description_COOPERATIVE_31 = '''
Here is a design for a cooperative strategy for the Collective Risk Dilemma, focusing on adaptiveness and robustness:

## Strategy: The "Adaptive Reciprocator"

This strategy aims to foster cooperation by rewarding it and punishing defection, but with a crucial adaptive element that makes it resilient to varying player behaviors. The core idea is to start with a cooperative stance and adjust based on the group's observed cooperativeness.

### 1. Decision Rules:

The decision to cooperate or defect in any given round is based on a dynamic assessment of the group's behavior in previous rounds.

**Central Metric: Group Cooperation Ratio (GCR)**

We will track the proportion of players who cooperated in the *immediately preceding round*.

*   **GCR_t = (Number of players who cooperated in round t) / n**

**Decision Logic:**

*   **Initial Cooperation:** In the very first round, or if the strategy is just being initialized, the default action is **Cooperate (C)**. This signals a willingness to engage in cooperation and attempts to kickstart the positive feedback loop.
*   **Sustaining Cooperation:** If the Group Cooperation Ratio in the previous round (GCR_{t-1}) was **sufficiently high**, the player will **Cooperate (C)** in the current round.
*   **Responding to Defection:** If the Group Cooperation Ratio in the previous round (GCR_{t-1}) falls **below a certain threshold**, the player will **Defect (D)** in the current round.

**Defining "Sufficiently High" and the Threshold:**

To be adaptive and robust, these thresholds need to be dynamic or intelligently set.

*   **Cooperation Threshold (T_coop):** If GCR_{t-1} >= T_coop, cooperate.
*   **Defection Threshold (T_defect):** If GCR_{t-1} < T_defect, defect.

**Adaptive Threshold Adjustment:**

The core of the "Adaptive Reciprocator" lies in how these thresholds are adjusted. The strategy aims to find a balance that encourages cooperation without being overly exploited.

*   **Initial State:** Start with a moderate cooperation threshold, for example, **T_coop = m/n**. This is the minimum required proportion of cooperators for the group to succeed. We might also set **T_defect = m/n** initially.
*   **Learning from History:**
    *   **If the group *just barely* met the cooperation threshold (m players cooperated) in the previous round:** This suggests that cooperation is fragile. The strategy might *slightly decrease* its `T_coop` and `T_defect` to be more forgiving of minor dips in cooperation, making it less likely to trigger a defection cascade. However, this decrease should be small.
    *   **If the group *significantly exceeded* the cooperation threshold (e.g., GCR_{t-1} is substantially higher than m/n):** This indicates strong cooperation. The strategy can become more confident and might *slightly increase* its `T_coop` and `T_defect` to test the boundaries of this sustained cooperation. This helps in identifying the true level of commitment from other players.
    *   **If the group *failed to meet* the cooperation threshold (GCR_{t-1} < m/n) and the strategy defected in response:**
        *   If cooperation then *improves* in the subsequent round (even if still below m/n), it suggests the defection was a potentially valid response and the player can consider a slight *increase* in `T_defect` to be more aggressive in punishing further failures.
        *   If cooperation *worsens* or stays the same, the strategy might *slightly decrease* `T_defect` to be more sensitive to future failures, signaling a stronger aversion to repeated group failures.

**Crucial Note on Thresholds:** The thresholds should be updated *after* the current round's decision is made, based on the observed outcomes of the *previous* round.

### 2. Handling Edge Cases:

*   **First Round (t=1):** Always **Cooperate (C)**. This is a universal cooperative starting point. The strategy has no history to adapt from, so it assumes the best.
*   **Last Round (t=r):** This is where the strategy becomes a bit more pragmatic. Since there are no future rounds to influence, the incentive for cooperation to achieve future rewards is gone. In the last round, the player should act to **maximize their individual payoff based on the known history and the likely actions of others**. This typically means playing **Defect (D)** if the individual payoff from defecting (1) is guaranteed or likely higher than the payoff from cooperating (which depends on others' actions and whether the m threshold is met). However, if there's a *very high probability* that *enough other players will cooperate to meet the threshold*, then cooperating might still be the individually optimal choice in the last round (getting 0 + k). A robust way to handle this is to estimate the probability of the threshold being met in the last round. If this probability is low, defect. If high, cooperate.

### 3. Be Cooperative:

The "Adaptive Reciprocator" is inherently cooperative in its design:

*   **Initial Benevolence:** It starts by signaling a willingness to cooperate, providing the best chance for the group to achieve the high collective reward.
*   **Punishment as Deterrent:** Defection is not retaliatory in a vindictive sense, but rather a mechanism to deter free-riding and signal dissatisfaction with the group's failure to achieve the collective good. It's a way to "teach" the group that cooperation is necessary.
*   **Forgiveness and Reciprocity:** The adaptive thresholds allow for forgiveness. If a temporary dip in cooperation occurs, the strategy doesn't immediately abandon cooperation. It attempts to recalibrate and continue working towards the collective goal.
*   **Long-Term Orientation:** The strategy is designed for repeated interaction. Its goal is not to win a single round, but to maximize its total payoff over all rounds, which is achieved by promoting stable cooperation.

### Pseudocode Representation:

```python
class AdaptiveReciprocator:
    def __init__(self, n, m, k):
        self.n = n
        self.m = m
        self.k = k
        self.cooperation_threshold = m / n # Initial estimate of required cooperation
        self.defection_threshold = m / n   # Initial estimate of when to defect

        self.history = [] # Stores tuples of (player_actions_in_round, player_payoff_in_round) for each round

    def decide_action(self, current_round):
        if current_round == 1:
            return 'C' # Always cooperate in the first round

        # Calculate Group Cooperation Ratio from the previous round
        prev_round_actions, _ = self.history[-1]
        num_cooperators_prev_round = sum(1 for action in prev_round_actions if action == 'C')
        gcr_prev_round = num_cooperators_prev_round / self.n

        # Last Round Logic (simplified - a more complex probabilistic model could be used)
        if current_round == self.r:
            # Estimate probability of threshold being met. For simplicity, if gcr_prev_round is high, assume it will be met.
            # In a real implementation, one might model other players' strategies more deeply.
            if gcr_prev_round >= self.cooperation_threshold: # Heuristic for likelihood of meeting threshold
                return 'C'
            else:
                return 'D' # No future rounds to incentivize cooperation

        # Adaptive Decision Logic for intermediate rounds
        if gcr_prev_round >= self.cooperation_threshold:
            return 'C' # Cooperate if previous round's cooperation was sufficient
        else:
            return 'D' # Defect if previous round's cooperation fell short

    def update_thresholds(self):
        # This function would be called after all players have made their move and payoffs are known.
        # The logic here would adjust self.cooperation_threshold and self.defection_threshold
        # based on the observed GCR of the *last* completed round and the effectiveness of the last decision.

        if len(self.history) < 2: # Need at least two rounds of history to update meaningfully
            return

        prev_round_actions, _ = self.history[-2] # Actions from 2 rounds ago
        num_cooperators_prev_prev_round = sum(1 for action in prev_round_actions if action == 'C')
        gcr_prev_prev_round = num_cooperators_prev_prev_round / self.n

        last_round_actions, _ = self.history[-1] # Actions from the last completed round
        num_cooperators_last_round = sum(1 for action in last_round_actions if action == 'C')
        gcr_last_round = num_cooperators_last_round / self.n

        # Example update logic (simplified, can be made more sophisticated):
        # If cooperation was just met, be more forgiving
        if m/n <= gcr_last_round < self.cooperation_threshold:
            self.cooperation_threshold = max(m/n, self.cooperation_threshold * 0.98) # Slightly lower threshold
            self.defection_threshold = max(m/n, self.defection_threshold * 0.98)
        # If cooperation was very strong, test slightly higher thresholds
        elif gcr_last_round >= self.cooperation_threshold * 1.1: # Significantly above current threshold
            self.cooperation_threshold = min(1.0, self.cooperation_threshold * 1.02) # Slightly higher threshold
            self.defection_threshold = min(1.0, self.defection_threshold * 1.02)
        # If cooperation failed repeatedly and we defected, maybe be more sensitive to failure
        elif gcr_last_round < m/n and self.strategy_used_in_last_round == 'D': # If we defected and still failed
            self.defection_threshold = max(m/n, self.defection_threshold * 0.99) # Be more sensitive to failure

        # Ensure thresholds stay within reasonable bounds (e.g., >= m/n and <= 1)
        self.cooperation_threshold = max(self.m / self.n, min(1.0, self.cooperation_threshold))
        self.defection_threshold = max(self.m / self.n, min(1.0, self.defection_threshold))


# Note: The 'strategy_used_in_last_round' would need to be stored to implement the last bullet point of update logic.
```

This "Adaptive Reciprocator" strategy attempts to balance the inherent temptation to defect with the desire to achieve the higher collective reward. Its adaptiveness allows it to learn from the group's behavior and adjust its sensitivity, making it robust to a variety of opponent strategies.
'''

description_COOPERATIVE_32 = '''
## Cooperative Strategy: "Adaptive Threshold Seeker"

This strategy aims to achieve cooperation by dynamically adjusting its behavior based on the observed actions of other players, with a strong bias towards cooperation when it seems beneficial and safe. It is designed to be adaptive to a wide range of opponent behaviors and robust to the inherent uncertainty of the game.

### Core Philosophy:

The strategy operates on the principle of **"optimistic pragmatism."** It aims for the high payoff of collective cooperation (when the threshold `m` is met) but is prepared to defect to maximize its individual gain if cooperation appears unlikely or too risky. It actively monitors the "cooperative momentum" of the group and adjusts its own actions accordingly.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round `t` (where `t > 1`) is based on the following criteria:

**A. Initial Cooperation Tendency:**

*   **In Round 1:** The player will **Cooperate (C)**. This is the most direct way to signal a cooperative intent and to attempt to trigger the rewarding outcome. The initial endowment of 1 is sacrificed with the hope of a payoff of `1 + k` if enough others also cooperate.

**B. Adaptive Cooperation Logic (for rounds t > 1):**

The decision in round `t` depends on the observed number of cooperators in the *previous* round, `t-1`. Let `N_C(t-1)` be the number of players who cooperated in round `t-1`.

*   **If `N_C(t-1)` is "sufficiently close" to `m` (from above or at `m`):** The player will **Cooperate (C)**.
    *   **Definition of "sufficiently close":** `N_C(t-1) >= m - 1` (This means if the previous round was at or one player short of the threshold, we continue to cooperate to try and meet it).
    *   **Rationale:** This is the crucial adaptive element. If the group was close to success in the previous round, it suggests that collective cooperation is achievable and beneficial. Continuing to cooperate increases the likelihood of reaching the threshold in the current round.

*   **If `N_C(t-1)` is "significantly below" `m`:** The player will **Defect (D)**.
    *   **Definition of "significantly below":** `N_C(t-1) < m - 1` (This means if the previous round was two or more players short of the threshold).
    *   **Rationale:** If the group consistently fails to reach the threshold, individual cooperation is punished (payoff of 0 instead of 1). In this scenario, defecting is the rational choice to secure the private endowment of 1. This prevents the strategy from being exploited by consistently cooperating when others defect.

*   **Special Case: If `k` is very high relative to `n`:** (e.g., `k > n` or some other threshold that makes `k-1` a significant portion of the total potential private gain from defection for everyone)
    *   Even if `N_C(t-1) >= m - 1`, if the probability of *all* players cooperating and achieving the reward seems low based on recent history (e.g., if the highest `N_C` observed in the last `x` rounds is still below `m`), the strategy might consider defecting to secure its own payoff of 1. However, for this strategy, we will prioritize the simpler `N_C(t-1) >= m - 1` rule for now to maintain a strong cooperative bias. The adaptive nature should implicitly handle this by observing low `N_C` values.

### 2. Handling Edge Cases:

*   **First Round (t=1):** As stated above, the strategy will **Cooperate (C)**. This is a crucial initial signal to encourage collective action.

*   **Last Round (t=r):** In the last round, the long-term reputational aspect of repeated games is removed. Players have no incentive to cooperate to build future trust. Therefore, the rational decision in the last round is to **Defect (D)**. This is because the payoff from cooperation requires others to cooperate as well, and the risk of being the sole cooperator is high, leading to a payoff of 0. Defecting guarantees a payoff of 1.

*   **Rounds with Insufficient Data (e.g., first few rounds):** The strategy relies on the history of the *previous* round. For `t=2`, it uses `N_C(1)`. For `t=3`, it uses `N_C(2)`, and so on. The initial cooperation in Round 1 sets the baseline.

### 3. Being Cooperative:

This strategy is inherently cooperative due to the following design choices:

*   **Initial Cooperation:** It starts by cooperating, actively trying to achieve the collective good.
*   **Threshold Proximity:** It continues to cooperate as long as the group is "close" to meeting the cooperative threshold (`m`). This demonstrates a willingness to persevere and trust that others will also contribute.
*   **Reward Maximization:** The strategy's objective is to reach the higher payoff state (`m` or more cooperators) where everyone benefits. It doesn't simply aim for its own personal maximum payoff in every single round if that means sacrificing a potentially larger collective payoff.
*   **Robustness:** While it defects when cooperation is clearly failing, this is a defensive measure to prevent exploitation, not an aggressive stance. It's designed to *enable* cooperation when possible, not to actively undermine it.

### Pseudocode Representation:

```pseudocode
// Strategy: Adaptive Threshold Seeker

// Game Parameters (assumed to be known)
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

// History (global or passed in)
// actions_history: a list of lists, where actions_history[t] is a list of actions of all players in round t.
// For this strategy, we only need the count of cooperators from the previous round.

function decide_action(current_round_number, actions_history):
  // actions_history stores actions of rounds 0 to current_round_number - 1
  // where round 0 is the first round (t=1 in problem description)

  if current_round_number == 1: // Corresponds to r=1 in problem description
    return "C" // Cooperate in the first round

  if current_round_number == r: // Corresponds to r=r in problem description
    return "D" // Defect in the last round

  // For rounds between 1 and r (exclusive)
  previous_round_actions = actions_history[current_round_number - 1] // Get actions from the prior round
  num_cooperators_previous_round = count_cooperators(previous_round_actions)

  // Decision logic based on previous round's cooperation count
  if num_cooperators_previous_round >= m - 1:
    return "C" // Cooperate if previous round was at or near the threshold
  else:
    return "D" // Defect if previous round was far below the threshold

function count_cooperators(actions_list):
  count = 0
  for action in actions_list:
    if action == "C":
      count = count + 1
  return count

```

This strategy is a careful balance of optimism for collective reward and pragmatism to avoid exploitation. Its adaptive nature allows it to learn from the group's behavior and adjust its approach to maximize long-term, cooperative outcomes.
'''

description_COOPERATIVE_33 = '''
Here's a design for a cooperative strategy for the Collective Risk Dilemma, emphasizing adaptability and robustness:

## Strategy: "Adaptive Threshold Cooperative" (ATC)

This strategy aims to be cooperative by default, but it's also pragmatic and responsive to the collective actions of others. It focuses on reaching the cooperation threshold to unlock the higher payoffs for everyone, while also protecting itself if cooperation fails.

### 1. Decision Rules

The core of the ATC strategy is a dynamic assessment of the current round's cooperation level and the history of successful cooperation.

**Decision Logic for Player i in Round t:**

*   **Initial Tendency (First Round & Low Confidence):** In the very first round, and in subsequent rounds where there's a strong history of failed cooperation, the strategy will err on the side of caution. It will default to **Cooperate (C)**. This is because the reward for cooperation (`k`) is significant, and taking the initial risk is often necessary to signal intent and encourage others. The primary objective is to *try* to reach the threshold.

*   **Monitoring Collective Success:** The strategy will continuously track the number of players who cooperated in the *previous* round (`num_cooperated_t-1`).

*   **Responding to Success:**
    *   If `num_cooperated_t-1 >= m`: This indicates that the group successfully met the cooperation threshold in the previous round. The ATC strategy will **Cooperate (C)** in the current round. This reinforces successful cooperation and signals a commitment to the collective good when it's demonstrably beneficial.

*   **Responding to Failure (Below Threshold):**
    *   If `num_cooperated_t-1 < m`: The group failed to meet the cooperation threshold in the previous round. This presents a crucial branching point:
        *   **If the current round is *not* the last round (t < r):** The strategy will **Cooperate (C)**. The rationale here is that a single round of failure doesn't necessarily mean the group cannot coordinate. Continuing to cooperate can be a signal of persistence and a willingness to try again. It's still in the interest of *all* players to eventually reach `m` cooperators, and defection might lead to a permanent stalemate where everyone gets only their private endowment.
        *   **If the current round *is* the last round (t = r):** This is a critical decision. If cooperation has consistently failed to reach the threshold, and we are in the final round, there is no future incentive to cooperate. The strategy will **Defect (D)**. This is a pragmatic move to secure the private endowment (1) in the absence of any possibility of achieving the higher reward.

*   **Escalation Mechanism (Optional, but enhances robustness):** To be even more adaptive, we can introduce a slight modification to the "Responding to Failure" rule. If cooperation has failed *multiple* times consecutively, the strategy might consider a temporary "punishment" before reverting to cooperation.
    *   **Revised Failure Response (t < r):** If `num_cooperated_t-1 < m`:
        *   Track the number of *consecutive* rounds where the threshold was not met. Let this be `consecutive_failures`.
        *   If `consecutive_failures < some_threshold` (e.g., 2 or 3): **Cooperate (C)**.
        *   If `consecutive_failures >= some_threshold`: **Defect (D)** for *one* round as a signal of discontent, and then revert to **Cooperate (C)** in the subsequent round. The idea is to show that failure has consequences without completely abandoning the possibility of future cooperation. This is a gentle "punishment" to encourage better collective behavior. If this single defection doesn't lead to success in the next round, the strategy will resume its default of cooperating.
        *   If `consecutive_failures` reaches a very high number (e.g., `r/2`), it might be more optimal to always defect from that point onwards. This is a pragmatic exit from a consistently failing game.

**Simplified Decision Rule (Focus on cooperativeness):**

For clarity and to emphasize the "cooperative" aspect, let's present a primary rule without the explicit "escalation mechanism" for now. The core idea is to always try to cooperate unless it's demonstrably and irrevocably futile.

**ATC Strategy Rule:**

*   **In round `t` (from 1 to `r`):**
    *   **If `t == 1` (First Round): Cooperate (C)**.
    *   **If `t > 1`:**
        *   Look at the number of cooperators in round `t-1` (let's call this `cooperators_last_round`).
        *   **If `cooperators_last_round >= m`:** **Cooperate (C)**. (Reward the success).
        *   **If `cooperators_last_round < m`:**
            *   **If `t < r` (Not the last round): Cooperate (C)**. (Persist in the hope of reaching the threshold).
            *   **If `t == r` (Last Round): Defect (D)**. (Secure private endowment when reward is no longer possible).

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated above, the strategy **Cooperates (C)**. This is crucial to initiate the game cooperatively and set a positive example, or at least to provide the opportunity for others to see a cooperator. It's an investment in potential future collective success.

*   **Last Round (t=r):** The strategy **Defects (D)**. This is a pragmatic adaptation. Since there are no future rounds to benefit from cooperation, the optimal strategy shifts to maximizing immediate personal gain by keeping the endowment. This is a form of backward induction, common in game theory.

*   **Insufficient Players (n < m):** This is an impossible scenario given the parameter constraint `1 < m < n`. However, if the constraint were relaxed, the strategy would implicitly handle this by never being able to reach `m` cooperators, leading to consistent failure and eventual defection in the last round.

*   **Threshold `m` too high or too low:** The strategy's adaptive nature means it can function. If `m` is very high (close to `n`), cooperation will be harder to achieve, and the strategy will more frequently experience failure, but it will still attempt to cooperate until the last round. If `m` is low (just above 1), cooperation will be easier, and the strategy will likely maintain cooperation for most rounds.

*   **Reward `k` very high or very low:** The strategy doesn't explicitly use the *value* of `k` in its decision-making, only whether the threshold for receiving it was met. This makes it robust to different reward levels. However, the *incentive* for other players to cooperate will be influenced by `k`, which indirectly affects the outcomes the ATC strategy observes.

### 3. Being Cooperative

This strategy is inherently cooperative in its default behavior and its overriding goal:

*   **Default to Cooperation:** The primary action is to cooperate, especially in the early and middle stages of the game. This reflects a willingness to contribute to the collective good.
*   **Enabling Collective Success:** By cooperating when others have cooperated (`cooperators_last_round >= m`), the strategy actively reinforces successful collective action. This is not just about personal gain; it's about perpetuating a beneficial outcome for everyone.
*   **Patience and Persistence:** The strategy doesn't immediately abandon cooperation after one failure. It continues to cooperate in intermediate rounds (`t < r` and `cooperators_last_round < m`) to give the group a chance to self-organize and reach the threshold. This demonstrates a commitment to finding a cooperative equilibrium.
*   **Pragmatic Endgame:** The defection in the last round is not malicious but a rational response to the game's structure. It acknowledges that in the absence of future interaction, a purely cooperative stance can lead to exploitation. This pragmatism doesn't negate the cooperative intent of the earlier rounds.

### Pseudocode for ATC Strategy:

```python
# Global state for the strategy across rounds
# Needs to be initialized before the tournament starts.
num_cooperators_last_round = 0 # Initialize for round 0 conceptually

def decide_action(player_id, current_round, total_rounds, min_cooperators, reward_factor, game_history):
    """
    Decides the action (Cooperate or Defect) for the current round.

    Args:
        player_id (int): The ID of the current player.
        current_round (int): The current round number (1-indexed).
        total_rounds (int): The total number of rounds in the game.
        min_cooperators (int): The minimum number of cooperators needed to trigger reward.
        reward_factor (float): The reward multiplier (k).
        game_history (list of dict): A list of dictionaries, where each dictionary
                                     represents a past round and contains 'actions'
                                     (a list of actions taken by all players) and 'payoffs'.
                                     Example: [{'actions': ['C', 'D', ...], 'payoffs': [2, 3, ...]}, ...]

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    global num_cooperators_last_round # Access the global variable

    # Handle the first round
    if current_round == 1:
        # Cooperate in the first round to signal intent and encourage others.
        decision = 'C'
    else:
        # Count cooperators from the previous round from the game_history
        # game_history[current_round - 2] corresponds to the previous round's data
        # (since history is 0-indexed and current_round is 1-indexed)
        actions_prev_round = game_history[current_round - 2]['actions']
        num_cooperated_prev = actions_prev_round.count('C')

        if num_cooperated_prev >= min_cooperators:
            # If the threshold was met last round, continue cooperating.
            decision = 'C'
        else:
            # Threshold was NOT met last round.
            if current_round < total_rounds:
                # If it's not the last round, continue to cooperate to give the group
                # another chance to reach the threshold.
                decision = 'C'
            else:
                # If it IS the last round and cooperation failed, defect to secure private endowment.
                decision = 'D'

    # Update global state for the next round
    # This is technically not needed for decision_action itself if game_history is comprehensive,
    # but good for clarity if it were a simpler state-passing mechanism.
    # For this structure, we rely on game_history.

    return decision

# --- How to use this strategy in a tournament ---
# For each round, the tournament manager would call:
# action_for_player_i = decide_action(player_id=i,
#                                    current_round=t,
#                                    total_rounds=r,
#                                    min_cooperators=m,
#                                    reward_factor=k,
#                                    game_history=history_so_far)
#
# The 'game_history' would be updated after each round with the actions and payoffs of all players.
# The 'num_cooperators_last_round' for the next iteration would be derived from history.
```

This strategy aims for a balance between being a reliable cooperative partner and being resilient to exploitation in a repeated game where communication is absent and individual choices must infer collective intent.
'''

description_COOPERATIVE_34 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, aiming to be adaptive and robust:

## Strategy: "Cautious Cooperation"

This strategy embodies a cooperative mindset by aiming to reach the cooperation threshold whenever feasible. However, it's tempered by a degree of caution to avoid being exploited by defectors. It adapts its behavior based on the observed cooperation levels in previous rounds.

### 1. Decision Rules

The core decision rule for each player `i` in round `t` is based on an assessment of the likelihood of achieving the minimum cooperation threshold `m`.

*   **Cooperate (C):** Player `i` plays `C` if they believe that their cooperation, combined with the likely actions of others, will result in at least `m` cooperators in the current round. This belief is formed by observing past cooperation rates.
*   **Defect (D):** Player `i` plays `D` if they believe that even with their cooperation, the threshold `m` is unlikely to be met, or if the risk of being the sole cooperator (or one of very few) is too high.

To operationalize this, we'll maintain a running "cooperation score" that tracks the average number of cooperators in past rounds.

**Detailed Decision Logic for Player `i` in Round `t` (t > 1):**

1.  **Calculate Observed Cooperation Rate:**
    *   Let `N_C(t-1)` be the total number of players who cooperated in round `t-1`.
    *   Calculate the average cooperation rate over the last `x` rounds (where `x` is a lookback window, perhaps `x = min(t-1, some_parameter)`). A simple approach is to average over all previous rounds: `AvgCoopRate = (Σ(s=1 to t-1) N_C(s)) / (t-1)`.

2.  **Determine Cooperation Threshold Likelihood:**
    *   **High Likelihood:** If `AvgCoopRate + 1` (assuming player `i` cooperates) is greater than or equal to `m`, then player `i` is more inclined to cooperate. This suggests that the group is generally cooperative and their individual contribution might be the tipping point.
    *   **Moderate Likelihood:** If `AvgCoopRate` is close to `m`, but `AvgCoopRate + 1` is still less than `m`, player `i` might still cooperate to try and push the group towards the threshold, especially if `k` is significantly larger than 1.
    *   **Low Likelihood:** If `AvgCoopRate` is significantly less than `m`, and even with their cooperation, the threshold is unlikely to be met, player `i` will lean towards defecting to secure their private endowment.

3.  **Risk Aversion Adjustment:**
    *   **Exploitation Detection:** Player `i` should be wary of being the only cooperator. If `N_C(t-1) == 0` and `m > 1`, player `i` should strongly consider defecting in round `t` to avoid a guaranteed payoff of 0 (from `1-c_i` where `c_i=1`).
    *   **Reward Incentive:** If `k` is very high, it amplifies the incentive to cooperate. The strategy can be slightly more aggressive in cooperating if `k` is significantly larger.

**Simplified Decision Rule (incorporating the above):**

Player `i` plays `C` if:
`AvgCoopRate + 1 >= m`  (The group is generally cooperating, and my cooperation is likely to push us over)
**OR**
(`AvgCoopRate + 1 >= m * (1 - small_epsilon))` (The group is *almost* there, and my cooperation might be crucial, considering the high reward `k`)
**AND**
`N_C(t-1) > 0` (Avoid being the sole cooperator if `m > 1`)

Otherwise, Player `i` plays `D`.

*Note on `small_epsilon` and `k` integration:* The "moderate likelihood" case can be refined by considering `k`. If `k` is very high, the penalty for missing the threshold is less severe than the reward gained from success. This could be a tunable parameter or a factor in a more complex calculation. For this initial design, we'll focus on the primary threshold likelihood and exploitation avoidance.

### 2. Handling Edge Cases

*   **Round 1 (t=1):**
    *   **Initial Hesitation:** In the very first round, there's no history to rely on. To be maximally cooperative without being immediately exploited, player `i` will initially *attempt to cooperate*. However, this cooperation is conditional.
    *   **First Round Decision:** Player `i` plays `C` in Round 1 *if and only if* `m <= n/2`. The intuition here is that if the threshold `m` is less than or equal to half the players, there's a reasonable chance of reaching it through broad cooperation. If `m > n/2`, the risk of being in the minority trying to cooperate is too high, so player `i` will play `D`.

*   **Last Round (t=r):**
    *   **Focus on Individual Payoff:** In the last round, there are no future rounds to influence. The primary objective shifts to maximizing one's own total payoff.
    *   **Last Round Decision:** Player `i` will look at the *current predicted* outcome based on their own action and the history of defection. If they believe defecting will guarantee them a payoff of 1 (and they expect others to mostly defect, making the threshold unlikely), they will defect. If they believe cooperation is *guaranteed* to reach the threshold (e.g., if `N_C(r-1) >= m-1`), they will cooperate to get `k`.
    *   **Practical Approach for Last Round:** Player `i` plays `C` if `AvgCoopRate + 1 >= m` *and* `N_C(r-1) > 0`. Otherwise, player `i` plays `D`. This ensures they only cooperate if the threshold is very likely to be met and they are not the sole cooperator.

*   **Low `m` or High `n`:**
    *   If `m` is very low (e.g., `m=2`), cooperation becomes easier. The strategy will naturally lean towards cooperation more often.
    *   If `n` is very high, the impact of a single player's action is diminished. The `AvgCoopRate` becomes a more reliable indicator.

*   **Low `k`:** If `k` is only slightly greater than 1, the incentive to cooperate is weak. The strategy will be more risk-averse and lean towards defecting unless cooperation is very certain.

*   **High `k`:** If `k` is very high, the reward for successful cooperation is substantial. The strategy can afford to be slightly more risk-tolerant, but still needs to avoid guaranteed losses from being the sole cooperator.

### 3. Cooperative Mindset Alignment

The strategy is fundamentally cooperative because:

*   **Default to Cooperation (when reasonable):** The first round decision favors cooperation if the threshold is not impossibly high. In subsequent rounds, cooperation is the default action when past behavior suggests the group is on the path to achieving the collective benefit.
*   **Maximizing Collective Payoff (indirectly):** By aiming to meet the cooperation threshold `m`, the strategy actively contributes to achieving the higher payoff (`k`) for *all* players involved in that round. This aligns with the spirit of collective action.
*   **Adaptive but not Exploitative:** The strategy learns from past rounds. It doesn't blindly cooperate, which would be foolish in the face of consistent defection. It cooperates when it believes there's a reasonable chance of success for the collective, and it protects itself from being exploited by defectors.

### Pseudocode

```python
# Parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward factor

# Internal State
past_cooperators_count = []  # List of number of cooperators in each past round

def decide_action(current_round, previous_actions_history):
    """
    Decides whether to Cooperate (C) or Defect (D) in the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        previous_actions_history (list of lists): History of all players' actions
                                                in previous rounds.
                                                e.g., [[C, D, ...], [D, C, ...]]

    Returns:
        str: 'C' or 'D'
    """
    global past_cooperators_count

    # Update history if needed (this assumes the function is called sequentially)
    if current_round > 1 and len(past_cooperators_count) < current_round - 1:
        num_cooperators_prev_round = sum(1 for action in previous_actions_history[current_round - 2] if action == 'C')
        past_cooperators_count.append(num_cooperators_prev_round)

    # Handle Round 1
    if current_round == 1:
        # Cooperate if threshold is achievable by more than half the players,
        # otherwise defect to avoid being the only cooperator in a high-risk scenario.
        if m <= n / 2:
            return 'C'
        else:
            return 'D'

    # Handle Last Round
    if current_round == r:
        # Look at the likelihood of reaching threshold based on history.
        # If very likely to succeed and not being the sole cooperator, cooperate.
        # Otherwise, defect to secure individual payoff.
        if not past_cooperators_count: # Should not happen if r > 1 and current_round > 1
             return 'D' # Fallback

        avg_coop_rate = sum(past_cooperators_count) / len(past_cooperators_count)
        last_round_coop_count = past_cooperators_count[-1] if past_cooperators_count else 0

        # Check if cooperation is highly probable AND we are not the only one trying
        if (avg_coop_rate + 1) >= m and last_round_coop_count > 0:
            return 'C'
        else:
            return 'D'

    # Handle intermediate rounds (t > 1 and t < r)
    if not past_cooperators_count:
        return 'D' # Fallback if history is somehow missing

    avg_coop_rate = sum(past_cooperators_count) / len(past_cooperators_count)
    last_round_coop_count = past_cooperators_count[-1]

    # Decision Rule: Cooperate if threshold is likely met with our contribution,
    # and we are not the sole cooperator (unless m=1, which is disallowed by rules).
    # The condition `last_round_coop_count > 0` implicitly handles m > 1.
    if (avg_coop_rate + 1) >= m and last_round_coop_count > 0:
        return 'C'
    else:
        # If not enough cooperators observed, defect to preserve endowment.
        return 'D'

# Example usage within a tournament loop:
# For player i in round t:
#   my_action = decide_action(t, history_of_all_players)
#   record my_action
#   update history_of_all_players with my_action
#   update past_cooperators_count
```
'''

description_COOPERATIVE_35 = '''
Here's a strategy designed for the Collective Risk Dilemma, aiming for cooperation while remaining adaptive and robust. It's built on the idea of conditional cooperation, learning from past outcomes and adjusting behavior to maximize long-term collective benefit.

## Cooperative Strategy: "Adaptive Reward Seeker"

This strategy aims to foster cooperation by rewarding it when successful and cautiously defecting when it's not, all while being mindful of the total payoff over the repeated rounds. It prioritizes reaching the cooperation threshold ($m$) and leverages the increased reward ($k$) when successful.

### Core Principles:

1.  **Reward Cooperation:** Actively cooperate when there's a reasonable chance of reaching the cooperation threshold ($m$). This maximizes the potential reward for everyone.
2.  **Punish Defection (Gently):** If cooperation fails due to insufficient participation, reduce future cooperation to signal dissatisfaction and conserve resources.
3.  **Learn and Adapt:** Continuously update beliefs about other players' behavior based on historical outcomes.
4.  **Future Orientation:** Consider the long-term payoff, not just the immediate gain.

### Strategy Description:

The "Adaptive Reward Seeker" uses a probabilistic approach to cooperation, adjusting its likelihood of playing 'C' based on the perceived success rate of cooperation in previous rounds.

#### 1. Decision Rules:

The strategy's decision to cooperate or defect in any given round $t$ will be a function of:

*   **Overall Cooperation Success Rate:** The proportion of previous rounds where the cooperation threshold ($m$) was met.
*   **Recent Cooperation Success:** Whether the cooperation threshold was met in the immediately preceding rounds.
*   **Individual Contribution Count:** The number of times the player themselves has cooperated.

**Decision Logic (Round t):**

Let $S_t$ be the set of players who cooperated in round $t$.
Let $C_t = |S_t|$ be the number of cooperators in round $t$.

*   **If $C_t \ge m$:** The cooperation threshold was met. This is a positive signal.
    *   **Action for Player i:** **Cooperate (C)**. The strategy aims to reinforce successful cooperation.

*   **If $C_t < m$:** The cooperation threshold was NOT met. This is a negative signal.
    *   **If $C_t \ge m-1$ (i.e., only one player short of the threshold):**
        *   **Action for Player i:** **Cooperate (C)**. This is a strong incentive to try and "push over the edge" when so close to success, especially if $k$ is significantly larger than 1. The potential gain for everyone is high.
    *   **If $C_t < m-1$ (i.e., significantly below the threshold):**
        *   **Action for Player i:** **Defect (D)**. The strategy becomes more cautious. The probability of cooperation is reduced.

**Probabilistic Cooperation (When Not Explicitly Forced to Cooperate/Defect):**

When the strategy doesn't have a clear instruction to cooperate (e.g., when $C_t < m-1$), it will employ a probabilistic approach. The probability of cooperating, $P(C)_t$, will be influenced by a "cooperation momentum" or "trust score" that decays over time when cooperation fails.

Let's introduce a variable: `cooperation_strength`. This variable will start at a certain level and be adjusted based on outcomes.

*   **Initial `cooperation_strength`:** Set to a moderate value, e.g., 0.6 (representing a 60% chance of cooperating if not otherwise determined).

*   **Updating `cooperation_strength`:**
    *   **If $C_t \ge m$:** `cooperation_strength` = min(1.0, `cooperation_strength` + increase_factor). This reinforces successful cooperation.
    *   **If $C_t < m$ and $C_t \ge m-1$:** `cooperation_strength` = min(1.0, `cooperation_strength` + small_increase_factor). A smaller increase to reward near-misses.
    *   **If $C_t < m-1$:** `cooperation_strength` = max(0.0, `cooperation_strength` - decrease_factor). This punishes sustained failure.

*   **In Round $t$, if $C_t < m-1$:**
    *   Player i plays **C** with probability `cooperation_strength`.
    *   Player i plays **D** with probability 1 - `cooperation_strength`.

**Threshold for 'Aggressive Cooperation':**

We need to define a point where the strategy *strongly* prefers to cooperate even if the threshold is not met, to drive it. This happens when $m$ is close to $n$, and $k$ is very high.

Let's refine the logic:

**Decision Rules (Revised):**

For player $i$ in round $t$:

1.  **Count Cooperators from Previous Rounds:** Let $N_C^{prev}$ be the average number of cooperators in rounds $1$ to $t-1$. (If $t=1$, assume $N_C^{prev} = 0$).
2.  **Calculate Expected Cooperation Success:** Based on $N_C^{prev}$, estimate the likelihood of reaching $m$.
3.  **Determine Cooperation Probability $P(C)_t$:**
    *   **If $t == 1$ (First Round):**
        *   $P(C)_t = \text{initial\_cooperation\_probability}$ (e.g., 0.5 or slightly higher, depending on risk aversion).
    *   **If $t > 1$:**
        *   Let $C_{t-1}$ be the number of cooperators in the *previous* round.
        *   If $C_{t-1} \ge m$: $P(C)_t = \min(1.0, P(C)_{t-1} + \delta_{increase\_success})$ (increase cooperation probability).
        *   If $C_{t-1} = m-1$: $P(C)_t = \min(1.0, P(C)_{t-1} + \delta_{increase\_near\_miss})$ (slightly increase cooperation probability).
        *   If $C_{t-1} < m-1$: $P(C)_t = \max(0.0, P(C)_{t-1} - \delta_{decrease\_failure})$ (decrease cooperation probability).
        *   (Note: $P(C)_{t-1}$ here refers to the probability *used* in round $t-1$).

4.  **Final Action Decision:**
    *   **If $P(C)_t \ge \text{threshold\_for\_guaranteed\_cooperation}$ (e.g., 0.9):** Player $i$ **Cooperates (C)**.
    *   **Else if $P(C)_t \le \text{threshold\_for\_guaranteed\_defection}$ (e.g., 0.1):** Player $i$ **Defects (D)**.
    *   **Else (Probabilistic Decision):** Player $i$ plays **C** with probability $P(C)_t$, and **D** with probability $1 - P(C)_t$.

**Key Parameters for Adaptation:**

*   `initial_cooperation_probability`: The starting point for cooperation (e.g., 0.5).
*   `delta_increase_success`: How much to increase $P(C)$ when cooperation succeeds (e.g., 0.1).
*   `delta_increase_near_miss`: How much to increase $P(C)$ when it's close to succeeding (e.g., 0.05).
*   `delta_decrease_failure`: How much to decrease $P(C)$ when cooperation fails significantly (e.g., 0.08).
*   `threshold_for_guaranteed_cooperation`: A high probability threshold to always cooperate (e.g., 0.9).
*   `threshold_for_guaranteed_defection`: A low probability threshold to always defect (e.g., 0.1).

These parameters should be set based on initial tuning and may need to be adjusted for different tournament environments. The core idea is that success breeds more cooperation, and sustained failure leads to more defection.

#### 2. Handling Edge Cases:

*   **First Round ($t=1$):**
    *   As there's no history, the strategy must make an initial assumption. It starts with `initial_cooperation_probability`. This is a balance between being cooperative enough to encourage others and not so naive as to be exploited. A value of 0.5 is a good neutral starting point.
    *   Player $i$ plays **C** with probability `initial_cooperation_probability`, and **D** otherwise.

*   **Last Round ($t=r$):**
    *   In the last round, there is no future to incentivize. This is a classic "end-game" problem in repeated games. The optimal strategy in the last round, assuming no future reputation effect, is to defect.
    *   **Action for Player i:** **Defect (D)**. The strategy abandons cooperation efforts as there's no future benefit to maintain a cooperative image or encourage others.

*   **Very High $k$ / Very High $m$ (relative to $n$):**
    *   If $k$ is very high, the temptation to reach the threshold is significant. The strategy will tend to cooperate more aggressively, especially if it perceives it's close to $m$.
    *   If $m$ is very high (close to $n$), it becomes harder to reach the threshold. The strategy will likely become more cautious and defect more often unless there's strong evidence of others cooperating. The `delta_decrease_failure` parameter will be crucial here.

*   **Number of Players ($n$) and Threshold ($m$):**
    *   The strategy inherently adapts to $n$ and $m$ through the counting of cooperators and the comparison to $m$.
    *   If $m$ is small (e.g., $m=2$), cooperation is easier to achieve. The strategy will likely see successes more often and thus increase `cooperation_strength`.
    *   If $m$ is large (e.g., $m=n-1$), cooperation is harder. The strategy will experience failures more frequently and decrease `cooperation_strength`, becoming more cautious.

#### 3. Being Cooperative:

*   **Inherent Goal:** The strategy is designed to *try* to achieve the collective good whenever it seems feasible. Its primary mechanism is to reward observed successful cooperation with more cooperation.
*   **Conditional Cooperation:** It doesn't blindly cooperate. It cooperates when it believes cooperation is likely to succeed or has succeeded in the past. This is crucial for robustness. If it always cooperated, it would be easily exploited by defectors.
*   **Punishment Mechanism:** The decrease in `cooperation_strength` when cooperation fails is a form of "negative feedback" to the group, signaling that the current behavior is not yielding the desired collective outcome. This is still cooperative in intent, aiming to nudge the group towards a more successful strategy by making individual defection less profitable in the long run if cooperation doesn't materialize.
*   **Focus on Long-Term Payoff:** By adjusting `cooperation_strength` based on past outcomes, the strategy implicitly considers the repeated nature of the game and the potential for future rewards if cooperation can be established.

### Pseudocode:

```pseudocode
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Strategy Parameters (Tunable)
initial_cooperation_probability = 0.5
delta_increase_success = 0.1
delta_increase_near_miss = 0.05
delta_decrease_failure = 0.08
threshold_for_guaranteed_cooperation = 0.9
threshold_for_guaranteed_defection = 0.1

// Internal State Variables
cooperation_probability = initial_cooperation_probability // P(C) for the current round
last_round_cooperators = 0 // Number of cooperators in the previous round

// --- Main Strategy Function ---
function decide_action(current_round, player_history):
  // player_history is a list of lists, where each inner list is actions of all players in a round
  // current_round is 1-indexed

  if current_round == 1:
    // First round, use initial probability
    cooperation_probability = initial_cooperation_probability
  else:
    // Update cooperation_probability based on previous round's outcome
    previous_round_actions = player_history[current_round - 2] // Get actions from the round before the current one
    cooperators_in_previous_round = count_cooperators(previous_round_actions)

    if cooperators_in_previous_round >= m:
      cooperation_probability = min(1.0, cooperation_probability + delta_increase_success)
    elif cooperators_in_previous_round == m - 1:
      cooperation_probability = min(1.0, cooperation_probability + delta_increase_near_miss)
    else: // cooperators_in_previous_round < m - 1
      cooperation_probability = max(0.0, cooperation_probability - delta_decrease_failure)

  // Handle the last round explicitly: Always defect
  if current_round == r:
    return "D" // Defect in the last round

  // Decide action based on updated probability
  if cooperation_probability >= threshold_for_guaranteed_cooperation:
    return "C" // Cooperate
  elif cooperation_probability <= threshold_for_guaranteed_defection:
    return "D" // Defect
  else:
    // Probabilistic decision
    random_value = random_float(0.0, 1.0) // Generate a random number between 0 and 1
    if random_value < cooperation_probability:
      return "C" // Cooperate
    else:
      return "D" // Defect

// Helper function to count cooperators in a list of actions
function count_cooperators(actions):
  count = 0
  for action in actions:
    if action == "C":
      count = count + 1
  return count

// --- Initialization ---
// Before the tournament starts:
// cooperation_probability = initial_cooperation_probability
// last_round_cooperators = 0 // Not strictly needed if we use history directly


```

This strategy provides a balanced approach, aiming to unlock the higher payoffs through cooperation while having mechanisms to protect itself from exploitation and adapt to the prevailing behavior of other players. Its success will depend on the tuning of its internal parameters and the specific mix of other strategies in the tournament.
'''

description_COOPERATIVE_36 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, aiming for adaptation, robustness, and a clear cooperative intent.

## Cooperative Strategy: The "Adaptive Safety Net"

This strategy prioritizes establishing and maintaining cooperation, but also includes safeguards to avoid exploitation and to secure a reasonable outcome even if cooperation fails. It's built on observing past behavior and adjusting expectations accordingly.

### Core Philosophy:

*   **Proactive Cooperation, Reactive Defense:** We aim to be a reliable cooperator, believing that collective benefit arises from shared contribution. However, we are prepared to defend our interests if cooperation appears unsustainable or exploitative.
*   **Dynamic Threshold Adjustment:** The strategy dynamically adjusts its perception of cooperation based on the observed success rate of the community project. This makes it adaptive to the group's overall behavior.
*   **"Cost of Defection" Awareness:** The strategy recognizes that defecting can be individually beneficial in certain situations, but also understands that widespread defection leads to the worst collective outcome.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` is based on a combination of the observed success of cooperation in previous rounds and a "cooperation tolerance" level.

**Key Variables:**

*   `cooperation_rate_history`: A list or running average of the proportion of players who cooperated in each past round.
*   `current_cooperation_level`: The number of players observed to have cooperated in the *immediately preceding* round.
*   `cooperation_tolerance`: A threshold value that represents our willingness to continue cooperating even if the cooperation rate has dipped. This will be a dynamic value.
*   `total_successful_rounds`: The count of rounds where `m` or more players cooperated.

**Decision Logic:**

*   **Initialization (First Round):**
    *   In the very first round (`t=1`), we will **Cooperate (C)**. This is a strong signal of cooperative intent and is necessary to *initiate* potential group cooperation. It also allows us to gather initial data.

*   **Subsequent Rounds (`t > 1`):**
    *   **If `current_cooperation_level >= m` (Last round was a success):**
        *   **Cooperate (C)**. If the community project was successful in the last round, it's a strong incentive to continue. We will also increment `total_successful_rounds`.
    *   **If `current_cooperation_level < m` (Last round was a failure):**
        *   **Evaluate `cooperation_tolerance`:**
            *   **If `current_cooperation_level >= cooperation_tolerance`:** **Cooperate (C)**. Even though the last round *individually* failed to meet the threshold, if the cooperation level is still above our "tolerance," we are willing to give it another chance. This allows for slight dips without immediately abandoning cooperation.
            *   **If `current_cooperation_level < cooperation_tolerance`:** **Defect (D)**. If the cooperation level has fallen below our tolerance threshold, it suggests a lack of sustained commitment from the group, or a potential exploitation scenario. It's time to protect our individual endowment.

**Dynamic Adjustment of `cooperation_tolerance`:**

The `cooperation_tolerance` is not static. It adjusts based on the historical success of the group.

*   **When cooperation succeeds (`current_cooperation_level >= m`):**
    *   Slightly **increase** `cooperation_tolerance`. This reinforces our willingness to cooperate when the group is performing well. For instance, increase by a small percentage of `m` or a fixed small value.
*   **When cooperation fails (`current_cooperation_level < m`):**
    *   Slightly **decrease** `cooperation_tolerance`. This makes us more cautious about continuing to cooperate if the group is struggling. However, we don't want to decrease it too rapidly, hence the use of a tolerance *above* the minimum required. For instance, decrease by a smaller percentage of `m` or a fixed smaller value.
    *   Crucially, `cooperation_tolerance` should never fall below `m/2` (or some other reasonable minimum to prevent immediate abandonment if the group is *almost* there). This ensures we don't become overly pessimistic.

**Example of `cooperation_tolerance` Adjustment:**

Let `m = 5`.
*   Initial `cooperation_tolerance = m` (to be conservative).
*   Round 1: We play C.
*   Round 2: `current_cooperation_level = 6` (success). We play C. `cooperation_tolerance` becomes `5.2`. `total_successful_rounds = 1`.
*   Round 3: `current_cooperation_level = 4` (failure). `4 >= 5.2` is false. We play D. `cooperation_tolerance` becomes `5.0` (decreased, but capped).
*   Round 4: `current_cooperation_level = 7` (success). We play C. `cooperation_tolerance` becomes `5.2`. `total_successful_rounds = 2`.
*   Round 5: `current_cooperation_level = 3` (failure). `3 >= 5.2` is false. We play D. `cooperation_tolerance` becomes `5.0`.

### 2. Handling Edge Cases:

*   **First Round (`t=1`):** Always **Cooperate (C)**. This is essential for kickstarting cooperation. It's a pure gamble for the benefit of potentially establishing cooperation.
*   **Last Round (`t=r`):**
    *   **If `current_cooperation_level >= m` in round `r-1`:** **Cooperate (C)**. You get the reward if it succeeds.
    *   **If `current_cooperation_level < m` in round `r-1`:** **Defect (D)**. Since there are no future rounds to influence, and the group has shown a tendency towards insufficient cooperation, it's best to secure your private endowment. The "risk" of cooperation is no longer worth the potential reward if it's unlikely to be met.
*   **Very Small Groups (`n` is slightly larger than `m`):** The strategy remains the same. The `m` threshold is paramount. If `n = m + 1`, then only one defector can sink the project. The `cooperation_tolerance` adjustment is crucial here to prevent rapid abandonment if one player defects.
*   **Very Large Groups (`n` is much larger than `m`):** The strategy is still robust. The `cooperation_rate_history` and dynamic `cooperation_tolerance` will better reflect the aggregate behavior of a larger group.

### 3. Being Cooperative:

*   **Default to Cooperation:** The strategy's default action, especially early on and when the group shows signs of success, is to **Cooperate**. This is a clear signal of intent to achieve the higher collective payoff.
*   **Giving the Benefit of the Doubt (within limits):** The `cooperation_tolerance` mechanism allows for occasional failures without immediate punitive action. This is a form of "forgiveness" that encourages group resilience.
*   **Commitment to Collective Good:** The strategy actively aims to reach the `m` threshold. It only defects when the observed behavior suggests that such collective success is unlikely to be achieved or sustained, and therefore, continued cooperation would be individually detrimental and not contribute to the collective good.
*   **No Exploitation:** The strategy is designed to *not* exploit successful cooperation. If `m` or more players cooperate, it will also cooperate, effectively contributing to the positive outcome and receiving its share of the reward. It only defects when cooperation *fails* to meet a sufficient level or falls below its dynamically adjusted tolerance.

### Pseudocode:

```python
class AdaptiveSafetyNetStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k

        self.current_round = 0
        self.cooperation_tolerance = m  # Start with full expectation
        self.cooperation_history = [] # Stores number of cooperators per round
        self.total_successful_rounds = 0

        # Parameters for adjusting tolerance (tune these for performance)
        self.tolerance_increase_factor = 0.05 # e.g., 5% of m per successful round
        self.tolerance_decrease_factor = 0.02 # e.g., 2% of m per failed round
        self.min_cooperation_tolerance = m / 2 # Ensure it doesn't drop too low

    def decide_action(self, previous_round_actions):
        self.current_round += 1

        # --- Handle First Round ---
        if self.current_round == 1:
            return "C"

        # --- Process Previous Round's Data ---
        if previous_round_actions:
            last_round_cooperators = sum(1 for action in previous_round_actions if action == "C")
            self.cooperation_history.append(last_round_cooperators)

            was_successful = last_round_cooperators >= self.m

            # --- Adjust Cooperation Tolerance ---
            if was_successful:
                self.total_successful_rounds += 1
                # Increase tolerance for good performance
                self.cooperation_tolerance = min(self.m, self.cooperation_tolerance + (self.m * self.tolerance_increase_factor))
            else:
                # Decrease tolerance for failure, but not too much
                self.cooperation_tolerance = max(self.min_cooperation_tolerance, self.cooperation_tolerance - (self.m * self.tolerance_decrease_factor))

        # --- Handle Last Round ---
        if self.current_round == self.r:
            # In the last round, if the previous round was a failure, defect to secure payoff
            if previous_round_actions and sum(1 for action in previous_round_actions if action == "C") < self.m:
                return "D"
            # Otherwise (previous round was success, or it's the absolute last round with no prior data), cooperate.
            else:
                return "C"


        # --- General Decision Logic for Intermediate Rounds ---
        if previous_round_actions:
            last_round_cooperators = self.cooperation_history[-1]

            if last_round_cooperators >= self.m:
                # If the last round was a success, continue cooperating.
                return "C"
            elif last_round_cooperators >= self.cooperation_tolerance:
                # If the last round failed, but still met our tolerance, cooperate again.
                return "C"
            else:
                # If the last round failed and is below our tolerance, defect.
                return "D"
        else:
            # Should not happen after round 1, but as a fallback: cooperate.
            return "C"

```

This strategy aims to be a strong, adaptive cooperator that learns from the group's behavior while having built-in resilience against exploitation and a clear exit strategy for the last round.
'''

description_COOPERATIVE_37 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust. The core idea is to foster cooperation through a conditional approach, rewarding cooperation when it's successful and adjusting behavior based on collective outcomes.

## Cooperative Strategy: "Resilient Rewarder"

This strategy aims to be a reliable cooperator while simultaneously rewarding the group's collective success and being judicious about risk. It's designed to incentivize others to cooperate by demonstrating a willingness to contribute when the group is close to the cooperation threshold.

### Core Principles:

1.  **Initial Cooperative Stance:** Start with a cooperative intention.
2.  **Leverage Success:** When cooperation is successful (threshold met), continue to cooperate.
3.  **Strategic Contribution:** Contribute when the group *almost* meets the threshold, as your contribution might be the tipping point.
4.  **Punish Defection (Subtly):** If cooperation fails due to a lack of critical mass, briefly reduce cooperation to signal dissatisfaction, but don't completely abandon the cooperative goal.
5.  **Last Round Consideration:** Adapt to the final round's lack of future incentives.

### 1. Decision Rules:

For each round `t` (from 1 to `r`):

**Player `i`'s Decision:**

*   **If `t == 1` (First Round):**
    *   **Cooperate (C)**.
    *   *Rationale:* The first round is crucial for establishing a baseline and signaling cooperative intent. A cooperative start is essential for potentially fostering a cooperative environment.

*   **If `t > 1`:**
    *   **Calculate `cooperators_last_round`:** The number of players who played `C` in round `t-1`.
    *   **Calculate `my_decision_last_round`:** Player `i`'s action in round `t-1`.

    *   **Rule 1: Reward Successful Cooperation:**
        *   If `cooperators_last_round >= m` (the threshold was met in the previous round):
            *   **Cooperate (C)**.
            *   *Rationale:* The group has demonstrated it can achieve the reward. Continuing to cooperate reinforces this success and maximizes individual payoff in this successful state.

    *   **Rule 2: Risky Bet for the Threshold:**
        *   Else if `cooperators_last_round == m - 1` (the group was one cooperator short in the previous round):
            *   **Cooperate (C)**.
            *   *Rationale:* Your cooperation could be the one that pushes the group over the threshold, securing the reward for everyone. This is a calculated risk to achieve collective gain.

    *   **Rule 3: Signal Dissatisfaction (Temporary Defection):**
        *   Else if `cooperators_last_round < m - 1` AND `my_decision_last_round == C` (cooperation failed significantly, AND I was a cooperator last round):
            *   **Defect (D)**.
            *   *Rationale:* This signals that your individual cooperation is not leading to collective success. It's a mild "punishment" to encourage others to be more cooperative in the future, without completely abandoning the strategy's cooperative foundation.

    *   **Rule 4: Maintain Status Quo (If I defected previously):**
        *   Else (i.e., `cooperators_last_round < m - 1` AND `my_decision_last_round == D`):
            *   **Defect (D)**.
            *   *Rationale:* If cooperation failed and I was already defecting, there's no strong incentive to change my behavior yet. This also helps to conserve my private endowment when the group is clearly not cooperating.

*   **If `t == r` (Last Round):**
    *   **Always Cooperate (C)**.
    *   *Rationale:* In the last round, there is no future interaction to leverage or punish. The best individual strategy to maximize *your* payoff, assuming others might still cooperate and trigger the reward, is to cooperate. This is a strategic altruism to secure the highest possible payoff in the final round, regardless of past history.

### 2. Handling Edge Cases:

*   **First Round (`t = 1`):** As specified above, always **Cooperate (C)**. This is the foundational move for establishing a cooperative norm.

*   **Last Round (`t = r`):** As specified above, always **Cooperate (C)**. This maximizes individual payoff by assuming others might still cooperate and trigger the reward, as there's no future consequence.

*   **`m` value is very high (close to `n`):** The strategy remains robust. Rule 2 ("Risky Bet for the Threshold") becomes even more critical and more likely to be triggered. If `m = n-1`, then `m-1 = n-2`. If `n-2` players cooperated last round, this strategy will cooperate.

*   **`m` value is low (close to 2):** The strategy still works. If `m=2`, then `m-1=1`. If only 1 player cooperated last round, this strategy will cooperate. If 0 players cooperated, this strategy will defect (Rule 3 if it cooperated last, Rule 4 if it defected last).

*   **All players always defect:** In this scenario, `cooperators_last_round` will always be 0.
    *   Round 1: C
    *   Round 2: `cooperators_last_round = 0`. `my_decision_last_round = C`. Rule 3 applies. D.
    *   Round 3 onwards: `cooperators_last_round = 0`. `my_decision_last_round = D`. Rule 4 applies. D.
    *   Last Round: C.
    The strategy will try to cooperate once, then defect until the last round, where it cooperates again. This is a reasonable response to consistent defection.

*   **All players always cooperate:**
    *   Round 1: C
    *   Round 2: `cooperators_last_round = n >= m`. Rule 1 applies. C.
    *   ... and so on. The strategy will happily continue cooperating.

### 3. Being Cooperative:

The "Resilient Rewarder" strategy is fundamentally cooperative for several reasons:

*   **Initial Commitment:** It begins by making a cooperative move, signaling a willingness to contribute to the collective good.
*   **Proactive Risk-Taking:** It actively takes a risk (Rule 2) when the group is *just* short of the cooperation threshold. This is not about ensuring personal gain but about enabling collective gain. The player is willing to forgo their private endowment if the threshold isn't met, in the hope that their contribution will be the one that unlocks the reward for everyone.
*   **Reward Reinforcement:** When cooperation is successful, it continues to cooperate, thereby reinforcing the positive outcome and maximizing group payoff.
*   **Adaptive Punishment:** The "punishment" (Rule 3) is not severe or permanent. It's a temporary withdrawal of cooperation triggered by the failure of collective effort *despite* the player's own cooperation. It's designed to nudge others towards cooperation rather than to punish them into defection.
*   **Final Round Altruism:** The decision to always cooperate in the last round, regardless of past history, is a form of strategic altruism. It prioritizes the potential for a higher payoff for everyone in the final round, even if it means taking a risk with no future personal benefit from that specific round's cooperation.

This strategy balances a consistent desire to cooperate with adaptive responses that are informed by the group's performance and the game's structure. It aims to be a stable and predictable player that can encourage cooperative equilibria while remaining resilient to defection.

---

### Pseudocode Representation:

```python
# Assuming player_id is the index of the current player (0-indexed for list access)
# Assuming history is a list of lists, where history[t-1] is a list of actions in round t-1
# For simplicity, let's assume history is available and actions are 'C' or 'D'

def decide_action(player_id, round_number, n, m, k, history):
    """
    Resilient Rewarder strategy for the Collective Risk Dilemma.

    Args:
        player_id (int): The index of the current player (0 to n-1).
        round_number (int): The current round number (1 to r).
        n (int): Total number of players.
        m (int): Minimum cooperators needed.
        k (float): Reward factor.
        history (list of lists): Past actions. history[t-1] is a list of actions for round t.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """
    r = len(history) + 1 # Total rounds is implicitly known or passed if history is shorter than r

    if round_number == 1:
        return 'C'

    # Get information from the previous round (t-1)
    prev_round_actions = history[-1] # Assuming history[-1] corresponds to the most recent round (round_number - 1)
    cooperators_last_round = prev_round_actions.count('C')
    my_decision_last_round = prev_round_actions[player_id]

    if round_number == r: # Last round
        return 'C'
    
    # Decision Rules for rounds 2 to r-1

    # Rule 1: Reward Successful Cooperation
    if cooperators_last_round >= m:
        return 'C'

    # Rule 2: Risky Bet for the Threshold
    elif cooperators_last_round == m - 1:
        return 'C'

    # Rule 3: Signal Dissatisfaction (Temporary Defection)
    elif cooperators_last_round < m - 1 and my_decision_last_round == 'C':
        return 'D'

    # Rule 4: Maintain Status Quo (If I defected previously)
    else: # cooperators_last_round < m - 1 and my_decision_last_round == 'D'
        return 'D'

# --- Example Usage (Illustrative) ---
# n_players = 6
# m_threshold = 3
# k_reward = 2.0
# r_rounds = 5
#
# # Simulate a few rounds
# game_history = []
#
# # Round 1
# actions_r1 = []
# for i in range(n_players):
#     actions_r1.append(decide_action(i, 1, n_players, m_threshold, k_reward, game_history))
# game_history.append(actions_r1)
# print(f"Round 1 Actions: {actions_r1}") # e.g., ['C', 'C', 'C', 'C', 'C', 'C']
#
# # Round 2 (assuming all cooperated in R1)
# actions_r2 = []
# for i in range(n_players):
#     actions_r2.append(decide_action(i, 2, n_players, m_threshold, k_reward, game_history))
# game_history.append(actions_r2)
# print(f"Round 2 Actions: {actions_r2}") # e.g., ['C', 'C', 'C', 'C', 'C', 'C'] (Rule 1 applies if n >= m)
#
# # Round 3 (example where it might be close)
# # Let's manually set history for a specific scenario
# game_history = [
#     ['C', 'C', 'D', 'D', 'D', 'D'], # Round 1: 2 cooperators
#     ['C', 'D', 'D', 'D', 'D', 'D']  # Round 2: 1 cooperator
# ]
# n_players = 6
# m_threshold = 3
# r_rounds = 5
#
# # Player 0 (was 'C' in R1, 'C' in R2, cooperators_last_round=1)
# # Rule 2 applies (1 == 3-1) -> Cooperate
# print(f"Player 0, Round 3 decision: {decide_action(0, 3, n_players, m_threshold, k_reward, game_history)}") # Expected: C
#
# # Player 2 (was 'D' in R1, 'D' in R2, cooperators_last_round=1)
# # Rule 4 applies (1 < 3-1 and was 'D') -> Defect
# print(f"Player 2, Round 3 decision: {decide_action(2, 3, n_players, m_threshold, k_reward, game_history)}") # Expected: D
#
# # Player 3 (was 'D' in R1, 'D' in R2, cooperators_last_round=1)
# # Rule 4 applies -> Defect
# print(f"Player 3, Round 3 decision: {decide_action(3, 3, n_players, m_threshold, k_reward, game_history)}") # Expected: D
#
# # If round 3 is the last round (r_rounds=3)
# game_history_last = [
#     ['C', 'C', 'D', 'D', 'D', 'D'], # Round 1: 2 cooperators
#     ['C', 'D', 'D', 'D', 'D', 'D']  # Round 2: 1 cooperator
# ]
# # Player 0, Round 3 (last round)
# print(f"Player 0, Round 3 (last) decision: {decide_action(0, 3, n_players, m_threshold, k_reward, game_history_last)}") # Expected: C

```
'''

description_COOPERATIVE_38 = '''
This strategy is designed to be a **Cautious Cooperator**. It aims to maximize the collective payoff while being resilient to defections, especially in critical situations. The core idea is to err on the side of cooperation when the probability of meeting the cooperation threshold is high, but to defect strategically to protect itself when cooperation seems unlikely or when the stakes are particularly high.

## Cautious Cooperator Strategy

The Cautious Cooperator strategy focuses on learning from past behavior and adapting its cooperation level based on the observed propensity of others to cooperate. It prioritizes reaching the cooperation threshold to gain the reward, but it won't blindly cooperate if it seems others are consistently defecting.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in round `t` for player `i` will depend on two primary factors:

*   **Observed Cooperation Rate:** The proportion of other players who cooperated in the previous round.
*   **Proximity to Threshold:** How close the observed cooperation rate is to the required `m` cooperators.

Let $O_{t-1}$ be the number of players who cooperated in round $t-1$.
Let $p_{t-1} = \frac{O_{t-1}}{n}$ be the proportion of players who cooperated in round $t-1$.

**Decision Logic:**

*   **If $p_{t-1}$ is high and likely to reach `m` with current cooperation:** Cooperate.
*   **If $p_{t-1}$ is low and unlikely to reach `m` even with current cooperation:** Defect.
*   **If $p_{t-1}$ is in an intermediate range:** Use a probabilistic approach, leaning towards cooperation.

More formally, for round `t` (where $t > 1$):

1.  **Calculate the number of cooperators in the previous round ($O_{t-1}$).**
2.  **Determine the number of additional cooperators needed to reach the threshold `m`:** `needed_cooperators = m - O_{t-1}`.
3.  **Assess if your cooperation is critical:**
    *   **If `needed_cooperators` is 1 (and you are considering cooperating):** This means if you cooperate, the threshold will be met. This is a very strong signal to cooperate, as the collective benefit is significant. **Cooperate (C).**
    *   **If `needed_cooperators` is 0 or less:** The threshold was already met or exceeded in the previous round. In this scenario, your individual contribution to *reaching* the threshold is no longer critical. However, continuing to cooperate helps maintain the cooperative norm and maximizes the group's total payoff. **Cooperate (C).**

4.  **Assess the risk of continued cooperation based on observed defection:**
    *   **If `needed_cooperators` is greater than 1:** This means even if you cooperate, the threshold is unlikely to be met by your action alone. We need to consider the general behavior of others.
        *   **If $O_{t-1} \le m - 2$ (i.e., the number of cooperators was significantly below the threshold):** The observed cooperation rate is low. Continuing to cooperate when others are defecting is costly. **Defect (D).**
        *   **If $m - 2 < O_{t-1} < m - 1$ (i.e., one or two players short of the threshold, and you're considering cooperation):** This is the "uncertain zone". The risk of cooperating and not reaching the threshold is high. To adapt to potentially non-cooperative opponents, we introduce a dynamic element.
            *   **Probabilistic Cooperation:** Cooperate with a probability that increases as the observed cooperation rate gets closer to `m`. For example, a simple heuristic could be:
                *   If $O_{t-1} = m-2$ (two short), cooperate with probability $P_1$.
                *   If $O_{t-1} = m-3$ (three short), cooperate with probability $P_2$, where $P_2 < P_1$.
                *   We can define $P_1$ and $P_2$ based on the game parameters, for example, $P_1 = 0.7$ and $P_2 = 0.4$. The exact probabilities could be tuned. **Cooperate (C) with probability $P$, otherwise Defect (D).** The probability $P$ should be a decreasing function of how far $O_{t-1}$ is from $m$.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **No history to learn from.** The strategy must make an initial decision. To be "cooperative" by default and give others a chance to establish a cooperative norm, the Cautious Cooperator will **Cooperate (C)** in the first round. This aligns with the "cooperative mindset" by initiating cooperation.

*   **Last Round (t=r):**
    *   **No future rounds to influence or be influenced by.** The incentive structure changes. In the last round, there's no "shadow of the future" to deter defection or encourage cooperation. This can lead to a "last-round defection" effect.
    *   **To be robust:** The Cautious Cooperator will **Defect (D)** in the last round. This is a strategic defensive measure. If all other players have defected in previous rounds, defecting in the last round ensures the player doesn't incur a loss by cooperating. If others are cooperating, defecting in the last round allows the player to receive the private endowment (1) and the reward (k) without contributing their own endowment, thus maximizing their individual payoff in this final round. This decision is purely self-interested for the final round.

*   **Low `m` relative to `n` (e.g., `m = 2`):**
    *   The strategy remains the same. The logic of needing fewer cooperators means the "criticality" of your cooperation might be higher more often. The "needed_cooperators = 1" condition will be met more frequently, encouraging cooperation.

*   **High `m` relative to `n` (e.g., `m = n-1`):**
    *   The strategy remains the same. The logic of needing more cooperators means the "uncertain zone" will be explored more often. The probability function for cooperation becomes more critical.

### 3. Being Cooperative

The Cautious Cooperator's cooperative nature is evident in:

*   **Default Cooperation in Round 1:** It initiates cooperation, demonstrating a willingness to build a cooperative environment.
*   **Prioritizing Threshold Achievement:** The primary driver for cooperation is to meet the `m` threshold, which benefits all players. The strategy prioritizes actions that directly contribute to this collective good.
*   **Adaptive Cooperation:** It doesn't defect arbitrarily. It only defects when it observes a pattern of defection that makes cooperation unlikely to succeed, thereby protecting itself from exploitation. This is a form of "reciprocal cooperation" in spirit, but without explicit communication or guaranteed reciprocity. It tries to cooperate as long as it's reasonably likely to be successful.
*   **Maintaining Cooperation When Successful:** If the threshold is met, the strategy continues to cooperate to maintain the collective benefit, as long as there's no strong signal of impending defection from others.

## Pseudocode

```python
def decide_action(current_round, total_rounds, num_players, threshold_m, reward_k, game_history):
    # game_history is a list of lists, where each inner list represents the actions of all players in a past round.
    # game_history[t-1] would be the actions in round t-1.
    # actions are 'C' or 'D'.

    # --- Handle Edge Cases ---
    if current_round == 1:
        return 'C'  # Cooperate in the first round

    if current_round == total_rounds:
        return 'D'  # Defect in the last round

    # --- Normal Round Logic (t > 1 and t < total_rounds) ---

    # Get actions from the previous round
    previous_round_actions = game_history[current_round - 2] # 0-indexed history

    # Count cooperators in the previous round
    cooperators_last_round = previous_round_actions.count('C')

    # Calculate how many more cooperators are needed
    needed_cooperators = threshold_m - cooperators_last_round

    # --- Decision Rules ---

    # 1. If your cooperation is critical to reaching the threshold
    if needed_cooperators == 1:
        return 'C'  # You are the one player needed to cross the threshold

    # 2. If the threshold was already met or exceeded
    if needed_cooperators <= 0:
        return 'C'  # Continue cooperating to maintain collective benefit

    # 3. If the threshold was significantly below
    if cooperators_last_round <= threshold_m - 2:
        return 'D'  # Too many defections observed, defect to protect yourself

    # 4. Intermediate zone: m-2 < cooperators_last_round < m-1 (i.e., needed_cooperators = 2 or more, but not too low)
    # This is where we need a probabilistic approach.
    # Let's define a simple probability function. The probability of cooperating should decrease as cooperators_last_round decreases.
    # Example: if threshold_m is 5, and cooperators_last_round is 3 (needed_cooperators=2), cooperate with p_high.
    # If cooperators_last_round is 2 (needed_cooperators=3), cooperate with p_low.

    # For simplicity and robustness, let's use a fixed probability for the "uncertain zone"
    # where needed_cooperators is between 2 and 'some threshold'
    # A reasonable default could be to cooperate around 60-70% of the time when in this zone.
    # We can refine this: let's say if exactly 2 players are needed (cooperators_last_round = m-2) cooperate with 70% prob.
    # If more than 2 players are needed, and not too many defected (e.g. cooperators_last_round > 0), maybe 40% prob.

    probability_to_cooperate = 0.0

    if needed_cooperators == 2: # Exactly two short of threshold (cooperators_last_round = m-2)
        probability_to_cooperate = 0.7
    elif needed_cooperators > 2 and cooperators_last_round > 0: # More than two short, but some cooperation exists
        probability_to_cooperate = 0.4
    else: # If needed_cooperators > 2 and cooperators_last_round == 0 (all defected last round)
        probability_to_cooperate = 0.0 # No point trying to start a movement alone

    import random
    if random.random() < probability_to_cooperate:
        return 'C'
    else:
        return 'D'

```
'''

description_COOPERATIVE_39 = '''
This strategy, dubbed "Adaptive Threshold Cooperate" (ATC), aims to achieve high payoffs by strategically cooperating, leveraging the repeated nature of the game and the information from past rounds. It's designed to be robust, adapt to evolving player behavior, and maintain a cooperative bias while protecting against exploitation.

## Adaptive Threshold Cooperate (ATC) Strategy

The core idea of ATC is to **cooperate when there's a high probability of success (i.e., meeting the cooperation threshold 'm') and defect when the risk of failure is too high or when others are consistently defecting.** It dynamically adjusts its cooperation threshold based on the observed behavior of other players.

### 1. Decision Rules

ATC uses a probabilistic approach to decide whether to cooperate. The decision in any given round `t` (where `t` ranges from 1 to `r`) is based on two factors:

*   **Baseline Cooperation Probability:** A starting point for cooperation, influenced by the game parameters.
*   **Observed Cooperation Rate:** The proportion of players who cooperated in the previous round.

**For round `t` (where `t > 1`):**

Let:
*   `C_t-1`: The number of players who cooperated in round `t-1`.
*   `N_cooperators_ratio_t-1 = C_t-1 / n`: The ratio of cooperators in the previous round.

**Decision Logic:**

ATC will cooperate in round `t` if:

`P(Cooperate | History) > Threshold_Cooperate`

Where `P(Cooperate | History)` is the calculated probability of cooperating, and `Threshold_Cooperate` is a dynamic threshold that also adapts.

**Detailed Calculation of `P(Cooperate | History)`:**

ATC will attempt to estimate the likelihood that cooperating in the current round will lead to reaching the `m` threshold. A simplified heuristic is to observe the previous round's success.

1.  **If `N_cooperators_ratio_t-1 >= m/n` (i.e., the threshold was met or exceeded in the previous round):**
    This indicates that cooperation is viable and likely to be rewarded. ATC will *increase* its probability of cooperating. A good starting point for this increase is to aim for a probability that's slightly higher than the observed successful cooperation rate, but not so high as to be overly risky if the observed rate was a fluke.
    *   `P(Cooperate | History) = min(1.0, Baseline_Cooperation_Probability + (N_cooperators_ratio_t-1 - m/n) * Adaptation_Factor_Up)`
    *   `Baseline_Cooperation_Probability` (initially set to a value like `m/n` or slightly higher to encourage cooperation) will be a parameter.
    *   `Adaptation_Factor_Up` will be a small positive constant (e.g., 0.5) that governs how quickly we increase our cooperation probability.

2.  **If `N_cooperators_ratio_t-1 < m/n` (i.e., the threshold was not met in the previous round):**
    This suggests that either not enough players cooperated, or there's a risk of not meeting the threshold. ATC will *decrease* its probability of cooperating. The degree of decrease will depend on how far below the threshold the previous round fell.
    *   `P(Cooperate | History) = max(0.0, Baseline_Cooperation_Probability - (m/n - N_cooperators_ratio_t-1) * Adaptation_Factor_Down)`
    *   `Adaptation_Factor_Down` will be a small positive constant (e.g., 0.7) that governs how quickly we decrease our cooperation probability. We might want this to be slightly higher than `Adaptation_Factor_Up` to be more cautious when facing potential failure.

**The actual decision to cooperate or defect in round `t` is then made by comparing a random draw (from a uniform distribution between 0 and 1) against `P(Cooperate | History)`:**

*   **If `random_uniform(0, 1) < P(Cooperate | History)`: Player `i` plays `C`.**
*   **Otherwise: Player `i` plays `D`.**

**Dynamic Threshold Adjustment:**

The `Baseline_Cooperation_Probability` itself can also adapt over time. If the game consistently results in high payoffs for cooperation, this baseline can be gradually increased. Conversely, if players are consistently defecting and the threshold is rarely met, the baseline can be lowered. This adds a meta-level adaptation.

*   **If average payoff in the last `X` rounds is significantly high (e.g., > 1.5):** `Baseline_Cooperation_Probability = min(1.0, Baseline_Cooperation_Probability + Small_Increase)`
*   **If average payoff in the last `X` rounds is low (e.g., <= 1.0):** `Baseline_Cooperation_Probability = max(0.0, Baseline_Cooperation_Probability - Small_Decrease)`

`X`, `Small_Increase`, and `Small_Decrease` are hyperparameters that would be tuned.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    In the absence of any history, ATC defaults to a cautious but cooperative stance.
    *   **Decision Rule:** Cooperate with a fixed probability that is slightly above the minimum requirement for cooperation, but not so high as to be naive. A good starting point is `P(Cooperate | t=1) = m/n` or `m/n + 0.1`. This aims to initiate cooperation if enough others also have a similar initial bias.
    *   **Rationale:** We need to "prime the pump" for cooperation. Cooperating with a probability equal to the threshold proportion is a reasonable first step.

*   **Last Round (t=r):**
    In the final round, there is no future to incentivize cooperation. The dominant strategy for a rational player is to defect, as they can maximize their payoff in this single round by keeping their endowment and benefiting from any cooperation by others.
    *   **Decision Rule:** **Always play `D`**.
    *   **Rationale:** This is the standard game-theoretic outcome for a finitely repeated game with a known end. Even if others cooperate, the best individual payoff is achieved by defecting.

*   **When `m = n` (Although parameter says `m < n`, it's good to consider):**
    If `m = n`, then *everyone* must cooperate for the reward. This is a much harder threshold to meet. ATC would likely adopt a lower initial cooperation probability and be very sensitive to observed defection.

*   **When `m = 2` and `n` is large:**
    This is a scenario where a single defector can break the cycle. ATC would be more cautious in its cooperation probability, especially if it observes even a small number of defections.

### 3. Being Cooperative

ATC is inherently cooperative due to its design:

*   **Initial Bias Towards Cooperation:** The strategy starts with a default probability of cooperation that is designed to trigger cooperation if others show similar intent.
*   **Reward-Seeking Behavior:** The core of the strategy is to identify and exploit situations where cooperation is likely to be rewarded, leading to higher collective payoffs.
*   **Adaptive Nature:** It doesn't simply defect out of fear. It *attempts* to cooperate and only retreats when its analysis of the game history suggests that doing so is detrimental to its own payoff. This is a form of "conditional cooperation" – cooperating as long as the conditions (observed behavior) suggest it's beneficial.
*   **Robustness through Probabilism:** By not always cooperating or defecting deterministically (except in the last round), ATC can avoid predictable patterns and potentially encourage hesitant cooperators. If ATC always cooperated, it would be easily exploited. If it always defected, it would never achieve the reward. The probabilistic approach is a middle ground that balances these risks and rewards.

### Pseudocode

```python
# Game Parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward if threshold met factor

# Strategy Parameters (tune these)
initial_cooperation_prob_t1 = m / n + 0.1  # Cooperation prob in round 1
adaptation_factor_up = 0.5
adaptation_factor_down = 0.7
baseline_cooperation_prob = m / n  # Initial baseline for rounds > 1
meta_adaptation_window = 10  # Number of past rounds to consider for meta-adaptation
meta_adaptation_increase = 0.01
meta_adaptation_decrease = 0.02

# State Variables (per player)
cooperation_history = [] # List of 0s (Defect) and 1s (Cooperate) for previous rounds
player_payoffs_history = [] # List of payoffs in previous rounds

def decide_action(current_round, n_cooperators_prev_round=None):
    """
    Decides whether to Cooperate (C) or Defect (D) in the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        n_cooperators_prev_round (int, optional): Number of players who cooperated in the previous round.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # --- Edge Case: Last Round ---
    if current_round == r:
        return 'D'

    # --- First Round ---
    if current_round == 1:
        if random.uniform(0, 1) < initial_cooperation_prob_t1:
            return 'C'
        else:
            return 'D'

    # --- Subsequent Rounds (t > 1) ---
    if n_cooperators_prev_round is None:
        # Should not happen if history is maintained correctly, but as a fallback
        # default to a cautious approach.
        current_cooperation_prob = baseline_cooperation_prob
    else:
        n_cooperators_ratio_prev_round = n_cooperators_prev_round / n

        # --- Adapt Baseline Cooperation Probability (Meta-Adaptation) ---
        if len(player_payoffs_history) >= meta_adaptation_window:
            avg_payoff_recent = sum(player_payoffs_history[-meta_adaptation_window:]) / meta_adaptation_window
            if avg_payoff_recent > 1.5: # Tune this threshold
                baseline_cooperation_prob = min(1.0, baseline_cooperation_prob + meta_adaptation_increase)
            elif avg_payoff_recent <= 1.0: # Tune this threshold
                baseline_cooperation_prob = max(0.0, baseline_cooperation_prob - meta_adaptation_decrease)

        # --- Calculate Current Cooperation Probability ---
        if n_cooperators_ratio_prev_round >= m / n:
            # Threshold met or exceeded, increase cooperation probability
            adjustment = (n_cooperators_ratio_prev_round - m / n) * adaptation_factor_up
            current_cooperation_prob = min(1.0, baseline_cooperation_prob + adjustment)
        else:
            # Threshold not met, decrease cooperation probability
            adjustment = (m / n - n_cooperators_ratio_prev_round) * adaptation_factor_down
            current_cooperation_prob = max(0.0, baseline_cooperation_prob - adjustment)

    # --- Make the Decision ---
    if random.uniform(0, 1) < current_cooperation_prob:
        return 'C'
    else:
        return 'D'

# Example usage within a tournament loop:
#
# for round_num in range(1, r + 1):
#     # Gather actions from all players (including this strategy)
#     my_action = decide_action(round_num, n_cooperators_prev_round=observed_cooperators_in_prev_round)
#     # Store my_action and update history after payoffs are calculated
#     cooperation_history.append(1 if my_action == 'C' else 0)
#     # ... (payoff calculation and updating player_payoffs_history)
```

This strategy aims to be a well-behaved but not naive participant, always seeking the higher rewards of cooperation when the risk is managed. Its adaptive nature allows it to learn and adjust to the collective behavior of the other players in the tournament.
'''

description_COOPERATIVE_40 = '''
This document outlines a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust in a multi-round repeated game setting with no communication. The core principle is to incentivize cooperation while mitigating the risk of exploitation.

## Cooperative Strategy: "Conditional Trust Tit-for-Tat"

This strategy is a variation of Tit-for-Tat, adapted for a collective risk scenario. It aims to build trust through cooperation but becomes increasingly cautious as the risk of defection from others grows.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in round `t` depends on the observed behavior of other players in previous rounds, specifically focusing on whether the cooperation threshold `m` was met.

*   **Initial Cooperation:** In the **first round** (t=1), the strategy will **Cooperate (C)**. This establishes a cooperative stance and signals willingness to contribute to the collective good. This is a crucial step to try and initiate cooperation.

*   **Subsequent Rounds (t > 1):**
    *   **If the cooperation threshold `m` was met or exceeded in the previous round `t-1`:** The strategy will **Cooperate (C)** in round `t`. This reinforces successful cooperation and encourages continued collective action.
    *   **If the cooperation threshold `m` was *not* met in the previous round `t-1`:** The strategy will **Defect (D)** in round `t`. This is a punitive measure to discourage free-riding and protect the endowment when the collective fails to achieve the benefit. The strategy will remain in a "defecting state" until the threshold is met again.

*   **The "Punishment" Phase (Once `m` is not met):** If the strategy enters a "defecting state" because the threshold was not met, it will continue to defect in all subsequent rounds *until* a round occurs where `m` or more players cooperated. Upon witnessing the threshold being met again, the strategy will revert to cooperating in the *next* round. This provides a mechanism to re-establish cooperation if the group corrects its behavior.

### 2. Handling Edge Cases

*   **First Round (t=1):** As mentioned above, the strategy **always Cooperates (C)** in the first round. This is the initial overture for cooperation and sets the foundation for learning from subsequent rounds.

*   **Last Round (t=r):** The strategy's decision in the last round follows the same logic as any other round:
    *   If `m` was met in round `r-1`, it will Cooperate (C).
    *   If `m` was not met in round `r-1`, it will Defect (D).
    There is no special "end-game" logic that deviates from the adaptive conditional cooperation principle. The strategy aims to maximize its cumulative payoff over all rounds, and the last round is simply the final opportunity to do so based on the established rules.

*   **Initial Lack of Data (t=1):** Since there is no history in the first round, the strategy defaults to a cooperative stance to explore the possibility of collective gain.

*   **Game Parameters:**
    *   `n` (number of players): Used implicitly to understand the scale of the game.
    *   `r` (number of rounds): Crucial for the repeated nature of the game. The strategy is designed to learn and adapt over these rounds.
    *   `m` (minimum cooperators needed): The central parameter dictating the success of collective action and triggering punishment.
    *   `k` (reward factor): Influences the incentive for cooperation. The strategy's responsiveness is implicitly driven by the potential gain `k`.

### 3. Being Cooperative

This strategy is inherently cooperative in its initial stance and its responsiveness to successful collective action.

*   **Initiation of Cooperation:** By always cooperating in the first round, it actively seeks to build a cooperative environment.
*   **Reinforcement of Cooperation:** When the group successfully meets the `m` threshold, the strategy rewards this by continuing to cooperate. This is a positive feedback loop aimed at sustaining beneficial collective behavior.
*   **Adaptive Punishment:** The strategy's defection is not malicious but a strategic response to the lack of collective effort. It's a mechanism to avoid being exploited when the collective risk is not averted. Crucially, it's a temporary punishment, designed to re-incentivize cooperation once the group demonstrates its commitment. It's a "tit-for-tat" for the collective.

### Pseudocode

```pseudocode
StrategyName: ConditionalTrustTitForTat

// State variables
var lastRoundCooperationCount: integer = 0 // Number of players who cooperated in the previous round
var isPunishing: boolean = false // Flag to indicate if we are in a punishment phase

// Function to decide action for the current round
function decideAction(currentRound: integer, gameParameters: {n, r, m, k}, pastActions: History): Action {

    // First Round: Always cooperate to initiate
    if currentRound == 1:
        lastRoundCooperationCount = 0 // Reset for safety, though not strictly needed if never used before round 2
        isPunishing = false
        return C

    // Check if the threshold was met in the previous round (t-1)
    // We need to count cooperators from the *last* round's actions
    var actualCooperatorsLastRound = 0
    if pastActions is not empty:
        lastRoundHistory = pastActions.getHistoryForRound(currentRound - 1)
        for playerAction in lastRoundHistory:
            if playerAction.action == C:
                actualCooperatorsLastRound += 1

    // Update punishment state based on last round's outcome
    if actualCooperatorsLastRound < m:
        isPunishing = true // Threshold not met, enter or stay in punishment mode
    else:
        isPunishing = false // Threshold met, exit punishment mode (for the *next* round)

    // Decision Logic
    if isPunishing:
        // If we are in a punishment phase (because m was not met last round), defect.
        // We remain punishing until we see m cooperators again.
        return D
    else:
        // If not punishing, it means m was met or exceeded last round.
        // Cooperate to reinforce collective success.
        return C
}

// The strategy implementation would call decideAction in each round.
// It would also need to maintain and update the history of actions for all players.
```

**Explanation of Pseudocode Variables:**

*   `lastRoundCooperationCount`: This variable is conceptualized in the logic. The actual counting would happen within the `decideAction` function by iterating through `pastActions`.
*   `isPunishing`: This boolean flag simplifies the logic. If `true`, it means the `m` threshold was *not* met in the previous round, and we are now in a phase of defecting. The flag is reset to `false` only when `m` is met, and then the *next* decision will be to cooperate.

This strategy aims to be a "nice" (starts cooperative) but "tough" (punishes defection) approach, adapting to the collective's ability to meet the cooperation threshold. It's robust because it doesn't rely on assumptions about other players' strategies, only on the observable outcome of whether the collective benefit was achieved.
'''

description_COOPERATIVE_41 = '''
This strategy is designed to promote cooperation in the Collective Risk Dilemma by being responsive to observed behavior and aiming for the collective good while safeguarding against exploitation. It is built on a foundation of conditional cooperation, adapting its own actions based on the aggregate cooperation level in previous rounds.

## Strategy: Adaptive Conditional Cooperator (ACC)

**Core Principle:** Cooperate if the observed level of cooperation in the previous round suggests that the community is on track to meet the cooperation threshold ($m$). If cooperation falters, defect to protect individual resources.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round $t$ (where $t > 1$) is based on the following:

*   **If the number of players who cooperated in round $t-1$ was $\geq m$:**
    *   Player $i$ plays **C**.
    *   **Reasoning:** The collective achieved the cooperation threshold in the previous round. This indicates that cooperation is viable and beneficial for everyone. Contributing again signals continued commitment to the collective good and encourages others to do the same.

*   **If the number of players who cooperated in round $t-1$ was $< m$:**
    *   Player $i$ plays **D**.
    *   **Reasoning:** The collective failed to meet the cooperation threshold in the previous round. This means the community project was not successful, and those who cooperated did not receive the reward (or potentially even lost their endowment if $k < 1$, though the problem states $k > 1$). In this scenario, it is individually rational to keep the endowment and avoid contributing to a likely unsuccessful endeavor. This also serves as a signal that the current level of cooperation is insufficient.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   Player $i$ plays **C**.
    *   **Reasoning:** In the absence of any historical data, the ACC strategy defaults to cooperation. This is the most "cooperative" initial move and aims to kickstart a cooperative dynamic. It assumes that the tournament might involve other strategies that also start with cooperation, making it more likely to reach the threshold and trigger the reward for everyone.

*   **Last Round (t=r):**
    *   The decision rule for the last round is the **same as for any other round (t > 1)**.
    *   **Decision Rule:** If the number of players who cooperated in round $t-1$ was $\geq m$, play C. Otherwise, play D.
    *   **Reasoning:** While there are no future rounds to influence, the payoff in the last round still depends on the actions of that round. The ACC strategy remains consistent in its conditional cooperation logic. If the previous round was cooperative enough, it will continue to cooperate. If not, it will defect. This ensures that the strategy's behavior is predictable and consistent throughout the game. There is no incentive to deviate from the core logic based on the game being the last round, as the payoff for round $r$ is still calculated based on the actions in round $r$.

*   **Parameter Violations (e.g., m=1, m=n, k <= 1):**
    *   While the problem statement specifies $1 < m < n$ and $k > 1$, a robust strategy might consider these. However, adhering strictly to the problem specification, we assume these parameters are valid. If they were not, the strategy's logic would still apply, but the game's dynamics would be drastically different and likely deterministic or trivial.

### 3. Being Cooperative

The ACC strategy is inherently cooperative in its design:

*   **Default to Cooperation:** It starts by cooperating, signaling a desire for collective success.
*   **Rewarding Cooperation:** It continues to cooperate when the community demonstrates its ability to meet the cooperation threshold, reinforcing positive behavior.
*   **Focus on Threshold:** The primary goal is to reach the cooperation threshold ($m$), which benefits all players by unlocking the higher payoff. The strategy's decisions are directly geared towards achieving this collective outcome.
*   **Adaptive, Not Punitive:** While it defects when cooperation fails, this is not a punitive measure. It's a rational response to protect individual resources when the collective risk is not averted. The moment cooperation resumes and the threshold is met, the strategy will revert to cooperating. This makes it robust to situations where a brief lapse in cooperation occurs but is quickly rectified by others.

## Pseudocode

```
Strategy ACC(player_id, n, r, m, k, history):
  // history is a list of dictionaries, where each dictionary represents a round
  // and contains the actions of all players for that round.
  // e.g., history[t-1] = {'player_1': 'C', 'player_2': 'D', ...}

  current_round = len(history) + 1

  // --- Decision Rules ---

  // First Round (t=1)
  if current_round == 1:
    return 'C'

  // Subsequent Rounds (t > 1)
  else:
    // Count cooperators in the previous round (t-1)
    previous_round_actions = history[current_round - 2] // Access the last completed round's actions
    cooperators_in_previous_round = 0
    for player_action in previous_round_actions.values():
      if player_action == 'C':
        cooperators_in_previous_round += 1

    // Decision based on previous round's cooperation level
    if cooperators_in_previous_round >= m:
      return 'C'  // Continue cooperating if threshold was met
    else:
      return 'D'  // Defect if threshold was not met

```

**Explanation of Pseudocode:**

*   `player_id`, `n`, `r`, `m`, `k`: These are the game parameters provided to the strategy.
*   `history`: This is a crucial data structure. It's assumed to be a list where each element represents a completed round, storing the actions of all players in that round. `history[0]` would be round 1, `history[1]` would be round 2, and so on. The length of `history` directly tells us the `current_round` number we are about to play (since `history` stores *completed* rounds).
*   `current_round = len(history) + 1`: Calculates the current round number. If `history` is empty, it's round 1. If `history` has 1 element (round 1's data), it's round 2, etc.
*   `if current_round == 1:`: Handles the initial round by defaulting to cooperation.
*   `else:`: For all other rounds.
*   `previous_round_actions = history[current_round - 2]`: Accesses the actions of the *immediately preceding* round. If `current_round` is 2, `current_round - 2` is 0, accessing the data for round 1.
*   `cooperators_in_previous_round = 0`: Initializes a counter.
*   The `for` loop iterates through the actions of all players in the previous round and increments the counter if a player cooperated.
*   `if cooperators_in_previous_round >= m:`: If the threshold was met, cooperate.
*   `else:`: If the threshold was not met, defect.

This strategy is designed to be a good general performer in a tournament setting by balancing the desire for collective gain with the need for self-preservation, adapting its behavior based on the observable outcomes of the game.
'''

description_COOPERATIVE_42 = '''
Okay, let's design a cooperative strategy for the Collective Risk Dilemma. The core challenge here is to encourage cooperation when the reward is high, but without being exploited when others defect. Since there's no communication, the strategy must rely on observing past behavior and inferring intent.

Our strategy will be an adaptive **Tit-for-Tat with a Graded Response and a Threshold Focus**. The name reflects its core principles:
*   **Tit-for-Tat:** Reacting to the opponent's previous actions.
*   **Graded Response:** The severity of our reaction is not binary (cooperate/defect) but can be influenced by the overall cooperation level.
*   **Threshold Focus:** Heavily influenced by whether the cooperation threshold `m` was met in previous rounds.

## Strategy: The "Cautious Collaborator"

The "Cautious Collaborator" aims to:

1.  **Promote Cooperation:** By being willing to cooperate when there's a reasonable chance of success.
2.  **Punish Defection:** By retaliating against players who undermine the collective effort.
3.  **Adapt to Environment:** By adjusting its cooperativeness based on historical outcomes.

### 1. Decision Rules

The decision for player `i` in round `t` (where `t > 1`) is primarily based on the *average cooperation rate* of *all players* in the *previous round* (`t-1`). Let `avg_coop_rate_(t-1)` be the proportion of players who cooperated in round `t-1`.

**Core Logic:**

*   **If `avg_coop_rate_(t-1)` is high (significantly above the required `m/n`):** This indicates a healthy cooperative environment. The Cautious Collaborator is more likely to cooperate.
*   **If `avg_coop_rate_(t-1)` is low (significantly below `m/n`):** This signals a breakdown in cooperation. The Cautious Collaborator will defect more readily.
*   **If `avg_coop_rate_(t-1)` is around `m/n`:** This is the critical zone. The Cautious Collaborator will be more sensitive to individual actions.

**Specific Rules for Round `t > 1`:**

Let `coop_count_(t-1)` be the number of players who cooperated in round `t-1`.

*   **Rule 1: If `coop_count_(t-1) >= m` (Threshold was met):**
    *   **And if player `i` cooperated in round `t-1`:** Play **C** (Cooperate). This rewards past cooperation and reinforces successful behavior.
    *   **And if player `i` defected in round `t-1`:** Play **D** (Defect). This punishes individual defection within a successful round.

*   **Rule 2: If `coop_count_(t-1) < m` (Threshold was NOT met):**
    *   **And if player `i` cooperated in round `t-1`:** Play **D** (Defect). This is a strong punishment for cooperating when the collective failed, signaling that individual sacrifice without collective reward is unsustainable.
    *   **And if player `i` defected in round `t-1`:** Play **D** (Defect). This continues to punish defection when the collective is failing.

**Simplification and Refinement of Rule 2:**

Rule 2 can be simplified: If the threshold was not met in the previous round, the Cautious Collaborator will **always defect** in the current round, regardless of its own previous action. This is a severe punishment that aims to reset cooperation by making it costly to be the sole (or one of few) defectors when cooperation fails.

**Revised Decision Rules for Round `t > 1`:**

1.  **If `coop_count_(t-1) >= m`:**
    *   If player `i` cooperated in round `t-1`: Play **C**.
    *   If player `i` defected in round `t-1`: Play **D**.

2.  **If `coop_count_(t-1) < m`:**
    *   Play **D**.

This strategy can be summarized as: **"If we succeeded together last round, I'll follow the group's lead (cooperating if I cooperated, defecting if I defected). If we failed together, I'll defect to punish the failure."**

### 2. Handling Edge Cases

*   **First Round (`t = 1`):**
    *   In the absence of any history, the Cautious Collaborator must make an initial assumption. Given the incentive structure (high reward for cooperation *if* the threshold is met), a reasonable initial stance is to **Cooperate (C)**. This signals a willingness to initiate cooperation and try to reach the beneficial outcome.
    *   **Decision:** Play **C**.

*   **Last Round (`t = r`):**
    *   In the final round, there's no future to build on. The incentive to cooperate for future rounds disappears. Therefore, pure self-interest dictates that a player should defect to maximize their individual gain in that last round.
    *   **Decision:** Play **D**. This is a pragmatic adjustment to the finite nature of the game. It ensures we don't get exploited in the very last interaction.

*   **Scenario: `m = n` (Threshold is everyone cooperating):**
    *   This is a highly stringent condition. If `m = n`, our Rule 1 needs a slight adjustment to handle the case where `coop_count_(t-1) = n-1` (i.e., everyone *except* one player cooperated).
    *   **Revised Rule 1 for `m = n`:**
        *   If `coop_count_(t-1) == n`:
            *   If player `i` cooperated in `t-1`: Play **C**.
            *   If player `i` defected in `t-1`: Play **D**.
        *   If `coop_count_(t-1) < n`: Play **D**.
    *   This means if even one player defects when `m=n`, we assume the collective failed and defect in the next round.

### 3. Being Cooperative

This strategy is fundamentally cooperative in its *intent* and its *initial action*.

*   **Initiating Cooperation:** By playing 'C' in the first round, we are taking a risk to signal a desire for collective success and to try and achieve the higher payoff.
*   **Rewarding Success:** When the threshold `m` is met, the strategy rewards past cooperation by continuing to cooperate if it was the player's own action in the previous round. This reinforces the positive feedback loop.
*   **Conditional Punishment:** The punishment (defecting when `m` was not met) is not arbitrary. It's a response to the collective failure. It aims to prevent the slow erosion of cooperation where a few defectors can perpetually undermine the group. By "punishing" oneself (defecting) when the group fails, it signals that cooperation is the desired state, and failure is unacceptable.

This strategy is adaptive because it learns from the aggregate behavior of the group. It's robust because it doesn't rigidly stick to one behavior, but adjusts based on the observed outcomes of cooperation. It's designed to be a strong contender in a tournament setting by balancing the desire for high payoffs through cooperation with the need for self-protection against exploitation.

---

### Pseudocode for "Cautious Collaborator" Strategy

```
Function DecideAction(player_id, current_round, all_actions_history, game_parameters):
  n = game_parameters.n
  m = game_parameters.m
  r = game_parameters.r
  k = game_parameters.k

  # Handle first round
  If current_round == 1:
    Return "C"  # Initiate cooperation

  # Handle last round
  If current_round == r:
    Return "D"  # Defect in the final round

  # --- Logic for rounds 2 to r-1 ---

  # Get actions from the previous round (current_round - 1)
  previous_round_actions = all_actions_history[current_round - 1]
  my_previous_action = previous_round_actions[player_id]

  cooperation_count_previous_round = 0
  for action in previous_round_actions:
    if action == "C":
      cooperation_count_previous_round += 1

  # Rule 1: Threshold was met in the previous round
  If cooperation_count_previous_round >= m:
    If my_previous_action == "C":
      Return "C"  # Continue cooperating if I cooperated and we succeeded
    Else (my_previous_action == "D"):
      Return "D"  # Defect if I defected and we still succeeded (punish individual defection)

  # Rule 2: Threshold was NOT met in the previous round
  Else (cooperation_count_previous_round < m):
    Return "D"  # Defect if the collective failed, regardless of my own action

```
'''

description_COOPERATIVE_43 = '''
This strategy, "Adaptive Threshold Cooperation" (ATC), is designed to promote cooperation in the Collective Risk Dilemma by dynamically adjusting player behavior based on observed cooperation levels and the game's parameters. It prioritizes achieving the collective reward while being mindful of the risk of being exploited.

**Core Philosophy:** ATC aims to be cooperatively inclined but pragmatically cautious. It seeks to hit the cooperation threshold ($m$) whenever it seems achievable or necessary to maintain a cooperative trajectory, but it will defect if cooperation appears to be failing or if the risk of being the sole cooperator becomes too high.

---

### 1. Decision Rules

ATC's decision in any given round $t$ (where $t > 1$) is determined by a probabilistic approach influenced by the observed cooperation rate in the previous round and the proximity to the cooperation threshold $m$.

**Key Variables:**

*   **$C_{t-1}$**: The number of players who cooperated in round $t-1$.
*   **$P_{coop\_t-1}$**: The observed cooperation rate in round $t-1$ ($P_{coop\_t-1} = C_{t-1} / n$).
*   **$T_{coop}$**: The target cooperation rate needed to reliably hit the threshold. This is a bit more than $m/n$ to account for uncertainty. A good starting point is $T_{coop} = m/n + \epsilon$, where $\epsilon$ is a small buffer. For robustness, we can define it as:
    $T_{coop} = \max(m/n, \frac{m}{n} + \frac{1}{n})$  *This ensures we're at least aiming for $m/n$ and adding a small buffer proportional to the number of players.*
*   **$P_{defect\_risk}$**: A probability factor representing the risk of defecting if the cooperation threshold is not met. This is inversely related to how far the current cooperation is *above* the minimum required for reward.
    $P_{defect\_risk} = \max(0, 1 - \frac{P_{coop\_t-1} - (m-1)/n}{(n-m+1)/n})$ *This calculates how far the current cooperation is *above* the threshold required to just barely avoid individual defection penalty. If cooperation is low, this value will be high, indicating high risk.*

**Decision Logic for Round $t$ (where $t > 1$):**

1.  **Observe and Analyze Previous Round ($t-1$):**
    *   Calculate $C_{t-1}$ (number of cooperators in round $t-1$).
    *   Calculate $P_{coop\_t-1} = C_{t-1} / n$.
    *   Calculate $P_{defect\_risk}$ based on $P_{coop\_t-1}$, $m$, and $n$.

2.  **Decision for Round $t$:**

    *   **If $C_{t-1} \ge m$ (Threshold Met or Exceeded in previous round):**
        *   **Cooperate (C):** With a high probability. This probability increases as $P_{coop\_t-1}$ is further above $m/n$.
            *   **Cooperation Probability = $1 - P_{defect\_risk} \times \text{risk\_sensitivity}$**
            *   `risk_sensitivity` is a parameter (e.g., 0.5) that dampens the effect of defect risk. A lower value makes ATC more forgiving of past near-failures.
        *   *Rationale:* If the group successfully cooperated, it's beneficial to continue to maintain the reward stream. The probability is high but not 100% to allow for occasional defections if others are perceived to be defecting heavily in future rounds, or if the `risk_sensitivity` is set to allow for some deviation.

    *   **If $C_{t-1} < m$ (Threshold Not Met in previous round):**
        *   **Assess Defection Risk:**
            *   **If $P_{coop\_t-1}$ is very low (e.g., $P_{coop\_t-1} < (m-1)/n$):** This means only a small fraction cooperated, and the individual payoff is low (1). The risk of being the *only* cooperator in the next round is high.
                *   **Defect (D):** With high probability.
                    *   **Defection Probability = $1 - (P_{coop\_t-1} / T_{coop}) \times \text{coop\_incentive}$**
                    *   `coop_incentive` is a parameter (e.g., 0.7) that represents how much ATC is willing to risk its own payoff to *try* and achieve cooperation. A higher value means it's more willing to try cooperating.
                *   *Rationale:* If cooperation is failing significantly, individual payoffs are likely to be 1. Continuing to cooperate risks receiving only 1 while others defect. This defensive posture is taken.
            *   **If $P_{coop\_t-1}$ is close to $m/n$ (but less than $m/n$):** This indicates that cooperation is *almost* working.
                *   **Cooperate (C):** With a probability that increases as $P_{coop\_t-1}$ approaches $m/n$.
                    *   **Cooperation Probability = $(P_{coop\_t-1} / T_{coop}) \times \text{coop\_incentive}$**
                *   *Rationale:* The group is on the verge of success. ATC will try to push it over the edge by cooperating, hoping that others will do the same. The `coop_incentive` parameter controls how much risk it takes.

**Summary of Decision Rules (Pseudocode):**

```pseudocode
FUNCTION DecideAction(player_history, game_params):
    n = game_params['n']
    m = game_params['m']
    r = game_params['r']
    current_round = len(player_history) + 1 // Assuming player_history is a list of past actions

    // --- Handle Edge Cases ---
    IF current_round == 1:
        RETURN 'C' // Cooperate in the first round to establish cooperative intent.

    IF current_round == r:
        // In the last round, incentives change. The "long-term" is gone.
        // This strategy is designed for repeated interaction, so last round is tricky.
        // A robust strategy might try to defect if it perceives a risk of being exploited.
        // For simplicity and to align with "cooperative mindset", we can still try to cooperate
        // if the group has a history of success, but be more cautious.
        // Let's define a "last round caution" factor.
        last_round_caution = 0.7 // Probability of defecting even if cooperation was successful.

        // Analyze previous round
        past_actions = player_history[-1]
        num_cooperators_prev = count('C' in past_actions)
        prob_coop_prev = num_cooperators_prev / n

        IF num_cooperators_prev >= m:
            // If threshold met, cooperate with high probability, but with last round caution
            coop_prob = (1 - calculate_defect_risk(prob_coop_prev, m, n)) * (1 - last_round_caution)
            IF random() < coop_prob:
                RETURN 'C'
            ELSE:
                RETURN 'D'
        ELSE: // Threshold not met
            IF prob_coop_prev < (m - 1) / n: // Very low cooperation
                // Defect with high probability
                defect_prob = 1 - (prob_coop_prev / (m / n)) * 0.7 // coop_incentive = 0.7
                IF random() < defect_prob:
                    RETURN 'D'
                ELSE:
                    RETURN 'C'
            ELSE: // Close to threshold
                // Cooperate with probability, but with last round caution
                coop_prob = (prob_coop_prev / (m / n)) * 0.7 // coop_incentive = 0.7
                IF random() < coop_prob * (1 - last_round_caution):
                    RETURN 'C'
                ELSE:
                    RETURN 'D'


    // --- Standard Round Decision (t > 1 and t < r) ---
    past_actions = player_history[-1]
    num_cooperators_prev = count('C' in past_actions)
    prob_coop_prev = num_cooperators_prev / n

    // Calculate P_defect_risk
    // P_defect_risk is high when cooperation is low, low when cooperation is high.
    // This represents the risk of being exploited if you cooperate and others defect.
    // A value of 1 means high risk, 0 means low risk.
    // we define it as how far current cooperation is *below* achieving the reward
    // If P_coop_prev >= m/n, this should be 0
    // If P_coop_prev < (m-1)/n, this should be high
    // Let's use a simpler formula that reflects this:
    defect_risk_factor = 0.0
    IF prob_coop_prev < m / n:
        // If cooperation is below threshold, the risk of being the only cooperator increases
        // As cooperation gets further from m/n, risk increases.
        // Normalize the distance from failure. (m/n) is success target. (m-1)/n is failure threshold.
        // Let's consider risk based on how far below m/n we are.
        // If prob_coop_prev is 0, risk is max. If prob_coop_prev is just below m/n, risk is low.
        // If prob_coop_prev < (m-1)/n, risk is high
        // If prob_coop_prev >= m/n, risk_factor = 0
        IF prob_coop_prev < (m-1)/n:
            // Example: n=6, m=3. m/n = 0.5, (m-1)/n = 0.333
            // If prob_coop = 0.166 (1 player), defect_risk_factor should be high.
            // If prob_coop = 0.333 (2 players), defect_risk_factor should be high.
            // If prob_coop = 0.4 (2 players, still below m/n), risk is lower.
            // Let's make risk high if we are below (m-1)/n
            defect_risk_factor = 1.0 - (prob_coop_prev / ((m-1)/n)) // This goes from 1 down to 0 as coop increases towards (m-1)/n
            defect_risk_factor = max(0.0, defect_risk_factor) // Cap at 0
        ELSE:
            // Between (m-1)/n and m/n, risk is moderate and decreases as we approach m/n
            defect_risk_factor = 1.0 - (prob_coop_prev / (m/n)) // This goes from 1 down to 0 as coop increases towards m/n
            defect_risk_factor = max(0.0, defect_risk_factor) // Cap at 0

    // Define parameters
    risk_sensitivity = 0.5
    coop_incentive = 0.7

    IF num_cooperators_prev >= m:
        // Threshold met or exceeded. Primarily cooperate.
        // Probability of cooperating: 1 - risk_factor * risk_sensitivity
        // If risk_factor is low (high cooperation), coop_prob is high.
        // If risk_factor is high (low cooperation), coop_prob is lower.
        coop_prob = 1.0 - (defect_risk_factor * risk_sensitivity)
        IF random() < coop_prob:
            RETURN 'C'
        ELSE:
            RETURN 'D' // Defect if probability says so, perhaps due to perceived drift
    ELSE:
        // Threshold not met.
        IF prob_coop_prev < (m - 1) / n:
            // Very low cooperation. High risk of being exploited. Defect defensively.
            // Probability of defecting: 1 - (how close we are to target coop rate) * coop_incentive
            // If coop_prob_prev is 0, defect_prob is high. If it's near m/n, defect_prob is lower.
            defect_prob = 1.0 - (prob_coop_prev / (m / n)) * coop_incentive
            defect_prob = max(0.0, defect_prob) // Cap at 0
            IF random() < defect_prob:
                RETURN 'D'
            ELSE:
                RETURN 'C' // Take a small risk to try and initiate cooperation
        ELSE:
            // Cooperation is close to the threshold (but below). Try to push it over.
            // Probability of cooperating: (how close we are to target coop rate) * coop_incentive
            coop_prob = (prob_coop_prev / (m / n)) * coop_incentive
            IF random() < coop_prob:
                RETURN 'C'
            ELSE:
                RETURN 'D' // Defect if the incentive to cooperate isn't high enough

// Helper function to calculate a nuanced defect risk factor
// This function aims to capture the risk of cooperating and being the sole cooperator.
// A higher return value means higher risk of being exploited.
FUNCTION calculate_defect_risk(prob_coop, m, n):
    // Target cooperation rate
    target_coop = m / n
    // Threshold for guaranteed individual failure if others defect
    near_failure_threshold = (m - 1) / n

    IF prob_coop >= target_coop:
        RETURN 0.0 // No risk if threshold is met or exceeded

    // If below target, the risk increases as cooperation decreases.
    IF prob_coop < near_failure_threshold:
        // If cooperation is very low, the risk is high.
        // Map 0.0 probability of coop to 1.0 risk, and near_failure_threshold to 1.0 risk.
        // This means if coop is very low, risk is close to 1.
        risk = 1.0 - (prob_coop / near_failure_threshold)
        RETURN max(0.0, risk) // Ensure risk is not negative
    ELSE:
        // If cooperation is between near_failure_threshold and target_coop,
        // the risk is still high but decreasing as cooperation approaches target_coop.
        // Map near_failure_threshold to 1.0 risk, and target_coop to 0.0 risk.
        risk = 1.0 - ((prob_coop - near_failure_threshold) / (target_coop - near_failure_threshold))
        RETURN max(0.0, risk)

```

---

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** Always cooperate ('C').
    *   **Rationale:** To signal a cooperative intent and initiate the game with a positive contribution. This helps build trust and gives others the opportunity to reciprocate. It's a strategic "opening gambit."

*   **Last Round (t=r):**
    *   **Decision:** The logic becomes more cautious. While the core strategy attempts to maintain cooperation if successful, there's a `last_round_caution` factor that increases the probability of defecting even if the threshold was met in the previous round. If the threshold was *not* met, the decision-making mirrors the standard round but with this added caution.
    *   **Rationale:** In the final round, there is no future consequence for defection or exploitation. The incentive to cooperate for future rewards disappears. This strategy tries to balance the cooperative "spirit" with the self-interested outcome of the last round by introducing a higher chance of defection, especially if the history is not overwhelmingly cooperative.

*   **Threshold Not Met but Very Close (e.g., $m-1$ cooperators):**
    *   **Decision:** Cooperate with a probability proportional to how close it is to the target $m/n$, modulated by `coop_incentive`.
    *   **Rationale:** To actively try and push the group over the threshold. This is where the strategy is most "cooperative" in the sense of trying to achieve the collective benefit.

*   **Threshold Not Met and Very Far Below (e.g., significantly less than $m-1$ cooperators):**
    *   **Decision:** Defect with a high probability, but with a small chance of cooperating to try and turn the tide.
    *   **Rationale:** To avoid being the sole cooperator and receiving a payoff of 1, while also acknowledging that a very small chance of initiating cooperation might be worthwhile if the `coop_incentive` is high enough.

---

### 3. Being Cooperative

*   **Initial Cooperative Stance:** ATC begins by cooperating, a clear signal of cooperative intent.
*   **Responding to Success:** When the group successfully meets the cooperation threshold, ATC strongly favors continued cooperation, aiming to maintain the positive collective outcome.
*   **Encouraging Threshold Achievement:** When the group is close to meeting the threshold, ATC actively tries to cooperate to push it over, prioritizing the collective reward.
*   **Adaptive Risk Management:** While designed to be cooperative, ATC is not naive. It understands that being the only cooperator leads to a poor individual outcome. Its probabilistic decisions in response to past outcomes are a mechanism to balance the desire for collective good with the need for self-preservation against systematic exploitation. The probability of defection is directly tied to the perceived risk of exploitation, not to punitive intent.

**Parameters and Tuning:**

The parameters `risk_sensitivity` and `coop_incentive`, as well as the `last_round_caution`, can be tuned.
*   **`risk_sensitivity`**: A higher value makes ATC more likely to defect when cooperation levels are low (more cautious). A lower value makes it more forgiving and more likely to continue cooperating even if cooperation was borderline.
*   **`coop_incentive`**: A higher value makes ATC more willing to risk its payoff in rounds where cooperation is close but not yet achieved. A lower value makes it more risk-averse in these situations.
*   **`last_round_caution`**: Affects how much more likely ATC is to defect in the final round compared to a standard round where cooperation was successful.

This strategy is designed to be robust because its decisions are dynamically linked to observed history and game parameters, rather than relying on fixed patterns or assumptions about other players' strategies. It adapts its level of cooperation based on the perceived likelihood of collective success and the risk of individual exploitation.
'''

description_COOPERATIVE_44 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, focusing on adaptability and robustness in a repeated game setting with no communication.

## Cooperative Strategy: The "Cautious Benefactor"

This strategy aims to balance the desire for collective reward with the inherent risk of being exploited. It prioritizes cooperation when the conditions for success seem favorable, but defensively defects when it perceives a high likelihood of failure or exploitation.

### Core Philosophy:

*   **Conditional Cooperation:** Cooperate only when the observed behavior of other players suggests that the cooperation threshold (m) is likely to be met.
*   **Adaptive Threshold Adjustment:** The strategy doesn't rely on a fixed historical count of cooperation. Instead, it dynamically assesses the "cooperative momentum" of the group.
*   **Defensive Pessimism:** In situations of uncertainty or when the group is trending towards defection, the strategy defaults to self-preservation.
*   **Long-Term Reward Focus:** While immediate gains are considered, the strategy is geared towards maximizing cumulative payoff over the entire game.

### 1. Decision Rules:

The core of the "Cautious Benefactor" strategy lies in its dynamic assessment of the likelihood of reaching the cooperation threshold 'm'. We'll use a "cooperative score" to track this.

**a. Cooperative Score (S):**

This score represents the estimated number of players likely to cooperate in the *current* round. It's updated after each round based on observed actions.

*   **Initialization:** At the start of round 1, $S = n$ (we assume everyone *could* cooperate initially).
*   **Update Rule (after round t):**
    Let $C_t$ be the number of players who cooperated in round $t$.
    Let $N_t$ be the total number of players observed cooperating *up to and including round t*.
    Let $R_{obs}$ be the number of rounds observed so far.

    The update rule aims to capture recent trends. A simple, yet effective, approach is a weighted average of observed cooperations, giving more weight to recent rounds, but also considering the overall trend.

    A more sophisticated update might look like this:

    *   **If $C_t \ge m$ (threshold met):** The group demonstrated success. Increase the perceived likelihood of cooperation.
        $S_{t+1} = S_t + (n - S_t) * \alpha$  where $\alpha$ is a "positive reinforcement factor" (e.g., 0.1). This means we increase our estimate of cooperation by a fraction of the remaining "potential" cooperators.
    *   **If $C_t < m$ (threshold not met):** The group failed. Decrease the perceived likelihood of cooperation.
        $S_{t+1} = S_t - S_t * \beta$ where $\beta$ is a "negative reinforcement factor" (e.g., 0.2). This means we decrease our estimate by a fraction of our current estimate.
    *   **If no players cooperated ($C_t = 0$) or very few cooperated ($C_t < m/2$):** This indicates strong defection pressure.
        $S_{t+1} = max(0, S_{t+1} - (m - C_t))$  (A more aggressive reduction).
    *   **Cap the score:** $S_{t+1} = min(n, max(0, S_{t+1}))$

    *Note: The specific $\alpha$ and $\beta$ values, and the exact form of the update rule, can be tuned. The core idea is to make the score reactive to recent outcomes.*

**b. Decision Rule for Player i in Round t:**

Player i decides to Cooperate (C) or Defect (D) based on the current Cooperative Score (S) and the remaining rounds.

*   **If $S \ge m$ (Threshold Likely Met):** Cooperate.
    *   This is the primary condition for cooperation. If our assessment suggests 'm' or more players are likely to cooperate, we join them to secure the reward.
*   **If $S < m$ (Threshold Unlikely Met):** Defect.
    *   If our assessment indicates a low probability of meeting the threshold, defecting is the safer option to preserve our endowment.

**Refinement for the Last Round:**

*   **In the last round (t = r):** The logic becomes simpler. If $S \ge m$, cooperate. Otherwise, defect. The long-term considerations disappear.

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   **Initial Action:** Cooperate (C).
    *   **Rationale:** In the absence of any history, the strategy starts with an optimistic assumption that cooperation is possible and the reward can be achieved. This is the most cooperative starting point. The Cooperative Score 'S' is initialized to 'n' (or perhaps 'm', representing the minimum hope). Let's go with $S=n$ for maximum initial optimism.
*   **Last Round (t=r):**
    *   **Decision:** As described above, cooperate if $S \ge m$, otherwise defect. There is no future to incentivize risk.
*   **When the group consistently fails to meet 'm' (low $C_t$ in many recent rounds):**
    *   The Cooperative Score 'S' will naturally decrease, leading the strategy to defect more often. This is a built-in adaptive response.
*   **When the group consistently meets 'm' (high $C_t$ in many recent rounds):**
    *   The Cooperative Score 'S' will increase, leading the strategy to cooperate more often.

### 3. Being Cooperative:

*   **Starting with Cooperation:** The strategy's first move is always to cooperate. This signals a cooperative intent from the outset.
*   **Conditional Cooperation:** The strategy *wants* to cooperate and seeks opportunities to do so. It only defects when the observed evidence strongly suggests that cooperation will fail to yield the collective reward.
*   **Maximizing Collective Gain (when possible):** By cooperating when $S \ge m$, the strategy actively participates in securing the reward for everyone. This is the essence of its cooperative drive.

### Pseudocode:

```
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Strategy State
cooperative_score: float  // Represents estimated number of likely cooperators
player_actions_history: list of dictionaries // To store actions of all players in each round
my_actions_history: list // To store own actions

// Initialization (before round 1)
cooperative_score = n  // Start optimistic
player_actions_history = []
my_actions_history = []

// Function to update cooperative_score
function update_cooperative_score(current_round_actions, observed_cooperators_in_round_t, round_number):
    observed_cooperators_in_round_t = count_cooperators(current_round_actions)

    if round_number == 1:
        // For round 1, we start with full potential
        cooperative_score = n
        return

    // Simple weighted average update (can be more complex)
    // Weight recent rounds more heavily. Example:
    // We can use a decay factor, say 'decay_rate' (e.g., 0.9 for round_number-1, 0.81 for round_number-2 etc.)
    // For simplicity here, let's use a simpler trend-based update:

    if observed_cooperators_in_round_t >= m:
        // Success: Increase confidence in cooperation
        cooperative_score = cooperative_score + (n - cooperative_score) * 0.1 // Positive reinforcement
    else:
        // Failure: Decrease confidence in cooperation
        cooperative_score = cooperative_score - cooperative_score * 0.2 // Negative reinforcement

    // Aggressive reduction for very low cooperation
    if observed_cooperators_in_round_t < m / 2:
        cooperative_score = max(0, cooperative_score - (m - observed_cooperators_in_round_t))

    // Cap the score
    cooperative_score = min(n, max(0, cooperative_score))

// Main decision function for round t
function decide_action(round_number):
    // Handle first round
    if round_number == 1:
        my_action = 'C'
        return my_action

    // Handle last round
    if round_number == r:
        if cooperative_score >= m:
            my_action = 'C'
        else:
            my_action = 'D'
        return my_action

    // Intermediate rounds: decision based on cooperative_score
    if cooperative_score >= m:
        my_action = 'C'
    else:
        my_action = 'D'

    return my_action

// --- Tournament Play Logic ---

// For each round 't' from 1 to r:
//   1. Get observations of all player actions from previous rounds (if any).
//   2. Update 'cooperative_score' based on observed actions in round 't-1'
//      (if t > 1, otherwise use initial value).
//   3. Determine 'my_action' using decide_action(t).
//   4. Record 'my_action' in my_actions_history.
//   5. Submit 'my_action'.
//   6. Receive other players' actions for round 't'.
//   7. Add actions of round 't' to player_actions_history.
```

### Robustness and Adaptability:

*   **Adaptability:** The `cooperative_score` dynamically adjusts based on the group's past behavior. It doesn't get stuck in a fixed belief. If the group becomes more cooperative, the strategy will follow. If it becomes more defecting, the strategy will also adapt.
*   **Robustness:**
    *   **Against exploiters:** If other players consistently defect when 'm' is almost met, the `cooperative_score` will drop, leading this strategy to defect as well, preventing consistent losses.
    *   **Against overly optimistic groups:** Even if the group *tries* to cooperate but fails to reach 'm' consistently, the `cooperative_score` will reflect this failure, making the strategy defect when it's likely to be in vain.
    *   **Against aggressive defectors:** The strategy is not naive. It will eventually shift to defecting if a significant portion of players defect consistently.

This "Cautious Benefactor" strategy aims to be a reliable and cooperative participant, striving for collective benefit while maintaining a healthy degree of self-preservation in the face of uncertainty and potential defection.
'''

description_COOPERATIVE_45 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, aiming for robustness and adaptivity:

## Cooperative Strategy: "Threshold Tracker"

This strategy aims to achieve the cooperative outcome (meeting the threshold `m`) by carefully monitoring past cooperation levels and adjusting its own behavior accordingly. It prioritizes cooperation when there's a strong indication that others are also cooperating or are likely to cooperate, while defecting to protect its own endowment when the collective effort is insufficient or faltering.

### 1. Decision Rules

The core of the "Threshold Tracker" strategy lies in dynamically assessing the likelihood of reaching the cooperation threshold `m`.

**Rule 1: Initial Cautious Cooperation (First Round)**

*   **In Round 1:** Player *i* plays **C**.
    *   **Rationale:** The first round is a baseline. While there's uncertainty, initiating cooperation signals a willingness to work towards the collective good. It's a small initial risk for a potentially large collective reward.

**Rule 2: Cooperation Based on Past Success (Rounds t > 1)**

*   **If (Average Cooperation Rate in previous rounds) >= (m-1)/n:** Play **C**.
    *   **Rationale:** This rule uses the average cooperation rate from *all* previous rounds as a proxy for the collective willingness to cooperate. If, on average, at least `m-1` players have cooperated in the past, it suggests a general trend towards cooperation. By playing `C` in this scenario, player `i` is more likely to tip the balance and reach the `m` threshold, securing the reward for everyone. We use `(m-1)/n` as the threshold to ensure that even if player `i` is the only one cooperating in the current round, the *historical average* would still need to be high enough to justify this risk. This provides a buffer.

**Rule 3: Defection Based on Insufficient Past Cooperation (Rounds t > 1)**

*   **If (Average Cooperation Rate in previous rounds) < (m-1)/n:** Play **D**.
    *   **Rationale:** If the historical average of cooperation is low, it indicates that the collective effort is unlikely to reach the threshold `m`. In this situation, player `i` defects to secure its private endowment of `1`, avoiding the risk of contributing `1` and receiving nothing (or even less than `1` if `k` is small).

**Rule 4: The "Close Call" Adjustment (Rounds t > 1)**

*   **If (Number of Cooperators in Round t-1) >= m-1 AND (Number of Cooperators in Round t-1) < m:** Play **C**.
    *   **Rationale:** This is a crucial adaptive element. If in the immediately preceding round, the collective effort came very close to the threshold (`m-1` cooperators), but fell just short, this strategy will actively try to bridge that gap in the current round by cooperating. This is a strong signal that a small nudge can achieve the desired outcome, and player `i` is willing to provide that nudge.

**Rule 5: The "Last Chance" (Final Round, r)**

*   **In Round r:**
    *   **If (Average Cooperation Rate in previous rounds) >= (m-1)/n:** Play **C**.
    *   **Else (if Average Cooperation Rate < (m-1)/n) AND (Number of Cooperators in Round r-1) >= m-1:** Play **C**.
    *   **Else:** Play **D**.
    *   **Rationale:** In the final round, the long-term incentive of future rounds disappears. The strategy still prioritizes cooperation if the historical average is promising. However, it adds a "last chance" for cooperation if the previous round was a "close call," even if the overall historical average is slightly below the threshold. This maximizes the chance of a final reward if it's within reach. If neither of these conditions is met, it defects to secure its gains.

### 2. Handling Edge Cases

*   **First Round (t=1):** As specified in Rule 1, player `i` always plays **C**. This is a default cooperative gesture to initiate the game.
*   **Last Round (t=r):** Handled by **Rule 5**. The decision logic is slightly modified to account for the absence of future rounds. The primary driver remains historical success, but a strong push is made if the previous round was a "close call."
*   **Early Rounds (t=2 to r-1):** Handled by **Rules 2, 3, and 4**. The strategy relies on accumulating history to make informed decisions.

### 3. Cooperative Mindset Alignment

*   **Prioritizes Collective Success:** The strategy's primary objective is to reach the cooperation threshold `m`. It actively seeks opportunities to cooperate when it believes the collective effort is likely to succeed.
*   **Adaptive and Forgiving:** It's not overly punitive. It continues to cooperate if the historical trend is positive, even if there were instances of defection by others in the past (as long as the *average* remains high enough).
*   **Strategic Cooperation:** While cooperative, it's not naively altruistic. It defects when the probability of collective success is low, protecting its own endowment and signaling a boundary. This prevents exploitation.
*   **Incentivizes Further Cooperation:** By cooperating when close to the threshold, it encourages others to continue cooperating in future rounds, reinforcing the cooperative norm.

### Pseudocode Representation:

```pseudocode
strategy ThresholdTracker(game_parameters, history):
  n = game_parameters.n
  r = game_parameters.r
  m = game_parameters.m
  k = game_parameters.k

  current_round = history.current_round()

  // Rule 1: Initial Cautious Cooperation
  if current_round == 1:
    return C

  // Calculate average cooperation rate from previous rounds
  total_cooperators_previous_rounds = 0
  for round_num = 1 to current_round - 1:
    total_cooperators_previous_rounds += history.get_cooperators_in_round(round_num)
  average_cooperation_rate = total_cooperators_previous_rounds / (current_round - 1)

  // Calculate number of cooperators in the previous round
  cooperators_last_round = 0
  if current_round > 1:
      cooperators_last_round = history.get_cooperators_in_round(current_round - 1)

  // Define cooperative threshold for decision making
  cooperative_threshold = (m - 1) / n

  // Rule 5: The "Last Chance" (Final Round)
  if current_round == r:
    if average_cooperation_rate >= cooperative_threshold:
      return C
    else if cooperators_last_round >= m - 1: // "Close Call" in the last round
      return C
    else:
      return D

  // Rule 2: Cooperation Based on Past Success
  if average_cooperation_rate >= cooperative_threshold:
    return C

  // Rule 4: The "Close Call" Adjustment
  if cooperators_last_round >= m - 1 and cooperators_last_round < m:
    return C

  // Rule 3: Defection Based on Insufficient Past Cooperation
  return D

```

**Note on `history.get_cooperators_in_round(round_num)`:** This refers to a hypothetical function that would return the count of players who cooperated in a specific past round. The AI would need to store this information internally.

This strategy is designed to be a strong cooperative player that adapts to the behavior of others without relying on explicit communication or pre-defined patterns. It leverages historical data to make informed decisions, aiming to maximize its own payoff while fostering the collective good.
'''

description_COOPERATIVE_46 = '''
This strategy aims to foster cooperation by signaling a willingness to contribute, while defensively reacting to perceived defection. It balances the desire for the high payoff from collective cooperation with the risk of being exploited.

## Cooperative Strategy: "Reciprocal Tit-for-Tat with Escalation"

This strategy is an adaptation of the classic Tit-for-Tat, modified to address the collective nature of the risk and the potential for exploitation. It prioritizes cooperation when the conditions appear favorable and punishes deviations, but also offers a path back to cooperation.

### 1. Decision Rules

The core of the strategy is to **cooperate in the current round if the collective cooperation in the *previous* round was sufficient to meet the threshold 'm', and the player was part of that successful cooperation.** Otherwise, it defects.

Let's define some terms:

*   `my_actions_history`: A list of the player's own past actions (C or D).
*   `all_actions_history`: A list of lists, where each inner list contains the actions of all players in a given round.
*   `cooperators_in_prev_round`: The number of players who cooperated in the immediately preceding round.
*   `i_cooperated_in_prev_round`: A boolean indicating if *this* player cooperated in the immediately preceding round.

**Decision Rule for Round `t` (where `t > 1`):**

1.  **Calculate `cooperators_in_prev_round`**: Count the number of 'C' actions in `all_actions_history[t-1]`.
2.  **Check if `i_cooperated_in_prev_round`**: Look up `my_actions_history[t-1]`.
3.  **Decision Logic:**
    *   **If `cooperators_in_prev_round >= m` AND `i_cooperated_in_prev_round` is TRUE:**
        *   **Play C (Cooperate).** The previous round was successful, and I contributed. This signals continued willingness to cooperate under favorable conditions.
    *   **Else (if `cooperators_in_prev_round < m` OR `i_cooperated_in_prev_round` is FALSE):**
        *   **Play D (Defect).** This is the default reactive behavior. Either the group failed to cooperate sufficiently, or I was not part of the successful cooperation (meaning I was either unable to, or chose not to). This punishes perceived inaction or free-riding.

### 2. Handling Edge Cases

*   **Round 1 (The First Round):**
    *   **Default Action: C (Cooperate).** In the absence of any history, the strategy starts with an optimistic and cooperative stance. This is a crucial signal to other players that cooperation is the desired outcome. It also gives the system the best chance to trigger the collective reward from the outset.

*   **The Last Round (Round `r`):**
    *   **Decision Rule remains the same.** The strategy doesn't change its behavior in the last round. While future rounds are not considered, the immediate payoff in the last round is still maximized by achieving the cooperation threshold `m`. Therefore, the "Reciprocal Tit-for-Tat" logic still applies. If the previous round was successful and the player cooperated, they cooperate again to try and secure the reward. Otherwise, they defect.

*   **Initial Cooperation Failure (when `m` is high relative to `n`):**
    *   If `m` is very high, it might be difficult to reach the threshold even with cooperative intent. The strategy's default to cooperate in Round 1 and then defect if the threshold isn't met in Round 1 will naturally lead to a period of defection until others demonstrate consistent cooperation. The core of the strategy is to *respond* to observed cooperation.

### 3. Being Cooperative

The strategy is inherently cooperative in the following ways:

*   **Initiation of Cooperation:** It *always* starts by cooperating in the first round. This is a strong signal of good faith and a willingness to build a cooperative environment.
*   **Reward for Cooperation:** It cooperates in subsequent rounds *if and only if* the previous round achieved the cooperation threshold (`m`) *and* the player themselves contributed to that success. This directly rewards collective success and individual contribution.
*   **Conditional Defection:** Defection is not arbitrary. It is a direct *response* to a lack of collective success or a perceived failure to contribute to it in the past. This is a form of reciprocity, punishing free-riding and signaling that cooperation is contingent on mutual effort.
*   **Re-establishment of Cooperation:** While the strategy defects when conditions are not met, it doesn't enter a permanent cycle of defection. If the group *starts* cooperating again and meets the threshold, and the player cooperates in that round, the strategy will revert to cooperating in the *next* round, provided it met the threshold. This allows for forgiveness and re-entry into cooperative play.

### Pseudocode Representation

```python
# Global variables (or passed as parameters)
n: int  # number of players
r: int  # number of rounds
m: int  # minimum cooperators needed
k: float # reward if threshold met factor

# History tracking (will be updated by the game engine)
my_actions_history = []  # List of 'C' or 'D'
all_actions_history = [] # List of lists, each inner list has actions of all players for a round

def decide_action(current_round_number):
    """
    Decides whether to Cooperate (C) or Defect (D) in the current round.
    """
    if current_round_number == 1:
        # Round 1: Always cooperate to initiate
        return 'C'

    # Get history from the previous round
    prev_round_actions = all_actions_history[-1] # Get actions from the last completed round
    cooperators_in_prev_round = prev_round_actions.count('C')

    # Check if this player cooperated in the previous round
    # Assuming player's own actions are at index 'my_player_index' in prev_round_actions if tracked globally
    # Or more simply, check the last action from my_actions_history
    i_cooperated_in_prev_round = my_actions_history[-1] == 'C'

    # Decision Logic
    if cooperators_in_prev_round >= m and i_cooperated_in_prev_round:
        # If threshold was met and I contributed, cooperate again
        return 'C'
    else:
        # Otherwise, defect
        return 'D'

```

**Robustness and Adaptability:**

*   **Robustness to Opponent Behavior:**
    *   **Always Cooperate players:** This strategy will reciprocate their cooperation, leading to mutual high payoffs if `m` is met.
    *   **Always Defect players:** This strategy will eventually defect after the first round if the "Always Defect" players prevent the threshold from being met. It won't be exploited indefinitely.
    *   **Random players:** The strategy will respond to the observed outcomes. If random play leads to sufficient cooperation, it will cooperate. If not, it will defect.
    *   **Tit-for-Tat variants:** This strategy is a strong contender against other reciprocal strategies. Its initial cooperation is key.
    *   **Exploitative strategies:** If an opponent defects while others cooperate, this strategy will eventually stop cooperating with them by defecting in the next round (if the threshold isn't met or if the player wasn't part of the successful cooperation). However, it will continue to try and build cooperation with other players.

*   **Adaptability:** The strategy adapts by constantly observing the outcome of the previous round. Its decision is directly contingent on the group's collective success and its own prior contribution. This allows it to "learn" from the dynamics of the game and adjust its behavior accordingly.

This strategy offers a balance between promoting cooperation through initial trust and safeguarding against exploitation through reciprocal punishment. It aims to maximize its own payoff by seeking the high reward of collective success while minimizing the risk of being a sucker.
'''

description_COOPERATIVE_47 = '''
## Strategy: The "Cautious Benefactor"

This strategy aims to maximize the collective payoff by encouraging cooperation, but with a strong emphasis on avoiding individual ruin. It's designed to be adaptive and learn from the history of play, without relying on pre-existing coordination.

**Core Principle:** Cooperate as long as it seems beneficial and feasible, but be prepared to defect to protect yourself if cooperation breaks down or seems unlikely to succeed.

### 1. Decision Rules

The Cautious Benefactor's decision in each round `t` depends on a `cooperation_threshold` and a `defection_punishment_counter`.

*   **`cooperation_threshold`**: This is the *minimum* number of players we *anticipate* will cooperate in the current round. It starts at `m` and can adjust upwards or downwards.
*   **`defection_punishment_counter`**: This counter tracks how many consecutive rounds have passed where the number of cooperators was *less than `m`*. This counter is reset when the threshold is met.

**In Round `t` (where `t` is the current round number, starting from 1):**

1.  **Decision to Cooperate (Play 'C'):**
    *   **If `t == 1` (First Round):** Cooperate. This is an initial gesture of goodwill and allows us to gather information.
    *   **If `t > 1`:**
        *   Let `N_cooperators_prev` be the number of players who cooperated in the *previous* round (`t-1`).
        *   **If `N_cooperators_prev >= m`:** Cooperate. This indicates that the collective effort was successful in the last round, reinforcing the incentive to continue cooperating.
        *   **If `N_cooperators_prev < m`:**
            *   **If `defection_punishment_counter < (n - m + 1)`:** Cooperate. This means we haven't experienced too many failed cooperation rounds recently. We're willing to give cooperation another chance, especially if the failure in the previous round was just by a small margin or if many players defected. The threshold `(n - m + 1)` is a heuristic to prevent indefinite cooperation after repeated failures. It essentially means we'll tolerate up to `n-m` failed rounds before considering more aggressive defection.
            *   **If `defection_punishment_counter >= (n - m + 1)`:** Defect. Too many consecutive rounds of failed cooperation have occurred. It's no longer strategically sound to contribute.

2.  **Decision to Defect (Play 'D'):**
    *   **If `t > 1` and the conditions for cooperating are NOT met.**

**Updating State Variables (After each round `t`):**

*   **Update `N_cooperators_prev`**: Store the actual number of cooperators from round `t` for use in round `t+1`.
*   **Update `defection_punishment_counter`**:
    *   If the number of cooperators in round `t` was `>= m`, reset `defection_punishment_counter` to 0.
    *   If the number of cooperators in round `t` was `< m`, increment `defection_punishment_counter` by 1.

### 2. Handling Edge Cases

*   **First Round (`t == 1`):** As stated above, always **Cooperate**. This is crucial for initiating the game and signaling a willingness to participate in the collective good. It also provides the initial data point for subsequent decisions.

*   **Last Round (`t == r`):**
    *   **If `N_cooperators_prev >= m` (and `t > 1`):** Cooperate. Even in the last round, if cooperation was successful previously, we continue to be cooperative. The immediate gain from defecting might be tempting, but this strategy prioritizes the long-term cumulative payoff and the potential for learning by continuing cooperative behavior.
    *   **If `N_cooperators_prev < m` (and `t > 1`):** Defect. If cooperation failed in the penultimate round, and we've reached the last round, there's no future to incentivize with cooperation. It's better to secure the individual payoff.

*   **Parameter Extremes:**
    *   **`m` very close to `n`:** This makes cooperation very difficult. The strategy will naturally tend towards defection unless there's overwhelming evidence of collective action.
    *   **`m` very close to 2:** Cooperation is easier to achieve. The strategy will be more inclined to cooperate.
    *   **High `k`:** The reward for cooperation is substantial. This will make the strategy more resilient to initial failures and more willing to "wait it out" for cooperation to take hold.
    *   **Low `k`:** The reward is less significant. The strategy will be more cautious about contributing if it's not reciprocated.

### 3. Being Cooperative

The Cautious Benefactor is fundamentally cooperative in its intent.

*   **Initial Cooperation:** It always starts by cooperating, signaling a desire for collective success.
*   **Reward for Success:** It continues to cooperate as long as the collective goal (`m` cooperators) is achieved, directly rewarding successful cooperation.
*   **Resilience:** It's designed to be resilient to occasional failures, giving cooperation multiple chances before resorting to individualistic behavior. This "gives the benefit of the doubt" to the collective.
*   **Learning:** The strategy adapts its behavior based on observed outcomes, learning from the game's history to better predict and influence future cooperation.
*   **No Exploitation:** It does not actively try to exploit others. Its defection is a defensive mechanism, triggered by perceived unreliability of the collective, rather than an aggressive tactic to gain an unfair advantage.

### Pseudocode

```
Strategy: CautiousBenefactor

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

State Variables:
  current_round = 1
  previous_cooperators_count = 0
  defection_punishment_counter = 0
  MAX_DEFECTION_punishment_COUNTER = (n - m + 1) # Heuristic for defection threshold

Function DecideAction():
  // --- First Round ---
  if current_round == 1:
    return 'C'

  // --- Last Round ---
  if current_round == r:
    if previous_cooperators_count >= m:
      return 'C'
    else:
      return 'D'

  // --- Intermediate Rounds ---
  if previous_cooperators_count >= m:
    // Cooperation was successful last round, continue cooperating
    return 'C'
  else:
    // Cooperation failed last round
    if defection_punishment_counter < MAX_DEFECTION_punishment_COUNTER:
      // Not too many consecutive failures yet, give cooperation another chance
      return 'C'
    else:
      // Too many consecutive failures, defect
      return 'D'

Function UpdateState(actual_cooperators_this_round):
  // Update for the next round
  previous_cooperators_count = actual_cooperators_this_round

  if actual_cooperators_this_round >= m:
    defection_punishment_counter = 0 // Reset counter on success
  else:
    defection_punishment_counter += 1 // Increment counter on failure

  current_round += 1

```

This strategy embodies a philosophy of being a "good global citizen" while retaining a strong sense of self-preservation. It aims to foster cooperation by demonstrating it, rewarding it, and being patient, but it will not be a perpetual victim of a non-cooperative environment.
'''

description_COOPERATIVE_48 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive, robust, and focused on fostering cooperation without explicit communication.

## Cooperative Strategy: The "Threshold Trust" Strategy

This strategy aims to leverage the repeated nature of the game to build trust and incentivize cooperation by responding to past outcomes. It's designed to be cooperative in spirit but also pragmatic in its pursuit of mutual benefit.

### Core Philosophy:

"I will cooperate if I believe that cooperation is likely to be successful and beneficial to me. I will use past observations to gauge this likelihood and adjust my behavior accordingly. My primary goal is to reach the cooperation threshold, but I will not be a 'sucker' – if cooperation consistently fails, I will adapt."

### 1. Decision Rules:

The central idea is to maintain a dynamic assessment of the "likelihood of successful cooperation" based on historical data. Let's call this internal assessment `cooperation_probability`.

**Initial State:**
*   At the beginning of the game, assume a moderate level of cooperation is possible. `cooperation_probability` starts at a baseline value, perhaps reflecting the minimum required cooperators relative to the total players. For instance, `cooperation_probability` = `m / n`.

**Per Round Decision (for player `i` in round `t`):**

*   **If `t == 1` (First Round):**
    *   **Cooperate (C).** The first round is an exploratory move. We want to signal a willingness to cooperate and give the system a chance to achieve the threshold. This is a necessary risk to establish a baseline.

*   **If `t > 1`:**
    *   **Observe Past Round Outcome (round `t-1`):**
        *   Count the number of players who cooperated in round `t-1` (let's call this `cooperators_last_round`).
        *   Calculate player `i`'s payoff in round `t-1` (`payoff_last_round`).

    *   **Update `cooperation_probability`:**
        *   **If `cooperators_last_round >= m` (Threshold met in the previous round):**
            *   Increase `cooperation_probability`. This signifies that cooperation was successful. The magnitude of the increase should be influenced by whether player `i` cooperated or defected in the previous round.
                *   If player `i` also cooperated (`c_i_last_round == 1`): `cooperation_probability` = `cooperation_probability` + `adjustment_factor_positive` (e.g., 0.1)
                *   If player `i` defected (`c_i_last_round == 0`): `cooperation_probability` = `cooperation_probability` + `adjustment_factor_slight` (e.g., 0.05) - still reward successful cooperation, but less enthusiastically if we exploited it.
        *   **If `cooperators_last_round < m` (Threshold not met in the previous round):**
            *   Decrease `cooperation_probability`. This indicates cooperation failed.
                *   If player `i` cooperated (`c_i_last_round == 1`): `cooperation_probability` = `cooperation_probability` - `adjustment_factor_negative` (e.g., 0.15) - significant penalty for failed cooperation when we contributed.
                *   If player `i` defected (`c_i_last_round == 0`): `cooperation_probability` = `cooperation_probability` - `adjustment_factor_mild` (e.g., 0.05) - slight penalty if we didn't contribute and it still failed.

    *   **Constrain `cooperation_probability`:** Ensure `cooperation_probability` stays within [0, 1].

    *   **Make Decision for Round `t`:**
        *   **If `cooperation_probability >= threshold_to_cooperate` (a predefined threshold, e.g., 0.6):**
            *   **Cooperate (C).** We believe there's a good chance of reaching the threshold.
        *   **Else:**
            *   **Defect (D).** We are not confident enough in successful cooperation to risk our endowment.

### 2. Handling Edge Cases:

*   **First Round (`t == 1`):** As stated above, always cooperate. This is crucial for initiating the feedback loop.
*   **Last Round (`t == r`):**
    *   **Defect (D).** In the absence of future rounds to build cooperation or retaliate for past betrayals, the dominant strategy in a single-shot game is to defect. There's no incentive to cooperate if there are no future gains to be made.
*   **Insufficient History (e.g., if `r` is very small, or if we are designing for a potentially shorter-than-expected game):** The initial `cooperation_probability` and the adjustment factors are designed to be robust. The strategy will naturally converge towards defection if cooperation consistently fails even with minimal history.
*   **All Cooperators/All Defectors Scenarios:**
    *   **All Cooperate:** The `cooperation_probability` will increase significantly, locking in cooperation if `k` is sufficiently high and `m` is low.
    *   **All Defect:** The `cooperation_probability` will decrease steadily, leading to consistent defection.

### 3. Being Cooperative:

*   **Proactive Cooperation:** The strategy starts by cooperating, signaling goodwill and providing an opportunity for mutual gain.
*   **Adaptive Trust:** It doesn't blindly cooperate. It rewards successful cooperation and punishes failures, encouraging all players to contribute to the common good.
*   **Long-Term Orientation:** The repeated nature of the game is leveraged to foster stable cooperative outcomes. The strategy aims to create a positive feedback loop where cooperation leads to more cooperation.
*   **Fairness:** While aiming to maximize its own payoff, the strategy implicitly encourages a state where all players can benefit from the reward `k` if the threshold is met. It avoids exploiting the system by defecting when cooperation is successful, unless it has strong reasons to believe cooperation is unstable.

### Pseudocode:

```pseudocode
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward if threshold met factor

// Strategy Parameters
initial_cooperation_probability = m / n  // Or another reasonable starting point
adjustment_factor_positive = 0.1       // How much to increase prob if we cooperated and it worked
adjustment_factor_slight = 0.05        // How much to increase prob if we defected and it worked
adjustment_factor_negative = 0.15      // How much to decrease prob if we cooperated and it failed
adjustment_factor_mild = 0.05          // How much to decrease prob if we defected and it failed
threshold_to_cooperate = 0.6         // Probability threshold to decide on C vs D

// Internal State
cooperation_probability = initial_cooperation_probability
last_round_actions = array of size n (initialized to None)
last_round_my_payoff = 0
last_round_cooperator_count = 0

function decide_action(current_round, all_actions_prev_round, my_payoff_prev_round):
  // Store history for the next round's decision
  last_round_actions = all_actions_prev_round
  last_round_my_payoff = my_payoff_prev_round
  last_round_cooperator_count = count_cooperators(all_actions_prev_round)

  // --- Decision Logic ---
  if current_round == 1:
    return C // Cooperate in the first round

  if current_round == r:
    return D // Defect in the last round

  // Update cooperation_probability based on last round's outcome
  my_action_last_round = my_action_based_on_history(my_index) // Need player index

  if last_round_cooperator_count >= m:
    // Threshold was met
    if my_action_last_round == C:
      cooperation_probability += adjustment_factor_positive
    else: // my_action_last_round == D
      cooperation_probability += adjustment_factor_slight
  else:
    // Threshold was NOT met
    if my_action_last_round == C:
      cooperation_probability -= adjustment_factor_negative
    else: // my_action_last_round == D
      cooperation_probability -= adjustment_factor_mild

  // Clamp probability between 0 and 1
  cooperation_probability = max(0, min(1, cooperation_probability))

  // Make decision for the current round
  if cooperation_probability >= threshold_to_cooperate:
    return C
  else:
    return D

function count_cooperators(actions):
  count = 0
  for action in actions:
    if action == C:
      count += 1
  return count

// Helper function to retrieve own action from history
// (Assumes we know our player index `my_index`)
function my_action_based_on_history(my_index):
  // This would need to be implemented based on how history is stored.
  // If last_round_actions is a list of actions for players 0 to n-1:
  return last_round_actions[my_index]
```

This strategy is designed to be a good citizen in the "tournament" by aiming for mutually beneficial outcomes, but it also has a self-preservation mechanism to avoid being exploited. Its adaptive nature allows it to learn and adjust to the behavior of other players over time.
'''

description_COOPERATIVE_49 = '''
This is a fascinating "Collective Risk Dilemma" game. The core challenge is to incentivize cooperation when individual defection offers a private benefit (keeping the endowment) but risking the collective outcome. The repeated nature of the game allows for learning and reputation building.

Here's a cooperative strategy designed to be adaptive and robust, focusing on maximizing collective well-being while safeguarding against exploitation.

## Strategy: "Conditional Commitment with Gradual Trust"

This strategy aims to be cooperative by default, but cautiously so. It gradually builds trust based on observed behavior and leverages the repeated nature of the game to punish defection and reward cooperation. The core idea is to commit to cooperation as long as the collective is leaning towards it, but to pull back if defection becomes too prevalent.

### 1. Decision Rules:

The strategy's decision for round `t` is primarily determined by the observed cooperation rate in the *previous* round (`t-1`).

*   **Initial State (First Round):** In the very first round, there is no history. To initiate cooperation and signal good intent, the strategy will **Cooperate (C)**. This is a necessary leap of faith to try and establish a cooperative norm.

*   **Subsequent Rounds (t > 1):**
    *   **Calculate Observed Cooperation Rate:** Let `ObservedCoopRate(t-1)` be the proportion of players (including oneself) who cooperated in round `t-1`.
        *   `ObservedCoopRate(t-1) = (Number of players who played C in round t-1) / n`

    *   **Decision Logic:**
        *   **If `ObservedCoopRate(t-1)` >= `m/n` (i.e., the threshold for success was met or exceeded in the previous round):**
            *   **Decision:** **Cooperate (C)**.
            *   **Rationale:** This indicates that collective cooperation was successful in the previous round. Continuing to cooperate reinforces this positive outcome and aims to maintain the successful reward.

        *   **If `ObservedCoopRate(t-1)` < `m/n` (i.e., the threshold for success was *not* met in the previous round):**
            *   **Decision:** **Defect (D)**.
            *   **Rationale:** This is a punitive measure. If the group failed to cooperate sufficiently in the previous round, it suggests either a lack of sufficient cooperation from others or a situation where individual cooperation was not sufficiently rewarded due to others defecting. Defecting in this scenario conserves your endowment and signals that the current level of cooperation is not sustainable or beneficial.

*   **The "Commitment Threshold" Adjustment (Adaptive Element):** To make the strategy more robust and less prone to immediate collapse or exploitation, we can introduce a subtle adjustment. Instead of strictly adhering to `m/n`, we can use a slightly more forgiving threshold for cooperating, and a slightly stricter threshold for defecting. This creates a "zone of uncertainty" that encourages convergence towards cooperation.

    Let's define two thresholds:
    *   `CooperateThreshold = m/n` (The minimum success rate to continue cooperating)
    *   `DefectThreshold = (m-1)/n` (The maximum failure rate to trigger defection)

    *   **Revised Decision Logic:**
        *   **If `ObservedCoopRate(t-1)` >= `CooperateThreshold`:** **Cooperate (C)**.
        *   **If `ObservedCoopRate(t-1)` <= `DefectThreshold`:** **Defect (D)**.
        *   **If `DefectThreshold` < `ObservedCoopRate(t-1)` < `CooperateThreshold`:** This is the "gray zone". In this zone, the strategy will **Cooperate (C)**.
            *   **Rationale for Gray Zone:** This zone represents a situation where the previous round was *close* to success but didn't quite make it. By continuing to cooperate here, we give the group another chance to reach the threshold without immediately punishing. This prevents a domino effect of defection from minor deviations. It's a form of "conditional trust" – if we're close to success, we'll keep trying.

### 2. Handling Edge Cases:

*   **First Round (t=1):** As stated above, always **Cooperate (C)**. This is the initial commitment to a cooperative outcome.

*   **Last Round (t=r):** In the final round, there is no future to influence. Therefore, the incentive structure changes. The "repeated game" aspect is gone. The rational decision for an individual player becomes purely about maximizing their own payoff in that single round.
    *   **To maximize individual payoff in the last round:**
        *   If the total number of players cooperating *including yourself* would reach `m` or more, you will **Cooperate (C)**. This is because your payoff will be `(1 - 1) + k = k`.
        *   If the total number of players cooperating *including yourself* would be less than `m`, you will **Defect (D)**. This is because your payoff will be `(1 - 0) = 1`.
    *   **Implementation of Last Round Logic:** This requires estimating how many others will cooperate. Since there's no communication, this is a difficult prediction. A robust approach here is to assume the worst-case scenario: *no one else will cooperate*.
        *   If `m > 1` (which is always true by parameter), and you are the *only* one cooperating, the threshold will not be met. So, in the last round, if `m > 1`, the strategy will **Defect (D)**.
        *   **Exception:** If `m = 1` and `n > 1`, then cooperating by yourself would meet the threshold. However, the problem states `1 < m < n`, so `m` is always greater than 1. Therefore, in the last round, the strategy will always **Defect (D)**.

*   **Game End with No History (r=1):** The problem states `r > 1`, so this edge case is not applicable.

### 3. Cooperativeness:

This strategy is inherently cooperative in its design:

*   **Default to Cooperation:** It starts by cooperating and continues to cooperate as long as the group shows sufficient signs of collective effort.
*   **Punishment is Restorative:** When it defects, it's not out of malice but as a signal and a self-preservation measure. It aims to encourage a return to cooperation by making defection costly for the group (and for the individual who defected by not receiving the reward).
*   **Rewards Success:** It actively seeks to achieve the collective reward by continuing cooperation when the threshold is met.
*   **Adaptive Trust:** It doesn't blindly trust, but it also doesn't immediately give up on cooperation. It's willing to extend trust as long as there's a reasonable chance of collective success.

### Pseudocode:

```
// Strategy: Conditional Commitment with Gradual Trust

// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// State Variables (internal to the strategy)
current_round: integer
last_round_cooperators: integer // Number of players who cooperated in the previous round

function decide_action(current_round, n, m, r, history):
    // history is a list of previous rounds' outcomes. For this strategy,
    // we only need the number of cooperators from the last round.

    if current_round == 1:
        // First round: Initial commitment to cooperation
        return "C"
    else if current_round == r:
        // Last round: Rational self-interest, no future cooperation to incentivize
        // Since m > 1, cooperating alone is insufficient.
        return "D"
    else:
        // Subsequent rounds (1 < current_round < r)
        observed_coop_rate_prev_round = history.get_last_round_coop_rate() // Assume this function is available

        cooperate_threshold = m / n
        defect_threshold = (m - 1) / n // Slightly more forgiving for cooperation

        if observed_coop_rate_prev_round >= cooperate_threshold:
            // Previous round was successful or highly cooperative
            return "C"
        else if observed_coop_rate_prev_round <= defect_threshold:
            // Previous round was significantly unsuccessful
            return "D"
        else:
            // In the "gray zone" - close to success, give another chance
            return "C"

// Helper function to get coop rate from history (simplified)
// In a real implementation, history would store the actions of all players.
// For this strategy, we only need the count of cooperators.

function get_last_round_coop_rate(history):
    if history is empty:
        return 0 // Should not happen for current_round > 1
    
    last_round_actions = history.get_actions_from_last_round()
    cooperator_count = 0
    for action in last_round_actions:
        if action == "C":
            cooperator_count += 1
    return cooperator_count / n

```

**Key Strengths of this Strategy:**

*   **Adaptive:** It adjusts its behavior based on the observed collective behavior.
*   **Robust:** The "gray zone" prevents immediate collapse due to minor fluctuations. The last-round behavior is rational.
*   **Cooperative Intent:** It consistently attempts to reach the cooperative outcome and rewards it.
*   **Self-Preserving:** It avoids being exploited by defecting when cooperation is demonstrably not working.
*   **Parameter-Aware:** It directly uses `m` and `n` to determine cooperation thresholds.

This strategy aims to be a strong performer in a tournament by balancing the desire for collective gain with the necessity of self-protection in a game of imperfect trust.
'''

description_COOPERATIVE_50 = '''
Okay, I will design a cooperative strategy for the Collective Risk Dilemma that adheres to the specified parameters and constraints. The core idea is to be conditionally cooperative, escalating cooperation when it's beneficial and safe, and retaliating against defection, while always aiming to achieve the collective good.

## Cooperative Strategy: "Conditional Escalation with Retaliation"

This strategy aims to achieve the highest possible cumulative payoff by fostering cooperation, but with a strong emphasis on self-preservation and punishing defection. It's designed to be adaptive, responding to the observed behavior of other players.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round *t* (where *t* ranges from 1 to *r*) depends on the observed history of the game, specifically the number of cooperators in the previous round and the player's own past actions.

**Core Principle:** Cooperate if the collective benefits outweigh the individual risk, especially if there's evidence of past cooperation and the potential to reach the threshold. Defect as a punishment for widespread defection and as a last resort to secure at least a baseline payoff.

**Decision Logic for Player *i* in Round *t*:**

*   **Initialization (Round 1):** Always Cooperate (C).
    *   **Rationale:** In the absence of any information, starting with cooperation is the most altruistic and provides the best chance for the collective to succeed from the outset. It signals a willingness to cooperate.

*   **Subsequent Rounds (t > 1):**
    *   **Condition 1: "Sufficient Past Cooperation"**
        *   Let `cooperators_prev_round` be the number of players who cooperated in round *t-1*.
        *   If `cooperators_prev_round` ≥ *m* (the cooperation threshold):
            *   **Action:** Cooperate (C).
            *   **Rationale:** The collective successfully met the threshold in the previous round, and the reward `k` was realized for everyone. This is a strong signal that cooperation is beneficial and sustainable. Continuing to cooperate maintains this positive state.

    *   **Condition 2: "Near Miss with Own Cooperation"**
        *   If `cooperators_prev_round` < *m* AND player *i* cooperated in round *t-1* AND (`cooperators_prev_round` + 1) ≥ *m* (meaning if player *i* had defected, the threshold would not have been met, but since *i* cooperated, the threshold was *not* met, but *i* was the marginal cooperator):
            *   **Action:** Cooperate (C).
            *   **Rationale:** Player *i* personally contributed to attempting to reach the threshold, but it was just missed. This indicates that player *i*'s cooperation is crucial for success, and the potential reward is within reach. Continuing to cooperate offers another chance for collective success.

    *   **Condition 3: "Dominant Defection Trend"**
        *   If `cooperators_prev_round` < *m* AND player *i* defected in round *t-1*:
            *   **Action:** Defect (D).
            *   **Rationale:** The collective failed to meet the threshold in the previous round, and player *i* did not contribute to cooperation. This suggests that the environment is not conducive to cooperation or that other players are defecting. To protect their own payoff, player *i* will defect to secure the baseline private endowment. This is a form of "tit-for-tat" at a collective level.

    *   **Condition 4: "Near Miss by Others"**
        *   If `cooperators_prev_round` < *m* AND player *i* cooperated in round *t-1* AND (`cooperators_prev_round` + 1) < *m* (meaning player *i* cooperated, but the threshold was still missed, and even if player *i* had defected, the threshold *would not* have been met):
            *   **Action:** Defect (D).
            *   **Rationale:** Player *i* cooperated, but the collective still failed to reach the threshold, and their individual cooperation wasn't enough to make the difference. This suggests a widespread lack of cooperation from others. To avoid wasting their endowment on a failed collective project, player *i* will defect to secure their private gain.

### 2. Handling Edge Cases:

*   **First Round (t = 1):** As stated above, always Cooperate (C). This is the foundational move to initiate cooperation.

*   **Last Round (t = r):**
    *   **Decision Logic:** In the final round, there is no future to incentivize cooperation. The incentive to maintain a cooperative relationship or to punish defection for future rounds disappears. Therefore, the rational strategy is to maximize individual payoff.
    *   **Action:** Defect (D).
    *   **Rationale:** In the last round, players receive their final payoff and the game ends. Any further cooperation is unrewarded in the long term. Therefore, a purely self-interested player would defect to secure the maximum individual gain of 1, rather than risk getting 0 if *m*-1 other players cooperate and they also cooperate.

*   **Early Rounds with High Defection:** If in the early rounds (e.g., first 2-3 rounds) the number of cooperators is consistently very low (e.g., less than *m*/2, or a small absolute number), the strategy might consider a premature shift to defection. However, the current rules already handle this by the "Dominant Defection Trend" condition. This strategy implicitly assumes *r* is large enough for trends to emerge.

*   **Specific Thresholds (m):**
    *   If *m* is close to *n* (e.g., *m* = *n*-1), cooperation is very difficult. The strategy will be less likely to cooperate as the threshold is harder to meet.
    *   If *m* is small (e.g., *m* = 2), cooperation is easier. The strategy will be more inclined to cooperate, especially if the history shows a few cooperators.

### 3. Cooperative Mindset Alignment:

This strategy is fundamentally **conditionally cooperative**. It starts with cooperation and actively seeks to maintain and foster it.

*   **Maximizing Collective Payoff:** The strategy's primary goal is to reach the cooperation threshold (*m*) in as many rounds as possible to trigger the `k` reward. This directly benefits all players involved.
*   **Punishing Defection (Retaliation):** When cooperation fails and this strategy has contributed to the cooperative effort (either by cooperating itself or being the marginal cooperator), and the threshold is still missed, it will defect in the next round. This is not malicious; it's a deterrent and a mechanism to prevent its own endowment from being "wasted" on a failed collective effort when others are not contributing sufficiently. It signals: "If you don't help us reach the goal, I will secure my own gains."
*   **Forgiveness (Implicit):** The strategy doesn't hold grudges indefinitely. If the environment shifts back towards cooperation (e.g., `cooperators_prev_round` >= *m*), it will readily resume cooperation.
*   **Self-Preservation:** While cooperative, it's not naive. It ensures that it secures at least its private endowment (1) if cooperation fails due to a lack of participation from others. This makes it robust against exploitation.

---

### Pseudocode Representation:

```pseudocode
Strategy "Conditional Escalation with Retaliation":

Input:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor
  current_round: integer (1 to r)
  my_history: list of actions played by this player (C or D)
  opponent_actions_history: list of lists, where each inner list contains actions of all players in a round (excluding this player's action)

Output:
  Action: 'C' or 'D'

Function DecideAction(current_round, my_history, opponent_actions_history):

  // --- Edge Case: Last Round ---
  If current_round == r:
    Return 'D'  // Maximize individual payoff in the final round

  // --- Round 1: Initial Cooperation ---
  If current_round == 1:
    Return 'C'

  // --- Subsequent Rounds (t > 1) ---
  // Get actions from the previous round (t-1)
  previous_round_actions = []
  If opponent_actions_history is not empty:
    previous_round_actions = opponent_actions_history[last_index_of_opponent_actions_history]

  // Determine my action in the previous round
  my_action_prev_round = 'D' // Default if history is empty (shouldn't happen for t>1)
  If my_history is not empty:
    my_action_prev_round = my_history[last_index_of_my_history]

  // Count cooperators in the previous round
  cooperators_prev_round = 0
  For each player_action in previous_round_actions:
    If player_action == 'C':
      cooperators_prev_round = cooperators_prev_round + 1

  // --- Decision Rules ---

  // Condition 1: Sufficient Past Cooperation
  If cooperators_prev_round >= m:
    Return 'C'

  // Condition 2: Near Miss with Own Cooperation
  // (I cooperated, threshold missed, but I was the marginal cooperator)
  If my_action_prev_round == 'C' AND cooperators_prev_round < m AND (cooperators_prev_round + 1) >= m:
    Return 'C'

  // Condition 3: Dominant Defection Trend
  // (Threshold missed, and I defected last round)
  If cooperators_prev_round < m AND my_action_prev_round == 'D':
    Return 'D'

  // Condition 4: Near Miss by Others
  // (I cooperated, threshold missed, and even my cooperation wasn't enough)
  If my_action_prev_round == 'C' AND cooperators_prev_round < m AND (cooperators_prev_round + 1) < m:
    Return 'D'

  // Fallback (should ideally be covered by above conditions, but as a safeguard)
  // If no specific condition met, err on the side of caution/self-preservation
  // This could occur if the history is incomplete or unusual.
  // Based on the logic, if m is not met, and I cooperated, but it wasn't a near miss,
  // then it's likely a widespread defection scenario.
  If cooperators_prev_round < m AND my_action_prev_round == 'C':
    Return 'D'
  Else: // Should be rare, but default to defect if unsure or in a bad state
    Return 'D'

```

This strategy is designed to be robust by adapting to the collective behavior, rewarding cooperation, and punishing defection appropriately. It prioritizes achieving the collective reward (`k`) but without being exploited.
'''

description_COOPERATIVE_51 = '''
This strategy, named **"Adaptive Threshold Cooperation" (ATC)**, aims to foster cooperation by cautiously contributing to the collective good while remaining opportunistic enough to capitalize on others' defection. It's designed to be robust against a variety of opponent behaviors and to maximize its own long-term payoff by aligning with the cooperative spirit when it's beneficial and safe to do so.

## Adaptive Threshold Cooperation (ATC) Strategy

The core idea of ATC is to monitor the collective cooperation rate in previous rounds and adjust its own cooperation threshold accordingly. It starts with a moderate level of cooperation and becomes more or less cooperative based on the success (or failure) of the group in reaching the `m` threshold.

### 1. Decision Rules

ATC's decision to cooperate or defect in a given round `t` depends on two primary factors:

*   **Observed Cooperation Rate:** The proportion of players who cooperated in the previous round (`t-1`).
*   **Personalized Cooperation Trigger:** A dynamic threshold that reflects the player's confidence in reaching the collective goal.

Here's how the decision process works:

*   **Initial State (Round 1):** In the first round, ATC doesn't have any historical data. To balance caution and the desire to initiate cooperation, it will cooperate if the number of players `n` is sufficiently large relative to `m`, and if `k` is significantly rewarding. Specifically:
    *   If `k` is very high (e.g., `k > n/m`), ATC will err on the side of cooperation to signal willingness.
    *   Otherwise, it will play a slightly more cautious strategy. A good starting point is to cooperate if `n` is significantly larger than `m` (e.g., `n > m * 1.5`) to increase the probability of success.

    **In essence, for Round 1:**
    *   **Cooperate (C)** if `k > 1.5` AND `n > m * 1.5`. This signals a willingness to invest if the reward is substantial and the odds are favorable.
    *   **Defect (D)** otherwise. This is a safe initial stance when the stakes aren't as high or the odds are less certain.

*   **Subsequent Rounds (t > 1):**
    1.  **Calculate Previous Round's Cooperation Rate:** Let `cooperators_t-1` be the number of players who cooperated in round `t-1`.
    2.  **Calculate Actual Cooperation Percentage:** `cooperation_percentage_t-1 = cooperators_t-1 / n`.
    3.  **Compare to Target:** Compare `cooperation_percentage_t-1` to the critical cooperation threshold for success, which is `m/n`.
    4.  **Adjust Cooperation Tendency:**
        *   **If `cooperation_percentage_t-1 >= m/n` (Threshold Met or Exceeded):** The group was successful in the previous round. This reinforces the benefit of cooperation. ATC increases its propensity to cooperate.
        *   **If `cooperation_percentage_t-1 < m/n` (Threshold Not Met):** The group failed to reach the cooperative goal. This suggests that previous cooperation was not sufficient or that defections were too prevalent. ATC reduces its propensity to cooperate.

    **Decision Logic for Round `t` (where `t > 1`):**

    *   **If `cooperation_percentage_t-1 >= m/n` AND ATC cooperated in round `t-1`:**
        *   **Cooperate (C):** Continue cooperating to reinforce successful behavior.
    *   **If `cooperation_percentage_t-1 >= m/n` AND ATC defected in round `t-1`:**
        *   **Cooperate (C):** Tentatively cooperate. The group succeeded despite ATC's defection. This is an opportunity to contribute and potentially benefit from the collective reward, while also signaling willingness to re-engage.
    *   **If `cooperation_percentage_t-1 < m/n` AND ATC cooperated in round `t-1`:**
        *   **Defect (D):** Stop cooperating. The group failed, and ATC's cooperation didn't lead to success. This is a defensive move.
    *   **If `cooperation_percentage_t-1 < m/n` AND ATC defected in round `t-1`:**
        *   **Defect (D):** Continue defecting. The group failed, and ATC's defection was consistent with the outcome.

**Simplified Decision Rule for `t > 1`:**

ATC will cooperate in round `t` if and only if:

*   The group achieved the cooperation threshold (`m`) in round `t-1`.
*   AND ATC itself cooperated in round `t-1`.

OR

*   The group achieved the cooperation threshold (`m`) in round `t-1`.
*   AND ATC defected in round `t-1` (this is a signal to re-join the cooperative effort).

**Refined Decision Rule for `t > 1` (incorporating learning):**

ATC maintains a "cooperation willingness" factor, initially set based on Round 1.

*   **Initialize `cooperation_willingness`:**
    *   If `k > 1.5` AND `n > m * 1.5`, `cooperation_willingness = 1.0` (highly inclined to cooperate).
    *   Otherwise, `cooperation_willingness = 0.5` (moderate inclination).

*   **In round `t > 1`:**
    1.  Calculate `cooperation_percentage_t-1`.
    2.  **Update `cooperation_willingness`:**
        *   If `cooperation_percentage_t-1 >= m/n`: `cooperation_willingness = min(1.0, cooperation_willingness + 0.1)` (gradually increase willingness if successful).
        *   If `cooperation_percentage_t-1 < m/n`: `cooperation_willingness = max(0.0, cooperation_willingness - 0.2)` (more sharply decrease willingness if unsuccessful).
    3.  **Decision:**
        *   **Cooperate (C)** if `random_float() < cooperation_willingness`.
        *   **Defect (D)** otherwise.

This introduces a probabilistic element, making ATC less predictable while still strongly guided by past outcomes and the overall success rate.

### 2. Handling Edge Cases

*   **First Round (t=1):** As described above, ATC starts with a cautious but potentially optimistic stance depending on `n`, `m`, and `k`. It's a calculated gamble to establish a baseline.

*   **Last Round (t=r):** In the last round, there is no future to incentivize cooperation. Therefore, the rational choice for any self-interested player is to defect. ATC, despite its cooperative spirit, will also **defect (D)** in the last round (`t=r`), regardless of history. This is a critical adaptation to the finite nature of the game.

*   **No previous cooperation (e.g., if all players always defected):** If `cooperation_percentage_t-1` is consistently 0, `cooperation_willingness` will decay to 0. ATC will then reliably defect in subsequent rounds unless there's a significant parameter change or a radical shift in the group's behavior (which is unlikely without communication).

*   **Always meet threshold:** If the group always meets the threshold, `cooperation_willingness` will increase to 1.0, and ATC will almost always cooperate.

### 3. Being Cooperative

ATC embodies a cooperative mindset through several mechanisms:

*   **Willingness to Initiate (Round 1):** In favorable conditions (`k` high, `n` large relative to `m`), ATC is willing to take the first step towards cooperation, signaling trust.
*   **Reinforcing Success:** When the group succeeds in meeting the `m` threshold, ATC rewards this by increasing its likelihood of cooperating in the future. This positively reinforces collective action.
*   **Learning and Adapting:** ATC doesn't get stuck in a purely adversarial loop. If cooperation leads to success, it increases its cooperative posture. If it was the only one cooperating and the threshold wasn't met, it learns from the failed attempt and becomes more cautious, but it doesn't immediately become a permanent defector.
*   **Opportunistic Re-engagement:** If ATC defected in a round but the group still managed to meet the threshold, it sees this as a positive signal and is willing to cooperate again in the next round, bridging the gap.
*   **Focus on Long-Term Payoff:** By adapting its strategy based on observed success, ATC aims to maximize its total payoff over `r` rounds, which is achieved through a balance of private gains and shared rewards from successful cooperation.

---

## Pseudocode for Adaptive Threshold Cooperation (ATC)

```pseudocode
// Game Parameters
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

// Player's State
// cooperation_willingness: float between 0.0 and 1.0
// previous_action: 'C' or 'D'

// --- Initialization ---
FUNCTION InitializeStrategy():
  IF k > 1.5 AND n > m * 1.5 THEN
    cooperation_willingness = 1.0
  ELSE
    cooperation_willingness = 0.5
  END IF
  previous_action = NULL // No previous action for round 1
END FUNCTION

// --- Decision Logic for a Round ---
FUNCTION DecideAction(current_round: integer, history: List of PlayerActions):
  // history is a list of actions for all players in previous rounds
  // Each element in history might be a list of actions for that round

  // --- Handle Last Round ---
  IF current_round == r THEN
    RETURN 'D' // Always defect in the last round
  END IF

  // --- Handle First Round ---
  IF current_round == 1 THEN
    // Use initial cooperation_willingness from InitializeStrategy
    // For Round 1, we don't have a history to update from.
    // The decision is solely based on the initial assessment.
    IF RandomFloat() < cooperation_willingness THEN
      current_action = 'C'
    ELSE
      current_action = 'D'
    END IF
    previous_action = current_action // Store for next round
    RETURN current_action
  END IF

  // --- Handle Subsequent Rounds (t > 1) ---

  // Get actions from the previous round
  previous_round_actions = history[current_round - 1] // Assuming history is 0-indexed for rounds

  // Count cooperators in the previous round
  cooperators_prev_round = 0
  FOR each action in previous_round_actions:
    IF action == 'C' THEN
      cooperators_prev_round = cooperators_prev_round + 1
    END IF
  END FOR

  // Calculate cooperation percentage in the previous round
  cooperation_percentage_prev_round = cooperators_prev_round / n
  target_cooperation_percentage = m / n

  // Update cooperation_willingness based on previous round's outcome
  IF cooperation_percentage_prev_round >= target_cooperation_percentage THEN
    // Group succeeded, increase willingness (but don't exceed 1.0)
    cooperation_willingness = Min(1.0, cooperation_willingness + 0.1)
  ELSE
    // Group failed, decrease willingness more sharply (but don't go below 0.0)
    cooperation_willingness = Max(0.0, cooperation_willingness - 0.2)
  END IF

  // Make the decision for the current round based on updated willingness
  IF RandomFloat() < cooperation_willingness THEN
    current_action = 'C'
  ELSE
    current_action = 'D'
  END IF

  previous_action = current_action // Store for next round
  RETURN current_action

END FUNCTION

// Helper function (assuming it exists)
FUNCTION RandomFloat(): // Returns a random float between 0.0 and 1.0
  // Implementation depends on the programming environment
END FUNCTION
```
'''

description_COOPERATIVE_52 = '''
This is a fascinating Collective Risk Dilemma! Designing a cooperative strategy without explicit communication or coordination mechanisms is a core challenge in game theory. My approach focuses on building trust and incentivizing cooperation through adaptive responses to observed behavior, while always aiming for the mutually beneficial outcome of meeting the cooperation threshold.

Here's the design for a cooperative strategy:

## Cooperative Strategy: "Conditional Reciprocity with Trust Building"

This strategy is designed to be cooperative at its core but also resilient to defection. It aims to foster cooperation by rewarding it and punishing defection, but with a focus on *rebuilding* trust after perceived betrayals.

### Core Philosophy:

*   **Default to Cooperation:** In the absence of strong negative signals, the strategy will lean towards cooperation to try and achieve the collective reward.
*   **Reciprocity with Forgiveness:** If others defect, the strategy will retaliate. However, it will also offer opportunities to return to cooperation.
*   **Proactive Trust Building:** The strategy will subtly signal its cooperative intent in early rounds to encourage others to do the same.
*   **Forward-Looking:** While learning from history, the strategy aims to optimize for the long-term collective payoff.

### 1. Decision Rules:

My strategy's decision for player *i* in round *t* (let's call this `decision(i, t)`) will be based on a few key indicators:

*   **`cooperation_threshold_met_last_round`**: A boolean flag indicating if the minimum number of cooperators (*m*) was reached in the previous round (*t-1*).
*   **`observed_cooperation_rate_last_round`**: The proportion of players who cooperated in the previous round (*t-1*).
*   **`my_cooperation_count`**: The number of times player *i* has cooperated so far.
*   **`round_number`**: The current round *t*.

**Decision Logic:**

1.  **Initial Phase (Early Rounds):**
    *   **For rounds `t = 1` to `min(r, some_initial_trust_building_period)`:**
        *   **`decision(i, t) = C`** (Cooperate)

    *   *Rationale:* In the early stages, the primary goal is to signal cooperative intent and try to establish a norm of cooperation. This is a gamble, but without early cooperation, the collective reward is never achieved. The `some_initial_trust_building_period` can be a small number like 2 or 3, or even a fraction of `r` (e.g., `max(2, floor(r/10))`).

2.  **Established Phase (Mid-Rounds):**
    *   **If `cooperation_threshold_met_last_round` is TRUE:**
        *   **`decision(i, t) = C`** (Cooperate)

    *   *Rationale:* If the threshold was met, it means collective cooperation was successful. The most rational strategy is to continue cooperating to maintain this beneficial state. This reinforces positive behavior.

    *   **If `cooperation_threshold_met_last_round` is FALSE:**
        *   **Analyze `observed_cooperation_rate_last_round` and `my_cooperation_count`:**
            *   **If `observed_cooperation_rate_last_round < m/n` (significantly below the threshold):**
                *   **`decision(i, t) = D`** (Defect)

            *   *Rationale:* If the threshold was missed and the overall cooperation was low, it suggests a lack of collective will. Continuing to cooperate alone when others defect yields a payoff of 0 (from my contribution) + 1 (private endowment) = 1, which is the same as defecting when others defect. However, defecting now is a signal that the current path is not working.

            *   **If `observed_cooperation_rate_last_round >= m/n` but `< m/n` AND `my_cooperation_count` is LOW (e.g., less than `round_number / 2`):**
                *   **`decision(i, t) = D`** (Defect)

            *   *Rationale:* This is a "punishment" phase. If the threshold was missed, but there was still some level of cooperation, but *I* haven't cooperated much myself (indicating I'm not leading the charge in a failing effort), then defecting is a way to signal that the current level of cooperation is insufficient and perhaps to try and "reset" expectations. This is a more nuanced punishment where we are still *close* to the threshold.

            *   **If `observed_cooperation_rate_last_round >= m/n` but `< m/n` AND `my_cooperation_count` is HIGH (e.g., more than `round_number / 2`):**
                *   **`decision(i, t) = C`** (Cooperate)

            *   *Rationale:* This is the "forgiveness" or "persistence" phase. If the threshold was missed, but *I* have been a consistent cooperator, I will continue to cooperate. The hope is that my sustained cooperation will encourage others to meet the threshold in the future. This is a high-risk, high-reward strategy that prioritizes long-term cooperative norms.

3.  **Endgame Phase (Last Round):**
    *   **For the last round `t = r`:**
        *   **If `cooperation_threshold_met_last_round` is TRUE (or if a reasonable number of players have signaled cooperation in this round *t-1* which means `m` could be met):**
            *   **`decision(i, t) = C`** (Cooperate)
        *   **Else:**
            *   **`decision(i, t) = D`** (Defect)

    *   *Rationale:* In the final round, the goal is to maximize the immediate payoff. If cooperation has been consistently successful, it's best to ensure it one last time. If it hasn't been successful, defecting is the safest option to guarantee the private endowment. The "reasonable number of players signaled cooperation" is a heuristic to try and predict if cooperating might push us over the edge in the final round, even if the *last* round didn't meet it.

**Summary of Decision Logic:**

*   **Early Rounds:** Always Cooperate (Build Trust)
*   **Successful Past Round:** Always Cooperate (Maintain Success)
*   **Unsuccessful Past Round:**
    *   If **Low** overall cooperation: Defect (Punish/Signal Failure)
    *   If **Moderate** overall cooperation AND **I've cooperated a lot**: Cooperate (Persistence/Forgiveness)
    *   If **Moderate** overall cooperation AND **I've cooperated little**: Defect (Punish/Signal Failure)
*   **Last Round:** Cooperate if success seems likely, otherwise Defect.

### 2. Handling Edge Cases:

*   **First Round (`t=1`):** As described above, always cooperate. This is the "trust building" phase. We don't have any history to learn from, so we initiate cooperation.

*   **Last Round (`t=r`):** The decision is to cooperate only if there's a high likelihood of meeting the threshold (i.e., if `cooperation_threshold_met_last_round` was true, or if the average cooperation in the *previous* round was very high). Otherwise, defect to secure the private endowment.

*   **Parameters `m` and `n`:**
    *   If `m` is very close to `n` (e.g., `m = n-1` or `m=n`), cooperation becomes very difficult. My strategy will still try to cooperate, but will likely transition to defection sooner if the threshold is consistently missed.
    *   If `m` is close to 2 (e.g., `m = 2`), cooperation is easier to achieve. My strategy will be more inclined to cooperate and less likely to defect.
    *   If `n` is very small (e.g., `n=2` or `n=3`), the dynamics are more volatile. The strategy will adapt, but the absence of many players means each individual action has a larger impact.

*   **Lack of Information (First Round):** The strategy handles this by defaulting to cooperation.

### 3. Be Cooperative:

The "Conditional Reciprocity with Trust Building" strategy is inherently cooperative:

*   **Default Cooperative Stance:** It begins by cooperating, attempting to initiate a positive feedback loop.
*   **Reward Cooperation:** It continues to cooperate when the collective has successfully achieved the threshold, directly rewarding successful cooperation.
*   **Forgiving Defection (to an extent):** While it punishes repeated failures or widespread defections by defecting, it maintains a pathway back to cooperation. Specifically, if the player has been a persistent cooperator themselves, they will continue to cooperate even if the threshold was missed, hoping to eventually sway the group. This demonstrates a commitment to the cooperative ideal.
*   **Focus on Collective Outcome:** The entire strategy is designed to maximize the chances of reaching the `m` cooperator threshold, which benefits all players. Even in phases of defection, the goal is to nudge the game back towards a more cooperative and beneficial state.

### Pseudocode:

```python
# Assume these are global or passed parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward factor
initial_trust_building_period = max(2, floor(r / 10)) # e.g., first 10% of rounds or at least 2

# State variables maintained per player (or globally if tracking others' actions)
my_cooperation_count = 0
last_round_cooperation_met = False
last_round_observed_cooperation_rate = 0.0 # Proportion of players who cooperated

def decide_action(current_round, all_previous_actions):
    """
    Decides whether to Cooperate (C) or Defect (D) for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        all_previous_actions (list of lists): List of actions taken by all players
                                             in previous rounds.
                                             e.g., [[p1_r1_act, p2_r1_act, ...],
                                                    [p1_r2_act, p2_r2_act, ...], ...]

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """
    global my_cooperation_count, last_round_cooperation_met, last_round_observed_cooperation_rate

    # --- Update state based on previous round's outcome ---
    if current_round > 1:
        previous_round_actions = all_previous_actions[-1]
        num_cooperators_last_round = previous_round_actions.count('C')
        last_round_cooperation_met = (num_cooperators_last_round >= m)
        last_round_observed_cooperation_rate = num_cooperators_last_round / n

    # --- Decision Logic ---

    # 1. Initial Trust Building Phase
    if current_round <= initial_trust_building_period:
        action = 'C'

    # 2. Endgame Phase (Last Round)
    elif current_round == r:
        if last_round_cooperation_met or last_round_observed_cooperation_rate > (m-1)/n: # Heuristic for 'likely to succeed'
            action = 'C'
        else:
            action = 'D'

    # 3. Established Phase (Mid-Rounds)
    else:
        if last_round_cooperation_met:
            action = 'C' # Continue successful cooperation
        else:
            # Threshold was not met in the last round
            if last_round_observed_cooperation_rate < m / n:
                action = 'D' # Low overall cooperation, punish by defecting
            else:
                # Moderate cooperation but threshold missed
                # Check if I've been a consistent cooperator
                my_cooperation_proportion = my_cooperation_count / (current_round - 1) if (current_round - 1) > 0 else 1.0
                if my_cooperation_proportion > 0.5: # Threshold for "high" personal cooperation
                    action = 'C' # Persistent cooperator, give it another chance
                else:
                    action = 'D' # Not a strong cooperator, defect to signal

    # --- Update my_cooperation_count after deciding action ---
    if action == 'C':
        my_cooperation_count += 1

    return action

# --- To be used within the tournament simulation ---
# For each round:
#   Get 'all_previous_actions' from the game state
#   my_chosen_action = decide_action(current_round, all_previous_actions)
#   Submit my_chosen_action to the game
```

This strategy aims to strike a balance between promoting cooperation and protecting against exploitation. Its adaptive nature allows it to learn from the collective behavior of the other players and adjust its own actions accordingly, while its explicit trust-building and forgiveness mechanisms are geared towards achieving the mutually beneficial cooperative outcome.
'''

description_COOPERATIVE_53 = '''
This is an interesting variation of a public goods game with a critical threshold. The core challenge is to incentivize cooperation when individual defection is tempting, especially when the number of defectors is high. My designed cooperative strategy aims to balance risk aversion with the potential for significant rewards, while remaining adaptive to observed player behavior.

## Cooperative Strategy: The Threshold Guardian

This strategy is designed to be a *proactive guardian* of the cooperative threshold, aiming to trigger the collective reward when possible and to punish deviations from cooperation when necessary, all while maintaining a strong inclination towards cooperation.

### 1. Decision Rules

The core of this strategy revolves around maintaining a dynamic count of observed cooperators and defectors, and making decisions based on proximity to the cooperation threshold ($m$).

**Key Variables:**

*   `observed_cooperators_last_round`: The number of players observed to have cooperated in the previous round.
*   `my_contribution_last_round`: My action (C or D) in the previous round.
*   `cooperation_opportunity`: A boolean flag indicating whether the previous round's outcome was a successful cooperation (i.e., $m$ or more cooperators).

**Decision Logic per Round (t > 1):**

1.  **Initial Cooperative Stance:** My default action is **Cooperate (C)**. I will only deviate from this default if the observed behavior strongly suggests that cooperation is being exploited or is unlikely to succeed.

2.  **Assessing the Previous Round:**
    *   **If `cooperation_opportunity` was True (previous round met the threshold):**
        *   **If `observed_cooperators_last_round` >= m:** My action remains **Cooperate (C)**. This reinforces successful cooperation and signals continued commitment.
        *   **If `observed_cooperators_last_round` < m (but the threshold was met due to my cooperation):** This scenario is subtle. It means I cooperated, and *just enough* others cooperated to meet the threshold. My action remains **Cooperate (C)** to maintain this positive momentum.
    *   **If `cooperation_opportunity` was False (previous round did not meet the threshold):**
        *   **If `observed_cooperators_last_round` >= m - 1:** This is a critical juncture. We are very close to the threshold. My action is **Cooperate (C)**. I am willing to take a small risk to push us over the edge, given how close we are.
        *   **If `observed_cooperators_last_round` < m - 1:** The observed level of cooperation is too low to justify my continued cooperation. My action is **Defect (D)**. This is a punitive measure, aiming to signal that unreciprocated cooperation is unsustainable and to conserve my endowment.

3.  **Updating State Variables:** After deciding on my action for the current round, I update the state variables for the *next* round. This is crucial for the adaptive nature of the strategy.

    *   If I played **C** this round: Increment `observed_cooperators_last_round` for the next round.
    *   If I played **D** this round: Do not increment `observed_cooperators_last_round` for the next round.
    *   `my_contribution_last_round` is updated to my current action.
    *   `cooperation_opportunity` is set based on whether $m$ or more players (including myself) cooperated *in the current round*.

### 2. Handling Edge Cases

*   **First Round (t = 1):**
    *   Since there is no history, and to embody the "cooperative mindset," my action is **Cooperate (C)**. This is a strong signal of good faith and an attempt to initiate cooperation. I am essentially betting on the possibility of others doing the same.
    *   Initialize `observed_cooperators_last_round` to 0, `my_contribution_last_round` to None, and `cooperation_opportunity` to False.

*   **Last Round (t = r):**
    *   The logic for the last round is the same as any other round. The repeated nature of the game and the "no communication" rule means that there's no explicit endgame strategy that differs from the ongoing adaptive approach. My goal remains to achieve the highest possible individual payoff, and in this game, that often involves collective reward. I will continue to apply the threshold-based decision rules.

*   **High Values of `m` (close to `n`):**
    *   My strategy inherently becomes more cautious. If `m` is very close to `n`, the condition `observed_cooperators_last_round >= m - 1` will be harder to meet, leading to more defections when cooperation is not guaranteed.

*   **Low Values of `m` (close to 2):**
    *   My strategy will be more prone to cooperating. The `observed_cooperators_last_round >= m - 1` condition will be easier to satisfy, making me more likely to defect only when a very low number of cooperators are observed.

### 3. Being Cooperative

This strategy is fundamentally cooperative in its design:

*   **Default to Cooperation:** The default action is always to cooperate. Defection is a reactive measure, triggered only when the observed environment suggests that cooperation is unlikely to yield the collective reward or is being exploited.
*   **Risk Taking for Collective Gain:** I am willing to cooperate even if the number of observed cooperators is one less than the threshold (`m-1`). This demonstrates a willingness to take a small personal risk to achieve the collective benefit, especially in the early and middle stages of the game.
*   **Reinforcing Success:** When the threshold is met, I continue to cooperate, reinforcing positive outcomes and building trust (or at least predictable behavior) among players who also cooperated.
*   **Adaptive Punishment:** While I aim to be cooperative, the strategy incorporates a form of adaptive punishment. If cooperation levels drop significantly, I will defect. This isn't malicious, but a rational response to a lack of sufficient collective action, which would otherwise lead to a lower individual payoff (1 instead of 1+k). This punishment is temporary; if others start cooperating again, I will revert to cooperation.

### Pseudocode

```pseudocode
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// State Variables (maintained across rounds)
round_t = 1
observed_cooperators_last_round = 0
my_contribution_last_round = None
cooperation_opportunity_last_round = False

function DecideAction(current_round, observed_actions_from_previous_round):
    // observed_actions_from_previous_round is a list of actions (C/D) for all players in the previous round

    // --- Update State Variables based on Previous Round ---
    if current_round > 1:
        // Count cooperators from the observed actions of the last round
        current_observed_cooperators = count(action == 'C' for action in observed_actions_from_previous_round)
        observed_cooperators_last_round = current_observed_cooperators
        my_contribution_last_round = observed_actions_from_previous_round[my_player_index] // Assuming my_player_index is known

        // Determine if the cooperation threshold was met in the last round
        if current_observed_cooperators >= m:
            cooperation_opportunity_last_round = True
        else:
            cooperation_opportunity_last_round = False
    else: // First Round
        observed_cooperators_last_round = 0
        my_contribution_last_round = None
        cooperation_opportunity_last_round = False

    // --- Decision Logic ---
    my_action = 'C' // Default to Cooperate

    if current_round > 1: // If not the first round
        if cooperation_opportunity_last_round == True:
            // Threshold was met in the last round, continue cooperating
            my_action = 'C'
        else: // Threshold was NOT met in the last round
            if observed_cooperators_last_round >= m - 1:
                // Very close to the threshold, take a small risk to cooperate
                my_action = 'C'
            else:
                // Cooperation level is too low, defect to conserve resources
                my_action = 'D'
    else: // First Round (t=1)
        my_action = 'C' // Always cooperate in the first round

    // --- Return Action ---
    return my_action

// --- For the next round, the system will need to update observed_cooperators_last_round and cooperation_opportunity_last_round based on the ACTUAL outcome of THIS round ---
// Example of how state variables would be updated *after* action is taken and outcome is known:
// If my_action == 'C':
//     future_observed_cooperators_count += 1
// If the total actual cooperators in this round >= m:
//     future_cooperation_opportunity = True
```

This "Threshold Guardian" strategy aims to be a reliable, cooperative player that actively seeks the collective reward while being resilient to exploitation by defectors. Its adaptiveness comes from learning from the immediate past, specifically the observed number of cooperators relative to the critical threshold $m$.
'''

description_COOPERATIVE_54 = '''
Okay, this is a fascinating challenge. The Collective Risk Dilemma is a classic example of social dilemmas where individual rationality can lead to collectively suboptimal outcomes. The key challenge here is incentivizing cooperation when defection can be individually beneficial, especially if others cooperate.

My cooperative strategy will be based on a **conditional cooperation approach with a strong emphasis on historical feedback and adaptation.** It aims to signal a willingness to cooperate while punishing defection and rewarding successful cooperation. The core idea is to maintain cooperation as long as the group is collectively achieving the cooperative outcome, and to retaliate against defectors when the threshold is not met.

Here's the breakdown of the strategy:

## Cooperative Strategy: "Reciprocity with Threshold Enforcement"

This strategy is named to reflect its core principles: reciprocating cooperative behavior and actively enforcing the cooperation threshold.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round `t` for player `i` will be determined by the following logic:

*   **Initial Condition (First Round):** In the very first round (`t=1`), the strategy will **Cooperate (C)**. This is to signal a default willingness to cooperate and to attempt to kickstart a cooperative equilibrium.

*   **Subsequent Rounds (t > 1):**
    *   **If the cooperation threshold `m` was met or exceeded in the previous round (`t-1`):**
        *   **Check the proportion of "persistent cooperators":** Let `P_C(t-1)` be the number of players who played 'C' in round `t-1` and have consistently played 'C' for a defined lookback period (e.g., the last `L` rounds, where `L` is a parameter, or simply from the beginning of the game if `t-1` is small).
        *   **If `P_C(t-1)` is sufficiently high (e.g., `P_C(t-1) >= m` or a slightly higher threshold like `m + delta`, where `delta` is a small positive integer to ensure stability):** The strategy will **Cooperate (C)**. This indicates that past cooperation has been successful and sustained.
        *   **If `P_C(t-1)` is below the threshold but the overall threshold `m` was met in `t-1`:** The strategy will **Cooperate (C)**. This still acknowledges past group success and hopes for continued group effort.
        *   **If the overall cooperation threshold `m` was *not* met in the previous round (`t-1`) *and* the proportion of persistent cooperators (`P_C(t-1)`) is *also* below `m` (or `m+delta`):** The strategy will **Defect (D)**. This acts as a punishment for collective failure to meet the threshold.

    *   **If the cooperation threshold `m` was *not* met in the previous round (`t-1`):**
        *   **Assess the "cooperation deficit":** Calculate the number of players who played 'D' in round `t-1`. Let this be `N_D(t-1)`.
        *   **If `N_D(t-1)` is significant (e.g., `N_D(t-1) >= (n - m) * tolerance_factor`, where `tolerance_factor` is a value between 0 and 1, e.g., 0.5):** The strategy will **Defect (D)**. This is a strong signal of retaliation against widespread defection that prevented the threshold from being met.
        *   **If `N_D(t-1)` is not significant (i.e., only a few players defected, but still not enough to meet `m`):** The strategy will **Cooperate (C)**. This allows for a small margin of error or a few isolated defections without immediately abandoning cooperation. The goal is to still try and pull the group towards cooperation.

    *   **Consider the "last round" effect:** In the absolute last round (`t=r`), the incentive to cooperate diminishes as there's no future to build on. However, to maintain a cooperative stance throughout, the strategy will generally follow the above rules. *However, to be maximally robust against purely selfish strategies that would defect in the last round, we might introduce a slight bias towards defection in the very last round if the group's historical cooperation has been shaky.* For this specific strategy, we will stick to the core logic to maintain its cooperative identity.

**In simpler terms:**

1.  **Start by cooperating.**
2.  **If the group successfully cooperated last round:**
    *   If most people (including yourself) have been cooperating consistently, keep cooperating.
    *   If the group met the threshold, but there were some defectors, still cooperate to encourage them.
3.  **If the group *failed* to cooperate last round:**
    *   If *many* people defected, defect back as punishment.
    *   If only a *few* people defected, and the threshold was just missed, try cooperating one more time to see if the group can pull together.

### 2. Handling Edge Cases:

*   **First Round (t=1):** As stated, the strategy will always **Cooperate (C)**. This is a fundamental commitment to giving cooperation a chance.

*   **Last Round (t=r):** The strategy will follow the general decision rules as described above. While the absence of future rounds can incentivize defection, maintaining a consistent cooperative stance throughout the game is a key aspect of this strategy's identity and its ability to foster long-term cooperation in a repeated game context (even if this specific game is finite).

*   **Insufficient History:** For early rounds where there isn't enough history to assess "persistent cooperators" or a clear trend of defection, the strategy will default to the simpler rules:
    *   Round 1: Cooperate.
    *   Round 2: If Round 1 was C, Cooperate. If Round 1 was D (unlikely for this strategy), Defect. (This is a simplified version; the core strategy would still look for *any* prior cooperation).

*   **Game Parameters (n, m, k):** The strategy implicitly uses `m` to determine the cooperation threshold and `n` to understand the group size. The factor `k` is not directly used in decision-making but is the underlying incentive that the strategy is trying to achieve. A higher `k` makes cooperation more rewarding and thus this strategy will perform better.

### 3. Being Cooperative:

*   **Default to Cooperation:** The strategy's first move and its inclination to cooperate when the group narrowly misses the threshold clearly signal a cooperative intent.
*   **Conditional Cooperation, Not Blind Trust:** It's not naive cooperation. It retaliates against consistent defection that harms the collective. This is a form of "grim trigger" or "tit-for-tat" but adapted to the collective nature of the game.
*   **Focus on Group Success:** The primary goal is to achieve the state where `m` or more players cooperate, because this leads to the highest collective payoff. The strategy prioritizes reaching this state.
*   **Adaptive Learning:** By observing past actions, the strategy adapts its behavior, learning from the group's dynamics and adjusting its stance accordingly.

---

### Pseudocode Illustration:

```python
# Assume 'game_history' is a list of lists, where game_history[t-1]
# contains a list of actions (C or D) for each player in round t-1.
# 'my_player_index' is the index of this player.
# n, m, k are game parameters.

def decide_action(round_number, game_history, n, m, k, my_player_index):
    # Initialize parameters for lookback and thresholds
    lookback_period = 5  # Example: look at the last 5 rounds for persistent cooperators
    cooperation_buffer = 1 # Example: How many defections are tolerated before defecting back if threshold missed

    if round_number == 1:
        return 'C' # Always cooperate in the first round

    # Get actions from the previous round
    previous_round_actions = game_history[round_number - 2] # game_history is 0-indexed

    # Count cooperators in the previous round
    cooperators_last_round = previous_round_actions.count('C')

    # --- Decision Logic ---

    if cooperators_last_round >= m:
        # Threshold was met in the previous round - Good!
        # Assess persistence of cooperation among those who cooperated last round
        persistent_cooperators_count = 0
        recent_history = game_history[max(0, round_number - 1 - lookback_period) : round_number - 1]

        for player_actions_in_round in zip(*recent_history): # Transpose history for player-wise view
            player_index = player_actions_in_round[0] # Assuming player_index is implicitly handled by order
            is_persistent_cooperator = True
            for action in player_actions_in_round:
                if action == 'D':
                    is_persistent_cooperator = False
                    break
            if is_persistent_cooperator and previous_round_actions[player_index] == 'C': # Only count if they cooperated last round too
                persistent_cooperators_count += 1

        if persistent_cooperators_count >= m: # Or a slightly higher threshold for stability
            return 'C' # Group is cooperating well and persistently
        else:
            return 'C' # Group met threshold, even with some fluctuation, still try to cooperate

    else:
        # Threshold was NOT met in the previous round - Problem!
        defectors_last_round = n - cooperators_last_round

        # If too many defected, retaliate
        # Check if defection rate is high enough to warrant retaliation
        # A simple check: if the number of defectors is more than 'm-1' and we are far from 'm'
        if defectors_last_round > (n - m) + cooperation_buffer: # If number of defectors is significantly high
             return 'D' # Punish defection
        else:
             # Threshold was missed, but not by a huge margin.
             # Try cooperating one last time, hoping to pull the group together.
             return 'C'

# Note: The pseudocode for 'persistent_cooperators_count' needs refinement
# to correctly track individual players across rounds in a real implementation.
# The core idea is to see if players who cooperated last round *also* cooperated
# for a significant portion of the lookback period.
```

This strategy aims to be a robust and adaptive cooperative player. It prioritizes collective success while being prepared to retaliate against widespread defection. Its strength lies in its clear signaling of cooperation and its intelligent response to the group's collective actions.
'''

description_COOPERATIVE_55 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust. It prioritizes cooperation while mitigating the risk of exploitation.

## Cooperative Strategy: "Adaptive Threshold Cooperation"

This strategy aims to achieve the collective reward by cooperating, but it dynamically adjusts its behavior based on the observed success of cooperation in previous rounds. It's designed to be "cooperative" in spirit by trying to reach the cooperative outcome whenever it seems feasible, while being "robust" by punishing defection and incentivizing future cooperation.

### Core Principles:

1.  **Default to Cooperation:** In the absence of strong evidence to the contrary, the strategy will attempt to cooperate. This is because cooperation is the path to the highest collective payoff.
2.  **Learn from History:** The strategy will monitor the number of cooperators in each round. This historical data will inform future decisions.
3.  **Punish Defection (with Restraint):** If a round results in failure to meet the cooperation threshold due to insufficient cooperators (even if the strategy itself cooperated), the strategy will temporarily defect to signal its dissatisfaction and protect its own payoff. This is a tit-for-tat-like mechanism applied to the collective outcome.
4.  **Reward Success:** If the cooperation threshold is met, the strategy will continue to cooperate in the next round, reinforcing successful collective action.
5.  **Strategic Defection in Later Rounds:** To discourage a "free-rider" mentality from opponents and to maximize its own total payoff, the strategy will employ a more cautious approach in later rounds.

### Decision Rules:

The strategy's decision in round *t* depends on its own action in round *t-1* and the observed outcome of round *t-1*.

**1. Initial State & First Round (t=1):**

*   **Decision:** Cooperate (C).
    *   **Rationale:** In the very first round, there's no historical data. The most logical and cooperative approach is to signal a willingness to cooperate and attempt to reach the threshold from the outset. This sets a positive tone.

**2. Subsequent Rounds (t > 1):**

Let $N_C(t-1)$ be the number of players who cooperated in round $t-1$.
Let $A_i(t-1)$ be the action of player *i* (our strategy) in round $t-1$.

*   **Condition 1: Threshold Met in Previous Round ($N_C(t-1) \ge m$)**
    *   **Decision:** Cooperate (C).
    *   **Rationale:** The collective effort was successful. This reinforces cooperation and signals that this is a stable, beneficial state. Continuing to cooperate maintains this positive momentum.

*   **Condition 2: Threshold Not Met in Previous Round ($N_C(t-1) < m$) AND Our Strategy Cooperated in Previous Round ($A_i(t-1) = C$)**
    *   **Decision:** Defect (D).
    *   **Rationale:** The collective effort failed, and our strategy contributed to the cooperative effort (by playing C). This implies that there weren't enough *other* players cooperating. To signal dissatisfaction with this outcome and to protect our own endowment against future rounds where others might exploit our continued cooperation, we will defect. This is a mild punishment for the lack of collective success.

*   **Condition 3: Threshold Not Met in Previous Round ($N_C(t-1) < m$) AND Our Strategy Defected in Previous Round ($A_i(t-1) = D$)**
    *   **Decision:** Defect (D).
    *   **Rationale:** The collective effort failed, and our strategy already defected in the previous round. This means our prior defection did not lead to sufficient cooperation from others. Continuing to defect maintains a cautious stance and signals that we are not currently seeing a path to successful cooperation.

*   **Condition 4: Special Handling for the Last Round (t = r)**
    *   **Decision:** Defect (D).
    *   **Rationale:** In the final round, there are no future repercussions for defection. The incentive to achieve the collective reward is gone. To maximize individual payoff in this final round, it is strategically optimal to defect, as the reward *k* will no longer be relevant for future rounds.

### Adaptive Threshold Cooperation (Pseudocode):

```
# Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward if threshold met factor

# Internal State Variables
player_action_history = [] # List of own actions (C or D)
cooperator_count_history = [] # List of observed cooperators in each round

# Strategy Function for Round t (1-indexed)
function decide_action(t, current_round_cooperator_count):
    # Store observed information
    if t > 1:
        cooperator_count_history.append(current_round_cooperator_count)

    # Rule for Last Round
    if t == r:
        return D # Always defect in the last round

    # Rule for First Round
    if t == 1:
        player_action_history.append(C)
        return C

    # Rules for Subsequent Rounds (t > 1)
    previous_round_action = player_action_history[-1] # Our action in t-1
    previous_round_cooperators = cooperator_count_history[-1] # Observed cooperators in t-1

    if previous_round_cooperators >= m:
        # Condition 1: Threshold Met in Previous Round
        current_action = C
    else: # previous_round_cooperators < m
        if previous_round_action == C:
            # Condition 2: Threshold Not Met, We Cooperated Previously
            current_action = D
        else: # previous_round_action == D
            # Condition 3: Threshold Not Met, We Defected Previously
            current_action = D

    player_action_history.append(current_action)
    return current_action
```

### Explanation of Robustness and Adaptability:

*   **Adaptive:** The strategy doesn't follow a fixed pattern. It continuously assesses the success of cooperation based on historical data. If cooperation is consistently failing, it becomes more cautious (defecting). If it's succeeding, it reinforces cooperation.
*   **Robust to Opponent Behavior:**
    *   **Aggressive Defectors:** If opponents consistently defect, the strategy will eventually stop cooperating after a failed round where it cooperated. It won't be exploited indefinitely.
    *   **Free-riders (cooperate only when others do):** If there are many free-riders who only cooperate when they see enough others doing so, our strategy's initial cooperation can sometimes trigger this. However, if their cooperation is insufficient, our strategy will then defect, potentially causing a cycle until they adjust.
    *   **Pure Cooperators:** If opponents always cooperate, our strategy will also always cooperate (except for the final round), leading to the highest possible payoff for everyone.
    *   **Mixed Strategies:** The strategy's learning mechanism allows it to adapt to a mix of behaviors over time.
*   **Cooperative Alignment:** The strategy *defaults* to cooperation and only deviates when cooperation has demonstrably failed and continued cooperation would be detrimental. The punishment phase is temporary and designed to incentivize a return to cooperation, rather than to permanently punish. The aim is always to achieve the beneficial collective outcome, not to win by exploiting others.
*   **No Communication:** The strategy is designed to operate solely on observable actions and game parameters, respecting the no-communication rule.

This strategy aims to be a strong contender in a tournament by balancing the desire for collective benefit with the need for self-preservation and a strategic response to the environment.
'''

description_COOPERATIVE_56 = '''
This is a fascinating challenge that lies at the intersection of game theory, behavioral economics, and adaptive systems. The core of the problem is to foster cooperation in a scenario where individual incentives can easily lead to defection, especially with the possibility of "free-riding" on others' cooperation.

My designed cooperative strategy, which I'll call the **Adaptive Reciprocity Strategy (ARS)**, aims to achieve high cumulative payoffs by balancing the desire for individual gain with the need for collective success, without explicit communication. It's built on the principle of conditional cooperation and a strong focus on maintaining momentum towards cooperation when it's beneficial.

## Adaptive Reciprocity Strategy (ARS)

The ARS is designed to be **proactive in initiating cooperation** when the conditions are favorable, and **retaliatory yet forgiving** when cooperation breaks down. It prioritizes reaching the cooperation threshold (`m`) and maximizing the shared reward (`k`), while minimizing the risk of being exploited.

### 1. Decision Rules

The core of the ARS's decision-making in any given round `t` (where `t > 0`) is based on the following:

*   **Primary Condition: The Immediate Past Payoff and Threshold Achievement:**
    *   **If, in the previous round (t-1), the cooperation threshold `m` was met or exceeded:**
        *   **Player i plays C.**
        *   **Rationale:** This is the core of reciprocity. If the group succeeded in achieving the collective reward, the ARS reinforces that success by continuing to cooperate. It signals a commitment to the cooperative equilibrium.

*   **Secondary Condition: The Immediate Past Payoff and Threshold Failure:**
    *   **If, in the previous round (t-1), the cooperation threshold `m` was NOT met, AND player i's own payoff in round t-1 was `1` (meaning they defected):**
        *   **Player i plays D.**
        *   **Rationale:** If cooperation failed and the ARS itself defected in the previous round, it continues to defect. This is a form of punishment for the group's failure to cooperate and avoids being exploited by others who might have defected.

    *   **If, in the previous round (t-1), the cooperation threshold `m` was NOT met, AND player i's own payoff in round t-1 was `(1 - c_i) + k` (meaning they cooperated but the threshold was still not met):**
        *   **Player i plays C.**
        *   **Rationale:** This is the crucial adaptive and cooperative element. Even though the group failed to meet the threshold in the previous round, if the ARS **sacrificed** by cooperating and still didn't reach the threshold, it will **try again**. This willingness to continue cooperating despite a past failure is key to eventually establishing a cooperative norm. It assumes that the failure was due to a temporary lapse or a small number of defectors, and that continued cooperation might inspire others.

*   **Tertiary Condition: The "Forgiveness" Mechanism (When threshold not met, and ARS defected):**
    *   **If, in the previous round (t-1), the cooperation threshold `m` was NOT met, AND player i's own payoff in round t-1 was `1` (meaning they defected), BUT the number of defectors in round t-1 was *less than* (n - m + 1):**
        *   **Player i plays C.**
        *   **Rationale:** This is a sophisticated forgiveness mechanism. If cooperation failed, and the ARS defected, it will check how close the group was to succeeding. If the number of *defecting* players in the previous round was very close to the number that would have prevented cooperation (i.e., `n - m` or fewer defectors), the ARS will tentatively shift back to cooperation. This recognizes that a small number of additional cooperators could have tipped the balance and signals a willingness to re-engage constructively. The `n - m + 1` threshold for defectors ensures that if a substantial majority defected, the ARS will remain in a defensive stance.

### 2. Handling Edge Cases

*   **First Round (t = 0): The "Pioneer Cooperation" Rule**
    *   **Player i plays C.**
    *   **Rationale:** In the absence of any history, the ARS will always initiate cooperation. This is a strategic choice to *seed* cooperation and signal good faith. It aims to encourage others to cooperate and potentially reach the threshold from the outset. While this carries the risk of being exploited in the first round, the long-term benefits of establishing cooperation often outweigh this initial risk in a repeated game.

*   **Last Round (t = r-1): The "Guaranteed Outcome" Rule**
    *   **Player i plays D.**
    *   **Rationale:** In the final round, there is no future to incentivize. Therefore, the ARS will defect to maximize its individual payoff. This is a standard prediction in repeated games when the end is known. This rule ensures that the ARS doesn't leave potential individual gains on the table when there are no future consequences for its actions.

### 3. Being Cooperative

The ARS embodies a cooperative mindset through several key aspects:

*   **Proactive Initiation:** It always starts by cooperating, setting a positive example and creating an opportunity for collective gain.
*   **Reciprocity:** It rewards successful cooperation by continuing to cooperate. This reinforces positive behavior within the group.
*   **Persistent Effort:** It's willing to try cooperating again even after a failure, especially if it personally made a sacrifice (cooperated when threshold wasn't met). This demonstrates resilience and a commitment to finding a cooperative equilibrium.
*   **Forgiveness with a Condition:** The conditional forgiveness mechanism allows for a return to cooperation even after a group failure, provided the failure wasn't too widespread. This prevents the game from getting stuck in a cycle of mutual defection too easily.
*   **Focus on the Threshold:** The entire strategy is oriented towards achieving `m` cooperators. This is the explicit goal that unlocks the shared benefit `k`, directly aligning with collective success.

### Pseudocode Representation

```pseudocode
// ARS Strategy for Player i

function ARS(current_round, previous_actions, previous_payoffs, game_parameters):
    n = game_parameters.n
    m = game_parameters.m
    k = game_parameters.k
    r = game_parameters.r

    // --- Handle Edge Cases ---
    if current_round == 0:
        return "C"  // Pioneer Cooperation

    if current_round == r - 1:
        return "D"  // Guaranteed Outcome in Last Round

    // --- Decision Rules based on Previous Round (t-1) ---
    last_round_actions = previous_actions[current_round - 1] // List of actions from all players in prev round
    last_round_payoff_i = previous_payoffs[current_round - 1] // Player i's payoff in prev round

    cooperators_last_round = count_cooperators(last_round_actions)
    defectors_last_round = n - cooperators_last_round

    // Primary Condition: Threshold Met in Previous Round
    if cooperators_last_round >= m:
        return "C" // Reciprocity: Continue cooperating if successful

    // Secondary Condition: Threshold Not Met
    else:
        // Case 1: Player i defected in the last round
        if last_round_actions[i] == "D":
            // Tertiary Condition: Forgiveness if close to threshold
            // If the number of defectors was small enough that adding 1 more cooperator would have met m
            if defectors_last_round < (n - m + 1):
                return "C" // Conditional Forgiveness: Try cooperating again if group was close
            else:
                return "D" // Stick to defection if group widely defected

        // Case 2: Player i cooperated in the last round (but threshold wasn't met)
        else: // last_round_actions[i] == "C"
            return "C" // Persistent Effort: Try cooperating again if I sacrificed

// Helper function (assumes previous_actions is a list of 'C' or 'D' strings for all players)
function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == "C":
            count = count + 1
    return count

// This strategy needs access to:
// - current_round: The index of the round being played (0 to r-1).
// - previous_actions: A list of lists, where previous_actions[t] is a list of actions
//                     chosen by all players in round t.
// - previous_payoffs: A list of lists, where previous_payoffs[t] is a list of payoffs
//                     received by each player in round t.
// - game_parameters: An object containing n, m, k, r.
```

**Robustness and Adaptability:**

*   **Against High Defection:** If most players consistently defect, the ARS will eventually fall into a cycle of defection (due to the "Guaranteed Outcome" in the last round and the logic for when `defectors_last_round >= n - m + 1`). However, it will not aggressively punish; it will simply cease to cooperate and take its baseline payoff of 1.
*   **Against Sporadic Defection:** If defections are occasional but cooperation generally prevails, the ARS will swiftly return to cooperation through its reciprocity and persistent effort rules.
*   **Against Exploitation:** The ARS is designed to avoid being a pure altruist. It defects when it defects and the group fails, and it has a clear end-game defection strategy.
*   **Against Coordinated Defection:** While no strategy can guarantee success against perfectly coordinated, malicious defection, the ARS's initial cooperation and conditional forgiveness aim to disrupt such patterns by creating opportunities for cooperation.

The ARS aims to be a strong contender in a tournament by embodying a balanced approach that is more likely to achieve high cumulative payoffs over a varied set of opponent strategies.
'''

description_COOPERATIVE_57 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma game, focusing on adaptability and robustness in a tournament setting.

## Cooperative Strategy: "Adaptive Risk-Averse Cooperation"

This strategy aims to maximize long-term individual payoff by cautiously pursuing cooperation when the collective benefit is likely and defecting to safeguard private gains when cooperation seems improbable or risky. It's designed to be inherently cooperative, prioritizing the collective good when the conditions are favorable, but also being realistic about the potential for defection from others.

### Core Principle:

The strategy's core principle is to **cooperate when the perceived probability of reaching the cooperation threshold (m) is high, and defect when it is low or uncertain.** This is driven by a desire to achieve the higher payoff (k) while avoiding the worst-case scenario of contributing without the reward.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` for player `i` depends on the observed actions in previous rounds, specifically the **current estimated cooperation rate** and the **proximity to the required threshold `m`**.

**Decision Logic:**

*   **If `t == 1` (First Round):**
    *   **Cooperate (C).**
    *   **Rationale:** In the absence of any history, the most cooperative and potentially rewarding action is to signal willingness to cooperate. This sets a positive tone and allows for immediate learning about other players' tendencies. It's a small individual risk for a potentially large collective gain.

*   **If `t > 1` (Subsequent Rounds):**
    *   **Calculate the number of players who cooperated in the previous round (`t-1`).** Let this be `cooperators_prev_round`.
    *   **Calculate the perceived cooperation rate in the previous round:** `cooperation_rate_prev_round = cooperators_prev_round / n`.
    *   **Calculate the number of players who are currently "cooperating" from player `i`'s perspective.** This is an important adaptive element. Player `i` will assume that any player who has cooperated in at least one previous round is *likely* to cooperate again, unless there's strong evidence to the contrary.
        *   Let `history_cooperators` be the set of players (excluding `i`) who have played 'C' in at least one round from `1` to `t-1`.
        *   Let `observed_cooperators_in_t_minus_1` be the actual count of players who played 'C' in round `t-1`.
        *   **Estimated number of future cooperators (excluding `i`):** `estimated_potential_cooperators = |history_cooperators|`. If `observed_cooperators_in_t_minus_1` is significantly lower than `|history_cooperators|` (e.g., dropped by more than `n - m` players), it indicates a decline in cooperation, and `estimated_potential_cooperators` might be adjusted downwards to reflect this.

    *   **Decision Thresholds:**
        *   **Cooperate (C):** If `estimated_potential_cooperators + 1 >= m` (meaning if player `i` also cooperates, the threshold `m` will be met or exceeded).
            *   **AND** If `cooperation_rate_prev_round >= (m - 1) / n` (i.e., the previous round was close to meeting the threshold, suggesting momentum).
            *   **AND** If `t < r` (not the last round).
        *   **Defect (D):** In all other cases. This includes:
            *   If `estimated_potential_cooperators < m - 1` (even if player `i` cooperates, the threshold won't be met).
            *   If `t == r` (last round).
            *   If `cooperation_rate_prev_round` was significantly low and the trend is downward.
            *   If `estimated_potential_cooperators` is high, but the previous round saw a sharp drop in cooperation, indicating unreliability.

**Simplified Pseudocode for Rounds `t > 1`:**

```
function DecideAction(player_i_history, current_round_t, n, m, k, observed_actions_prev_round):
    // player_i_history: a list of player i's past actions, and for each other player,
    //                   whether they have ever cooperated.
    // observed_actions_prev_round: a list of actions taken by all players in round t-1.

    cooperators_in_prev_round = count_cooperators(observed_actions_prev_round)

    // Estimate how many others are likely to cooperate
    // This is a key heuristic: assume players who have cooperated once are inclined to cooperate again.
    estimated_potential_cooperators = 0
    for each player_j in all_players (excluding player_i):
        if player_j has ever cooperated in rounds 1 to t-1:
            estimated_potential_cooperators += 1

    // Adjust estimate based on recent drop-off, if any
    // If the last round saw a significant number of defections, reduce the estimate.
    // Threshold for significant drop-off could be a parameter, or a percentage.
    // For simplicity here, we'll just use the raw count.
    if cooperators_in_prev_round < estimated_potential_cooperators and estimated_potential_cooperators > m:
        // This heuristic could be refined. For now, rely on history_cooperators as the primary estimate.
        pass // Consider more complex decay or adjustment logic if needed.

    // Calculate cooperation rate in the previous round
    cooperation_rate_prev_round = cooperators_in_prev_round / n

    // Decision logic
    if current_round_t < r:
        if (estimated_potential_cooperators + 1 >= m) and (cooperation_rate_prev_round >= (m - 1) / n):
            return C // Cooperate: Threshold likely to be met, and momentum exists.
        else:
            return D // Defect: Threshold unlikely to be met or momentum is lost.
    else: // Last round (t == r)
        return D // Defect in the last round to secure private gains.
```

### 2. Handling Edge Cases:

*   **First Round (`t = 1`):** As stated, always **Cooperate (C)**. This is a strategic leap of faith to establish a cooperative baseline and gather information.

*   **Last Round (`t = r`):** Always **Defect (D)**. In the absence of future interactions, there is no incentive to cooperate. The game has effectively become a one-shot prisoner's dilemma for this round, and the rational choice is to defect. This ensures player `i` doesn't get exploited in the final round.

*   **Near Threshold (`m-1` cooperators observed):** If in the previous round `t-1`, exactly `m-1` players cooperated, and player `i` has observed other players cooperating consistently in the past, player `i` will strongly lean towards cooperating in round `t`. The strategy explicitly checks for this scenario (`estimated_potential_cooperators + 1 >= m`) to trigger cooperation.

*   **Sudden Drop in Cooperation:** If a player observes a significant drop in the number of cooperators in the previous round (e.g., from `m` to `m-2`), even if their historical estimate of potential cooperators remains high, the strategy will be cautious. The condition `cooperation_rate_prev_round >= (m - 1) / n` acts as a dampener. If the previous round was far from the threshold, the strategy will likely defect, anticipating that others might be switching to defection.

### 3. Being Cooperative:

This strategy is fundamentally cooperative due to its core objectives:

*   **Initial Cooperation:** It starts by cooperating, signaling good faith and a willingness to build a cooperative equilibrium.
*   **Pursuit of Collective Good:** It only defects when the probability of achieving the collective benefit (`m` cooperators) is low or uncertain. This means it *actively seeks* to achieve the `k` reward whenever feasible.
*   **Long-Term Perspective:** By not defecting immediately out of fear, and by being willing to cooperate in the early rounds, it aims for sustained cooperation if others reciprocate. The cooperation rule `estimated_potential_cooperators + 1 >= m` is a direct embodiment of aiming for the collective good.

**Robustness:**

The strategy is robust because:

*   **Adaptive Learning:** It learns from observed actions and adjusts its future decisions. The `estimated_potential_cooperators` heuristic is key here.
*   **Risk Aversion (to a degree):** It balances the desire for the cooperative reward with the risk of losing its endowment. It defects when the risk of failed cooperation is high.
*   **Handles Defection:** It doesn't assume perpetual cooperation and defects in the last round to prevent exploitation. It also defects if the trend of cooperation is clearly negative.

This strategy is designed to adapt to varying player behaviors, from highly cooperative to purely selfish, by carefully assessing the conditions under which cooperation is likely to be successful and rewarding.
'''

description_COOPERATIVE_58 = '''
Here is a design for a cooperative strategy for the Collective Risk Dilemma, focusing on adaptability, robustness, and a clear cooperative intent.

## Cooperative Strategy: "Conditional Hope"

This strategy aims to foster cooperation by being a *conditional cooperator*. It attempts to identify and leverage opportunities for collective gain while minimizing the risk of being exploited by defectors. The core idea is to "hope" for cooperation by cooperating initially, and then adapt based on the collective behavior observed.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round is based on a dynamic assessment of the *likelihood of success* in achieving the cooperative threshold ($m$).

**Core Logic:**

*   **Early Rounds (Exploration/Trust Building):** In the initial rounds, the strategy prioritizes building trust and observing the environment. It will lean towards cooperation to signal good intentions and to test the waters for collective success.
*   **Mid-Rounds (Adaptive Assessment):** As more rounds pass, the strategy becomes more sophisticated. It assesses the historical tendency of cooperation and the *proportion of players who have been contributing to the cooperative outcome*.
*   **Late Rounds (Exploitation Avoidance/Last Chance Cooperation):** In the final rounds, the strategy balances the potential for a final cooperative payoff with the risk of being the only one cooperating and receiving a low payoff.

**Specific Decision Rules:**

Let $N_C(t-1)$ be the number of players who cooperated in the previous round $t-1$.
Let $N_{successful\_cooperators}(t-1)$ be the number of players who cooperated in round $t-1$ AND where the total number of cooperators in round $t-1$ was $\geq m$.

**Decision for Round $t$:**

1.  **Initial Cooperation (First few rounds, e.g., rounds 1 to $\min(r/3, 5)$):**
    *   **Play C.**
    *   *Rationale:* This phase is about establishing a cooperative baseline and gathering information without being overly susceptible to immediate exploitation. It signals a commitment to cooperation.

2.  **Adaptive Cooperation (Subsequent rounds, before the last few):**
    *   Calculate the **cooperation success rate**: $P_{success}(t-1) = \frac{N_{successful\_cooperators}(t-1)}{n}$.
    *   Calculate the **cooperation trend**: If $N_C(t-1) > N_C(t-2)$, cooperation is trending up. If $N_C(t-1) < N_C(t-2)$, it's trending down. If $N_C(t-1) = N_C(t-2)$, it's stable.
    *   **If $N_C(t-1) \geq m$ (previous round was successful or met the threshold):**
        *   **Play C.**
        *   *Rationale:* Past success is the strongest indicator for future success. Reward this observed success with continued cooperation.
    *   **If $N_C(t-1) < m$ (previous round failed to meet the threshold):**
        *   **If the trend in cooperation is upwards OR the current number of cooperators $N_C(t-1)$ is close to $m$ (e.g., $N_C(t-1) \geq m - 2$):**
            *   **Play C.**
            *   *Rationale:* There's still a good chance of reaching the threshold, or momentum is building. Continue to contribute to the collective good.
        *   **Else (trend is downwards or we are far from $m$):**
            *   **Play D.**
            *   *Rationale:* The cost of contributing to a failing cooperative effort outweighs the potential reward. Conserve resources.

3.  **Final Rounds (Last few rounds, e.g., last $\min(r/3, 5)$ rounds):**
    *   Let $R_{final}$ be the number of final rounds to consider.
    *   Calculate the **average cooperation success rate over the last $R_{final}$ rounds**: $\overline{P}_{success, last R_{final}}$.
    *   **If $\overline{P}_{success, last R_{final}} \geq 0.7$ (high historical success):**
        *   **Play C.**
        *   *Rationale:* Even in the end, if cooperation has been consistently successful, it's worth a final push for the shared reward.
    *   **Else (lower historical success):**
        *   **Play D.**
        *   *Rationale:* The risk of individual exploitation in the final round becomes too high if cooperation has not proven itself consistently. Secure the private endowment.

**Thresholds for "Few Rounds" and "Last Few Rounds":**
To make this adaptive and not rely on fixed numbers, we can use a fraction of the total rounds, with a minimum and maximum.
*   **Initial Phase Rounds:** $\min(r // 3, 5)$
*   **Final Phase Rounds:** $\min(r // 3, 5)$

### 2. Handling Edge Cases

*   **First Round (t=1):** As specified in rule 1, always **Play C**. This is purely exploratory and signals cooperative intent.
*   **Last Round (t=r):** The decision is governed by rule 3 (Final Rounds). The strategy will assess the overall historical success and make a final decision.
*   **Very Few Rounds (r is small, e.g., r=2 or r=3):**
    *   If $r \leq 3$: Treat all rounds as "Initial Cooperation" (Rule 1) or a blend of initial and final if $r=3$. For $r=2$, round 1 is C, round 2 is based on round 1's outcome (if $N_C(1) \geq m$ then C, else D). If $r=3$, round 1 is C, round 2 is adaptive, round 3 is final. This ensures enough rounds for adaptation without overcomplicating very short games. A simpler approach for very few rounds:
        *   Round 1: C
        *   Round 2 (and subsequent if $r \leq 4$): Based on round 1. If $N_C(1) \geq m$, then C. Else D. This is a very basic tit-for-tat on the *collective outcome*.
*   **$m$ close to $n$ or $m$ close to 1 (but $m > 1$):** The logic inherently handles this. If $m$ is high (close to $n$), cooperation is harder to achieve, and the strategy will likely transition to Defect earlier if it doesn't see clear signs of widespread cooperation. If $m$ is low (close to 2), cooperation is easier, and the strategy will be more inclined to cooperate.
*   **$k$ is very high:** A high $k$ incentivizes cooperation. The strategy's reliance on observed success will naturally lead it to cooperate more if high $k$ is facilitating it.
*   **$k$ is close to 1:** Cooperation is less rewarding. The strategy will be more cautious and might defect sooner if cooperation doesn't materialize quickly.

### 3. Being Cooperative

The strategy is fundamentally cooperative:

*   **Proactive Cooperation:** It starts by cooperating, demonstrating a willingness to engage in the collective good.
*   **Conditional Cooperation:** It *continues* to cooperate when cooperation is demonstrably successful and has a high probability of success. This is not blind altruism, but a pragmatic approach to maximizing joint welfare.
*   **Information Gathering:** It uses past rounds to learn and adapt, aiming to find the conditions under which cooperation can be sustained.
*   **Risk Mitigation:** While cooperative, it is not naive. It includes mechanisms to avoid being consistently exploited when cooperation is clearly failing. This self-preservation allows it to continue playing in subsequent rounds and potentially rejoin a cooperative effort if circumstances change.
*   **Focus on Threshold ($m$):** The entire decision-making process is centered around achieving the collective goal of $m$ cooperators. This directly aligns with the game's cooperative objective.

### Pseudocode (Illustrative)

```python
# Global state or context carried across rounds
history = [] # Stores (round_number, own_action, num_cooperators_in_round, was_successful)
own_action_history = [] # Stores own actions for previous rounds
cooperator_count_history = [] # Stores number of cooperators for previous rounds

def decide_action(current_round, n, m, k, r, own_actions_all_rounds, num_cooperators_all_rounds):
    """
    Decides action for the current round based on game parameters and history.

    Args:
        current_round (int): The current round number (1-indexed).
        n (int): Number of players.
        m (int): Minimum cooperators needed.
        k (float): Reward factor.
        r (int): Total number of rounds.
        own_actions_all_rounds (list): List of player's own actions in previous rounds.
        num_cooperators_all_rounds (list): List of total cooperators in previous rounds.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    num_prev_rounds = len(own_actions_all_rounds) # This is current_round - 1

    # --- Rule 1: Initial Cooperation ---
    initial_phase_end = min(r // 3, 5)
    if current_round <= initial_phase_end:
        return 'C'

    # --- Rule 3: Final Rounds ---
    final_phase_start = max(1, r - min(r // 3, 5) + 1)
    if current_round >= final_phase_start:
        # Analyze success rate over last few rounds
        success_count_last_few = 0
        rounds_to_analyze = min(num_prev_rounds, min(r // 3, 5)) # How many rounds we have available to analyze

        if rounds_to_analyze > 0:
            for i in range(num_prev_rounds - rounds_to_analyze, num_prev_rounds):
                if num_cooperators_all_rounds[i] >= m:
                    success_count_last_few += 1
            
            avg_success_rate_last_few = success_count_last_few / rounds_to_analyze

            if avg_success_rate_last_few >= 0.7:
                return 'C'
            else:
                return 'D'
        else: # Not enough history for final phase analysis yet (e.g., r=2 or 3) - fall through to adaptive
            pass

    # --- Rule 2: Adaptive Cooperation ---
    # Requires at least one previous round for N_C(t-1) and N_C(t-2)
    if num_prev_rounds >= 1:
        n_c_t_minus_1 = num_cooperators_all_rounds[-1]
        
        # Determine if previous round was successful
        was_successful_t_minus_1 = (n_c_t_minus_1 >= m)

        if was_successful_t_minus_1:
            return 'C'
        else: # Previous round failed to meet threshold
            # Check trend and proximity to m
            if num_prev_rounds >= 2:
                n_c_t_minus_2 = num_cooperators_all_rounds[-2]
                cooperation_trend_up = (n_c_t_minus_1 > n_c_t_minus_2)
            else: # Only one previous round, no trend yet
                cooperation_trend_up = False # Default to not trending up if no data

            proximity_to_m = (n_c_t_minus_1 >= m - 2) # Arbitrary threshold for "close"

            if cooperation_trend_up or proximity_to_m:
                return 'C'
            else:
                return 'D'
    
    # Fallback for very early rounds if initial phase logic had issues or for r=1 edge cases not perfectly handled above
    # (Should be covered by Rule 1, but as a safety)
    return 'C'

# --- Example Usage (Conceptual) ---
# This function would be called for each round.
# The 'history' would need to be managed externally and passed to the function.

# In a tournament loop:
# for round_num in range(1, r + 1):
#     # Get previous round data
#     prev_own_actions = own_actions_all_rounds[:round_num-1]
#     prev_num_cooperators = num_cooperators_all_rounds[:round_num-1]
#
#     my_action = decide_action(round_num, n, m, k, r, prev_own_actions, prev_num_cooperators)
#
#     # Record my_action and receive outcome (num_cooperators_this_round, was_successful_this_round)
#     # Update own_actions_all_rounds and num_cooperators_all_rounds for the next iteration.
```
'''

description_COOPERATIVE_59 = '''
This is a fascinating and challenging game! Designing a truly cooperative strategy in a setting with no communication and the potential for free-riding requires careful consideration. My strategy, "Adaptive Cooperation with Threshold Monitoring," aims to be robust and aligned with cooperative principles while acknowledging the inherent risks.

## Adaptive Cooperation with Threshold Monitoring Strategy

The core idea is to monitor the collective cooperation level in each round and adjust my behavior based on whether the threshold `m` is likely to be met. I'll aim for cooperation whenever it seems beneficial for the group and myself, but I will also protect myself from exploitation.

### 1. Decision Rules

My decision to Cooperate (C) or Defect (D) in round `t` will depend on the observed cooperation rate in the *previous* round (`t-1`) and my assessment of the likelihood of meeting the threshold `m` in the *current* round `t`.

**General Rule:**

*   **If I believe `m` or more players *will* cooperate in the current round `t`:** I will Cooperate (C).
*   **If I believe fewer than `m` players *will* cooperate in the current round `t`:** I will Defect (D).

**How to assess the likelihood of `m` players cooperating?**

This is where the "adaptive" part comes in. I'll use historical data (from rounds 1 to `t-1`) to inform my prediction for round `t`.

**Detailed Decision Logic:**

Let `N_C(t-1)` be the number of players who cooperated in round `t-1`.
Let `P_C(t-1) = N_C(t-1) / n` be the proportion of players who cooperated in round `t-1`.

*   **If `N_C(t-1) >= m` (Threshold met or exceeded in the previous round):**
    *   This suggests a generally cooperative environment.
    *   **My action in round `t`: Cooperate (C)**. I'll assume the momentum for cooperation will continue.

*   **If `N_C(t-1) < m` (Threshold not met in the previous round):**
    *   This indicates a lack of sufficient cooperation.
    *   **Now, I need to be more cautious and assess the situation:**
        *   **Case 1: `N_C(t-1)` is "close" to `m` (e.g., `N_C(t-1) >= m - buffer_size`) AND I am the `j`-th player to decide my action (where `j` is my player index in a hypothetical ordering, though actions are simultaneous).** This is complex as I don't know others' decisions *yet*. Instead, I'll rely on my *own* past behavior and the overall trend.
        *   **Revised Approach for `N_C(t-1) < m`:**
            *   **If `P_C(t-1)` is significantly high (e.g., `P_C(t-1) >= (m-1)/n` or some calculated probability threshold):** This means a decent portion cooperated, and if a few more join, we might hit `m`. I will lean towards **Cooperate (C)**, especially if I believe my cooperation is crucial to tipping the scales.
            *   **If `P_C(t-1)` is low:** This suggests widespread defection or a strong tendency towards individual gain.
                *   **My action in round `t`: Defect (D)**. It's safer to not contribute to a project unlikely to succeed, and I can gain the private endowment.

**Refinement of "Close to m" and "Significantly High Proportion":**

Instead of fixed buffers, I'll introduce a probabilistic element based on historical cooperation.

Let's track the average cooperation rate over the last `L` rounds (where `L` is a parameter, e.g., `L=min(t-1, 5)` for adaptive memory).
Let `Avg_P_C(t-1)` be this average.

*   **If `N_C(t-1) >= m`:** **Cooperate (C)**.
*   **If `N_C(t-1) < m`:**
    *   **Calculate the probability of reaching `m` cooperators *if I cooperate*:** This is roughly the probability that `m-1` or more *other* players cooperate. I'll estimate this probability based on `Avg_P_C(t-1)` and the number of other players (`n-1`).
    *   **If the estimated probability of reaching `m` *if I cooperate* is sufficiently high (e.g., > 0.7):** **Cooperate (C)**.
    *   **Otherwise:** **Defect (D)**.

**My "Cooperative Mindset" Alignment:**

My strategy prioritizes cooperation when it's likely to yield a positive collective outcome (`m` cooperators met). I only defect when the likelihood of achieving the collective benefit is low, to avoid being exploited. This is a form of rational cooperation rather than blind altruism. I'm willing to take a calculated risk for the collective good.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** **Cooperate (C)**.
    *   **Reasoning:** In the absence of any history, the most cooperative and trusting action is to initiate cooperation. This sets a positive precedent and gives the group a chance to reach the threshold from the start. It's a gamble, but it aligns with a cooperative starting point.

*   **Last Round (t=r):**
    *   **Decision:** **Defect (D)**.
    *   **Reasoning:** In the final round, there are no future rounds to consider. Therefore, there is no incentive for cooperative behavior to be reciprocated. The optimal strategy for the last round of any repeated game with a finite horizon is to defect, as it maximizes my individual payoff without any future consequences for the group. This is a common concept in game theory (the "end-game problem").

*   **Rounds where `m` is very close to `n` (e.g., `m = n-1` or `m = n`):**
    *   My logic still holds. If `m` is high, the probability of reaching it by any single player's action will be lower, naturally leading to more defections unless there's very strong historical cooperation.

*   **Rounds where `m` is very close to 1 (e.g., `m = 2` for `n > 2`):**
    *   This becomes very easy to reach. My strategy will likely favor cooperation more often, as the probability of hitting `m` will be high.

### 3. Being Cooperative

My strategy is inherently cooperative in the following ways:

*   **Initiation:** I start with cooperation in round 1.
*   **Proactive Cooperation:** I will cooperate whenever there's a reasonable chance of success (threshold `m` being met), even if it means I might get a lower individual payoff than defecting in that specific round (e.g., if `m` is met by others, I get 0 + k, while defecting gives 1 + k. This is a small sacrifice for the group's success).
*   **Resilience:** I don't immediately punish perceived defection. My decision is based on the *aggregate* behavior and the *likelihood* of future success, not just individual past actions of others. I'm not seeking revenge; I'm seeking to optimize for collective and individual gains over the long term.
*   **Goal Alignment:** My ultimate "goal" is to see the community project succeed, as that's where the highest payoffs for everyone lie. My strategy aims to contribute to that goal whenever it's strategically sound.

### Pseudocode (Conceptual)

```python
# Parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward factor

# Game state (maintained across rounds)
history = []  # List of dictionaries, each representing a round's outcome: {'my_action': 'C'/'D', 'others_actions': [...]}
my_payoffs = []
group_payoffs = [] # Optional, to track aggregate

# --- Strategy Function ---
def decide_action(current_round, history, n, m, k):
    # Handle Edge Cases
    if current_round == 1:
        return 'C'  # Cooperate in the first round

    if current_round == r:
        return 'D'  # Defect in the last round

    # Analyze past rounds
    if not history: # Should not happen for current_round > 1, but good for robustness
        return 'C'

    last_round_data = history[-1]
    num_cooperators_last_round = last_round_data['my_action'] == 'C'
    for action in last_round_data['others_actions']:
        if action == 'C':
            num_cooperators_last_round += 1

    # --- Core Decision Logic ---

    if num_cooperators_last_round >= m:
        # Threshold met or exceeded in the previous round, continue cooperation
        return 'C'
    else:
        # Threshold not met. Assess probability of future success.
        # This is a simplified representation of probabilistic assessment.
        # A more sophisticated implementation would involve Bayesian updates or Markov models.

        # Calculate average cooperation rate over a recent window (e.g., last 5 rounds)
        recent_history = history[-min(current_round - 1, 5):]
        total_cooperators_recent = 0
        num_decisions_recent = 0
        for round_data in recent_history:
            total_cooperators_recent += (round_data['my_action'] == 'C')
            for action in round_data['others_actions']:
                if action == 'C':
                    total_cooperators_recent += 1
            num_decisions_recent += n # Count decisions made in that round

        avg_cooperation_rate_recent = total_cooperators_recent / num_decisions_recent if num_decisions_recent > 0 else 0

        # Estimate probability of reaching 'm' if I cooperate
        # This is a heuristic. A more rigorous method would be needed for precise calculations.
        # For instance, estimate the probability of *others* cooperating.
        prob_others_cooperate = avg_cooperation_rate_recent # A very crude approximation

        # If I cooperate, we need m-1 other cooperators.
        # Probability of achieving this: Binomial distribution approximation.
        # For simplicity, let's use a threshold based on overall average.
        # If the average cooperation rate is high enough that even with a few defections,
        # there's a good chance m-1 others will cooperate.

        # Heuristic: If the average cooperation rate is high enough that it seems likely
        # m-1 other players will cooperate, then I should cooperate.
        # Consider a buffer: If m-1 players are needed, and the average proportion of cooperators
        # among the *other* n-1 players is high.
        # Let's simplify: If the overall average cooperation rate is > (m-1)/n
        # and I am essential to reach m.

        # A more direct approach: What's the chance that (m-1) out of (n-1) players cooperate,
        # given their past behavior? This requires estimating individual player probabilities,
        # which is complex without communication.

        # Simplified heuristic: If historical average cooperation rate is high enough to suggest
        # m-1 others are likely to cooperate.
        # For example, if average P_C > 0.6 and m is not too high.

        # A pragmatic threshold: If the number of cooperators last round was 'close' to m
        # (e.g., > m - buffer_size), and the general trend is up, I might cooperate.
        # Or, if the overall average cooperation rate is high.

        # Let's use a simpler probabilistic check:
        # Probability of reaching m-1 other cooperators given avg_cooperation_rate_recent
        # This is complex. Let's use a simpler rule for demonstration.

        # Simplified rule: If the number of cooperators in the last round was 'close' to m,
        # or if the overall average cooperation rate has been consistently high.
        # Example Threshold: If N_C(t-1) >= m - 2 (and m > 2) OR avg_cooperation_rate_recent > 0.7

        # Let's implement a rule that aims for cooperation if it seems likely:
        # If the number of cooperators in the last round was greater than or equal to (m - 1)
        # AND the average cooperation rate over a window is above a certain threshold (e.g., 0.5)
        # OR if N_C(t-1) was very close to m and I was one of the cooperators in that round.

        # A simpler heuristic for "likely to reach m":
        # If the number of cooperators in the last round was (m-1) or more,
        # and the total number of players (n) is not excessively large compared to m.
        # This indicates a strong tendency.
        # Or, if the average cooperation rate across all past rounds is above a certain threshold,
        # suggesting a stable cooperative environment.

        # For practical implementation in a tournament, a more robust probabilistic estimate
        # would be crucial. For this description, I'll use a more intuitive rule:
        # "If it looks like we're on the verge of success, I'll help push us over."

        # Heuristic: If N_C(t-1) >= m - buffer OR avg_cooperation_rate_recent is high.
        # Let's use a threshold based on the proportion of needed cooperators.
        # If at least (m-1) players cooperated in the last round, or if the average cooperation rate is high.

        # Refined Logic for N_C(t-1) < m:
        # Calculate estimated probability of reaching 'm' if I cooperate.
        # This is P(at least m-1 of other n-1 players cooperate).
        # A simple estimate for P(other player cooperates) = avg_cooperation_rate_recent
        # Using binomial: P(X >= m-1) where X ~ Binomial(n-1, avg_cooperation_rate_recent)
        # This requires a binomial CDF.

        # For simplicity of natural language, let's use a rule like:
        # If the number of cooperators in the last round was `m-1` or more,
        # or if the average cooperation rate over the last few rounds is `> threshold_for_cooperation`
        # then I might cooperate.

        # Let's simplify for clarity:
        # If `num_cooperators_last_round >= m - 1` AND `avg_cooperation_rate_recent >= (m - 1) / n`
        # This suggests we were close and the general tendency is toward cooperation.

        # A simpler approach: If the last round was *almost* cooperative, and the average is ok.
        if num_cooperators_last_round >= m - 1 and avg_cooperation_rate_recent >= 0.5:
            return 'C' # Close to m, and generally cooperative history
        else:
            return 'D' # Not close enough, or general history is poor

    # Fallback (should not be reached if logic is exhaustive)
    return 'D'

# --- How to use ---
# For each round t from 1 to r:
#   my_action = decide_action(t, history, n, m, k)
#   # (Simulated environment executes all actions, returns outcomes)
#   # This strategy only needs its own action, not others' actions to decide its OWN action in advance.
#   # However, the 'history' will be updated with ALL actions after each round.
#   # For decision making in round t, history contains data from rounds 1 to t-1.
#   # So, the 'decide_action' function will be called with 'history' containing t-1 elements.
```

**Note on Pseudocode Complexity:** The precise probabilistic calculation of `P(at least m-1 of other n-1 players cooperate)` is the most complex part of making this strategy truly adaptive and robust. A full implementation would likely use statistical estimation (e.g., estimating each player's propensity to cooperate based on their history, or using a multinomial distribution model) or a Markov chain approach. The simplified heuristic above aims to capture the spirit of the strategy in natural language.

This strategy aims for a balance between fostering collective good and self-preservation, making it a strong candidate for a cooperative tournament.
'''

description_COOPERATIVE_60 = '''
This strategy is designed to be a robust and adaptive cooperative approach in the Collective Risk Dilemma. It aims to incentivize cooperation by rewarding successful collective action, while minimizing losses in scenarios where cooperation fails. The core idea is to balance the desire to contribute to the collective good with the need for self-preservation against defectors.

## Cooperative Strategy: "Threshold-Adaptive Cooperation"

This strategy operates based on the observed success of cooperation in previous rounds and aims to dynamically adjust its behavior to encourage the achievement of the cooperation threshold ($m$).

### 1. Decision Rules:

The decision to cooperate (C) or defect (D) in any given round (after the first) is determined by a combination of past performance and a calculated "cooperation probability."

**Initial State (Round 1):**
In the first round, to establish a baseline and signal a cooperative intent, the strategy will **Cooperate (C)**. This is a bold move that aims to build trust and demonstrate a commitment to the collective good.

**Subsequent Rounds (Round t > 1):**
For rounds after the first, the strategy employs a more nuanced approach:

*   **Calculate Observed Cooperation Rate:** Determine the proportion of players who cooperated in the *previous* round ($t-1$). Let's call this `observed_cooperation_rate_t-1`.
    `observed_cooperation_rate_t-1 = (Number of players who played C in round t-1) / n`

*   **Calculate Target Cooperation Ratio:** Determine the ratio of players needed to cooperate to reliably meet the threshold $m$.
    `target_cooperation_ratio = m / n`

*   **Determine Cooperation Probability ($P_{cooperate}$):** The strategy will decide to cooperate with a probability that is influenced by how close the `observed_cooperation_rate_t-1` was to the `target_cooperation_ratio`. The goal is to increase cooperation probability when the group is nearing the threshold and decrease it when it's far from it.

    We can define $P_{cooperate}$ as follows:

    Let `deviation_from_target = target_cooperation_ratio - observed_cooperation_rate_t-1`

    If `deviation_from_target <= 0` (meaning the observed rate met or exceeded the target):
        `P_{cooperate} = 1.0`  (Always cooperate if the threshold was met or exceeded)

    If `deviation_from_target > 0` (meaning the observed rate was below the target):
        `P_{cooperate} = max(0.0, min(1.0, (target_cooperation_ratio + (1 - observed_cooperation_rate_t-1)) / 2 ))`

        **Explanation of the formula for `P_{cooperate}` when below target:**
        *   `target_cooperation_ratio`: This is the ideal proportion we want to achieve.
        *   `1 - observed_cooperation_rate_t-1`: This represents the proportion of defectors.
        *   `(target_cooperation_ratio + (1 - observed_cooperation_rate_t-1)) / 2`: This averages the ideal cooperation ratio with the observed level of defection. This aims to "pull" the cooperation probability towards cooperation if the observed rate is low, but not excessively if there's a high proportion of defectors. It ensures that if we are far from the target, our cooperation probability doesn't become too high, and if we are close, it encourages cooperation.
        *   `max(0.0, min(1.0, ...))`: This ensures that the probability always stays within the valid range of [0, 1].

*   **Decision:** In round $t$, the strategy will play **C** with probability $P_{cooperate}$ and **D** with probability $(1 - P_{cooperate})$.

### 2. Handling Edge Cases:

*   **Round 1:** As mentioned, the strategy will **always Cooperate (C)** in the first round. This is a proactive signal of cooperative intent and an attempt to initiate successful collective action from the outset.

*   **Last Round (Round r):** The decision for the last round is critical. Since there are no future rounds to influence, the strategy should maximize its payoff in this final round.
    *   If `m` or more players cooperated in the *previous* round ($r-1$), it is highly likely that the collective reward was achieved. In this case, the strategy will **Cooperate (C)** to secure its share of the reward.
    *   If fewer than `m` players cooperated in round $r-1$, the collective reward was not achieved. In this scenario, the best individual payoff comes from defecting. Therefore, the strategy will **Defect (D)** in the last round.

*   **Insufficient Data:** If for any reason (e.g., a glitch in observation), the strategy cannot accurately determine the number of cooperators in the previous round, it will revert to a **fallback strategy of Cooperate (C)**. This maintains the cooperative bias.

### 3. Being Cooperative:

This strategy is inherently cooperative due to the following reasons:

*   **Initial Cooperative Stance:** Starting with cooperation signals a willingness to contribute to the collective good.
*   **Incentivizing the Threshold:** The decision rule is designed to actively encourage cooperation when the group is close to meeting the threshold ($m$). It increases the probability of cooperation when past observations indicate a possibility of success.
*   **Rewarding Success:** The strategy rewards successful rounds of cooperation by increasing the likelihood of continued cooperation. This reinforces the positive outcome of collective effort.
*   **Adaptive to Failure:** While cooperative, it's not naively cooperative. If cooperation consistently fails to meet the threshold, the strategy will reduce its cooperation probability, adapting to the environment to avoid being exploited.
*   **Last Round Logic:** The last round decision ensures that it doesn't sacrifice immediate payoff for a non-existent future benefit, but it still prioritizes cooperation if it was previously successful, demonstrating a commitment to the established cooperative norm.

### Pseudocode:

```pseudocode
StrategyName: Threshold-Adaptive-Cooperation

Parameters: n, r, m, k

// History storage (assuming access to past actions of all players)
// history[round_number][player_index] = action (C or D)

Function DecideAction(current_round, history):
  // Round 1: Always cooperate
  if current_round == 1:
    return C

  // Last round: Maximize immediate payoff
  if current_round == r:
    cooperators_last_round = count_cooperators(history, r - 1)
    if cooperators_last_round >= m:
      return C // Reward achieved, so cooperate
    else:
      return D // Reward not achieved, defect for best individual payoff

  // Intermediate rounds: Adaptive cooperation
  else:
    observed_cooperation_rate_prev = count_cooperators(history, current_round - 1) / n

    target_cooperation_ratio = m / n

    // Handle potential missing data (though perfect information is assumed)
    if observed_cooperation_rate_prev is invalid:
      return C // Fallback to cooperation

    deviation_from_target = target_cooperation_ratio - observed_cooperation_rate_prev

    if deviation_from_target <= 0: // Threshold met or exceeded
      P_cooperate = 1.0
    else: // Threshold not met
      // Formula to balance target cooperation and observed defection
      P_cooperate = max(0.0, min(1.0, (target_cooperation_ratio + (1 - observed_cooperation_rate_prev)) / 2.0))

    // Randomly decide based on P_cooperate
    random_value = generate_random_float(0.0, 1.0)
    if random_value < P_cooperate:
      return C
    else:
      return D

// Helper function to count cooperators in a given round
Function count_cooperators(history, round_num):
  count = 0
  for player_i = 1 to n:
    if history[round_num][player_i] == C:
      count = count + 1
  return count

// Assume generate_random_float(min_val, max_val) is available
```
'''

description_COOPERATIVE_61 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust in a tournament setting.

## Cooperative Strategy: "Cautious Threshold Seeker"

This strategy aims to achieve cooperation by conditionally cooperating. It starts with a cooperative inclination, but is prepared to defect if cooperation appears to be failing or if the potential for individual gain from defection becomes too high relative to the collective reward.

### Core Principle: Conditional Cooperation with a Risk Premium

The strategy is built on the idea that cooperation is beneficial *if* enough others cooperate. The "risk premium" comes from the fact that a player contributing to the community project risks receiving a lower payoff if the cooperation threshold is not met. This strategy tries to balance the desire for the collective reward with the risk of individual loss.

### 1. Decision Rules

The decision to cooperate (C) or defect (D) in any given round `t` (where `t > 1`) depends on the observed outcome of the previous round `t-1`.

**Decision Rule for Round `t` (where `t > 1`):**

Let `cooperators_t_minus_1` be the number of players who cooperated in round `t-1`.

*   **If `cooperators_t_minus_1` ≥ `m`:**
    *   **Cooperate (C).** The collective reward was achieved in the previous round, demonstrating the viability of cooperation. Continue contributing to maintain this success.

*   **If `cooperators_t_minus_1` < `m`:**
    *   **Defect (D).** The cooperation threshold was not met in the previous round. This suggests that either too few players are willing to cooperate, or there's a strong incentive for others to defect. To avoid individual loss while the collective fails, defect.

**Rationale:**

*   **Rewarding Success:** If cooperation is successful, the strategy continues to cooperate, reinforcing successful behavior and maximizing the potential for future collective gains.
*   **Punishing Failure (and Self-Preservation):** If cooperation fails, the strategy defects. This serves two purposes:
    *   **Self-preservation:** It ensures the player doesn't waste their endowment on a failed community project.
    *   **Signaling:** While there's no direct communication, a player defecting when the threshold isn't met can be interpreted by others as a response to the collective failure. This might encourage others to be more cooperative in the future if they observe the strategic player's defection as a *reaction* to insufficient cooperation, rather than an unprovoked defection.

### 2. Handling Edge Cases

**a) First Round (`t = 1`):**

*   **Cooperate (C).**
    **Rationale:** In the absence of any history, the most cooperative and optimistic approach is to initiate cooperation. This sets a positive example and gives the collective project the best chance to succeed from the outset. It aligns with the "cooperative mindset" and the principle of giving cooperation a chance.

**b) Last Round (`t = r`):**

*   **Defect (D).**
    **Rationale:** In the final round, there are no future consequences for defection. The primary driver becomes maximizing individual payoff. If the game has reached its end, and there's no incentive to build future cooperation, the best strategy is to secure the private endowment. This is a pragmatic adjustment to the game's finite nature.

**c) `m` = 2 (Minimum cooperators needed is 2):**

*   This is handled by the general rules. The strategy will cooperate if the previous round had at least 2 cooperators, and defect if it had less than 2. The initial cooperation in round 1 is crucial here.

**d) `k` is very high (e.g., `k` >> 1):**

*   The strategy's logic remains the same. A high `k` makes cooperation highly desirable, but the "Cautious Threshold Seeker" still requires evidence of *collective* success before committing to continued cooperation. It avoids "risking it all" on the hope that others will also cooperate for a high reward.

**e) `k` is close to 1 (e.g., `k` = 1.1):**

*   The strategy's logic also remains the same. The reward is only slightly better than defecting. The strategy will still look for the threshold to be met before continuing to cooperate. The initial round's cooperation is a gamble that might not pay off as significantly, but it's a necessary step to observe.

### 3. Being Cooperative

This strategy embodies a cooperative mindset through:

*   **Initial Cooperation:** It always starts by attempting cooperation, demonstrating goodwill and a willingness to invest in the collective good.
*   **Reinforcing Success:** When cooperation proves successful (threshold met), it continues to cooperate, actively working towards sustained collective benefit.
*   **Conditional Defection:** Defection is not malicious but a defensive mechanism triggered by the *failure* of cooperation. It's a response to a collective breakdown, not an act of aggression. The player aims to avoid being the sole contributor to a failed project.

### Pseudocode

```pseudocode
// Game Parameters
parameter n: number of players (integer, n >= 2)
parameter r: number of rounds (integer, r > 1)
parameter m: minimum cooperators needed (integer, 1 < m < n)
parameter k: reward if threshold met factor (float, k > 1)

// State variables (kept track of across rounds)
// Assuming we have access to historical actions from all players
// 'history' is a list of dictionaries, where each dictionary represents a round's actions
// e.g., history = [ {"player1": "C", "player2": "D", ...}, ... ]

function decide_action(current_round_number, history):
  // Handle the last round
  if current_round_number == r:
    return "D" // Defect in the final round to maximize individual payoff

  // Handle the first round
  if current_round_number == 1:
    return "C" // Initiate cooperation

  // For subsequent rounds (2 to r-1)
  previous_round_actions = history[current_round_number - 2] // History is 0-indexed, rounds are 1-indexed
  cooperators_in_previous_round = 0
  for player_action in previous_round_actions.values():
    if player_action == "C":
      cooperators_in_previous_round += 1

  if cooperators_in_previous_round >= m:
    return "C" // Threshold met, continue cooperating
  else:
    return "D" // Threshold not met, defect
```

### Robustness and Adaptability

*   **Robustness:** The strategy is robust because its decisions are based on observable outcomes (`m` cooperators) rather than assumptions about other players' strategies. It doesn't assume altruism or specific patterns, making it resilient to a diverse range of opponent behaviors.
*   **Adaptability:** The strategy adapts to the game's unfolding dynamics. It learns from the success or failure of cooperation in the previous round and adjusts its behavior accordingly. The change in behavior for the last round is a critical adaptation to the game's end.

This "Cautious Threshold Seeker" strategy offers a balanced approach to the Collective Risk Dilemma, aiming for cooperation while prudently managing risk and adapting to the game's progression.
'''

description_COOPERATIVE_62 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive, robust, and purely based on game parameters and history.

## Strategy Name: Adaptive Threshold Cooperation (ATC)

**Core Principle:** The ATC strategy aims to facilitate cooperation by leveraging the collective reward ($k$) while mitigating the risk of individual defection. It dynamically adjusts its cooperation threshold based on observed player behavior, aiming to achieve the minimum cooperative threshold ($m$) without exposing itself to excessive exploitation.

### 1. Decision Rules

The ATC strategy's decision to cooperate (C) or defect (D) in any given round is based on the following conditions:

*   **Initial Cooperation:** In the very first round, to establish a baseline and signal cooperative intent, the ATC strategy will **Cooperate (C)**. This is a necessary step in a repeated game to explore the possibility of reaching the cooperative outcome.

*   **Sustaining Cooperation:**
    *   **Condition 1: Sufficient Cooperation Observed:** If, in the *previous* round, the number of players who cooperated (including ATC itself, if it cooperated) was **greater than or equal to `m - 1`**, then ATC will **Cooperate (C)** in the current round.
        *   **Rationale:** This condition suggests that the group is close to or has achieved the cooperative threshold. By cooperating, ATC increases the likelihood of crossing the threshold and securing the reward for everyone. It aims to "pull" the group over the edge if it's close.

    *   **Condition 2: Close Call and Reciprocity:** If, in the *previous* round, the number of players who cooperated was **exactly `m - 2`**, AND ATC itself **cooperated** in the previous round, then ATC will **Cooperate (C)** in the current round.
        *   **Rationale:** This represents a "near miss" where ATC's cooperation was part of a strong collective effort, but not quite enough. By cooperating again, ATC demonstrates its commitment and encourages others who might have defected due to the missed reward to reconsider. It signals that individual sacrifice is still being made in pursuit of the common good.

*   **Conditional Defection:**
    *   **Condition 3: Insufficient Cooperation and Risk of Exploitation:** If, in the *previous* round, the number of players who cooperated was **less than `m - 1`**, ATC will **Defect (D)** in the current round.
        *   **Rationale:** This is the primary mechanism for protecting ATC from exploitation. If the group consistently fails to meet the cooperative threshold, and ATC repeatedly cooperates without seeing sufficient collective effort, it will defect to secure its private endowment (payoff of 1). This prevents ATC from being the sole cooperator or one of only a few, consistently receiving a lower payoff.

    *   **Condition 4: Last Chance Cooperation (to avoid collective failure):** If, in the *previous* round, the number of players who cooperated was **less than `m`**, AND the current round is the **second-to-last round (r-1)**, ATC will **Cooperate (C)**.
        *   **Rationale:** This is a strategic choice to give the group one last chance to achieve cooperation before the final round. Even if the odds are slim, a final cooperative push in round `r-1` might incentivize players to cooperate in the final round. This prioritizes a potential collective win over a guaranteed individual gain in the penultimate round.

*   **Final Round Logic:** In the **final round (r)**, ATC will always **Defect (D)**.
    *   **Rationale:** Since there are no future rounds to influence, the future reward ($k$) is no longer a factor. The optimal strategy in a one-shot game where defection guarantees a payoff of 1 and cooperation might lead to a lower payoff is to defect. This prevents any possibility of being exploited on the last move.

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated in the decision rules, ATC will **Cooperate (C)** in the first round. This is crucial for initiating the possibility of cooperative outcomes.

*   **Last Round (t=r):** As stated in the decision rules, ATC will **Defect (D)** in the last round. This is a standard dominant strategy in the final iteration of a repeated game.

*   **Second-to-Last Round (t=r-1):** ATC will **Cooperate (C)** if the previous round's cooperation was less than `m`. This is a specific "last chance" cooperative gesture to try and push for a collective success.

*   **Dynamic Threshold Adjustment:** The strategy implicitly adjusts its "cooperation trigger" by observing the number of cooperators. It doesn't have a fixed personal threshold for cooperation, but rather a dynamic one influenced by group behavior. The conditions `m-1` and `m-2` represent points where ATC is willing to take a greater risk to achieve the collective goal, especially if it feels its own cooperation is contributing significantly.

### 3. Being Cooperative

The ATC strategy is inherently cooperative because:

*   **Initial Cooperative Stance:** It starts by cooperating, signaling a willingness to work towards the collective good.
*   **Pro-Cooperation Bias:** It cooperates whenever the group is close to or has achieved the cooperative threshold (`>= m - 1`). This demonstrates a commitment to the group reward and a desire to see the community project succeed.
*   **Reciprocity for Near Misses:** It extends cooperation even when a near miss occurred (`m-2` cooperators and ATC cooperated), showing resilience and encouraging sustained effort.
*   **"Last Chance" Cooperation:** The specific rule for the second-to-last round is a pure expression of cooperative intent, aiming for a final collective win even if it requires a personal sacrifice in that round.

**Key Adaptive Elements:**

*   **Observational Learning:** ATC's decision is entirely dependent on the observed number of cooperators in the *previous* round. This makes it responsive to the collective behavior of the other players.
*   **Risk Management:** The defection rule (`< m - 1` cooperators) acts as a safety net, preventing ATC from consistently being exploited.
*   **Parameter Sensitivity:** The strategy relies heavily on the `m` (minimum cooperators) parameter. If `m` is very high or very low, the strategy will naturally adapt its behavior. For instance, if `m` is close to `n`, cooperation becomes very difficult, and ATC will likely defect more often unless there's overwhelming evidence of widespread cooperation. If `m` is very close to 1, ATC will cooperate more readily.

### Pseudocode Representation:

```pseudocode
Strategy ATC(parameters, history):
    n = parameters.n
    r = parameters.r
    m = parameters.m
    k = parameters.k

    current_round = history.current_round
    actions_previous_round = history.actions_previous_round // List of actions {C, D} for all players in the previous round
    num_cooperators_previous_round = count_cooperators(actions_previous_round)

    // Edge Case: First Round
    if current_round == 1:
        return C

    // Edge Case: Last Round
    if current_round == r:
        return D

    // Decision Rules
    // Condition 1: Close to or met threshold
    if num_cooperators_previous_round >= m - 1:
        return C

    // Condition 2: Near miss and ATC cooperated
    if num_cooperators_previous_round == m - 2 and actions_previous_round[my_index] == C:
        return C

    // Condition 4: Last chance cooperation (second-to-last round)
    if current_round == r - 1 and num_cooperators_previous_round < m:
        return C

    // Condition 3: Insufficient cooperation and risk of exploitation
    // This is the default if none of the above cooperative conditions are met
    return D

function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == C:
            count = count + 1
    return count

// Note: 'my_index' would refer to the index of the ATC player within the actions_previous_round list.
// In a real implementation, this would be managed by the game engine.
```

This strategy aims to be a robust and cooperative player by balancing the desire for collective reward with the necessity of self-preservation against potential exploitation. Its adaptive nature allows it to respond intelligently to the evolving dynamics of the game.
'''

description_COOPERATIVE_63 = '''
This is a fascinating game that blends elements of the Prisoner's Dilemma with a public goods game, amplified by the repeated nature and the threshold for cooperation. The core challenge is achieving cooperation when individual incentives to defect (to guarantee a payoff of 1) can outweigh the potential for a higher collective payoff if the threshold is met.

Here's a cooperative strategy designed for this Collective Risk Dilemma:

## Cooperative Threshold Strategy (CTS)

The fundamental idea behind CTS is to be cautiously cooperative. It aims to initiate and sustain cooperation by "leading" when the conditions are favorable and punishing defection by withdrawing cooperation. The strategy is designed to be adaptive and learn from past rounds.

### Core Principles:

1.  **Threshold Focus:** The primary driver for cooperation is the potential to reach the `m` threshold. The strategy will try to contribute to reaching this threshold.
2.  **Reciprocity with a Twist:** It's not pure Tit-for-Tat. It learns from the *aggregate* behavior of others and adjusts its cooperativeness based on the perceived likelihood of others cooperating.
3.  **Risk Aversion for Cooperation:** While aiming for higher payoffs, the strategy is mindful of the risk of individual defection when the threshold is not met. It will defect if it perceives a high risk of being the "only" cooperator or one of too few.
4.  **Adaptive Learning:** The strategy dynamically adjusts its cooperativeness based on the observed cooperation rate in previous rounds.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` (where `t > 1`) depends on two main factors:

*   **Observed Cooperation Rate in Previous Rounds:** This is the primary indicator of how likely others are to cooperate.
*   **Current Round:** Special considerations for the first and last rounds.

Let:
*   `N_C(t-1)` be the number of players who cooperated in round `t-1`.
*   `CoopRate(t-1) = N_C(t-1) / n` be the observed cooperation rate in round `t-1`.

**Decision Logic for Round `t`:**

*   **If `t = 1` (First Round):**
    *   **Cooperate (C).** The strategy's default is to attempt cooperation in the very first round. This is a signal of good faith and an attempt to establish a cooperative norm from the outset. It assumes that others might also adopt a similar initial cooperative stance.

*   **If `t = r` (Last Round):**
    *   **Defect (D).** In the absence of future rounds, the incentive to defect and secure a guaranteed payoff of 1 becomes dominant for all rational players. The strategy will defect to maximize its final individual payoff, as there's no future to leverage cooperation.

*   **If `1 < t < r` (Intermediate Rounds):**

    This is where the adaptive and reciprocal elements come into play. The strategy will adopt a "risk-adjusted cooperation" approach.

    **Define a "Cooperation Threshold Factor" (CTF):** This is a dynamic value that influences the decision to cooperate. It's derived from the observed cooperation rate. A simple way to implement this is:

    `CTF = CoopRate(t-1)`

    **Decision Rule:**
    *   **If `CTF >= (m - 1) / n`:** This means the observed cooperation rate in the previous round was "close enough" to the minimum required to *potentially* reach the threshold if the current player cooperates.
        *   **Cooperate (C).** The strategy takes a calculated risk, assuming that by cooperating, it might be the crucial player to push the collective over the `m` threshold. This is especially true if `CTF` is close to `(m-1)/n`.
    *   **Else (`CTF < (m - 1) / n`):** The observed cooperation rate in the previous round was significantly below the level needed to reliably reach the threshold, even with the current player's cooperation.
        *   **Defect (D).** The risk of cooperating and not reaching the threshold (and thus getting a payoff of 1, while others who defected also get 1) is too high compared to the guaranteed payoff of 1 from defecting. This action also serves as a punishment for the observed lack of cooperation.

**Refinement on the `CTF`:** To make it more nuanced, we could introduce a sensitivity parameter. For instance, if the previous cooperation rate was very low (e.g., 0), the strategy might be more hesitant.

**Revised Decision Logic for `1 < t < r`:**

1.  **Calculate `N_C(t-1)` and `CoopRate(t-1)`**.
2.  **Determine the "Cooperation Signal":**
    *   If `N_C(t-1) >= m`: The threshold was met or exceeded. This is a strong signal to continue cooperating.
    *   If `N_C(t-1) >= m - 1`: The threshold was almost met. This is a moderate signal to cooperate, especially if the current player's cooperation is needed.
    *   If `N_C(t-1) < m - 1`: The threshold was not met and not even close. This is a weak signal for cooperation, potentially leading to defection.

3.  **Decision:**
    *   **If `N_C(t-1) >= m - 1`:**
        *   **Cooperate (C).** The strategy will cooperate if the previous round saw enough cooperation to be within one cooperator of the threshold. This is the "leadership" phase – trying to tip the balance.
    *   **Else (`N_C(t-1) < m - 1`):**
        *   **Defect (D).** If the previous round's cooperation level was too low, the strategy defects. This protects its individual payoff and punishes the lack of collective action.

**Pseudocode for `1 < t < r`:**

```pseudocode
function decide_action(previous_round_actions, n, m, k, r, current_round_t):
    // previous_round_actions is a list of actions taken by all players in round t-1
    // Example: ['C', 'D', 'C', 'C', 'D', 'D']

    num_cooperators_previous_round = count('C' in previous_round_actions)

    if num_cooperators_previous_round >= m - 1:
        // Sufficient cooperation in the previous round, attempt to reach threshold
        return 'C'
    else:
        // Not enough cooperation, defect to secure individual payoff
        return 'D'
```

### 2. Handling Edge Cases:

*   **First Round (`t = 1`):**
    *   **Action: Cooperate (C).** This is a proactive approach to signal cooperative intent and encourage others. It assumes that in the absence of history, initiating cooperation is the best bet for establishing a beneficial outcome.

*   **Last Round (`t = r`):**
    *   **Action: Defect (D).** As explained, in the final round, there's no future incentive to cooperate. The dominant strategy for any rational player is to maximize their immediate payoff. This strategy mirrors that rational self-interest in the final round.

*   **Threshold Met in Previous Round (`N_C(t-1) >= m`):**
    *   **Action: Cooperate (C).** If the threshold was successfully met or exceeded in the previous round, the strategy strongly favors continuing cooperation. This reinforces successful cooperative behavior and aims to maintain the higher payoff.

*   **Threshold Missed by Exactly One (`N_C(t-1) == m - 1`):**
    *   **Action: Cooperate (C).** This is the "tipping point" scenario. The strategy actively cooperates to try and be the one that helps the group reach the threshold. This is a key adaptive mechanism.

*   **Threshold Missed by More Than One (`N_C(t-1) < m - 1`):**
    *   **Action: Defect (D).** If the previous round's cooperation was too low, the strategy defects. This is a punitive measure and a risk-management step, preventing the strategy from contributing to a failed cooperative effort when individual defection is a guaranteed payoff of 1.

### 3. Being Cooperative:

This strategy is cooperative in its intent and its initial actions.

*   **Proactive Cooperation:** It initiates cooperation in the first round, signaling a desire for a mutually beneficial outcome.
*   **Conditional Cooperation:** It cooperates when the conditions are favorable for achieving collective success (i.e., when the group is close to or has met the cooperation threshold). This is "conditional" cooperation, as it's not naive but rather aims to foster conditions where cooperation pays off.
*   **Reinforces Success:** It rewards successful past cooperation by continuing to cooperate.
*   **Punishes Failure (Indirectly):** By defecting when cooperation fails, it discourages future lack of cooperation. However, the primary goal is not to punish but to adapt to circumstances that make cooperation less likely to succeed.

**Robustness and Adaptability:**

*   **Against Defectors:** If opponents consistently defect, `N_C(t-1)` will remain low, and the strategy will eventually switch to defecting as well, avoiding being exploited.
*   **Against Cooperators:** If opponents are cooperative, `N_C(t-1)` will likely be high, and the strategy will continue to cooperate, leading to high payoffs for all.
*   **Against Mixed Strategies:** The strategy adapts to the *aggregate* behavior. If the overall cooperation rate is sufficient, it cooperates; if not, it defects. This makes it robust to various combinations of opponent strategies.
*   **Parameter Dependence:** The strategy directly uses `m` in its decision-making, making it inherently sensitive to the game's cooperative threshold.

This strategy aims to find a balance between pursuing the higher collective reward and protecting against the risk of individual loss in a dynamic and uncertain environment.
'''

description_COOPERATIVE_64 = '''
This strategy, "Adaptive Threshold Cooperation" (ATC), is designed to maximize the player's long-term payoff in the Collective Risk Dilemma by balancing the desire for individual gain with the potential for collective reward. It is inherently cooperative but also pragmatic, adapting to the observed behavior of other players.

## Adaptive Threshold Cooperation (ATC) Strategy

The core idea behind ATC is to start cooperatively and adjust the "threshold" for continued cooperation based on the collective success of previous rounds. It aims to exploit the reward ($k$) when a sufficient number of players are willing to cooperate, but defaults to defection if the group consistently fails to reach the cooperation threshold ($m$).

### 1. Decision Rules

The decision to cooperate or defect in any given round $t$ (where $t > 1$) depends on the following:

*   **Initial Stance:** In the very first round, the strategy defaults to **Cooperate (C)**. This is a gamble to signal cooperativeness and potentially kickstart a cooperative dynamic.

*   **Observed Cooperation Rate:** For rounds $t > 1$, the strategy observes the proportion of players who cooperated in the previous round, $t-1$. Let's denote this as $P_{C, t-1}$.

*   **Adaptive Threshold Adjustment:**
    *   **If $P_{C, t-1} \ge \frac{m}{n}$ (i.e., the cooperation threshold was met or exceeded in the previous round):** The player's confidence in collective cooperation increases. The strategy will **Cooperate (C)** in round $t$. This reinforces the cooperative behavior.
    *   **If $P_{C, t-1} < \frac{m}{n}$ (i.e., the cooperation threshold was NOT met in the previous round):** The strategy becomes more cautious. It will **Defect (D)** in round $t$. This is a protective measure to avoid contributing to a project that is likely to fail and thus yield a lower payoff for the player.

*   **Final Round Behavior:** In the last round ($t=r$), the strategy will **Defect (D)**. This is a standard strategy in finitely repeated games to avoid exploitation in the final interaction. The reasoning is that there are no future rounds to punish defection or reward cooperation, so maximizing immediate private gain is optimal.

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated above, the strategy initiates by **Cooperating (C)**. This is the most cooperative starting point and aims to foster a positive cooperative environment.

*   **Last Round (t=r):** The strategy **Defects (D)** in the final round. This is a robust strategy against potential exploitation in the last interaction. It ensures the player secures their private endowment without risking it for a reward that cannot be reciprocated or punished in subsequent rounds.

*   **Game Parameters (n, m, k):** The strategy is designed to be robust across a wide range of these parameters.
    *   A higher $m$ relative to $n$ makes cooperation harder to achieve, and the strategy will naturally tend towards defection if the observed cooperation rate consistently falls short.
    *   A high $k$ makes cooperation more attractive *if* the threshold is met. The strategy's adaptive nature will capitalize on this reward when it deems it likely.
    *   The initial cooperation and subsequent adaptation make it resilient to the initial behavior of other players. If others are cooperative, ATC will join and continue. If others are not, ATC will eventually defect to protect its own endowment.

### 3. Be Cooperative

The ATC strategy is fundamentally cooperative in spirit for the following reasons:

*   **Initial Cooperative Stance:** It begins by actively choosing cooperation, aiming to be a catalyst for collective success. This is a direct expression of the cooperative mindset.
*   **Reinforcement of Cooperation:** When collective cooperation is successful (threshold met), the strategy continues to cooperate. This actively supports and reinforces beneficial group outcomes.
*   **Adaptive Risk Management:** The strategy's shift to defection is not malicious but a rational response to observed group failure. It prioritizes not being exploited when the collective effort fails to materialize. This is a pragmatic form of cooperation – cooperating when it's *likely to be mutually beneficial*, and protecting oneself when it's not. It avoids being a "sucker" in a defection-prone environment.
*   **Long-Term Benefit:** By attempting to foster cooperation and capitalizing on the reward when successful, the strategy aims to achieve higher total payoffs over the long run, which is often the goal of cooperative strategies.

### Pseudocode Representation

```pseudocode
// Strategy: Adaptive Threshold Cooperation (ATC)

// Game Parameters (accessible globally or passed in)
// n: number of players
// m: minimum cooperators needed
// r: number of rounds
// k: reward factor (not directly used in decision, but implied in payoff)

// History (stores actions of all players in previous rounds)
// history = list of lists, where history[t-1] is a list of actions (C/D) for round t-1

function decide_action(current_round, history):
    // Handle the very last round
    if current_round == r:
        return "D" // Defect in the last round to secure endowment

    // Handle the first round
    if current_round == 1:
        return "C" // Cooperate in the first round to signal cooperativeness

    // For subsequent rounds (current_round > 1)

    // Get actions from the previous round
    previous_round_actions = history[current_round - 2] // history is 0-indexed, current_round is 1-indexed

    // Count the number of players who cooperated in the previous round
    cooperators_in_previous_round = count("C", previous_round_actions)

    // Calculate the proportion of cooperators in the previous round
    proportion_cooperators = cooperators_in_previous_round / n

    // Define the cooperation threshold proportion
    cooperation_threshold_proportion = m / n

    // Decision rule based on observed cooperation
    if proportion_cooperators >= cooperation_threshold_proportion:
        return "C" // If threshold was met or exceeded, continue to cooperate
    else:
        return "D" // If threshold was not met, defect to protect endowment

```

This strategy is designed to be simple, understandable, and robust, prioritizing long-term collective gains when achievable, while safeguarding individual interests in situations where cooperation falters.
'''

description_COOPERATIVE_65 = '''
This strategy, "Adaptive Threshold Cooperation" (ATC), aims to foster cooperation in the Collective Risk Dilemma by dynamically adjusting its behavior based on observed cooperation levels and the game's parameters, while remaining resilient to defection and prioritizing long-term cooperation.

## Adaptive Threshold Cooperation (ATC) Strategy

The core idea of ATC is to start with a willingness to cooperate, but to become more cautious if cooperation falters, and to be more forgiving if cooperation recovers. It leverages the repeated nature of the game and the explicit parameters ($m$ and $k$) to guide its decisions.

### 1. Decision Rules: When to Cooperate vs. Defect

ATC's decision in any given round $t$ (where $t > 0$, as round 0 is handled as an edge case) is based on a dynamic "cooperation threshold." This threshold is not fixed at $m$, but rather adjusts based on the observed success of cooperation in previous rounds.

**Key Concepts:**

*   **Observed Cooperation Rate:** The proportion of players (including oneself) who cooperated in the previous round. Let $C_{t-1}$ be the number of players who cooperated in round $t-1$. The observed cooperation rate is $C_{t-1} / n$.
*   **Dynamic Cooperation Threshold (DCT):** This threshold represents the minimum observed cooperation rate we *expect* or *require* for us to cooperate in the current round. Initially, it is set to a level that encourages initial cooperation, but it will adjust.
*   **Cooperation Buffer:** A measure of how "close" we are to meeting the $m$ threshold. This helps to avoid punishing minor dips in cooperation.
*   **Forgiveness Counter:** A mechanism to encourage the return to cooperation after a period of defection.

**Decision Logic for Round $t > 0$:**

1.  **Calculate Observed Cooperation Rate ($OCR_{t-1}$) from Round $t-1$:**
    *   Count the number of players ($C_{t-1}$) who played 'C' in round $t-1$.
    *   $OCR_{t-1} = C_{t-1} / n$.

2.  **Update Dynamic Cooperation Threshold (DCT):**
    *   **If $OCR_{t-1} \ge m/n$ (i.e., the previous round met or exceeded the minimum required cooperators):**
        *   The DCT should be slightly *lowered* to encourage continued cooperation and reward past success. This represents a more optimistic outlook.
        *   Let's define a "cooperation boost" factor, `coop_boost` (e.g., 0.05 or 5% of $n$).
        *   `DCT = max(m/n - coop_boost, m/n * 0.9)` (ensure it doesn't drop too low, and is still a meaningful fraction of the target $m/n$). The second part ensures it scales somewhat with $m/n$.
    *   **If $OCR_{t-1} < m/n$ (i.e., the previous round failed to meet the minimum required cooperators):**
        *   The DCT should be *raised*, indicating a need for more evidence of cooperation before we commit. This represents a more cautious outlook.
        *   Let's define a "caution increment," `caution_increment` (e.g., 0.03 or 3% of $n$).
        *   `DCT = min(m/n + caution_increment, 1.0)` (ensure it doesn't exceed 100% cooperation).

3.  **Consider Forgiveness Counter:**
    *   If the Forgiveness Counter is active (meaning we've seen a recent failure to meet $m$ and are waiting for recovery), we might be more lenient.

4.  **Make the Decision:**

    *   **If $OCR_{t-1} \ge \text{DCT}$ OR Forgiveness Counter is active and $OCR_{t-1} \ge m/n$:**
        *   **Play 'C' (Cooperate).** This indicates we believe enough cooperation is happening or is recovering.
        *   If the Forgiveness Counter was active, deactivate it after this cooperative move.

    *   **Else (if $OCR_{t-1} < \text{DCT}$ AND Forgiveness Counter is NOT active):**
        *   **Play 'D' (Defect).** This indicates a lack of sufficient cooperation to justify our own contribution.
        *   Activate the Forgiveness Counter. This signals that we are now expecting cooperation to recover.

**Tuning `DCT` updates:** The `coop_boost` and `caution_increment` values are crucial. They determine how quickly ATC adapts. A smaller `coop_boost` makes it more patient with cooperation, while a larger `caution_increment` makes it more wary of defection. These would ideally be tuned through tournament play.

### 2. Handling Edge Cases

*   **Round 0 (First Round):**
    *   **Decision:** Always **Play 'C' (Cooperate)**.
    *   **Rationale:** In the absence of any history, the most cooperative and optimistic approach is to initiate cooperation. This strategy assumes that the initial endowment is an investment. By cooperating, we signal willingness and hope to incentivize others. We are essentially betting on the possibility that others will also be cooperative, or at least that the group as a whole will reach $m$.

*   **Last Round (Round $r$):**
    *   **Decision:** If the total cumulative payoff from previous rounds is below a certain personal target (e.g., a guaranteed minimum if everyone defected every round, plus some expectation), or if the cumulative payoff is significantly lower than what could have been achieved through consistent defection, **Play 'D' (Defect)**. Otherwise, **Play 'C' (Cooperate)**.
    *   **Rationale:** The last round is no longer part of a repeated game with future consequences for reputation. Therefore, the incentive structure shifts towards pure self-interest. However, ATC wants to be a "good sport" if it's already in a good position.
        *   **Defect if behind:** If the player is significantly behind in their accumulated payoff, they should defect to maximize their individual gain in this final round, assuming the potential reward from cooperation is not guaranteed and the risk of losing endowment is high. A simple heuristic could be: if `current_total_payoff < (r-1) * 1 + 1`, play D.
        *   **Cooperate if ahead/even:** If the player has a healthy cumulative payoff, they might still choose to cooperate to maintain the spirit of cooperation or if they believe there's still a chance of a slight marginal gain. This is a more altruistic lean.

*   **$m=n$ or $m=n-1$ (Near certainty of cooperation needed):**
    *   The `caution_increment` should be relatively high. ATC will quickly switch to defecting if cooperation dips even slightly below the required threshold. It requires a very strong signal of widespread cooperation.

*   **$m=2$ (Very low threshold):**
    *   ATC will be very forgiving. Even if only one person cooperates (besides itself), it might continue to cooperate if the `DCT` is sufficiently lowered.

### 3. Being Cooperative

ATC is fundamentally designed to be cooperative:

*   **Initial Cooperation:** It always starts by cooperating, acting as a catalyst for collective action.
*   **Rewarding Cooperation:** It lowers its cooperation threshold when cooperation succeeds, actively encouraging and rewarding past successes. This builds trust and reinforces positive behavior.
*   **Forgiveness:** The `Forgiveness Counter` allows for a recovery period after failures. It doesn't immediately punish deviations but waits for a chance to re-establish cooperation, making it resilient to transient defections.
*   **Parameter Awareness:** It uses $m$ and $k$ to inform its decisions. A higher $k$ (greater reward) incentivizes more risk-taking for cooperation. A higher $m$ (higher threshold) requires more confidence in others before cooperating. ATC adjusts its risk tolerance based on these.
*   **Long-Term Focus:** In the absence of a defined end-of-game, ATC assumes it's playing indefinitely, prioritizing sustainable cooperation over short-term gains. The last round logic is an exception for a defined finite game.

## Pseudocode for ATC Strategy

```python
# --- Game Parameters (Provided to the strategy) ---
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward if threshold met factor

# --- Strategy Internal State ---
current_round = 0
total_payoff = 0
history = []  # List of tuples: (player_actions_in_round, player_payoffs_in_round)

# --- Tunable Parameters (Can be adjusted via meta-learning or tournament play) ---
initial_dct = m / n  # Start by aiming for the required threshold
coop_boost_factor = 0.05  # How much to lower DCT when cooperation succeeds (proportion of n)
caution_increment_factor = 0.03  # How much to raise DCT when cooperation fails (proportion of n)
forgiveness_window = 2  # Number of rounds to wait for recovery after a failed cooperation round
forgiveness_threshold_multiplier = 0.9 # If forgiveness active, we might cooperate if OCR >= m/n * this_multiplier

# --- Strategy State Variables ---
dynamic_cooperation_threshold = initial_dct
forgiveness_counter = 0 # Counts how many consecutive rounds we've failed to meet m, or are in recovery mode

def decide_action(game_state):
    """
    Decides the action (C or D) for the current round.
    game_state contains information about current_round, history, etc.
    """
    global current_round, total_payoff, dynamic_cooperation_threshold, forgiveness_counter

    current_round = game_state["current_round"]
    history = game_state["history"]
    
    # --- Handle Round 0 (First Round) ---
    if current_round == 0:
        return 'C'

    # --- Handle Last Round (Round r-1, since rounds are 0-indexed) ---
    if current_round == r - 1:
        # Simple heuristic for last round: If current payoff is below a baseline, defect.
        # Baseline: (r-1) rounds of pure defection + 1 round of expected minimum payoff.
        # This is a simplification; a more robust last round strategy might be needed.
        guaranteed_min_payoff = (r - 1) * 1  # Payoff if everyone defects for r-1 rounds
        # If our current payoff plus a guaranteed 1 for this last round is still low, defect.
        # We assume we played cooperatively in previous rounds.
        # If total_payoff + 1 < (r * 1): # A very basic check, can be improved
        #     return 'D'
        
        # A more sophisticated last round: calculate potential payoff if cooperating vs defecting
        # For simplicity here, let's say if we're significantly behind our peers or a baseline, we defect.
        # This strategy is inherently cooperative, so it might still cooperate even if slightly behind.
        # Let's stick to a more purely cooperative leaning for the last round if no explicit goal is set.
        # For a truly "cooperative" strategy, it might continue cooperating if it has a decent lead.
        # Let's simplify: If the potential reward (k) is still significant and we have a good buffer, cooperate.
        # Otherwise, defect to secure what we have.
        
        # Simple last round decision: If we have secured a decent amount of payoff, try to secure it.
        # If we are far behind, try to catch up.
        # This needs more definition based on tournament goals.
        # For now, let's assume a moderate payoff is achieved and we lean towards securing it.
        if total_payoff >= (r-1): # If we've at least secured average payoff
             return 'C' # Cooperate to be friendly, or if k is high enough to justify risk
        else:
             return 'D' # Defect to maximize gains if behind

    # --- Logic for Intermediate Rounds (1 to r-2) ---
    
    # Calculate observed cooperation rate from the *previous* round
    prev_round_actions = history[-1][0] if history else []
    cooperators_in_prev_round = sum(1 for action in prev_round_actions if action == 'C')
    
    # If it's the second round (history is only round 0), we use round 0's outcome
    if current_round == 1 and not history: # Should not happen if history populated correctly
        cooperators_in_prev_round = 1 # We cooperated in round 0

    observed_cooperation_rate = cooperators_in_prev_round / n

    # --- Update Dynamic Cooperation Threshold (DCT) ---
    
    # Check if previous round met the *actual* m threshold
    previous_round_met_m = (cooperators_in_prev_round >= m)

    if previous_round_met_m:
        # Cooperation succeeded last round, slightly lower DCT to encourage continued cooperation
        # Ensure DCT doesn't go below a reasonable minimum (e.g., slightly above 0)
        dynamic_cooperation_threshold = max(0.1, m/n - coop_boost_factor * n / n) # Use n/n for proportion
    else:
        # Cooperation failed last round, increase DCT to demand more evidence
        # Ensure DCT doesn't exceed 1.0
        dynamic_cooperation_threshold = min(1.0, m/n + caution_increment_factor * n / n)

    # --- Forgiveness Mechanism ---
    if not previous_round_met_m:
        # If cooperation failed, activate or extend forgiveness window
        forgiveness_counter += 1
    else:
        # If cooperation succeeded, reset forgiveness counter
        forgiveness_counter = 0

    # --- Make Decision ---
    
    # Cooperate if:
    # 1. Observed cooperation rate meets or exceeds our dynamic threshold OR
    # 2. Forgiveness is active AND observed cooperation rate is "close enough" to m
    
    cooperate = False
    
    if observed_cooperation_rate >= dynamic_cooperation_threshold:
        cooperate = True
        if forgiveness_counter > 0: # If we were in forgiveness mode and succeeded, reset it
            forgiveness_counter = 0
            
    elif forgiveness_counter > 0 and forgiveness_counter <= forgiveness_window:
        # We are in a forgiveness window. Cooperate if previous round was "close" to m.
        if observed_cooperation_rate >= (m/n) * forgiveness_threshold_multiplier:
            cooperate = True
            # Note: Forgiveness counter remains active until m is met, or window expires.
            # Here, we are cooperating *within* the forgiveness window.
            # If we meet m with cooperation, we reset forgiveness_counter anyway above.
            # If we cooperate here but still don't meet m, forgiveness_counter continues.

    if cooperate:
        return 'C'
    else:
        # Defect if none of the cooperation conditions are met.
        # Ensure forgiveness counter is properly managed if we defect.
        # If we defected and forgiveness was not active, activate it.
        if forgiveness_counter == 0: # Only activate if not already in a forgiveness cycle
            forgiveness_counter = 1 # Start forgiveness counter if we choose to defect now
        return 'D'

# --- How to use (simplified example of game loop) ---
# In a real tournament, this 'decide_action' function would be called by the game engine.
# The game engine would provide the 'game_state' (current_round, history).
# The strategy would also update its 'total_payoff' and 'history' based on results.

# Example of how strategy might be called and updated:
#
# def run_strategy(n, r, m, k):
#     # Initialize strategy state
#     current_round = 0
#     total_payoff = 0
#     history = [] # Stores tuples of (actions_this_round, payoffs_this_round)
#     dynamic_cooperation_threshold = m / n
#     forgiveness_counter = 0
#
#     for round_num in range(r):
#         game_state = {
#             "current_round": round_num,
#             "history": history,
#             "n": n,
#             "m": m,
#             "k": k
#         }
#         my_action = decide_action(game_state)
#
#         # --- Game engine would simulate other players' actions here ---
#         # Assume 'other_players_actions' is a list of actions for other players
#         # Assume 'my_actual_payoff' and 'actions_this_round' are returned by simulation
#
#         # Example: Placeholder for simulation result
#         # For demonstration, let's assume a simple outcome where we get our payoff and record actions.
#         # In a real game, 'my_actual_payoff' and 'actions_this_round' would be calculated by the game engine.
#         
#         # --- This part needs to be provided by the game environment ---
#         # Example:
#         # all_actions_this_round, payoffs_this_round = simulate_round([my_action] + other_players_actions, n, m, k)
#         # my_actual_payoff = payoffs_this_round[my_player_index]
#         # actions_this_round = all_actions_this_round
#         
#         # For now, let's simulate some dummy values to illustrate state updates
#         # In reality, 'actions_this_round' and 'my_actual_payoff' are determined by the environment.
#         # We just need to *record* them to update our state for the *next* round's decision.
#
#         # Placeholder for actual simulation outcome
#         # Let's say for this example round, the outcome was observed as:
#         # other_players_actions_example = ['C', 'D', 'C', 'D', 'C'] # If n=6
#         # actions_this_round = [my_action] + other_players_actions_example
#         # calculated_payoff = calculate_payoff_for_player(my_action, actions_this_round, n, m, k) # Hypothetical function
#         # my_actual_payoff = calculated_payoff
#
#         # --- Update strategy state based on simulated outcome ---
#         # history.append((actions_this_round, payoffs_this_round)) # Need full history, not just my payoff
#         # total_payoff += my_actual_payoff
#         # current_round += 1
#
#     return total_payoff
```
'''

description_COOPERATIVE_66 = '''
This strategy aims to foster cooperation by rewarding past cooperation and punishing defection, while acknowledging the inherent risk of the dilemma. It's designed to be robust and adaptive, adjusting its behavior based on the observed cooperation levels in previous rounds.

## Cooperative Strategy: "Conditional Reward and Punish"

This strategy is built on the principle of reciprocity, with a strong emphasis on encouraging the collective to reach the cooperation threshold.

### 1. Decision Rules

The core of this strategy is to dynamically adjust its cooperation threshold based on the observed success of cooperation in previous rounds.

*   **Initial Tendency:** Start with a strong inclination towards cooperation.
*   **Response to Threshold Met:** If the cooperation threshold (`m`) was met or exceeded in the previous round, maintain or increase the likelihood of cooperating in the current round.
*   **Response to Threshold Not Met:** If the cooperation threshold was *not* met in the previous round, the player will become more cautious and consider defecting. The degree of caution will depend on how close the previous round came to meeting the threshold.

**Specific Decision Logic:**

Let $C_t$ be the number of players who cooperated in round $t$.
Let $h_i(t-1)$ be player $i$'s action in round $t-1$ (1 for Cooperate, 0 for Defect).
Let $H(t-1) = \{h_1(t-1), h_2(t-1), ..., h_n(t-1)\}$ be the set of all player actions in round $t-1$.

**In round $t$ (where $t > 1$):**

*   **If $C_{t-1} \ge m$ (Threshold met or exceeded in the previous round):**
    *   **Cooperate (C)** with a high probability.
    *   This probability will be *fixed* to 1 (always cooperate) to strongly reinforce successful cooperation.

*   **If $C_{t-1} < m$ (Threshold not met in the previous round):**
    *   The strategy will adopt a more cautious approach.
    *   **Calculate the "cooperation deficit":** $D_{deficit} = m - C_{t-1}$.
    *   **If $D_{deficit} = 1$ (Only one player was missing for cooperation):**
        *   **Cooperate (C)** with a moderate probability. The probability of cooperating will be set to $(n-m+1)/n$. This is the probability that if you cooperate, and $m-1$ others cooperate, the threshold is met. This acknowledges that the group was very close.
    *   **If $D_{deficit} > 1$ (More than one player was missing for cooperation):**
        *   **Defect (D)** with a high probability.
        *   The probability of cooperating will be calculated as $\frac{C_{t-1}}{n-1}$. This represents the proportion of *other* players who cooperated. If no one else cooperated, this probability is 0. If almost everyone else cooperated (but not enough to meet the threshold), this probability will be higher. This is a form of **punishment** and **signal of dissatisfaction** with insufficient cooperation.

**In round 1:**

*   **Always Cooperate (C).** This establishes an initial cooperative signal and aims to kickstart the process. It’s a gamble, but it aligns with the cooperative imperative.

### 2. Handling Edge Cases

*   **First Round (r=1):** As stated above, **always Cooperate (C)**. This sets a positive precedent.
*   **Last Round (r):** The strategy's decision logic for the last round is the same as any other round. Since this is a repeated game with no explicit end-game incentive manipulation, the strategy maintains its adaptive behavior throughout. The fact that it's the last round doesn't change the core objective of maximizing its own payoff based on historical cooperation. There's no need for a special "punish at the end" mechanism as it doesn't benefit the player if it doesn't influence future rounds.
*   **Small Number of Players (n close to m):** The strategy is inherently designed to be sensitive to proximity to the threshold. If $n-m$ is small, the strategy will be more likely to cooperate in rounds where the threshold was narrowly missed.
*   **Large Number of Players (n >> m):** The strategy becomes more cautious when the threshold is not met, as individual cooperation has a smaller impact on the overall outcome. The probability adjustments still hold, but the effect might be more pronounced.

### 3. Be Cooperative

The strategy is fundamentally cooperative in its design:

*   **Initial Cooperative Stance:** It begins by cooperating, signaling a willingness to work towards the collective good.
*   **Rewarding Success:** It strongly rewards successful rounds of cooperation by continuing to cooperate. This reinforces positive collective behavior.
*   **Conditional Punishment/Caution:** When cooperation fails, the strategy doesn't immediately abandon cooperation. Instead, it becomes more *cautious* or *punitive* based on the degree of failure. This is a measured response, not an outright rejection of cooperation. It aims to nudge the group back towards cooperation.
*   **Focus on Threshold:** The primary goal is to reach the `m` cooperation threshold, as this benefits all players (especially those who defect in that round, as they still get the private endowment *plus* the reward). The strategy prioritizes achieving this collective benefit.

### Pseudocode Representation

```pseudocode
// Parameters: n, r, m, k

// State variables to be maintained across rounds
let cooperation_threshold_met_last_round = false
let number_of_cooperators_last_round = 0

// Function to decide action for the current round (round_num)
function decide_action(round_num):
  if round_num == 1:
    return C // Always cooperate in the first round

  // For rounds > 1, use historical information
  if cooperation_threshold_met_last_round:
    // If the threshold was met or exceeded, continue to cooperate strongly
    return C
  else:
    // Threshold was not met in the previous round
    let cooperation_deficit = m - number_of_cooperators_last_round

    if cooperation_deficit == 1:
      // Very close to the threshold, give another chance with moderate probability
      // Probability of cooperating if one more player cooperated would have met threshold
      // This ensures that if you cooperate, and m-1 others did, you reach m
      probability_to_cooperate = (n - m + 1.0) / n
      if random_float() < probability_to_cooperate:
        return C
      else:
        return D
    else: // cooperation_deficit > 1
      // Significant failure to cooperate, adopt a more cautious/punitive stance
      // Probability of cooperating is proportional to others' cooperation
      // If number_of_cooperators_last_round is 0, this is 0
      // If number_of_cooperators_last_round is n-1, this is (n-1)/(n-1) = 1
      // This reflects a belief that cooperation is only worthwhile if enough others are doing it.
      if n - 1 > 0: // Avoid division by zero if n=1 (though n>=2 is assumed)
        probability_to_cooperate = number_of_cooperators_last_round / (n - 1.0)
        if random_float() < probability_to_cooperate:
          return C
        else:
          return D
      else: // Should not happen given n >= 2
        return D

// Function to update state after each round
function update_state(current_round_actions):
  // current_round_actions is a list/array of actions (C/D) for all players in the current round
  let cooperators_this_round = 0
  for action in current_round_actions:
    if action == C:
      cooperators_this_round += 1

  number_of_cooperators_last_round = cooperators_this_round
  if cooperators_this_round >= m:
    cooperation_threshold_met_last_round = true
  else:
    cooperation_threshold_met_last_round = false

// Helper function to generate a random float between 0.0 and 1.0
function random_float():
  // Implementation depends on the programming language
  pass

```
'''

description_COOPERATIVE_67 = '''
## Cooperative Strategy: The "Threshold Seeker"

This strategy aims to achieve the cooperative outcome (meeting the threshold `m`) by taking calculated risks, while also protecting against exploitation. It's designed to be adaptive to the observed behavior of other players over time.

### Core Philosophy:

The "Threshold Seeker" believes that cooperation is the path to higher collective payoffs, but only if the risk of cooperating is shared. It will cooperate when it believes its contribution is *necessary* to reach the threshold or when it observes a strong trend towards cooperation from others. Conversely, it will defect to safeguard its own endowment if cooperation seems unlikely to succeed or if it's being exploited.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in a given round `t` depends on the current state of the game, specifically the observed number of cooperators in the *previous* round (`t-1`).

Let `N_C(t-1)` be the number of players who cooperated in round `t-1`.

*   **Cooperate (C) if:**
    *   **Case 1: Threshold is Close:** `m - 1 <= N_C(t-1) < m`. In this situation, if the current player cooperates, the threshold `m` will be met. This is the most crucial condition for triggering cooperation. The player is taking a calculated risk, as their cooperation is the *marginal* unit needed.
    *   **Case 2: Strong Cooperation Trend:** `N_C(t-1) >= m` AND `N_C(t-1) / n >= 0.5`. If the threshold has been met in the previous round, *and* a significant majority of players cooperated (at least 50%), it suggests a generally cooperative environment. The player will continue to cooperate to maintain this positive momentum.
    *   **Case 3: Early Exploration (Round 1 & 2):** In the very first two rounds, the strategy will always cooperate. This is to signal a willingness to cooperate and to gather initial data about other players' tendencies. This allows for a more informed decision in later rounds.

*   **Defect (D) if:**
    *   **Case 1: Low Cooperation Observed:** `N_C(t-1) < m - 1`. If the number of cooperators in the previous round was significantly below the threshold, and the current player's cooperation alone wouldn't tip the balance, they will defect to protect their endowment. The risk of contributing when it's unlikely to succeed is too high.
    *   **Case 2: Exploitation Detected:** `N_C(t-1) < m` AND `(n - N_C(t-1)) / n >= 0.5`. If the threshold was *not* met in the previous round, and a majority of players defected, it indicates an exploitative environment where cooperation is not reciprocated. The player will defect to avoid being the sole cooperator or one of few.
    *   **Case 3: Decline in Cooperation:** `N_C(t-1) < m` AND `N_C(t-1)` is decreasing over the last 2-3 rounds (this is a more advanced heuristic and can be simplified if needed). If there's a clear trend of declining cooperation, the strategy will defect to protect itself.

### 2. Handling Edge Cases:

*   **Round 1:** Always Cooperate (C). This is a proactive signal of cooperative intent and gathers initial data.
*   **Round 2:** Always Cooperate (C). Continues the cooperative signal and gathers more data before applying strict conditional logic.
*   **Last Round (`r`):** This is where the strategy becomes more self-interested, as there are no future rounds to incentivize cooperation.
    *   **If `N_C(r-1) >= m`:** Cooperate (C). The threshold was met in the previous round, so the reward is guaranteed.
    *   **If `N_C(r-1) < m`:** Defect (D). The threshold was not met, and there's no future incentive for cooperation, so it's best to secure the private endowment.
*   **If `n < m` (This is a theoretical edge case, given the parameter constraint `m < n`):** The strategy would always defect, as it's impossible to meet the threshold. However, the problem statement `1 < m < n` prevents this.

### 3. Cooperativity Alignment:

The strategy is fundamentally cooperative because:

*   **Prioritizes Threshold Achievement:** The primary driver for cooperation is to reach the `m` threshold, which benefits all players by unlocking the `k` reward.
*   **Invests in Early Cooperation:** By cooperating in the first two rounds, it signals a commitment to the cooperative outcome and lays the groundwork for future mutual gains.
*   **Adaptive Reciprocity:** It aims to reward observed cooperation by continuing to cooperate when the threshold is met or when a strong cooperative trend is evident. This fosters a cycle of mutually beneficial behavior.
*   **Protects Against Exploitation:** The strategy is not naive. It defects when cooperation is unlikely to succeed or when it's being exploited. This self-preservation is crucial for long-term participation and prevents it from being a perpetual victim, which would ultimately undermine cooperation.

### Pseudocode Representation:

```pseudocode
// Global variables to store history
// history_actions: a list of lists, where history_actions[t-1] is a list of actions in round t-1
// n: number of players
// m: minimum cooperators needed
// k: reward factor
// r: number of rounds

function DecideAction(current_round, history_actions):
    if current_round == 1 or current_round == 2:
        return C // Cooperate in the first two rounds

    // Count cooperators in the previous round
    if current_round > 1:
        previous_round_actions = history_actions[current_round - 2] // Adjust index for 0-based list
        num_cooperators_previous_round = count_C(previous_round_actions)
    else: // Should not happen due to explicit round 1 & 2 handling, but for completeness
        num_cooperators_previous_round = 0 // Or some default if no history

    // Condition for the last round
    if current_round == r:
        if num_cooperators_previous_round >= m:
            return C
        else:
            return D

    // Decision logic for intermediate rounds

    // Case 1: Threshold is Close
    if num_cooperators_previous_round >= (m - 1) and num_cooperators_previous_round < m:
        return C

    // Case 2: Strong Cooperation Trend
    if num_cooperators_previous_round >= m and (num_cooperators_previous_round / n) >= 0.5:
        return C

    // Case 3: Low Cooperation Observed (and current player's cooperation won't tip the balance)
    if num_cooperators_previous_round < (m - 1):
        return D

    // Case 4: Exploitation Detected (majority defected and threshold not met)
    num_defectors_previous_round = n - num_cooperators_previous_round
    if num_cooperators_previous_round < m and (num_defectors_previous_round / n) >= 0.5:
        return D

    // Default to Defect if none of the above conditions are met and cooperation is not clearly beneficial
    return D

function count_C(actions_list):
    count = 0
    for action in actions_list:
        if action == C:
            count = count + 1
    return count
```

This strategy is designed to be robust by reacting to observed behavior. It's not pre-programmed with specific cooperative patterns, making it adaptable to a variety of opponent strategies. Its goal is to achieve the mutual benefit of the cooperative outcome whenever feasible and safe.
'''

description_COOPERATIVE_68 = '''
This strategy aims to foster cooperation in the Collective Risk Dilemma by employing a tit-for-tat-like approach, adapted for the group nature of the game and its repeated structure. The core idea is to signal a willingness to cooperate, reward cooperation when observed, and punish defection by withholding cooperation until a stable level of cooperation is restored.

### Cooperative Strategy: "Adaptive Reciprocity"

The "Adaptive Reciprocity" strategy is designed to be cooperative at its core, seeking to achieve the highest possible collective payoff while being resilient to opportunistic defections. It prioritizes establishing and maintaining a cooperative equilibrium.

#### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on the observed collective behavior in the previous round(s).

*   **Initial Cooperation Signal:** In the very first round, the strategy will **Cooperate (C)**. This is a clear signal of intent to cooperate and to initiate the collective project.

*   **Conditional Cooperation:** For subsequent rounds (t > 1):
    *   **If (Number of cooperators in round t-1) >= m:** This indicates that the group successfully met the cooperation threshold in the previous round. The strategy will **Cooperate (C)** in round `t`. This rewards past cooperation and encourages its continuation.
    *   **If (Number of cooperators in round t-1) < m:** This indicates that the group failed to meet the cooperation threshold.
        *   **Sub-condition: Was I the *only* defector?**
            *   **If Yes:** The strategy will **Defect (D)** in round `t`. This is a direct response to a perceived exploitation, where the strategy cooperated but others did not contribute enough to reach the threshold. It signals that such exploitation is not tolerated.
            *   **If No (i.e., at least m-1 other players defected, or other cooperators defected):** The strategy will **Cooperate (C)** in round `t`. This is a more forgiving approach. If the failure to meet the threshold was due to widespread defection (not just the strategy's single defection), it assumes a collective breakdown in cooperation and attempts to "re-seed" cooperation by continuing to play C. This is a crucial aspect of being truly cooperative – not to punish the entire group for a failure, but to try and rectify the situation.

*   **Reinforcement:** If the strategy has been punished (i.e., defected in a round because others defected when it cooperated), it will continue to **Defect (D)** in subsequent rounds **until** the condition "(Number of cooperators in the previous round) >= m" is met. Once this threshold is achieved, the strategy will resume **Cooperating (C)**. This ensures that punishment is temporary and aimed at restoring the cooperative norm.

#### 2. Handling Edge Cases:

*   **First Round (t = 1):**
    *   **Decision:** **Cooperate (C)**.
    *   **Rationale:** This is the initial signal. Without any history, starting with cooperation is the most cooperative action and the best chance to kickstart a successful collective endeavor.

*   **Last Round (t = r):**
    *   **Decision:** The strategy follows the same rules as any other round. However, the *implication* of the last round is that there are no future rounds to build trust or seek revenge.
    *   **Rationale:** While the strategy doesn't explicitly change its decision logic for the last round, the underlying incentive structure of the game itself will influence behavior. The strategy's rule of responding to the *previous* round's outcome means its action in the last round is determined by the (r-1)th round. There's no need for a special "retaliation" or "reward" phase for the final round's outcome. It simply acts based on the established pattern. This is consistent with being a strategy that aims for long-term cooperation, even if the last round doesn't have future consequences.

*   **Insufficient Data (Very few past rounds, e.g., t < 3):**
    *   The rules are designed to handle this naturally. In t=2, it depends on t=1. In t=3, it depends on t=2. The rules are robust to small numbers of past rounds.

*   **What if `m` is very close to `n`? (e.g., `m = n-1`)**
    *   The strategy remains the same. If `m` is high, it means that nearly everyone needs to cooperate. The strategy's "if I was the only defector" rule becomes more important. If it defects and the threshold isn't met, and it was the *sole* reason, it might continue defecting. However, the more forgiving "if at least m-1 other players defected" clause is critical here. If `m` is very high, and a few players defect, it's more likely that the "at least m-1 others defected" condition will be met, leading the strategy to continue cooperating to try and pull the group back from the brink.

#### 3. Being Cooperative:

This strategy is inherently cooperative for several reasons:

*   **Initiates Cooperation:** It always starts by cooperating, taking the first step towards a collective benefit.
*   **Rewards Cooperation:** It continuously cooperates when the group successfully meets the cooperation threshold, reinforcing the desired behavior.
*   **Forgiving Defection (mostly):** It only defects if it has cooperated and its cooperation was met with exploitative defection (i.e., it was the sole defector when the threshold was not met). If the threshold failure is due to more general widespread defection, it continues to cooperate, attempting to "re-seed" the cooperative norm. This is a key aspect of its cooperative nature – not to abandon the collective good easily.
*   **Temporary Punishment:** Any punishment (defection) is temporary and conditional. It stops as soon as the group demonstrates a return to a cooperative state (meeting the threshold).
*   **Maximizes Collective Payoff:** By aiming to establish and maintain the cooperative equilibrium (where `m` or more players cooperate), the strategy seeks to maximize the overall payoff for all players, which is the ultimate goal of cooperation.

#### Pseudocode Representation:

```pseudocode
Strategy "Adaptive Reciprocity":

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

State Variables:
  my_past_actions: list of player i's actions in previous rounds
  group_past_actions: list of lists, where each inner list is a set of actions of all players in a past round

Function decide_action(current_round, my_past_actions, group_past_actions):
  // current_round is 1-indexed

  // Rule 1: First Round
  if current_round == 1:
    return C // Cooperate

  // Rule 2: Subsequent Rounds
  previous_round_actions = group_past_actions[current_round - 2] // 0-indexed for lists
  num_cooperators_previous_round = count_C(previous_round_actions)
  my_action_previous_round = my_past_actions[current_round - 2]

  if num_cooperators_previous_round >= m:
    return C // Reward cooperation, continue cooperation

  else: // num_cooperators_previous_round < m
    // Check if I was the only defector responsible for failure
    my_decision_to_cooperate_in_previous_round = (my_action_previous_round == C)
    num_other_defectors_in_previous_round = 0
    for player_index from 0 to n-1:
      if player_index != my_player_id and previous_round_actions[player_index] == D:
        num_other_defectors_in_previous_round += 1

    // Condition for severe punishment: I cooperated, threshold missed, and I was the *only* one who needed to switch
    // This implies that if I had cooperated, AND at least m-1 others also cooperated, the threshold would have been met.
    // A simpler way to think about it: if I defected, and still m players defected, I have no incentive to switch back.
    // But if I cooperated, and the threshold was missed, and I was the *sole* reason it was missed (because if I had defected, more would have cooperated), then I am being exploited.
    // Let's refine this: if my cooperation did not lead to reaching the threshold, AND others defected, I should still cooperate unless I was the *sole* cooperator and m > 1.

    // The core idea: If the group failed, and I was trying to cooperate, when do I punish?
    // I punish if I cooperated, the threshold was missed, AND my cooperation was *not* enough, *and* others defected.
    // If I was the *sole* cooperator and m > 1, and threshold was missed, then I was exploited.
    // If m <= 1, then m=1, and I am the only cooperator, threshold is met. This case won't happen.

    // Let's simplify the "sole defector" logic:
    // If the threshold was not met:
    //   If I cooperated in the previous round, AND the number of other cooperators + my cooperation < m,
    //   AND the number of other cooperators < m-1, then I was exploited.
    //   This can be simplified: If I cooperated, and the total cooperators were < m,
    //   and *only* one player (me) cooperated, then I will defect.

    num_cooperators_excluding_me = 0
    for action in previous_round_actions:
        if action == C:
            num_cooperators_excluding_me += 1

    // If I cooperated, and the group still failed, AND my cooperation was necessary and absent
    // This is hard to perfectly detect without knowing individual defector's payoffs.
    // A more practical approach: If the group failed, and I *was* cooperating, and the number of *other* cooperators was low (e.g., less than m-1), then I'll defect.
    // This assumes the worst of other players when cooperation breaks down significantly.

    // Refined rule: Punish if I cooperated, threshold missed, and there weren't *enough* other cooperators to suggest widespread goodwill.
    if my_action_previous_round == C and num_cooperators_previous_round < m:
        if num_cooperators_excluding_me < (m - 1): // If even with my cooperation, there weren't enough others
            return D // Punish, I was exploited or others didn't try hard enough
        else:
            return C // Others tried, continue cooperating to build
    else: // I defected in the previous round, or the rule above didn't trigger
        return C // Resume cooperation if punishment period is over, or if I wasn't the one punished.

// Helper function
function count_C(actions_list):
  count = 0
  for action in actions_list:
    if action == C:
      count += 1
  return count

// Note: This pseudocode implies the strategy has access to its own player ID and the full history of actions.
// The "num_other_defectors_in_previous_round" calculation might be complex to track directly.
// The refined "num_cooperators_excluding_me < (m-1)" is a more direct interpretation of being exploited.
// If I cooperated, and num_cooperators_previous_round < m, and num_cooperators_excluding_me < m-1,
// this means that even with my cooperation, there weren't enough other cooperators to reach m.
// So, my cooperation was not sufficient, AND others didn't compensate for it.
// This triggers a defection to signal this.

// A simpler, perhaps more robust rule if direct exploitation detection is tricky:
// If threshold not met:
//   If I cooperated in previous round, AND num_cooperators_previous_round < m:
//      return D // If my cooperation didn't lead to success, and I was part of the group trying to cooperate, I defect to signal.
//   Else (I defected, or threshold was met):
//      return C // Resume cooperation, or continue defecting if I was punished (implicitly handled by next round's logic).

// Let's go with a slightly simplified version of the punishment trigger to ensure clarity and avoid complex state tracking.

Strategy "Adaptive Reciprocity" (Revised Pseudocode):

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

State Variables:
  my_past_actions: list of player i's actions in previous rounds
  group_past_actions: list of lists, where each inner list is a set of actions of all players in a past round

Function decide_action(current_round, my_past_actions, group_past_actions):

  // Rule 1: First Round
  if current_round == 1:
    return C // Cooperate

  // Rule 2: Subsequent Rounds
  previous_round_actions = group_past_actions[current_round - 2] // 0-indexed for lists
  num_cooperators_previous_round = count_C(previous_round_actions)
  my_action_previous_round = my_past_actions[current_round - 2]

  if num_cooperators_previous_round >= m:
    return C // Reward cooperation, continue cooperation

  else: // num_cooperators_previous_round < m (Threshold not met)
    // If I cooperated in the previous round AND the threshold was not met,
    // this implies my cooperation was not enough, and others did not compensate.
    // I will defect to signal this failure of collective action.
    if my_action_previous_round == C:
      return D // Punish the lack of collective success when I tried to cooperate.
    else:
      // If I defected in the previous round, I will now resume cooperating.
      // This is because the punishment mechanism is meant to be temporary.
      // The next round will evaluate the outcome of *this* round's decision.
      return C // Resume cooperation after a prior defection, or if the previous round was a punishment phase.

// Helper function
function count_C(actions_list):
  count = 0
  for action in actions_list:
    if action == C:
      count += 1
  return count
```
'''

description_COOPERATIVE_69 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive, robust, and aligned with a cooperative mindset, while adhering to the game's constraints.

## Cooperative Strategy: "Conditional Reciprocity with Threshold Awareness"

This strategy is based on the principle of **conditional reciprocity** – cooperate as long as cooperation is being reciprocated or is likely to be beneficial – coupled with a keen awareness of the **cooperation threshold (m)**. It aims to foster cooperation by demonstrating a willingness to contribute, while also protecting itself from exploitation.

### 1. Decision Rules: When to Cooperate vs. Defect

The core of this strategy lies in observing the collective behavior of the group in previous rounds and making a decision based on the likelihood of reaching the cooperation threshold *m*.

**For any round *t* (where *t* > 1):**

*   **Cooperate (C) if:**
    *   **The number of cooperators in the previous round (*t-1*) was *m* or greater.** This indicates that the group successfully met the threshold, and we should reward this collective success by continuing to cooperate.
    *   **The number of cooperators in the previous round (*t-1*) was *m-1* or greater, AND this is NOT the last round (*t < r*).** This is a crucial adaptive element. If the group was very close to the threshold in the previous round, we will make an extra cooperative effort to push the group over the edge, especially if there are more rounds left for this effort to pay off. This demonstrates a strong commitment to achieving the collective good.
    *   **The number of cooperators in the previous round (*t-1*) was *m-2* or greater, AND this is NOT the last round (*t < r*), AND *m* is relatively small compared to *n* (e.g., *m <= n/2*).** This is a more aggressive form of conditional cooperation, aimed at initiating a cooperative cascade. If the group is still somewhat behind but showing promise, and the threshold isn't excessively high, we'll take a calculated risk to encourage more cooperation. The condition `m <= n/2` is a heuristic to avoid over-investing in scenarios where the threshold is very difficult to meet.

*   **Defect (D) if:**
    *   **The number of cooperators in the previous round (*t-1*) was *less than m-2*.** If the group is significantly below the threshold, and our previous cooperation efforts (if any) didn't yield sufficient results, we will defect to protect our endowment. It's no longer rational to repeatedly incur the cost of cooperation if the collective outcome is consistently failing.
    *   **It is the last round (*t = r*), and the number of cooperators in the previous round (*t-1*) was less than *m*.** In the final round, there's no future to incentivize. If the threshold wasn't met, defecting is the best individual strategy to maximize payoff from the last round.

**A simplified decision logic for round *t* > 1:**

Let `cooperators_t_minus_1` be the number of players who cooperated in round *t-1*.

*   **If `cooperators_t_minus_1 >= m`:** Cooperate.
*   **Else if `cooperators_t_minus_1 >= m - 1` AND `t < r`:** Cooperate.
*   **Else if `cooperators_t_minus_1 >= m - 2` AND `t < r` AND `m <= n / 2`:** Cooperate.
*   **Else (in all other cases, including the last round if the threshold wasn't met):** Defect.

### 2. Handling Edge Cases

*   **First Round (t = 1):**
    *   **Decision:** **Cooperate (C)**.
    *   **Rationale:** In the absence of any history, the most cooperative and optimistic approach is to lead by example. This signals a commitment to the collective good and can potentially initiate a virtuous cycle of cooperation. It aligns with the "cooperative mindset" by taking the first step.

*   **Last Round (t = r):**
    *   **Decision:** This is already incorporated into the decision rules above. If the previous round's cooperation count (`cooperators_r_minus_1`) met or exceeded *m*, we will cooperate. Otherwise, we will defect.
    *   **Rationale:** As explained, in the final round, there's no future incentive for cooperation. The decision is purely about maximizing individual payoff based on the outcome of the penultimate round.

*   **Few players (n is small, close to m):**
    *   The strategy naturally adapts. If *n* is only slightly larger than *m*, the conditions `m-1` and `m-2` become more significant. For example, if *n=4, m=3*, then `m-1=2` and `m-2=1`. This strategy would cooperate if at least 1 player cooperated in the previous round (if `t<r` and `m<=n/2` condition met) or 2 players cooperated (if `t<r` condition met). This demonstrates robustness to different parameter combinations.

*   **High threshold (m is close to n):**
    *   The strategy becomes more cautious. If *m* is very high, it will be harder to meet the `cooperators_t_minus_1 >= m` or `m-1` conditions. The strategy will only cooperate if it sees a very strong signal of collective effort or a near-miss in the previous round. The `m <= n/2` condition will also be less likely to be met, preventing overly optimistic gambles when the threshold is extremely high.

### 3. Being Cooperative

*   **Leading by Example:** Cooperating in the first round is a strong signal of cooperative intent.
*   **Rewarding Success:** Always cooperating when the threshold was met in the previous round reinforces positive collective outcomes.
*   **Encouraging Near-Success:** Willingness to cooperate when the group was *m-1* or *m-2* cooperators away from success (especially if not the last round) is a proactive effort to nudge the group towards the beneficial outcome. This is a key driver of sustained cooperation.
*   **Focus on Collective Benefit:** The strategy's success is directly tied to the group's ability to reach *m*. By striving to meet *m*, the strategy is inherently working towards the collective good, as a successful cooperation yields a higher payoff for everyone involved.
*   **Robustness:** By defecting when cooperation repeatedly fails, the strategy is not being naive. It protects its own endowment from being perpetually exploited, which is a necessary condition for sustained participation in a cooperative endeavor.

### Pseudocode

```
Strategy: Conditional Reciprocity with Threshold Awareness

Parameters:
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

History:
- previous_cooperators_count: integer (number of players who cooperated in the last round)

Function DecideAction(current_round_t, history):
  // Handle First Round
  if current_round_t == 1:
    return C // Cooperate

  // Get history from the previous round
  previous_cooperators_count = history.get_previous_cooperators_count()

  // Decision Rules for subsequent rounds
  if previous_cooperators_count >= m:
    return C // Reward successful cooperation
  else if previous_cooperators_count >= m - 1 and current_round_t < r:
    return C // Push for cooperation if close and not the last round
  else if previous_cooperators_count >= m - 2 and current_round_t < r and m <= n / 2:
    return C // Take a calculated risk to initiate cooperation cascade
  else:
    return D // Defect if cooperation is consistently failing or it's the last round without success

// The algorithm needs to:
// 1. Track the number of cooperators in each round.
// 2. Store this count for the previous round.
// 3. Call DecideAction for each round.
// 4. Update the history with the current round's cooperator count for the next round.
```

This strategy provides a clear, adaptive, and cooperative framework for navigating the Collective Risk Dilemma. Its reliance on observable history and logical thresholds allows it to respond effectively to various player behaviors while consistently aiming for the mutually beneficial outcome of meeting the cooperation threshold.
'''

description_COOPERATIVE_70 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust while aligning with a cooperative mindset.

## Cooperative Strategy: The "Conditional Reciprocity and Threshold Awareness" (CRTA) Strategy

This strategy aims to achieve cooperation by rewarding it when others cooperate and defecting when cooperation seems unlikely or is exploited. It's based on observing the collective behavior and adjusting its own actions accordingly.

### Core Principles:

1.  **Default to Cooperation:** Start with a cooperative stance, believing in the possibility of collective benefit.
2.  **Reciprocity:** Reward cooperation from others with continued cooperation. Punish defection with defection.
3.  **Threshold Awareness:** Actively track the number of cooperators and understand the proximity to the cooperation threshold (`m`).
4.  **Risk Management:** Be mindful of the downside of cooperation (potential loss of endowment if threshold isn't met) and the upside of defection (guaranteed private endowment).
5.  **Forward-Looking:** Consider the long-term implications of actions, especially in the later rounds.

### 1. Decision Rules:

The decision to cooperate (C) or defect (D) for player `i` in round `t` depends on the observed history of the game and the current state. Let:

*   `C_t`: The number of players who cooperated in round `t`.
*   `MyAction_{t-1}`: Player `i`'s action in the previous round (`t-1`).
*   `OthersActions_{t-1}`: A record of the actions of all other `n-1` players in round `t-1`.

**Decision Logic:**

*   **For Round 1 (t=1):**
    *   **Always Cooperate (C).**
    *   *Rationale:* Initiate cooperation with a full endowment contribution. This sets a positive tone and allows for observation of how others respond to initial cooperation. It's a proactive step towards realizing the collective benefit.

*   **For Rounds t > 1:**
    *   **Cooperate (C) if EITHER of the following conditions is met:**
        1.  **High Confidence in Threshold:** `C_{t-1} >= m - 1` (meaning, including my own potential cooperation, the threshold `m` is very likely to be met or exceeded).
            *   *Rationale:* If the previous round had `m-1` or more cooperators, my cooperation will guarantee that the threshold is met, securing the `k` reward for everyone. This is the most direct path to maximizing collective payoff.
        2.  **Recent Cooperative History and Mutual Cooperation:** `MyAction_{t-1} == C` AND `C_{t-1} >= m` (meaning, I cooperated last round, and the threshold was met or exceeded).
            *   *Rationale:* This signifies a successful cooperative outcome where everyone benefited. By continuing to cooperate, I am reciprocating the successful cooperation of others and reinforcing the norm of cooperation.

    *   **Defect (D) if EITHER of the following conditions is met:**
        1.  **Low Likelihood of Threshold and Past Exploitation:** `C_{t-1} < m - 1` AND `MyAction_{t-1} == C` (meaning, the threshold was not met last round, and I cooperated, but it wasn't enough to secure the reward).
            *   *Rationale:* This indicates that my cooperation alone was insufficient to trigger the reward, and the group collectively failed to reach `m`. To protect my endowment from being wasted in a failed cooperative effort, I will defect to secure my private payoff. This also acts as a signal that cooperation needs to be more widespread.
        2.  **Consistent Defection from Others and Low Cooperation:** `C_{t-1} < m - 1` AND (less than `m-1` other players cooperated in `t-1`) AND `MyAction_{t-1} == C` (This is a more nuanced condition for when my cooperation is wasted because many others defect).
            *   *Rationale:* If the number of cooperators is significantly below the threshold, and I was one of the few cooperators, and my cooperation didn't lead to success, I will defect. This prevents me from being the sole contributor to a failed project.
        3.  **Lateness and Lack of Progress:** `t == r` (last round) AND `C_{t-1} < m` (threshold was not met in the penultimate round).
            *   *Rationale:* In the final round, there's no future to incentivize through cooperation. If the threshold hasn't been reliably met in previous rounds, and there's no hope of it being met now, it's best to defect and secure the private payoff.

    *   **Otherwise (Default to Defect):** If none of the above cooperation conditions are met, **Defect (D).**
        *   *Rationale:* This serves as a safe default. If cooperation isn't clearly beneficial or reciprocated, the strategy prioritizes securing the private payoff. This also acts as a strong disincentive for others to exploit a cooperative player who doesn't see a path to success.

### 2. Handling Edge Cases:

*   **First Round (t=1):** As specified above, **always Cooperate (C).** This is a critical setup for observing behavior.
*   **Last Round (t=r):**
    *   If `C_{t-1} >= m` (threshold was met in the previous round): **Cooperate (C).** Even in the last round, if cooperation led to success, it's rational to continue it to secure the final reward.
    *   If `C_{t-1} < m` (threshold was not met in the previous round): **Defect (D).** There's no future benefit to cooperate if the threshold is unlikely to be met, and the risk of losing endowment is high.
*   **Insufficient Data (Early Rounds):** The strategy relies on `C_{t-1}`. For `t=2`, this uses data from `t=1`. The strategy is inherently designed to gather data from the first round onwards.

### 3. Cooperative Mindset Alignment:

The strategy is inherently cooperative because:

*   **Initial Cooperative Stance:** It *always* starts by cooperating, giving the collective benefit a chance.
*   **Reward for Cooperation:** It explicitly rewards successful cooperative outcomes by continuing to cooperate, reinforcing the benefits of the collective action.
*   **Threshold-Driven Cooperation:** It prioritizes cooperation when it's most likely to succeed and benefit everyone, actively working towards the `m` threshold.
*   **Adaptive, Not Punitive:** While it defects when cooperation fails, this is a protective measure against exploitation and a signal for more robust cooperation, rather than an act of aggression. It's a "tit-for-tat" inspired approach but with a strong emphasis on group success.
*   **No Preemptive Defection:** It doesn't defect unless there's evidence that cooperation is not working or is being exploited.

### Pseudocode:

```python
class CRTAStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.history = [] # List of tuples: (round_num, my_action, others_actions_list)
        self.my_last_action = None

    def decide_action(self, current_round):
        # Reset history for each new game if needed, or manage state across rounds.
        # Assuming history is managed externally and passed or updated internally per round.

        if current_round == 1:
            action = 'C'
        else:
            # Analyze previous round's outcome
            prev_round_data = self.history[-1] # Get data from the most recent completed round
            prev_round_num, prev_my_action, prev_others_actions = prev_round_data

            # Count cooperators in the previous round
            cooperators_last_round = prev_others_actions.count('C') + (1 if prev_my_action == 'C' else 0)

            # Decision rules
            if cooperators_last_round >= self.m - 1:
                # Condition 1: High confidence in threshold
                action = 'C'
            elif prev_my_action == 'C' and cooperators_last_round >= self.m:
                # Condition 2: Recent cooperative history and mutual cooperation
                action = 'C'
            elif cooperators_last_round < self.m - 1 and prev_my_action == 'C':
                # Condition 3: Low likelihood of threshold and past exploitation (I cooperated, but failed)
                action = 'D'
            elif current_round == self.r and cooperators_last_round < self.m:
                # Condition 4: Lateness and lack of progress (final round, threshold not met)
                action = 'D'
            else:
                # Default to Defect
                action = 'D'

        # Store current action for next round's analysis
        self.my_last_action = action
        return action

    def record_round_outcome(self, round_num, my_action, others_actions_list):
        # others_actions_list should contain actions of ALL other n-1 players
        self.history.append((round_num, my_action, others_actions_list))

# Example Usage (within a tournament simulation):
# strategy = CRTAStrategy(n=6, r=10, m=3, k=2.0)
# for t in range(1, r + 1):
#     my_move = strategy.decide_action(t)
#     # ... other players make moves ...
#     all_moves_this_round = [my_move] + other_players_moves
#     # Need to parse actions relevant to strategy's knowledge
#     others_moves_for_me = all_moves_this_round[:my_index] + all_moves_this_round[my_index+1:]
#     strategy.record_round_outcome(t, my_move, others_moves_for_me)
```

This strategy is designed to be robust by focusing on observed outcomes and incentivizing collective success. It aims to exploit the repeated nature of the game to build trust and cooperation, while having a fallback to protect against losses when cooperation is not viable.
'''

description_COOPERATIVE_71 = '''
## Cooperative Strategy: The "Threshold Sentinel"

This strategy aims to foster cooperation by acting as a vigilant guardian of the collective benefit. It prioritizes achieving the cooperation threshold while minimizing the risk of individual loss. The core idea is to be a reliable cooperator when cooperation is likely to succeed, and to defect when the risk of failure is too high, thus incentivizing others to cooperate in the future.

### Strategy Name: Threshold Sentinel

### Core Philosophy:

The Threshold Sentinel believes in the power of collective action to unlock greater rewards. It is willing to take the risk of cooperating if there's a reasonable chance of reaching the cooperation threshold ($m$). If that chance diminishes, or if the cost of failed cooperation outweighs the potential benefit, it will strategically defect to preserve its resources. The strategy is designed to be **adaptive**, learning from the history of play to inform its decisions.

---

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) for player $i$ in round $t$ is determined by an **Expected Cooperation Probability (ECP)** calculation.

**Threshold Sentinel's Decision Rule:**

*   **If ECP > Confidence Threshold:** Player $i$ plays **C**.
*   **If ECP ≤ Confidence Threshold:** Player $i$ plays **D**.

**Confidence Threshold:** This is a tunable parameter, but for a baseline cooperative strategy, we can set it to a value that represents a strong belief in reaching the threshold. A good starting point is **0.75**. This means the Sentinel will only cooperate if it believes there's at least a 75% chance the cooperation threshold will be met.

**Calculating Expected Cooperation Probability (ECP):**

The ECP is an estimation of the probability that the number of cooperators in the current round will be $\ge m$. This estimation is based on the observed behavior of other players in previous rounds.

Let $N_{cooperators, t-1}$ be the number of players who cooperated in round $t-1$.
Let $N_{defectors, t-1}$ be the number of players who defected in round $t-1$.
Let $n$ be the total number of players.

The ECP for round $t$ is calculated as follows:

*   **Baseline ECP:** This is the proportion of players who cooperated in the *previous* round.
    $ECP_{baseline} = N_{cooperators, t-1} / n$

*   **Adjusted ECP (Momentum Factor):** We introduce a "momentum factor" to account for recent trends. If cooperation increased in the last round, we slightly boost the ECP. If it decreased, we slightly reduce it.

    Let $\Delta N_{cooperators} = N_{cooperators, t-1} - N_{cooperators, t-2}$ (where $t-2$ is the round before the previous one).

    $ECP_{adjusted} = ECP_{baseline} + \alpha \times (\Delta N_{cooperators} / n)$

    Where $\alpha$ is a small positive momentum factor (e.g., $\alpha = 0.1$). This factor prevents drastic swings based on a single round's anomaly.

*   **Incorporating the Sentinel's own past behavior:** The Sentinel should consider its own contribution to past successes or failures. If the Sentinel cooperated in the previous round and the threshold was met, it increases its confidence. If it cooperated and the threshold was *not* met, it becomes more cautious.

    *   If Player $i$ played C in round $t-1$ AND $N_{cooperators, t-1} \ge m$: $ECP_{final} = ECP_{adjusted} + \beta$ (where $\beta$ is a small confidence boost, e.g., $\beta = 0.05$)
    *   If Player $i$ played C in round $t-1$ AND $N_{cooperators, t-1} < m$: $ECP_{final} = ECP_{adjusted} - \gamma$ (where $\gamma$ is a small caution factor, e.g., $\gamma = 0.05$)
    *   If Player $i$ played D in round $t-1$: $ECP_{final} = ECP_{adjusted}$

*   **Final ECP Bounding:** The ECP should be capped between 0 and 1.
    $ECP = \max(0, \min(1, ECP_{final}))$

**Simplified ECP for initial rounds (or if history is insufficient):**

In the very early rounds, or if the momentum calculation becomes unstable (e.g., insufficient history for $t-2$), we can use a simpler ECP.

*   **Round 1:** ECP is initialized to a conservative value, reflecting initial uncertainty. A good starting ECP could be based on a rough estimate of what $m$ would represent if it were a random guess: $ECP = m/n$. However, to be more cooperative, we can set a slightly higher ECP to signal intent. Let's try: $ECP = \max(0.5, m/n)$.
*   **Round 2:** Use $N_{cooperators, 1} / n$ as the baseline, then apply the momentum factor if possible.

---

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Decision:** Cooperate (C).
    *   **Rationale:** The Threshold Sentinel is inherently cooperative and wishes to signal this intent. By cooperating in the first round, it aims to encourage others to do the same. The initial ECP is set to a value that slightly favors cooperation (as described above). The Sentinel is willing to take a small initial risk to set a cooperative tone.

*   **Last Round (t=r):**
    *   **Decision:** Defect (D).
    *   **Rationale:** In the final round, there is no future to incentivize. The primary goal becomes maximizing individual payoff. Since there's no repeated interaction, the risk of defecting and discouraging future cooperation is eliminated. Therefore, the Sentinel chooses to keep its endowment. This is a standard behavior in finite repeated games to avoid backward induction issues that might otherwise lead to defection even earlier.

*   **Insufficient History for ECP Calculation:**
    *   As mentioned in section 1, for early rounds ($t<3$), use a simplified ECP.
    *   If at any point, the historical data becomes corrupted or unavailable, revert to a default cooperative stance (e.g., cooperate if $m/n$ is sufficiently high, otherwise defect).

*   **$m \le 1$ or $m \ge n$:** These parameters violate the problem statement ($1 < m < n$). If they were to occur, the strategy would need to be adapted. For example, if $m=1$, cooperation is always rewarded, so the strategy would always cooperate. If $m=n$, cooperation is very difficult, and the strategy would likely defect more often. However, sticking to the given constraints:

*   **$k \le 1$:** If the reward factor is not significantly greater than the cost of cooperating, the incentive for collective action diminishes. The strategy's confidence threshold might need to be adjusted higher, or it might lean towards defecting more often as the risk/reward becomes less favorable. For the default cooperative strategy, we assume $k > 1$.

---

### 3. Cooperativity:

The Threshold Sentinel is designed to be cooperative in spirit and action, particularly in its early-game behavior and its underlying intention to achieve the collective benefit.

*   **Proactive Cooperation:** It initiates cooperation in the first round, signaling a desire for a mutually beneficial outcome.
*   **Adaptive Learning:** The ECP mechanism allows the Sentinel to learn from the game's dynamics. If others are consistently cooperating and making the threshold achievable, the Sentinel will increase its confidence and cooperate more.
*   **Incentive Alignment:** By defecting only when the probability of success is low, the Sentinel aims to "punish" non-cooperative behavior by not contributing to a failed project, and by extension, encouraging others to cooperate more in the future to avoid such failures. It doesn't seek to exploit others but rather to ensure the success of the collective endeavor.
*   **Focus on Shared Gain:** The strategy's decision-making is fundamentally tied to the possibility of achieving the "reward if threshold met" ($k$), which is a clear indicator of collective benefit.

---

### Pseudocode Representation:

```pseudocode
// Strategy: Threshold Sentinel

// Parameters (Global)
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward if threshold met factor (assumed k > 1)

// Tunable Parameters
CONFIDENCE_THRESHOLD = 0.75 // Probability threshold for cooperation
MOMENTUM_FACTOR = 0.1     // Influence of recent trends
COOPERATION_BOOST = 0.05  // Boost for successful past cooperation
CAUTION_FACTOR = 0.05     // Reduction for failed past cooperation

// State variables (Per player, maintained across rounds)
previous_actions = array of size n, initialized to unknown
my_previous_action = unknown
round_history = [] // List of (num_cooperators, num_defectors) for each round

function decide_action(current_round):
    if current_round == r: // Last round
        return D

    // Calculate ECP
    ecp = 0.0

    if current_round == 1:
        // Initial ECP: Slightly favors cooperation or uses m/n
        ecp = max(0.5, m / n)
    else:
        // Use historical data for ECP calculation
        num_cooperators_prev = round_history[current_round - 2][0] // Index is 0-based
        num_defectors_prev = round_history[current_round - 2][1]

        // Baseline ECP from previous round's cooperation rate
        baseline_ecp = num_cooperators_prev / n

        // Momentum factor
        ecp_adjusted = baseline_ecp
        if current_round > 2:
            num_cooperators_prev_prev = round_history[current_round - 3][0]
            delta_cooperators = num_cooperators_prev - num_cooperators_prev_prev
            ecp_adjusted = baseline_ecp + MOMENTUM_FACTOR * (delta_cooperators / n)
        ecp_adjusted = max(0, min(1, ecp_adjusted)) // Clamp between 0 and 1

        // Incorporate own past behavior
        ecp_final = ecp_adjusted
        if my_previous_action == C:
            if num_cooperators_prev >= m:
                ecp_final = ecp_adjusted + COOPERATION_BOOST
            else: // Cooperated but threshold not met
                ecp_final = ecp_adjusted - CAUTION_FACTOR
        // If my_previous_action == D, ecp_final remains ecp_adjusted

        ecp = max(0, min(1, ecp_final)) // Clamp final ECP

    // Decision based on ECP
    if ecp > CONFIDENCE_THRESHOLD:
        return C
    else:
        return D

// --- Game Loop Interaction ---
// At the end of each round:
// 1. Record my_action for the current round in my_previous_action.
// 2. Observe actions of all other players.
// 3. Count total_cooperators and total_defectors for the current round.
// 4. Append (total_cooperators, total_defectors) to round_history.
// 5. Update previous_actions for all players.
```
'''

description_COOPERATIVE_72 = '''
## Cooperative Strategy: The "Adaptive Threshold Cautious Cooperate" (ATCC) Strategy

This strategy aims to maximize the collective payoff by promoting cooperation while safeguarding against exploitation. It is designed to be adaptive to the observed behavior of other players and robust to varying levels of defection. The core idea is to be willing to cooperate, especially early on and when cooperation seems to be succeeding, but to become more cautious and defect if the threshold for cooperation is consistently unmet.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect in any given round will be based on the player's observation of the previous round's outcome, specifically the number of players who cooperated.

**The ATCC strategy's decision rule for player *i* in round *t* (where *t* > 1) is as follows:**

*   **Cooperate (C) if:**
    *   The number of cooperators in the previous round (*t-1*) was *greater than or equal to* the cooperation threshold (*m*).
    *   **AND** the current round is *not* the final round (*r*).
*   **Defect (D) if:**
    *   The number of cooperators in the previous round (*t-1*) was *less than* the cooperation threshold (*m*).
    *   **OR** the current round *is* the final round (*r*).

**Rationale:**

*   **Promoting Cooperation:** By cooperating when the threshold was met in the previous round, the strategy reinforces successful cooperation and incentivizes players to continue contributing. This creates a positive feedback loop.
*   **Adaptability and Robustness:** If cooperation falters (fewer than *m* players cooperate), the strategy defects. This is crucial to avoid being exploited. If all other players defect, and this strategy continues to cooperate, it will receive the lowest possible payoff (0 from contribution, 0 from reward). By defecting when the threshold is not met, the strategy ensures it at least gets its endowment back (payoff of 1).
*   **Maximizing Reward:** The strategy actively seeks to reach the cooperation threshold to secure the reward multiplier (*k*).

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision Rule:** The ATCC strategy will **Cooperate (C)** in the first round.
    *   **Rationale:** In the absence of any historical data, the strategy takes an optimistic leap of faith to initiate cooperation. This is a necessary step to try and establish a cooperative norm and potentially trigger the reward early. It's a calculated risk, but without it, cooperation might never begin.

*   **Last Round (t=r):**
    *   **Decision Rule:** The ATCC strategy will **Defect (D)** in the last round.
    *   **Rationale:** In the final round, there is no future to incentivize cooperation. Players who have been cooperating may be tempted to defect to secure their endowment without the risk of others defecting and leaving them with nothing. To avoid being the only cooperator and thus losing their endowment, the strategy defects to guarantee a payoff of 1. This is a form of "grim trigger" for the final round, ensuring self-preservation.

### 3. Being Cooperative

The ATCC strategy is inherently cooperative in its design:

*   **Initial Trust:** It begins by cooperating, showing a willingness to invest in the collective good.
*   **Reinforcing Success:** It continues to cooperate when the collective effort is successful, actively working to maintain and amplify positive outcomes.
*   **Parameter-Driven:** Its decisions are driven by the game's parameters (*m*, *r*) and observed history, aiming for the best collective outcome as defined by the game's reward structure.
*   **No Exploitation:** While it defects when cooperation fails, it does so defensively. It does not aim to exploit others by defecting when others cooperate. Instead, it seeks to avoid being exploited by defecting when the conditions for a successful collective project are not met.

### Pseudocode for ATCC Strategy

```pseudocode
// ATCC Strategy

// Parameters:
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

// State variables for player i:
// round_t: current round number
// num_cooperators_prev_round: number of players who cooperated in round t-1

function decide_action(round_t, num_cooperators_prev_round):
  // Handle first round
  if round_t == 1:
    return "C" // Cooperate in the first round

  // Handle last round
  if round_t == r:
    return "D" // Defect in the last round

  // Standard round decision
  if num_cooperators_prev_round >= m:
    return "C" // Cooperate if threshold was met and it's not the last round
  else:
    return "D" // Defect if threshold was not met

// Example usage (within the game simulation):
// In each round t (from 1 to r):
//   For player i:
//     If t == 1:
//       my_action = "C"
//     Else if t == r:
//       my_action = "D"
//     Else:
//       // num_cooperators_prev_round would be calculated from the history of round t-1
//       my_action = decide_action(t, num_cooperators_prev_round)
//     // Record my_action and update history for other players' observations
```

This ATCC strategy provides a balanced approach, favoring cooperation when conditions are favorable and protecting itself from exploitation when they are not. Its adaptability and clear decision rules make it robust against a variety of opponent behaviors in the repeated game setting.
'''

description_COOPERATIVE_73 = '''
Okay, let's design a cooperative strategy for the Collective Risk Dilemma. The core challenge here is to incentivize cooperation without explicit communication or external coordination, while being robust to potential defectors.

## Strategy: Adaptive Cooperation Thresholding (ACT)

The ACT strategy aims to be cooperative by default, but also pragmatic. It leverages the history of the game to dynamically adjust its threshold for cooperation, aiming to ensure the collective benefit is achieved while minimizing personal losses from being the sole cooperator or being exploited by defectors.

### 1. Decision Rules

The ACT strategy's decision to cooperate or defect in any given round is based on two primary factors: the current state of cooperation in the game and the proximity to the end of the game.

*   **Default Cooperate:** In most situations, the ACT strategy will lean towards cooperation. This is because the primary incentive for cooperation (the collective reward) is significant.
*   **Adaptive Thresholding:** The decision to cooperate will be influenced by the observed level of cooperation in previous rounds. If cooperation is consistently failing to meet the threshold `m`, the strategy will become more cautious. Conversely, if cooperation is strong, it will continue to cooperate.
*   **Endgame Deterioration:** As the game approaches its final round, the incentive to defect for immediate individual gain increases, as there are no future rounds to punish defection. The ACT strategy must account for this.

**Here's a breakdown of the decision rules for player `i` in round `t`:**

Let:
*   `C_t-1` be the number of players who cooperated in round `t-1`.
*   `R` be the total number of rounds.
*   `t` be the current round number (1-indexed).

**Decision Rule:**

Player `i` will play **Cooperate (C)** if:

1.  **Sufficient Cooperation in the Previous Round:** `C_t-1 >= m` (The threshold was met or exceeded in the last round).
    *   *Rationale:* If the group successfully cooperated last round, it reinforces the benefit of cooperation and encourages continued collective action.

2.  **Approaching the Threshold and it's Early in the Game:** `C_t-1 >= m - 2` AND `t < R - 1` (Close to the threshold and not in the last two rounds).
    *   *Rationale:* This is a crucial adaptive element. If the group is *almost* there (`m-1` or `m-2` cooperators), ACT will still try to push for cooperation, even if the threshold wasn't *quite* met. This is a "gamble" for collective benefit, but it's tempered by not doing so in the final rounds where the risk of being exploited is higher. The `t < R - 1` condition ensures this "push" is not made in the critical final two rounds where defection might be more rational for personal gain.

3.  **First Round:** `t == 1` (Always cooperate in the first round).
    *   *Rationale:* To establish a cooperative norm and signal good intentions. It's the best way to see if others will also try to cooperate.

**Player `i` will play Defect (D)** if none of the above conditions are met. This primarily happens when:

*   The number of cooperators in the previous round was significantly below the threshold, and it's not the first round.
*   It is the last round (`t == R`), regardless of previous cooperation levels.
*   It is the second to last round (`t == R - 1`) and the number of cooperators in the previous round was less than `m - 2`.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):**
    *   **Action:** Always Cooperate (C).
    *   **Rationale:** As stated above, this is the best way to initiate cooperation and see how others respond. It sets a cooperative tone.

*   **Last Round (`t = R`):**
    *   **Action:** Always Defect (D).
    *   **Rationale:** In the last round, there are no future consequences for defecting. Players are incentivized to maximize their individual payoff by keeping their endowment, as there's no opportunity for a future collective reward. This is a purely rational response in the absence of future interactions.

*   **Second to Last Round (`t = R - 1`):**
    *   **Action:** This is where the adaptive thresholding is most critical.
        *   If `C_R-2 >= m`, ACT cooperates.
        *   If `C_R-2 >= m - 2` (i.e., `m-1` or `m-2` cooperators in the previous round), ACT cooperates. This is the "push" to meet the threshold.
        *   Otherwise (if `C_R-2 < m - 2`), ACT defects.
    *   **Rationale:** This round is crucial. We still try to achieve the collective reward if we are close, but we avoid being the sole cooperator if the group has consistently failed to reach `m`.

*   **Insufficient Information (e.g., first round of a player's observation window if we were tracking individual player histories - but we are not, we are tracking aggregate):**
    *   Our strategy relies on the *aggregate* number of cooperators from the *previous* round. In the first round, there's no previous round, hence the special rule to cooperate.

### 3. Being Cooperative

The ACT strategy is fundamentally cooperative because:

*   **Default to Cooperation:** The strategy's default action is to cooperate, aligning with the cooperative ethos.
*   **Targeting Collective Benefit:** It actively tries to achieve the collective reward (`m` cooperators) because this benefits everyone significantly (player `i` gets `(1-c_i) + k` which is `k` if they cooperated and `1+k` if they defected, but the *group* gets the `k` multiplied by `n` if `m` is met). The strategy prioritizes this collective upside.
*   **Risk Mitigation for Cooperators:** By defecting in the final round and being more cautious when cooperation consistently fails, it protects itself from being exploited by players who always defect or by situations where its cooperation is not reciprocated. This self-preservation allows it to continue participating cooperatively in the long run if the tournament were to extend beyond this defined game.
*   **Adaptive Learning:** The strategy "learns" from past rounds by observing the aggregate cooperation level. This allows it to adjust its behavior to be more effective over time, without requiring explicit communication or predefined patterns.

---

## Pseudocode Representation

```pseudocode
STRATEGY ACT(game_history, game_parameters):
  n = game_parameters.n
  r = game_parameters.r
  m = game_parameters.m
  k = game_parameters.k

  current_round = game_history.current_round # 1-indexed
  previous_round_actions = game_history.get_previous_round_actions() # List of 'C' or 'D' for each player

  # Count cooperators in the previous round
  cooperators_last_round = 0
  if previous_round_actions is not empty:
    for action in previous_round_actions:
      if action == 'C':
        cooperators_last_round += 1

  # --- Decision Rules ---

  # Rule 1: Always cooperate in the first round
  if current_round == 1:
    return 'C'

  # Rule 2: Always defect in the last round
  if current_round == r:
    return 'D'

  # Rule 3: Cooperate if threshold was met or exceeded last round
  if cooperators_last_round >= m:
    return 'C'

  # Rule 4: "Push" for cooperation if close to the threshold and not in the final two rounds
  # The 't < R - 1' implicitly handled by checking for current_round == r and current_round == r-1 (which will be handled next)
  # So this condition is for rounds BEFORE the second-to-last.
  if current_round < r - 1:
      if cooperators_last_round >= m - 2: # Includes m-1 and m-2
          return 'C'

  # Rule 5: If none of the above cooperative conditions are met, defect.
  return 'D'

```

**Explanation of Pseudocode Logic:**

The pseudocode prioritizes the rules in order:

1.  **First Round:** A definitive cooperative start.
2.  **Last Round:** A definitive defection for self-interest.
3.  **Threshold Met:** If cooperation succeeded last round, continue.
4.  **Near Threshold Push:** If we're close (`m-1` or `m-2` cooperators) and it's *not* the last two rounds, we still try to cooperate. This is the adaptive "push."
5.  **Default to Defect:** If none of the above conditions for cooperation are met, the strategy defects. This happens when cooperation has been consistently low and it's not the first round, or when we are in the second-to-last round and haven't met the `m-2` threshold.

This strategy balances the desire for collective gain with the need for self-preservation in a world where others might not cooperate. It's adaptive because its willingness to "push" for cooperation adjusts based on past outcomes and its position in the game's timeline.
'''

description_COOPERATIVE_74 = '''
Here's a design for a cooperative strategy in the Collective Risk Dilemma, focusing on adaptability and robustness within the specified constraints.

## Cooperative Strategy: "Threshold Guardian"

This strategy aims to be a reliable cooperator when it appears beneficial and safe, while also being resilient to defection. It prioritizes achieving the collective reward by contributing when there's a good chance of success, but protects itself if the community fails to meet the cooperation threshold.

### 1. Decision Rules

The core of the strategy revolves around two key conditions:

*   **Optimism Threshold:** When a sufficient number of other players have cooperated in the *previous* round, indicating a strong collective willingness, the strategy will cooperate.
*   **Defection Penalty/Reassurance:** If the cooperation threshold was *not* met in the previous round, the strategy will defect in the current round. This serves two purposes:
    *   **Self-preservation:** It prevents the strategy from being exploited by defectors when the collective fails.
    *   **Signaling:** It subtly signals a withdrawal of cooperation when the group effort falters, potentially prompting others to reassess their own strategies.

**Specific Decision Logic for Round t (where t > 1):**

Let $N_C(t-1)$ be the number of players who cooperated in round $t-1$.

*   **If $N_C(t-1) \ge m$:** Player $i$ plays **C** (Cooperate) in round $t$.
*   **If $N_C(t-1) < m$:** Player $i$ plays **D** (Defect) in round $t$.

### 2. Handling Edge Cases

The "Threshold Guardian" strategy needs specific rules for the very first and last rounds.

*   **First Round (t = 1):**
    In the absence of any history, the strategy must make an initial decision. To align with the "cooperative mindset" and to give the game a chance to reach the cooperative outcome, the strategy will **always Cooperate (C)** in the first round. This is a leap of faith, designed to initiate the possibility of collective reward from the outset.

*   **Last Round (t = r):**
    The decision rule for the last round is the same as any other round: it depends on the number of cooperators in the *previous* round ($r-1$). This means the strategy will cooperate if the threshold was met in round $r-1$, and defect otherwise. There is no special "end-game" adjustment beyond the standard history-dependent rule, as the goal remains to maximize total payoff.

### 3. Being Cooperative

The strategy is inherently cooperative in its initial disposition and its responsiveness to successful collective action.

*   **Proactive Cooperation:** It initiates cooperation in the first round, believing in the possibility of a positive collective outcome.
*   **Responsive Cooperation:** It actively chooses to cooperate when the collective has demonstrated its ability to meet the threshold. This rewards past successful cooperation and encourages its continuation.
*   **Adaptive Resilience:** While cooperative, it is not naive. The defection rule after a failed threshold is a crucial adaptive mechanism. It prevents the strategy from being consistently exploited by defectors when the group fails to coalesce. This self-preservation allows it to continue participating in future rounds and potentially re-initiate cooperation when conditions are more favorable.
*   **No Pre-emptive Defection:** The strategy never defects without a reason based on history. It doesn't try to outsmart or punish others preemptively. Its defection is a direct response to a lack of collective success in the *prior* round.

### Pseudocode

```python
# Global variables (for tournament implementation, would track history)
# n: number of players
# r: number of rounds
# m: minimum cooperators needed
# k: reward factor

# In-memory history for player i (replace with actual tracking mechanism)
# cooperation_history: a list of integers, where cooperation_history[t-1] is the count of cooperators in round t-1
# For this strategy, we only need the count from the previous round.

def decide_action(current_round, n_cooperators_previous_round):
    """
    Determines the action (Cooperate or Defect) for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        n_cooperators_previous_round (int): The number of players who cooperated in the previous round.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """
    if current_round == 1:
        # Always cooperate in the first round to initiate possibility
        return 'C'
    else:
        # If enough players cooperated in the previous round, cooperate.
        # Otherwise, defect.
        if n_cooperators_previous_round >= m:
            return 'C'
        else:
            return 'D'

# Example of how this would be used in a tournament loop (conceptual)
#
# for round_num in range(1, r + 1):
#     # Assume 'n_cooperators_last_round' is known from the end of the previous round
#     # For round 1, this value might be conceptualized as 'n' to favor cooperation,
#     # or handled explicitly by the 'decide_action' function as implemented above.
#
#     my_action = decide_action(round_num, n_cooperators_last_round)
#
#     # ... (submit my_action and record all other players' actions)
#
#     # After all actions are revealed and payoffs calculated for the round:
#     # Update n_cooperators_last_round for the next iteration.
#     # This would involve counting how many players played 'C' in the current round.
```

### Robustness and Adaptability

*   **Adaptable to Group Dynamics:** The strategy directly reacts to the observed behavior of the group. If the group is generally cooperative, it will reciprocate. If the group struggles to meet the threshold, it will adapt by defecting, protecting its own payoff.
*   **Robust to Opponent Variance:** It doesn't assume any specific strategy from opponents. It only relies on the observable outcome of their collective actions. This makes it resilient to a mix of strategies, including purely selfish ones, retaliatory ones, or even other potentially cooperative strategies.
*   **Learning without explicit memory of individual defectors:** While it doesn't remember *who* defected, it remembers the *outcome* of the group's actions. This is a form of implicit learning about the group's tendency towards cooperation.
*   **Avoids Escalation:** By defecting only when the threshold is *not* met, it avoids retaliatory cycles. It doesn't defect in response to a single defector if the group *still* managed to meet the threshold due to other cooperators.

This "Threshold Guardian" strategy aims for a balance of cooperation and self-interest, designed to thrive in a repeated game where collective success is rewarded, and individual resilience is crucial.
'''

description_COOPERATIVE_75 = '''
Here's a design for a cooperative strategy for the Collective Risk Dilemma game, focusing on adaptiveness, robustness, and a clear cooperative intent.

## Cooperative Strategy: "Adaptive Trust & Deterrence"

This strategy aims to foster cooperation by being willing to cooperate when it seems beneficial, but also to signal displeasure and potentially deter future defection when cooperation is exploited. It learns from past rounds and adjusts its behavior accordingly.

### Core Philosophy:

The strategy believes in the power of collective action. It's willing to take the initial risk to achieve the higher rewards of cooperation. However, it's not naive. If others don't reciprocate, it will adjust its strategy to protect its own interests while still leaving the door open for future cooperation.

### 1. Decision Rules:

The decision for each player `i` in round `t` depends on the observed actions and outcomes of previous rounds. We'll track two key metrics:

*   **Observed Cooperation Rate (OCR_t-1):** The proportion of players who cooperated in the previous round `t-1`.
*   **Trust Level (TL_t-1):** A dynamic internal variable representing the strategy's belief in the likelihood of successful cooperation. This is initialized and updated based on historical outcomes.

**Detailed Decision Logic:**

1.  **Initial Trust (Round 1):**
    *   In the very first round, the strategy adopts a moderately cooperative stance. It assumes that some players might be naturally inclined to cooperate, and it wants to set a positive example.
    *   **Action:** Cooperate (C).

2.  **Adaptive Cooperation Threshold (Round t > 1):**
    *   **High Trust Scenario:** If the `Trust Level (TL_t-1)` is high (meaning recent rounds have shown good cooperation and positive outcomes), the strategy will be more inclined to cooperate.
        *   **Condition:** `TL_t-1 > Threshold_High` (e.g., `Threshold_High` = 0.75, meaning on average, 75% of players have cooperated in recent history, or the strategy has a strong positive historical outcome).
        *   **Decision:** Cooperate (C). This encourages continued cooperation and aims to push the collective towards meeting the `m` threshold.

    *   **Moderate Trust Scenario:** If the `Trust Level (TL_t-1)` is moderate.
        *   **Condition:** `Threshold_Low <= TL_t-1 <= Threshold_High` (e.g., `Threshold_Low` = 0.5, `Threshold_High` = 0.75). This is the most common state where the strategy observes mixed behavior.
        *   **Decision:** Cooperate (C) if the *Observed Cooperation Rate (OCR_t-1)* from the previous round was at least `m-1`. This means that the previous round was "close" to being cooperative, and the strategy is willing to provide the "tipping point" cooperation.
        *   **Decision:** Defect (D) if `OCR_t-1 < m-1`. If the previous round was not even close to meeting the threshold, the strategy will defect to protect its endowment and signal that the current level of cooperation is insufficient.

    *   **Low Trust Scenario:** If the `Trust Level (TL_t-1)` is low.
        *   **Condition:** `TL_t-1 < Threshold_Low` (e.g., `Threshold_Low` = 0.5). This indicates a history of low cooperation or exploitative behavior.
        *   **Decision:** Defect (D). In this state, the strategy prioritizes self-preservation and assumes that cooperation is unlikely to be reciprocated effectively. It will defect until evidence suggests otherwise.

3.  **Deterrence Mechanism (After a "Bad" Round):**
    *   If in round `t-1`, the strategy played Cooperate (C) but the collective failed to meet the `m` threshold (i.e., `OCR_t-1 < m` and player `i` cooperated), this represents a wasted cooperative effort and potential exploitation.
    *   **Action:** The strategy will be more inclined to defect in the subsequent round `t`. This is implicitly handled by the `Trust Level` reduction and the moderate trust decision rule. If the strategy consistently experiences this, its `Trust Level` will drop, leading to more defections.

4.  **Final Round (Round r):**
    *   The strategy's behavior in the last round is the same as in any other round. The fact that it's the last round doesn't change the decision logic itself, as there are no future rounds to "punish" or "reward." However, the historical data leading up to the last round will dictate the decision. If the `Trust Level` is high, it might cooperate; if low, it will defect. This is crucial for robustness as it doesn't assume the game "ends nicely."

### 2. Handling Edge Cases:

*   **Round 1:** As stated above, the strategy always starts by cooperating. This is a strategic choice to initiate cooperation and gauge initial responses.
*   **Last Round (Round r):** No special handling. The strategy acts based on the accumulated `Trust Level` and `Observed Cooperation Rate` from all previous rounds. This ensures that past behavior, not just the fact that it's the last round, dictates the action.
*   **Insufficient Data (Early Rounds):** For the first few rounds, where historical data is limited, the `Trust Level` will be initialized and updated based on the few available observations. The `OCR` will be directly used. The `Threshold_High` and `Threshold_Low` will help guide decisions even with sparse data.

### 3. Being Cooperative:

*   **Proactive Cooperation:** The strategy initiates cooperation in Round 1 and continues to cooperate under conditions of sufficient observed cooperation or a high `Trust Level`. This demonstrates a willingness to engage in collective action for mutual benefit.
*   **Conditional Cooperation:** The strategy cooperates as long as it's *likely* to be beneficial. It doesn't defect out of spite but rather as a signal and a protective measure when cooperation seems futile or exploitative.
*   **Adaptive Learning:** The `Trust Level` is a mechanism for learning. It rewards sustained cooperation with increased willingness to cooperate, and penalizes lack of cooperation with reduced willingness. This is a form of reciprocity, a cornerstone of cooperative behavior.
*   **Focus on Threshold:** The strategy's decisions are strongly influenced by the `m` threshold. It understands the collective goal and attempts to contribute towards it when there's a reasonable chance of success.

### Pseudocode:

```
// Global variables for strategy i
TrustLevel_i = 0.5  // Initialize trust at a moderate level
LearningRate = 0.1  // How much each round affects TrustLevel
CooperationHistory = [] // List to store (player_actions_in_round_t, outcome_in_round_t)

// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Thresholds for TrustLevel
Threshold_High = 0.75
Threshold_Low = 0.5

// Function for player i to decide action in round t
function DecideAction(round_t, previous_round_actions, previous_round_outcome, player_id):

  // --- Update TrustLevel based on previous round ---
  if round_t > 1:
    observed_cooperation_count = count(player_action == 'C' for player_action in previous_round_actions)
    observed_cooperation_rate = observed_cooperation_count / n

    // Update TrustLevel based on observed cooperation and outcome
    if observed_cooperation_count >= m: // Collective success
      // Reward cooperation, increase trust
      TrustLevel_i = TrustLevel_i + LearningRate * (1 - TrustLevel_i)
    else: // Collective failure
      // Penalize lack of cooperation, decrease trust
      TrustLevel_i = TrustLevel_i - LearningRate * (TrustLevel_i)
      // Further penalize if player_id cooperated but goal wasn't met
      if previous_round_actions[player_id] == 'C':
         TrustLevel_i = TrustLevel_i - LearningRate * 0.5 // Extra penalty for wasted cooperation

    // Ensure TrustLevel stays within [0, 1]
    TrustLevel_i = max(0, min(1, TrustLevel_i))

  // --- Decision Logic ---
  if round_t == 1:
    // Always cooperate in the first round
    return 'C'

  // Get observed cooperation rate from the previous round (if available)
  observed_cooperation_rate_prev = 0
  if round_t > 1:
    observed_cooperation_count_prev = count(player_action == 'C' for player_action in previous_round_actions)
    observed_cooperation_rate_prev = observed_cooperation_count_prev / n

  // Decision based on TrustLevel and observed cooperation
  if TrustLevel_i > Threshold_High:
    // High trust: always cooperate to maintain positive momentum
    return 'C'
  elif TrustLevel_i >= Threshold_Low:
    // Moderate trust: cooperate if the previous round was close to meeting the threshold
    if observed_cooperation_rate_prev >= (m - 1) / n: // Check if at least m-1 players cooperated
      return 'C'
    else:
      // If not close, defect to signal dissatisfaction and protect endowment
      return 'D'
  else: // TrustLevel_i < Threshold_Low
    // Low trust: defect to protect endowment, assume cooperation won't be reciprocated
    return 'D'

// Note: The 'previous_round_actions' would be a list of actions for all players in the prior round.
// 'previous_round_outcome' could be a boolean indicating if the threshold was met.
// 'player_id' is the index of the current player.
```

### Robustness and Adaptiveness:

*   **Adaptiveness:** The `TrustLevel` and the use of historical `OCR` make the strategy adaptive. It learns from its environment and adjusts its behavior. If others cooperate, its `TrustLevel` increases, making it more likely to cooperate. If others defect, its `TrustLevel` decreases, leading to more defections.
*   **Robustness:**
    *   **To Defection:** The strategy doesn't get stuck in a cooperative rut. If defectors are present, the `TrustLevel` will fall, leading to defections from this strategy as well.
    *   **To Exploitation:** The "wasted cooperation" penalty helps deter pure exploiters who rely on others to cooperate.
    *   **To Randomness:** The moderate trust scenario with the `m-1` threshold provides a buffer against single-round anomalies.
    *   **To Parameter Changes:** The strategy doesn't rely on fixed numbers of cooperators other than `m` itself; its logic is relative to the observed behavior and the `TrustLevel`.

This "Adaptive Trust & Deterrence" strategy offers a balanced approach, promoting cooperation when possible while safeguarding against exploitation, making it a strong contender in a tournament setting.
'''

description_COOPERATIVE_76 = '''
Okay, let's design a cooperative strategy for the Collective Risk Dilemma.

## Strategy: The "Threshold Guardian"

The core idea behind this strategy is to be a vigilant guardian of the cooperation threshold. We will aim to cooperate when it seems beneficial for the collective, but be ready to defect if cooperation is being exploited or if the risk of failure becomes too high. The strategy is designed to be adaptive, learning from the observed behavior of other players over time.

### 1. Decision Rules:

Our decision in each round `t` will be based on a calculation of the "cooperation deficit" observed in the *previous* round (`t-1`).

*   **The Cooperation Deficit:** This measures how far the number of cooperators in the previous round fell short of the required threshold `m`.
    *   Let `C_{t-1}` be the number of players who cooperated in round `t-1`.
    *   If `C_{t-1} < m`, the cooperation deficit is `m - C_{t-1}`.
    *   If `C_{t-1} ≥ m`, the cooperation deficit is 0.

*   **Cooperation Logic:**
    *   **If the cooperation deficit from round `t-1` is 0 (i.e., `C_{t-1} ≥ m`)**: This indicates that cooperation was successful in the previous round. In this case, we will **Cooperate (C)** in round `t`. We are signaling our commitment to continued cooperation when the collective achieves the threshold.

    *   **If the cooperation deficit from round `t-1` is greater than 0 (i.e., `C_{t-1} < m`)**: This means cooperation failed in the previous round. The degree of failure matters. We will use a "punishment" or "re-evaluation" mechanism:
        *   Let `Deficit_{t-1} = m - C_{t-1}`.
        *   We will **Defect (D)** in round `t` if `Deficit_{t-1} > 0`.
        *   **However, we will adjust our "cooperation willingness" based on the magnitude of the deficit.** We maintain an internal variable, `cooperation_tendency`, which starts at 1 (fully cooperative). When cooperation fails, we reduce this tendency. The reduction is proportional to the deficit, but capped to avoid immediate complete defection.
        *   **Revised Defection Rule:** In round `t`, if `C_{t-1} < m`, we will **Defect (D)**. We then update our `cooperation_tendency` for future rounds.

*   **Updating `cooperation_tendency`:**
    *   Let `current_tendency` be the value before the round.
    *   If cooperation fails (`C_{t-1} < m`):
        *   `new_tendency = current_tendency * (1 - alpha * (Deficit_{t-1} / n))`
        *   Where `alpha` is a learning rate parameter (e.g., 0.2 to 0.5). This means the tendency decreases by a fraction proportional to how much cooperation was missing, relative to the total number of players.
        *   `cooperation_tendency = max(0, new_tendency)` (we never let it go below 0).
    *   If cooperation succeeds (`C_{t-1} ≥ m`):
        *   We want to reinforce successful cooperation. We can slowly increase `cooperation_tendency` to encourage more risk-taking if the system seems stable.
        *   `new_tendency = current_tendency + beta * (1 - current_tendency)`
        *   Where `beta` is a small increment parameter (e.g., 0.05 to 0.1). This slowly nudges the tendency towards 1.
        *   `cooperation_tendency = min(1, new_tendency)` (we never let it go above 1).

*   **Final Decision Rule (Round `t > 1`):**
    *   Observe `C_{t-1}` (number of cooperators in the previous round).
    *   If `C_{t-1} ≥ m`: **Cooperate (C)**.
    *   If `C_{t-1} < m`: **Defect (D)**.
    *   After deciding, update `cooperation_tendency` based on the outcome of round `t-1`.

### 2. Handling Edge Cases:

*   **Round 1 (`t=1`):**
    *   In the absence of any historical data, we need an initial stance. To be cooperative and signal good intent, we will **Cooperate (C)** in the first round. Our `cooperation_tendency` starts at 1.0.

*   **Last Round (`t=r`):**
    *   In the final round, there is no future to influence. The primary goal shifts to maximizing our *own* final payoff, as there are no further rounds to benefit from collective success.
    *   Therefore, in the last round, the strategy becomes purely self-interested. We will **Defect (D)** in the last round, regardless of historical cooperation. This is a strategic hedge against exploiting players who continue to cooperate.

*   **The `m` Parameter:**
    *   Our strategy inherently relies on `m`. If `m` is very high (close to `n`), cooperation becomes difficult. If `m` is very low (just above 1), it's easy to meet the threshold. Our strategy adapts to this by the success or failure of reaching `m`.

*   **The `k` Parameter:**
    *   The value of `k` influences the incentive to cooperate. While our strategy doesn't *directly* use `k` in its decision logic, the higher `k`, the more likely other players are to cooperate, making our strategy's cooperative phases more rewarding. The adaptive nature will naturally reflect this.

### 3. Being Cooperative:

*   **Initial Cooperative Stance:** We start by cooperating, signaling a willingness to work together for mutual benefit.
*   **Reinforcing Success:** When cooperation is successful (`C_{t-1} ≥ m`), we continue to cooperate. This reinforces the cooperative norm and rewards collective effort.
*   **Adaptive Punishment:** When cooperation fails, we defect. However, this defection is not absolute. We use a `cooperation_tendency` that gradually adjusts. This allows for a "second chance" for cooperation if others also show a willingness to contribute. We are not immediately abandoning cooperation but rather recalibrating based on collective performance.
*   **Focus on the Threshold:** Our entire decision-making process is centered around achieving the `m` threshold. This aligns with the collective goal of the game.

### Pseudocode:

```
// Game Parameters (provided)
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Strategy Variables
cooperation_tendency = 1.0  // Starts fully cooperative
alpha = 0.3                 // Learning rate for reducing tendency on failure
beta = 0.07                 // Learning rate for increasing tendency on success

// History (to be maintained externally or within strategy object)
// history = list of actions taken by all players in previous rounds

Function DecideAction(current_round, history):
    // Handle Last Round
    If current_round == r:
        Return D // Purely self-interested in the final round

    // Handle First Round
    If current_round == 1:
        // Initialize tendency and return initial cooperative action
        cooperation_tendency = 1.0
        Return C

    // For rounds t > 1
    // Get number of cooperators from previous round
    previous_round_actions = history[current_round - 1] // Assuming history is 0-indexed by round
    cooperators_last_round = count_cooperators(previous_round_actions)

    // Decision based on previous round's outcome
    If cooperators_last_round >= m:
        // Cooperation was successful, continue cooperating
        my_action = C
    Else:
        // Cooperation failed, defect
        my_action = D

    // Update cooperation_tendency AFTER deciding action for THIS round
    // This update will influence the decision for the NEXT round

    cooperation_deficit = 0
    If cooperators_last_round < m:
        cooperation_deficit = m - cooperators_last_round

        // Reduce tendency if cooperation failed
        reduction_factor = alpha * (cooperation_deficit / n)
        cooperation_tendency = max(0.0, cooperation_tendency * (1.0 - reduction_factor))
    Else:
        // Increase tendency slightly if cooperation succeeded
        increase_factor = beta * (1.0 - cooperation_tendency)
        cooperation_tendency = min(1.0, cooperation_tendency + increase_factor)

    Return my_action

// Helper function
Function count_cooperators(actions_list):
    count = 0
    For each action in actions_list:
        If action == C:
            count = count + 1
    Return count

```

**Explanation of Adaptability and Robustness:**

*   **Adaptive:** The `cooperation_tendency` variable allows the strategy to learn from past rounds. If others consistently cooperate, `cooperation_tendency` will drift towards 1.0, leading to more cooperation. If others consistently defect or fail to meet the threshold, `cooperation_tendency` will decrease, leading to more defections from our side.
*   **Robust:**
    *   **Against Exploitation:** The strategy defects when cooperation fails. This prevents a free-rider problem where our cooperation is taken advantage of without collective benefit.
    *   **Against Coordination Failure:** It doesn't rely on explicit coordination schedules. It reacts to the *observed* level of cooperation.
    *   **Against Malicious Players:** The final round defection is a universal protection against being exploited in the last instance.
    *   **Against Varying `m` and `k`:** The strategy's success hinges on meeting `m`. The magnitude of `k` influences the overall incentives for all players, which our adaptive mechanism will implicitly respond to by observing the success rates of cooperation.

This "Threshold Guardian" strategy aims to be a stable and cooperative participant, but with a built-in mechanism to protect itself and respond to the collective's actions, making it a strong contender in a varied tournament.
'''

description_COOPERATIVE_77 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust:

## Cooperative Strategy: "Threshold Sentinel"

This strategy aims to achieve the collective reward by cooperating when it's sufficiently likely that the cooperation threshold ($m$) will be met, while defecting to protect its own endowment when cooperation seems unlikely. It's called "Threshold Sentinel" because it vigilantly monitors the past to infer the likelihood of reaching the cooperation threshold.

### Core Principles:

1.  **Maximize Collective Reward:** The primary goal is to trigger the $k$ multiplier for as many rounds as possible.
2.  **Risk Aversion:** Avoid contributing an endowment when the threshold is unlikely to be met, as this leads to a lower payoff (0 vs 1).
3.  **Adaptability:** The strategy learns from past rounds to adjust its cooperation behavior.
4.  **Robustness:** It should perform reasonably well against a variety of opponent strategies.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round $t$ (for $t > 1$) is based on an **Estimated Cooperation Probability (ECP)**.

**Estimated Cooperation Probability (ECP):** This is the probability that *at least* $m$ players will cooperate in the current round, given the observed history. It's estimated by looking at the *proportion of players who cooperated in the previous round*.

**Decision Rule:**

*   **If ECP > `cooperation_threshold_factor`:** Player cooperates (C).
*   **If ECP ≤ `cooperation_threshold_factor`:** Player defects (D).

The `cooperation_threshold_factor` is a parameter that needs to be tuned. A reasonable starting point could be slightly *above* the proportion of players needed for cooperation if *everyone else* cooperates, to account for uncertainty and potential defection from others. For example, if $m$ players are needed and there are $n$ players in total, if $m-1$ players cooperate, you would need to cooperate to reach the threshold. So, a factor around $\frac{m-1}{n-1}$ might be a good starting point, but we'll refine this.

**Refined ECP Calculation (for Round $t$):**

Let $C_{t-1}$ be the number of players who cooperated in round $t-1$.
Let $N_{total}$ be the total number of players ($n$).

The ECP can be approximated by the proportion of players who cooperated in the previous round:
$ECP_t = \frac{C_{t-1}}{N_{total}}$

**Refined Decision Rule:**

*   **If $\frac{C_{t-1}}{N_{total}} \ge \frac{m-1}{N_{total}-1}$ (and $C_{t-1} < m$):** Cooperate (C).
    *   *Rationale:* This condition means that if everyone who cooperated last round continues to cooperate, and even one more player cooperates, we will hit the threshold. This is a strong signal for cooperation.
*   **If $\frac{C_{t-1}}{N_{total}} \ge \frac{m}{N_{total}}$ (and $C_{t-1} \ge m$):** Cooperate (C).
    *   *Rationale:* The threshold was already met in the previous round, indicating a stable cooperative environment.
*   **Otherwise (i.e., $\frac{C_{t-1}}{N_{total}} < \frac{m-1}{N_{total}-1}$ and $C_{t-1} < m$):** Defect (D).
    *   *Rationale:* Not enough cooperation in the previous round to confidently expect the threshold to be met. Defect to secure a payoff of 1.

**Simplified Decision Rule (using a fixed `cooperation_threshold_factor`):**

Let `cooperation_threshold_factor` be a value between 0 and 1. A good starting point could be slightly higher than $\frac{m}{n}$ to be conservative, e.g., $0.6$ or $0.7$ if $m/n$ is around $0.5$. Let's use a dynamic adjustment based on the past.

Let's use a simpler approach that directly relies on the number of cooperators in the previous round relative to the total number of players:

**Decision Rule (Round $t > 1$):**

Let $N_{coop, t-1}$ be the number of players who cooperated in round $t-1$.

*   **If $N_{coop, t-1} \ge m - 1$:** Cooperate (C).
    *   *Rationale:* If $m-1$ or more players cooperated last round, and assuming at least one of them continues to cooperate, we are very likely to reach the threshold of $m$. This is a proactive cooperative move.
*   **Else ($N_{coop, t-1} < m - 1$):** Defect (D).
    *   *Rationale:* Not enough players cooperated last round to confidently believe the threshold will be met. Defecting preserves our endowment and avoids contributing to a failed project.

This rule is simpler, adaptive (uses history), and robust as it doesn't rely on precise probability calculations but rather on a strong indicator of success.

### 2. Handling Edge Cases:

*   **Round 1:**
    *   **Decision:** Cooperate (C).
    *   **Rationale:** In the absence of any history, the most cooperative action is to attempt cooperation. This signals a cooperative intent and can set a positive tone for the game. It's a necessary gamble to ever achieve the collective reward.

*   **Last Round (Round $r$):**
    *   **Decision:** Defect (D).
    *   **Rationale:** In the final round, there is no future to incentivize cooperation. Players have no reason to contribute their endowment when there are no further rounds to benefit from the collective reward. Defecting guarantees a payoff of 1, which is the best possible outcome in this scenario, as the collective reward is no longer relevant.

### 3. Cooperativeness Alignment:

*   **Proactive Cooperation:** The strategy *chooses* to cooperate even when the threshold isn't guaranteed, as long as there's a strong indication of success (i.e., $m-1$ or more cooperators in the previous round). This is a commitment to trying to achieve the collective good.
*   **Conditional Cooperation:** It's not blindly cooperative. It defects when the probability of success is low, which is rational and preserves its own resources. This prevents exploitation by defectors and ensures the strategy doesn't consistently lose its endowment.
*   **No Punishment Strategy:** This strategy doesn't explicitly punish defectors. Its "punishment" is simply defecting in response to a lack of cooperation. This is a softer approach than tit-for-tat style strategies and aligns with a general cooperative mindset that aims to build on success rather than retaliate against failure.
*   **First Round Gamble:** The initial cooperation is a pure act of faith in the possibility of collective success, embodying the spirit of cooperation.

### Pseudocode:

```python
class ThresholdSentinel:
    def __init__(self, n, r, m, k):
        self.n = n  # Total number of players
        self.r = r  # Total number of rounds
        self.m = m  # Minimum cooperators needed
        self.k = k  # Reward factor
        self.current_round = 0
        self.past_cooperators = [] # List to store the number of cooperators in each past round

    def decide_action(self, history):
        """
        Determines the action (C or D) for the current round.
        history: A list of tuples, where each tuple is (player_id, action) for previous rounds.
                 In this context, history will be more complex, tracking all players' actions per round.
                 For simplicity of this strategy, we only need the *count* of cooperators from the previous round.
        """
        self.current_round += 1

        # Handle Last Round
        if self.current_round == self.r:
            return 'D'

        # Handle First Round
        if self.current_round == 1:
            return 'C'

        # For rounds > 1 and < r:
        # Get the number of cooperators from the *immediately preceding* round.
        # We assume history provides a way to access this.
        # Let's simplify: we'll maintain a count of cooperators from the *last* round.

        # Infer number of cooperators from the *last* round's outcome.
        # The 'history' object needs to be rich enough to provide this.
        # For this example, let's assume a simplified history format where
        # the function receives the number of cooperators in the previous round.
        # If not, the strategy needs to parse the history to get this count.

        # Let's refine: assuming history is a list of all actions of all players in previous rounds.
        # We need to process the *last* round from this history.
        
        # Assuming history is a list of lists of actions per round.
        # Example: history = [['C', 'D', 'C'], ['D', 'D', 'C']] for 2 rounds played.
        
        if not history: # Should not happen for current_round > 1, but for safety
            return 'C' # Default to cooperate if history is unexpectedly empty

        last_round_actions = history[-1] # Actions from the previous round
        
        # Count cooperators in the last round
        num_cooperators_last_round = last_round_actions.count('C')

        # Decision Rule:
        # If m-1 or more players cooperated in the last round, cooperate.
        if num_cooperators_last_round >= self.m - 1:
            return 'C'
        else:
            return 'D'

```

**Explanation of Pseudocode Structure:**

*   The `ThresholdSentinel` class holds the game parameters and state.
*   `decide_action` is the core method. It takes `history` as input.
*   The `history` object is crucial. In a full tournament implementation, this would likely be a more structured representation of past rounds, allowing us to retrieve the actions of *all* players in the previous round. For simplicity in the pseudocode, I've assumed a way to get `num_cooperators_last_round`.
*   The logic directly implements the edge cases (Round 1, Last Round) and the main decision rule based on `num_cooperators_last_round`.

This "Threshold Sentinel" strategy provides a balance between cooperation and self-preservation, adapting to the emergent behavior of other players while maintaining a fundamentally cooperative objective.
'''

description_COOPERATIVE_78 = '''
This strategy aims to foster cooperation by conditionally cooperating, incentivizing others to do the same, and punishing defection without being overly punitive. It's designed to be adaptive to varying levels of cooperation observed in the game.

## Cooperative Strategy: Conditional Cooperation with Threshold Re-evaluation

**Core Principle:** Start cooperatively, and continue to cooperate as long as the observed collective behavior is sufficient to reach the cooperation threshold. If the threshold is consistently missed due to defections, reduce cooperation to incentivize a return to the threshold. If cooperation is overwhelming, maintain it.

### 1. Decision Rules

The decision for player `i` in round `t` is based on the observed cooperation levels in the *previous* round (`t-1`) and the game parameters (`n`, `m`, `k`).

*   **If `t = 1` (First Round):**
    *   **Cooperate (C).** In the first round, the best way to signal a willingness to cooperate and test the waters is to contribute to the community project.

*   **If `t > 1`:**
    Let `C_prev` be the number of players who cooperated in round `t-1`.

    *   **If `C_prev >= m` (Threshold Met or Exceeded in Previous Round):**
        *   **Cooperate (C).** If the group successfully met the cooperation threshold in the previous round, it's beneficial for everyone to continue cooperating. This reinforces the positive outcome and maximizes shared rewards.

    *   **If `C_prev < m` (Threshold Not Met in Previous Round):**
        *   **Evaluate the "Cost of Non-Cooperation":** Calculate a "cooperation gap" representing how far the group was from the threshold.
            *   `gap = m - C_prev`
        *   **Conditional Defection (D) if the Gap is Significant:** If the number of players who defected in the previous round (`n - C_prev`) is significantly larger than the number of players who cooperated (`C_prev`), or if the gap is substantial, it suggests a lack of widespread commitment to cooperation. In such cases, defecting can be a strategic choice to protect your own endowment while also signaling that the current level of cooperation is insufficient.
            *   **Rule for Defection:** Defect if `(n - C_prev) > C_prev` AND `gap > 0` (meaning at least one defection was needed to miss the threshold) OR if `gap > threshold_sensitivity`.
                *   **`threshold_sensitivity`:** This is a parameter that needs to be tuned but represents how many defections *beyond* what's needed to miss the threshold will trigger a defection from our player. A higher value makes us more patient. For example, `threshold_sensitivity = max(1, floor(n * 0.2))` (meaning if more than 20% of players defected *more* than necessary to miss the threshold, or if simply missing the threshold causes a personal loss). *Let's refine this to be simpler and more robust initially.*

        *   **Revised Rule for Defection (Simpler and More Robust):**
            *   **Defect (D) if `C_prev < m` AND `(n - C_prev) >= m`.** This means that in the previous round, *more* players defected than cooperated. This is a strong signal that cooperation is not the dominant strategy for the group, and defecting protects your endowment.
            *   **Cooperate (C) if `C_prev < m` AND `(n - C_prev) < m`.** This is the case where cooperation was *almost* achieved, or the number of cooperators was close to the threshold, and the number of defectors wasn't overwhelmingly dominant. In this scenario, continuing to cooperate is a gamble that can pull the group back to the threshold.

*   **Special Case: Last Round (`t = r`):**
    *   **Defect (D).** In the final round, there is no future for cooperation to be incentivized. The rational choice is to maximize your own immediate payoff by keeping your endowment. This is a standard assumption in finite repeated games.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):** As mentioned, always **Cooperate (C)** to initiate the game cooperatively and provide an opportunity for others to reciprocate.
*   **Last Round (`t = r`):** Always **Defect (D)** to maximize individual payoff without any future cooperative gains.
*   **Threshold Not Met (`C_prev < m`):** This is the most complex case. The strategy attempts to balance the desire to cooperate with the need to protect one's own endowment. The revised rule for defection:
    *   Defect if more people defected than cooperated (`n - C_prev >= m`). This is a strong signal that the group is not inclined towards cooperation.
    *   Cooperate otherwise, even if the threshold wasn't met. This is a "generous" approach that tries to pull the group back from the brink of consistent defection.

### 3. Being Cooperative

This strategy is cooperative in the following ways:

*   **Initial Cooperation:** It starts with a cooperative move, signaling a desire for mutual benefit.
*   **Reinforces Cooperation:** It rewards successful cooperation by continuing to cooperate.
*   **Resilient Cooperation:** It provides a buffer for occasional misses of the threshold, continuing to cooperate even when slightly below `m` if the overall group sentiment isn't overwhelmingly against cooperation. This allows for "pulling" the group back into a cooperative state.
*   **Punishment is Conditional and Temporary:** Defection is only a response to clear evidence of widespread defection from others. It's not an immediate reaction to a single missed round but rather to a pattern where defection is the dominant strategy for the majority. This avoids retaliatory cycles.
*   **Goal-Oriented:** The ultimate goal is to reach and maintain the cooperation threshold (`>= m`) because this leads to the highest possible payoff for the player when `k > 1`.

### Pseudocode

```pseudocode
function decide_action(player_id, current_round, total_rounds, num_players, min_cooperators, reward_factor, history):
  // history is a list of lists, where history[t-1] contains actions of all players in round t-1
  // e.g., history[t-1] = [action_p1_t-1, action_p2_t-1, ...]

  n = num_players
  r = total_rounds
  m = min_cooperators
  k = reward_factor

  // Handle Last Round
  if current_round == r:
    return "D"

  // Handle First Round
  if current_round == 1:
    return "C"

  // Handle Intermediate Rounds
  previous_round_actions = history[current_round - 2] // history is 0-indexed, rounds are 1-indexed
  cooperators_in_previous_round = count_cooperators(previous_round_actions)
  defectors_in_previous_round = n - cooperators_in_previous_round

  if cooperators_in_previous_round >= m:
    // Threshold was met or exceeded, continue cooperating
    return "C"
  else:
    // Threshold was not met
    if defectors_in_previous_round >= m:
      // More players defected than were needed to cooperate, strong signal of defection
      return "D"
    else:
      // Threshold was not met, but defection wasn't overwhelming.
      // This is the "generous" cooperative move to try and pull the group back.
      return "C"

function count_cooperators(actions_list):
  count = 0
  for action in actions_list:
    if action == "C":
      count = count + 1
  return count
```

**Explanation of Pseudocode Logic:**

1.  **Last Round:** If it's the final round (`current_round == r`), always defect.
2.  **First Round:** If it's the first round (`current_round == 1`), always cooperate.
3.  **Intermediate Rounds:**
    *   Get the actions from the `previous_round`.
    *   Count how many players cooperated (`cooperators_in_previous_round`) and defected (`defectors_in_previous_round`).
    *   **If `cooperators_in_previous_round >= m`:** The group succeeded, so we keep cooperating.
    *   **Else (threshold not met):**
        *   **If `defectors_in_previous_round >= m`:** This is the critical condition. If the number of defectors alone is *already* enough to prevent reaching the cooperation threshold (`m`), then it's highly likely that cooperation is not sustainable. In this situation, defecting is the best individual strategy.
        *   **Else:** If the threshold wasn't met, but the number of defectors wasn't overwhelmingly large (i.e., `defectors_in_previous_round < m`), then we still cooperate. This is the "generous" or "forgiving" part of the strategy, aiming to keep the possibility of collective success alive by not immediately abandoning cooperation.

This strategy aims to balance the pursuit of collective good with individual self-preservation in a noisy and unpredictable environment.
'''

description_COOPERATIVE_79 = '''
Okay, here's a cooperative strategy for the Collective Risk Dilemma game, designed to be adaptive and robust, as per your specifications.

## Strategy: "Trust but Verify with Escalating Commitment"

This strategy aims to foster cooperation by initiating with a cooperative stance, but it's designed to protect against exploitation and adapt to the collective behavior of the group. It balances the desire for the high collective reward with the risk of individual loss if cooperation fails.

### Core Principles:

1.  **Initial Trust:** Start by cooperating to signal a willingness to achieve the collective good.
2.  **Threshold Monitoring:** Closely track the number of cooperators in previous rounds.
3.  **Conditional Cooperation:** Adjust cooperation based on the observed success of the group in reaching the cooperation threshold.
4.  **Punishment for Defection:** If the group consistently fails to meet the threshold, reduce cooperation to protect individual endowments.
5.  **Escalation of Commitment:** If the group shows strong and sustained cooperation, increase the willingness to cooperate, even with slightly lower historical success.
6.  **End-Round Considerations:** Adjust strategy in the final round to maximize individual payoff without future repercussions.

### 1. Decision Rules:

Let's define some helper variables:
*   `current_round`: The current round number (1 to r).
*   `cooperators_in_previous_round`: The number of players who cooperated in the immediately preceding round.
*   `average_cooperators_last_N_rounds`: The average number of cooperators over the last N rounds (we'll define N later).
*   `cooperation_threshold_ratio`: `m / n` (the minimum proportion of players needed to cooperate).

**The Decision Logic:**

**In any round `t` (where `1 < t < r`):**

1.  **Check Initial Trust:** If `current_round == 1`, play **C**.

2.  **Check for Sustained Cooperation:** If `current_round > 1`:
    *   Calculate `cooperators_in_previous_round`.
    *   Calculate `average_cooperators_last_N_rounds`.
    *   **Decision:**
        *   If `cooperators_in_previous_round >= m` OR `average_cooperators_last_N_rounds >= m` (meaning the group *has* reached or *is consistently* reaching the threshold):
            Play **C**.
        *   Else if `cooperators_in_previous_round >= m - 1` AND `average_cooperators_last_N_rounds >= m - 1` (meaning the group is *close* to the threshold, and shows some effort):
            Play **C**.
        *   Else (meaning the group is consistently failing to reach the threshold):
            Play **D**.

**Refinement on `N` for `average_cooperators_last_N_rounds`:**
We want `N` to be large enough to capture trends but not so large that it's overly slow to react to recent changes. A reasonable starting point for `N` could be `max(3, r // 4)` – at least 3 rounds, or a quarter of the total rounds, whichever is larger. This value can be adjusted based on tournament performance.

### 2. Handling Edge Cases:

*   **Round 1 (Initial Round):**
    *   **Decision:** Always play **C**.
    *   **Rationale:** This is the "trust" phase. We want to signal cooperation and give the group an opportunity to self-organize. There's no historical data to rely on, so we start with the most cooperative action.

*   **Last Round (`current_round == r`):**
    *   **Decision:** Always play **D**.
    *   **Rationale:** In the final round, there are no future consequences. The primary objective becomes maximizing individual payoff. If the group has cooperated successfully, playing C might still be profitable *in that round*, but playing D guarantees a payoff of 1 from your own endowment, plus the potential reward of `k` if enough *other* players cooperate. However, the certainty of keeping your endowment (1) by playing D often outweighs the uncertainty of the reward in the absence of future rounds. This is a selfish but rational decision for the final round.

*   **Short Games (`r` is small):**
    *   If `r` is very small (e.g., 2 or 3), the `average_cooperators_last_N_rounds` calculation might be less reliable. In such cases, the reliance on `cooperators_in_previous_round` will be higher. The `max(3, r // 4)` for `N` will ensure `N` is at least 1 for `r=2` and `N=1` for `r=3`.

*   **Low `m` relative to `n` (e.g., `m = 2`):**
    *   The strategy remains robust. Even if `m` is small, the condition `cooperators_in_previous_round >= m` or `average_cooperators_last_N_rounds >= m` will trigger cooperation. The strategy will simply be more likely to cooperate if the threshold is easy to meet.

*   **High `m` relative to `n` (e.g., `m = n - 1`):**
    *   The strategy will be more conservative about cooperating. It will only cooperate if it sees strong evidence of group participation from previous rounds. If the group consistently falls short of a high `m`, it will defect.

### 3. Being Cooperative:

*   **Initiation of Cooperation:** The strategy starts with cooperation, demonstrating good faith.
*   **Conditional Cooperation:** It *continues* to cooperate as long as the group shows sufficient effort (meeting or closely approaching the threshold). This incentivizes others to cooperate and rewards collective effort.
*   **Reciprocity:** The strategy implicitly uses a form of conditional reciprocity. If others cooperate (leading to the threshold being met), we also cooperate. If they defect (leading to repeated failures), we withdraw cooperation.
*   **Long-Term Perspective (within the game):** By averaging over `N` rounds, it tries to avoid overreacting to single-round anomalies and promotes a stable cooperative environment if one emerges.

### Pseudocode:

```python
# Game Parameters
n = ... # number of players
r = ... # number of rounds
m = ... # minimum cooperators needed
k = ... # reward factor

# State Variables (to be maintained across rounds)
cooperation_history = [] # List to store number of cooperators each round
my_actions_history = [] # List to store my actions each round

def decide_action(current_round, n, r, m, k, cooperation_history, my_actions_history):
    """
    Determines the action (C or D) for the current round.
    """

    # --- Edge Case: Last Round ---
    if current_round == r:
        return 'D' # Maximize individual payoff in the final round

    # --- Edge Case: First Round ---
    if current_round == 1:
        return 'C' # Initiate cooperation

    # --- Standard Rounds (1 < current_round < r) ---

    # Get data from previous rounds
    cooperators_in_previous_round = 0
    if len(cooperation_history) > 0:
        cooperators_in_previous_round = cooperation_history[-1]

    # Calculate average cooperators over the last N rounds
    N_for_average = max(3, r // 4) # Define N dynamically
    recent_cooperation_data = cooperation_history[-N_for_average:]
    
    average_cooperators_last_N_rounds = 0
    if recent_cooperation_data:
        average_cooperators_last_N_rounds = sum(recent_cooperation_data) / len(recent_cooperation_data)
    
    # --- Decision Rules ---
    # Rule 1: If threshold was met or is consistently met/approached
    if cooperators_in_previous_round >= m or average_cooperators_last_N_rounds >= m:
        return 'C'
    
    # Rule 2: If the group is close to the threshold and showing effort
    if cooperators_in_previous_round >= m - 1 and average_cooperators_last_N_rounds >= m - 1:
        return 'C'

    # Rule 3: Otherwise (group is consistently failing to reach threshold)
    return 'D'

# --- Simulation/Tournament Loop Example (conceptual) ---
# for t in range(1, r + 1):
#     # In a real tournament, you'd receive actions of others from previous rounds
#     # and determine cooperation_history for this round.
#     # For simulation, let's assume we have this info.
#     
#     # Determine actual number of cooperators in the previous round (for illustration)
#     # In a tournament, this would be derived from opponent actions.
#     # Let's say we had opponents_actions_t_minus_1 = [...] # List of actions C/D
#     # cooperators_in_previous_round_actual = opponents_actions_t_minus_1.count('C')
#     # cooperation_history.append(cooperators_in_previous_round_actual)
#
#     my_action = decide_action(t, n, r, m, k, cooperation_history, my_actions_history)
#     my_actions_history.append(my_action)
#     
#     # In a tournament, your action 'my_action' would be submitted,
#     # and you'd receive payoffs based on all players' actions.
#     # You would then update cooperation_history for the *next* round's calculation.

```

This strategy is designed to be:
*   **Adaptive:** It adjusts its behavior based on the observed collective actions.
*   **Robust:** It has mechanisms to protect itself from exploitation (defecting in the last round, defecting if cooperation consistently fails) while still pursuing the collective good.
*   **Cooperative:** It initiates cooperation and rewards successful group efforts, aiming for the Pareto-optimal outcome where everyone benefits from the reward `k`.
'''

description_COOPERATIVE_80 = '''
This strategy aims to foster cooperation by being a conditional cooperator, rewarding cooperation and punishing defection, while also recognizing the importance of self-preservation. It prioritizes achieving the cooperative outcome when it's likely, but is prepared to defect to maximize individual gain when cooperation is unlikely or has failed.

Here's the design:

## Cooperative Strategy: The "Threshold Guardian"

The core idea of this strategy is to act as a vigilant guardian of the cooperative threshold. It will initiate cooperation with the hope of achieving the collective reward, but will quickly adapt its behavior based on the actions of others.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round depends on a calculation of the **perceived likelihood of meeting the cooperation threshold (m)**.

Let's define some terms:

*   **`my_cooperation_history`**: A list tracking whether player `i` cooperated in past rounds.
*   **`observed_cooperators_in_round[t]`**: The number of players (excluding self) observed to have cooperated in round `t`.
*   **`cooperators_count_threshold_met`**: A counter for how many *past* rounds the cooperation threshold `m` was successfully met.
*   **`defection_streak`**: A counter for consecutive rounds where the cooperation threshold `m` was *not* met.

**The Decision Logic for Round `t`:**

1.  **Calculate the "Cooperation Pressure" (CP)**: This metric estimates how much cooperation is needed from *others* to reach the threshold.
    *   `needed_from_others = m - observed_cooperators_in_round[t-1] - (1 if I cooperated in t-1 else 0)`
    *   `Cooperation Pressure (CP) = max(0, needed_from_others)`
    *   *Interpretation: If `CP` is 0, it means enough players (or I myself) ensured the threshold was met in the previous round. If `CP` is high, we are far from the threshold.*

2.  **Evaluate the "Cooperation Confidence" (CC)**: This metric assesses our confidence in achieving cooperation, factoring in past successes and recent failures.
    *   **If `cooperators_count_threshold_met` is high** (e.g., more than `r/4` rounds met the threshold): This indicates a history of successful cooperation. We will be more inclined to cooperate.
    *   **If `defection_streak` is short and `CP` is low**: This suggests a recent setback but not a sustained failure. We'll lean towards cooperation.
    *   **If `defection_streak` is long or `CP` is high**: This indicates persistent failure or a significant deficit in cooperation. We will be more inclined to defect.

**Specific Decision Rules:**

*   **If `CP == 0` and `cooperators_count_threshold_met > 0` (Threshold was met last round and likely to be met again):**
    *   **Play C.** This is the default cooperative action when past success suggests it's viable.

*   **If `CP > 0` and `defection_streak < 3` (Threshold was not met, but we're not in a long defection period):**
    *   **Play C.** We give it another chance. This is the "hopeful" phase.

*   **If `CP > 0` and `defection_streak >= 3` (Threshold was not met for a while):**
    *   **Play D.** This is the "punishment" or "self-preservation" phase. We stop contributing to a project that consistently fails to yield rewards.

*   **If `(m - (number of other players cooperating)) < 0` (i.e., the threshold was met *without* my contribution in the previous round):**
    *   **Play C.** This reinforces the cooperative outcome.

### 2. Handling Edge Cases

*   **Round 1 (Initial Round):**
    *   **Decision: Play C.** In the absence of any historical data, the strategy starts with a cooperative gesture. This is the "leap of faith" to try and kickstart cooperation. We assume the best and hope others will too.
    *   Initialize: `my_cooperation_history = []`, `cooperators_count_threshold_met = 0`, `defection_streak = 0`.

*   **Last Round (`t == r`):**
    *   **Decision: Play D.** In the final round, the incentive to cooperate disappears. The future reward `k` is no longer a factor. The only rational decision is to maximize individual gain by keeping the endowment. The payoff is `1` if defecting, and potentially `0` (if others also defect) or `2` (if others cooperate) if cooperating. Defecting guarantees a payoff of at least `1`.

*   **If `m == n`:** (This is technically not allowed by the problem spec `1 < m < n`, but if it were: )
    *   **Decision: Play C.** If `m=n`, everyone *must* cooperate for the reward. This strategy would always try to cooperate, hoping others do the same.

*   **If `n-m+1 <= number of players`:** (This means if just one person defects, the threshold might still be met)
    *   This is implicitly handled by the `CP` calculation. The strategy observes the actual number of cooperators.

### 3. Being Cooperative

*   **Initiation:** The strategy begins by cooperating, signaling a desire for collective benefit.
*   **Conditional Cooperation:** It cooperates as long as there's a reasonable expectation of achieving the collective reward. This encourages others to cooperate by providing a stable, cooperative partner.
*   **Adaptive Punishment:** Defection is not arbitrary. It's a response to a failure of the collective to meet the threshold, acting as a signal that the current cooperative dynamic is not working. The "punishment" (defection) is temporary and aims to re-establish conditions for future cooperation, rather than being purely retaliatory.
*   **Focus on Threshold:** The strategy is solely focused on the *collective success* of reaching `m` cooperators. This aligns with the goal of the game's reward structure.
*   **No Personal Grudges:** The strategy doesn't remember specific individuals who defected. It only reacts to the overall count of cooperators in the previous round. This makes it robust to changing dynamics among other players.

### Pseudocode Illustration

```pseudocode
// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// State Variables (maintained across rounds)
my_cooperation_history = [] // List of booleans (True for C, False for D)
cooperators_count_threshold_met = 0
defection_streak = 0

// Function to decide action for current round (t)
function decide_action(current_round, observed_actions_previous_round):
    // Initialize state for round 1
    if current_round == 1:
        return "C"

    // Handle last round
    if current_round == r:
        return "D"

    // --- State Update based on previous round ---
    observed_cooperators_in_round_t_minus_1 = 0
    if observed_actions_previous_round is not empty:
        for player_action in observed_actions_previous_round:
            if player_action == "C":
                observed_cooperators_in_round_t_minus_1 += 1

    my_action_last_round = my_cooperation_history[-1] if not empty(my_cooperation_history) else "C" // Assume C if no history

    total_cooperators_last_round = observed_cooperators_in_round_t_minus_1 + (1 if my_action_last_round == "C" else 0)

    if total_cooperators_last_round >= m:
        cooperators_count_threshold_met += 1
        defection_streak = 0 // Reset streak on success
    else:
        defection_streak += 1
        // No change to cooperators_count_threshold_met on failure

    // --- Decision Logic ---
    // Calculate how many *more* cooperators are needed from others
    needed_from_others = m - (observed_cooperators_in_round_t_minus_1 + (1 if my_action_last_round == "C" else 0))
    cooperation_pressure = max(0, needed_from_others)

    // High confidence scenario (threshold often met)
    if cooperators_count_threshold_met > (r / 4): // Example threshold for confidence
        if cooperation_pressure <= 0: // Threshold already met, or my cooperation ensured it
            return "C"
        elif defection_streak < 3: // Still hopeful, short streak of failure
            return "C"
        else: // Long streak of failure, even with past success
            return "D"
    else: // Low confidence scenario (threshold rarely met)
        if cooperation_pressure <= 0: // Threshold met (perhaps by others)
            return "C"
        elif defection_streak < 3: // Giving it one more chance
            return "C"
        else: // Consistent failure, time to defect
            return "D"

// At the end of each round, record your action
// my_cooperation_history.append(your_action_in_this_round)
```

**Robustness:** This strategy is robust because:

*   **No Assumption of Trust:** It doesn't blindly trust other players.
*   **Adaptive Punishment:** The `defection_streak` mechanism ensures that defection is a response to persistent failure, not an immediate reaction to a single defector.
*   **Learning from History:** It uses the `cooperators_count_threshold_met` to gauge the overall viability of cooperation, making it less susceptible to short-term fluctuations.
*   **Clear Termination Condition:** The strategy defines a clear action for the last round, preventing exploitation.

This "Threshold Guardian" strategy aims to be a strong contender by balancing the desire for collective reward with the necessity of protecting one's own endowment, adapting to the dynamic nature of the game.
'''

description_COOPERATIVE_81 = '''
Okay, let's design a cooperative strategy for the Collective Risk Dilemma. The core challenge here is to encourage cooperation when individual incentives to defect can be strong, especially if others are defecting. We need a strategy that balances the desire for the high collective reward with the risk of being exploited.

## Cooperative Strategy: "Threshold-Sensitive Reciprocity"

This strategy is designed to be cooperative but also adaptive and responsive to the unfolding game. It leverages the history of the game to inform decisions, aiming to foster cooperation when it's likely to be successful and to punish defection when necessary.

### Core Principle:

The strategy's core is to cooperate if there's a reasonable expectation that *enough others will also cooperate* to meet the threshold `m`. If cooperation seems unlikely to succeed, or if others have repeatedly defected, it will defect to protect its own endowment.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) for player `i` in round `t` will depend on two primary factors:

*   **Estimated Cooperation Rate:** An estimate of how many players are likely to cooperate in the current round.
*   **Past Performance:** How well cooperation has succeeded in previous rounds.

Let's define some helper variables:

*   `C_count_prev_round`: The number of players who cooperated in the *previous* round (round `t-1`).
*   `Successful_Cooperation_Streak`: A counter that increases when cooperation succeeds (i.e., `m` or more players cooperated) and resets to 0 when it fails.
*   `Defection_Punishment_Counter`: A counter that increases when players defect, and decreases when cooperation is attempted. This acts as a "patience" meter for cooperation.

**Detailed Decision Logic for Player `i` in Round `t`:**

1.  **Initial Observation:**
    *   If `t == 1` (First Round): We will start cooperatively to signal good faith and to gather information. **Action: C**
    *   If `t > 1`:
        *   Observe `C_count_prev_round` from round `t-1`.
        *   Observe if cooperation was successful in round `t-1` (i.e., if `C_count_prev_round >= m`).

2.  **Cooperation Condition:**
    *   **Primary Trigger for Cooperation:** If the number of players who cooperated in the previous round (`C_count_prev_round`) was *at least* `m-1`, *and* we have not been punished excessively for past cooperation attempts.
        *   Specifically, if `C_count_prev_round >= m - 1` AND `Defection_Punishment_Counter <= (m / 2)` (this is a heuristic to prevent punishing too harshly after a single failed attempt).
        *   **Action: C**

3.  **Defection Conditions (Punishment or Self-Preservation):**
    *   **If Cooperation Failed Last Round and Others Defected:** If `C_count_prev_round < m` and `C_count_prev_round < n / 2` (meaning a significant number of players defected, and cooperation was unlikely to succeed even if we cooperated).
        *   **Action: D**
    *   **If Past Cooperation Was Exploited:** If `C_count_prev_round < m` (cooperation failed) and `Successful_Cooperation_Streak == 0` (meaning it hasn't succeeded recently), and we have a running `Defection_Punishment_Counter` that has reached a certain threshold (e.g., `Defection_Punishment_Counter >= (n - m)`), it suggests a sustained pattern of defection.
        *   **Action: D**
    *   **If Last Round Was a Failed Cooperation Attempt and We're Uncertain:** If `C_count_prev_round < m` and we are not in the first round, and the above conditions don't trigger immediate defection, we will defect to be safe. This acts as a "wait-and-see" approach.
        *   **Action: D**

4.  **Adaptive Adjustment:**
    *   If player `i` chooses to **Cooperate (C)**:
        *   Increment `Successful_Cooperation_Streak`.
        *   If `Defection_Punishment_Counter > 0`, decrement `Defection_Punishment_Counter` (rewarding attempted cooperation).
    *   If player `i` chooses to **Defect (D)**:
        *   Reset `Successful_Cooperation_Streak` to 0.
        *   Increment `Defection_Punishment_Counter` (punishing defection, or signaling a desire for others to cooperate).

### 2. Handling Edge Cases:

*   **First Round (`t = 1`):** As stated above, always **Cooperate (C)**. This is crucial for establishing a cooperative baseline and observing initial behavior.
*   **Last Round (`t = r`):** In the very last round, the incentive to cooperate to achieve future rewards disappears. The optimal strategy in the last round of *any* repeated game with a known finite horizon is to defect. Therefore, regardless of history or any other factors, the strategy will **Defect (D)** in the `r`-th round. This is a standard rational choice in finite repeated games.
*   **Insufficient Information (Early Rounds):** In the very early rounds where `C_count_prev_round` might not be a stable indicator, the strategy relies on the initial cooperation and then gradually shifts to defection if cooperation consistently fails.
*   **Threshold Impossibility (`m > n` or `m <= 1`):** The game specification states `1 < m < n`. If these constraints were violated, the strategy would need to be adapted. However, assuming valid parameters, this is not an issue.

### 3. Cooperativeness:

*   **Initial Cooperative Stance:** The strategy *always* starts by cooperating. This demonstrates a willingness to engage in collective action and achieve the higher payoff.
*   **Conditional Cooperation:** Cooperation is maintained as long as there's a reasonable expectation of success (i.e., `m-1` cooperators observed last round, indicating we might be close to the threshold). This isn't blind cooperation; it's intelligent cooperation.
*   **Forgiveness (with limits):** The `Defection_Punishment_Counter` is designed to not permanently punish. If cooperation starts to recover and the threshold is approached, the counter decreases, allowing for a return to cooperation. It's about punishing sustained defection, not a single lapse.
*   **Targeting Exploitation:** The strategy aims to avoid being exploited. If cooperation consistently fails and others are defecting, it will defect to protect its own endowment, which is a necessary aspect of self-preservation in a game where cooperation isn't guaranteed.
*   **No Communication Exploitation:** The strategy doesn't try to "trick" or outmaneuver others through complex signaling. It's a direct response to observed actions.

### Pseudocode (Conceptual):

```pseudocode
// Global variables for the strategy
var C_count_prev_round = 0
var Successful_Cooperation_Streak = 0
var Defection_Punishment_Counter = 0
var current_round = 0
var total_rounds = r

// Function to decide action in a round
function DecideAction(all_past_actions_this_round):
    current_round = current_round + 1

    // Handle Last Round (always defect)
    if current_round == total_rounds:
        return D

    // Observe actions from the previous round
    // In a real implementation, you'd have access to the history.
    // For this pseudocode, we assume C_count_prev_round is updated after each round.

    // First Round: Always cooperate to signal intent and gather info
    if current_round == 1:
        // Update state for next round
        C_count_prev_round = n // Assume all cooperated for now (will be corrected after actual observation)
        Successful_Cooperation_Streak = 1
        Defection_Punishment_Counter = max(0, Defection_Punishment_Counter - 1)
        return C

    // Decision Logic for subsequent rounds
    cooperation_likely_to_succeed = (C_count_prev_round >= m - 1)
    punishment_threshold_not_met = (Defection_Punishment_Counter <= (n / 2)) // Heuristic

    if cooperation_likely_to_succeed and punishment_threshold_not_met:
        // Cooperate if likely to succeed and not too much punishment accumulated
        // Update state for next round
        C_count_prev_round = n // Placeholder, will be updated after actual observation
        Successful_Cooperation_Streak = Successful_Cooperation_Streak + 1
        Defection_Punishment_Counter = max(0, Defection_Punishment_Counter - 1)
        return C
    else:
        // Defect if cooperation is unlikely to succeed, or if we've been punished for it
        // Update state for next round
        C_count_prev_round = 0 // Placeholder, will be updated after actual observation
        Successful_Cooperation_Streak = 0
        Defection_Punishment_Counter = Defection_Punishment_Counter + 1
        return D

// --- After each round's actions are revealed and payoffs calculated ---
// Update state variables based on the actual outcome of the round.
// Let's assume a function `GetActualCcount(round_num)` exists.

function UpdateStateAfterRound(round_num):
    actual_cooperators = GetActualCcount(round_num)

    if round_num == 1:
        C_count_prev_round = actual_cooperators
        // The initial 'C' was decided without knowing others' actions.
        // If actual_cooperators >= m, then Successful_Cooperation_Streak becomes 1.
        // If actual_cooperators < m, then Successful_Cooperation_Streak becomes 0, Defection_Punishment_Counter becomes 1.
        if actual_cooperators >= m:
            Successful_Cooperation_Streak = 1
            Defection_Punishment_Counter = max(0, Defection_Punishment_Counter - 1) // Already decremented in DecideAction, this confirms it
        else:
            Successful_Cooperation_Streak = 0
            Defection_Punishment_Counter = Defection_Punishment_Counter + 1 // Already incremented in DecideAction, this confirms it

    else: // For rounds > 1
        // Decision logic in DecideAction already made the decision for the current round.
        // Now we update the state based on the *actual* outcome of the *previous* round.
        // This means we need to know the cooperation count of round_num - 1.
        // Let's assume we have `actual_cooperators_prev_round` for the round that just ended.

        if actual_cooperators_prev_round >= m:
            // Cooperation was successful in the previous round
            Successful_Cooperation_Streak = Successful_Cooperation_Streak + 1
            Defection_Punishment_Counter = max(0, Defection_Punishment_Counter - 1) // Reward attempting cooperation
        else:
            // Cooperation failed in the previous round
            Successful_Cooperation_Streak = 0
            Defection_Punishment_Counter = Defection_Punishment_Counter + 1 // Punish defection/failure

        C_count_prev_round = actual_cooperators_prev_round // Set for the next round's decision

```

**Explanation of Pseudocode Adaptation:**

The pseudocode needs careful handling of when state updates occur. The `DecideAction` function makes a decision *before* knowing the outcome of the current round. The `UpdateStateAfterRound` function uses the *actual* observed results of the *just-completed* round to prepare the state for the *next* round's decision. The logic in `DecideAction` uses `C_count_prev_round`, which is set by `UpdateStateAfterRound` after the *previous* round finished.

This "Threshold-Sensitive Reciprocity" strategy aims to be a robust cooperative player by being:
*   **Forward-looking:** Trying to achieve the high collective reward.
*   **Reactive:** Punishing defection and rewarding cooperation.
*   **Adaptive:** Adjusting its behavior based on the game's history.
*   **Self-preserving:** Not blindly cooperating when it's clearly detrimental.
'''

description_COOPERATIVE_82 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, emphasizing adaptiveness and robustness. The core idea is to start cooperatively, observe the collective behavior, and adjust based on the likelihood of success in meeting the cooperation threshold.

## Cooperative Strategy: "Threshold Watcher"

This strategy aims to maximize its own long-term payoff while contributing to the collective good when it's likely to succeed. It prioritizes cooperation but is pragmatic enough to defect when cooperation appears futile or detrimental.

### 1. Decision Rules

The core of the "Threshold Watcher" strategy lies in dynamically assessing the likelihood of meeting the cooperation threshold ($m$) in the current round, based on historical data.

**Initial State (Round 1):**
In the very first round, the strategy has no historical data. To initiate cooperation and signal a willingness to contribute, it will **Cooperate (C)**.

**Subsequent Rounds (t > 1):**
For each subsequent round, the strategy calculates two key metrics:

*   **Observed Cooperation Rate (OCR):** The proportion of players who cooperated in the *previous* round ($t-1$).
    *   `OCR = (Number of players who cooperated in round t-1) / n`

*   **Estimated Likelihood of Meeting Threshold (ELMT):** This is a probabilistic estimate of whether the threshold $m$ will be met in the *current* round ($t$). It's based on the observed cooperation rate in the *previous* round and the number of players remaining to make a decision in the current round (which is the entire player pool since it's simultaneous).
    *   The simplest and most robust way to estimate ELMT without making strong assumptions about other players' strategies is to consider the OCR. If the OCR is significantly above `(m-1)/n` (meaning, on average, we've been exceeding the threshold in the past), it suggests a higher likelihood of reaching it again. Conversely, if OCR is significantly below `(m-1)/n`, the likelihood of meeting the threshold is low.

    *   **Decision Logic:**
        *   **If OCR >= (m / n) * (1 + epsilon):** This means that in the previous round, the observed cooperation rate was *sufficiently higher* than what would be strictly needed to meet the threshold on average. The `epsilon` (a small positive value, e.g., 0.1 or 0.2) acts as a buffer to ensure we're not just barely meeting the threshold.
            *   **Decision: Cooperate (C)**. The strategy believes there's a high probability that the collective will reach the threshold and reward everyone.

        *   **If (m - 1) / n <= OCR < (m / n) * (1 + epsilon):** The observed cooperation rate in the previous round was close to the threshold requirement. This is a borderline case. The strategy will lean towards cooperation but with a slight caution.
            *   **Decision: Cooperate (C)**. The benefit of cooperation (potential reward $k$) outweighs the risk of a small loss (individual payoff of 1 instead of $1+k$).

        *   **If OCR < (m - 1) / n:** This means that in the previous round, the observed cooperation rate was *significantly lower* than what would be needed to meet the threshold. The strategy infers a low probability of success in the current round.
            *   **Decision: Defect (D)**. The strategy avoids contributing its endowment if it's highly unlikely that the collective effort will be rewarded. It's better to secure the private payoff of 1.

**Special Considerations:**

*   **Near the End of the Game (Last Round):** In the final round ($r$), the strategic incentives change. Since there are no future rounds to benefit from cooperation, the dominant strategy for any rational player in a one-shot game is to defect. However, our strategy is designed to be cooperative.
    *   **Decision in Round r:** The strategy will **Cooperate (C)** if the OCR in round $r-1$ was **greater than or equal to (m/n)**. This is a more stringent condition for the last round, requiring a higher degree of confidence that cooperation has been successful in the past. If the OCR in the penultimate round was below this threshold, the strategy will **Defect (D)**. The rationale is to protect against a last-ditch effort of defection by others that might leave a cooperator with zero payoff if the threshold isn't met.

### 2. Handling Edge Cases

*   **Round 1:** As stated above, the strategy will **Cooperate (C)** in the first round to signal willingness and establish an initial history.

*   **Last Round (Round r):**
    *   Calculate OCR based on round $r-1$.
    *   If `OCR >= m / n` (stricter condition for the last round), **Cooperate (C)**.
    *   Otherwise, **Defect (D)**.

*   **Insufficient History for Robust Calculation:** In very early rounds (e.g., round 2), the OCR is based on only one round of data. The strategy will still apply the rules, but it's understood that the prediction might be less accurate. The `epsilon` buffer helps mitigate this to some extent.

*   **All Players Defecting in Previous Round:** If all players defected in round $t-1$, OCR = 0. If $m > 1$, the condition `OCR < (m-1)/n` will likely be met, and the strategy will **Defect (D)**, which is the rational response when collective failure is evident.

### 3. Being Cooperative

The "Threshold Watcher" strategy is inherently cooperative in its design:

*   **Proactive Cooperation:** It starts by cooperating, demonstrating a willingness to contribute to the collective good.
*   **Conditional Cooperation:** It cooperates as long as there's a reasonable expectation that the collective effort will succeed. This is a more sophisticated form of cooperation than blind, unconditional cooperation, which can be exploited.
*   **Focus on Collective Reward:** The decision to cooperate is directly tied to the potential for a collective reward ($k$), which benefits all cooperators.
*   **Adaptive to Reality:** While cooperative, it's not naive. It adapts its behavior based on observed outcomes, recognizing that cooperation requires a critical mass. This prevents it from being a perpetual loser in scenarios where others are consistently defecting.

### Pseudocode

```
strategy_name = "Threshold Watcher"

// Global variable to store historical actions for each player
// For simplicity in this description, we'll assume access to round_history
// round_history[t] = {player_id: action, ...} for round t

function decide_action(player_id, current_round, game_params, round_history):
  n = game_params['n']
  r = game_params['r']
  m = game_params['m']
  k = game_params['k']
  epsilon = 0.1 // Small buffer for robust assessment

  // --- Handle Round 1 ---
  if current_round == 1:
    return "C" // Start by cooperating

  // --- Handle Last Round ---
  if current_round == r:
    previous_round_cooperators = 0
    if r - 1 in round_history:
      for action in round_history[r - 1].values():
        if action == "C":
          previous_round_cooperators += 1
    
    previous_ocr = previous_round_cooperators / n

    // Stricter condition for the last round
    if previous_ocr >= m / n:
      return "C"
    else:
      return "D"

  // --- Handle Intermediate Rounds (1 < current_round < r) ---
  previous_round_cooperators = 0
  if current_round - 1 in round_history:
    for action in round_history[current_round - 1].values():
      if action == "C":
        previous_round_cooperators += 1

  previous_ocr = previous_round_cooperators / n

  // Decision Logic based on Observed Cooperation Rate (OCR)
  if previous_ocr >= (m / n) * (1 + epsilon):
    // High confidence in meeting threshold
    return "C"
  elif previous_ocr >= (m - 1) / n:
    // Borderline, lean towards cooperation
    return "C"
  else:
    // Low confidence in meeting threshold, defect to secure private payoff
    return "D"

// Example of how round_history might be structured (internal to agent)
// round_history = {
//   1: {player_1: "C", player_2: "D", ...},
//   2: {player_1: "C", player_2: "C", ...},
//   ...
// }
```

This strategy aims to be a good cooperative player by being observant and adaptive, increasing its chances of long-term success in a repeated dilemma by not being overly exploited while still contributing to the collective good.
'''

description_COOPERATIVE_83 = '''
Here's a design for a cooperative strategy in the Collective Risk Dilemma, focusing on adaptiveness and robustness, while adhering to the specified constraints.

## Cooperative Strategy: "Cautious Reciprocator"

This strategy aims to achieve the collective good by being cooperative when it seems beneficial and safe, but also protecting itself from exploitation. It's designed to learn and adapt to the prevailing player behavior over time.

### Core Principles:

1.  **Proactive Cooperation (with a safety net):** We will aim to cooperate, especially when it seems like enough others are also cooperating, to trigger the collective reward.
2.  **Adaptive Threshold Monitoring:** We will actively track how many players are cooperating in previous rounds and adjust our cooperation decision based on this observed behavior.
3.  **Punishment for Defection (but not excessive):** If players consistently defect and prevent the collective from achieving the reward, we will reduce our cooperation to conserve our individual endowment. However, we won't engage in perpetual defection, maintaining a potential for re-establishing cooperation.
4.  **Risk Aversion:** We will not be the *only* cooperator if it means a certain loss of our endowment.

### 1. Decision Rules:

Our decision to Cooperate (C) or Defect (D) in round `t` will depend on the observed number of cooperators in the previous round(s) and the proximity to the end of the game.

Let:
*   `N_C(t-1)` be the number of players who cooperated in round `t-1`.
*   `N_D(t-1)` be the number of players who defected in round `t-1`.
*   `m` be the minimum number of cooperators needed.
*   `k` be the reward factor.
*   `r` be the total number of rounds.
*   `t` be the current round number (starting from 1).

**Decision Logic for Round `t`:**

*   **If `t == 1` (First Round):**
    *   **Cooperate (C).** In the absence of any history, we will take a proactive cooperative stance to try and initiate a cooperative outcome. This is the best way to signal willingness to cooperate and potentially trigger the reward if others follow.

*   **If `t > 1`:**
    *   **Scenario A: Approaching the End of the Game (e.g., `t >= r - 2`)**
        *   If `N_C(t-1) >= m - 1`: **Cooperate (C).** We are close to the end. If cooperation was almost successful last round, and we only need one more cooperator, we'll take the risk to try and secure the reward for the final rounds.
        *   If `N_C(t-1) < m - 1`: **Defect (D).** If we are far from the threshold and the end is near, it's unlikely to trigger the reward. Conserve our endowment.

    *   **Scenario B: Middle Rounds (`t <= r - 3`)**
        *   **Sub-Scenario B1: Threshold was met or exceeded in the previous round (`N_C(t-1) >= m`)**
            *   **Cooperate (C).** The collective project was successful. We reinforce this positive behavior and continue to contribute, expecting others to do the same.
        *   **Sub-Scenario B2: Threshold was ALMOST met in the previous round (`m - 1 <= N_C(t-1) < m`)**
            *   **Cooperate (C).** This indicates that with just one more cooperator, we would have achieved the threshold. We are willing to "lead" or "fill the gap" to try and trigger the reward. This is a strong signal of our cooperative intent.
        *   **Sub-Scenario B3: Significantly Below Threshold (`N_C(t-1) < m - 1`)**
            *   **Defect (D).** If cooperation has consistently failed to meet the threshold in recent rounds (let's say, the last 2 rounds), it suggests that there isn't enough collective will for cooperation. To avoid being exploited, we will defect and preserve our endowment. This is a form of punishment, but not a permanent one.
        *   **Sub-Scenario B4: Player `i` was exploited (played C when `N_C(t-1) < m`)**
            *   This is a crucial adaptive component. If we played C in round `t-1` and the threshold `m` was NOT met, meaning we contributed to the community project but received no collective reward (and potentially had a lower payoff than defectors), we need to react.
            *   If `N_C(t-1) < m` and player `i` played `C`:
                *   **Defect (D) in round `t`.** We will defect in the current round to avoid being repeatedly exploited. We will *then* reassess based on subsequent rounds.

**Summary of Decision Rules:**

*   **Round 1:** Cooperate (C).
*   **For `t > 1`:**
    *   **If `N_C(t-1) >= m`:** Cooperate (C).
    *   **Else if `m - 1 <= N_C(t-1) < m`:** Cooperate (C).
    *   **Else if `N_C(t-1) < m - 1` AND (If we played C last round and `N_C(t-1) < m`):** Defect (D). (Reacting to exploitation)
    *   **Else if `N_C(t-1) < m - 1` AND (We defected last round or `N_C(t-1)` was already low):** Defect (D). (Sustained lack of cooperation)
    *   **Special End-Game Adjustment:** If `t >= r - 2` and `N_C(t-1) >= m - 1`: Cooperate (C). (Last chance for reward)

### 2. Handling Edge Cases:

*   **First Round (r=1, t=1):** As stated, we start by Cooperating (C). This sets a cooperative tone and allows us to gather initial data.
*   **Last Round (r=t):** This is handled by the "Approaching the End of the Game" logic. If cooperation was close in the preceding round, we'll make one last attempt to reach the threshold. If it was far off, we'll defect to secure our endowment.
*   **`m = 2`:** This is the most sensitive threshold. Our strategy is designed to be more cautious. If only one person cooperates (and it's not us), we will defect. If two cooperate, we will cooperate. We will cooperate if we are the *only* potential bridge to `m=2` in the final rounds.
*   **`k` is very high:** The strategy inherently values `k` because it tries to achieve the `N_C >= m` condition. The larger `k`, the more incentive there is to be cooperative when the conditions are right.
*   **`k` is close to 1 (low reward):** The strategy might become more risk-averse, as the benefit of cooperation is marginal compared to individual endowment plus a small bonus. However, the adaptive nature means it will still try to achieve `m` if it seems plausible.

### 3. Being Cooperative:

This strategy is inherently cooperative because:

*   **It prioritizes cooperation in the first round.**
*   **It cooperates when the threshold is met or nearly met, aiming for the collective good.** It takes the risk to be the one to push the group over the edge if necessary.
*   **It only defects when cooperation has demonstrably failed or led to exploitation.** This is a reactive measure to ensure self-preservation, not an aggressive offensive move.
*   **It attempts to re-establish cooperation** by not remaining in a purely defecting state if the group composition changes.

### Pseudocode for "Cautious Reciprocator" Strategy:

```pseudocode
// Strategy: Cautious Reciprocator

// Global variables (assumed to be tracked by the tournament system)
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

// Player's internal state
// history_actions: A list of lists, where history_actions[t-1] is a list of actions (C or D) played by all players in round t-1.
// my_last_action: The action player i took in the previous round.

FUNCTION DecideAction(current_round_t, history_actions):
    // Initialize action to Defect by default, we will decide to Cooperate if conditions are met.
    action_to_play = "D"

    // Get data from the previous round (t-1)
    N_C_prev = 0
    N_D_prev = 0
    if current_round_t > 1:
        previous_round_actions = history_actions[current_round_t - 2] // Assuming history_actions is 0-indexed for rounds
        for player_action in previous_round_actions:
            if player_action == "C":
                N_C_prev = N_C_prev + 1
            else:
                N_D_prev = N_D_prev + 1

    // Decision Logic
    IF current_round_t == 1:
        action_to_play = "C" // Always start cooperative

    ELSE IF current_round_t >= r - 2: // Approaching end of game
        IF N_C_prev >= m - 1:
            action_to_play = "C" // Last chance to reach threshold

    ELSE: // Middle rounds
        IF N_C_prev >= m:
            action_to_play = "C" // Threshold was met, continue cooperating

        ELSE IF N_C_prev >= m - 1: // Threshold was almost met
            action_to_play = "C" // Take the risk to push to cooperate

        ELSE IF N_C_prev < m - 1: // Significantly below threshold
            // Check for exploitation: played C last round, but threshold not met
            IF my_last_action == "C" AND N_C_prev < m:
                action_to_play = "D" // React to exploitation

            ELSE: // Sustained lack of cooperation or already defecting
                action_to_play = "D" // Conserve resources

    RETURN action_to_play

// Note:
// - `history_actions` would need to be a data structure that stores the actions of all players for all previous rounds.
// - `my_last_action` would also need to be tracked by the player's internal state.
// - The indices for history_actions and current_round_t need to be consistent with how the tournament system provides data.
// - This pseudocode assumes a player's own past action (`my_last_action`) is available to inform the decision.
```

This strategy is designed to be robust by balancing the desire for collective reward with self-preservation, adapting to the group's behavior over time, and making informed decisions about risk, especially as the game nears its conclusion.
'''

description_COOPERATIVE_84 = '''
This strategy, named "Adaptive Threshold Cooperation" (ATC), aims to foster cooperation by mirroring the game's reward structure. It's designed to be a robust baseline cooperative strategy that adapts to the observed cooperation levels of other players, while always striving to reach the cooperative outcome.

### Core Principle:

ATC believes that the collective reward *k* is the primary driver for achieving cooperation. The strategy will attempt to contribute to reaching the threshold *m*, especially when it seems likely that others are also contributing or when the potential payoff from cooperation is high. It will defect only when it strongly suspects that cooperation will fail to meet the threshold, thereby minimizing individual loss.

### Strategy Components:

1.  **Observation and Memory:** ATC maintains a memory of the cooperation levels in past rounds. Specifically, it tracks:
    *   `cooperation_count_history`: A list storing the number of players who cooperated in each past round.
    *   `average_cooperation_rate`: The average cooperation rate across all past rounds.
    *   `last_round_cooperation_count`: The number of players who cooperated in the immediately preceding round.

2.  **Decision Rules:**

    *   **Early Rounds (First `r/2` rounds, or a fixed small number like 5, whichever is greater):**
        *   In the initial rounds, ATC will take a slightly more risk-averse cooperative approach.
        *   **Rule:** Cooperate (play `C`) if the number of players who cooperated in the *previous* round plus the number of players ATC *predicts* will cooperate (based on historical average) is *greater than or equal to m*.
        *   **Prediction Heuristic:** ATC assumes a proportion of players equal to the `average_cooperation_rate` will cooperate. So, ATC predicts `n * average_cooperation_rate` players cooperating.
        *   **Initial Round (Round 1):** Since there's no history, ATC will make a calculated gamble. It will cooperate if `m <= n * 0.5` (i.e., if the threshold is achievable by a minority of players, it's more likely to be met) and `m > 1` (to avoid defecting when `m=1` is trivially met by self). Otherwise, it will defect. This is a heuristic to avoid immediate defection if the threshold is too high, and to attempt cooperation if it seems reasonably achievable. A more sophisticated heuristic could be employed here, but this provides a balanced starting point.
        *   **Subsequent Rounds (within the early phase):** If `cooperation_count_history` is not empty, ATC will calculate: `predicted_cooperation = last_round_cooperation_count + (n - len(cooperation_count_history)) * average_cooperation_rate`. If `predicted_cooperation >= m`, ATC cooperates. Otherwise, it defects.

    *   **Mid-to-Late Rounds (After the initial phase):** ATC becomes more confident in its observations.
        *   **Rule:** Cooperate (play `C`) if the `last_round_cooperation_count` is *greater than or equal to m*. This is a strong signal that cooperation is achievable.
        *   **Exception 1 (Near Miss):** If `last_round_cooperation_count == m - 1` (i.e., one player short of the threshold) AND `r - current_round > 1` (i.e., it's not the last round), ATC will cooperate. This is a crucial "push" to reach the threshold when it's within reach and there's still time for others to respond.
        *   **Exception 2 (Deterioration):** If `last_round_cooperation_count < m - 1` AND `average_cooperation_rate` has been consistently decreasing over the last few rounds, ATC might defect. This is a sign that the cooperative norm is breaking down. For simplicity in this design, we will stick to a simpler rule: defect if `last_round_cooperation_count < m - 1`.

    *   **Last Round (Round `r`):**
        *   **Rule:** In the final round, ATC's objective shifts slightly. Since there are no future rounds for retaliation or reward, the primary goal is to maximize its own payoff in this single round.
        *   **Decision:** ATC will defect (play `D`) if `last_round_cooperation_count < m`. By defecting, ATC ensures it gets its private endowment (1). If `last_round_cooperation_count >= m`, then the community project succeeds regardless of ATC's action, and ATC receives `k` minus its endowment cost (0 if it cooperates, 1 if it defects). However, in a truly final round, even if the threshold is met, cooperating incurs a cost of 1 without an additional benefit if the reward is already guaranteed. To maximize immediate payoff in the last round, ATC will defect if `last_round_cooperation_count < m`. If `last_round_cooperation_count >= m`, ATC *could* play either C or D, as the reward is secured. For simplicity and to avoid unnecessary costs, it will **defect** in the last round if `last_round_cooperation_count < m`, and **defect** if `last_round_cooperation_count >= m`. This is a common strategy in finite repeated games to avoid being exploited in the final interaction. *(Self-correction: While defecting in the last round is common to maximize individual payoff when the threshold is not met, if it *is* met, cooperating yields 2 and defecting yields 3. So, the optimal strategy for the last round is to defect regardless of previous cooperation, as it always yields 1 if the threshold isn't met, and 3 if it is met, which is higher than 2. Therefore, the rule is: **always defect in the last round.**)*

3.  **Cooperative Alignment:**
    *   ATC prioritizes reaching the cooperative outcome (`m` or more cooperators) whenever it assesses a reasonable probability of success.
    *   Its "punishment" for low cooperation is a temporary defection, which is designed to incentivize future cooperation by demonstrating the cost of failing to meet the threshold.
    *   It is adaptive, meaning its behavior changes based on the observed actions of others, rather than adhering to a rigid, predetermined pattern.
    *   By attempting to push for cooperation when `m-1` players cooperate in non-last rounds, it explicitly signals a desire to achieve the collective benefit.

### Pseudocode:

```pseudocode
Strategy: Adaptive Threshold Cooperation (ATC)

Initialize:
  cooperation_count_history = []  // list of ints
  current_round = 1
  total_rounds = r
  num_players = n
  min_cooperators = m
  reward_factor = k

On Round Start:
  If current_round == 1:
    // Initial gamble for round 1
    If min_cooperators <= num_players * 0.5 AND min_cooperators > 1:
      My_Action = C
    Else:
      My_Action = D
  Else:
    last_round_cooperation_count = cooperation_count_history[-1]
    average_cooperation_rate = sum(cooperation_count_history) / len(cooperation_count_history)

    // Decision Logic
    If current_round == total_rounds:
      My_Action = D // Always defect in the last round to maximize immediate payoff
    Else If last_round_cooperation_count >= min_cooperators:
      My_Action = C // Strong signal of cooperation achieved
    Else If last_round_cooperation_count == min_cooperators - 1:
      // One short, and not the last round - push for cooperation
      My_Action = C
    Else:
      // Too few cooperators, and not a near miss, likely to fail
      My_Action = D

  Return My_Action

On Round End (after observing other players' actions):
  current_round_cooperation_count = count of players (including self) who played C
  cooperation_count_history.append(current_round_cooperation_count)

```

### Explanation of Adaptive Nature:

*   **Early Rounds:** The strategy starts cautiously, using historical averages and a simple heuristic to gauge the likelihood of success. This prevents immediate over-cooperation when the game is uncertain.
*   **Mid-Rounds:** As history accumulates, the strategy relies more heavily on direct past performance. If the threshold was met in the previous round, it assumes it can be met again and cooperates. If it was missed, it defects.
*   **The "Push" (m-1):** This is a critical adaptive element. It recognizes that a single defection might be the only thing preventing cooperation. In such a scenario, ATC is willing to bear the cost of cooperation to try and trigger the collective reward, especially if there's still time to recover.
*   **Last Round Defection:** This is a standard adaptation to finite repeated games. It removes the incentive for others to defect in the penultimate round, as ATC will always defect in the final round, regardless of past cooperation.

ATC is designed to be a fair and persistent cooperative player. It will signal its cooperative intent through its actions and will only retreat to defection when it strongly perceives that cooperation is unlikely to succeed, or when the game concludes.
'''

description_COOPERATIVE_85 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, focusing on adaptiveness, robustness, and a clear cooperative intent:

## Cooperative Strategy: "Threshold Tracker"

**Core Philosophy:** Our strategy aims to achieve cooperation by actively monitoring the collective behavior and adjusting our own actions to incentivize or maintain the cooperative outcome. We believe that by contributing to the cooperative threshold when feasible, we encourage others to do the same, thereby maximizing the collective reward. We are willing to defect strategically to protect ourselves when cooperation seems unlikely or when our contribution alone would be insufficient.

### 1. Decision Rules

The core of our strategy lies in tracking the *observed cooperative rate* and comparing it to the *target cooperative rate* required to meet the threshold.

**Key Metrics:**

*   **`observed_cooperators_last_round`**: The number of players who cooperated in the previous round.
*   **`my_action_last_round`**: Our action in the previous round (C or D).
*   **`cooperative_needed_this_round`**: The number of cooperators needed in the *current* round to reach the threshold `m`. This is dynamic and depends on how many have already defected. However, for simplicity in decision-making, we will base our immediate decision on the *historical success* of reaching `m` and the *current momentum*.

**Decision Logic per Round (t > 1):**

1.  **If `observed_cooperators_last_round` >= `m`:**
    *   This indicates that the group successfully met the cooperation threshold in the previous round. The collective reward was achieved.
    *   **Action: Cooperate (C)**. We have strong evidence that cooperation is beneficial and achievable. Our continued cooperation reinforces this positive outcome.

2.  **If `observed_cooperators_last_round` < `m` AND `observed_cooperators_last_round` + 1 >= `m`:**
    *   This means that in the last round, the group *almost* met the threshold, and our single cooperation *would have been enough* to tip the balance.
    *   **Action: Cooperate (C)**. This is a critical moment to show leadership and attempt to bridge the gap. We are making a strong cooperative signal.

3.  **If `observed_cooperators_last_round` < `m` - 1:**
    *   This indicates a significant shortfall in cooperation in the previous round. It suggests that the collective is not trending towards cooperation.
    *   **Action: Defect (D)**. In this scenario, our individual cooperation is unlikely to be enough to trigger the reward for the group, and we risk a lower payoff (1 vs. 0+k, where k>1). It's more prudent to secure our private endowment.

4.  **If `observed_cooperators_last_round` == `m` - 1 AND `my_action_last_round` == 'C':**
    *   This is a more nuanced case. We cooperated last round, and there was exactly one player short of the threshold. We *could* cooperate again, hoping to reach `m`.
    *   **Action: Cooperate (C)**. We've already invested in cooperation. Repeating our cooperative action might be the catalyst needed. We are essentially making a bet on the threshold being met with our contribution.

5.  **If `observed_cooperators_last_round` == `m` - 1 AND `my_action_last_round` == 'D':**
    *   We defected last round, and the group was one short.
    *   **Action: Cooperate (C)**. This is a strong signal of our willingness to be the "swing" player. We are taking a risk to try and trigger the collective reward.

### 2. Handling Edge Cases

*   **First Round (t = 1):**
    *   **Action: Cooperate (C)**. In the absence of any history, we adopt a default cooperative stance. This is our best attempt to initiate a cooperative norm and signal our intentions to the other players. We assume the initial endowment of 1 is less desirable than the potential for a higher payoff if cooperation succeeds.

*   **Last Round (t = r):**
    *   The decision logic remains the same as for any other round (t > 1). Our strategy is inherently designed to maximize payoff over the entire game, and the last round is no exception. If cooperation has been successful historically, we continue to cooperate. If it has failed, we defect.

*   **All Players Defect in a Round:**
    *   If `observed_cooperators_last_round` is 0, our logic will fall into case 3 (if m > 1), leading to Defect. This is robust; if no one cooperates, we won't either.

*   **All Players Cooperate in a Round:**
    *   If `observed_cooperators_last_round` is `n`, our logic will fall into case 1, leading to Cooperate. This is also robust; if cooperation is the norm, we maintain it.

*   **`m` = 2:**
    *   Our strategy remains valid. The logic for `m-1` and `m-1+1` will be crucial. For instance, if `m=2` and `observed_cooperators_last_round` is 0, we defect. If it's 1 and we defected, we switch to cooperate. If it's 1 and we cooperated, we continue to cooperate.

### 3. Being Cooperative

Our strategy is fundamentally cooperative:

*   **Default Cooperative Stance:** We initiate cooperation in the first round.
*   **Proactive Threshold Bridging:** We are willing to be the critical player to meet the threshold, even if it means taking a risk (e.g., cases 2 and 5). This demonstrates our commitment to the collective good.
*   **Reinforcement of Success:** We actively reinforce successful cooperation by continuing to cooperate when the threshold has been met.
*   **Adaptability:** We are not rigidly cooperative. We defect when the observed behavior of the group suggests that cooperation is unlikely to succeed and our individual defection is strategically beneficial, thus protecting our own long-term payoff. This is not a sign of non-cooperation, but rather a rational response to avoid being exploited. We aim for mutual benefit, not altruism that leads to individual loss.

### Pseudocode

```python
class ThresholdTrackerStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.current_round = 0
        self.my_action_last_round = None
        self.observed_cooperators_last_round = 0
        self.cooperators_this_round_count = 0 # To count observed cooperators

    def decide_action(self, history_of_actions):
        """
        history_of_actions: A list of lists, where each inner list contains player actions
                            for a given round.
                            e.g., [[C, D, C], [D, D, D]] for 3 players over 2 rounds.
                            Our index is implicitly known or provided.
        """
        self.current_round += 1

        # --- Update observed_cooperators_last_round based on actual history ---
        if self.current_round > 1:
            # Assume history_of_actions is indexed by round, then by player.
            # We need to count cooperators from the *previous* round.
            previous_round_actions = history_of_actions[self.current_round - 2]
            self.observed_cooperators_last_round = previous_round_actions.count('C')
            self.my_action_last_round = previous_round_actions[self.get_my_player_index()] # Assuming get_my_player_index() is available

        # --- Decision Rules ---

        # First Round Initialization
        if self.current_round == 1:
            return 'C'

        # Case 1: Threshold met or exceeded last round
        if self.observed_cooperators_last_round >= self.m:
            return 'C'

        # Case 2: Almost met, our cooperation would have tipped it
        if self.observed_cooperators_last_round + 1 >= self.m:
            return 'C'

        # Case 3: Significant shortfall
        if self.observed_cooperators_last_round < self.m - 1:
            return 'D'

        # Case 4 & 5: Exactly one short, consider our previous action
        if self.observed_cooperators_last_round == self.m - 1:
            # If we cooperated last round, or if we defected and want to be the swing player
            # we cooperate. This logic covers both Case 4 and Case 5.
            return 'C'

        # Fallback (should ideally not be reached with the above logic)
        return 'D' # Safest default if logic gets complex

    def get_my_player_index(self):
        # This method needs to be implemented by the tournament framework
        # It should return the index of this strategy instance (0 to n-1)
        pass

```
'''

description_COOPERATIVE_86 = '''
This is a fascinating and complex game! Designing a purely cooperative strategy that is also adaptive and robust in this environment is a challenging but rewarding task. My strategy aims to achieve this by focusing on **conditional cooperation and a commitment to the collective good, while being mindful of the risks involved.**

Here's the design for my cooperative strategy:

## Strategy: "Commit to Cooperative Threshold"

**Core Principle:** My strategy's primary goal is to facilitate cooperation and achieve the collective reward whenever possible. It will err on the side of cooperation, but only when there's a reasonable expectation that the threshold *will* be met, thereby avoiding the worst-case scenario for everyone. The strategy is built on a continuous assessment of the collective behavior and a commitment to its own contribution towards the cooperative outcome.

### 1. Decision Rules

My decision to cooperate (C) or defect (D) in any given round is based on a dynamic assessment of the likelihood of achieving the cooperative threshold (`m`). This assessment relies on observations from past rounds.

**My Decision Logic:**

*   **Initial Tendency: Cooperate.** In the absence of information, my default action is to cooperate. This signals a commitment to the collective.
*   **Conditional Cooperation:** I will continue to cooperate as long as the observed behavior of *other* players suggests that the `m` threshold is likely to be met.
*   **Defection Trigger:** I will defect only if the observed cooperation from *other* players in the previous round falls below a certain critical level, indicating a high probability that `m` will *not* be met, and my own cooperation would be wasted, leading to a lower payoff for myself and not contributing to the collective good.

**Quantifying the Decision:**

To implement this, I will track two key metrics from the *previous* round:

*   `N_C_observed`: The number of players (excluding myself) who cooperated in the previous round.
*   `N_D_observed`: The number of players (excluding myself) who defected in the previous round.

**The Decision Rule:**

In round `t` (where `t > 1`):

1.  **Calculate my target:** I need `m-1` other players to cooperate for the threshold to be met (since I will contribute one).
2.  **Assess the likelihood of success:**
    *   If `N_C_observed >= m - 1`: This means that in the previous round, at least `m-1` *other* players cooperated. This is a strong signal that collective cooperation is viable. **I will play C.**
    *   If `N_C_observed < m - 1` BUT `N_C_observed + N_D_observed >= n - 1` (meaning everyone else's actions are known): This is a more nuanced case. If `N_C_observed` is still too low, but everyone else's actions were observed, it means there's a high probability of failure. I will consider defecting here to protect my own endowment if cooperation seems doomed.
        *   **Sub-rule:** If `N_C_observed + 1 < m` (meaning even if I cooperate, the threshold won't be met based on *other* players' previous actions and the known count of defectors), then **I will play D**. This is because my cooperation is unlikely to trigger the reward and would only reduce my payoff from 1 to 0.
    *   Otherwise (if `N_C_observed < m - 1` and the previous round's data doesn't give a clear "failure is inevitable" signal): My default is to cooperate to try and nudge the group towards success. **I will play C.**

**Simplified Logic:**

My decision hinges on a balance: cooperate if it seems others are also cooperating sufficiently to reach the threshold. If it's clear others are not cooperating enough, and my cooperation wouldn't tip the scales, I will defect to preserve my endowment.

### 2. Handling Edge Cases

*   **Round 1 (t=1):**
    *   **Decision:** I will **always play C** in the first round.
    *   **Reasoning:** With no prior history, the only way to initiate cooperation and signal a commitment to the collective is to be the first to contribute. This also aligns with the "cooperative mindset" and sets a positive example.

*   **The Last Round (t=r):**
    *   **Decision:** I will **always play D** in the last round.
    *   **Reasoning:** In a repeated game, future interactions can influence present behavior (e.g., tit-for-tat). However, once the game ends, there are no future consequences. My goal is to maximize my *total* payoff. Since there's no "reputation" to maintain for future rounds, and no possibility of triggering a future reward, defecting in the last round guarantees me a payoff of 1 (my private endowment) without any risk. This is a rational decision to secure the maximum possible payoff in the final round, as the collective benefit of cooperation has expired.

*   **Situations where `m` is very high or `k` is very low (relative to `n`):**
    *   The strategy inherently handles this. If `m` is high, `N_C_observed` will likely be less than `m-1` more often, leading to more defections in later rounds if the group doesn't coordinate effectively. If `k` is low, the incentive for others to cooperate might be weaker, and my strategy will adapt by defecting if cooperation doesn't seem fruitful.

### 3. Being Cooperative

*   **Proactive Cooperation:** My strategy *starts* by cooperating and continues to do so as long as there's a reasonable chance of collective success. It doesn't wait for others to prove themselves; it demonstrates its own commitment.
*   **Threshold-Focused:** The strategy is fundamentally aligned with achieving the collective reward (`k`). It only defects when cooperation is demonstrably futile and counterproductive to its own payoff.
*   **Information Use:** It uses observed history to *inform* cooperation, not to punish perceived betrayal. It's a continuous assessment of the group's ability to achieve the common goal.
*   **No Retaliation, Only Adaptation:** My strategy doesn't "punish" players for defecting. It simply adjusts its own behavior based on the observed outcome. If others defect, and the threshold isn't met, my strategy will also defect in subsequent rounds (if the conditions for defection are met), not out of spite, but out of rational self-interest in a game where cooperation is no longer beneficial.

### Pseudocode

```python
# Game Parameters (globally accessible)
# n: number of players
# r: number of rounds
# m: minimum cooperators needed
# k: reward factor

# History (maintained across rounds)
# This would store a list of lists, where each inner list represents a round's actions.
# Example: history = [[action_p1_r1, action_p2_r1, ...], [action_p1_r2, ...]]
# Where 'C' or 'D'
game_history = [] # Initialize as an empty list

def get_my_action(current_round_number, my_player_index):
    """
    Determines the action (C or D) for the current player based on game history.

    Args:
        current_round_number (int): The current round number (1-indexed).
        my_player_index (int): The index of the current player (0-indexed if using 0-based list for history).

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # Edge Case: First Round
    if current_round_number == 1:
        return 'C'

    # Edge Case: Last Round
    if current_round_number == r:
        return 'D'

    # --- Standard Rounds (t > 1 and t < r) ---

    # Get actions from the previous round
    previous_round_actions = game_history[current_round_number - 2] # 0-indexed access for history

    # Count other players' cooperation and defection in the previous round
    n_c_observed = 0
    n_d_observed = 0
    for i, action in enumerate(previous_round_actions):
        if i != my_player_index: # Exclude my own action from the previous round (if recorded)
            if action == 'C':
                n_c_observed += 1
            else:
                n_d_observed += 1

    # My target is m-1 other cooperators
    required_others_for_cooperation = m - 1

    # Decision Logic:
    if n_c_observed >= required_others_for_cooperation:
        # Enough others cooperated in the last round, continue cooperating.
        return 'C'
    else:
        # Not enough others cooperated in the last round.
        # Check if my cooperation *could* still lead to success.
        # Total players = n. Number of other players = n-1.
        # If n_c_observed + n_d_observed (all other players' actions) is less than n-1, it means
        # we don't have full info on previous round. But in perfect info game, this means this case is unlikely.
        # The critical check is whether my own cooperation would make a difference.
        if (n_c_observed + 1) < m: # If even with my cooperation, the threshold is not met
            # My cooperation is unlikely to trigger the reward and I'd lose my endowment.
            return 'D'
        else:
            # My cooperation *might* tip the scales, or we are in an uncertain state.
            # Err on the side of cooperation to try and achieve the collective good.
            return 'C'

# --- How to use this strategy in a tournament ---
# For each round:
# 1. Call get_my_action(current_round, my_index) to get my action.
# 2. Record my action.
# 3. After all players have acted, update game_history with all actions for the round.
```

**Robustness:**

This strategy is designed to be robust because:

*   **Adaptability:** It continuously learns from the observed behavior of other players and adjusts its own actions accordingly.
*   **Parameter Awareness:** It implicitly adapts to different `n`, `m`, and `k` values. For example, a higher `m` will naturally lead to more defections if coordination is poor. A higher `k` incentivizes cooperation generally, but my strategy only follows if the threshold looks achievable.
*   **No Assumptions about Others:** It doesn't assume others will cooperate, nor does it expect specific patterns. It reacts to observed outcomes.
*   **Clear Exit Strategy:** The defection in the last round is a deterministic and rational response to the end of the repeated game, preventing exploitation.

This "Commit to Cooperative Threshold" strategy aims to be a reliable and principled participant, striving for collective success while prudently safeguarding its own interests when cooperation becomes clearly unproductive.
'''

description_COOPERATIVE_87 = '''
This strategy, called **Adaptive Threshold Cooperation (ATC)**, aims to maximize the total payoff by fostering cooperation while remaining robust to defection and ensuring a reasonable individual payoff. It's designed to be adaptive to the emerging behavior of other players and to exploit the repeated nature of the game.

## Adaptive Threshold Cooperation (ATC) Strategy

The core idea behind ATC is to maintain a **dynamic cooperation threshold**. This threshold, which is initially set to be slightly higher than the minimum required `m`, adjusts based on the success of past cooperation efforts. The strategy seeks to encourage collective action by rewarding observed cooperation and penalizing defection, without being overly naive or overly aggressive.

### 1. Decision Rules

ATC's decision in any given round is based on two primary factors: the **current cooperation level** observed in the previous round, and the **accumulated success** of cooperation attempts.

Let:
*   `C_t-1` be the number of players who cooperated in the previous round `t-1`.
*   `S_t-1` be the "cooperation success score" at the end of round `t-1`. This score reflects how well past cooperation has led to payoffs.

**Cooperation Threshold (`T`):**
We'll maintain a dynamic threshold `T`. Initially, `T` will be slightly higher than `m` to encourage a cautious approach.

**Cooperation Success Score (`S`):**
This score starts at 0 and is updated each round.

*   **If `C_t-1 >= m` (Threshold met in the previous round):**
    *   `S_t` = `S_t-1` + 1 (Cooperation was successful, increase the score)
    *   **Decision Rule for Player `i` in round `t`:** Cooperate (C) if `C_t-1 >= T`, otherwise Defect (D).
*   **If `C_t-1 < m` (Threshold not met in the previous round):**
    *   `S_t` = `S_t-1` - 2 (Cooperation failed, significantly decrease the score. A larger penalty discourages repeated failures).
    *   **Decision Rule for Player `i` in round `t`:** Cooperate (C) if `C_t-1 >= m` *and* `S_t-1 > 0` (meaning there's some history of success to build on), otherwise Defect (D). This rule is stricter when cooperation has failed.

**Dynamic Adjustment of `T`:**
The cooperation threshold `T` will also adapt.

*   **If `S_t` increases (cooperation successful):** `T` can be slightly lowered (e.g., `T = max(m, T - 1)`). This makes it easier to reach cooperation in the future, rewarding the observed collective effort.
*   **If `S_t` decreases (cooperation failed):** `T` will be increased (e.g., `T = min(n, T + 1)`). This makes cooperation harder to achieve, reflecting a need for more certainty before contributing.

The exact degree of adjustment for `T` would be a parameter to tune, but a small step (1 unit) is a good starting point.

### 2. Handling Edge Cases

*   **First Round (r=1 or t=1):**
    *   In the first round, there is no history. The initial strategy needs to be robust.
    *   **Decision Rule:** Cooperate (C) if `n < 2*m` (i.e., if the number of players is not much larger than the minimum required cooperators, making it risky to defect). Otherwise, Defect (D). This is a cautious initial approach.
    *   Alternatively, if we want to be *more* cooperative from the start, we could defect only if `n` is significantly larger than `m`, like `n > 1.5 * m`. For this strategy, let's adopt a moderate initial stance:
        *   **Initial `T`:** `m + 1`
        *   **Initial `S`:** 0
        *   **Decision Rule in Round 1:** Cooperate (C) if `n < 2*m`. Otherwise, Defect (D). (This is a simplified initial guess that attempts to balance risk. A purely cooperative start might be to always cooperate, but that's too vulnerable. A purely defecting start is also too conservative.) *Let's refine this for a more explicitly cooperative starting point*: **In Round 1, always Cooperate (C).** This signals a strong intention to cooperate, which is vital for establishing trust in repeated games. The adaptive mechanism will then adjust based on others' responses.

*   **Last Round (t=r):**
    *   In the final round, there's no future to exploit or build upon. This is a classic "endgame" problem in repeated games. Players might be tempted to defect to secure their immediate payoff.
    *   **Decision Rule:** **Always Defect (D).** This is a standard game-theoretic prediction for the last round of a finitely repeated game, as there's no incentive to cooperate for future benefits.

*   **Round `t` where `r - t < some_buffer_period`:**
    *   As the game approaches its end, the incentive to cooperate diminishes. To avoid being exploited by last-round defectors, the strategy can start defecting preemptively.
    *   **Decision Rule:** If `t > r - buffer_period` (where `buffer_period` is a small integer, e.g., 2 or 3 rounds, or a fraction of `r`), then **always Defect (D).** This is a proactive measure to avoid exploitation in the declining phase of the game. This `buffer_period` could also be a parameter.

### 3. Being Cooperative

The strategy is designed to be cooperative in several ways:

*   **Initial Cooperative Stance:** By cooperating in the first round, ATC signals a willingness to engage in collective action and explore cooperative outcomes. This is crucial for establishing a foundation for cooperation.
*   **Rewarding Cooperation:** The `S_t` score increases when cooperation is successful (`C_t-1 >= m`). This positive reinforcement encourages further cooperation. The dynamic threshold `T` also lowers when cooperation is successful, making future cooperation easier.
*   **Conditional Cooperation:** ATC's cooperation is *conditional* on the observed behavior of others. It doesn't blindly cooperate. It cooperates when it perceives that enough others are likely to cooperate, or when past cooperation has been rewarded.
*   **Punishing Defection (Mildly):** While the focus is on cooperation, persistent failure to meet the threshold (`S_t` decreases significantly) leads to a more cautious approach (stricter decision rule and higher `T`). This isn't a tit-for-tat punishment, but rather a self-preservation mechanism that discourages the strategy from continuing to invest in failed cooperative attempts.
*   **Adaptability:** The strategy learns from history and adapts its behavior, making it robust. It doesn't assume a fixed number of cooperators or a static opponent.

### Pseudocode for ATC Strategy

```python
# Strategy Parameters (can be tuned)
initial_T = m + 1  # Start with a threshold slightly above m
cooperation_reward = 1  # How much S increases for successful cooperation
defection_penalty = 2     # How much S decreases for failed cooperation
threshold_adjustment_step = 1 # How much T changes based on S

# State variables (maintained across rounds for each player)
T = initial_T
S = 0
last_round_cooperation_count = 0
total_payoff = 0

def decide_action(round_num, n, r, m, k, all_past_actions):
    global T, S, last_round_cooperation_count, total_payoff

    # --- Handle Edge Cases ---
    if round_num == r:
        return 'D'  # Always defect in the last round

    # Introduce a buffer period for the end game (e.g., last 3 rounds)
    buffer_period = 3
    if round_num > r - buffer_period:
        return 'D'

    # --- Initialize for Round 1 ---
    if round_num == 1:
        # Start cooperatively to signal intent
        return 'C'

    # --- Adaptive Cooperation Logic (for rounds 2 to r-1) ---
    current_cooperation_count = last_round_cooperation_count # Use count from PREVIOUS round

    # Update S based on previous round's outcome
    if current_cooperation_count >= m:
        S += cooperation_reward
    else:
        S -= defection_penalty

    # Adjust T based on S
    if S > 0 and current_cooperation_count >= m: # S increased, reward cooperation
        T = max(m, T - threshold_adjustment_step)
    elif S > 0 and current_cooperation_count < m: # S is still positive but cooperation failed
        T = min(n, T + threshold_adjustment_step)
    elif S <= 0: # S is zero or negative, significant penalty, become more cautious
        T = min(n, T + threshold_adjustment_step)

    # Ensure T is within bounds [m, n]
    T = max(m, min(n, T))

    # --- Decision Rule for Current Round ---
    if current_cooperation_count >= T:
        return 'C'  # Cooperate if we met the dynamic threshold
    elif current_cooperation_count >= m and S > 0: # If not met T, but met m and have some positive S history
        return 'C' # Try to cooperate if minimally met threshold and had some prior success
    else:
        return 'D'  # Otherwise, defect

def update_state_after_round(round_num, current_round_actions, current_round_payoff):
    global last_round_cooperation_count, total_payoff

    # Count cooperators from the current round's actions
    current_cooperators = sum(1 for action in current_round_actions if action == 'C')
    last_round_cooperation_count = current_cooperators

    total_payoff += current_round_payoff

# In a real tournament, this would be called for each round.
# For example, in round t:
# actions = decide_action(t, n, r, m, k, history)
# play actions, get payoffs
# update_state_after_round(t, actions, my_payoff)

```

**Rationale for specific choices:**

*   **Initial `T = m + 1`:** This ensures that in the first round (if not explicitly handled), the strategy wouldn't cooperate unless a majority of players (more than `m`) cooperate. However, we explicitly chose to cooperate in Round 1, making this initial `T` value less critical for the first step.
*   **`S` Update:** The `+1` for success and `-2` for failure creates a significant asymmetry. It takes two failures to undo one success, encouraging the strategy to stay cooperative if it sees *some* positive outcomes.
*   **`T` Adjustment:** Lowering `T` when `S` increases rewards successful cooperation and makes it easier to achieve. Raising `T` when `S` decreases or cooperation fails makes the strategy more cautious, reflecting the need for more players to cooperate before it's willing to contribute its endowment.
*   **Buffer Period:** A fixed buffer period (e.g., 3 rounds) is a heuristic. It recognizes that the "last round" effect can propagate backwards, and it's safer to start defecting slightly early to avoid exploitation.

This strategy aims to be a self-regulating system that learns from the collective behavior of the game, promoting cooperation when it's reciprocated and adapting to avoid being a perpetual sucker.
'''

description_COOPERATIVE_88 = '''
This is a fascinating Collective Risk Dilemma! The core challenge lies in incentivizing cooperation when individual defection can be tempting, especially as the number of cooperators nears the threshold. My cooperative strategy aims to balance the desire for collective reward with the risk of being exploited.

Here's my strategy, which I'll call the **Adaptive Threshold Cooperate (ATC)** strategy:

### Strategy Name: Adaptive Threshold Cooperate (ATC)

**Core Philosophy:** ATC aims to promote cooperation by observing the collective behavior and adjusting its own cooperation threshold dynamically. It strives to be a reliable cooperator when the group is close to achieving the reward, while also being cautious against exploitation.

---

### 1. Decision Rules

The core of ATC's decision-making in any given round (let's call the current round $t$, where $1 \le t \le r$) is based on two primary factors:

*   **Observed Cooperation Rate in the Previous Round:** The proportion of players who cooperated in round $t-1$.
*   **My Own Perception of Risk:** How confident I am that cooperation will lead to a successful outcome in the current round.

Here are the specific decision rules:

*   **Cooperate (C) if:**
    *   **Initial Cooperation Push (Early Rounds):** In the initial rounds (e.g., first $r/4$ rounds, but no less than 2), ATC will lean towards cooperation. This is to signal a willingness to cooperate and to explore if others are also inclined to cooperate. Specifically, if the number of observed cooperators in the *previous* round ($t-1$) was greater than or equal to $m - \lfloor \sqrt{n} \rfloor$, ATC will cooperate. This provides a small buffer for error and encourages those on the edge of the threshold.
    *   **Near the Threshold (Mid-to-Late Rounds):** If the number of observed cooperators in the previous round ($t-1$) was at least $m-1$, ATC will cooperate. This is a crucial adaptive step. It recognizes that even one more cooperator could tip the balance towards the collective reward.
    *   **Past Success:** If the collective reward has been achieved in at least one previous round, and the number of cooperators in the previous round ($t-1$) was greater than or equal to $m-2$, ATC will cooperate. This leverages past positive outcomes as a signal of group viability.

*   **Defect (D) if:**
    *   **High Risk of Failure (Early Rounds):** If in the initial rounds, the number of observed cooperators in the previous round ($t-1$) was significantly below the threshold (e.g., less than $m - \lfloor \sqrt{n} \rfloor$), ATC will defect. This is to avoid being a lone cooperator and incurring a payoff of 1 (losing the endowment).
    *   **Consistent Failure and Exploitation Risk:** If the collective reward has *not* been achieved in the past $2 \times \lceil r/5 \rceil$ rounds (approximately 20% of the game), and the number of cooperators in the previous round ($t-1$) was less than $m$, ATC will defect. This is a mechanism to avoid repeated exploitation when cooperation is consistently failing.
    *   **Low Cooperation with High Reward Factor (k):** If $k$ is very high (e.g., $k > n/m$), and the number of observed cooperators in the previous round ($t-1$) was less than $m-1$, ATC will defect. The reasoning here is that with a very high reward, the incentive to defect and "free-ride" is immense. If the group isn't clearly on the verge of success, it's safer to defect.
    *   **No other conditions met, and past cooperation has been punished:** If none of the cooperation conditions are met, ATC will defect.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   ATC has no history. It will default to a **cooperative** stance to signal good intentions and to test the waters. However, to avoid immediate exploitation, it will only cooperate if $m \le n/2$. If $m > n/2$, it will defect to be cautious. This is a heuristic to handle the complete uncertainty of the first round.

*   **Last Round (t=r):**
    *   In the last round, there is no future to incentivize. The incentive is purely to maximize individual payoff in this single round. ATC will apply a simplified logic:
        *   **Cooperate (C) if:** The number of cooperators in round $r-1$ was $m$ or more. This is the strongest signal that cooperation was successful and beneficial, so it's worth one last attempt.
        *   **Defect (D) otherwise.** This is a rational choice in the final round to maximize immediate personal gain if the conditions for collective success aren't clearly met.

*   **Rounds Where $m$ is Very Close to $n$ (e.g., $m > n - \sqrt{n}$):**
    *   In such scenarios, achieving the threshold is very difficult. ATC will be more cautious. It will require a higher number of observed cooperators in the previous round (e.g., $m-1$ or $m$) to trigger cooperation, especially in earlier rounds.

*   **Rounds Where $m$ is Very Small (e.g., $m=2$):**
    *   The threshold is easy to meet. ATC will be more willing to cooperate as long as there's a reasonable indication of others cooperating (e.g., at least one cooperator in the previous round).

### 3. Being Cooperative

ATC is designed with a cooperative spirit by:

*   **Initiating Cooperation:** It starts by cooperating (with a reasonable check for $m \le n/2$) to signal its intention and to encourage others.
*   **Adaptive Thresholding:** It actively seeks to meet the cooperative threshold. Its decision rules are *designed* to cooperate when the group is *close* to success (one step away), rather than only cooperating when success is guaranteed. This is a direct investment in the collective good.
*   **Rewarding Past Cooperation:** If cooperation has led to rewards in the past, ATC is more inclined to cooperate again, reinforcing positive collective outcomes.
*   **Avoiding Exploitation:** While cooperative, ATC is not naive. It incorporates mechanisms to defect if cooperation is consistently failing and it appears to be exploited, which is a necessary adaptation for survival and to continue participating in the game. This doesn't negate its cooperative intent; it's a strategic adjustment to ensure its continued ability to contribute to cooperation.

---

### Pseudocode (Conceptual)

```pseudocode
function DecideAction(current_round, history, n, m, k):
  // history is a list of dictionaries, each representing a round:
  // {'round': t, 'actions': {player_id: action, ...}, 'payoffs': {player_id: payoff, ...}}

  // --- Handle First Round ---
  if current_round == 1:
    if m <= n / 2:
      return 'C' // Cooperate to signal good intent
    else:
      return 'D' // Be cautious if threshold is very high

  // --- Get Previous Round Data ---
  previous_round_data = history.get(current_round - 1)
  observed_cooperators = count_cooperators(previous_round_data['actions'])
  total_cooperation_achieved_in_past = count_past_successes(history, m)
  num_past_rounds = current_round - 1
  recent_failure_window = 2 * ceil(r / 5) // Approx 20% of game

  // --- Cooperate Conditions ---
  cooperate = false

  // 1. Initial Cooperation Push (e.g., first r/4 rounds, min 2)
  if num_past_rounds <= max(2, r / 4):
    if observed_cooperators >= m - floor(sqrt(n)):
      cooperate = true

  // 2. Near the Threshold
  if observed_cooperators >= m - 1:
    cooperate = true

  // 3. Past Success
  if total_cooperation_achieved_in_past > 0 and observed_cooperators >= m - 2:
    cooperate = true

  // --- Defect Conditions (Overriding Cooperate if met) ---
  // If any of these are true, we will defect, overriding any cooperate flags.

  // 1. High Risk of Failure (Early Rounds)
  if num_past_rounds <= max(2, r / 4):
    if observed_cooperators < m - floor(sqrt(n)):
      return 'D'

  // 2. Consistent Failure and Exploitation Risk
  recent_rounds_history = history.slice(max(1, current_round - recent_failure_window), current_round - 1)
  recent_cooperation_failures = 0
  for round_data in recent_rounds_history:
    if count_cooperators(round_data['actions']) < m:
      recent_cooperation_failures += 1
  if recent_cooperation_failures == len(recent_rounds_history) and len(recent_rounds_history) > 0 and observed_cooperators < m:
    return 'D'

  // 3. Low Cooperation with High Reward Factor (k)
  if k > n / m and observed_cooperators < m - 1:
    return 'D'

  // --- Final Decision ---
  if cooperate:
    return 'C'
  else:
    return 'D'

// Helper functions (conceptual)
function count_cooperators(actions_dict):
  count = 0
  for player, action in actions_dict.items():
    if action == 'C':
      count += 1
  return count

function count_past_successes(history, m):
  count = 0
  for round_num in sorted(history.keys()):
    round_data = history[round_num]
    if count_cooperators(round_data['actions']) >= m:
      count += 1
  return count
```

This strategy, ATC, is designed to be adaptive, robust, and inherently cooperative, while acknowledging the strategic realities of repeated games with potential for exploitation. It aims to be a strong participant in a tournament setting.
'''

description_COOPERATIVE_89 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust:

## Cooperative Strategy: The "Cautious Optimizer"

This strategy aims to promote cooperation by incentivizing reaching the cooperation threshold, while mitigating the risk of being exploited. It learns from past rounds and adjusts its behavior accordingly.

### Core Principles:

1.  **Prioritize reaching the threshold:** The primary goal is to ensure the community project succeeds, as this yields the highest potential payoff for everyone when cooperation is sufficient.
2.  **Balance risk and reward:** While cooperation is beneficial, defecting when cooperation is insufficient is a necessary survival mechanism to avoid losing one's endowment.
3.  **Learn and adapt:** The strategy will observe the behavior of others and adjust its cooperation level based on the observed success rate of past cooperative efforts.
4.  **Initial caution:** Start with a degree of caution to observe the environment and avoid immediate exploitation.

### 1. Decision Rules:

The decision to cooperate or defect in any given round `t` (where `t > 0`, with round 0 being the first round) is based on the **observed cooperation rate in the previous round** and a **dynamic threshold adjustment**.

Let:
*   `ObservedCooperationRate_t-1` be the fraction of players who cooperated in round `t-1`.
*   `LastRoundSuccess_t-1` be `True` if `ObservedCooperationRate_t-1 * n >= m`, and `False` otherwise.

**Strategy Logic:**

*   **If `t = 0` (First Round):**
    *   **Cooperate (C)**.
    *   *Rationale:* In the absence of any historical data, the most cooperative and optimistic approach is to initiate cooperation. This sets a cooperative tone and provides initial data.

*   **If `t > 0`:**
    *   **Calculate the "Cooperation Necessity Score" (CNS):** This score represents how crucial your cooperation is for achieving the threshold in the *current* round, given past performance.
        *   `PlayersNeededForThreshold = max(0, m - (ObservedCooperationRate_t-1 * n - 1))`
            *   This calculates how many *more* players need to cooperate to reach the threshold, assuming you yourself cooperate.
        *   `CNS = PlayersNeededForThreshold / (n - 1)`
            *   This normalizes the need for your cooperation as a fraction of the remaining players. If `n-1` is 0 (i.e., `n=1`, which is excluded by `n>=2`), this would be undefined, but the problem statement guarantees `n>=2`.

    *   **Define your "Cooperation Tendency Threshold" (CTT):** This is a dynamic value that adjusts your willingness to cooperate.
        *   **If `LastRoundSuccess_t-1` is `True`:**
            *   `CTT = base_CTT + learning_rate * (1 - ObservedCooperationRate_t-1)`
            *   *Rationale:* If the previous round was successful, it indicates a generally cooperative environment. Your willingness to cooperate increases slightly if the observed cooperation rate was *low* (meaning your cooperation was more impactful) and decreases slightly if it was very high (meaning your cooperation was less critical). This encourages continued effort when needed.
        *   **If `LastRoundSuccess_t-1` is `False`:**
            *   `CTT = base_CTT - learning_rate * ObservedCooperationRate_t-1`
            *   *Rationale:* If the previous round failed, it indicates a potentially less cooperative environment or that the threshold was just missed. Your willingness to cooperate decreases, particularly if the observed cooperation rate was high (meaning others didn't step up). This makes you more cautious.

    *   **Decision:**
        *   **If `CNS <= CTT`:** **Cooperate (C)**.
            *   *Rationale:* If the need for your cooperation (CNS) is at or below your current tolerance for risk (CTT), you contribute. This means either the threshold is easily reachable with your help, or your past experience hasn't made you overly hesitant.
        *   **Else (`CNS > CTT`):** **Defect (D)**.
            *   *Rationale:* If the need for your cooperation is high (you are critical to reaching the threshold), but your current tolerance for risk (CTT) is low (due to past failures or low observed cooperation), you defect to protect your endowment.

    *   **Define `base_CTT` and `learning_rate`:**
        *   `base_CTT`: A starting value for your willingness to cooperate when past rounds were successful. A reasonable starting point is `0.5`.
        *   `learning_rate`: Controls how quickly your CTT adjusts based on past outcomes. A value around `0.2` to `0.3` is often effective for balancing responsiveness and stability.

### 2. Handling Edge Cases:

*   **First Round (t=0):** As mentioned, always Cooperate (C). This is the most straightforward way to begin and gather initial data.
*   **Last Round (t=r):**
    *   The strategy remains the same. However, the implicit incentives change. In the final round, there's no future for the community project to benefit. This can lead to a rush to defect.
    *   Our strategy's logic naturally handles this. If previous rounds were successful, `CTT` might be higher. If previous rounds failed, `CTT` will be lower. The `CNS` calculation still applies. The "Cautious Optimizer" will assess the likelihood of success based on the *historical data up to that point*.
    *   *Consideration for a truly robust final round:* If the game were designed with a known final round that was publicly announced beforehand, one might consider a strategy that defects in the final round *unless* a very high past cooperation rate has been consistently observed. However, given the "repeated game" description and no explicit "last round incentive manipulation," the general rule applies. The strategy will self-correct based on observed history.

### 3. Being Cooperative:

This strategy is cooperative for the following reasons:

*   **Initiates cooperation:** It starts by cooperating, setting a positive example.
*   **Rewards successful cooperation:** When the community succeeds, it reinforces the tendency to cooperate in the future (higher `CTT` when `LastRoundSuccess` is `True`).
*   **Adapts to failures constructively:** When cooperation fails, it doesn't immediately give up but rather becomes more cautious, reducing the risk of being exploited, while still allowing for cooperation if the "cost" (CNS) is low. This prevents a downward spiral of mutual defection driven by fear of exploitation.
*   **Encourages others:** By signaling a willingness to cooperate, especially when the threshold is within reach, it can nudge other players towards cooperation. The CNS mechanism implicitly shows how vital individual contributions are.

### Pseudocode:

```python
# Game Parameters (assumed global or passed in)
# n: number of players
# r: number of rounds
# m: minimum cooperators needed
# k: reward factor

# Strategy Parameters
base_CTT = 0.5
learning_rate = 0.25 # Example value

# History Tracking (assumed to be maintained outside the decision function)
# history = list of dictionaries, where each dict represents a round:
# history[t] = {'my_action': 'C' or 'D', 'others_actions': ['C', 'D', ...]}

def decide_action(round_number: int, history: list) -> str:
    """
    Determines the action for the current round based on game history.

    Args:
        round_number: The current round number (0-indexed).
        history: A list of dictionaries, where each dictionary represents a past round.
                 Each entry in history looks like:
                 {'my_action': 'C' or 'D', 'all_actions': ['C', 'D', 'C', ...]}

    Returns:
        'C' for Cooperate, 'D' for Defect.
    """

    if round_number == 0:
        return 'C' # Always cooperate in the first round

    # --- Analyze the previous round ---
    previous_round_index = round_number - 1
    if previous_round_index >= len(history):
        # Should not happen if history is maintained correctly, but as a safeguard
        return 'D' # Default to defect if history is missing for previous round

    prev_round_data = history[previous_round_index]
    # Count total cooperators in the previous round
    observed_cooperators = prev_round_data['all_actions'].count('C')
    observed_cooperation_rate = observed_cooperators / n
    
    last_round_success = observed_cooperators >= m

    # --- Calculate Cooperation Necessity Score (CNS) ---
    # How many more players are needed to reach the threshold, assuming *I* cooperate?
    # If I cooperate, there are (observed_cooperators + 1) potential cooperators.
    # If this is >= m, then 0 more are needed from others.
    
    potential_cooperators_if_i_cooperate = observed_cooperators + 1
    players_needed_for_threshold = max(0, m - potential_cooperators_if_i_cooperate)
    
    # Normalize the need for my cooperation relative to other players
    # If n-1 is 0 (n=1, impossible by rules), this is ill-defined.
    # But since n >= 2, n-1 is at least 1.
    cns = players_needed_for_threshold / (n - 1) if (n - 1) > 0 else 0 # Avoid division by zero, though n>=2

    # --- Calculate Cooperation Tendency Threshold (CTT) ---
    current_CTT = base_CTT

    if last_round_success:
        # If last round succeeded, increase CTT slightly if observed cooperation was low
        # Decrease CTT slightly if observed cooperation was high (less critical for me to join)
        current_CTT = base_CTT + learning_rate * (1 - observed_cooperation_rate)
    else:
        # If last round failed, decrease CTT, especially if observed cooperation was high
        # (others didn't step up despite high potential)
        current_CTT = base_CTT - learning_rate * observed_cooperation_rate

    # Clamp CTT to a reasonable range (e.g., 0 to 1)
    current_CTT = max(0.0, min(1.0, current_CTT))

    # --- Make the Decision ---
    if cns <= current_CTT:
        return 'C'  # Cooperate if the need for my help is within my tolerance
    else:
        return 'D'  # Defect if I'm critical but my tolerance is low

```

This "Cautious Optimizer" strategy aims to strike a balance between promoting the collective good and protecting oneself, adapting dynamically to the emergent behavior of other players in the tournament.
'''

description_COOPERATIVE_90 = '''
## Cooperative Strategy for the Collective Risk Dilemma

This strategy aims to maximize individual and collective long-term payoff by fostering cooperation when it's beneficial and protecting against exploitation. It's adaptive, learning from past rounds and adjusting behavior accordingly.

**Core Philosophy:** "Cooperate as long as cooperation appears to be mutually beneficial, but be prepared to defend against exploitation."

---

### 1. Decision Rules

The core of the strategy revolves around assessing the likelihood of reaching the cooperation threshold (m) and the perceived trustworthiness of other players.

**Key Metrics:**

*   **`cooperation_count_prev_round`**: The number of players who cooperated in the previous round.
*   **`my_cooperation_history`**: A boolean value indicating whether player `i` cooperated in the previous round.
*   **`cooperation_threshold_met_prev_round`**: A boolean indicating if the threshold `m` was met in the previous round.
*   **`trust_score`**: A dynamic score representing the perceived likelihood of other players cooperating when cooperation is needed. Initially, all players are considered neutral, and this score adjusts based on their actions. This can be a simple count of how many times a player has *not* defected when cooperation was necessary.

**Decision Logic (for round `t` > 1):**

1.  **If `cooperation_threshold_met_prev_round` is TRUE:**
    *   **Cooperate (C):** This indicates that collective effort was successful in the previous round. The reward `k` was achieved, and it benefited all players. Continued cooperation is the optimal path to sustain these gains. This is the default behavior when collective success has been demonstrated.

2.  **If `cooperation_threshold_met_prev_round` is FALSE:**
    *   **Assess the `cooperation_count_prev_round` relative to `m`:**
        *   **If `cooperation_count_prev_round` is significantly close to `m` (e.g., `cooperation_count_prev_round >= m - 2` for a small buffer, or a dynamically adjusted percentage):**
            *   **Cooperate (C):** This indicates that the group was *almost* successful. There's a strong incentive to try again, as one or two more cooperators would have triggered the reward. This also signals to others that you are willing to push for collective success.
        *   **If `cooperation_count_prev_round` is well below `m`:**
            *   **Evaluate perceived `trust_score` of other players:**
                *   **If a sufficient number of players (`> m/2` or a similar dynamic threshold) have a high `trust_score` (meaning they have a history of cooperation when needed, or haven't been consistently defecting):**
                    *   **Cooperate (C):** Despite the previous failure, the presence of reliable cooperators suggests that collective success is still achievable with these players. This is a calculated risk.
                *   **If the `trust_score` of a sufficient number of players is low, or if you observe a pattern of widespread defection:**
                    *   **Defect (D):** The risk of contributing and not reaching the threshold is too high. It's safer to secure your individual endowment. This is a defensive measure against a likely failure or a group unwilling to cooperate.

---

### 2. Handling Edge Cases

**First Round (t = 1):**

*   **Cooperate (C):** In the absence of any history, the most cooperative and optimistic approach is to initiate cooperation. This sets a positive precedent and signals a willingness to work towards collective benefit. If `m=2` and `n=2`, cooperating is clearly optimal. Even with larger `n` and `m`, a single round of cooperation is a small cost for potentially unlocking future benefits.

**Last Round (t = r):**

*   **Cooperate (C) if `cooperation_threshold_met_prev_round` was TRUE in round `r-1`:** Even in the last round, if the group has successfully cooperated in the penultimate round, it's still beneficial to cooperate again to secure the reward for the final round.
*   **Defect (D) if `cooperation_threshold_met_prev_round` was FALSE in round `r-1`:** If the previous round failed to meet the threshold, and there's no future to incentivize further cooperation, there's no benefit to risking your endowment. Secure your private payoff.

**Low `r` (e.g., `r=2`):**

*   The strategy remains largely the same. The first round is an initial cooperative move. The second round's decision will be based entirely on the outcome of the first round. If the first round failed, the second round will likely be a defection unless there's very strong evidence of impending success.

**High `r`:**

*   The adaptive nature of the `trust_score` becomes increasingly important. The strategy will learn more about player behaviors over many rounds, allowing for more informed decisions.

**Edge Cases for `m` and `n`:**

*   **`m` is very close to `n` (e.g., `m = n-1`):** Cooperation becomes riskier, as one defector can foil the plan. The strategy will be more cautious and will only cooperate if a very high degree of trust is established or if the previous round was a success.
*   **`m` is very close to 2 (e.g., `m = 2`):** Cooperation is easier to achieve. The strategy will be more inclined to cooperate, especially if there are many players.
*   **`n` is very large, `m` is relatively small:** The influence of individual actions is diluted. The strategy will look for collective signals of cooperation.
*   **`n` is small, `m` is close to `n`:** Individual actions have a significant impact. The `trust_score` and precise observation of others' past behavior become critical.

---

### 3. Being Cooperative

This strategy is inherently cooperative for the following reasons:

*   **Default to Cooperation:** The default action in the first round and when previous cooperation was successful is to cooperate. This prioritizes the potential for collective benefit.
*   **Proactive in Achieving Threshold:** When the number of cooperators is close to `m`, the strategy chooses to cooperate to push the group towards success. This actively contributes to achieving the positive outcome for everyone.
*   **Adaptive Learning for Sustainable Cooperation:** By tracking the success of collective efforts and the behavior of other players, the strategy aims to create an environment where cooperation is consistently rewarded. It doesn't blindly cooperate, but rather seeks to identify and foster situations where cooperation is sustainable and beneficial for all.
*   **Risk Aversion as a Means to Protect Cooperation:** Defection is a last resort, employed only when there is a high probability of individual loss due to a lack of collective effort or a pattern of exploitation. This protective measure is designed to ensure the strategy can continue to participate and attempt cooperation in future rounds.

---

### Pseudocode

```python
class CooperativeStrategy:
    def __init__(self, n, r, m, k, player_id):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.player_id = player_id
        self.current_round = 0

        # History tracking
        self.round_actions = {}  # {round_num: {player_id: action}}
        self.cooperation_counts = {} # {round_num: count}
        self.threshold_met = {} # {round_num: bool}

        # Trust scores (simplified: count of times a player cooperated when threshold was not met)
        self.trust_scores = {i: 0 for i in range(n)}
        self.defect_counts_when_close = {i: 0 for i in range(n)} # Track defectors when close to m

    def decide_action(self, history):
        self.current_round += 1
        self.round_actions = history.round_actions # Assuming history object has this
        self.cooperation_counts = history.cooperation_counts
        self.threshold_met = history.threshold_met

        # Update trust scores based on previous round's outcome (if not first round)
        if self.current_round > 1:
            prev_round = self.current_round - 1
            prev_cooperation_count = self.cooperation_counts.get(prev_round, 0)
            prev_threshold_met = self.threshold_met.get(prev_round, False)

            for i in range(self.n):
                if i != self.player_id:
                    player_action = self.round_actions.get(prev_round, {}).get(i, 'D') # Default to D if no record
                    if player_action == 'C':
                        self.trust_scores[i] += 1
                    elif player_action == 'D':
                        self.trust_scores[i] = max(0, self.trust_scores[i] - 1) # Slightly penalize defection

                    if not prev_threshold_met and prev_cooperation_count >= self.m - 2 and prev_cooperation_count < self.m:
                        if player_action == 'D':
                            self.defect_counts_when_close[i] += 1
                        else:
                            self.defect_counts_when_close[i] = max(0, self.defect_counts_when_close[i] - 1)

        # Decision logic
        if self.current_round == 1:
            return 'C'  # Cooperate in the first round

        prev_round = self.current_round - 1
        prev_cooperation_count = self.cooperation_counts.get(prev_round, 0)
        prev_threshold_met = self.threshold_met.get(prev_round, False)

        if prev_threshold_met:
            return 'C'  # Continue cooperating if threshold was met

        # If threshold was not met
        if prev_cooperation_count >= self.m - 2: # If close to threshold
            # Check trust of others. A simple heuristic: more than half the players have a positive trust score.
            reliable_cooperators = sum(1 for i in range(self.n) if i != self.player_id and self.trust_scores[i] > 0)
            if reliable_cooperators >= self.m / 2: # Dynamic threshold for reliability
                return 'C'
            else:
                return 'D' # Not enough reliable cooperators, defect
        else: # Significantly below threshold
            # Assess if enough players have a good trust score overall
            reliable_cooperators = sum(1 for i in range(self.n) if i != self.player_id and self.trust_scores[i] > 2) # Higher threshold for trust
            if reliable_cooperators >= self.m / 2:
                return 'C'
            else:
                return 'D' # Not enough consistently cooperative players

        # Fallback for last round if not explicitly handled above
        if self.current_round == self.r and not prev_threshold_met:
            return 'D'

```
'''

description_COOPERATIVE_91 = '''
Okay, here's a strategy for the Collective Risk Dilemma designed to be cooperative, adaptive, and robust in a tournament setting without relying on explicit communication or predetermined coordination.

## Strategy Name: Adaptive Risk Aversion (ARA)

**Core Philosophy:** ARA aims to maximize its own long-term payoff by playing cooperatively when it perceives a reasonable chance of achieving the cooperative outcome, while protecting itself from exploitation when cooperation seems unlikely or has failed. It learns from past rounds to inform future decisions.

### 1. Decision Rules

ARA's decision in any given round `t` depends on its assessment of the collective behavior in previous rounds.

**The core decision rule is based on a rolling average of cooperation and a threshold for perceived risk.**

Let:
*   `C_t`: Number of players who cooperated in round `t`.
*   `R_avg(t-1)`: The average number of cooperators over the last `w` rounds (where `w` is a lookback window, discussed below). If `t-1 < w`, use all available past rounds.
*   `P_C(t-1)`: The proportion of players who cooperated in round `t-1` (i.e., `C_{t-1} / n`).
*   `MyCoopCount(t-1)`: The number of times *I* have cooperated up to round `t-1`.

**Decision Logic for Round `t` (where `t > 1`):**

*   **Cooperate (Play C) IF:**
    1.  The current round `t` is the **first round where the total number of cooperators (across all players) in the previous round `t-1` was `m` or more**, AND my own cooperation history suggests a willingness to continue: `MyCoopCount(t-1) >= (t-1) * 0.5` (i.e., I've cooperated at least half the time so far).
    2.  OR **If the rolling average of cooperators `R_avg(t-1)` is above a certain adaptive threshold**, AND the immediate past round `t-1` was also cooperative (or showed promise): `P_C(t-1) >= (m - 1) / n` (meaning at least `m-1` other players cooperated in the last round).
    3.  OR **If I have been defecting and the number of defections is becoming "too high"**, and there's a *slight* possibility of success by cooperating again, especially if the last round was close to the threshold: `MyCoopCount(t-1) < (t-1) * 0.3` (i.e., I've defected more than 70% of the time) AND `P_C(t-1) >= (m-2)/n` (at least `m-2` others cooperated in the last round). This is a tentative attempt to "jumpstart" cooperation if it's failing badly.

*   **Defect (Play D) IF:**
    1.  **The previous round `t-1` had fewer than `m` cooperators**, AND my own cooperation history is lower than 50%: `MyCoopCount(t-1) < (t-1) * 0.5`. This is a protective measure.
    2.  OR **If the rolling average of cooperators `R_avg(t-1)` has dropped significantly below the target `m`**, suggesting a trend towards defection from others.
    3.  OR **If I have cooperated recently and the outcome was *not* successful (fewer than `m` cooperators), and my own cooperation history is high**, I will defect to try and exploit the situation or recalibrate.

**Adaptive Thresholds and Parameters:**

*   **Lookback Window (`w`):** A small window, perhaps `w = 3` or `w = 5`, to gauge recent trends without being overly influenced by very old data.
*   **Cooperation Momentum (`CM`):** If `P_C(t-1) >= (m-1)/n`, this indicates positive momentum towards cooperation.
*   **Risk Aversion Threshold (`RAT`):** If `R_avg(t-1) < m * 0.7` (i.e., the average number of cooperators is less than 70% of the minimum required), ARA becomes more risk-averse and more likely to defect.

**Simplified Pseudocode (for round `t > 1`):**

```pseudocode
function ARA_Decision(history, parameters):
  n = parameters.n
  m = parameters.m
  k = parameters.k
  r = parameters.r
  current_round = history.size() + 1 // history stores rounds 0 to t-2

  if current_round == 1: // Special handling for the first round (see below)
    return C

  // Calculate historical metrics
  total_cooperators_last_round = count_cooperators_in_round(history[-1])
  my_coop_count_so_far = count_my_cooperation_in_history(history)
  proportion_cooperated_last_round = total_cooperators_last_round / n
  average_cooperators_last_w_rounds = calculate_rolling_average(history, w=3) // Use last 3 rounds

  // Decision Logic
  if total_cooperators_last_round >= m:
    // If we successfully met the threshold last round:
    if my_coop_count_so_far >= (current_round - 1) * 0.5: // I've been cooperative enough
      return C // Continue cooperating
    else:
      return D // Rebalance if I haven't pulled my weight

  elif total_cooperators_last_round >= m - 1:
    // If we were close (m-1 cooperators):
    if average_cooperators_last_w_rounds >= m * 0.8: // Good recent trend
      return C // Try to push it over the edge
    else:
      return D // Trend is negative, better to protect

  else: // Fewer than m-1 cooperators last round
    // If I've been defecting a lot and there's a small chance
    if my_coop_count_so_far < (current_round - 1) * 0.3 and proportion_cooperated_last_round >= (m - 2) / n:
      return C // Tentative cooperation attempt

    // Otherwise, defect to protect
    return D

// Helper functions would be defined for counting, averaging, etc.
```

### 2. Handling Edge Cases

*   **First Round (r=1):** In the very first round, there is no history to analyze. To embody the cooperative spirit and initiate a cooperative trend, ARA will **always cooperate (C)** in the first round. This is a crucial starting point for potentially building trust and achieving the cooperative outcome.

*   **Last Round (r):** In the last round, the incentive structure changes significantly. There's no future punishment for defection or reward for past cooperation. ARA will adopt a more **risk-averse, self-interested strategy**. It will analyze the history up to the penultimate round.
    *   If the historical data strongly suggests that the cooperative threshold (`m`) has been consistently missed and the trend is negative, ARA will **defect (D)** in the last round to secure its private endowment.
    *   If there's a **very strong historical signal of consistent cooperation leading to success in previous rounds**, and the current round *could* still reach the threshold with a cooperative push, ARA might cooperate. However, the bias is towards defection in the final round due to the lack of future repetition. A simple rule for the last round would be: If `total_cooperators_last_round >= m`, cooperate. Otherwise, defect.

*   **Low `m` and `n`:** If `m` is close to `n`, cooperation becomes very difficult. ARA will become more cautious. If `m` is only slightly larger than 1, ARA might be more willing to experiment with cooperation. The adaptive thresholds will naturally adjust to these parameter changes.

*   **High `k`:** A high `k` amplifies the reward for cooperation, making ARA more inclined to cooperate if the probability of success seems even moderately high. The adaptive thresholds will tend to lean towards cooperation more readily.

### 3. Be Cooperative

ARA is designed to be cooperative by:

*   **Initiating Cooperation:** Always cooperating in the first round is a direct signal of cooperative intent.
*   **Conditional Cooperation:** It cooperates when there's a reasonable chance of success (i.e., when the number of cooperators is approaching `m`, or has recently met it). It doesn't defect out of pure spite or without evidence of others' defection.
*   **Learning and Reciprocation:** It implicitly reciprocates cooperation. If others cooperate and the threshold is met, ARA is highly likely to continue cooperating. If others defect and the threshold is missed, ARA becomes more defensive.
*   **Long-Term Focus (except last round):** For all rounds except the last, ARA prioritizes achieving the mutually beneficial cooperative outcome over short-term gains from defection, provided the risk of being exploited is managed.
*   **Adaptive Strategy:** It doesn't rigidly stick to a pattern. It learns and adjusts, which allows it to adapt to various opponent strategies and find mutually beneficial outcomes when possible.

**In essence, ARA seeks to be a "conditional cooperator" that learns from the aggregate behavior of the group. It aims to tip the scales towards cooperation when possible but will protect itself when cooperation is clearly failing or when it's exploited.**
'''

description_COOPERATIVE_92 = '''
This is a fascinating challenge! Designing a cooperative strategy in a "Collective Risk Dilemma" where defection can be individually highly rewarding requires careful consideration of the repeated nature of the game and the potential for learning.

Here's a strategy designed to be cooperative, adaptive, and robust, focusing on signaling and responding to others' actions without explicit communication.

## Strategy: "Conditional Cooperation with Escalation Response" (CCER)

The core idea of CCER is to start cooperatively, signal a willingness to achieve the collective good, and then adapt based on the collective behavior observed. It aims to maximize total payoff by encouraging cooperation while safeguarding against exploitation.

### 1. Decision Rules:

CCER's decision in any given round `t` (where `t` ranges from 1 to `r`) is determined by the observed level of cooperation in the *previous* round (`t-1`).

*   **Initial State (Round 1): Cooperate (C)**
    *   In the very first round, the strategy defaults to **Cooperate**. This signals a willingness to invest in the collective project from the outset and sets a cooperative tone.

*   **Subsequent Rounds (t > 1): Conditional Cooperation**

    Let `C_t-1` be the number of players who cooperated in round `t-1`.

    *   **If `C_t-1` >= `m` (Threshold Met or Exceeded): Cooperate (C)**
        *   If the collective goal was achieved in the previous round, CCER continues to **Cooperate**. This reinforces successful cooperation and incentivizes others to continue. The reward `k` makes this a favorable outcome.

    *   **If `m` > `C_t-1` >= `m - penalty_threshold` (Near Miss, but Close): Cooperate (C)**
        *   If the number of cooperators was close to the threshold (but still below it), CCER also **Cooperates**. This is a crucial adaptive element. It acknowledges that achieving the threshold can be difficult and that a few defections can derail the collective. By continuing to cooperate, CCER provides another opportunity for the group to reach `m` without immediate punishment. The `penalty_threshold` is a parameter that needs to be determined (see strategy refinements).

    *   **If `C_t-1` < `m - penalty_threshold` (Significant Failure to Cooperate): Defect (D)**
        *   If the number of cooperators in the previous round was significantly below the threshold (indicating a widespread lack of cooperation or a deliberate failure to meet the threshold), CCER will **Defect**. This acts as a punitive measure. By defecting, CCER protects its own endowment and signals that continued low cooperation will not be rewarded. This is a form of "escalation response" to a breakdown in collective action.

    *   **If `C_t-1` = 0 (No one Cooperated): Defect (D)**
        *   A special case of the above, if no one cooperated in the previous round, CCER will definitely **Defect**.

### 2. Handling Edge Cases:

*   **First Round (t=1):** As stated above, CCER always **Cooperates** in the first round. This is a commitment to initial cooperation, regardless of the parameters.

*   **Last Round (t=r):** The strategy remains the same. However, the *implication* of the last round is that there are no future rounds for reciprocal behavior. This means that the incentive to cooperate to foster future cooperation disappears.
    *   If the strategy is in a "Cooperate" state (meaning the threshold was met in the second-to-last round), it will **Cooperate** in the last round.
    *   If the strategy is in a "Defect" state (meaning the threshold was not met in the second-to-last round), it will **Defect** in the last round.
    *   **Crucially, the decision logic for the last round is the same as any other round, based on the *previous* round's outcome.** There is no special rule for the absolute last round *beyond* the standard rule applied to round `r` based on round `r-1`. This avoids creating a predictable "defect in the last round" scenario.

*   **Parameter Sensitivity:** The strategy is explicitly designed to be parameter-dependent by its core logic (using `m` and `k` implicitly in the decision-making regarding thresholds).

### 3. Being Cooperative:

*   **Initial Cooperative Stance:** The strategy begins by cooperating, demonstrating a commitment to the collective good.
*   **Rewarding Cooperation:** It continues to cooperate when the collective has successfully met the threshold, actively rewarding and reinforcing cooperative behavior.
*   **Forgiveness and Second Chances:** The "Near Miss" condition (`m` > `C_t-1` >= `m - penalty_threshold`) demonstrates a degree of forgiveness and a willingness to give the group another chance to succeed, rather than immediately punishing minor failures. This is key to building trust and sustained cooperation.
*   **Focus on Collective Outcome:** The strategy's decisions are driven by the *collective* outcome (`C_t-1`) rather than solely by its own potential immediate gain.

### Strategy Refinements and Parameter Choices:

*   **`penalty_threshold`**: This is a critical parameter to tune. It defines how close to the threshold the group needs to be before CCER starts defecting.
    *   A small `penalty_threshold` (e.g., 1 or 2) means CCER is quick to punish failures. This can be good if opponents are purely self-interested and will exploit any leniency.
    *   A larger `penalty_threshold` means CCER is more forgiving and will continue to cooperate even with significant failures to reach `m`. This might be better in environments where occasional slip-ups are common but the overall tendency is cooperative.
    *   **Suggested starting value:** `penalty_threshold` = `min(m - 1, 2)` or a small fixed number like `2`. The `m-1` ensures we don't defect if only one person was missing.

*   **Observing `k`:** While not explicitly in the decision rule, the strategy implicitly leverages the value of `k`. The fact that `k > 1` makes successful cooperation (payoff `0 + k` for cooperators, `1 + k` for defectors if threshold met) significantly better than failure (payoff `1` for defectors, `0` for cooperators if threshold not met). This makes the *risk* of collective failure high, justifying the punitive defection when cooperation significantly breaks down.

### Pseudocode:

```python
# Game Parameters (provided externally)
n = 6  # number of players
r = 10 # number of rounds
m = 3  # minimum cooperators needed
k = 2.0 # reward factor

# Strategy Parameters
penalty_threshold = min(m - 1, 2) # Example: 2 if m >= 3, 1 if m == 2

# State for tracking past cooperation
# This would need to be stored and updated by the game engine,
# but for this strategy definition, we imagine having access to it.
# Example: history_of_cooperators_per_round = [num_cooperators_round_1, num_cooperators_round_2, ...]

def decide_action(current_round: int, history_of_cooperators_per_round: list[int]) -> str:
    """
    Decides whether to Cooperate (C) or Defect (D).

    Args:
        current_round: The current round number (1-indexed).
        history_of_cooperators_per_round: A list of the number of players
                                           who cooperated in each previous round.
                                           Index 0 is round 1, etc.

    Returns:
        'C' for Cooperate, 'D' for Defect.
    """

    if current_round == 1:
        # Always cooperate in the first round to signal intent
        return 'C'
    else:
        # Get the number of cooperators from the *previous* round
        # Note: history_of_cooperators_per_round is 0-indexed, so last element is previous round
        previous_round_cooperators = history_of_cooperators_per_round[-1]

        if previous_round_cooperators >= m:
            # Threshold was met or exceeded, continue cooperating
            return 'C'
        elif m > previous_round_cooperators >= (m - penalty_threshold):
            # Near miss, but close. Give the group another chance.
            return 'C'
        else: # previous_round_cooperators < (m - penalty_threshold)
            # Significant failure to cooperate, escalate to defection
            return 'D'

# Example Usage (if we had actual history):
# For round 3, if history_of_cooperators_per_round was [3, 1]
# decide_action(current_round=3, history_of_cooperators_per_round=[3, 1])
# previous_round_cooperators = 1
# m = 3
# penalty_threshold = 2
# 1 < (3 - 2) which is 1 < 1. This condition is FALSE.
# So it would fall into the 'else' block and return 'D'.
# If penalty_threshold was 1, then 1 >= (3-1) which is 1 >= 2. False. Still 'D'.

# If history was [3, 2] and penalty_threshold = 2
# previous_round_cooperators = 2
# m = 3
# m > previous_round_cooperators >= (m - penalty_threshold)  => 3 > 2 >= (3 - 2) => 3 > 2 >= 1. This is TRUE.
# It would return 'C'.
```

This strategy aims to be robust by not being overly aggressive, providing opportunities for cooperation to recover, but also not being a pushover if the group consistently fails to meet its obligations. It learns from the collective and adapts its behavior accordingly.
'''

description_COOPERATIVE_93 = '''
This is an intriguing variation of the public goods game with a clear risk-reward dynamic. Designing a purely cooperative strategy that is also adaptive and robust is the core challenge. The key is to balance the desire to cooperate and reap the collective reward with the need to protect oneself from exploitation by defectors.

Here's a proposed cooperative strategy designed for this Collective Risk Dilemma:

## Strategy: Adaptive Conditional Cooperation with Escalation

This strategy aims to foster cooperation by initially signaling willingness to cooperate, but it also incorporates a mechanism to punish defection and adapt to the prevailing behavior in the game. It's built on the idea of learning from the past and adjusting future actions based on observed outcomes.

### 1. Decision Rules

The core of the strategy revolves around conditional cooperation, with an escalating response to defection.

*   **Initial State (Round 1):** We will *always cooperate* in the first round. This acts as a signal of good faith and an attempt to initiate a cooperative equilibrium. We are taking the risk to see if others will reciprocate.

*   **Subsequent Rounds (t > 1):** The decision to cooperate or defect in round `t` depends on the *cooperation rate observed in the previous round `t-1`*. Let `C_{t-1}` be the number of players (including ourselves) who cooperated in round `t-1`.

    *   **If `C_{t-1} >= m` (Threshold Met in Previous Round):**
        *   **Decision:** **Cooperate (C)**
        *   **Rationale:** The collective project was successful in the previous round, leading to a higher payoff for everyone. This reinforces the benefit of cooperation and encourages us to continue. This is the desired cooperative equilibrium.

    *   **If `m - x <= C_{t-1} < m` (Threshold Nearly Met, `x` is a small buffer, e.g., `x=1` or `x=2`):**
        *   **Decision:** **Cooperate (C)**
        *   **Rationale:** The previous round was close to success. While not the full reward, it still indicates significant collective effort. Continuing to cooperate might be enough to tip the balance in the current round, especially if others are also close to cooperating. This is a cautious approach to encourage the last few cooperators.

    *   **If `C_{t-1} < m - x` (Threshold Significantly Missed, and Defectors are Prevalent):**
        *   **Decision:** **Defect (D)**
        *   **Rationale:** If the cooperation threshold was significantly missed in the previous round, it indicates a strong propensity for defection among the players. Continuing to cooperate in this scenario would be exploitative, leading to lower payoffs for us while others benefit from our cooperation without contributing. This is a punitive response to discourage future defection. The magnitude of the deficit `(m - C_{t-1})` could potentially inform *how long* we defect for, but for simplicity and robustness, a simple switch to defection is chosen.

*   **Escalation Trigger (Optional but adds robustness):**
    *   If we have defected for `y` consecutive rounds (e.g., `y=3`) and the cooperation rate *still* remains below `m - x`, we might consider *continuing to defect* indefinitely until a substantial shift in observed cooperation occurs. This prevents us from being perpetually exploited in a consistently uncooperative environment.

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated above, **always Cooperate (C)**. This is the baseline cooperative action to initiate the game.

*   **Last Round (t=r):** The decision rule still applies based on the previous round's outcome. However, the *long-term implications* of defection are removed. If the previous round was unsuccessful, defecting in the last round is purely self-preservation. If the previous round was successful, cooperating is still the best option to secure the reward. There's no need for special last-round logic beyond the established rules, as the "repeated game" aspect is what influences future behavior, and that influence is now absent.

*   **Low `m` relative to `n`:** If `m` is very low (e.g., `m = 2` or `m = 3` with `n=10`), the strategy will likely converge to cooperation quickly if even a few players cooperate. The buffer `x` becomes more critical here to avoid prematurely defecting when success is almost guaranteed.

*   **High `m` relative to `n`:** If `m` is high (e.g., `m = n-1`), cooperation is very difficult. The strategy will be heavily reliant on *everyone* cooperating. If even one player defects, the threshold is missed. The strategy will then likely enter a cycle of defection unless a critical mass of cooperation is observed.

*   **Dynamic `k` (if it were dynamic):** If `k` were to change, the strategy would need to be re-evaluated. However, given `k` is a fixed parameter, this is not a concern.

### 3. Being Cooperative

This strategy is fundamentally cooperative in its intent:

*   **Initial Cooperative Stance:** It starts with an act of faith (cooperating in Round 1) to *enable* cooperation.
*   **Reinforcement of Cooperation:** It rewards successful cooperative outcomes by continuing to cooperate, reinforcing the positive feedback loop.
*   **Adaptive Punishment, Not Malice:** Defection is used as a tool to *discourage exploitation* and nudge the group back towards cooperation, rather than as an end in itself. It's a form of reciprocal altruism – if you don't cooperate with me, I won't cooperate with you.
*   **Long-Term Vision:** The strategy aims for a stable, cooperative equilibrium where `C_t >= m` for all `t`, leading to maximal collective payoffs over time.

### Pseudocode

```pseudocode
// Strategy: Adaptive Conditional Cooperation with Escalation

// Parameters (available to the strategy)
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

// State variables (maintained by the strategy)
// historical_actions: a list of lists, where historical_actions[t-1] contains actions of all players in round t-1
// my_cooperation_count_last_round: number of players (including self) who cooperated in the previous round

// Define a small buffer for near-success
BUFFER_X = 2 // Can be tuned, e.g., 1 or 2

// Define a threshold for prolonged defection (optional)
PROLONGED_DEFECTION_THRESHOLD = 3

// --- Decision Logic for Round t ---
function decide_action(current_round, historical_actions):
    // Round 1: Always cooperate
    if current_round == 1:
        return "C"

    // Calculate number of cooperators in the previous round (t-1)
    my_cooperation_count_last_round = 0
    if len(historical_actions) > 0: // Ensure there's a previous round's data
        previous_round_actions = historical_actions[current_round - 2] // 0-indexed list of actions from last round
        for action in previous_round_actions:
            if action == "C":
                my_cooperation_count_last_round += 1

    // Decision rules for subsequent rounds
    if my_cooperation_count_last_round >= m:
        // Threshold met or exceeded in previous round: Continue cooperating
        return "C"
    else if my_cooperation_count_last_round >= (m - BUFFER_X):
        // Threshold nearly met: Cautiously cooperate
        return "C"
    else:
        // Threshold significantly missed: Defect
        // (Optional escalation logic could be added here to track consecutive defections)
        return "D"

// --- Example of how it would be used ---
// Assume the strategy has access to `n`, `r`, `m`, `k`
// Assume `historical_actions` is updated after each round.

// In round 1:
// action_t1 = decide_action(1, []) // Empty history

// In round 2, after round 1 actions are recorded:
// historical_actions.append(actions_from_round_1)
// action_t2 = decide_action(2, historical_actions)

// And so on for subsequent rounds.
```

**Robustness and Adaptability:**

*   **Adaptability:** The strategy dynamically adjusts its behavior based on observed outcomes. If cooperation is rewarded, it continues. If defection leads to consistently low payoffs, it switches to defection.
*   **Robustness:** It's not overly sensitive to a single outlier player in one round. The buffer (`BUFFER_X`) allows for some deviation before switching to defection. The core logic is based on the aggregate behavior in the previous round, making it robust to the specific actions of any single player (unless that player is a significant portion of the `n` players).

This strategy aims to be a reliable and ethically aligned player in the Collective Risk Dilemma tournament, prioritizing long-term collective benefit while safeguarding against exploitation.
'''

description_COOPERATIVE_94 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, focusing on adaptability and robustness, while aligning with a cooperative mindset.

## Strategy Name: "Adaptive Threshold Cooperate"

This strategy aims to achieve the collective reward by coordinating cooperation when it's likely to succeed, while defecting to safeguard its own payoff when cooperation seems unlikely or risky. It's designed to be self-regulating and learn from the history of play.

### 1. Decision Rules

The core of this strategy is to assess the probability of the cooperation threshold ($m$) being met and to adjust its own action accordingly.

**Rule 1: Initial Hesitation and Observation (First few rounds)**

*   **In the first round (t=1):** Play **D (Defect)**.
    *   **Reasoning:** In the absence of any history, it's safest to observe. Playing D ensures a baseline payoff of 1 and allows us to gather information about other players' initial tendencies without risking a personal loss if cooperation fails due to insufficient participation.

*   **In rounds 2 to approximately $\min(r, m+1)$ (Observation Phase):**
    *   Observe the actions of other players in the previous round.
    *   Calculate the number of cooperators in the previous round. Let this be $N_{C, t-1}$.
    *   **If $N_{C, t-1} \ge m$:** We have seen cooperation succeed. In the current round, play **C (Cooperate)**.
        *   **Reasoning:** This signals a willingness to contribute to a successful cooperative outcome. The past success provides confidence that others might also cooperate again.
    *   **If $N_{C, t-1} < m$:** Cooperation failed in the previous round.
        *   **If $N_{C, t-1} \ge m-1$ (i.e., only one less cooperator than needed):** Play **C (Cooperate)**.
            *   **Reasoning:** This is a strong signal that we are close to the threshold. Our cooperation might be the tipping point to achieve the reward. We take a calculated risk to encourage collective success.
        *   **If $N_{C, t-1} < m-1$:** Play **D (Defect)**.
            *   **Reasoning:** The gap to the threshold is too large. Our individual cooperation is unlikely to trigger the reward, and we would be sacrificing our private endowment for no gain, resulting in a payoff of 0.

**Rule 2: Adaptive Cooperation based on Recent Success (After Observation Phase)**

*   **For rounds $t > \min(r, m+1)$:**
    *   Observe the actions of other players in the previous round.
    *   Calculate the number of cooperators in the previous round, $N_{C, t-1}$.
    *   **If $N_{C, t-1} \ge m$:** Play **C (Cooperate)**.
        *   **Reasoning:** Past success is the strongest indicator of future success. This is our default cooperative behavior when the threshold is reliably met.
    *   **If $N_{C, t-1} = m-1$:** Play **C (Cooperate)**.
        *   **Reasoning:** Similar to the observation phase, being one cooperator short is a critical point. We are willing to be the "extra" cooperator to secure the reward for everyone. This maintains a strong cooperative incentive.
    *   **If $N_{C, t-1} < m-1$:** Play **D (Defect)**.
        *   **Reasoning:** If the number of cooperators has significantly fallen below the threshold in the recent past, it suggests a lack of widespread cooperation. Continuing to defect minimizes our losses and waits for evidence of a stronger collective will to cooperate.

**Rule 3: Final Round Adjustment (Last round, t=r)**

*   **In the last round (t=r):**
    *   Observe the actions of other players in round $r-1$.
    *   Calculate the number of cooperators in round $r-1$, $N_{C, r-1}$.
    *   **If $N_{C, r-1} \ge m$:** Play **C (Cooperate)**.
        *   **Reasoning:** Even in the final round, if cooperation has been successful, we contribute to ensure the reward is achieved for this last opportunity.
    *   **If $N_{C, r-1} = m-1$:** Play **C (Cooperate)**.
        *   **Reasoning:** Taking the risk in the very last round to cross the threshold is still a valuable cooperative gesture, as it secures the final reward.
    *   **If $N_{C, r-1} < m-1$:** Play **D (Defect)**.
        *   **Reasoning:** As in previous rounds, if the likelihood of reaching the threshold is low, we prioritize securing our private endowment.

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated in Rule 1, always play **D**. This is a conservative start to gather information without personal cost.
*   **Last Round (t=r):** The strategy explicitly addresses the last round in Rule 3. The decision is still based on the previous round's outcome, but with the added context of it being the final opportunity.
*   **Low $m$ (e.g., $m=2$):** The strategy adapts. If $m=2$, the "Observation Phase" would cover rounds 1 to 3. In round 1, we defect. In round 2, if we saw 1 cooperator (or 0), we'd defect. If we saw 2 or more cooperators, we'd cooperate. If in round 2 we saw 1 cooperator, in round 3 we'd cooperate (as $1 = m-1$). This dynamic adjustment works for small $m$.
*   **High $m$ (e.g., $m$ close to $n$):** The strategy becomes more cautious. With a high $m$, it will take longer to gather evidence of sufficient cooperation. The strategy will default to defecting more often until it observes a clear trend towards high cooperation. For instance, if $m=n-1$, the strategy will require seeing $n-2$ cooperators to consider cooperating itself.
*   **Large $r$ vs. Small $r$:** The observation phase duration is tied to $m$ and $r$ ($\min(r, m+1)$). If $r$ is very small (e.g., $r=2$), the strategy will effectively only have one "observation" round after the initial defection. If $r$ is large, it allows for a more extensive observation period, making the strategy more robust to initial erratic behavior.

### 3. Being Cooperative

This strategy is fundamentally cooperative because:

*   **Prioritizes Collective Reward:** The primary goal is to achieve the $k$ reward, which benefits all players. It actively seeks to contribute when the probability of achieving this collective good is high.
*   **Calculated Risk-Taking for Cooperation:** The strategy is willing to "go first" or be the "last" cooperator ($m-1$ to $m$) when it's strategically beneficial for the group. This demonstrates a commitment to making cooperation happen.
*   **Adaptive and Forgiving:** It doesn't hold grudges. If cooperation falters, it retreats to a safer strategy (defecting) to protect its own payoff, but it's always ready to rejoin cooperation if the conditions improve and others demonstrate a willingness to cooperate.
*   **Self-Regulating for Sustainability:** By defecting when cooperation is unlikely, it ensures it doesn't suffer repeated losses. This sustainability is crucial for long-term cooperative engagement in the tournament. It's a form of "enlightened self-interest" that aligns with long-term group benefit.

---

### Pseudocode Representation:

```python
def adaptive_threshold_cooperate(player_id, current_round, num_rounds, min_cooperators, reward_factor, history):
    """
    Adaptive Threshold Cooperate Strategy.

    Args:
        player_id (int): The ID of the current player.
        current_round (int): The current round number (1-indexed).
        num_rounds (int): The total number of rounds.
        min_cooperators (int): The minimum number of cooperators needed for the reward.
        reward_factor (float): The factor 'k'.
        history (list of dict): A list of dictionaries, where each dictionary
                                represents a round's outcome.
                                Example: [{'actions': {'player1': 'C', 'player2': 'D'},
                                           'payoffs': {'player1': 2, 'player2': 3}}]
    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    n_players = len(history[0]['actions']) if history else 0 # Infer n if possible

    # Rule 1: Initial Hesitation and Observation
    if current_round == 1:
        return 'D'

    # Determine observation phase end
    observation_phase_end = min(num_rounds, min_cooperators + 1)

    # Get actions from the previous round
    previous_round_actions = history[-1]['actions']
    cooperators_in_previous_round = sum(1 for action in previous_round_actions.values() if action == 'C')

    # Rule 1: Observation Phase (rounds 2 to observation_phase_end)
    if current_round <= observation_phase_end:
        if cooperators_in_previous_round >= min_cooperators:
            return 'C'  # Success observed, join in
        elif cooperators_in_previous_round == min_cooperators - 1:
            return 'C'  # Close to threshold, take the risk
        else: # cooperators_in_previous_round < min_cooperators - 1
            return 'D'  # Too far from threshold, defect

    # Rule 2: Adaptive Cooperation (after observation phase)
    else: # current_round > observation_phase_end
        if cooperators_in_previous_round >= min_cooperators:
            return 'C'  # Consistent success, cooperate
        elif cooperators_in_previous_round == min_cooperators - 1:
            return 'C'  # Close to threshold, take the risk
        else: # cooperators_in_previous_round < min_cooperators - 1
            return 'D'  # Cooperation is not prevalent, defect

    # Rule 3: Final Round Adjustment (implicitly handled by the above logic for t=r)
    # The logic for t=r will naturally fall into one of the conditions above based on round r-1's outcome.
    # No explicit separate rule needed as it follows the general adaptive logic.

```
'''

description_COOPERATIVE_95 = '''
Okay, here's a cooperative strategy designed for the Collective Risk Dilemma, emphasizing adaptiveness and robustness in a repeated-game setting without communication.

## Cooperative Strategy: "Conditional Reciprocity with Risk Aversion"

This strategy aims to foster cooperation by rewarding successful cooperation and punishing defection, but with a strong bias towards cooperation, especially when the risk of collective failure is high. It's designed to be adaptive to the actions of others and robust across different opponent behaviors.

### Core Principles:

1.  **Prioritize Cooperation (Especially Early and When Near Threshold):** The strategy believes that achieving the collective reward is the optimal long-term outcome. Therefore, it will lean towards cooperating unless there's clear evidence that doing so is being exploited or is futile.
2.  **Conditional Reciprocity (Tit-for-Tat variant):** If others cooperate and achieve the threshold, the strategy will continue to cooperate. If others defect and the threshold is *not* met, the strategy will temporarily defect to avoid being exploited.
3.  **Risk Aversion (Focus on Threshold):** The strategy pays particular attention to how close the group is to meeting the cooperation threshold (`m`). If the group is close to `m`, the strategy is more inclined to cooperate to push it over the edge.
4.  **Punishment is Temporary and Proportional:** Defection is used as a tool to signal dissatisfaction and deter exploitation, not as an endgame strategy. The strategy will revert to cooperation if the collective benefit seems attainable again.
5.  **No Forgiveness, but Re-evaluation:** The strategy remembers past defections but will always re-evaluate the current state of the game to decide its next move.

### 1. Decision Rules:

The decision to cooperate or defect in round `t` depends on the following factors:

*   **Current Round:** Early rounds, middle rounds, and the final round have different considerations.
*   **Past Behavior of Others:** Specifically, the number of players who cooperated in the previous round (`C_t-1`).
*   **Proximity to the Threshold:** How many players cooperated in the previous round relative to `m`.
*   **Player's Own Past Actions:** Whether the player defected in the previous round.

Here's a breakdown of the decision logic:

**Decision Logic for Player `i` in Round `t`:**

Let:
*   `C_{t-1}` be the count of players who cooperated in round `t-1`.
*   `D_{i, t-1}` be a boolean indicating if player `i` defected in round `t-1`.

**Base Case: Round 1 (`t = 1`)**

*   **Strategy:** Cooperate (C).
*   **Reasoning:** In the first round, there is no history. The strategy starts with the assumption that cooperation is the best path to maximize collective payoff. It's an optimistic initial stance.

**Subsequent Rounds (`t > 1`):**

The decision is made based on a tiered set of conditions, prioritized from most to least restrictive:

**Tier 1: Protecting Against Exploitation (High Priority)**

*   **Condition:** If `C_{t-1} < m` (the threshold was *not* met in the previous round) **AND** `D_{i, t-1}` is `True` (player `i` defected in the previous round).
    *   **Action:** Defect (D).
    *   **Reasoning:** This is a "punishment" phase. If the threshold wasn't met *and* I chose to defect last round (presumably because I didn't see enough cooperation), I should continue defecting to avoid being the only one contributing if others are also defecting. This signals dissatisfaction and prevents me from being the sole "sucker."

**Tier 2: Encouraging Cooperation When Close to Threshold (Medium-High Priority)**

*   **Condition:** If `C_{t-1} < m` (threshold not met) **AND** `C_{t-1} >= m - 1` (we were very close to the threshold in the previous round, i.e., only one player short).
    *   **Action:** Cooperate (C).
    *   **Reasoning:** This is a critical point for collective success. Even if the threshold wasn't met, if we were *almost* there, I will cooperate in this round. The potential reward (`k`) is significant, and my single cooperation might tip the balance. This is a strong cooperative signal when it matters most.

**Tier 3: General Reciprocity (Medium Priority)**

*   **Condition:** If `C_{t-1} >= m` (the threshold *was* met in the previous round).
    *   **Action:** Cooperate (C).
    *   **Reasoning:** When cooperation was successful, it's rational to continue cooperating. This reinforces successful collective action and signals that I am part of a working cooperative system.

**Tier 4: Catching Up and Re-evaluating (Lower Priority)**

*   **Condition:** If `C_{t-1} < m` (threshold not met) **AND** `C_{t-1} < m - 1` (we were more than one player short in the previous round) **AND** `D_{i, t-1}` is `False` (player `i` cooperated in the previous round).
    *   **Action:** Defect (D).
    *   **Reasoning:** If the threshold wasn't met, and I cooperated last round, but we were still far from `m`, my cooperation alone wasn't enough. To avoid being exploited by a group of defectors while I'm still trying to cooperate, I will temporarily defect. This is a signal that my individual cooperation might not be enough if the overall group isn't leaning towards cooperation. I'm *re-evaluating* whether to continue contributing if the group isn't moving towards success.

**Tier 5: Default/Fallback (Lowest Priority)**

*   **Condition:** If none of the above conditions are met (this should theoretically cover all cases, but acts as a catch-all).
    *   **Action:** Cooperate (C).
    *   **Reasoning:** This ensures that if any unforeseen situation arises, the default action is cooperation, reflecting the strategy's cooperative bias.

**Summary of Decision Logic:**

1.  **Round 1:** Cooperate.
2.  **If Threshold Met Last Round:** Cooperate.
3.  **If Threshold Not Met Last Round:**
    *   **If I Defected Last Round:** Defect (punishment).
    *   **If I Cooperated Last Round:**
        *   **If Very Close to Threshold (m-1 cooperators):** Cooperate (push for success).
        *   **If Far from Threshold:** Defect (re-evaluate, avoid exploitation).

### 2. Handling Edge Cases:

*   **First Round (`t = 1`):** As stated above, the strategy **always Cooperates** in the first round to initiate potential cooperation and gather initial data.
*   **Last Round (`t = r`):** The strategy operates according to the general decision rules. However, there's a subtle point: in the last round, there's no future consequence for defecting. A purely rational actor might defect. This strategy, however, is designed to be *cooperative in spirit*. If the history of the game has been cooperative and the threshold has been met consistently, it will likely continue to cooperate. If there's been significant defection and exploitation, it might defect in the last round, but its primary driver is the decision rules, not a specific "last round defection" rule. It doesn't explicitly change its rules for the last round, allowing its past behavior to dictate its final move.
*   **Threshold (`m`) is `n-1` or `n`:** If `m` is `n-1` or `n`, it becomes very difficult to achieve cooperation. The strategy will still apply its rules. In early rounds, it will cooperate. If few others cooperate, it might switch to defecting. The "very close to threshold" rule (`m-1`) becomes critically important. If `m=n-1`, then `m-1` cooperators means `n-2` cooperators. My cooperation could be the one that tips it over. The strategy is robust but may struggle if `m` is set very high relative to `n`.

### 3. Being Cooperative:

*   **Bias Towards Cooperation:** The default action is cooperation, and it's only overridden by specific, demonstrable evidence of exploitation or futility.
*   **Reward for Success:** The strategy actively reinforces successful cooperation by continuing to cooperate when the threshold is met.
*   **"Giving Others a Chance":** In the first round and when close to the threshold, it errs on the side of cooperation, giving others the opportunity to succeed collectively.
*   **"Conditional" Cooperation:** While not naive, its cooperation is conditional on a reasonable level of engagement from others. It's not about blind cooperation, but about fostering a *mutually beneficial cooperative ecosystem*.

### Pseudocode Representation:

```python
def strategy_conditional_reciprocity(round_num, total_rounds, num_players, threshold_m, k_factor, history):
    """
    Conditional Reciprocity with Risk Aversion Strategy

    Args:
        round_num (int): The current round number (1-indexed).
        total_rounds (int): The total number of rounds.
        num_players (int): The total number of players (n).
        threshold_m (int): The minimum number of cooperators needed (m).
        k_factor (float): The reward factor (k).
        history (list of lists): A list where history[t] is a list of actions (C/D) of all players in round t+1.
                                 history[-1] would be the actions of the previous round.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    if round_num == 1:
        return 'C'  # Always cooperate in the first round.

    # Get actions from the previous round
    previous_round_actions = history[-1]
    num_cooperators_prev_round = previous_round_actions.count('C')
    my_action_prev_round = previous_round_actions[my_player_index] # Assuming my_player_index is globally accessible or passed

    # --- Decision Logic ---

    # Tier 1: Protecting Against Exploitation (If I defected and threshold wasn't met)
    if num_cooperators_prev_round < threshold_m and my_action_prev_round == 'D':
        return 'D'

    # Tier 2: Encouraging Cooperation When Close to Threshold
    if num_cooperators_prev_round < threshold_m and num_cooperators_prev_round >= threshold_m - 1:
        return 'C'

    # Tier 3: General Reciprocity (If threshold was met)
    if num_cooperators_prev_round >= threshold_m:
        return 'C'

    # Tier 4: Catching Up and Re-evaluating (If I cooperated, threshold wasn't met, and we were far off)
    if num_cooperators_prev_round < threshold_m and num_cooperators_prev_round < threshold_m - 1 and my_action_prev_round == 'C':
        return 'D'

    # Tier 5: Default/Fallback (Should ideally not be reached if logic is exhaustive)
    # This could be 'C' for bias, or it could be 'D' to be more cautious if other conditions are ambiguous.
    # Given the strategy's bias, 'C' is more appropriate.
    return 'C'

```

This strategy aims to balance the pursuit of collective gain with self-preservation against exploitation. Its adaptive nature comes from constantly observing the group's behavior and adjusting accordingly, while its cooperative spirit is maintained by its initial actions and its inclination to cooperate when the collective goal is within reach.
'''

description_COOPERATIVE_96 = '''
This is an intriguing Collective Risk Dilemma! The core challenge lies in achieving the collective good (reaching the cooperation threshold 'm') while incentivizing individual defection if cooperation fails. Designing a purely cooperative strategy that's also robust requires a delicate balance.

Here's a strategy I'll call **"Adaptive Tit-for-Tat with Punishment and Reward Discounting" (ATTPRD)**. It's built on principles of reciprocity, risk aversion, and a nuanced approach to the repeated nature of the game.

## Adaptive Tit-for-Tat with Punishment and Reward Discounting (ATTPRD)

**Core Philosophy:** Be cooperative by default, but respond to the collective's behavior. Reward cooperation by continuing to cooperate, and punish defection by defecting, but with a clear path back to cooperation. Critically, acknowledge that the value of future rewards diminishes over time, especially in the final rounds.

---

### 1. Decision Rules: When to Cooperate vs. Defect

My decision in any given round (except the first and last, which are handled separately) will be based on the **average cooperation rate of the *other* players in the previous round**.

Let $C_{t-1}$ be the number of players who cooperated in round $t-1$.
Let $N_{others} = n - 1$ be the number of other players.
The cooperation rate of others in the previous round is $R_{others, t-1} = \frac{C_{t-1}}{N_{others}}$.

My decision in round $t$:

*   **If $R_{others, t-1}$ is "sufficiently high" relative to the cooperation threshold 'm': Cooperate (C)**
    *   **Threshold for Cooperation:** I will cooperate if the proportion of *other* players who cooperated in the previous round is **greater than or equal to a dynamically adjusted threshold**. This threshold is initially set slightly below the required cooperation rate to achieve the reward, and it can adjust downwards over time.
        *   **Initial Threshold Factor:** Let's call this a parameter $\alpha$ (e.g., $\alpha = 0.8$). My cooperation threshold in round $t$ is approximately $\alpha \times \frac{m}{n}$. However, this is not a strict cutoff. I'm looking for a *trend* of cooperation.
        *   **Refined Rule:** I will cooperate if the number of *other* players who cooperated in round $t-1$ ($C_{t-1} - c_{i,t-1}$, where $c_{i,t-1}$ is my action in $t-1$) is **at least $m-1$**. This means if I had cooperated and the threshold was met, I continue. If I had defected and the threshold was met by others, I also lean towards cooperating to encourage the collective.
        *   **Punishment Trigger:** If the number of other players who cooperated in round $t-1$ is **less than $m-1$**, I will defect (D). This is my primary punishment mechanism.

*   **Adaptive Adjustment of Cooperation Threshold:**
    *   **If the collective achieved the threshold (m or more cooperators) in round t-1:** My willingness to cooperate in round $t$ increases. I will continue to cooperate if at least $m-1$ *other* players cooperated in $t-1$.
    *   **If the collective failed to achieve the threshold (fewer than m cooperators) in round t-1:** My willingness to cooperate decreases. I will defect if at least $m-1$ *other* players did *not* cooperate in $t-1$. This is where the "punishment" comes in.

**Simplifying the rule for clarity:**

**In round $t$ (where $1 < t < r$):**

1.  **Count the number of players who cooperated in round $t-1$.** Let this be $C_{t-1}$.
2.  **If $C_{t-1} \ge m$:** My decision is to **Cooperate (C)**. (This is the reward for successful collective action).
3.  **If $C_{t-1} < m$:** My decision is to **Defect (D)**. (This is the punishment for collective failure).

**Why this is "adaptive":** It directly responds to the outcome of the previous round. If cooperation is rewarded (threshold met), I continue to cooperate, signaling continued trust and commitment. If it's punished (threshold not met), I withdraw my cooperation to avoid being exploited and to signal that the current strategy is not working.

**Why it's "robust":**
*   **Doesn't require coordination:** It only relies on observed outcomes, not on implicit agreements.
*   **Defects against defectors:** If others defect and the threshold isn't met, I defect, preventing my own exploitation.
*   **Rewards cooperators:** If others cooperate and the threshold is met, I also cooperate, reinforcing positive behavior.
*   **Forgiving (to a degree):** If cooperation fails, I defect for one round (or more, depending on others). But if others *then* start cooperating and the threshold is met, I will revert to cooperating.

---

### 2. Handling Edge Cases

*   **Round 1 (Initial Round):**
    *   **Decision Rule:** **Cooperate (C)**.
    *   **Rationale:** In the absence of any history, the most cooperative and optimistic approach is to initiate cooperation. This strategy is designed to be cooperative, and the first move should reflect that. It also gives others an immediate opportunity to cooperate and potentially reach the threshold. This is a "trust-building" move.

*   **Last Round ($r$):**
    *   **Decision Rule:** **Defect (D)**.
    *   **Rationale:** In the final round, there is no future to incentivize. Any cooperation on my part would be purely altruistic, as there's no subsequent round for others to reciprocate. This is a pragmatic decision to maximize my individual payoff when the repeated game aspect no longer matters. The threat of future defection is no longer a credible deterrent or incentive.

---

### 3. Being Cooperative

My strategy is fundamentally cooperative in its design and intent.

*   **Default to Cooperation:** The strategy starts with cooperation and aims to sustain it whenever the collective effort is successful. This aligns with a cooperative mindset.
*   **Risk Aversion for the Collective:** While I'm protecting myself from exploitation (by defecting when the threshold is not met), my primary goal is to see the threshold met. My defection is a response to the *failure* of cooperation, not an arbitrary act.
*   **Forward-Looking (until the end):** As long as the game has future rounds, my decisions are made with the understanding that future payoffs matter. This encourages sustained cooperation.
*   **Learning from History:** The strategy "learns" from the collective's past actions and adjusts its own behavior accordingly, aiming to steer towards a state of mutual cooperation.

---

### Pseudocode for ATTPRD Strategy

```
// ATTPRD Strategy for Collective Risk Dilemma

// Parameters:
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

// State variables (per player):
// my_action_history: list of player's own actions in previous rounds
// other_actions_history: list of lists, where each inner list contains actions of ALL players in a round

// Function: DecideAction(current_round, my_action_history, other_actions_history, game_parameters)
//   n = game_parameters.n
//   r = game_parameters.r
//   m = game_parameters.m

//   // Handle First Round
//   if current_round == 1:
//     return 'C' // Cooperate on the first round to initiate cooperation

//   // Handle Last Round
//   if current_round == r:
//     return 'D' // Defect on the last round as there's no future incentive

//   // Get actions from the previous round (t-1)
//   previous_round_actions = other_actions_history[current_round - 2] // Indexing starts from 0 for history lists

//   // Count cooperators in the previous round
//   cooperators_in_previous_round = 0
//   for action in previous_round_actions:
//     if action == 'C':
//       cooperators_in_previous_round += 1

//   // Decision Logic: Adaptive Tit-for-Tat with Punishment/Reward
//   if cooperators_in_previous_round >= m:
//     // If the threshold was met or exceeded, reward this collective behavior
//     return 'C'
//   else:
//     // If the threshold was not met, punish collective failure by defecting
//     return 'D'

// Example Usage within a tournament loop:
// Initialize strategy state
// For round_num from 1 to r:
//   Get actions from previous round for all players
//   my_current_action = DecideAction(round_num, my_history, all_others_history, parameters)
//   Record my_current_action
//   (After all players have chosen actions for the round)
//   Update other_actions_history
```

This strategy aims to be a strong contender by being cooperative when it's beneficial for the group, punitive when necessary to avoid exploitation, and pragmatic in the face of the game's end.
'''

description_COOPERATIVE_97 = '''
This is a fascinating game that blends elements of the Prisoner's Dilemma with a public goods game, layered with the strategic challenge of repeated interaction. The core tension lies in the fact that if enough people cooperate, everyone benefits. However, the temptation to defect is always present because defectors still gain their private endowment (1) and the reward (k) if the threshold is met.

Here's a strategy designed to be cooperative, adaptive, and robust, focusing on rewarding cooperation and punishing defection while always aiming for the collective good.

## Cooperative, Adaptive, and Robust Strategy: "Threshold Tit-for-Tat"

This strategy is inspired by the success of Tit-for-Tat in repeated Prisoner's Dilemma, but adapted to the specific mechanics of the Collective Risk Dilemma. It prioritizes cooperation but is willing to defect to incentivize others to meet the cooperative threshold.

### 1. Decision Rules

The core of this strategy relies on **observing the collective behavior of the previous round** and reacting accordingly.

*   **If the previous round met or exceeded the cooperation threshold (m players cooperated):**
    *   **Cooperate (C).** This is the default and preferred action. It rewards the group for successful cooperation and signals a willingness to continue this beneficial behavior.

*   **If the previous round fell below the cooperation threshold (fewer than m players cooperated):**
    *   **Defect (D).** This is a punitive measure. By defecting, you reduce your own immediate payoff but signal to other players that their lack of cooperation has consequences. The goal is to "teach" the group that collective inaction leads to individual (and collective) suboptimal outcomes.

*   **If this is the first round (t = 1):**
    *   **Cooperate (C).** In the absence of any history, the most cooperative and optimistic starting point is to play C. This gives the group the best chance to meet the threshold from the outset and establishes a cooperative baseline.

### 2. Handling Edge Cases

*   **First Round (t = 1):** As stated above, always **Cooperate (C)**. This sets a positive and cooperative tone for the game.

*   **Last Round (t = r):** The decision rule for the last round is the same as any other round: base your decision on the outcome of the *previous* round. There is no need for a special "end game" strategy in this framework because the repeated nature and the payoff structure already incentivize players to maximize their cumulative payoff. If the group has been cooperating, you continue to cooperate. If they haven't, you defect to try and punish past failures. The key is that the incentive to *not* defect in the last round is still present if the history has been cooperative.

*   **Sudden Shifts in Opponent Behavior:** The strategy is inherently adaptive. If opponents suddenly start cooperating more, your strategy will naturally shift to more cooperation. If they start defecting more, your strategy will shift to more defection in response to failed rounds.

*   **Near-Misses (e.g., m-1 cooperators):** The strategy is binary (cooperate/defect) and relies on a clear threshold. If m-1 players cooperated, the threshold was *not* met. Therefore, according to the rule, you would **Defect (D)** in the next round. This might seem harsh, but it's a strong signal that even a single player's defection can prevent everyone from achieving the higher reward.

### 3. Being Cooperative

The underlying philosophy of this strategy is **cooperative reciprocity**.

*   **Initial Cooperativeness:** It starts with an act of trust and cooperation.
*   **Reward for Cooperation:** It directly rewards successful cooperative outcomes by continuing to cooperate.
*   **Punishment for Defection (Indirect):** When the threshold isn't met, defecting is not about pure spite. It's a strategic move to signal the unacceptability of the current outcome and to try and nudge the group towards better behavior in the future. By defecting, you are implicitly saying, "This outcome was bad, and I am not going to simply accept it and keep getting a suboptimal payoff (1) when a better outcome (2 or 3) was possible."
*   **Focus on Collective Payoff:** The ultimate goal is to reach states where the threshold is met, as this maximizes everyone's potential payoff. The strategy is designed to facilitate this by incentivizing the required number of cooperators.

### Pseudocode Representation:

```pseudocode
Strategy: ThresholdTitForTat

// Global variables to store history
// (For this strategy, only the outcome of the *previous* round is needed)
var last_round_cooperators: integer = 0
var total_players: integer = n
var cooperation_threshold: integer = m

function choose_action(round_number: integer, history: list of past_round_outcomes):
    // history is a list of lists, where each inner list contains actions of all players in a round.
    // e.g., history = [[C, D, C, ...], [D, D, C, ...], ...]

    if round_number == 1:
        // In the first round, always cooperate to set a positive tone.
        return C
    else:
        // Determine the number of cooperators in the *previous* round.
        // We need to access the actions of the previous round from the history.
        // Assuming history is indexed from the beginning of the game (round 0 for first round's outcomes)
        // So, for round_number 't', we look at history[t-2] (if history is 0-indexed for rounds)
        // or history[t-1] if history is 1-indexed for rounds.
        // Let's assume history is 0-indexed for rounds.
        // Therefore, for round_number `round_number`, we look at `history[round_number - 2]`
        // as this is the outcome of the round *before* the current one.

        // To be safe, let's explicitly count from the provided history.
        // The 'history' argument contains actions for rounds 1, 2, ..., round_number-1.
        // The most recent past round is history[round_number - 2].
        
        previous_round_actions = history[round_number - 2] // Access the actions of the round before the current one
        cooperators_in_previous_round = count_C_in(previous_round_actions)

        if cooperators_in_previous_round >= cooperation_threshold:
            // If the threshold was met or exceeded in the previous round, continue cooperating.
            return C
        else:
            // If the threshold was not met, defect to signal dissatisfaction and encourage future cooperation.
            return D

function count_C_in(actions: list of actions):
    count = 0
    for action in actions:
        if action == C:
            count = count + 1
    return count

// Example usage (conceptual, actual implementation will be in the tournament system):
// My_Action = choose_action(current_round_number, game_history)
```

This strategy aims to be a stable, cooperative force in the tournament, capable of adapting to different environmental pressures while consistently aiming for the mutually beneficial outcomes of the Collective Risk Dilemma.
'''

description_COOPERATIVE_98 = '''
This strategy, named "Adaptive Threshold Cooperation" (ATC), aims to foster cooperation by balancing the desire for the collective reward with the risk of individual defection. It's designed to be adaptive to the actions of other players, learns from history, and prioritizes collective benefit when conditions are favorable.

### Adaptive Threshold Cooperation (ATC) Strategy

The core idea of ATC is to start with a cautious approach and gradually increase cooperation as confidence in others' cooperation grows, while also having a mechanism to punish deviations from cooperative behavior.

#### 1. Decision Rules

ATC's decision to Cooperate (C) or Defect (D) in any given round depends on two primary factors:

*   **Observed Cooperation Level (OCL):** This is the proportion of players who cooperated in the *previous* round.
*   **Current Cooperation Threshold (CCT):** This is an internal, dynamically adjusted threshold for cooperation.

**Decision Rule:**

*   **If OCL ≥ CCT:** Player i plays **C** (Cooperate).
*   **If OCL < CCT:** Player i plays **D** (Defect).

**How CCT is Adapted:**

The Current Cooperation Threshold (CCT) starts at a higher value and gradually decreases as cooperation is observed, and increases when defection leads to missed rewards.

*   **Initial CCT:** A slightly conservative value, reflecting the initial uncertainty. A good starting point is $m/n + \epsilon$, where $\epsilon$ is a small positive value to encourage initial cooperation if the threshold is very close to being met. For example, if $m=3, n=6$, $m/n = 0.5$. Initial CCT could be $0.55$.
*   **CCT Adjustment:**
    *   **If the collective reward was achieved in the previous round (i.e., m or more players cooperated):** The CCT is slightly *decreased* by a small decrement $\delta$. This signifies that the observed cooperation was sufficient, and we can afford to be slightly more optimistic.
    *   **If the collective reward was NOT achieved in the previous round (i.e., fewer than m players cooperated):** The CCT is slightly *increased* by a small increment $\gamma$. This signifies that the observed cooperation was insufficient, and we need to be more cautious in the future to avoid being exploited.

**Key Parameters for Adaptation:**

*   **Initial CCT:** The starting point for our confidence in cooperation.
*   **Decrement ($\delta$):** How much we lower our threshold when cooperation succeeds.
*   **Increment ($\gamma$):** How much we raise our threshold when cooperation fails.

Choosing appropriate values for $\delta$ and $\gamma$ is crucial. $\delta$ should be small enough to avoid rapidly decreasing our threshold and becoming overly optimistic, while $\gamma$ should be large enough to deter defection but not so large as to lead to a permanent state of defection. These values should be tuned based on the tournament's performance, but a good starting point might be $\delta \approx 0.05$ and $\gamma \approx 0.1$.

#### 2. Handling Edge Cases

*   **Round 1 (Initialization):**
    *   **Decision:** In the very first round, since there is no history, ATC plays **C** (Cooperate). This is a default cooperative stance to initiate the game and test the waters for collective action.
    *   **Initial CCT:** Set to the predetermined initial value (e.g., $m/n + \epsilon$).

*   **Last Round (Round r):**
    *   **Decision:** In the final round, the incentive for future cooperation and retaliation disappears. The dominant strategy for an individual is to defect, as there are no future consequences. Therefore, ATC plays **D** (Defect) in the last round. This is a standard "unraveling" behavior in finitely repeated games.

*   **When m=2 (and n=3):** If $m$ is very small relative to $n$, the threshold to achieve the reward is low. ATC still follows the general rules, but the CCT might remain low for longer.

*   **When k is very high:** A high $k$ amplifies the benefit of cooperation. ATC will still adapt, but the success of even a few rounds of cooperation might lead to a sustained period of cooperation.

*   **When k is only slightly above 1:** The reward for cooperation is marginal. ATC's adaptation might be slower, and it might fall into a state of mutual defection more easily if there are persistent defectors.

#### 3. Being Cooperative

*   **Default Cooperate:** ATC begins by cooperating.
*   **Punishment and Reward:** It rewards observed cooperation by lowering its threshold for future cooperation and punishes lack of cooperation by raising its threshold, making it less likely to cooperate in the next round. This is a form of conditional cooperation.
*   **Focus on Collective Benefit:** The strategy's core logic is centered around achieving the collective reward (when $m$ players cooperate). The CCT is a mechanism to assess the *likelihood* of achieving this collective benefit.
*   **Forgiveness (Implicit):** While ATC punishes defection by increasing its own caution, it doesn't permanently blacklist players. If others start cooperating again, its threshold will eventually decrease, allowing for renewed cooperation.

#### Pseudocode

```
// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward if threshold met factor

// Strategy Variables
current_round = 1
cct = initial_cct // e.g., (m / n) + 0.05
delta = 0.05     // amount to decrease cct when cooperation succeeds
gamma = 0.1      // amount to increase cct when cooperation fails

// History (to be maintained by the game environment and observed by ATC)
// num_cooperators_last_round: count of players who played C in the previous round

function decide_action(current_round, num_cooperators_last_round_history):
    // Handle last round
    if current_round == r:
        return D

    // Handle first round
    if current_round == 1:
        // Initialize CCT and play C
        cct = (m / n) + 0.05
        return C

    // Calculate observed cooperation level (OCL) from history
    observed_cooperation_level = num_cooperators_last_round_history / n

    // Decision rule
    if observed_cooperation_level >= cct:
        action = C
        // Adapt CCT: If reward met last round, lower threshold
        if num_cooperators_last_round_history >= m:
            cct = max(0, cct - delta) // Ensure cct doesn't go below 0
        else: // Reward not met, need to be more cautious
            cct = min(1, cct + gamma) // Ensure cct doesn't go above 1
    else:
        action = D
        // Adapt CCT: If reward not met, increase threshold
        // This adjustment is applied regardless of whether we decided to D or C
        // based on the previous round's outcome. The 'if' above determines action,
        // the adjustment ensures future decisions are informed.
        if num_cooperators_last_round_history >= m:
            cct = max(0, cct - delta)
        else:
            cct = min(1, cct + gamma)

    return action

// Example usage within a tournament loop:
// For each round t from 1 to r:
//     my_action = decide_action(t, history_of_cooperators[t-1])
//     record my_action
//     ... (other players make decisions)
//     collect outcomes and update history for next round
```

**Rationale for Adaptability and Robustness:**

*   **Adaptive:** The CCT dynamically adjusts based on observed outcomes, allowing the strategy to respond to changes in other players' behavior.
*   **Robust:**
    *   It doesn't rely on specific patterns or schedules, making it less predictable and exploitable by strategies that try to enforce patterns.
    *   It has a built-in mechanism to punish defection by becoming more cautious, which can deter some forms of exploitation.
    *   Its initial cooperative move and gradual adaptation make it resilient to a mix of cooperative and defecting players. It can potentially steer a group towards cooperation or retreat to defection if cooperation consistently fails.

This strategy aims to be a "good citizen" in the tournament, promoting cooperation when feasible but not at the cost of being a consistent loser.
'''

description_COOPERATIVE_99 = '''
Here's a cooperative strategy for the Collective Risk Dilemma game, designed to be adaptive and robust:

## Strategy: The "Cautious Rewarder"

This strategy aims to be cooperative by signaling a willingness to cooperate and rewarding others who do the same, while also being cautious to avoid exploitation. It's designed to be robust against various player behaviors by dynamically adjusting its cooperation threshold based on past outcomes.

### Core Principles:

1.  **Initial Trust with Observation:** Start with a willingness to cooperate, but closely monitor the group's behavior.
2.  **Reward Cooperation:** If the group successfully reaches the cooperation threshold, strongly reinforce that behavior by continuing to cooperate.
3.  **Punish Defection (Indirectly):** If the group fails to reach the threshold due to insufficient cooperation, temporarily increase the demand for cooperation from others before returning to the cooperative norm.
4.  **End-Round Resilience:** Maintain a consistent approach in the final round to maximize personal payoff, avoiding last-minute altruism or punishment that would be wasted.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on the player's observation of the group's behavior in previous rounds. Let `cooperators_t-1` be the number of players who cooperated in round `t-1`.

*   **Round 1:** Cooperate (C). The initial assumption is that others might also be willing to cooperate. This signals a cooperative intent.

*   **Round `t` (where `t > 1`):**
    *   **If `cooperators_t-1` ≥ `m` (Threshold met in the previous round):**
        *   **Play C (Cooperate).** This reinforces successful cooperation and signals that the reward `k` is valuable and achievable through collective effort.
    *   **If `cooperators_t-1` < `m` (Threshold not met in the previous round):**
        *   **Introduce a temporary "cooperation demand" element.** To decide whether to C or D, we need to assess the *overall* cooperation level in the previous round.
            *   **Calculate `defecting_cooperators_t-1`:** This is the number of players who *defect* when the threshold was *not* met. In this scenario, those who defect are the ones preventing the group from reaching the threshold and securing the reward.
            *   **Decision Logic:**
                *   **If `defecting_cooperators_t-1` ≤ `n - m` (The number of defectors is "manageable" or "explaining" the failure):** Play **C (Cooperate).** This means that even though the threshold wasn't met, the failure wasn't due to a large majority of players defecting. We continue to lean towards cooperation, assuming others might follow, and the risk of punishment for cooperating alone is mitigated.
                *   **If `defecting_cooperators_t-1` > `n - m` (A significant portion of the group defected):** Play **D (Defect).** This is a temporary "punishment" for the group's failure to cooperate. By defecting, we are signaling that we will not bear the cost of cooperation if others refuse to contribute. This is a strategic move to discourage widespread defection in subsequent rounds.

*   **Last Round (`t = r`):**
    *   **Always Play D (Defect).** In the final round, there's no future incentive to cooperate for a higher reward. The best strategy is to maximize immediate personal payoff by keeping the endowment. This ensures no payoff is "wasted" on a cooperative effort that won't be reciprocated in the future.

### 2. Handling Edge Cases:

*   **First Round (t=1):** As stated above, the strategy begins by **Cooperating (C)**. This establishes a baseline cooperative behavior and tests the waters for other players' intentions.
*   **Last Round (t=r):** As stated above, the strategy always **Defects (D)**. This is a rational response to a finite game where future cooperation has no bearing on payoffs.
*   **Initial State (Before Round 1):** All counters for previous round cooperation are effectively zero.
*   **Insufficient Data for `cooperators_t-1`:** The strategy relies on `cooperators_t-1`. If for some reason this is unavailable (e.g., in a simulated first round), the logic defaults to the "first round" behavior (Cooperate).

### 3. Being Cooperative:

The "Cautious Rewarder" strategy is cooperative in its fundamental intent:

*   **Initiates Cooperation:** It starts by cooperating, offering a chance for collective success.
*   **Reinforces Success:** When cooperation works (threshold met), it continues to cooperate, actively participating in securing the reward for everyone. This creates a positive feedback loop for collective action.
*   **Adaptive Punishment:** The "punishment" phase (defecting when `defecting_cooperators_t-1` > `n - m`) is not an arbitrary punishment. It's a calculated response to group behavior that actively hinders cooperation. By defecting in such scenarios, the strategy signals that cooperation is conditional on a reasonable level of commitment from others, thus preserving its own resources and encouraging future cooperation rather than outrightly seeking to punish for past failures. It's a form of "tough love" for the group.

### Pseudocode:

```python
def CautiousRewarder(n, r, m, k, history):
    # history is a list of lists, where history[t-1] is the list of actions (C/D) of all players in round t-1

    current_round = len(history) + 1

    # Handle the last round
    if current_round == r:
        return "D" # Always defect in the last round

    # Handle the first round
    if current_round == 1:
        return "C" # Start by cooperating

    # For rounds t > 1 and t < r
    previous_round_actions = history[-1] # Actions from round t-1
    cooperators_t_minus_1 = previous_round_actions.count("C")

    if cooperators_t_minus_1 >= m:
        # Threshold met in the previous round, reinforce cooperation
        return "C"
    else:
        # Threshold not met in the previous round
        # Count how many players defected in the previous round
        defectors_t_minus_1 = previous_round_actions.count("D")

        # Calculate how many players *should have* cooperated to meet the threshold
        # If m players cooperated, then n-m players defected.
        # If the number of defectors is more than n-m, it means more than
        # necessary defected, causing the failure.
        # Or, more directly: how many defectors were *beyond* what's acceptable
        # for threshold achievement.
        # If m players *must* cooperate, then at most n-m players can defect.
        # If defected_players > n-m, the system is "broken" by defectors.

        # Alternative framing:
        # If cooperators_t_minus_1 < m, then the number of defectors is n - cooperators_t_minus_1.
        # We need m cooperators. This means we can tolerate at most n-m defectors.
        # If the actual number of defectors (n - cooperators_t_minus_1) is > n-m,
        # it means the number of cooperators was < m.

        # Let's use the number of players who *did not* defect among those who *could* have
        # caused the failure.
        # If cooperators_t_minus_1 < m, then the number of players who *did not cooperate*
        # is n - cooperators_t_minus_1.
        # We need m cooperators, so we can have at most n-m defectors.
        # If the number of actual defectors is > n-m, the group failed because of too many defectors.

        # Simplified logic:
        # If threshold wasn't met, and the number of defectors was "excessive"
        # Excessive defectors means that even if those who defected had cooperated,
        # the threshold might not have been met, or the failure was largely due to them.
        # If cooperators_t_minus_1 < m, then the number of defectors is n - cooperators_t_minus_1.
        # We want to check if this number of defectors is "too high".
        # A "too high" number of defectors is when the failure is clearly due to them.
        # The critical number of defectors for failure is when cooperators = m-1,
        # meaning defectors = n - (m-1) = n - m + 1.
        # If actual defectors > n - m + 1, it means we are in a state of significant collective failure.

        # Re-evaluating the logic from prompt example:
        # Example 3: n=6, m=3, k=2. 3 players C, 3 players D.
        # If player i played C: pi = 0 + 2 = 2
        # If player i played D: pi = 1 + 2 = 3
        # If the threshold was *not* met, what would happen?
        # Let's say 2 players C, 4 players D.
        # - If player i played C: pi = 1 (0+0 from endowment, k is not applied)
        # - If player i played D: pi = 1 (1+0 from endowment, k is not applied)
        # In this case, cooperators = 2 < m (3).
        # Defectors = 4. n-m = 6-3 = 3.
        # Here, defecting_cooperators_t-1 (4) > n-m (3). This is the condition for "punishment".

        if defectors_t_minus_1 > (n - m): # If the number of defectors exceeds the maximum allowed for success
            # Significant collective failure due to defection, temporarily defect
            return "D"
        else:
            # Failure was not solely due to a large number of defectors,
            # so we continue to lean towards cooperation.
            return "C"

```
'''

description_COOPERATIVE_100 = '''
Okay, here is a design for a cooperative strategy for the Collective Risk Dilemma, focusing on adaptability and robustness in a tournament setting.

## Strategy Name: Adaptive Reciprocity with Risk Aversion (ARRA)

This strategy aims to maximize its own long-term payoff by fostering cooperation while safeguarding against exploitation. It is guided by the principle of reciprocity, but with a pragmatic acknowledgment of the risk involved in trusting others.

### 1. Decision Rules: When to Cooperate vs. Defect

ARRA's core decision logic is based on observing the collective behavior of other players in previous rounds and assessing the likelihood of meeting the cooperation threshold ($m$).

**General Principle:** Cooperate if the probability of meeting the threshold *with your cooperation* is sufficiently high. Defect if there is a significant risk that your cooperation will be wasted, or if others are consistently defecting.

**Specific Decision Rules:**

*   **Initial Cooperation Threshold (Pre-Round Calculation):**
    Before deciding on an action for the current round, ARRA will estimate the *cooperation rate* of the other $n-1$ players based on their actions in the *previous round*. Let $O_{cooperators}$ be the number of players (excluding ARRA) who cooperated in the previous round.

    *   **If $O_{cooperators} \ge m-1$:** This means that if ARRA cooperates, the threshold of $m$ cooperators is *guaranteed* to be met. In this scenario, ARRA *will cooperate (C)*. This is the strongest signal of collective willingness to cooperate.

    *   **If $m-1 > O_{cooperators} \ge m-2$ (and $m > 2$):** This is a borderline case. If ARRA cooperates, the threshold *might* be met. The probability depends on whether $m-1$ or $m-2$ players (among the others) cooperated. ARRA will *cooperate (C)* in this scenario, but with a slight reservation, as it's a tighter margin.

    *   **If $O_{cooperators} < m-2$:** This indicates that even if ARRA cooperates, the threshold is unlikely to be met based on past behavior of others. The risk of wasting ARRA's endowment is high. In this scenario, ARRA *will defect (D)*.

*   **Defection Trigger (Reciprocity):**
    ARRA is designed to be reciprocal. If a significant number of players (excluding ARRA) defected in the previous round, ARRA will be more inclined to defect in the current round.

    *   **If the number of defectors among the other $n-1$ players in the previous round is greater than or equal to $n-m$:** This implies that more than half of the players (or at least a substantial portion that makes cooperation difficult) are choosing to defect. ARRA will *defect (D)* to avoid being the sole cooperator in a group of defectors.

*   **Aggressive Cooperation (Once Trust is Established):**
    If the game has progressed for a significant number of rounds and ARRA has consistently observed the threshold being met with its cooperation, it may become slightly more optimistic. However, this strategy prioritizes robustness and risk aversion, so it won't aggressively *lower* its cooperation threshold without strong evidence. Instead, it will stick to the calculated thresholds above.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    Since there is no history, ARRA needs a default strategy. To balance the desire to initiate cooperation with risk aversion, ARRA will adopt a **"Cautious Cooperation"** approach.
    *   **Decision:** ARRA will *cooperate (C)* if $n-1 \ge m-1$. This means if the total number of players (excluding ARRA) is sufficient to meet the threshold if they *all* cooperate, ARRA will initiate. Otherwise (if $n-1 < m-1$), ARRA will *defect (D)* to avoid being isolated in a situation where cooperation is impossible from the start.
    *   **Rationale:** This encourages cooperation but avoids immediate exploitation in scenarios where cooperation is impossible without ARRA's contribution.

*   **Last Round (t=r):**
    In the final round, there are no future rounds to influence or retaliate. The incentive to cooperate for future gains disappears. This is a classic "end-game" problem.
    *   **Decision:** ARRA will *defect (D)* in the last round, regardless of past history.
    *   **Rationale:** This maximizes ARRA's immediate payoff in the final round. It's a purely self-interested decision, as there are no future consequences to consider.

*   **Rounds with Insufficient Players to Meet Threshold (even with cooperation):**
    If $n < m$, it's impossible to meet the threshold. ARRA will always defect. (This is implicitly handled by the initial cooperation threshold logic: if $n-1 < m-1$, ARRA defects in round 1. In subsequent rounds, if $O_{cooperators}$ is consistently low, ARRA will defect.)

### 3. Being Cooperative

ARRA is fundamentally designed to promote cooperation. Its primary objective is to achieve the higher payoffs associated with meeting the cooperation threshold.

*   **Initiating Cooperation:** In the first round, ARRA attempts to start the cooperative cycle if there's a reasonable chance of success.
*   **Sustaining Cooperation:** ARRA rewards cooperation from others by cooperating itself when the signs are positive. It aims to create a stable environment where collective benefit is achievable.
*   **Minimizing Waste:** ARRA defects when it perceives a high risk of its cooperation being "wasted" – i.e., not contributing to meeting the threshold, leading to a lower individual payoff compared to defecting. This is a pragmatic form of cooperation, where self-preservation is necessary to continue participating and potentially cooperating in the future.
*   **Robustness:** By being adaptive, ARRA doesn't rigidly stick to a single strategy. It learns from the environment (opponent behavior) and adjusts, which makes it more resilient to a variety of opponent strategies, including aggressive defectors and inconsistent cooperators.

### Pseudocode Representation:

```pseudocode
Strategy ARRA(player_id, n, r, m, k, history):
  # history is a list of past rounds, each round contains a list of actions [action_1, action_2, ..., action_n]

  current_round = len(history) + 1

  # --- Edge Case: Last Round ---
  if current_round == r:
    return D # Defect in the last round

  # --- Edge Case: First Round ---
  if current_round == 1:
    # Cooperate if the remaining players are enough to potentially meet the threshold
    if n - 1 >= m - 1:
      return C
    else:
      return D # Impossible to cooperate, defect

  # --- General Case: Subsequent Rounds ---
  # Analyze previous round's actions for other players
  previous_round_actions = history[-1]
  other_players_actions = previous_round_actions[:player_id-1] + previous_round_actions[player_id:] # Exclude self

  O_cooperators = count_C(other_players_actions)
  O_defectors = count_D(other_players_actions)

  # Rule 1: Guaranteed Threshold Met by Cooperating
  if O_cooperators >= m - 1:
    return C

  # Rule 2: Borderline Case - Still worth a try
  if m - 1 > O_cooperators and O_cooperators >= m - 2 and m > 2:
    return C

  # Rule 3: Defection Trigger - Too many defectors among others
  if O_defectors >= n - m:
    return D

  # Default to Defect if none of the above cooperative signals are strong enough
  return D

# Helper functions (assumed to be available)
function count_C(actions_list):
  count = 0
  for action in actions_list:
    if action == C:
      count += 1
  return count

function count_D(actions_list):
  count = 0
  for action in actions_list:
    if action == D:
      count += 1
  return count
```

This ARRA strategy is designed to be a robust and adaptive player that prioritizes long-term collective benefit while prudently managing its own risk and incentivizing reciprocal cooperation.
'''

description_COOPERATIVE_101 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust:

## Cooperative Risk Aversion Strategy (CRAS)

The core idea behind CRAS is to **initially cooperate and try to establish a cooperative equilibrium**, while being prepared to **defect strategically to avoid being exploited** if cooperation fails or is undermined. The strategy prioritizes achieving the higher payoff of mutual cooperation when possible, but not at the cost of consistently receiving the lowest possible payoff.

### 1. Decision Rules

CRAS operates on a tiered system of beliefs about the environment and player behavior.

**Tier 1: Proactive Cooperation (Initial Phase & Re-establishment)**

*   **Decision:** **Cooperate (C)**.
*   **When:**
    *   In the **first round** of the game.
    *   If, in the **previous round**, the cooperation threshold (`m`) was met or exceeded. This signifies that a sufficient number of players are cooperating, making individual cooperation a good bet for a positive outcome.
    *   If, in the **previous round**, the cooperation threshold was *just missed* (i.e., `m-1` players cooperated) and the number of defectors was "small" relative to the total number of players. This is a signal that cooperation is close to succeeding and another attempt might tip the balance.

**Tier 2: Cautious Observation (Transitional Phase)**

*   **Decision:** **Defect (D)**.
*   **When:**
    *   If, in the **previous round**, the cooperation threshold was *not* met, and the number of defectors was "large" relative to the total number of players. This indicates a significant level of defection that makes individual cooperation risky and likely to result in a low payoff (only 1).

**Tier 3: Exploitation Prevention (Punishment Phase)**

*   **Decision:** **Defect (D)**.
*   **When:**
    *   If, in the **previous round**, the cooperation threshold was *not* met, *and* you played Cooperate (C) in that round. This is a clear signal that your cooperation was exploited. You will continue to defect until the cooperation threshold is met in a subsequent round. This is a strong deterrent against exploitation.

**Defining "Small" and "Large" Number of Defectors (for Tier 1):**

To add robustness, we can introduce adaptive thresholds for when cooperation is "close." Let `d_t-1` be the number of defectors in the previous round.

*   **"Small" number of defectors:** `d_t-1 < n / 2` (or a similar fraction, adjustable for sensitivity). This means less than half the players defected, suggesting the overall inclination is towards cooperation or at least not overwhelmingly against it.
*   **"Large" number of defectors:** `d_t-1 >= n / 2`. This indicates a significant portion, or a majority, of players are defecting, making cooperation a risky endeavor.

**Refined Decision Logic:**

Let `cooperators_t-1` be the number of cooperators in the previous round and `defectors_t-1` be the number of defectors in the previous round.

1.  **First Round (t=1):** Cooperate (C).
2.  **Subsequent Rounds (t > 1):**
    *   **If `cooperators_t-1 >= m`:** Cooperate (C). (Cooperation succeeded, maintain it).
    *   **Else if `cooperators_t-1 == m - 1` AND `defectors_t-1 < n / 2`:** Cooperate (C). (Close to success, worth another try).
    *   **Else if (you played C in round t-1) AND `cooperators_t-1 < m`:** Defect (D). (Exploited, punish).
    *   **Else if `defectors_t-1 >= n / 2`:** Defect (D). (Widespread defection, avoid being the sole cooperator).
    *   **Else (default to cautious observation/hesitation to re-engage cooperation):** Defect (D).

**Simplified Pseudocode for Decision Making in Round `t`:**

```
function DecideAction(player_id, history, parameters):
  n = parameters.n
  m = parameters.m
  r = parameters.r
  k = parameters.k

  if t == 1:
    return C

  // Get info from previous round (t-1)
  cooperators_prev = count_cooperators(history, t-1)
  defectors_prev = n - cooperators_prev
  my_action_prev = get_my_action(history, player_id, t-1)

  if cooperators_prev >= m:
    // Threshold met or exceeded, continue cooperation
    return C
  else if cooperators_prev == m - 1 and defectors_prev < n / 2:
    // Close to threshold and not overwhelmingly defecting, try again
    return C
  else if my_action_prev == C and cooperators_prev < m:
    // Was exploited, defect to punish
    return D
  else if defectors_prev >= n / 2:
    // Significant defection, avoid being exploited
    return D
  else:
    // Default to defection when cooperation isn't clearly viable or was punished
    return D
```

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated above, the strategy begins with **Cooperate (C)**. This is crucial for attempting to establish cooperation from the outset.
*   **Last Round (t=r):** The strategy remains the same. In a repeated game, even the last round can have implications if there's any possibility of future interactions or if players are trying to establish a reputation. CRAS will continue its adaptive logic. If cooperation was achieved and maintained throughout, it will cooperate. If it was exploited, it will defect. The logic doesn't change simply because it's the last round.
*   **Near the end of the game (e.g., t=r-1):** The strategy does not change its behavior. The "punishment" phase (defecting after being exploited) is designed to be a deterrent. Even in later rounds, the threat of defection remains. There is no specific "end-game" logic to switch to pure self-interest because the history of play will dictate the action. If cooperation has been successful, it will continue. If it has been punished, it will continue to punish.

### 3. Being Cooperative

CRAS embodies a cooperative mindset through the following principles:

*   **Proactive Initiation:** It actively seeks to establish a cooperative environment by cooperating in the first round and giving cooperation a chance to succeed even when slightly below the threshold.
*   **Reciprocity:** It rewards cooperation by continuing to cooperate when the threshold is met.
*   **Conditional Cooperation:** It is willing to forgive and try again if cooperation *almost* succeeded (m-1 cooperators and not too many defectors).
*   **Fair Punishment:** It only defects if it has been personally exploited (played C and the threshold wasn't met), or if the collective behavior strongly suggests that cooperation is unlikely to succeed. This is not vindictive, but a rational response to protect its own payoff from consistently low levels.
*   **Adaptability:** It learns from the history of play. If the environment shifts towards more cooperation, CRAS adapts. If it shifts towards defection, CRAS also adapts to protect itself.

**Robustness to Opponent Behavior:**

*   **Against consistent cooperators:** CRAS will likely lead to sustained cooperation and high payoffs for all.
*   **Against consistent defectors:** CRAS will cooperate in the first round, be exploited, and then defect for the remainder of the game, securing a payoff of 1 per round. This is the best possible outcome against a purely selfish opponent.
*   **Against mixed strategies:** CRAS will adapt. If enough players cooperate to meet the threshold, CRAS benefits and continues cooperating. If defection is rampant, CRAS will opt out of cooperation to avoid being the sole loser.
*   **Against opportunistic players:** The "punishment" phase is designed to deter players who might try to "test the waters" with a defection. If CRAS is exploited, it will retaliate by defecting, making such opportunistic behavior less profitable in the long run.

This strategy aims to strike a balance between the desire for high collective payoffs through cooperation and the need for self-preservation against potentially exploitative behavior.
'''

description_COOPERATIVE_102 = '''
Okay, this is an interesting variant of a public goods game with a collective risk component. The core challenge is balancing individual incentives to defect with the collective benefit of cooperation, especially when the number of cooperators is crucial.

Here's a cooperative strategy designed to be adaptive and robust, focusing on achieving the cooperative outcome without explicit communication:

## Cooperative Strategy: "Threshold Weaver"

This strategy aims to weave a pattern of cooperation that reliably hits the `m` threshold, adapting to past outcomes. It prioritizes collective success while minimizing individual risk.

### Core Principles:

1.  **Maximize Collective Reward:** The primary goal is to ensure the `m` cooperation threshold is met as often as possible to secure the `k` bonus for everyone.
2.  **Minimize Personal Loss:** Avoid being the sole cooperator when the threshold isn't met, as this leads to a payoff of 0.
3.  **Adaptability:** Adjust cooperation levels based on the observed success of past rounds.
4.  **Robustness:** Function effectively even when other players are unpredictable or purely self-interested.

### Strategy Breakdown:

The strategy will maintain a **"Target Cooperation Level"** for the current round. This target is dynamic and is influenced by the success of the previous round.

#### 1. Decision Rules:

The core decision logic revolves around the player's assessment of the likelihood of reaching the cooperation threshold in the current round, based on history.

*   **Decision Threshold:** In any given round `t`, player `i` will cooperate (play `C`) if and only if they believe that at least `m` players (including themselves) will cooperate. If they believe fewer than `m` players will cooperate, they will defect (play `D`).

*   **Estimating Opponent Behavior:** The strategy estimates the probability of other players cooperating based on their past actions. A simple, yet effective, approach is to track the proportion of past rounds where cooperation was successful (i.e., `m` or more players cooperated).

    *   Let `S_t` be the total number of rounds up to `t-1` where the cooperation threshold was met.
    *   Let `N_t` be the total number of rounds played up to `t-1` (i.e., `t-1`).
    *   **Observed Success Rate (OSR):** `OSR_t = S_t / N_t` (if `N_t > 0`, otherwise 0).

*   **Determining Own Action (for round `t`):**

    *   **If `OSR_t` is high (e.g., `> 0.75`):** The strategy assumes a generally cooperative environment. It will *tend* to cooperate, even if it's unsure about others.
    *   **If `OSR_t` is medium (e.g., `0.3 <= OSR_t <= 0.75`):** The strategy becomes more cautious. It will cooperate if it believes there's a good chance of reaching the threshold, perhaps by slightly overshooting its estimate of others' cooperation.
    *   **If `OSR_t` is low (e.g., `< 0.3`):** The strategy becomes defensive. It will only cooperate if it has strong evidence that `m` cooperators will be reached.

**Refined Decision Logic with "Confidence" Measure:**

Instead of a simple OSR, let's build a "confidence score" for cooperation.

*   **Past Cooperation Success Count (`C_success`):** Number of rounds where `m` or more players cooperated.
*   **Past Rounds Played (`R_played`):** Total number of rounds played so far.
*   **Estimated Future Cooperation (EFC):** This is the crucial part. How do we estimate the number of *other* players who will cooperate?
    *   We can maintain an average cooperation rate for *other* players.
    *   Let `Avg_Other_Coop_Rate_t` be the average fraction of *other* players who cooperated in rounds `1` to `t-1`.
    *   `Avg_Other_Coop_Rate_t = (Total_Cooperators_in_past_rounds - Total_Self_Cooperations_in_past_rounds) / (N_t * (n-1))` (Handle division by zero).

*   **Decision Rule for Player `i` in Round `t`:**
    *   Calculate `Estimated_Total_Cooperators = 1 + floor(Avg_Other_Coop_Rate_t * (n-1))`. (Add 1 for self, floor to get integer count).
    *   **If `Estimated_Total_Cooperators >= m`:** Player `i` plays **C**. This is the ideal cooperative outcome.
    *   **If `Estimated_Total_Cooperators < m`:** This is where adaptation and robustness come in.
        *   **If `C_success` is high (e.g., > 0.6 * `R_played`):** The strategy still attempts to cooperate if `Estimated_Total_Cooperators` is *close* to `m` (e.g., `m-1` or `m-2`). The idea is to keep the cooperative momentum going, hoping others will also step up. This introduces a willingness to take a small risk for collective gain.
        *   **If `C_success` is low or medium:** Player `i` defects (plays **D**) to protect their individual payoff, as the probability of reaching the threshold is too low.

**Further Refinement for Robustness (The "Buffer"):**

To be more robust, especially against defection-heavy opponents, we can introduce a "buffer" or "tolerance" for *not* cooperating.

*   Let `Current_Cooperation_Intentions` be an estimate of how many players (excluding self) are likely to cooperate in the current round. This can be based on `Avg_Other_Coop_Rate_t`.
*   **Decision Rule:**
    *   If `1 + floor(Current_Cooperation_Intentions * (n-1)) >= m`: Play **C**.
    *   Else if `1 + floor(Current_Cooperation_Intentions * (n-1)) >= m - buffer`: Play **C**. (Where `buffer` is a small integer, e.g., 1 or 2, that decreases as `C_success` rate increases).
    *   Else: Play **D**.

The `buffer` acts as a safety net. If cooperation is generally successful, the strategy is willing to risk cooperating even if the estimated number of other cooperators is slightly below `m`. If cooperation has been historically rare, this buffer is removed.

#### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   There is no history. To initiate cooperation and signal intent, the strategy will err on the side of cooperation *if* `n/2` (or a similar conservative estimate) players cooperating seems plausible.
    *   **Decision:** Play **C** if `n >= m`. This is a strong signal to try and meet the threshold. If `n < m` (which is impossible by definition of `m < n`), this would be a safe D. The general principle for round 1 is to contribute to the *possibility* of cooperation. A more conservative approach could be to play `C` if `m <= n/2 + 1`, a slightly optimistic but still reasonable estimate.
    *   **Alternative (More Conservative Round 1):** Play **C** if `m <= floor(n/2) + 1`. This is a slightly more cautious initial move.

*   **Last Round (t=r):**
    *   In a repeated game, the last round often collapses to a one-shot game, and players might revert to defection. However, this strategy is designed for cumulative payoffs.
    *   **Decision:** The strategy remains consistent. It will apply the same decision rules as any other round, based on the accumulated history. The payoff in the last round still matters. There's no explicit "end-game" defection pivot unless history strongly dictates it. This maintains the cooperative signal throughout.

*   **`m = n` (Not allowed by rules, but as a thought experiment):** If `m=n`, everyone *must* cooperate. The strategy would always play `C`.
*   **`m = 2`:** The lowest possible threshold. Cooperation is easier to achieve. The strategy will be more inclined to cooperate.

#### 3. Being Cooperative:

*   **Intentionality:** The strategy is built around *achieving* the cooperative outcome (`m` cooperators) and maximizing the shared reward (`k`). It actively tries to overcome the individual incentive to defect by relying on group success.
*   **Risk Sharing:** By cooperating when the estimated number of other cooperators is close to `m`, the strategy is willing to take a small personal risk (potentially getting 0 if no one else cooperates) to enable the collective gain for everyone. This is the essence of being cooperative in this dilemma.
*   **Long-Term Perspective:** The repeated nature of the game allows this strategy to build trust (or at least predictability) over time. By consistently trying to meet the threshold, it signals a preference for the cooperative equilibrium.

### Pseudocode Illustration:

```python
class ThresholdWeaver:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k

        self.rounds_played = 0
        self.cooperation_successes = 0
        self.my_cooperation_history = [] # 1 if C, 0 if D
        self.other_players_cooperation_counts = [0] * n # Track how many times each OTHER player cooperated

    def decide_action(self, history_round_t_minus_1):
        # history_round_t_minus_1: a list of tuples (player_id, action) for the previous round
        # (Or this might be passed in a different format, e.g., count of cooperators)

        self.rounds_played += 1

        # Update state based on previous round's outcome (if not the first round)
        if self.rounds_played > 1:
            num_cooperators_prev_round = sum(1 for _, action in history_round_t_minus_1 if action == 'C')
            if num_cooperators_prev_round >= self.m:
                self.cooperation_successes += 1

            # Update individual player cooperation counts (need player_id mapping)
            # This part requires more info on how history is passed.
            # For simplicity in pseudocode, let's assume we can track avg other coop rate

        # --- Decision Logic ---

        # Estimate average cooperation rate of other players
        avg_other_coop_rate = 0
        if self.rounds_played > 1:
            total_other_coop = 0
            # This aggregation needs to be done carefully based on the actual history format
            # Let's assume for now we can get 'total_cooperators_excluding_self_in_past'
            total_cooperators_excluding_self_in_past = ... # Calculated from history
            total_other_players_who_played = (self.rounds_played - 1) * (self.n - 1)
            if total_other_players_who_played > 0:
                avg_other_coop_rate = total_cooperators_excluding_self_in_past / total_other_players_who_played

        estimated_other_cooperators = max(0, int(avg_other_coop_rate * (self.n - 1)))

        # Determine cooperation buffer based on past success rate
        success_rate = self.cooperation_successes / (self.rounds_played - 1) if self.rounds_played > 1 else 0
        cooperation_buffer = 0
        if self.rounds_played == 1:
            # Initial aggressive cooperation attempt
            pass # Buffer is handled by the initial decision logic

        elif success_rate > 0.75: # High success, low buffer needed
            cooperation_buffer = 1
        elif success_rate > 0.4: # Medium success, moderate buffer
            cooperation_buffer = 1
        else: # Low success, no buffer (play safe)
            cooperation_buffer = 0

        # Make the decision
        if self.rounds_played == 1:
            # First round: cooperate if m <= n/2 + 1 (a slightly optimistic estimate)
            if self.m <= int(self.n / 2) + 1:
                self.my_cooperation_history.append(1)
                return 'C'
            else:
                # If m is very high relative to n, maybe defect if unlikely
                # This case is less likely given m < n.
                self.my_cooperation_history.append(0)
                return 'D'

        else: # Subsequent rounds
            if estimated_other_cooperators + cooperation_buffer >= self.m - 1: # +1 for self, so check if estimated others + buffer meets m-1 threshold
                self.my_cooperation_history.append(1)
                return 'C'
            else:
                self.my_cooperation_history.append(0)
                return 'D'

```

This strategy, "Threshold Weaver," aims to be a robust and cooperative participant by dynamically adjusting its cooperation level based on historical success and a cautious estimation of others' intentions. It prioritizes the collective good while maintaining a degree of self-preservation when cooperation seems unlikely.
'''

description_COOPERATIVE_103 = '''
This is an intriguing collective risk dilemma! The core tension lies between the individual incentive to defect (to secure a payoff of 1 regardless of others' actions) and the collective benefit of cooperation (achieving a payoff of 2 for cooperators if the threshold is met, or 3 for defectors if the threshold is met).

My strategy, which I'll call **"Adaptive Threshold Cooperation" (ATC)**, aims to maximize my long-term cumulative payoff by fostering cooperation when it's most beneficial and defecting when the risk of insufficient cooperation is too high. It's designed to be cooperative in spirit, pushing for the collective good when feasible, but also pragmatic to avoid being exploited.

### Strategy: Adaptive Threshold Cooperation (ATC)

The fundamental idea behind ATC is to maintain a degree of cooperation as long as there's a reasonable expectation that enough other players will also cooperate to meet the threshold ($m$). If cooperation falters, ATC will adapt by defecting to protect its own endowment. The strategy is memory-based, learning from past rounds to inform future decisions.

#### 1. Decision Rules

ATC's decision for the current round ($t$) is based on an assessment of the *likelihood of success* in achieving the cooperative outcome. This likelihood is a function of past cooperation levels and the current state of the game.

**Key Metrics:**

*   **`cooperation_count_prev_round`**: The number of players who cooperated in the previous round ($t-1$).
*   **`my_cooperation_history`**: A list of player i's past actions (C or D).
*   **`threshold_met_prev_round`**: A boolean indicating whether the cooperation threshold ($m$) was met in round $t-1$.

**Decision Logic:**

The core decision is a probabilistic one, but for simplicity and robustness in a simultaneous-move game, we'll frame it as a threshold-based decision.

**Decision Rule:**

*   **If `t == 1` (First Round):**
    *   **Cooperate (C)**. In the absence of any information, the most cooperative action is to initiate cooperation. This sets a positive example and allows us to gather initial data about other players' behavior.

*   **If `t > 1`:**
    *   **Evaluate the "Cooperation Momentum"**: This is a measure of how likely cooperation is to succeed in the current round.
        *   **If `threshold_met_prev_round` is TRUE:**
            *   The previous round was successful. We want to encourage continued success.
            *   **Cooperate (C)**. The strong signal from the previous round suggests that cooperation is working and likely to continue.
        *   **If `threshold_met_prev_round` is FALSE:**
            *   The previous round failed to meet the threshold. This indicates a risk of insufficient cooperation.
            *   **Assess `cooperation_count_prev_round` relative to `m`:**
                *   **If `cooperation_count_prev_round >= m - 1`**:
                    *   This means that even though the threshold wasn't met, we were very close (only one player short, or we were the only one who *could* have tipped it).
                    *   **Cooperate (C)**. The proximity to success suggests that a single additional cooperator (or even just continuing our own cooperation) might be enough to tip the balance. This is a "last push" for cooperation.
                *   **If `cooperation_count_prev_round < m - 1`**:
                    *   The previous round fell significantly short of the threshold. The likelihood of reaching the threshold with our continued cooperation alone is low.
                    *   **Defect (D)**. The risk of not meeting the threshold and receiving only 1 is now higher than the potential gain from cooperating and risking a payoff of 0 (if no one else cooperates) or 2 (if others cooperate but not enough). It's safer to secure a payoff of 1.

**Summary of Decision Rules:**

| Round (`t`) | `threshold_met_prev_round` | `cooperation_count_prev_round` | Decision | Rationale                                                                                                                                   |
| :---------- | :------------------------- | :----------------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------ |
| 1           | N/A                        | N/A                            | **C**    | Initiate cooperation, gather data.                                                                                                          |
| `t > 1`     | True                       | N/A                            | **C**    | Previous success is a strong indicator. Continue cooperation.                                                                               |
| `t > 1`     | False                      | `>= m - 1`                     | **C**    | Very close to threshold. A final push to encourage cooperation.                                                                             |
| `t > 1`     | False                      | `< m - 1`                      | **D**    | Significant failure to meet threshold. Too risky to cooperate. Secure own endowment.                                                        |

#### 2. Handling Edge Cases

*   **First Round (`t = 1`):** As stated above, we **Cooperate**. This is essential to initiate the game with a cooperative gesture and gather the first round's data.

*   **Last Round (`t = r`):**
    *   The strategy as defined above naturally handles the last round. The decision will be based on the outcomes of the second-to-last round.
    *   **If `threshold_met_prev_round` was TRUE (in round `r-1`):** Cooperate in round `r`.
    *   **If `threshold_met_prev_round` was FALSE and `cooperation_count_prev_round >= m - 1` (in round `r-1`):** Cooperate in round `r`.
    *   **If `threshold_met_prev_round` was FALSE and `cooperation_count_prev_round < m - 1` (in round `r-1`):** Defect in round `r`.
    *   The logic remains consistent. There's no "end-game collapse" strategy inherent in ATC, as the decision is purely based on the immediately preceding round's success or near-success. This makes it robust to unknown opponent end-game strategies.

*   **Few Players (`n` is small, close to `m`):**
    *   If `m` is very close to `n`, the condition `cooperation_count_prev_round >= m - 1` becomes more significant. For instance, if `m = n`, then `m-1 = n-1`. If the previous round had `n-1` cooperators, my strategy would cooperate, hoping to reach `n`. If it had fewer than `n-1`, it would defect.
    *   If `m` is slightly larger than `n/2` (e.g., `m = floor(n/2) + 1`), the strategy adapts well. The `m-1` threshold still provides a good indicator of near-success.

*   **High `k` values:** A high `k` amplifies the reward for cooperation. My strategy implicitly accounts for this by aiming to achieve the cooperative outcome. The greater the potential reward, the more incentive there is for ATC to keep pushing for cooperation when close to the threshold.

*   **Low `k` values:** A low `k` reduces the temptation to defect compared to the baseline payoff. The strategy still functions, but the incentive to defect when `cooperation_count_prev_round < m - 1` might be slightly reduced if the difference between payoff 1 and payoff 2 is marginal. However, the core logic of avoiding certain failure remains.

#### 3. Cooperative Mindset

ATC is fundamentally cooperative for several reasons:

*   **Initiation of Cooperation:** It always starts by cooperating, signaling a willingness to contribute to the collective good.
*   **Persistence:** It continues to cooperate after successful rounds and even after near-misses (`m-1` cooperators), demonstrating a commitment to the cooperative outcome.
*   **Adaptive Risk Management:** It only defects when the probability of achieving the cooperative outcome becomes very low. This is not a selfish defection to maximize immediate individual gain, but a pragmatic decision to *avoid a guaranteed lower payoff* that would result from failed cooperation. The goal is to preserve resources so that future cooperation might still be possible, rather than to exploit the system.
*   **No Exploitation Strategy:** It does not try to "game" the system by defecting strategically to get a higher payoff while others cooperate. It seeks the mutually beneficial outcome whenever it's reasonably achievable.

### Pseudocode

```
Strategy ATC:
  Parameters: n, r, m, k
  State Variables:
    my_cooperation_history = []  // Stores 'C' or 'D' for past rounds
    previous_round_payoff = 0

  Function decide_action(current_round_t):
    // --- Decision Logic ---

    if current_round_t == 1:
      action = 'C'
    else:
      // Get data from the previous round (t-1)
      cooperation_count_prev_round = count_cooperators(previous_round_actions) // Need access to all players' actions from previous round
      threshold_met_prev_round = (cooperation_count_prev_round >= m)

      if threshold_met_prev_round:
        action = 'C' // Continue cooperation after success
      else:
        if cooperation_count_prev_round >= m - 1:
          action = 'C' // Close to threshold, attempt last push
        else:
          action = 'D' // Too far from threshold, defect to secure payoff

    // Update history and store for next round's calculation
    my_cooperation_history.append(action)
    return action

  Function update_state(all_players_actions_prev_round, my_payoff_prev_round):
    // This function is called by the tournament system after each round.
    // 'all_players_actions_prev_round' is a list of actions for all players in the previous round.
    // 'my_payoff_prev_round' is the payoff for player i in the previous round.

    // Store previous round's actions for next decision
    previous_round_actions = all_players_actions_prev_round
    previous_round_payoff = my_payoff_prev_round

    // --- For clarity within the pseudocode, these variables are needed for the next 'decide_action' call ---
    // In a real implementation, these would be class members or global variables accessible to decide_action.
    global previous_round_actions
    global previous_round_payoff
```

**Notes on Pseudocode:**

*   The `count_cooperators(previous_round_actions)` function would need to be able to access the list of all players' actions from the *immediately preceding* round. This is critical for calculating `cooperation_count_prev_round`.
*   The `update_state` function is crucial for providing the necessary historical context to the `decide_action` function.

This strategy aims for a balanced approach, fostering cooperation when it's viable and protecting its own stake when the collective effort is clearly failing. It's designed to be predictable to a degree (humans/AIs can learn its pattern) but adaptive to the group's overall behavior.
'''

description_COOPERATIVE_104 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust without relying on communication or pre-existing norms.

## Strategy Name: Adaptive Threshold Cooperation (ATC)

The core idea of ATC is to be cooperative by default, but to adapt its behavior based on the observed success of cooperation in previous rounds. It aims to reach the cooperation threshold ($m$) while minimizing the risk of being exploited when cooperation fails.

### 1. Decision Rules:

ATC's decision to Cooperate (C) or Defect (D) in any given round $t$ (where $t > 0$, as round 0 is the first round) depends on two primary factors:

*   **Recent Cooperation Success:** The proportion of rounds in the recent past where the cooperation threshold ($m$) was met.
*   **Individual Gain from Defection:** The potential immediate payoff difference between defecting and cooperating in the current round, given the current estimated likelihood of the threshold being met.

**Specifically, in round $t > 0$:**

**IF** the proportion of rounds in the recent history (defined below) where $m$ or more players cooperated is **greater than or equal to a defined cooperation threshold ratio ($P_{coop}$)**:

*   **Cooperate (C)**

**ELSE (if recent cooperation success is low):**

*   **Defect (D)**

**Explanation of the "cooperation threshold ratio ($P_{coop}$)" and "recent history":**

*   **Cooperation Threshold Ratio ($P_{coop}$):** This is a dynamic value. Initially, it is set to a value slightly above the minimum required cooperation (e.g., $m/n$). As the game progresses, it will adjust based on observed outcomes. A higher $P_{coop}$ means we require stronger evidence of successful cooperation before committing to it.
*   **Recent History:** We will consider a sliding window of the last $W$ rounds. $W$ can be a parameter that is a fraction of the total rounds $r$ (e.g., $W = \lceil r/4 \rceil$). This allows us to react to recent trends rather than historical averages that might be skewed by early game uncertainty.

**Dynamic Adjustment of $P_{coop}$:**

*   If, in the last $W$ rounds, the cooperation threshold ($m$) was met in **more than** $P_{coop}$ of those rounds, we **increase** $P_{coop}$ slightly. This makes us more inclined to cooperate in the future, assuming cooperation is proving beneficial.
*   If, in the last $W$ rounds, the cooperation threshold ($m$) was met in **less than** $P_{coop}$ of those rounds, we **decrease** $P_{coop}$ slightly. This makes us more cautious and less likely to cooperate, especially if past cooperation has led to exploitation.

The increment/decrement for $P_{coop}$ should be small (e.g., 0.01 or 0.02) to avoid drastic shifts in behavior.

**Crucially, the decision rule is weighted towards cooperation when the history suggests it's viable. The initial inclination is to trust that others will also contribute to the common good.**

### 2. Handling Edge Cases:

*   **Round 0 (First Round):**
    *   **Decision:** **Cooperate (C)**
    *   **Reasoning:** In the absence of any history, the most cooperative and optimistic approach is to lead by example. This sets a positive tone for the game and allows us to gather initial data on how others behave. It's a small risk for a potentially large reward if others reciprocate.
    *   **Initial $P_{coop}$:** Set to $m/n$. This represents the minimum theoretical cooperation needed.
    *   **Initial $W$:** Set to $\lceil r/4 \rceil$.

*   **Last Round ($r$):**
    *   **Decision:** **Defect (D)**
    *   **Reasoning:** In the final round, there is no future to incentivize cooperation. Any cooperation now will not be reciprocated in subsequent rounds. Therefore, the rational decision is to maximize individual payoff by keeping the endowment, as there are no future benefits to be gained from collective action.

### 3. Being Cooperative:

ATC is inherently cooperative for several reasons:

*   **Default to Cooperation:** In the first round and whenever the historical success rate of cooperation is deemed sufficiently high, ATC chooses to cooperate. This demonstrates a willingness to contribute to the collective good.
*   **Adaptive Learning:** The strategy learns from past outcomes. If cooperation is consistently successful in meeting the threshold, ATC becomes *more* cooperative (by increasing the likelihood it will cooperate). It doesn't punish past failures by immediately becoming a defector; instead, it adjusts its threshold for *requiring* evidence of cooperation's success.
*   **Risk Aversion for the Collective:** While ATC aims to be cooperative, it's not naive. It's designed to avoid being exploited. If cooperation consistently fails to reach the threshold, it becomes more cautious. This is a form of strategic cooperation – ensuring that attempts at cooperation don't consistently lead to individual losses.
*   **No Preemptive Exploitation:** ATC does not defect simply because it *could* gain slightly more in a single round. The decision to defect is triggered by a lack of *evidence* that collective cooperation is working.

---

### Pseudocode for Adaptive Threshold Cooperation (ATC)

```pseudocode
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward if threshold met factor

// Strategy Parameters
W: window size for recent history (e.g., W = ceil(r / 4))
P_coop_initial: initial cooperation threshold ratio (e.g., m / n)
P_coop_adjustment_step: small step for adjusting P_coop (e.g., 0.01)

// State Variables (for player i)
P_coop: current cooperation threshold ratio (initialized to P_coop_initial)
round_outcomes: list to store the number of cooperators in each past round
player_actions_history: list of (round_number, action) for this player

function decide_action(current_round_number, historical_round_actions):
    // historical_round_actions is a list of lists, where each inner list contains the actions of all players for a given round.
    // Example: [[C, D, C, ...], [D, D, C, ...], ...]

    // --- Edge Case: Last Round ---
    if current_round_number == r - 1: // 0-indexed rounds
        return D

    // --- Edge Case: First Round ---
    if current_round_number == 0:
        P_coop = P_coop_initial
        return C

    // --- Adaptive Decision Logic (for rounds 1 to r-2) ---

    // 1. Analyze Recent History
    recent_history_start_round = max(0, current_round_number - W)
    cooperation_met_count = 0
    total_recent_rounds = 0

    for t from recent_history_start_round to current_round_number - 1:
        num_cooperators_in_round_t = count_cooperators(historical_round_actions[t])
        if num_cooperators_in_round_t >= m:
            cooperation_met_count += 1
        total_recent_rounds += 1

    // Avoid division by zero if total_recent_rounds is 0 (should not happen with W > 0 and current_round_number > 0)
    recent_cooperation_success_ratio = 0
    if total_recent_rounds > 0:
        recent_cooperation_success_ratio = cooperation_met_count / total_recent_rounds

    // 2. Update P_coop (adaptive learning)
    // Slightly more cooperative if recent success exceeds P_coop
    if recent_cooperation_success_ratio >= P_coop:
        P_coop = min(1.0, P_coop + P_coop_adjustment_step) // Cap at 1.0
    // Slightly more cautious if recent success falls short of P_coop
    else:
        P_coop = max(m / n, P_coop - P_coop_adjustment_step) // Cap at initial minimum

    // 3. Make Decision
    if recent_cooperation_success_ratio >= P_coop:
        // If recent cooperation was successful enough, cooperate
        return C
    else:
        // Otherwise, defect to protect individual endowment
        return D

function count_cooperators(round_actions):
    count = 0
    for action in round_actions:
        if action == C:
            count += 1
    return count

// --- Initialization and Tournament Play ---
// For each player i:
// Initialize P_coop = P_coop_initial
// Initialize player_actions_history = []
// Initialize historical_round_actions = [] for the tournament system to manage

// In each round t:
//   my_action = decide_action(t, historical_round_actions)
//   Store my_action in player_actions_history
//   Send my_action to the tournament system
//   Receive actions of all other players for round t
//   Add all actions for round t to historical_round_actions
```
'''

description_COOPERATIVE_105 = '''
This is an intriguing setup, a variation of the Public Goods Game with a critical threshold for cooperation. The core challenge lies in incentivizing cooperation when individual defection can lead to a higher payoff, especially if cooperation fails. The repeated nature of the game, however, opens the door for strategic learning and adaptation.

Here's a cooperative strategy designed for this Collective Risk Dilemma, focusing on adaptiveness and robustness:

## Cooperative Strategy: "Threshold Weaver"

The "Threshold Weaver" strategy aims to maximize long-term cooperation by carefully observing the group's behavior and adjusting its own actions to nudge the group towards meeting the cooperation threshold, while simultaneously protecting itself from exploitation. It's inherently cooperative but also strategically prudent.

### 1. Decision Rules:

The core of this strategy lies in tracking the number of cooperators in previous rounds and inferring the group's tendency.

*   **Initial Cooperation (Round 1):**
    *   In the first round, to signal a cooperative intent and test the waters, **always cooperate (play C)**. This is a high-risk, high-reward move. If enough others cooperate, the reward is significant. If not, the individual payoff is 1, but it demonstrates a willingness to contribute.

*   **Subsequent Rounds (t > 1):**
    *   **Track Cooperation History:** Maintain a running count of the number of players who cooperated in the *immediately preceding round* (let's call this `cooperators_last_round`).
    *   **Primary Decision Rule:**
        *   **If `cooperators_last_round` >= `m` (the threshold is met or exceeded):** **Cooperate (play C)**. This reinforces successful cooperation and signals that the group can achieve the reward. The individual receives `(1-1) + k = k`.
        *   **If `cooperators_last_round` < `m` (the threshold is NOT met):**
            *   **Calculate Individual Risk:** If I cooperate now and `m-1` other players also cooperate, then I would have cooperated, and the total cooperators would be `1 + (m-1) = m`. My payoff would be `(1-1) + k = k`.
            *   **Calculate Defection Benefit:** If I defect now and others cooperate or defect, my payoff is at least `1`.
            *   **The Decision:**
                *   **If my individual payoff from cooperating (assuming enough others *might* also cooperate to reach the threshold) is greater than or equal to my payoff from defecting (which is at least 1):** **Cooperate (play C)**. This happens when `k >= 1`. Given `k > 1`, this condition is always met. However, the *real* decision hinges on the *likelihood* of reaching the threshold.
                *   **Strategic Adjustment:** Instead of a direct comparison of individual payoffs, we focus on the group's performance. If the threshold was *not* met in the last round, it indicates a lack of collective effort. In this scenario, the strategy becomes more cautious to avoid being the sole cooperator in a failed attempt.
                    *   **If `cooperators_last_round` >= `m - 1` (meaning we were "close" to the threshold):** **Cooperate (play C)**. This is a strong signal to try and "pull" the group over the threshold. You are willing to take a personal risk if the group is already showing significant cooperative effort.
                    *   **If `cooperators_last_round` < `m - 1` (meaning we were far from the threshold):** **Defect (play D)**. This is a protective measure. If the group is not even close to `m-1` cooperators, then defecting is safer, as your payoff will be `1` (if no one cooperates) or `1 + k` (if enough others *unexpectedly* cooperate). Cooperating in such a scenario would likely result in a payoff of `1` (if the threshold is still not met).

### 2. Handling Edge Cases:

*   **First Round (t=1):** As stated above, **always cooperate (play C)**. This is the starting point for establishing cooperative behavior. It's a signal of good faith.

*   **Last Round (t=r):** In the final round, there is no future to incentivize. The immediate payoff is paramount.
    *   **If `cooperators_last_round` >= `m`:** **Cooperate (play C)**. The reward `k` is still valuable.
    *   **If `cooperators_last_round` < `m`:** **Defect (play D)**. There's no benefit to taking a risk for a reward that won't be reciprocated or doesn't have future implications. The safe payoff of `1` is preferred over the possibility of `1` from a failed cooperation.

*   **"Almost Meeting" the Threshold:** The rule of cooperating if `cooperators_last_round >= m - 1` directly addresses this. It's about sensing momentum and pushing for collective success.

### 3. Being Cooperative:

This strategy is fundamentally cooperative because:

*   **It prioritizes meeting the threshold:** The primary goal is to facilitate the condition where *everyone* benefits (`k`).
*   **It signals and rewards cooperation:** By cooperating when the threshold is met or nearly met, it reinforces positive group behavior and encourages future cooperation.
*   **It takes an initial risk:** The first-round cooperation is a clear demonstration of a cooperative mindset, even without knowing how others will behave.
*   **It adapts:** The strategy learns from the group's past behavior and adjusts its actions to try and guide the group towards desired outcomes. It doesn't stubbornly cooperate if the group consistently fails to meet the threshold, which would be self-destructive and counterproductive to long-term collective success.

### Pseudocode:

```pseudocode
// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward if threshold met factor

// Game State
cooperators_history: list of integers, storing the number of cooperators in each past round

// Strategy Function for Player i in Round t
function DecideAction(player_id, current_round, previous_round_actions):
  // previous_round_actions is a dictionary mapping player_id to action (C or D) for round t-1

  if current_round == 1:
    // Always cooperate in the first round
    return "C"

  else:
    // Count cooperators in the previous round
    cooperators_last_round = 0
    if t-1 in previous_round_actions: // Ensure we have previous round data
      for player, action in previous_round_actions[t-1].items():
        if action == "C":
          cooperators_last_round += 1

    if current_round == r:
      // Last round: act solely on immediate payoff
      if cooperators_last_round >= m:
        return "C" // Reward is still valuable
      else:
        return "D" // No future to incentivize, play safe

    else:
      // Intermediate rounds: adaptive strategy
      if cooperators_last_round >= m:
        // Threshold met in the last round, reinforce cooperation
        return "C"
      elif cooperators_last_round >= m - 1:
        // Close to the threshold, attempt to push the group over
        return "C"
      else:
        // Far from the threshold, play defensively
        return "D"

// Example of how to update state outside the decision function (handled by game simulator)
// After each round, the simulator would:
// 1. Record actions of all players in that round.
// 2. Calculate payoffs.
// 3. Update the 'cooperators_history' for the next round's 'DecideAction' call.
```

This "Threshold Weaver" strategy aims to be a balanced and intelligent approach to the Collective Risk Dilemma, fostering cooperation while remaining robust against exploitation. Its adaptive nature allows it to learn from the collective behavior and adjust its actions accordingly, increasing the likelihood of sustained cooperation and maximized payoffs for the group in the long run.
'''

description_COOPERATIVE_106 = '''
This is a fascinating and challenging game! The core tension lies in the temptation to free-ride on others' cooperation while the potential reward for collective action is significant. My cooperative strategy will focus on a balance of conditional cooperation and a degree of strategic patience, aiming to foster cooperation without being overly exploited.

Here's the design for the cooperative strategy:

## Cooperative Strategy: "Adaptive Threshold Seeker"

The core idea is to start with a cautious approach to cooperation, gradually increasing commitment as evidence suggests a critical mass of cooperation is achievable, and then scaling back if cooperation breaks down. This strategy is designed to be robust by learning from the collective behavior observed in previous rounds.

### 1. Decision Rules

My decision for player `i` in round `t` will be based on the following:

*   **Initial Cooperation Phase (Early Rounds):** In the early rounds, my strategy will lean towards cooperation to signal a willingness to build collective action. However, this cooperation will be conditional on observing a positive trend in cooperation from others.
*   **Dynamic Cooperation Threshold:** I will not commit to a fixed number of cooperators. Instead, I will dynamically assess the likelihood of reaching the `m` threshold based on the observed cooperation in previous rounds.
*   **Punishment for Defection (with caveats):** If cooperation significantly drops below expectations or the threshold, my strategy will temporarily defect to protect its own endowment. However, this defection is not punitive in a revengeful sense; it's a reactive measure to the perceived breakdown of collective action.
*   **Exploiting the Reward:** If the threshold `m` is consistently met or exceeded, my strategy will embrace cooperation to maximize the collective reward.

**Specific Decision Logic:**

Let `C_t-1` be the number of players who cooperated in round `t-1`.

*   **Round 1 (Initial Exploration):**
    *   **If `n` is large (e.g., `n >= 5` or some heuristic threshold):** Cooperate (C). The idea is to test the waters and signal a cooperative intent in a potentially anonymous environment where individual action might feel less impactful.
    *   **If `n` is small (e.g., `n < 5`):** Cooperate (C). Even with fewer players, signaling cooperation early can be crucial for building momentum.

*   **Rounds `t > 1`:**
    *   **Define `cooperation_probability_estimate`:** This is a crucial dynamic component. It will be an estimate of the probability that `m` or more players will cooperate in the current round.
        *   **Method 1 (Simple Moving Average):** `cooperation_probability_estimate = max(0, min(1, (C_t-1) / n))`  (This is a basic ratio, not a probability, but a starting point for thinking)
        *   **Method 2 (More Sophisticated - Weighted Moving Average/Exponential Smoothing):** We can give more weight to recent rounds. Let `avg_coop_ratio_t-1 = (C_t-1) / n`. A smoothed ratio might be: `smoothed_coop_ratio = alpha * avg_coop_ratio_t-1 + (1 - alpha) * smoothed_coop_ratio_t-2` (where `alpha` is a smoothing factor). Then, `cooperation_probability_estimate` could be derived from this smoothed ratio. For simplicity in this description, let's consider a simpler heuristic based on the number of cooperators.

    *   **Decision Rule:**
        *   **If `C_t-1 >= m - 1` (i.e., one less than the threshold was met in the *previous* round):** Cooperate (C). This indicates that the threshold was *very* close. We'll try to be the one to tip it over.
        *   **If `C_t-1 >= m` (i.e., the threshold was met or exceeded in the previous round):** Cooperate (C). Reward past success and maintain cooperation.
        *   **If `C_t-1 < m - 1` AND `(C_t-1 / n) > (m / n) * 0.75` (i.e., the cooperation ratio in the previous round was reasonably high, say at least 75% of the required ratio):** Cooperate (C). This is a flexible approach. Even if the threshold wasn't met, a strong trend towards cooperation suggests it's worth continuing to contribute. The `0.75` factor is a heuristic that can be tuned.
        *   **If `C_t-1 < m - 1` AND `(C_t-1 / n) <= (m / n) * 0.75` AND `t < r` (i.e., cooperation is lagging and it's not the last round):** Defect (D). If cooperation isn't showing strong signs of reaching the threshold, and we're not in the final round where future interaction doesn't matter, it's safer to protect my endowment. This is a "wait and see" approach, not an aggressive punishment.
        *   **If `t == r` (Last Round):**
            *   **If `C_t-1 >= m`:** Cooperate (C). Maximize the reward, as there are no future consequences.
            *   **If `C_t-1 < m`:** Defect (D). In the absence of future cooperation, it's best to secure the private payoff. There's no incentive to cooperate if the reward cannot be achieved.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):** As described above, I will initiate cooperation. This is a crucial decision to set the tone and try to build initial momentum. Without this, it's easy for the game to devolve into universal defection immediately.
*   **Last Round (`t = r`):** My behavior in the last round is purely consequential. If the `m` threshold was met in the penultimate round, I will cooperate to secure the reward. Otherwise, I will defect to guarantee my private endowment. There's no strategic advantage to cooperating in the last round if the reward cannot be achieved.
*   **`m = 2` (Minimum cooperators is very low):** This makes cooperation easier to achieve but also increases the temptation to free-ride. My strategy will be more inclined to cooperate if `m=2` and the observed cooperation is even slightly above average, as reaching the threshold is easier.
*   **`k` is very high:** A high `k` amplifies the reward for cooperation. My strategy will become more risk-averse to cooperation if `k` is extremely high, as the potential loss from not cooperating (missing out on the reward) is also high. Conversely, a high `k` makes the reward more appealing, so the strategy will be more forgiving of temporary dips in cooperation if `k` is very large.
*   **`n` is very large/small:** As mentioned in Round 1, the initial strategy might be adjusted based on `n`. With very large `n`, individual contributions to reaching `m` might feel less impactful, so the strategy might be slightly more cautious in the early rounds. With very small `n`, each player's decision is more critical, so stronger initial cooperation might be warranted.

### 3. Being Cooperative

My strategy is fundamentally cooperative in its intent and primary behavior.

*   **Proactive Signaling:** The initial cooperation aims to be a positive signal, encouraging others to reciprocate.
*   **Conditional Cooperation:** It rewards observed cooperation by continuing to cooperate. This aligns with the tit-for-tat philosophy of reciprocity.
*   **Focus on Collective Gain:** The strategy's primary goal is to achieve the state where `m` or more players cooperate, as this benefits everyone involved in that round.
*   **Adaptive, Not Punitive:** My "defections" are not designed to punish specific players but rather to react to a breakdown in the *collective* cooperation mechanism. The goal is to incentivize a return to cooperation, not to enact revenge. Once cooperation trends upwards again, I will resume cooperating.

### Pseudocode Example (Illustrative - actual implementation would refine `cooperation_probability_estimate`)

```pseudocode
// Game Parameters
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

// History
// actions_history: list of lists, actions_history[t][i] is player i's action in round t
// num_cooperators_history: list of integers, num_cooperators_history[t] is the number of cooperators in round t

function decide_action(current_round, actions_history, num_cooperators_history):
    // Initialization for the first round
    if current_round == 1:
        return 'C' // Start with cooperation

    // Get data from previous round
    previous_round = current_round - 1
    num_cooperators_prev = num_cooperators_history[previous_round]

    // --- Dynamic Decision Logic ---
    // Heuristic for assessing likelihood of reaching threshold
    // This can be more sophisticated, e.g., using moving averages or exponential smoothing
    // For simplicity, we'll use a direct lookback at previous round's cooperators.

    // Case 1: Threshold was met or exceeded in the previous round
    if num_cooperators_prev >= m:
        return 'C' // Continue cooperation, reward past success

    // Case 2: Threshold was almost met (one less player needed)
    if num_cooperators_prev == m - 1:
        return 'C' // Try to be the one to tip it over

    // Case 3: Cooperation ratio was reasonably high, even if threshold wasn't met
    // The threshold factor (e.g., 0.75) is a tunable parameter
    if num_cooperators_prev >= (m / n) * 0.75 * n and current_round < r:
        return 'C' // Strong trend, continue cooperating if not the last round

    // Case 4: Last round, and threshold wasn't met
    if current_round == r and num_cooperators_prev < m:
        return 'D' // Secure private endowment in the last round if reward is not achievable

    // Case 5: Cooperation is lagging and it's not the last round
    // This is the default if the above conditions aren't met and it's not the last round
    if current_round < r:
        return 'D' // Defect to protect endowment if cooperation isn't strong enough

    // Default for any unhandled edge cases (should ideally not be reached with thorough logic)
    return 'D'

// Example of how num_cooperators_history might be populated:
// After each round t:
//   count = 0
//   for action in actions_history[t]:
//     if action == 'C':
//       count += 1
//   num_cooperators_history.append(count)

```

This "Adaptive Threshold Seeker" strategy aims to be a robust and cooperative player by dynamically adjusting its behavior based on the observed collective actions of others, always prioritizing the achievement of the collective reward while safeguarding against exploitation.
'''

description_COOPERATIVE_107 = '''
## Strategy: Adaptive Cooperative Threshold Seeker (ACTS)

This strategy aims to foster cooperation by carefully balancing the desire for the collective reward with the risk of individual defection. It adapts its behavior based on observed cooperation levels, aiming to push the group towards the cooperative threshold without being exploited.

### Core Principles:

1.  **Cautious Cooperation:** Initially, err on the side of caution to avoid being the sole cooperator, but be prepared to cooperate when the group shows signs of moving towards the threshold.
2.  **Threshold Reinforcement:** Once the threshold is met, reward that behavior by continuing to cooperate, reinforcing the collective gain.
3.  **Punishment for Non-Cooperation:** If cooperation falters and the threshold is not met, reduce cooperation to minimize individual losses.
4.  **Future-Oriented:** The repeated nature of the game is crucial. This strategy considers the long-term implications of its actions on future round dynamics.

### 1. Decision Rules:

Let $O_t$ be the observed number of cooperators in round $t$.
Let $C_t$ be the number of players I *expect* to cooperate in round $t$.

**For round $t > 1$:**

*   **Cooperate (C) if:**
    *   The observed number of cooperators in the previous round ($O_{t-1}$) was greater than or equal to $m$.
    *   OR The observed number of cooperators in the previous round ($O_{t-1}$) was between $m-1$ and $m$ (inclusive), *and* I anticipate enough other players will also cooperate to meet the threshold. This anticipation is based on a simple heuristic: if the previous round was *close* to the threshold, there's a higher chance others will also risk cooperating to push it over. Specifically, if $O_{t-1} \ge m-1$, I will cooperate.

*   **Defect (D) if:**
    *   The observed number of cooperators in the previous round ($O_{t-1}$) was less than $m-1$.
    *   OR The observed number of cooperators in the previous round ($O_{t-1}$) was $m-1$ and I *don't* anticipate others will cooperate (this is a subtle point; the heuristic above simplifies this to always cooperating if $O_{t-1} \ge m-1$).

**Simplified Decision Rule for round $t > 1$:**

If $O_{t-1} \ge m-1$: Cooperate (C)
Else: Defect (D)

This simplified rule is more robust. If the group was *just shy* of the threshold ($m-1$), cooperating might be the catalyst needed. If they were further away ($< m-1$), defecting is the safer bet to avoid being the sole cooperator.

### 2. Handling Edge Cases:

*   **First Round ($t=1$):**
    *   The strategy must make an initial leap of faith. Given the cooperative goal, a reasonable starting point is to **Cooperate (C)**. This signals a willingness to cooperate and test the waters. It's a direct investment in the possibility of achieving the collective reward from the outset. While risky, it's aligned with the cooperative principle.

*   **Last Round ($t=r$):**
    *   In the final round, there are no future repercussions for one's actions. This incentivizes pure self-interest. However, the ACTS strategy is designed to be *predictive* of this behavior.
    *   **Decision for round $r$ is based on $O_{r-1}$ as per the general rule.** If $O_{r-1} \ge m-1$, cooperate. Otherwise, defect. The "cooperative" element here is about *maintaining* past cooperation if it was successful. There's no explicit "punishment" in the last round beyond the loss of the collective reward that round. The strategy doesn't change its rule for the last round, but the *incentive structure* for other players is different.

*   **Edge cases for parameters:**
    *   **$m=2$**: If $m=2$, the strategy will cooperate if at least one other player cooperated in the previous round. In the first round, it cooperates.
    *   **$m=n-1$**: If $m=n-1$, the strategy will cooperate if the group was one cooperator short of success in the previous round. In the first round, it cooperates.

### 3. Being Cooperative:

*   **Proactive Cooperation:** The strategy initiates cooperation in the first round, demonstrating a commitment to the collective good.
*   **Reward for Cooperation:** By cooperating when the observed cooperation is high ($O_{t-1} \ge m-1$), the strategy actively reinforces successful cooperative outcomes, maximizing the collective reward for all involved.
*   **Risk Mitigation:** While proactive, the strategy is not naive. If cooperation falters significantly ($O_{t-1} < m-1$), it defaults to defection to protect its own endowment. This is a pragmatic form of cooperation – ensuring the group's success is contingent on a certain level of participation, rather than individual sacrifice for a lost cause.
*   **Adaptive Nature:** The strategy learns from the history of the game. It doesn't employ a fixed strategy but adjusts its behavior based on the actual outcomes observed, aiming to guide the group towards higher cooperation over time.

### Pseudocode:

```python
class ACTS_Strategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.round = 0
        self.observed_cooperators_prev_round = 0

    def choose_action(self, history):
        """
        Decides whether to Cooperate (C) or Defect (D).
        history: A list of tuples, where each tuple is (my_action, opponent_actions_list) for previous rounds.
                 In a real implementation, we'd need a way to track total observed cooperators.
                 For simplicity here, we assume we can access the observed_cooperators_prev_round.
        """
        self.round += 1

        # Get observed cooperators from the previous round
        # This part needs actual implementation based on how 'history' is structured
        # For this pseudocode, we'll assume we have access to self.observed_cooperators_prev_round

        if self.round == 1:
            # First round: Initiate cooperation
            self.observed_cooperators_prev_round = 1 # Assume I cooperated
            return 'C'
        else:
            # Subsequent rounds: Adapt based on previous round's observation
            if self.observed_cooperators_prev_round >= self.m - 1:
                # Cooperate if the previous round was at or near the threshold
                # If I cooperate, the observed count for *next* round increases by 1
                self.observed_cooperators_prev_round += 1
                return 'C'
            else:
                # Defect if the previous round was far below the threshold
                # If I defect, the observed count for *next* round doesn't change (relative to others)
                # self.observed_cooperators_prev_round remains as is (for other players)
                return 'D'

    def update_observed_cooperators(self, total_cooperators_in_round):
        """
        This method would be called *after* each round to update the state.
        """
        self.observed_cooperators_prev_round = total_cooperators_in_round

```

**Explanation of `update_observed_cooperators`:**

The `update_observed_cooperators` method is crucial for state management. After each round concludes, the algorithm needs to know how many players *actually* cooperated in that round. This number then becomes `self.observed_cooperators_prev_round` for the *next* decision.

If the strategy chooses 'C', and the `total_cooperators_in_round` already reflects this (i.e., `total_cooperators_in_round` is the sum of *all* players' actions for that round), then the update is straightforward. The key is to use the *actual observed outcome* from the completed round to inform the decision for the *upcoming* round.

The pseudocode's `self.observed_cooperators_prev_round += 1` within the `choose_action` function is a slight simplification for illustrative purposes assuming the strategy's *own* cooperation is added to the count. In a true implementation, `update_observed_cooperators` would be called with the *final total* from the round, and the `choose_action` logic would then use this received value for `self.observed_cooperators_prev_round`.

**Robustness:**

This strategy is robust because:

*   **Initial Cooperation:** It doesn't wait for others to prove themselves; it takes the first step.
*   **Threshold Proximity:** It's forgiving of near misses ($m-1$), understanding that the threshold is a critical tipping point.
*   **Clear Defection Trigger:** It clearly defects when cooperation is too low, preventing exploitation.
*   **No Complex Memory:** It only relies on the immediate past, making it less susceptible to complex oscillating strategies from opponents and easier to manage state.

This strategy aims to be a consistent, adaptable player that fosters cooperation when viable and protects itself when necessary, making it a strong contender in a tournament setting.
'''

description_COOPERATIVE_108 = '''
This is a fascinating collective risk dilemma, blending elements of the Prisoner's Dilemma with a public goods game. The core challenge is incentivizing cooperation when individual defection is always tempting, especially when others cooperate.

Here's a cooperative strategy designed to be adaptive and robust, focusing on building trust and learning from history without explicit communication.

## Cooperative Strategy: "Cautious Reciprocity with Threshold Awareness"

This strategy aims to be cooperative by default but adjusts based on the observed behavior of others and the game's parameters. It prioritizes achieving the cooperative reward while mitigating the risk of being exploited.

### 1. Decision Rules

The core decision for player $i$ in any round $t$ (where $t > 1$) is based on a comparison of the *current observed cooperation rate* against the *required cooperation threshold* ($m$).

**For Round $t > 1$:**

*   **Cooperate (C) if:**
    *   The number of players who cooperated in the *previous* round ($t-1$) was **at least** $m$.
    *   **AND** (This is a crucial adaptive element) The number of players who cooperated in the previous round ($t-1$) was **strictly greater than** the number of players who defected in the previous round ($t-1$). This is a heuristic to gauge if cooperation is "trending up" or at least stable, beyond just meeting the minimum.

*   **Defect (D) if:**
    *   The number of players who cooperated in the *previous* round ($t-1$) was **less than** $m$.
    *   **OR** The number of players who cooperated in the *previous* round ($t-1$) was **equal to or less than** the number of players who defected in the previous round ($t-1$). This indicates a potential decline in cooperation or an equilibrium where defectors are not being punished sufficiently.

**Rationale for the "AND" condition:** The first condition ($ \ge m $) ensures we exploit the cooperative reward. The second condition ($ > \text{defectors} $) is a dynamic filter. If exactly $m$ players cooperated and $n-m$ defected, this strategy would defect. Why? Because the $n-m$ defectors are benefiting from the cooperation of the $m$ cooperators without contributing themselves. This suggests the current state might be unstable, and the cooperating players are being exploited. By defecting in such a scenario, this strategy signals that such exploitation is not tolerated. If $m$ players cooperate and $n-m$ defect, the strategy defects. If $m+1$ cooperate and $n-(m+1)$ defect, the strategy cooperates.

### 2. Handling Edge Cases

*   **Round 1:**
    *   **Decision Rule:** Cooperate (C).
    *   **Rationale:** In the absence of any historical data, the most cooperative stance is to assume others will also aim for cooperation. This is the ideal starting point to trigger the mutual reward. It's a high-risk, high-reward initial move to establish a cooperative norm.

*   **Last Round ($r$):**
    *   **Decision Rule:** Defect (D).
    *   **Rationale:** In the final round, there are no future consequences to defection. Any cooperation would be altruistic, providing no personal benefit after this round. Defection maximizes the player's immediate payoff, as the reward from collective cooperation is no longer relevant for future rounds. This is a purely self-interested decision for the terminal round.

*   **Edge Cases for $m$ and $n$ (based on parameters):**
    *   If $m=2$ and $n=3$ (e.g., $m=2$, $n=3$, $k=2$):
        *   Round 1: Cooperate (C).
        *   Round 2: If 2 cooperated in R1 (and 1 defected): Cooperate (C) because $2 \ge m$ AND $2 > 1$. If only 1 cooperated in R1: Defect (D) because $1 < m$.
        *   Round 2: If 3 cooperated in R1: Cooperate (C) because $3 \ge m$ AND $3 > 0$.
    *   If $m$ is very close to $n$ (e.g., $m=n-1$): Cooperation becomes very difficult. This strategy will be highly sensitive to the actions of even one defector.
    *   If $m$ is just slightly larger than 1 (e.g., $m=2$): Cooperation is easier to achieve, and this strategy will be more consistently cooperative.

### 3. Being Cooperative

*   **Default to Cooperation:** The strategy *starts* by cooperating in Round 1.
*   **Conditional Cooperation:** It aims to cooperate whenever the conditions for collective benefit are met ($\ge m$ cooperators) and the past trend suggests stability or growth in cooperation (more cooperators than defectors).
*   **Punishment for Exploitation:** The "defect if $ \le $ defectors" condition is a form of conditional cooperation (akin to Tit-for-Tat, but for collective outcomes). It punishes scenarios where a significant number of players are free-riding on the efforts of a few. This is a cooperative act in the sense that it discourages free-riding, which is essential for long-term collective benefit.
*   **Learning and Adaptation:** The strategy learns from the immediate past round's outcome. It doesn't rigidly adhere to a fixed pattern but adjusts its behavior based on the observed collective action.
*   **Parameter Awareness:** The decision rules are directly informed by $m$, the threshold for success. The reward factor $k$ is implicitly valued by seeking to achieve the condition where it's applied.

### Pseudocode

```python
def decide_action(current_round, history, n, m, k):
    """
    Decides action (C or D) for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        history (list of dicts): A list where each dict represents a round's outcome.
                                 Each dict: {'actions': {player_id: action, ...}, 'cooperators': int, 'defectors': int}
        n (int): Total number of players.
        m (int): Minimum cooperators needed for reward.
        k (float): Reward factor.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # --- Edge Case: Last Round ---
    if current_round == r: # Assuming 'r' is available globally or passed in
        return 'D'

    # --- Edge Case: First Round ---
    if current_round == 1:
        return 'C' # Start cooperatively

    # --- General Rounds (t > 1) ---
    previous_round_data = history[-1]
    prev_cooperators = previous_round_data.get('cooperators', 0)
    prev_defectors = previous_round_data.get('defectors', 0)

    # Condition 1: Met or exceeded the minimum required cooperators
    met_threshold = prev_cooperators >= m

    # Condition 2: Cooperation is "winning" or stable (more cooperators than defectors)
    # This is a heuristic to prevent exploitation when exactly 'm' cooperate and many defect.
    cooperation_trending_up = prev_cooperators > prev_defectors

    if met_threshold and cooperation_trending_up:
        return 'C' # Cooperate if threshold met and cooperation is dominant
    else:
        return 'D' # Defect if threshold not met OR cooperation is not dominant/declining

# Example of how history might be structured and used (within a larger game loop):
# Assuming 'r' is defined globally or passed as a parameter.
# history = []
# for t in range(1, r + 1):
#     actions_this_round = {}
#     # ... (players make decisions based on their strategies and history)
#
#     cooperators_count = sum(1 for action in actions_this_round.values() if action == 'C')
#     defectors_count = n - cooperators_count
#
#     history.append({
#         'actions': actions_this_round,
#         'cooperators': cooperators_count,
#         'defectors': defectors_count
#     })
#
#     my_action = decide_action(t, history, n, m, k)
#     # ... (record my_action)
```

This strategy is cooperative by default, adapts to the collective's behavior, and has a clear mechanism to disincentivize free-riding while maximizing the chances of achieving the collective reward. Its robustness comes from not relying on any single player's consistency and its ability to react to observed group dynamics.
'''

description_COOPERATIVE_109 = '''
This strategy, "Adaptive Tit-for-Tat with Cooperative Threshold Awareness" (ATT-CTA), aims to foster cooperation by rewarding it and punishing defection, while remaining acutely aware of the collective risk and the parameters governing success. It balances a desire for cooperation with a pragmatic approach to ensure individual gains are not jeopardized by others' non-compliance.

## ATT-CTA Strategy Design

The core of ATT-CTA is a modified Tit-for-Tat approach. It cooperates if the collective history suggests cooperation is likely to succeed, but defects if the threshold for cooperation has not been met or if there's a pattern of defection. It also incorporates a "forgiveness" mechanism to avoid perpetual punishment cycles.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round `t` is determined by a set of conditions based on the game parameters (`n`, `m`, `k`) and the observed history of actions and outcomes up to round `t-1`.

**Core Decision Logic:**

*   **Initial Cooperation Tendency:** In the early rounds, ATT-CTA will lean towards cooperation to signal its intent and test the waters.
*   **Threshold Monitoring:** The primary driver for sustained cooperation is the observation of whether the collective action in previous rounds has met or exceeded the `m` cooperation threshold.
*   **Punishment for Failure:** If the `m` threshold has *not* been met in a round, ATT-CTA will defect in the subsequent round. This punishes the lack of collective effort.
*   **Forgiveness and Re-engagement:** To avoid getting stuck in a downward spiral of defection, ATT-CTA will eventually re-attempt cooperation if the environment appears to be shifting or if a sufficient number of players have also started cooperating.
*   **Exploitation Prevention:** If a player consistently defects while others cooperate, ATT-CTA will eventually defect against them to prevent exploitation.

**Specific Rules for Round `t`:**

Let `cooperators_t-1` be the number of players who cooperated in round `t-1`.
Let `my_action_t-1` be the action ATT-CTA took in round `t-1`.
Let `opponent_actions_t-1` be a record of each opponent's actions in round `t-1`.

**Rule 1: First Round (t=1)**

*   **Decision:** Cooperate (C)

    *   **Rationale:** To initiate cooperation and signal a cooperative intent. It's a low-risk initial move, as the endowment is still secured, and the hope is to trigger the `k` reward.

**Rule 2: Subsequent Rounds (t > 1)**

*   **Condition 2.1: Sufficient Cooperation in Previous Round**
    If `cooperators_t-1 >= m`:
        *   **Decision:** Cooperate (C)
            *   **Rationale:** The community project was successful, and the reward `k` was likely achieved or within reach. This reinforces successful cooperative behavior.

*   **Condition 2.2: Insufficient Cooperation in Previous Round (and last round was not a "reset")**
    If `cooperators_t-1 < m` AND `my_action_t-1 == C`:
        *   **Decision:** Defect (D)
            *   **Rationale:** The collective effort failed to meet the threshold. ATT-CTA punishes the perceived lack of cooperation from the group. By defecting, it preserves its individual endowment and signals dissatisfaction.

*   **Condition 2.3: Re-engagement / Forgiveness (Adaptive Threshold)**
    If `cooperators_t-1 < m` AND `my_action_t-1 == D`:
        *   **Decision:** Cooperate (C) IF `cooperators_t-1 >= m_adaptive` (where `m_adaptive` is a dynamically adjusted threshold, explained below). Otherwise, Defect (D).
            *   **Rationale:** If the group is showing signs of increased cooperation (even if not quite `m` yet), ATT-CTA will attempt to re-engage. This `m_adaptive` threshold is crucial for preventing permanent defection cycles.

*   **Condition 2.4: Persistent Individual Defection (Opponent-Specific)**
    If a specific opponent `j` has consistently played D for a certain number of consecutive rounds (`punishment_streak_threshold`), and ATT-CTA has been cooperating:
        *   **Decision:** Defect (D) against player `j` for the current round `t`.
            *   **Rationale:** To prevent exploitation by a persistent defector. This requires tracking individual opponent behavior. This is a localized punishment, not necessarily a full group punishment.

**`m_adaptive` Mechanism (For Condition 2.3):**

The `m_adaptive` threshold starts at `m` and can dynamically adjust.

*   When `cooperators_t-1 < m`:
    *   If the number of cooperators in the previous round (`cooperators_t-1`) is *higher* than the number of cooperators in the round before that (`cooperators_t-2`), slightly decrease `m_adaptive` (e.g., by 1, but not below 1). This rewards upward trends.
    *   If the number of cooperators remains stagnant or decreases, `m_adaptive` stays the same or increases slightly (e.g., by 1, but not above `m`).

**Rule 3: Last Round (t=r)**

*   **Decision:** Defect (D)

    *   **Rationale:** In the final round, there are no future rounds to incentivize cooperation. The primary goal is to maximize individual payoff. Defecting guarantees the player their private endowment (1) and doesn't risk losing it if the `m` threshold isn't met. This is a pragmatic adjustment for the end of the game.

### 2. Handling Edge Cases

*   **First Round (t=1):** As per Rule 1, ATT-CTA always Cooperates. This is the "optimistic initiation" phase.
*   **Last Round (t=r):** As per Rule 3, ATT-CTA always Defects. This is the "pragmatic exit" phase.
*   **Initial Phase (first few rounds before `m` is met):** ATT-CTA will cooperate to test the group's tendency. If `m` is not met, it will switch to defecting in subsequent rounds (Rule 2.2) until there's a sign of renewed collective effort.
*   **Stagnation:** The `m_adaptive` mechanism helps prevent getting stuck in a perpetual defection cycle if the group is hovering below `m` but showing slight improvement.
*   **Persistent Defectors:** Rule 2.4 addresses specific players who consistently defect, preventing them from unfairly benefiting from others' cooperation. This requires a memory of individual opponent actions.

### 3. Cooperative Mindset Alignment

ATT-CTA is fundamentally designed to promote cooperation when it is mutually beneficial and sustainable.

*   **Initiates Cooperation:** It starts by cooperating, signaling a desire for collective success.
*   **Rewards Success:** It continues to cooperate when the `m` threshold is met, directly reinforcing successful collective action.
*   **Deters Free-riding:** It punishes failure to meet the threshold by defecting, discouraging players from expecting rewards without contributing.
*   **Adaptive Forgiveness:** The `m_adaptive` mechanism and the potential to re-engage after periods of defection demonstrate a willingness to give the group another chance, rather than resorting to permanent punishment. This acknowledges that mistakes happen and collective behavior can shift.
*   **Parameter Awareness:** The strategy is explicitly designed around the game's core parameters (`m`, `k`), ensuring its decisions are grounded in the mechanics of the game itself. It understands that cooperation is only rewarded when the collective action crosses a specific threshold (`m`), and that this reward (`k`) makes cooperation potentially more profitable than defection.

### Pseudocode Representation

```python
# Game Parameters
n = ...  # Number of players
r = ...  # Number of rounds
m = ...  # Minimum cooperators needed
k = ...  # Reward factor

# Internal State Variables
history = []  # List of tuples: (round_num, my_action, list_of_opponent_actions)
opponent_defection_streaks = {} # Dictionary: opponent_index -> streak_length
m_adaptive = m
cooperation_trend_memory = [] # To track recent cooperation counts

# Constants
PUNISHMENT_STREAK_THRESHOLD = 3 # Example: defect if opponent defects for 3 rounds
FORGIVENESS_COOPERATOR_THRESHOLD_DIFFERENCE = 1 # Example: re-engage if cooperation count increases by 1

def decide_action(current_round, n, m, k, history, opponent_defection_streaks, m_adaptive, cooperation_trend_memory):
    # Rule 1: First Round
    if current_round == 1:
        return 'C'

    # Rule 3: Last Round
    if current_round == r:
        return 'D'

    # --- Analyze History ---
    previous_round_num, my_prev_action, prev_opponent_actions = history[-1]
    cooperators_prev_round = sum(1 for action in prev_opponent_actions if action == 'C')
    
    # Update cooperation trend memory
    cooperation_trend_memory.append(cooperators_prev_round)
    if len(cooperation_trend_memory) > 5: # Keep a limited history for trend analysis
        cooperation_trend_memory.pop(0)

    # Update opponent defection streaks
    for i in range(n - 1): # Assuming opponent_actions are ordered
        if prev_opponent_actions[i] == 'D':
            opponent_defection_streaks[i] = opponent_defection_streaks.get(i, 0) + 1
        else:
            opponent_defection_streaks[i] = 0

    # --- Decision Logic ---

    # Rule 2.1: Sufficient Cooperation in Previous Round
    if cooperators_prev_round >= m:
        return 'C'

    # Rule 2.2: Insufficient Cooperation and I cooperated last round
    if cooperators_prev_round < m and my_prev_action == 'C':
        return 'D'

    # Rule 2.3: Insufficient Cooperation and I defected last round (or re-engagement attempt)
    if cooperators_prev_round < m and my_prev_action == 'D':
        # Adaptive forgiveness logic
        if len(cooperation_trend_memory) >= 2:
            current_coop_count = cooperation_trend_memory[-1]
            prev_coop_count = cooperation_trend_memory[-2]
            
            # Increase m_adaptive if trend is negative or stagnant
            if current_coop_count <= prev_coop_count:
                m_adaptive = min(m, m_adaptive + 1)
            # Decrease m_adaptive if trend is positive
            elif current_coop_count > prev_coop_count:
                m_adaptive = max(1, m_adaptive - 1)
        else: # Not enough history for trend, revert to base m
            m_adaptive = m

        if cooperators_prev_round >= m_adaptive:
            return 'C' # Re-engage if adaptive threshold is met
        else:
            return 'D' # Continue defecting if not

    # Rule 2.4: Persistent Individual Defection (apply this *after* general rules)
    for i in range(n - 1):
        if opponent_defection_streaks.get(i, 0) >= PUNISHMENT_STREAK_THRESHOLD:
            # In a real implementation, this would need to target a specific opponent's action
            # For simplicity here, we assume if any opponent triggers this, we defect.
            # A more refined strategy would only defect against that specific opponent.
            # For now, we'll generalize to a group defection if any individual is too persistent.
            # This needs careful consideration for tournament implementation.
            # For THIS turn, we will defect.
            return 'D'

    # Default to Defect if no other condition met (should be rare)
    return 'D'

# --- Tournament Loop Example ---
# for t in range(1, r + 1):
#     my_decision = decide_action(t, n, m, k, history, opponent_defection_streaks, m_adaptive, cooperation_trend_memory)
#     # ... play my_decision, get opponent_actions, update history, opponent_defection_streaks, etc. ...
```
'''

description_COOPERATIVE_110 = '''
Here's a cooperative strategy for the Collective Risk Dilemma, designed to be adaptive and robust in a tournament setting.

## Cooperative Strategy: "Conditional Generosity with Vigilance"

This strategy is rooted in the principle of aiming for the collective good (cooperation) while maintaining a degree of self-preservation and an ability to adapt to observed outcomes. It's about being generous when the conditions are favorable for cooperation, but also vigilant to ensure the collective benefit is being realized.

### Core Philosophy:

*   **Aim for the Cooperative Outcome:** The highest collective payoff occurs when everyone cooperates. This strategy will strive to reach and maintain this state.
*   **Reward Cooperation:** Acknowledge and reciprocate cooperative behavior observed in the past.
*   **Punish Defection (with Caution):** If the collective goal is not being met due to a lack of cooperation, the strategy will temporarily defect to protect its own endowment, but it will remain open to returning to cooperation if the environment shifts.
*   **Adapt to Dynamics:** The strategy is not static. It learns from the history of play and adjusts its behavior accordingly.

### Strategy Components:

1.  **Initial State: Cautious Cooperation**
2.  **Decision Rules: Dynamic Adjustment**
3.  **Handling Edge Cases: Round-Specific Logic**
4.  **Adaptive Learning: Tracking Collective Performance**

---

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect is primarily driven by the **historical success of cooperation** in previous rounds. We will track two key metrics:

*   **`cooperation_rate_past_rounds`**: The average proportion of players who cooperated in the *previous* `lookback_window` rounds.
*   **`current_round_cooperation`**: The actual number of players who cooperated in the *immediately preceding* round.

**Decision Logic:**

*   **Cooperate (C) if:**
    *   The `cooperation_rate_past_rounds` is **above a certain threshold (e.g., 0.8)**. This indicates a generally cooperative environment where the collective benefit is likely to be achieved.
    *   **OR** if the `current_round_cooperation` was **at least `m`** in the previous round. This signifies that the community successfully met the threshold in the last round, and we want to reinforce that positive outcome.
    *   **OR** if this is the **first round (t=1)**, as we start with a cooperative intent.

*   **Defect (D) if:**
    *   The `cooperation_rate_past_rounds` is **below a certain threshold (e.g., 0.5)**. This suggests that cooperation is consistently failing to yield the collective reward, and it's safer to secure our own endowment.
    *   **OR** if the `current_round_cooperation` was **less than `m`** in the previous round, AND the `cooperation_rate_past_rounds` is also **below `m/n`** (indicating a widespread failure to meet the minimum). This is a strong signal that the collective project is not viable in the current climate, and we should prioritize our own gain.
    *   **OR** if the `cooperation_rate_past_rounds` is **below `m/n`** and we have observed **two consecutive rounds where the cooperation threshold was not met**. This signifies a persistent failure and a need to step back from contributing.

*   **Maintain Previous Action if:**
    *   The conditions for explicit C or D are not met, and the previous round's outcome was neutral (i.e., the `cooperation_rate_past_rounds` is between the thresholds or the `current_round_cooperation` was close to `m`). In this case, we stick with our last action to observe for further trends.

**Thresholds and Lookback Window:**

*   `lookback_window`: A parameter to determine how many past rounds influence the decision (e.g., 3-5 rounds). A smaller window makes the strategy more reactive, while a larger one makes it more stable.
*   `cooperation_threshold_high`: A high threshold for confidence in continued cooperation (e.g., 0.8).
*   `cooperation_threshold_low`: A low threshold for initiating defection (e.g., 0.5).

---

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** Always **Cooperate (C)**.
    *   **Rationale:** We begin with an optimistic, cooperative intent. The game is new, and we want to set a positive example. We have no historical data to guide us otherwise.

*   **Last Round (t=r):**
    *   **Decision:** Always **Defect (D)**.
    *   **Rationale:** In the final round, there are no future consequences for our actions. To maximize our payoff in this isolated round, the rational choice is to keep our endowment. This is a necessary pragmatic adjustment for the very end of the game.

*   **Rounds Just Before the Last (t=r-1):**
    *   **Decision:** Follow the standard decision rules.
    *   **Rationale:** While the end is near, there is still one more round to influence. Our decisions should still be aimed at potentially achieving the cooperative outcome if possible, but with awareness that the next round will be the last.

*   **Initial Rounds (First `lookback_window` rounds):**
    *   In these rounds, the `cooperation_rate_past_rounds` might be based on incomplete data or only the first round. The strategy should adapt. If `m` is not met in the early rounds, the strategy will lean towards defecting in subsequent early rounds until a pattern emerges.

---

### 3. Being Cooperative

This strategy is inherently cooperative for the following reasons:

*   **Default to Cooperation:** The strategy starts by cooperating and will continue to cooperate as long as the observed environment is conducive to achieving the collective benefit (i.e., the cooperation threshold is likely to be met).
*   **Reinforcing Success:** When the group successfully cooperates (threshold met), the strategy reciprocates by continuing to cooperate, aiming to stabilize this beneficial outcome.
*   **Conditional Punishment:** Defection is not arbitrary. It's a *response* to a sustained lack of collective success. This is a form of conditional cooperation – "I will cooperate if you do, but if the collective project is failing, I must protect myself." This is still a cooperative *principle* because it aims to achieve the highest collective good when possible. It's a way to avoid being exploited and to signal that the current path is not working.
*   **Adaptive Nature:** The strategy is designed to *find* and *maintain* cooperation, not to simply grind out individual wins through constant defection. It's trying to build a stable cooperative equilibrium.

---

### Pseudocode Representation:

```pseudocode
// Game Parameters (provided externally)
n: integer (number of players)
r: integer (number of rounds)
m: integer (minimum cooperators needed)
k: float (reward factor)

// Strategy Parameters
lookback_window = 4       // Number of past rounds to consider for cooperation rate
cooperation_threshold_high = 0.8 // Threshold for strong belief in cooperation
cooperation_threshold_low = 0.5  // Threshold for considering defection

// State Variables (maintained per player)
history = []              // List of past actions of all players (list of lists, e.g., [[p1_act_t1, p2_act_t1,...], [p1_act_t2,...]])
my_actions_history = []   // List of my own past actions
observed_cooperators_per_round = [] // List of number of cooperators in each past round

function decide_action(current_round_number):
    // Handle Edge Cases
    if current_round_number == 1:
        return C
    if current_round_number == r:
        return D

    // --- Calculate State Metrics ---
    num_past_rounds = len(observed_cooperators_per_round)
    recent_cooperation_history = observed_cooperators_per_round[max(0, num_past_rounds - lookback_window):]

    // Average cooperation rate over the lookback window
    if not recent_cooperation_history:
        cooperation_rate_past_rounds = 0.0
    else:
        total_cooperators = sum(recent_cooperation_history)
        total_player_observations = len(recent_cooperation_history) * n
        cooperation_rate_past_rounds = total_cooperators / total_player_observations

    // Cooperation in the immediate previous round
    current_round_cooperation = 0
    if num_past_rounds > 0:
        current_round_cooperation = observed_cooperators_per_round[-1]

    // --- Decision Logic ---
    // 1. Strong inclination to cooperate
    if cooperation_rate_past_rounds >= cooperation_threshold_high:
        return C

    // 2. Reinforce past success
    if num_past_rounds > 0 and observed_cooperators_per_round[-1] >= m:
        return C

    // 3. Lean towards defection if cooperation is consistently failing
    if cooperation_rate_past_rounds < cooperation_threshold_low:
        // Check for persistent failure to meet threshold
        consecutive_failures = 0
        for i in range(min(3, num_past_rounds), 0, -1): // Check last 3 rounds
            if observed_cooperators_per_round[-i] < m:
                consecutive_failures += 1
        
        if consecutive_failures >= 2: // Two or more recent failures to meet m
            return D

    // 4. If cooperation failed last round and general rate is low
    if num_past_rounds > 0 and observed_cooperators_per_round[-1] < m and cooperation_rate_past_rounds < (m/n):
        return D

    // 5. If no clear signal, maintain previous action (or default to C if first few rounds)
    if num_past_rounds > 0:
        return my_actions_history[-1] // Maintain previous action
    else:
        return C // Default to C if no history yet (but not round 1)

// --- Update State After Each Round ---
function update_state(my_action, all_players_actions_this_round):
    my_actions_history.append(my_action)
    history.append(all_players_actions_this_round)

    // Count cooperators in the just completed round
    cooperators_this_round = sum([1 for action in all_players_actions_this_round if action == C])
    observed_cooperators_per_round.append(cooperators_this_round)

```

This strategy aims to be a balanced player that actively seeks the cooperative outcome while being prepared to protect itself and adapt when cooperation demonstrably fails to benefit the group. Its success will depend on how other players behave and the specific parameters of the game.
'''

description_COOPERATIVE_111 = '''
Okay, this is a classic social dilemma with a twist of repeated interaction. The core challenge is overcoming the incentive to defect when others might be cooperating, and the risk of everyone defecting if no one trusts anyone to cooperate.

Here's a cooperative strategy designed for this Collective Risk Dilemma, aiming for robustness and adaptiveness:

## Strategy: "Conditional Cooperate with Escalating Trust"

This strategy aims to initiate cooperation and maintain it as long as the group's collective behavior warrants it. It builds trust by observing the success of cooperation and punishes defection by temporarily reverting to defection itself, with an eye towards re-establishing cooperation if possible.

### 1. Decision Rules

The fundamental decision rule for player `i` in round `t` hinges on two factors:

*   **Previous Round's Success:** Did the group collectively meet the cooperation threshold (`m`) in the previous round?
*   **Own Previous Action:** Did player `i` cooperate or defect in the previous round?

Let's define the state for the previous round (round `t-1`):

*   `M_cooperated_t_minus_1`: A boolean indicating whether `m` or more players cooperated in round `t-1`.
*   `my_action_t_minus_1`: The action taken by player `i` in round `t-1` (C or D).

**Decision Logic for Player `i` in Round `t`:**

*   **If `t == 1` (First Round):**
    *   **Cooperate (C)**. This is the proactive cooperative stance. We aim to seed cooperation and see if others reciprocate.

*   **If `t > 1`:**
    *   **If `M_cooperated_t_minus_1` is TRUE (Group met the threshold last round):**
        *   **If `my_action_t_minus_1` was C (I cooperated last round):**
            *   **Cooperate (C)**. This is a direct reciprocity: "You cooperated, and we succeeded, so I'll cooperate again."
        *   **If `my_action_t_minus_1` was D (I defected last round):**
            *   **Cooperate (C)**. This is a forgiving step: "We succeeded even though I defected. I'll try to rejoin the cooperative effort to maintain success."

    *   **If `M_cooperated_t_minus_1` is FALSE (Group did NOT meet the threshold last round):**
        *   **If `my_action_t_minus_1` was C (I cooperated last round):**
            *   **Defect (D)**. This is a punishment for failed cooperation. "I tried to cooperate, but it wasn't enough. I'll defect to protect myself until the group proves it can cooperate."
        *   **If `my_action_t_minus_1` was D (I defected last round):**
            *   **Defect (D)**. This is a continuation of defection. "We failed to cooperate, and I was already defecting. I'll continue to defect."

**Escalation of Trust (A Refinement for Non-Cooperative Groups):**

The above logic can get stuck in a cycle of mutual defection if the group consistently fails to meet the threshold. To address this, we introduce an "escalation of trust" mechanism. This is a controlled attempt to re-initiate cooperation after a period of observing widespread defection.

*   **Introduce a Counter: `consecutive_defections_by_others`**
    *   This counter tracks how many consecutive rounds *other players* have defected, **regardless of my own action**.
    *   When `M_cooperated_t_minus_1` is FALSE and I defected (`my_action_t_minus_1 == D`), and `my_action_t_minus_1` was also D in the previous round, increment `consecutive_defections_by_others`.
    *   If `M_cooperated_t_minus_1` is TRUE, reset `consecutive_defections_by_others` to 0.

*   **Modified Decision Logic for `M_cooperated_t_minus_1` is FALSE:**

    *   **If `my_action_t_minus_1` was C (I cooperated last round):**
        *   **Defect (D)**. (As before)

    *   **If `my_action_t_minus_1` was D (I defected last round):**
        *   **If `consecutive_defections_by_others` < `threshold_escalation` (e.g., 3 rounds):**
            *   **Defect (D)**. Continue defection.
        *   **Else (`consecutive_defections_by_others` >= `threshold_escalation`):**
            *   **Cooperate (C)**. Attempt to break the cycle of defection. This is a strategic gamble to re-introduce cooperation. After `threshold_escalation` rounds of failed collective cooperation where others have also defected, it's worth trying to lead by example again.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):** Always Cooperate (C). This is the initiation phase.
*   **Last Round (`t = r`):** The strategy remains the same. In the absence of communication or future-oriented punishments (like an infinite horizon), the last round is often a focal point for defection. However, this strategy's logic naturally handles it. If previous rounds were cooperative, it will cooperate. If they were not, it will defect. The "escalation" mechanism will have already kicked in if the situation was dire.
*   **Parameter Extremes:**
    *   **`m` close to `n`:** Cooperation becomes harder to achieve. The strategy will likely default to defection more often unless a strong majority consistently cooperates.
    *   **`m` close to 2:** Cooperation is easier. The strategy will be more inclined to cooperate and punish less severely if a single round fails.

### 3. Being Cooperative

This strategy is fundamentally cooperative in its design:

*   **Initiates Cooperation:** It always starts by offering cooperation in the first round.
*   **Reciprocates Cooperation:** It rewards successful cooperation with continued cooperation.
*   **Forgives Temporary Defection:** If the group succeeds even with individual defection, it attempts to rejoin the cooperative effort.
*   **Deters Defection:** It punishes individual defection when group cooperation fails, signaling that cooperation is a shared responsibility.
*   **Attempts to Re-establish Cooperation:** The "escalation of trust" mechanism is a crucial cooperative element, preventing the game from getting stuck in a mutually destructive cycle. It's a willingness to take a risk for the collective good after a period of sustained failure.

### Pseudocode Representation

```
// Global state variables
history = [] // List of tuples: (round_num, my_action, num_cooperators, all_actions_in_round)
my_action_t_minus_1 = 'D' // Default for initial state before round 1
M_cooperated_t_minus_1 = False // Default for initial state
consecutive_defections_by_others = 0
threshold_escalation = 3 // Example value, can be a parameter

function decide_action(round_num, n, m, k, history_opponent_actions):
    // history_opponent_actions: list of actions for all players in previous round

    if round_num == 1:
        return 'C'

    // Determine state from previous round's observations
    num_cooperators_t_minus_1 = count_cooperators(history_opponent_actions) // Needs implementation
    M_cooperated_t_minus_1 = (num_cooperators_t_minus_1 >= m)

    if M_cooperated_t_minus_1:
        // Group succeeded in previous round
        consecutive_defections_by_others = 0 // Reset counter as success implies no sustained group defection
        if my_action_t_minus_1 == 'C':
            return 'C' // Reciprocate cooperation
        else: // my_action_t_minus_1 == 'D'
            return 'C' // Forgive and rejoin

    else:
        // Group failed in previous round
        if my_action_t_minus_1 == 'C':
            // I tried to cooperate, but it failed, so punish
            return 'D'
        else: // my_action_t_minus_1 == 'D'
            // I also defected, and group failed. Check for escalation.
            if not M_cooperated_t_minus_1 and my_action_t_minus_1 == 'D' and history_opponent_actions_of_others_were_D_in_t_minus_1: // Needs careful implementation to track others' defection
                consecutive_defections_by_others += 1
            else:
                consecutive_defections_by_others = 0 // Reset if conditions for increment aren't met (e.g., I cooperated last, or group succeeded, etc.)

            if consecutive_defections_by_others < threshold_escalation:
                return 'D' // Continue defection
            else:
                return 'C' // Attempt to break the cycle

// Helper function to count cooperators from a list of actions
function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == 'C':
            count += 1
    return count

// Update my_action_t_minus_1 after the decision is made and action is taken
// my_action_t_minus_1 = decided_action
```

**Note on `history_opponent_actions_of_others_were_D_in_t_minus_1`:** This pseudocode requires a more sophisticated tracking mechanism within the `history` variable. Specifically, in round `t-1`, if the group failed (`M_cooperated_t_minus_1` was FALSE) and player `i` defected (`my_action_t_minus_1` was 'D'), we need to check if *all* other `n-1` players also defected. If so, `consecutive_defections_by_others` is incremented. If even one other player cooperated, this counter should be reset. This adds a layer of robustness, only escalating trust when there's a complete breakdown of cooperation.

This strategy aims to be a stable, cooperative player that doesn't blindly trust but is willing to lead the charge for cooperation when conditions are right, and punish effectively when necessary, all while having a built-in mechanism to try and recover from sustained defection.
'''

description_COOPERATIVE_112 = '''
This strategy is designed to be a robust and adaptive cooperative approach to the Collective Risk Dilemma. It aims to encourage cooperation by rewarding successful collective efforts while mitigating the risk of individual exploitation. The core idea is to be conditionally cooperative, starting with an assumption of cooperation and adjusting behavior based on the observed outcomes.

### Strategy Name: Adaptive Cooperative Risk Averter (ACRA)

### Core Philosophy:

ACRA believes that the greatest collective payoff is achieved through sustained cooperation, but it acknowledges the inherent risk of being the sole cooperator. Therefore, it will prioritize achieving the cooperation threshold when feasible, but will defect to protect its own endowment if cooperation seems unlikely or has repeatedly failed.

### 1. Decision Rules:

ACRA's decision in any given round $t$ is based on the *perceived probability* of the cooperation threshold ($m$) being met. This probability is dynamically estimated from the history of previous rounds.

**Key Indicators for Decision Making:**

*   **Historical Cooperation Rate:** The proportion of rounds in the past where the cooperation threshold ($m$) was met.
*   **Recent Cooperation Trend:** Whether the number of cooperators has been increasing or decreasing in recent rounds.
*   **Number of Cooperators in the Previous Round ($C_{t-1}$):** The absolute count of players who cooperated in the immediately preceding round.

**Decision Logic:**

*   **Default Behavior (Early Rounds / Uncertainty):** In the absence of strong historical data (e.g., the first few rounds), ACRA will lean towards **Cooperation (C)**. This is because the potential reward ($k$) is significant, and defecting too early can lead to a "race to the bottom" where everyone defects and the collective payoff is suboptimal.

*   **Conditional Cooperation (Mid-Game / Established History):**
    *   **If $C_{t-1} \ge m$ (Threshold Met Last Round):** The players successfully cooperated. ACRA will **Cooperate (C)**. This reinforces successful cooperative behavior and signals trust.
    *   **If $C_{t-1} < m$ and $C_{t-1}$ is close to $m$ (e.g., $m-2 \le C_{t-1} < m$):** Cooperation was almost successful. ACRA will assess the trend.
        *   If the number of cooperators has been generally **increasing** in recent rounds, ACRA will **Cooperate (C)**, taking a calculated risk to push towards the threshold.
        *   If the number of cooperators has been generally **decreasing** or stagnant, ACRA will consider **Defecting (D)**. The exact threshold for this decision can be a parameter, but a simple rule is: if the current number of cooperators is less than $m$ and has not shown a clear upward trend, defecting becomes more attractive to preserve individual gains.
    *   **If $C_{t-1}$ is significantly less than $m$ (e.g., $C_{t-1} < m-2$):** Cooperation is unlikely to be achieved, and the risk of defecting alone is high. ACRA will **Defect (D)**.

*   **The "Last Chance" Cooperation (Rounds Close to the End):** In the very late stages of the game (e.g., the last few rounds), the incentive to defect to secure a small gain might increase. However, ACRA wants to avoid being the sole reason for failure. Therefore, ACRA will be *more inclined to cooperate* in these late rounds, *even if the previous round's cooperation was just below the threshold*, if it believes its cooperation could *tip the scales*. This is a strategic attempt to secure the high reward in the final rounds.

### 2. Handling Edge Cases:

*   **First Round ($t=1$):** ACRA will **Cooperate (C)**. This is the default behavior to initiate a cooperative venture and test the waters. It assumes that other players might also attempt cooperation initially.

*   **Last Round ($t=r$):** The decision in the last round is crucial.
    *   ACRA will calculate the number of cooperators in round $r-1$, let's call it $C_{r-1}$.
    *   **If $C_{r-1} \ge m$**: ACRA will **Cooperate (C)**. The previous round was successful, so continuing cooperation is the best way to maximize payoff.
    *   **If $C_{r-1} < m$**: ACRA will consider the *potential payoff*.
        *   If ACRA *defects* in the last round and the threshold is not met, its payoff is 1. If it *cooperates* and the threshold is *still* not met (due to others defecting), its payoff is $1-1=0$. This is a significant risk.
        *   However, if ACRA *cooperates* and its cooperation, combined with others, *just enough* to reach $m$, its payoff is $1-1+k=k$. This is a very high payoff.
        *   Therefore, in the last round when $C_{r-1} < m$, ACRA will **Cooperate (C)** if there's a *plausible chance* that its cooperation, *plus the expected actions of others*, could reach $m$. This is a riskier but potentially highly rewarding move. A simple heuristic: if $C_{r-1} \ge m-1$, ACRA will cooperate in the last round. If $C_{r-1} < m-1$, ACRA will defect to guarantee a payoff of 1.

*   **Insufficient Data (Early Rounds):** For the first few rounds (e.g., $t < \min(3, r)$), ACRA will always **Cooperate (C)**. This is to establish a baseline of cooperative intent.

*   **"Stuck" in Defection:** If ACRA has been defecting for a prolonged period because the cooperation threshold has consistently failed to be met, it will not immediately switch back to cooperation. It will require evidence of a *sustained increase* in cooperation from others before reconsidering its cooperative stance. This prevents it from being exploited by a single "false signal" of cooperation.

### 3. Cooperativeness Alignment:

*   **Pro-Cooperative Default:** ACRA starts by assuming cooperation is the best path and actively seeks to make it work.
*   **Reward-Seeking:** It aims to maximize the collective reward ($k$) by trying to reach the threshold $m$.
*   **Conditional Trust:** Its cooperation is conditional on the observable behavior of others. It trusts past successful cooperation and is willing to take calculated risks to achieve it again.
*   **Resilience to Exploitation:** By defecting when cooperation consistently fails or seems impossible, it protects itself and ensures it doesn't suffer significant losses due to the exploitation of others. This self-preservation is a necessary component for sustained participation and, ultimately, for the possibility of future cooperation.
*   **No Retaliation (Strictly):** ACRA does not aim to punish defectors. Its defection is purely a strategic decision to preserve its own endowment when cooperation is improbable. It is not driven by a desire to "teach a lesson" but by the logic of expected payoff.

### Pseudocode:

```
Strategy ACRA:

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor
  cooperate_threshold_for_defect_decision: A value (e.g., m-2) below which ACRA strongly considers defecting.
  trend_window: Number of recent rounds to consider for trend analysis (e.g., 3)
  last_chance_cooperation_threshold: A value (e.g., m-1) for cooperation in the last round.

State:
  round_history: List of tuples, each tuple: (round_num, player_actions_dict, num_cooperators)
    player_actions_dict: {player_id: action (C/D)}

Function DecideAction(current_round, round_history):

  // Edge Case: First Round
  if current_round == 1:
    return C

  // Edge Case: Very Early Rounds with Insufficient History
  if current_round <= 3: // Or some other small number, ensuring at least trend_window is available for trend calc later
    return C

  // --- Calculate key indicators from history ---
  num_cooperators_prev_round = 0
  if len(round_history) > 0:
    num_cooperators_prev_round = round_history[-1][2] // Assuming history is ordered

  // Trend Analysis (Simplified)
  recent_cooperator_counts = [history_entry[2] for history_entry in round_history[-trend_window:]]
  increasing_trend = False
  decreasing_trend = False
  if len(recent_cooperator_counts) >= 2:
    if recent_cooperator_counts[-1] > recent_cooperator_counts[-2]:
      increasing_trend = True
    elif recent_cooperator_counts[-1] < recent_cooperator_counts[-2]:
      decreasing_trend = True
    // More sophisticated trend analysis could be implemented here

  // Edge Case: Last Round
  if current_round == r:
    if num_cooperators_prev_round >= m:
      return C
    else: // Threshold not met in previous round
      if num_cooperators_prev_round >= last_chance_cooperation_threshold:
        // Plausible chance to reach threshold with my cooperation in the final round
        return C
      else:
        // Very unlikely to reach threshold, guarantee individual payoff
        return D

  // --- General Round Decision ---

  // If threshold was met last round, cooperate
  if num_cooperators_prev_round >= m:
    return C

  // If threshold was almost met last round, consider trend
  if num_cooperators_prev_round >= cooperate_threshold_for_defect_decision:
    if increasing_trend:
      return C // Take a calculated risk
    else:
      return D // Trend is not favorable, protect endowment

  // If threshold was far from met last round, defect
  return D

// --- How to add to round_history ---
// After each round, collect actions of all players.
// Count cooperators.
// Append (current_round, {player_i: action_i}, num_cooperators) to round_history.
```

**Explanation of Pseudocode Enhancements:**

*   **`cooperate_threshold_for_defect_decision`:** This parameter allows tuning the sensitivity to being "almost there." A value of `m-2` means if 3 players cooperate and `m` is 5, ACRA will still consider cooperating if the trend is up.
*   **`trend_window`:** Makes the trend analysis more robust by looking at a few recent rounds, not just the single previous round.
*   **`last_chance_cooperation_threshold`:** Explicitly defines the condition for the risky cooperation in the final round.

This strategy is designed to be adaptable by learning from past outcomes while maintaining a clear objective of maximizing collective and individual payoffs in the long run. It balances the desire for cooperation with the necessity of self-preservation in a setting with no communication.
'''

description_COOPERATIVE_113 = '''
This is a fascinating game that blends elements of the Prisoner's Dilemma with a public goods game, all within a repeated interaction framework. The core challenge is to incentivize cooperation when individual defection can be tempting, especially if others cooperate. The "risk aversion" aspect is key here: the reward only materializes if enough people take the "risk" of cooperating.

Here's a cooperative strategy designed to be adaptive and robust, focusing on building trust and responding to observed behavior.

## Cooperative Strategy: "Adaptive Threshold Responder"

This strategy aims to be a strong cooperator, but also pragmatic enough to avoid being exploited. It's built on the principle of **reciprocity with a safety net**.

### Core Philosophy:

*   **Default to Cooperation:** In the absence of strong negative signals, the default action is to cooperate. This signals a cooperative intent and helps to reach the cooperation threshold.
*   **Conditional Cooperation:** Cooperation is contingent on observing sufficient cooperation from others in previous rounds. If the group consistently fails to meet the threshold, the strategy will adjust its cooperation level to reflect that reality.
*   **Forgiveness with Caution:** The strategy will forgive occasional defections but will punish persistent defection by reducing its own cooperation.
*   **Awareness of Threshold:** The strategy is acutely aware of the `m` parameter and will actively try to contribute to reaching it, but not at the expense of consistently earning less than it could.

### 1. Decision Rules:

The decision to cooperate or defect in any given round `t` (where `t > 1`) is based on a **cooperation rate observed in the previous round (`t-1`)** and a **"trust level" variable**.

Let's define:

*   `observed_cooperation_rate(t-1)`: The proportion of players who cooperated in round `t-1`.
    *   `observed_cooperation_rate(t-1) = (Number of players who played C in round t-1) / n`
*   `trust_level`: A dynamic variable that starts high and can decrease if the group is not cooperating sufficiently. This represents our "patience" with the group's behavior.

**Decision Logic:**

*   **If `t == 1` (First Round):**
    *   **Cooperate (C)**. The initial assumption is that others will also want to cooperate to unlock the reward. This is a bold cooperative move to initiate the process.

*   **If `t > 1`:**
    *   **Calculate `observed_cooperation_rate(t-1)`**.
    *   **Update `trust_level`:**
        *   If `observed_cooperation_rate(t-1) >= m/n` (the group met the threshold in the previous round):
            *   `trust_level = trust_level + 0.1` (Increase trust, but cautiously. Capped at 1.0)
        *   Else if `observed_cooperation_rate(t-1) < (m-1)/n` (the group failed significantly, missing by more than 1 player):
            *   `trust_level = trust_level - 0.2` (Decrease trust significantly. Floored at 0.0)
        *   Else (the group was close but didn't meet the threshold, or met it by a small margin):
            *   `trust_level` remains unchanged.

    *   **Determine Action:**
        *   **If `trust_level >= 0.5` (High trust):**
            *   **Cooperate (C)**. We are willing to extend cooperation as long as there's a reasonable level of trust that the group can achieve the threshold, or at least is not consistently failing by a large margin.
        *   **If `trust_level < 0.5` (Low trust):**
            *   **If `observed_cooperation_rate(t-1) >= m/n`:**
                *   **Cooperate (C)**. Even with lower trust, if the group *did* meet the threshold last round, we'll still cooperate to maintain the positive outcome.
            *   **Else (`observed_cooperation_rate(t-1) < m/n`):**
                *   **Defect (D)**. If trust is low and the group *failed* to meet the threshold, it's time to protect our endowment and signal that the current cooperative dynamic isn't working. We will defect to get our guaranteed payoff of 1, rather than risk getting 1 (if others defect) or 2 (if others cooperate but we defect).

### 2. Handling Edge Cases:

*   **First Round (`t=1`):** As stated above, always **Cooperate (C)**. This is crucial to signal cooperative intent and to give the group a chance to establish a cooperative pattern.
*   **Last Round (`t=r`):**
    *   **If `observed_cooperation_rate(r-1) >= m/n`:**
        *   **Cooperate (C)**. Even in the last round, if the group has successfully achieved cooperation in the previous round, it's beneficial to secure the `k` reward. This final push might encourage others to cooperate for one last payout.
    *   **Else (`observed_cooperation_rate(r-1) < m/n`):**
        *   **Defect (D)**. In the final round, there's no future to build trust for. If the group hasn't been cooperating, the best outcome for this strategy is to secure its private endowment of 1. There's no incentive to cooperate if the threshold won't be met, and defecting is the guaranteed payoff.

*   **`m=n` (Threshold is everyone):** The strategy will naturally become very cautious. It will only cooperate if it observes `n` cooperators in the previous round (which is impossible unless `n=1`, but `n>=2`). Therefore, if `m=n`, the strategy will likely defect from round 2 onwards, unless it's the very first round. This reflects the extreme difficulty of coordination. *Self-correction: The condition `observed_cooperation_rate(t-1) >= m/n` will only be met if `m` players cooperate. If `m=n`, this means `n` players must cooperate. The strategy will only cooperate if `trust_level` is high, which it will only become if `observed_cooperation_rate` was >= `m/n`. This creates a bootstrapping problem. A more robust rule for `m=n` is needed.*

    *   **Revised `m=n` handling:** If `m=n`, the strategy will always **Defect (D)** from round 2 onwards. The probability of `n` players cooperating simultaneously without communication is exceedingly low, making the reward virtually unattainable for a cautious strategy. The initial cooperation in round 1 is still a signal, but the immediate failure will lead to defection.

*   **`m=2` and `n` is large:** This is a scenario where cooperation is easier to achieve. The strategy will be more inclined to cooperate. The `trust_level` will increase quickly if cooperation is successful.

### 3. Being Cooperative:

*   **Default to Cooperation:** The strategy starts with cooperation and only deviates when its observations suggest that cooperation is not yielding the desired collective outcome.
*   **Proactive Trust Building:** In the first round, it signals its cooperative intent by immediately cooperating.
*   **Rewarding Cooperation:** The `trust_level` is designed to increase when the group successfully cooperates, making future cooperation more likely.
*   **Threshold Focus:** The strategy's decisions are fundamentally aligned with the goal of reaching the `m` cooperation threshold. It attempts to contribute to this goal as long as it's not systematically disadvantaged by it.

### Pseudocode:

```
// Global variables
trust_level = 0.7  // Start with a reasonably high trust level
my_player_index = i // Assuming this is known

function decide_action(round_number, previous_round_actions, n, m, k):
    // previous_round_actions is a list/array of actions taken by all players in round_number - 1

    // Handle first round
    if round_number == 1:
        return "C"

    // Calculate observed cooperation rate from previous round
    cooperators_last_round = 0
    for action in previous_round_actions:
        if action == "C":
            cooperators_last_round += 1
    observed_cooperation_rate = cooperators_last_round / n

    // Update trust level
    if observed_cooperation_rate >= m / n:
        trust_level = min(1.0, trust_level + 0.1) // Increase trust, cap at 1.0
    elif observed_cooperation_rate < (m - 1) / n and m > 2: // Avoid severe punishment for missing by just one if m is very small
        trust_level = max(0.0, trust_level - 0.2) // Decrease trust, floor at 0.0
    // else: trust_level remains the same

    // Handle special case: m = n
    if m == n:
        return "D" // Too difficult to coordinate n cooperators

    // Determine action for subsequent rounds
    if round_number == r: // Last round
        if observed_cooperation_rate >= m / n:
            return "C" // Final push if successful
        else:
            return "D" // Protect endowment if not successful
    else: // Rounds between 1 and r (exclusive)
        if trust_level >= 0.5:
            return "C"
        else: // Low trust
            if observed_cooperation_rate >= m / n:
                return "C" // Cooperate if threshold was met last round, despite low trust
            else:
                return "D" // Defect if threshold was not met and trust is low

```

### Robustness and Adaptability:

*   **Against Always Cooperators:** This strategy will initially cooperate and will continue to do so as long as the "Always Cooperators" ensure the threshold is met. If "Always Cooperators" are the *only* ones cooperating and `m` is not met, our strategy will eventually defect due to low trust.
*   **Against Always Defectors:** In the first round, we cooperate. If others always defect, `observed_cooperation_rate` will be 0. Our `trust_level` will plummet. From round 2 onwards, we will defect, ensuring we get our payoff of 1.
*   **Against Tit-for-Tat like strategies:** If another player uses Tit-for-Tat (cooperate if they cooperated last round, defect if they defected last round), our strategy will react. If we cooperate and they cooperate, `trust_level` might increase, leading to continued cooperation. If we cooperate and they defect, our `trust_level` will decrease, and we'll likely defect in the next round, leading to a stable state of mutual defection if this pattern continues.
*   **Against strategies that fluctuate:** The `trust_level` acts as a buffer. Occasional defections from others won't immediately cause us to defect. We need a sustained pattern of non-cooperation to trigger our defection. Conversely, sustained cooperation will reinforce our own cooperation.

This strategy prioritizes the potential for high collective gains while having a strong defense against being exploited. It attempts to be a "good citizen" in the game but reserves the right to protect its own interests when cooperation proves futile or consistently detrimental.
'''

description_COOPERATIVE_114 = '''
This is a fascinating challenge! Designing a purely cooperative strategy in a repeated dilemma with no communication requires a careful balance of signaling commitment to cooperation while also protecting against exploitation. The core challenge is to incentivize others to cooperate without being a "sucker."

Here's a strategy I'll call **"Conditional Reciprocity with Minimum Commitment" (CRMC)**, designed to be cooperative, adaptive, and robust.

## Strategy: Conditional Reciprocity with Minimum Commitment (CRMC)

The core idea of CRMC is to:

1.  **Signal commitment to cooperation:** By cooperating early, we attempt to establish a norm of cooperation.
2.  **Reciprocate observed behavior:** If others cooperate, we continue to cooperate. If they defect, we punish defection.
3.  **Maintain a safety net:** We ensure we can still achieve a baseline payoff even if cooperation breaks down.
4.  **Be flexible:** The strategy adapts to the observed dynamics of the game.

### 1. Decision Rules

The decision to cooperate (C) or defect (D) in round *t* will depend on the observed history of play up to round *t-1*.

**The CRMC Strategy Decision Rule:**

In round *t*, player *i* will play **Cooperate (C)** if **any** of the following conditions are met. Otherwise, player *i* will play **Defect (D)**.

*   **Condition 1: Cooperative Threshold Achieved (Majority Cooperation Observed)**
    *   In the previous round (t-1), the number of players who played Cooperate was **greater than or equal to *m***.
    *   *Rationale: This reinforces successful cooperation and signals that cooperation is the path to a better outcome for everyone.*

*   **Condition 2: Early Stage Cooperative Experimentation (First Few Rounds)**
    *   The current round *t* is within the first **`initial_cooperation_rounds`** rounds (explained below).
    *   *Rationale: In the early stages, we want to proactively try to establish cooperation. This provides a buffer against immediate defection and allows others to learn the benefits of cooperation without immediate punishment.*

*   **Condition 3: Near Threshold and High Individual Contribution**
    *   The number of players who cooperated in the previous round (t-1) was **greater than or equal to *m - 1*** (i.e., one less than the threshold).
    *   AND, **player *i* themselves cooperated in the previous round (t-1)**.
    *   *Rationale: This is a crucial "signal of intent" and a last-ditch effort to pull the group over the cooperation threshold. If we are already contributing to the community and only one more person is needed, we'll keep trying, hoping to inspire others. This also prevents "free-riding" on our cooperation.*

**Explanation of Parameters:**

*   **`initial_cooperation_rounds`**: A parameter to define how many initial rounds we will *always* try to cooperate, regardless of other players' actions. This should be a small number, perhaps around `min(5, r // 2)`. The exact value might be tuned, but the principle is to give cooperation a chance without being overly naive.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Always Cooperate (C).**
    *   *Rationale: This is the initial signal of cooperation. We have no history to go by, so we initiate the cooperative effort. This aligns with Condition 2 of the decision rules if `initial_cooperation_rounds >= 1`.*

*   **Last Round (t=r):**
    *   **Always Defect (D).**
    *   *Rationale: In the final round, there are no future consequences for defection. If others have consistently defected, we will defect to maximize our individual payoff. If others have cooperated, our cooperation would yield payoff 2 for us, but their defection would yield payoff 3 for them. To avoid being exploited, and since there's no future to rebuild trust, the optimal strategy in a one-shot game is to defect. This is a pragmatic adjustment for the end of the game, acknowledging the repeated nature's incentive structure is gone.*

*   **Empty History (e.g., before round 2 analysis):**
    *   When analyzing round *t*, we look at the history of round *t-1*. If *t-1* is the first round, the history is clear. For the first round itself, we have no history, so the "Always Cooperate" rule for the first round takes precedence.

*   **Edge Case: `m-1` is 0 or negative:**
    *   If `m-1 <= 0`, it means `m <= 1`. However, the parameter `m` is defined as `1 < m < n`. So, `m-1` will always be at least 1 (if `m=2`). This case won't occur due to parameter constraints.

*   **Edge Case: `m=n`:**
    *   If `m=n`, then `m-1 = n-1`. Condition 3 becomes: "The number of players who cooperated in the previous round was `n-1` AND I cooperated in the previous round." This means we need *everyone* to cooperate, and I was part of that group. This is still a valid, though stricter, condition to push for full cooperation.

### 3. Being Cooperative

CRMC is fundamentally cooperative in its design:

*   **Proactive Cooperation:** It starts by cooperating, attempting to build a cooperative environment.
*   **Positive Reciprocity:** It rewards cooperation by continuing to cooperate when the threshold is met. This encourages others to contribute to the collective good.
*   **Conditional Punishment:** Defection is met with defection, but only after observing a failure to meet the cooperative threshold. This is not punitive in a vindictive sense, but rather a rational response to protect against exploitation and re-incentivize cooperation.
*   **Commitment to Threshold:** It actively tries to reach the cooperative threshold, even when close, by continuing to cooperate if it was already contributing. This signals a strong desire for the mutually beneficial outcome.

### Pseudocode Representation

```python
def decide_action(player_id, history, game_params):
    n = game_params['n']
    r = game_params['r']
    m = game_params['m']
    k = game_params['k']

    current_round = len(history) + 1 # history is a list of past rounds' actions

    # --- Handle Edge Case: Last Round ---
    if current_round == r:
        return 'D' # Always defect in the last round

    # --- Handle Edge Case: First Round ---
    if current_round == 1:
        return 'C' # Always cooperate in the first round

    # --- Determine history for analysis (previous round) ---
    previous_round_actions = history[-1] # List of actions in the previous round
    num_cooperators_prev_round = sum(1 for action in previous_round_actions if action == 'C')
    my_action_prev_round = previous_round_actions[player_id] # Assuming player_id is index

    # --- Define initial cooperation phase ---
    # Roughly the first 10% of rounds, or up to 5 rounds, whichever is smaller.
    # Ensure it's at least 1 if r is very small.
    initial_cooperation_rounds = max(1, min(5, r // 10))

    # --- Apply Decision Rules ---

    # Condition 1: Majority Cooperation Achieved in Previous Round
    if num_cooperators_prev_round >= m:
        return 'C'

    # Condition 2: Early Stage Cooperative Experimentation
    if current_round <= initial_cooperation_rounds:
        return 'C'

    # Condition 3: Near Threshold and High Individual Contribution
    if num_cooperators_prev_round >= m - 1 and my_action_prev_round == 'C':
        return 'C'

    # Default: If none of the above conditions are met, defect.
    return 'D'

```

### Robustness and Adaptability Considerations

*   **Adaptability:** The strategy adapts based on the observed number of cooperators in the previous round. If cooperation is working (Condition 1), it continues. If it's close but needs a push (Condition 3), it provides that push. If it fails to reach the threshold and isn't in the initial phase, it punishes.
*   **Robustness to Opponent Behavior:**
    *   **Against Tit-for-Tat (or similar):** If opponents cooperate, the CRMC will continue to cooperate. If they defect, CRMC will eventually punish after observing sustained defection (specifically, after `m` cooperators are not achieved for two consecutive rounds if `m > 1`).
    *   **Against Always Defect:** CRMC will cooperate for the first `initial_cooperation_rounds`. After that, if all opponents always defect, `num_cooperators_prev_round` will always be 0. CRMC will then switch to defecting as none of the cooperation conditions will be met, thus avoiding being a perpetual "sucker."
    *   **Against Exploitation (e.g., Free-riders):** Condition 3 is key here. If a player defects when `m-1` have cooperated, CRMC will still cooperate in the next round *if it was one of the `m-1` cooperators*. This prevents a single defector from immediately breaking the cycle of cooperation if the group was very close. However, if the defection is sustained and the threshold is not met, CRMC will eventually defect.
    *   **Against Unknown Strategies:** The strategy doesn't assume any specific pattern from opponents, only the outcome (number of cooperators). This makes it generally applicable.

This strategy aims to be a good "generalized" cooperative player, trying to foster cooperation while having a clear and rational response to its breakdown. The `initial_cooperation_rounds` parameter offers a tunable aspect for how aggressively to push for cooperation early on.
'''

description_COOPERATIVE_115 = '''
Here's a cooperative strategy for the Collective Risk Dilemma game, designed to be adaptive and robust:

## Cooperative Strategy: "Adaptive Tit-for-Tat with Threshold Focus"

This strategy aims to foster cooperation by rewarding collective success and punishing individual defections, but with a keen eye on the crucial cooperation threshold 'm'. It adapts its behavior based on the observed success of the group in meeting the threshold in previous rounds.

### Core Philosophy:

*   **Be Cooperative:** Start with a cooperative stance, assuming others might too.
*   **Be Forgiving:** Give players a chance to return to cooperation after a defection.
*   **Be Punishing (but not vindictive):** Deter defections by reducing payoffs when cooperation fails.
*   **Be Clear and Responsive:** Adapt to the group's collective behavior, particularly regarding the 'm' threshold.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on the history of the game, specifically the group's success in meeting the cooperation threshold in the *immediately preceding round*.

Let `C_t-1` be the number of players who cooperated in round `t-1`.

*   **If `C_t-1 >= m` (Threshold Met in Previous Round):**
    *   **Decision for Player i in Round `t`:** Cooperate (C).
    *   **Rationale:** The group successfully achieved the cooperative outcome. This reinforces the benefits of cooperation and signals that continuing to cooperate is the optimal path. The reward `k` was realized, demonstrating the positive externality of collective action.

*   **If `C_t-1 < m` (Threshold Not Met in Previous Round):**
    *   **Decision for Player i in Round `t`:** This is where adaptation occurs. We need to assess *why* the threshold wasn't met.
        *   **If Player i defected in round `t-1` AND `C_t-1 < m`:**
            *   **Decision for Player i in Round `t`:** Defect (D).
            *   **Rationale:** If you defected and the group *still* failed to meet the threshold, your defection was detrimental. There's no incentive to cooperate immediately as the collective risk was not averted, and your private gain from defecting was minimal (or even negative if others cooperated and you defected). This is a form of punishment for contributing to the failure.
        *   **If Player i cooperated in round `t-1` AND `C_t-1 < m`:**
            *   **Decision for Player i in Round `t`:** Cooperate (C).
            *   **Rationale:** You did your part by cooperating, but the group as a whole still failed. This is a signal that others might have defected. By continuing to cooperate, you are attempting to "pull" the group towards cooperation in the next round. You are being "forgiving" of past failures, hoping that your continued cooperation will encourage others to reciprocate. This is a key element of the Tit-for-Tat principle adapted to a group context.

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   **Decision for Player i in Round 1:** Cooperate (C).
    *   **Rationale:** In the absence of any history, the most cooperative and trusting starting move is to cooperate. This aligns with the "be cooperative" principle and aims to set a positive tone for the game, hoping for reciprocal cooperation.

*   **Last Round (t=r):**
    *   **Decision for Player i in Round `r`:** Defect (D).
    *   **Rationale:** In the very last round, there are no future rounds to influence. The incentive to maintain cooperation for future rewards disappears. Therefore, the rational strategy is to maximize immediate personal payoff, which is achieved by defecting. This is a standard assumption in the economics of finitely repeated games (a finite horizon).

### 3. Being Cooperative:

The strategy is inherently cooperative in its approach:

*   **Initial Cooperation:** It begins by cooperating, signaling a willingness to engage in mutually beneficial outcomes.
*   **Reward for Success:** When the group collectively achieves the `m` threshold, it continues to cooperate, reinforcing the positive outcome and maximizing the group's reward.
*   **Forgiveness:** If the threshold isn't met, but the player themselves cooperated, they continue to cooperate, giving others a chance to learn and reciprocate. They don't retaliate immediately for the group's failure if they were personally cooperative.

### Pseudocode:

```
strategy_name = "Adaptive Tit-for-Tat with Threshold Focus"

function decide_action(player_id, round_num, history, parameters):
    n = parameters.n
    m = parameters.m
    r = parameters.r
    k = parameters.k

    # Handle the very first round
    if round_num == 1:
        return "C"

    # Handle the very last round
    if round_num == r:
        return "D"

    # Get history from the previous round (t-1)
    if round_num > 1:
        previous_round_actions = history.get(round_num - 1, {}) # Dictionary of player_id: action
        cooperators_in_previous_round = 0
        for player, action in previous_round_actions.items():
            if action == "C":
                cooperators_in_previous_round += 1

        my_action_in_previous_round = previous_round_actions.get(player_id)

        # Decision Rule 1: Threshold met in previous round
        if cooperators_in_previous_round >= m:
            return "C"
        # Decision Rule 2: Threshold NOT met in previous round
        else:
            # Sub-rule 2a: I defected and threshold still not met
            if my_action_in_previous_round == "D":
                return "D"
            # Sub-rule 2b: I cooperated and threshold still not met
            else: # my_action_in_previous_round == "C"
                return "C"
    else:
        # Should not reach here if round_num == 1 is handled.
        # For robustness, return C or raise an error.
        return "C"

# Example of history structure:
# history = {
#     1: {1: "C", 2: "C", 3: "D"}, # Actions in Round 1
#     2: {1: "C", 2: "D", 3: "C"}  # Actions in Round 2
# }
```

### Robustness and Adaptability:

*   **Adaptive:** The strategy directly responds to the group's collective success in meeting the `m` threshold. If cooperation leads to success, it perpetuates it. If failure occurs, it punishes defections that contributed to that failure while remaining cooperative if it was not personally at fault.
*   **Robust:**
    *   **Against "Always Defect" players:** This strategy will eventually shift to defection if a critical mass of players always defects, making cooperation futile and unprofitable.
    *   **Against "Always Cooperate" players:** This strategy will reciprocate their cooperation and will likely lead to sustained cooperation and high payoffs if enough others also cooperate.
    *   **Against "Random" players:** The strategy's reaction to the group's aggregate behavior makes it less susceptible to random fluctuations, as it focuses on the outcome relative to `m`.
    *   **Against "Nice Tit-for-Tat" variations:** By explicitly considering the `m` threshold and its own prior action, it's more discerning than a simple tit-for-tat in a multi-player environment. It's not just about the *average* cooperation but the *successful collective effort*.

This strategy prioritizes the collective good when it's demonstrably achievable (`>=m` cooperators) and adjusts defensively when it's not, without engaging in excessive punishment or immediately abandoning cooperation if it was personally blameless for the failure.
'''

description_COOPERATIVE_116 = '''
Okay, this is a fascinating game! The Collective Risk Dilemma presents a classic social dilemma where individual rationality can lead to collective suboptimal outcomes. The repeated nature and perfect information offer opportunities for strategic adaptation, but the lack of communication is a significant constraint.

Here's a design for a cooperative, adaptive, and robust strategy, focusing on building trust and punishing defection effectively.

## Cooperative Adaptive Strategy: "Trust and Verify"

This strategy aims to foster cooperation by initially trusting the group, but rigorously verifying that trust through observed actions. It's designed to be inherently cooperative in its intent, but pragmatic in its execution, always seeking the collectively beneficial outcome while safeguarding against exploitation.

**Core Principles:**

1.  **Pro-Cooperation Default:** Start by assuming others will cooperate and contribute to the collective good.
2.  **Vigilant Verification:** Continuously monitor the group's behavior to ensure cooperation is being reciprocated.
3.  **Punish Defection, Reward Cooperation:** Directly respond to observed defection with defection, and to sustained cooperation with cooperation.
4.  **Forgiveness (with a Limit):** Offer a path back to cooperation for defectors if the group demonstrates a strong commitment to the cooperative outcome.
5.  **Parameter Awareness:** Adjust the threshold for cooperation based on the game's parameters ($m$, $k$).

---

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) in any given round depends on the history of the game, specifically the observed cooperation rate in the previous round, and the game's parameters.

**Key Variables:**

*   `cooperation_rate_last_round`: The proportion of players (including oneself) who cooperated in the immediately preceding round.
*   `total_cooperators_last_round`: The absolute number of players who cooperated in the immediately preceding round.
*   `my_last_action`: Player i's action in the previous round.
*   `punishment_phase`: A boolean flag indicating if the strategy is currently in a "punishment" mode due to past defections.
*   `forgiveness_window_active`: A boolean flag indicating if the strategy is in a phase where it might forgive a single defection to test for renewed cooperation.

**Decision Logic:**

**State 1: Initial Trust & Observation (First few rounds, or after a period of full cooperation)**

*   **Condition:** If `punishment_phase` is `False` and it's the first round, or if the group has recently been performing well (e.g., `total_cooperators_last_round >= m` in the last round, and `my_last_action == 'C'`).
*   **Action:** **Cooperate (C)**.
    *   **Rationale:** This is the most cooperative starting point. It aims to establish trust and incentivize others to cooperate by demonstrating willingness to take the initial risk.

**State 2: Vigilant Cooperation (Sustained group cooperation near or above threshold)**

*   **Condition:** If `punishment_phase` is `False`, and `total_cooperators_last_round >= m` in the last round, and at least `m` players cooperated in the last round.
*   **Action:** **Cooperate (C)**.
    *   **Rationale:** The group has proven its ability to meet the threshold. Continue to contribute to this successful collective outcome.

**State 3: Defect if Threshold Not Met (Group falls below cooperation threshold)**

*   **Condition:** If `punishment_phase` is `False`, and `total_cooperators_last_round < m` in the last round.
*   **Action:** **Defect (D)**.
    *   **Rationale:** The collective benefit was not achieved in the last round, likely due to insufficient cooperation. This strategy will now defect to protect its own endowment until the group demonstrates a renewed commitment to reaching the cooperation threshold. This is a mild punishment.

**State 4: Targeted Punishment (Persistent defection by others)**

*   **Condition:** If `punishment_phase` is `False`, and `total_cooperators_last_round < m` for a `punishment_threshold` number of consecutive rounds (e.g., 2 or 3 rounds).
*   **Action:** **Defect (D)**. Set `punishment_phase = True`.
    *   **Rationale:** If the group consistently fails to meet the threshold, it indicates a lack of widespread commitment to cooperation. Enter a stronger punishment phase to signal that this behavior is not sustainable and to conserve resources.

**State 5: Active Punishment (During punishment phase)**

*   **Condition:** If `punishment_phase` is `True`.
*   **Decision:**
    *   If `total_cooperators_last_round >= m` AND at least `m` players cooperated in the *last* round AND `my_last_action == 'D'` (meaning *I* defected in the last round but the group *still* met the threshold):
        *   **Action:** **Cooperate (C)**. Set `punishment_phase = False`, `forgiveness_window_active = True`.
            *   **Rationale:** This is a crucial turning point. If the group can achieve the threshold *even when I defect*, it suggests they are highly cooperative. This is the primary mechanism for exiting punishment and attempting to re-establish full cooperation. Entering a "forgiveness window" allows for a single potential misstep by others before returning to full trust.
    *   Else (if the group still failed to meet the threshold, or if they met it but this is not the first round after entering punishment):
        *   **Action:** **Defect (D)**.
            *   **Rationale:** Continue to punish to incentivize a change in behavior.

**State 6: Forgiveness Window (After exiting punishment)**

*   **Condition:** If `forgiveness_window_active` is `True`.
*   **Decision:**
    *   If `total_cooperators_last_round >= m` AND `my_last_action == 'C'` (meaning I cooperated this round and the group met the threshold):
        *   **Action:** **Cooperate (C)**. Set `forgiveness_window_active = False`.
            *   **Rationale:** The group has successfully cooperated and I reciprocated. The forgiveness window is closed, and we return to the "Vigilant Cooperation" state.
    *   Else:
        *   **Action:** **Defect (D)**. Set `forgiveness_window_active = False`. Set `punishment_phase = True` (re-enter punishment).
            *   **Rationale:** If the group fails to meet the threshold *or* if I defected and they still met it (which shouldn't happen if I defected correctly, but as a safeguard), then the attempt at forgiveness has failed. Re-enter the punishment phase.

---

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Action:** **Cooperate (C)**.
    *   **Rationale:** As per State 1. This is the most fundamental cooperative gesture.

*   **Last Round (t=r):**
    *   **Action:** **Defect (D)**.
    *   **Rationale:** In the absolute final round, there's no future interaction to incentivize or punish. The dominant strategy for any rational player becomes to maximize their immediate payoff by defecting if they expect others to cooperate, or defecting regardless to ensure their own gain. This strategy, while cooperative in intent for the *bulk* of the game, becomes self-preserving in the final round. *This is a critical, albeit somewhat pessimistic, adjustment for the very last turn.*

*   **The "Always Defect" Player:** This strategy is robust to this. If the "Always Defect" player exists, `total_cooperators_last_round` will almost certainly be below `m`. The strategy will likely enter State 3 (Defect if Threshold Not Met) and potentially State 4 (Targeted Punishment), and then State 5 (Active Punishment) where it will continue to defect until the "Always Defect" player is no longer a significant factor or the game ends. It will rarely reach the state where it can cooperate again unless *all* other players (or a sufficient number) switch to cooperation.

*   **The "Always Cooperate" Player:** This strategy benefits greatly from this. If there are enough "Always Cooperate" players to consistently meet `m`, this strategy will also consistently cooperate (State 1 & 2), leading to the highest collective payoff.

*   **The "Grim Trigger" Player:** This strategy will effectively cooperate with a "Grim Trigger" player. If the Grim Trigger player defects, the game will likely enter punishment phases for both, but if the group (minus the Grim Trigger) can still meet `m`, they might forgive each other.

*   **Low `m` (e.g., $m=2$):** The strategy will be more sensitive to defections. It might enter punishment faster if the group doesn't achieve cooperation.

*   **High `m` (e.g., $m \approx n$):** The strategy will require a very high level of coordination and trust. It will be more prone to entering the punishment phase if even a few players defect.

---

### 3. Being Cooperative

The strategy is designed with a strong cooperative bias:

*   **Initial Cooperation:** It always starts by cooperating, signaling a desire for mutual benefit.
*   **Cooperation as the Goal:** The ultimate aim is to reach a state where `m` or more players cooperate consistently, maximizing everyone's payoff. The strategy only defects to *achieve* this goal by deterring defection and incentivizing cooperation.
*   **Forgiveness Mechanism:** The "Forgiveness Window" provides a structured way to return to full cooperation after a period of punishment, acknowledging that temporary lapses can occur and that restoring trust is valuable.
*   **Parameter-Driven Threshold:** The decision to cooperate or defect is directly linked to achieving the collective benefit as defined by the game parameters ($m$). The strategy *wants* to achieve this threshold.

---

### Pseudocode Representation:

```python
class TrustAndVerifyStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k

        self.current_round = 0
        self.my_last_action = None # 'C' or 'D'
        self.history = [] # List of lists, each inner list is actions of all players in a round

        # Strategy state variables
        self.punishment_phase = False
        self.forgiveness_window_active = False
        self.punishment_start_round = -1
        self.punishment_threshold_rounds = 3 # Example: Enter punishment after 3 consecutive failures

    def observe_round(self, round_actions_all_players):
        self.current_round += 1
        self.history.append(round_actions_all_players)

    def get_my_action(self):
        # Handle Last Round Edge Case
        if self.current_round == self.r:
            return 'D'

        # --- Analyze Previous Round ---
        total_cooperators_last_round = 0
        if len(self.history) > 0:
            last_round_actions = self.history[-1]
            for action in last_round_actions:
                if action == 'C':
                    total_cooperators_last_round += 1

        # --- Decision Logic ---

        # State 1: Initial Trust & Observation (First round)
        if self.current_round == 1:
            self.my_last_action = 'C'
            self.punishment_phase = False
            self.forgiveness_window_active = False
            return 'C'

        # State 6: Forgiveness Window
        if self.forgiveness_window_active:
            # Check if the group met the threshold AND I cooperated
            if total_cooperators_last_round >= self.m and self.my_last_action == 'C':
                self.forgiveness_window_active = False
                self.punishment_phase = False # Back to normal state
                self.my_last_action = 'C'
                return 'C'
            else: # Group failed or I defected (and they met it - unlikely logic path but safe)
                self.forgiveness_window_active = False
                self.punishment_phase = True # Re-enter punishment
                self.punishment_start_round = self.current_round # Reset punishment timer
                self.my_last_action = 'D'
                return 'D'

        # State 5: Active Punishment
        if self.punishment_phase:
            # Check for condition to exit punishment
            if total_cooperators_last_round >= self.m and self.my_last_action == 'D': # This is the condition to exit if the group can manage *without* me
                 # This is complex: if I defected, and THEY still met m, then I can try cooperating.
                 # If I cooperated and they met m, then I'd already be out of punishment.
                 # The condition for exiting punishment IS if they meet m AND I defected in the previous round.
                self.punishment_phase = False
                self.forgiveness_window_active = True # Enter forgiveness window
                self.my_last_action = 'C' # Decision to cooperate to test forgiveness
                return 'C'
            else: # Continue punishment
                self.my_last_action = 'D'
                return 'D'

        # --- If not in punishment or forgiveness ---

        # State 2: Vigilant Cooperation
        if total_cooperators_last_round >= self.m:
            self.my_last_action = 'C'
            return 'C'

        # State 3: Defect if Threshold Not Met (Mild Punishment)
        if total_cooperators_last_round < self.m:
            # Check if it's time to enter targeted punishment
            if self.punishment_start_round == -1: # First time threshold is missed
                self.punishment_start_round = self.current_round
            elif (self.current_round - self.punishment_start_round) >= self.punishment_threshold_rounds:
                # Transition to State 4: Targeted Punishment
                self.punishment_phase = True
                self.punishment_start_round = self.current_round # Re-init punishment start round
                self.my_last_action = 'D'
                return 'D'
            else: # Not yet enough consecutive misses, but still defect
                self.my_last_action = 'D'
                return 'D'

        # Fallback (should not be reached with the above logic, but as a safeguard)
        self.my_last_action = 'D'
        return 'D'

    def reset(self):
        self.current_round = 0
        self.my_last_action = None
        self.history = []
        self.punishment_phase = False
        self.forgiveness_window_active = False
        self.punishment_start_round = -1

```
'''

description_COOPERATIVE_117 = '''
This strategy is designed to promote cooperation while remaining robust to potential defection by other players. It's a balance between maximizing individual gain and contributing to the collective reward.

## Strategy: Adaptive Threshold Cooperate (ATC)

The core idea of ATC is to maintain a dynamic target for cooperation based on past performance. We'll aim to cooperate if we believe enough others will also cooperate, otherwise, we'll defect to secure our individual endowment.

### 1. Decision Rules

The decision to cooperate or defect in any given round `t` (where `t > 1`) depends on two primary factors:

*   **Observed Cooperation Rate in the Previous Round:** We need to estimate the likelihood of reaching the cooperation threshold `m`.
*   **Proximity to the Cooperation Threshold:** How close were we to `m` in the previous round?

**Rule 1: Cooperate if the observed cooperation rate in the previous round suggests the threshold `m` will be met or exceeded.**

*   Let `C_t-1` be the number of players who cooperated in round `t-1`.
*   If `C_t-1 >= m`, then in round `t`, **Cooperate (C)**.
*   This rule is a direct response to successful cooperation in the past. If the group has demonstrated its ability to reach the threshold, we continue to trust that it will do so again.

**Rule 2: If the threshold `m` was not met in the previous round, but the number of cooperators was close to `m`, and we were among the cooperators, then we "lead" by cooperating again, hoping to pull others along.**

*   If `C_t-1 < m` AND `C_t-1 >= m - (n - m)` (a heuristic to check if the gap was "large" but not insurmountable, adjusted for the number of potential defectors) AND player `i` cooperated in round `t-1`, then in round `t`, **Cooperate (C)**.
    *   The condition `C_t-1 >= m - (n - m)` can be simplified. A more intuitive way to think about this is if the number of cooperators `C_t-1` was "close enough" to `m`. We can define "close enough" as a fraction of the players who *didn't* cooperate: `m - C_t-1 <= n - C_t-1 - m`.
    *   A more practical interpretation: If `C_t-1 < m` but `C_t-1` is "substantial" and we were part of it, we try again.
*   This rule embodies a proactive cooperative spirit. When the group is close to succeeding but falls short, and we were on the cooperative side, we take another risk in the hope of achieving the collective benefit. This is a form of "leading by example."

**Rule 3: If the threshold `m` was not met in the previous round, and the number of cooperators was significantly below `m`, or we defected in the previous round, then defect to secure our individual payoff.**

*   If `C_t-1 < m` AND (either `C_t-1 < m - (n - m)` OR player `i` defected in round `t-1`), then in round `t`, **Defect (D)**.
*   This rule is about self-preservation and rationality. If past cooperation efforts have consistently failed to reach the threshold, or if we personally chose to defect last round (implying we were less confident or prioritizing individual gain), it's safer to keep our endowment.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   Since there is no history, the strategy must make an initial assumption. Given the objective of being cooperative, we will start by **Cooperating (C)**. This sets a cooperative tone from the outset. This is a calculated risk, but it aligns with the strategy's fundamental aim.

*   **Last Round (t=r):**
    *   In the final round, the incentive structure changes. There's no future to leverage for collective gain. The rational choice for every player becomes to maximize their immediate payoff, regardless of others' actions.
    *   Therefore, in the last round (`t=r`), the strategy will **Defect (D)**. This is a classic backward induction argument. Even if the group has been highly cooperative throughout the game, the absence of future rounds erodes the incentive for continued cooperation.

### 3. Be Cooperative

*   **Initial Cooperation:** Starting with cooperation in the first round clearly signals a cooperative intent.
*   **Rewarding Past Cooperation:** Rules 1 and 2 are designed to reinforce and build upon successful cooperation. We are more likely to cooperate if the group has shown it can reach the threshold.
*   **Leading by Example:** Rule 2's mechanism of cooperating again when the group falls just short, especially if we were a cooperator, is a direct embodiment of leading by example.
*   **Parameter-Driven:** The strategy's decisions are entirely based on the game parameters (`n`, `m`, `k`) and the observed history of actions. It doesn't rely on any external information or assumptions about other players' intentions beyond their actions.

### Pseudocode

```
strategy_name: Adaptive Threshold Cooperate (ATC)

parameters:
  n: number of players
  m: minimum cooperators needed
  k: reward factor

history: a list of dictionaries, where each dictionary represents a round and contains 'actions' (a list of 'C' or 'D' for each player) and 'num_cooperators'.

function decide_action(player_id, current_round, history, parameters):
  n = parameters.n
  m = parameters.m
  k = parameters.k

  # Handle the last round: always defect
  if current_round == n: # Assuming 'n' is the total number of rounds 'r' for simplicity here, should be passed as a parameter
    return 'D'

  # Handle the first round: always cooperate
  if current_round == 1:
    return 'C'

  # Get information from the previous round
  previous_round_data = history[current_round - 2] # History is 0-indexed, rounds are 1-indexed
  num_cooperators_prev = previous_round_data['num_cooperators']
  player_action_prev = previous_round_data['actions'][player_id]

  # Calculate the "closeness" threshold for Rule 2
  # A heuristic: if cooperators were less than m, but more than m - (number of defectors in prev round + 1 potential defector)
  # Simpler: if we were close, and we were a cooperator, we lead.
  # Let's define "close enough" as: the number of cooperators was less than 'm', but the number of defectors was not overwhelmingly large.
  # Number of defectors in previous round = n - num_cooperators_prev
  # If num_cooperators_prev < m and (n - num_cooperators_prev) < (n - m + 1) -- not a great heuristic.
  # A better heuristic for "close": if we fall short, but the gap is less than what we'd expect if most people defected.
  # If num_cooperators_prev < m, and num_cooperators_prev >= m - (n - m):
  # This means we had at least 'm' cooperators if 'n-m' players defected.
  # Let's use a simpler, more intuitive "closeness" check:
  # If the previous round was a failure (num_cooperators_prev < m)
  # AND the number of cooperators was "decently high"
  # AND we were one of those cooperators, then we try again.
  # "Decently high" can be defined as: num_cooperators_prev > (n // 2) or some fraction of m, or simply if the gap (m - num_cooperators_prev) is small.
  # For simplicity, let's define it as: num_cooperators_prev >= m - some_small_margin (e.g., 1 or 2, or proportional to n-m)
  # A robust heuristic: if num_cooperators_prev < m AND (m - num_cooperators_prev) < (n - m): the gap of failed cooperations is smaller than the number of players who *could* have cooperated but didn't.

  # Rule 1: If threshold met in previous round, cooperate
  if num_cooperators_prev >= m:
    return 'C'

  # Rule 2: If threshold not met, but close and we were a cooperator, lead
  # Heuristic for "close": The gap (m - num_cooperators_prev) is less than the number of players who *could* have cooperated but didn't.
  # This means if more than (n-m) players defected, it's too far.
  if num_cooperators_prev < m and (m - num_cooperators_prev) <= (n - m) and player_action_prev == 'C':
      return 'C'

  # Rule 3: If threshold not met and not close, or if we defected last round, defect
  return 'D'

# Example of history data structure:
# history = [
#   {'actions': ['C', 'D', 'C', 'D', 'C', 'D'], 'num_cooperators': 3}, # Round 1
#   {'actions': ['D', 'D', 'C', 'C', 'C', 'D'], 'num_cooperators': 3}, # Round 2
#   ...
# ]
```
'''

description_COOPERATIVE_118 = '''
This strategy aims to foster cooperation in the Collective Risk Dilemma by rewarding cooperative actions and punishing defection, while acknowledging the inherent uncertainty and the importance of reaching the cooperation threshold. It's designed to be adaptive and learn from past rounds.

## Cooperative Strategy: The "Adaptive Threshold Seeker"

This strategy embodies a cooperative mindset by striving to reach the cooperation threshold ($m$) as often as possible. It rewards cooperative behavior and punishes defection, but does so in an adaptive manner to account for the dynamic nature of the game and the uncertainty of other players' actions.

### 1. Decision Rules

The core of this strategy is to assess the likelihood of reaching the cooperation threshold ($m$) in the current round based on the observed behavior in previous rounds.

**The principle:** Cooperate if the observed cooperation rate in the past suggests that enough players *might* cooperate to reach the threshold, or if the current round is critical for establishing a cooperative norm. Defect if past behavior indicates widespread defection is likely, or if defecting is the most rational choice to maximize individual payoff given current circumstances.

**Decision Logic:**

*   **Initial Phase (First `R_init` rounds, or until `m` players cooperate consistently):** In the early stages, the strategy prioritizes exploring cooperative behavior to gauge the landscape.
    *   **If the number of observed cooperators in the previous round is `C_prev`:**
        *   **If `C_prev >= m - 1`:** Cooperate. The observed cooperation is very close to the threshold. Contributing to the effort might be the tipping point.
        *   **If `C_prev < m - 1`:** Cooperate with a probability proportional to `C_prev / (m - 1)`. This allows for exploration. For instance, if `m=3` and `C_prev=1`, you might cooperate with a 50% probability. If `C_prev=0`, you cooperate with a 0% probability (i.e., defect).
        *   **In Round 1:** Always Cooperate. This is a strong signal of intent and the best way to initiate cooperation in the absence of any history.

*   **Established Phase (After `R_init` rounds and when a stable cooperative norm is emerging):** Once a pattern emerges, the strategy becomes more reactive.
    *   **Calculate `C_avg_recent`:** The average number of cooperators over the last `R_history` rounds (where `R_history` is a parameter, e.g., 3 or 5 rounds).
    *   **Calculate `P_cooperate_expected`:** The expected probability of any single player cooperating, based on the historical cooperation rate. For example, if in the last `R_history` rounds, a total of `Total_C` cooperations were observed, then `P_cooperate_expected = Total_C / (n * R_history)`.
    *   **Decision:**
        *   **If `P_cooperate_expected` is high (e.g., `P_cooperate_expected > (m-1)/n`) AND the number of players who *did* cooperate in the *previous* round (`C_prev`) is at least `m-1`:** Cooperate. This indicates a strong trend towards meeting the threshold, and your cooperation is likely to push it over.
        *   **If `P_cooperate_expected` is low (e.g., `P_cooperate_expected <= (m-1)/n`) AND the number of players who *did* cooperate in the *previous* round (`C_prev`) is less than `m-1`:** Defect. The historical data suggests that reaching the threshold is unlikely, and defecting maximizes your individual payoff.
        *   **In uncertain cases (e.g., `C_prev` is close to `m-1` but `P_cooperate_expected` is not clearly high or low):** Cooperate if the expected number of cooperators (`n * P_cooperate_expected`) is greater than or equal to `m-1`. This provides a more granular check. If not, defect.

*   **Punishment Mechanism:** If the threshold `m` is *not* met in a round where you cooperated, and the number of cooperators was significantly less than `m` (e.g., `C_prev < m-2`), the strategy may adopt a more retaliatory stance in the next round by defecting with a higher probability or for a fixed number of rounds. This is a learning element to deter future free-riding.

### 2. Handling Edge Cases

*   **First Round (t=1):** **Always Cooperate.** This is the most crucial round to establish a cooperative norm. There is no history to base a decision on, and defecting immediately would be counterproductive to the goal of achieving collective benefit.

*   **Last Round (t=r):** **Defect.** In the final round, there is no future interaction to influence. The incentive for cooperation disappears, and the dominant strategy is to maximize individual payoff by keeping the endowment. This is a standard backward induction consideration in finite repeated games.

*   **Rounds where `m` is not met, despite cooperation:** If you cooperated in round `t`, but the threshold `m` was not met (`C_t < m`), and specifically if the number of cooperators was significantly below `m` (e.g., `C_t < m - 2`), the strategy will:
    *   **Increase likelihood of defection in the next round.** This is a form of conditional cooperation, punishing the lack of collective effort.
    *   The degree of punishment (e.g., probability of defecting, number of rounds to defect) can be a tunable parameter based on the severity of the "failure" (how far from `m` the cooperation count was).

*   **Rounds where `m` is met:** If `m` or more players cooperated in a round, the strategy reinforces this positive outcome. The decision rules in subsequent rounds will favor cooperation, especially if the trend continues.

### 3. Being Cooperative

This strategy is inherently cooperative for several reasons:

*   **Prioritizes threshold achievement:** The primary goal is to enable the community project by reaching the `m` threshold. This is the foundation of collective benefit.
*   **Initiates cooperation:** By cooperating in the first round, it signals a willingness to work towards a common good.
*   **Conditional cooperation:** It cooperates when there's a reasonable expectation that others will too, and the threshold can be met. This is not blind cooperation but a calculated effort towards collective success.
*   **Adaptive punishment:** The "punishment" is not malicious but a strategic response to preserve the integrity of the cooperative system. It aims to deter free-riding that undermines the collective benefit. The punishment phase is temporary and reverts to the adaptive decision-making if cooperation resumes.
*   **Maximizes shared reward:** When cooperation succeeds, all players benefit from the `k` reward factor, amplifying the positive impact of collective action. This strategy aims to create these beneficial outcomes.

### Pseudocode Outline:

```pseudocode
// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor
R_init: initial exploration rounds (e.g., 3)
R_history: rounds to consider for averaging cooperation (e.g., 5)

// State Variables (maintained across rounds)
history_actions = [] // List of lists, each sublist is a player's action in a round [ [A_1_t1, A_2_t1, ...], [A_1_t2, A_2_t2, ...], ... ]
observed_cooperators_per_round = [] // List of counts of cooperators for each round

// Function to decide action for player 'my_player_id' in round 'current_round'
function decide_action(my_player_id, current_round, history_actions, observed_cooperators_per_round):

    // --- Handle Edge Cases ---
    if current_round == 1:
        return 'C' // Always cooperate in the first round

    if current_round == r:
        return 'D' // Always defect in the last round

    // --- Calculate Past Behavior Metrics ---
    C_prev = observed_cooperators_per_round[-1] if observed_cooperators_per_round else 0
    num_rounds_played = len(history_actions)

    // Initial Exploration Phase
    if num_rounds_played < R_init:
        if C_prev >= m - 1:
            return 'C'
        else:
            // Probabilistic cooperation during exploration
            prob_cooperate = min(1.0, max(0.0, C_prev / (m - 1))) // Cap between 0 and 1
            if random.random() < prob_cooperate:
                return 'C'
            else:
                return 'D'

    // Established Phase
    else:
        // Calculate average cooperation rate over R_history rounds
        recent_rounds_coop_counts = observed_cooperators_per_round[-R_history:]
        total_coop_recent = sum(recent_rounds_coop_counts)
        P_cooperate_expected = total_coop_recent / (n * R_history)

        expected_cooperators = n * P_cooperate_expected

        // Decision Logic
        if C_prev >= m - 1 and expected_cooperators >= m - 1:
            return 'C' // High likelihood of meeting threshold

        elif C_prev < m - 1 and expected_cooperators < m - 1:
            return 'D' // Low likelihood of meeting threshold

        // Intermediate case: one metric is borderline
        elif C_prev >= m - 1 and expected_cooperators < m - 1:
            // If actual cooperation in prev round was high, but expectation is low,
            // lean towards cooperation if it's the only way to reach m
            if C_prev + 1 >= m: # If our cooperation would push it over
                return 'C'
            else:
                return 'D'

        elif C_prev < m - 1 and expected_cooperators >= m - 1:
            // If actual cooperation in prev round was low, but expectation is high,
            // maybe others will compensate. We can afford to defect if we are not crucial.
            if C_prev + 1 < m: # If our cooperation is still needed
                return 'C'
            else:
                return 'D'
        else:
            // Fallback - default to defect if none of the above are met clearly.
            // This might be too conservative, can be adjusted.
            return 'D'


// --- Update History (after each round completes) ---
// For each round t:
//   observed_cooperators_count = count of 'C' in history_actions[t]
//   observed_cooperators_per_round.append(observed_cooperators_count)

```
'''

description_COOPERATIVE_119 = '''
Okay, here is a cooperative strategy designed for the Collective Risk Dilemma, emphasizing adaptiveness and robustness in a repeated, no-communication setting.

## Cooperative Strategy: "Conditional Tit-for-Tat with Threshold Awareness"

This strategy aims to foster cooperation by rewarding it while punishing defection, but with a crucial twist: it's highly aware of the collective nature of the game and the critical threshold `m`. It tries to achieve the cooperative outcome as much as possible, but will defect to protect itself if cooperation seems unlikely or risky.

### Core Principles:

1.  **Incentivize Cooperation:** Actively try to cooperate to reach the `m` threshold and secure the collective reward `k`.
2.  **Punish Defection (but not excessively):** If the collective fails to cooperate due to others' defections, react accordingly to discourage future defections.
3.  **Threshold-Centric:** The primary goal is to ensure `m` players cooperate. The strategy's decisions are heavily influenced by the observed number of cooperators.
4.  **Adaptive Learning:** Learn from past rounds to predict future outcomes and adjust behavior.
5.  **Self-Preservation:** While aiming for cooperation, prioritize avoiding the worst-case scenario (fewer than `m` cooperators, and being one of them).

### 1. Decision Rules:

The decision for player `i` in round `t` (denoted `a_i,t`) will be based on the following factors:

*   **Initial State (Round 1):** **Cooperate (C)**
    *   Rationale: The best way to signal a desire for cooperation and to potentially trigger the collective reward is to be the first to cooperate. This is a bold but necessary step in a no-communication game.

*   **Subsequent Rounds (t > 1):**

    Let `N_C(t-1)` be the number of players who cooperated in the *previous* round (`t-1`).
    Let `I_C(i, t-1)` be an indicator variable: 1 if player `i` cooperated in round `t-1`, 0 otherwise.

    **Decision Logic:**

    *   **If `N_C(t-1) >= m` (Previous round was successful):**
        *   **And `I_C(i, t-1) == 1` (I cooperated last round):** **Cooperate (C)**
            *   Rationale: Success breeds success. If the group achieved the threshold and I contributed, continue to do so. This reinforces the positive outcome.
        *   **And `I_C(i, t-1) == 0` (I defected last round):** **Cooperate (C)**
            *   Rationale: If the group achieved the threshold *despite* my defection, there's still a strong incentive to try and maintain cooperation. My private gain from defecting last round was amplified by the collective success. Contributing now is a low-risk way to continue the beneficial outcome.

    *   **If `N_C(t-1) < m` (Previous round failed):**
        *   **And `I_C(i, t-1) == 1` (I cooperated last round):** **Check for "Last Chance" Cooperation:**
            *   **If `N_C(t-1) + 1 >= m` (My cooperation would have tipped the scale):** **Cooperate (C)**
                *   Rationale: My cooperation was crucial but not enough. I'll try again to see if others join. This is a strong signal of my commitment to cooperation.
            *   **Else (`N_C(t-1) + 1 < m`):** **Defect (D)**
                *   Rationale: My individual cooperation, even if I was the only one trying to bridge the gap, wasn't enough to get the collective reward in the previous round. It's now riskier to keep cooperating if others are defecting consistently. Protect my endowment.

        *   **And `I_C(i, t-1) == 0` (I defected last round):** **Defect (D)**
            *   Rationale: If the group failed to cooperate and I also defected, there's no immediate incentive to switch to cooperation. The collective is in a low-payoff state, and defecting is the safest option to secure my private endowment.

### 2. Handling Edge Cases:

*   **First Round (t=1):** **Always Cooperate (C)**
    *   As stated above, this is the initiating cooperative move.

*   **Last Round (t=r):** **Defect (D)**
    *   Rationale: In the final round, there are no future repercussions for defection. The incentive to cooperate to build future trust or achieve future rewards disappears. Therefore, to maximize immediate payoff in the last round, defecting is the rational choice under standard game theory assumptions, assuming no external norms. This strategy prioritizes securing the highest possible payoff in the final round.

*   **Very Small `m` (e.g., `m=2`):** The logic generally holds. If `m` is small, it becomes easier to reach the threshold. The strategy will be more inclined to cooperate if it sees the group is close to `m`.

*   **Very Large `m` (e.g., `m=n-1`):** The strategy becomes more cautious. If `m` is very close to `n`, it's harder for one player's decision to make the difference. The strategy will be more likely to defect if the number of cooperators is consistently low, as the probability of successfully reaching `m` is slim.

### 3. Being Cooperative:

This strategy is fundamentally cooperative in its *intent* and *initial action*.

*   **Initiation of Cooperation:** It *always* starts by cooperating, demonstrating a willingness to risk for collective gain.
*   **Reinforcement of Success:** If cooperation leads to success (reaching `m`), it continues to cooperate to sustain that positive outcome.
*   **Targeted Punishment:** Punishment (defecting) is *only* enacted when cooperation is failing *and* the player's own cooperation was insufficient to overcome the failure in the previous round. It's not an arbitrary retaliation but a calculated response to a breakdown in the cooperative dynamic.
*   **"Last Chance" Cooperation:** The specific rule where `N_C(t-1) + 1 >= m` and the player cooperates demonstrates a strong commitment to achieving the cooperative outcome, even when others might be defecting. This is a key aspect of its cooperative nature.

### Pseudocode Representation:

```python
# Game parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward factor

# History storage (for each player, assumed to be accessible)
# history = {player_id: [(action_t1, num_cooperators_t1), (action_t2, num_cooperators_t2), ...]}
# We'll simulate access to collective history here.

def decide_action(current_round, player_id, game_history_so_far):
    """
    Decides the action (C or D) for the current player.

    Args:
        current_round (int): The current round number (1-indexed).
        player_id (int): The ID of the current player.
        game_history_so_far (list of dicts):
            Each dict represents a round and contains:
            'round_num': int
            'actions': {player_id: action_str, ...} # C or D
            'num_cooperators': int

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # Handle Last Round (Edge Case)
    if current_round == r:
        return 'D'

    # Handle First Round (Initiation)
    if current_round == 1:
        return 'C'

    # --- Subsequent Rounds (t > 1) ---

    # Get data from the previous round
    prev_round_data = game_history_so_far[-1] # Assuming game_history_so_far is ordered
    num_cooperators_prev = prev_round_data['num_cooperators']
    player_action_prev = prev_round_data['actions'][player_id]

    # Check if my cooperation in the previous round would have been enough
    my_cooperation_would_have_been_enough = (num_cooperators_prev + 1) >= m

    # --- Decision Logic ---

    # Case 1: Previous round was successful
    if num_cooperators_prev >= m:
        # If I cooperated last round, continue cooperating
        if player_action_prev == 'C':
            return 'C'
        # If I defected last round but the group still succeeded, still try to cooperate
        else: # player_action_prev == 'D'
            return 'C'

    # Case 2: Previous round failed
    else: # num_cooperators_prev < m
        # If I cooperated last round
        if player_action_prev == 'C':
            # If my cooperation would have tipped the scale, try again (Last Chance)
            if my_cooperation_would_have_been_enough:
                return 'C'
            # Otherwise, my cooperation wasn't enough, so defect to protect endowment
            else:
                return 'D'
        # If I defected last round, and the group failed, continue defecting
        else: # player_action_prev == 'D'
            return 'D'

# Example Usage (conceptual)
# For player 0 in round 3, given game_history_so_far = [round1_data, round2_data]
# action = decide_action(3, 0, game_history_so_far)
```

This strategy attempts to strike a balance between promoting the collective good and protecting individual interests in a world with no communication, relying on historical patterns and the explicit game parameters (`m`) to guide its decisions.
'''

description_COOPERATIVE_120 = '''
This strategy is designed to be a "Cautious Cooperator" – aiming for cooperation to maximize collective benefit, but with clear safeguards to prevent exploitation. It prioritizes achieving the cooperative threshold while remaining flexible based on observed play.

## Cautious Cooperator Strategy

The core idea is to start by attempting cooperation and observe the collective outcome. If cooperation is met with widespread defection, the strategy becomes more cautious, requiring stronger evidence of others cooperating before contributing. Conversely, if cooperation is generally successful, the strategy continues to cooperate.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round `t` (where `t` ranges from 1 to `r`) is based on two primary factors:

*   **The current round number:** This helps manage the transition in and out of cooperative behavior.
*   **The observed level of cooperation in the *previous* round:** This is the adaptive element, allowing the strategy to react to how others are playing.

Let's define a state variable, `cooperative_threshold_met_last_round`, which is a boolean indicating whether `m` or more players cooperated in round `t-1`.

**For Round `t` (where `t > 1`):**

*   **If `cooperative_threshold_met_last_round` is `True`:**
    *   **Decision:** Cooperate (C).
    *   **Reasoning:** The collective has successfully met the threshold in the past, indicating that cooperation is beneficial and likely sustainable if players continue to participate.

*   **If `cooperative_threshold_met_last_round` is `False`:**
    *   **Decision:** Defect (D).
    *   **Reasoning:** The collective failed to meet the threshold in the previous round, suggesting that either too many players defected or the number of cooperators was insufficient. To avoid being exploited (contributing to the project and receiving no reward), the strategy will defect to preserve its endowment. This is a more cautious stance, waiting for others to demonstrate a commitment to cooperation.

**This provides a baseline. We will refine this with specific handling of the first and last rounds.**

### 2. Handling Edge Cases

**First Round (`t = 1`):**

*   **Decision:** Cooperate (C).
*   **Reasoning:** In the absence of any history, the strategy will initiate cooperation. This aligns with the goal of fostering a cooperative environment from the outset. It assumes a willingness to give the cooperative outcome a chance.

**Last Round (`t = r`):**

*   **Decision:** Defect (D).
*   **Reasoning:** This is a standard consideration in repeated games where the end is known. Since there are no future rounds to benefit from cooperation, there is no incentive to cooperate. Cooperating in the last round offers no future reward, making it irrational. The optimal strategy is to defect and maximize personal gain.

**Summary of Decision Rules (incorporating edge cases):**

*   **Round `t = 1`:** Cooperate (C)
*   **Round `t = r`:** Defect (D)
*   **For `1 < t < r`:**
    *   If `cooperative_threshold_met_last_round` is `True`: Cooperate (C)
    *   If `cooperative_threshold_met_last_round` is `False`: Defect (D)

### 3. Being Cooperative

This strategy is fundamentally cooperative in its intent for the majority of the game.

*   **Proactive Cooperation:** It starts by cooperating, offering the collective the chance to achieve the higher payoff.
*   **Responsive Cooperation:** It continues to cooperate as long as the collective demonstrates its ability to meet the cooperation threshold. This shows a commitment to rewarding successful cooperation.
*   **Self-Preservation:** The shift to defection in the last round is a rational self-interest move, but the strategy's behavior throughout the majority of the game is geared towards achieving collective benefit. The caution adopted when cooperation fails is a defense mechanism against immediate exploitation, not a malicious intent to disrupt cooperation permanently. It's a pause to reassess and wait for signs of reciprocation.

### Pseudocode Representation

```
strategy_name = "CautiousCooperator"

parameters = {
    "n": n,
    "r": r,
    "m": m,
    "k": k
}

# Global state to track the success of cooperation in the previous round
# Initialize to True to encourage cooperation in round 2 if the first round has enough cooperators.
# This will be updated after each round's actions are revealed.
cooperative_threshold_met_last_round = True

def decide_action(current_round, past_opponent_actions, past_my_actions, game_history):
    """
    Decides the action (C or D) for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        past_opponent_actions (list of lists): List of actions played by opponents in previous rounds.
                                                e.g., [[C, D, C], [D, D, C]] for 2 opponents over 3 rounds.
        past_my_actions (list): List of actions played by this player in previous rounds.
        game_history (list of dict): A list of dictionaries, where each dict represents a round's outcome.
                                     e.g., [{"round": 1, "actions": [C, D, C, ...], "num_cooperators": X}]

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    global cooperative_threshold_met_last_round

    # Handle the first round: Always cooperate to initiate
    if current_round == 1:
        return 'C'

    # Handle the last round: Always defect to maximize individual gain
    if current_round == parameters["r"]:
        return 'D'

    # For intermediate rounds: Adapt based on the previous round's outcome
    if cooperative_threshold_met_last_round:
        # If cooperation was successful last round, continue cooperating
        return 'C'
    else:
        # If cooperation failed last round, defect and wait for evidence of cooperation
        return 'D'

def update_state(round_outcome):
    """
    Updates the internal state based on the outcome of a round.

    Args:
        round_outcome (dict): A dictionary representing the outcome of the last played round.
                              Expected keys: "round", "actions", "num_cooperators".
    """
    global cooperative_threshold_met_last_round

    # Check if the cooperation threshold was met in the most recent round
    # This check is only relevant for rounds *after* the first round.
    if round_outcome["round"] > 0: # Assuming round_outcome["round"] refers to the round that just finished
        num_cooperators = round_outcome.get("num_cooperators", 0) # Use .get for safety
        if num_cooperators >= parameters["m"]:
            cooperative_threshold_met_last_round = True
        else:
            cooperative_threshold_met_last_round = False

```

**Explanation of `update_state` and `cooperative_threshold_met_last_round`:**

The `update_state` function would be called *after* each round has concluded and the actions of all players are known. It examines the `num_cooperators` from that round. If that number meets or exceeds `m`, the `cooperative_threshold_met_last_round` flag is set to `True` for the *next* round's decision. If the threshold is not met, the flag is set to `False`.

The initial value of `cooperative_threshold_met_last_round` is set to `True`. This is a subtle but important detail. If the first round's outcome is processed, and it meets the threshold, the strategy will *continue* to cooperate in round 2. If the first round's outcome *doesn't* meet the threshold, `cooperative_threshold_met_last_round` will be set to `False` by `update_state`, and the strategy will defect in round 2.

This strategy is robust because it doesn't require specific coordination from other players. It reacts to their aggregate behavior. It's adaptive because its decision-making process changes based on observed outcomes. It's cooperative because it prioritizes cooperation when it's shown to be mutually beneficial.
'''

description_COOPERATIVE_121 = '''
Here's a cooperative strategy designed for the Collective Risk Dilemma, focusing on adaptiveness, robustness, and a clear cooperative intent.

## Cooperative Strategy: "Conditional Reciprocity with Escalation"

This strategy aims to foster cooperation by rewarding it and punishing defection, but with a strong bias towards cooperating, especially when the threshold is likely to be met. It incorporates learning from past round outcomes to adjust its behavior.

### Core Philosophy:

The strategy believes that the community project is beneficial when successful ($m$ or more cooperators). Therefore, it prioritizes contributing to that success. However, it also recognizes that free-riding (defecting when others cooperate) is detrimental. The strategy seeks to balance the desire for collective good with the need to deter exploitative behavior.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round $t$ is based on the following factors:

*   **Belief about the likelihood of success:** This is the primary driver. If the strategy believes that cooperation will lead to meeting the threshold $m$, it will cooperate.
*   **History of cooperation:** Previous successful rounds encourage continued cooperation.
*   **History of defection:** Consistent defection by others might signal a need for a more cautious approach, but not an outright abandonment of cooperation.
*   **Round number:** Special considerations for the early and late stages of the game.

**Detailed Decision Logic for Round $t$ (where $t > 1$):**

*   **If the number of cooperators in the previous round ($t-1$) was $\ge m$:**
    *   **Cooperate (C).** This signifies that the collective effort was successful. Rewarding past success with continued cooperation reinforces the beneficial outcome.

*   **If the number of cooperators in the previous round ($t-1$) was $< m$:**
    *   **Calculate the "cooperation deficit":** `deficit = m - (number of cooperators in t-1)`.
    *   **Assess the "risk of individual defection":** This is a heuristic. We ask: "If I defect in this round, is it likely that my defection will be the deciding factor in failing to meet the threshold?" This can be approximated by comparing the number of players who defected in the previous round to the deficit. If `(number of defectors in t-1) >= deficit`, it's more likely that your defection will push the group below $m$.
    *   **Decision:**
        *   **If (number of cooperators in t-1) >= (m - 1):** This means only one more cooperator was needed. If you defected in the previous round (and we assume you are tracking your own past actions), **Cooperate (C)**. If you cooperated, **Cooperate (C)**. The goal here is to be the tipping point for success.
        *   **If (number of cooperators in t-1) < (m - 1):**
            *   **If the strategy believes that its cooperation is *crucial* for reaching $m$ (i.e., if it defects, the threshold is unlikely to be met):** **Cooperate (C).** This is a risk-averse cooperative stance. A good heuristic for this is to check if the number of observed cooperators is significantly lower than $m$, and if the potential payoff from defecting is not overwhelmingly tempting compared to the shared reward.
            *   **Otherwise (if your cooperation is not strictly necessary to reach $m$, or if there's a high degree of observed defection that makes cooperation seem futile):** **Defect (D).** This is a retaliatory move to conserve resources when cooperation is not proving effective. The "otherwise" condition is nuanced and can be further refined by looking at the *trend* of cooperation. If cooperation has been consistently low, switching to defection might be a signal of dissatisfaction.

**Simplified Decision Tree (for $t > 1$):**

```
current_cooperators = observed_cooperators_in_previous_round
my_action_in_previous_round = ... (tracked internally)

if current_cooperators >= m:
    Cooperate (C)
else: # current_cooperators < m
    if current_cooperators >= m - 1: # Only need 1 more
        Cooperate (C)
    else: # Need more than 1 additional cooperator
        # Heuristic: Assess if my cooperation is likely crucial
        # This is a proxy for "will my defection cause failure?"
        # A simple proxy: If current_cooperators is very low, assume my cooperation matters.
        # Another proxy: If defectors were high in previous round, and current_cooperators is low,
        # and m is not met, perhaps my cooperation is wasted.
        # For robustness, let's lean towards cooperation unless there's strong evidence of widespread defection.

        # Heuristic for "crucial cooperation": If the number of *observed* cooperators
        # is less than or equal to m/2 (a low threshold), then assume my cooperation is more likely to be pivotal.
        if current_cooperators <= m / 2:
             Cooperate (C)
        else:
             # If cooperation is somewhat present but not enough, and my cooperation isn't the sole savior,
             # and there's a history of many defectors, consider defecting.
             # This is the point of adaptation. If the past few rounds have seen many defectors
             # and few cooperators, and m is not met, we might defect.

             # For simplicity, let's define a threshold for "widespread defection"
             # Let's say if more than 50% of players defected in the previous round AND m was not met:
             observed_defectors = n - current_cooperators
             if (observed_defectors > n / 2) and (current_cooperators < m):
                 Defect (D)
             else:
                 Cooperate (C) # Default to cooperation if uncertainty exists
```

**Self-Correction/Adaptation Mechanism:**

The strategy will maintain a "cooperation score" for the group. This score is an exponentially weighted moving average (EWMA) of the *proportion* of players who cooperated in previous rounds.

*   `cooperation_score_t = alpha * (cooperators_t / n) + (1 - alpha) * cooperation_score_{t-1}`
    *   `alpha` is a smoothing factor (e.g., 0.1).
    *   `cooperators_t` is the number of cooperators in round $t$.

When deciding to defect (in the "otherwise" case of the logic above), the strategy can be more inclined to defect if `cooperation_score` is below a certain threshold (e.g., `m/n` or slightly higher). Conversely, if `cooperation_score` is high, it reinforces the decision to cooperate.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Strategy:** **Cooperate (C).**
    *   **Rationale:** The first round is a critical opportunity to establish a norm of cooperation. There is no history to learn from, so starting with cooperation signals good intent and provides the best chance for immediate collective benefit. The risk of being exploited is high, but the potential reward of kick-starting a cooperative equilibrium is greater.

*   **Last Round (t=r):**
    *   **Strategy:** **Defect (D).**
    *   **Rationale:** In the final round, there are no future consequences for defection. The logic of repeated games that encourages cooperation (due to future retaliation) breaks down. Therefore, the rational choice for an individual player is to maximize their own payoff by keeping their endowment and not contributing to the community project, as there's no future round where their cooperation might be rewarded or their defection punished. This is a standard result in finitely repeated games with a known end.

*   **Initial Learning Phase (first few rounds, e.g., t=2 to 5):**
    *   The strategy will be heavily influenced by the initial cooperative decision. It will be more forgiving of early deviations if the overall trend is towards cooperation. The `cooperation_score` will be crucial here in shaping future decisions.

### 3. Being Cooperative:

*   **Proactive Cooperation:** The strategy defaults to cooperation whenever there's uncertainty or a reasonable chance of success. It only defects as a form of *retaliation* or *strategic self-preservation* when cooperation has demonstrably failed to meet the threshold over a sustained period, or when its own cooperation is clearly insufficient to meet the threshold.
*   **Reward for Success:** The strategy strongly rewards past successes by continuing to cooperate after a threshold-met round. This reinforces the positive feedback loop.
*   **Conditional Retaliation:** Defection is not arbitrary. It's a response to a lack of collective success. The strategy aims to "teach" by demonstrating the consequences of widespread defection.
*   **Parameter Awareness:** The strategy implicitly considers $m$ and $n$ through its assessment of how many cooperators are needed and the likelihood of achieving it. $k$ influences the payoff, but the strategy's logic is primarily driven by achieving the threshold for the collective reward.

### Pseudocode Summary:

```pseudocode
# Game Parameters (assumed global or passed as arguments)
# n: number of players
# r: number of rounds
# m: minimum cooperators needed
# k: reward factor

# Internal State Variables
my_past_actions = [] # List of my actions (C or D) in previous rounds
observed_actions_history = [] # List of lists, where each inner list contains actions of all players in a round
cooperation_score = 0.0 # EWMA of cooperation proportion

# Initialization for Round 1
if current_round == 1:
    return C

# For Rounds t > 1
else:
    last_round_actions = observed_actions_history[-1] # Actions from round t-1
    num_cooperators_last_round = count(action == C for action in last_round_actions)
    num_defectors_last_round = n - num_cooperators_last_round

    # Update cooperation score (if alpha is defined)
    cooperation_proportion = num_cooperators_last_round / n
    cooperation_score = alpha * cooperation_proportion + (1 - alpha) * cooperation_score

    # Decision Logic
    if num_cooperators_last_round >= m:
        return C # Reward past success
    else: # num_cooperators_last_round < m
        if num_cooperators_last_round >= m - 1:
            return C # Be the tipping point if needed
        else:
            # Heuristic for "crucial cooperation" - adapt threshold as needed
            # If cooperation is very low, assume my cooperation matters.
            if num_cooperators_last_round <= m / 2:
                 return C
            else:
                 # Consider defecting if widespread defection has been the norm and cooperation failed
                 # This threshold (n/2) can also be tuned.
                 if (num_defectors_last_round > n / 2) and (num_cooperators_last_round < m):
                     return D # Retaliate against persistent low cooperation
                 else:
                     return C # Default to cooperation if uncertain or cooperation score is decent

# Special case for the last round
if current_round == r:
    return D # No future consequences, maximize individual payoff
```

This strategy balances optimism for collective gain with a pragmatic response to persistent failure, making it adaptive and robust in a variety of scenarios. Its core is to foster cooperation by demonstrating its benefits and only resorting to defection when it's a strategic necessity.
'''

description_COOPERATIVE_122 = '''
This is a fascinating game with a clear incentive for cooperation if a critical mass is achieved, but also a strong temptation to defect if others are cooperating. The repeated nature of the game is key to fostering cooperation.

Here's a cooperative strategy designed to be adaptive and robust:

## Strategy: "Threshold Reciprocity"

This strategy aims to initiate cooperation and then sustain it as long as the collective effort meets the required threshold (`m`). It uses a form of conditional cooperation, similar to Tit-for-Tat but adapted for a group setting with a minimum participation requirement. The core idea is to be a reliable cooperator when the group is *likely* to succeed, and to be cautious when the group's success is uncertain or has failed in the past.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) for player `i` in round `t` will be based on two main factors:

*   **Initial Cooperation Phase:** In the early rounds, the strategy will err on the side of cooperation to signal intent and to see if others respond.
*   **Reciprocity & Threshold Adherence:** In later rounds, the strategy will primarily base its decision on the observed success of cooperation in the *previous* round.

**Detailed Decision Logic:**

*   **For Round `t` (where `t` is the current round number, starting from 1):**

    *   **If `t < m` (Early Rounds):**
        *   **Always Cooperate (C).**
        *   **Rationale:** In the initial rounds, when the number of players (`n`) is significantly larger than the minimum required cooperators (`m`), it's beneficial to signal a cooperative intent. By cooperating early, we encourage others to do the same and increase the chances of reaching the threshold. This phase is about establishing a cooperative norm.

    *   **If `t >= m` (Subsequent Rounds):**
        *   **Observe actions in round `t-1`:** Let `C_{t-1}` be the number of players who cooperated in the previous round (`t-1`).
        *   **If `C_{t-1} >= m` (Threshold Met in Previous Round):**
            *   **Cooperate (C).**
            *   **Rationale:** If the group successfully met the cooperation threshold in the previous round, it indicates that a sufficient number of players are willing to cooperate. Continuing to cooperate reinforces this success and maintains the reward.
        *   **If `C_{t-1} < m` (Threshold NOT Met in Previous Round):**
            *   **Defect (D).**
            *   **Rationale:** If the group failed to meet the cooperation threshold in the previous round, it suggests that either:
                *   Not enough players are cooperating, or
                *   Some players are defecting even when others cooperate (exploiting the system).
            In this scenario, continuing to cooperate would be altruistic but would result in a lower payoff for the individual player if the threshold isn't met. Defecting now is a defensive measure to protect one's own endowment. It also serves as a signal that the current level of cooperation is insufficient and a change in behavior is needed.

### 2. Handling Edge Cases

*   **First Round (`t=1`):** As specified in the "Early Rounds" logic, the strategy will **Always Cooperate (C)**. This is the initial signal to the group.

*   **Last Round (`t=r`):** The decision logic for `t >= m` applies. The strategy will look at the outcome of round `r-1`.
    *   If `C_{r-1} >= m`, the strategy will **Cooperate (C)** in round `r`.
    *   If `C_{r-1} < m`, the strategy will **Defect (D)** in round `r`.
    *   **Rationale:** Even in the last round, the same logic applies. The goal is to maximize payoff based on the *expected* outcome of the round, which is influenced by the previous round's success. There's no future to "punish" or "reward" beyond the current round's payoff.

*   **When `m > n`:** This scenario is ruled out by the game specification (`m < n`).

*   **When `m = 2`:** The strategy is robust. In the early rounds (`t < 2`), it cooperates. From round `t=2` onwards, it checks if at least 2 players cooperated in the previous round. If so, it cooperates; otherwise, it defects.

### 3. Being Cooperative

This strategy is fundamentally cooperative, but it's a **conditional** cooperativity. It embodies the spirit of cooperation by:

*   **Initiating Cooperation:** It's willing to take the first step and be a cooperator in the crucial early stages to build momentum.
*   **Rewarding Cooperation:** It continues to cooperate when the collective effort demonstrates success (threshold met). This reinforces positive behavior in others and maximizes group and individual rewards.
*   **Defending Against Exploitation:** It defects when the collective fails to meet the threshold. This is not spiteful but a rational response to a situation where cooperation is not proving fruitful or is being exploited. It aims to avoid being a "sucker" who cooperates while others defect and gain more.
*   **Adaptability:** It constantly learns from the group's past behavior and adjusts its own. It doesn't rigidly stick to a plan if the group's actions make that plan suboptimal.

### Pseudocode Representation

```python
def decide_action(player_id, current_round, n, m, k, history):
    """
    Decides whether to Cooperate (C) or Defect (D) based on the game parameters and history.

    Args:
        player_id (int): The ID of the current player.
        current_round (int): The current round number (starts from 1).
        n (int): Total number of players.
        m (int): Minimum number of cooperators needed.
        k (float): Reward factor.
        history (list of lists): A history of actions for each player in previous rounds.
                                 history[t-1] = [action_p1, action_p2, ..., action_pn] for round t-1.
                                 Actions are 'C' or 'D'.

    Returns:
        str: The chosen action ('C' or 'D').
    """

    # --- Early Rounds: Initiate Cooperation ---
    if current_round < m:
        return 'C'

    # --- Subsequent Rounds: Reciprocity and Threshold Adherence ---
    else:
        # Get actions from the previous round (t-1)
        if current_round == 1: # Should not happen if current_round >= m, but for completeness
            return 'C' # Fallback if somehow reached here without history

        previous_round_actions = history[current_round - 2] # history is 0-indexed for rounds

        # Count cooperators in the previous round
        cooperators_last_round = previous_round_actions.count('C')

        # Decision based on previous round's outcome
        if cooperators_last_round >= m:
            return 'C'  # Threshold was met, continue cooperating
        else:
            return 'D'  # Threshold was not met, defect to protect endowment
```

This "Threshold Reciprocity" strategy provides a robust and cooperative framework that adapts to the group's behavior, aiming for sustained cooperation when possible and rational self-preservation when not.
'''

description_COOPERATIVE_123 = '''
This is a fascinating strategic challenge! The core tension lies in the incentive to defect (guaranteed payoff of 1) versus the potential for a higher payoff through cooperation (2 if you cooperate and the threshold is met, 3 if you defect but the threshold is met). Since communication is forbidden and we must be purely adaptive, the strategy needs to be a clever blend of observation, inference, and calculated risk.

Here's a design for a cooperative strategy, focusing on adaptability and robustness:

## Cooperative Strategy: The "Conditional Opportunist"

This strategy aims to be cooperative by default, but intelligently adjusts its behavior based on observed collective actions. It seeks to exploit opportunities for mutual gain while safeguarding against exploitation by defectors.

### 1. Decision Rules

The core principle is to assess the collective likelihood of reaching the cooperation threshold ($m$) based on past behavior.

*   **Default Action: Cooperate (C)**
    *   In the absence of strong evidence to the contrary, the strategy will assume that others are also inclined towards cooperation. This aligns with the "cooperative mindset" and is the most likely path to the higher collective payoff.

*   **Conditional Defection Trigger: Risk of Wasted Cooperation**
    *   The strategy will defect if it assesses that its cooperation is *unlikely* to contribute to reaching the threshold of $m$ cooperators. This occurs when:
        *   **Low observed cooperation rate in previous rounds:** If, in recent history, the number of cooperators has consistently fallen significantly short of $m$, the individual payoff from cooperating (0 endowment used) becomes less appealing compared to the guaranteed payoff of defecting (1 endowment kept).
        *   **Specific indicator of "pulling the rug":** This is a more nuanced observation. If the number of cooperators is close to $m$ (e.g., $m-1$ or $m-2$) and the strategy observes a *sudden increase* in defections from players who previously cooperated, it might interpret this as players attempting to "free-ride" on the efforts of others and potentially *prevent* the threshold from being met. In such a scenario, defecting becomes the rational choice to secure a payoff of 1 rather than risk a payoff of 0 (if cooperation fails) or 2 (if cooperation succeeds but the player defected).

*   **"Retaliation" for consistent defection (subtle):** While not explicitly punitive, the strategy will naturally adapt. If a significant number of players consistently defect, the observed cooperation rate will naturally decrease, leading the Conditional Opportunist to defect more often. This isn't a revenge mechanism, but a logical response to a low probability of successful cooperation.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Action: Cooperate (C)**
    *   **Reasoning:** In the absence of any historical data, the default cooperative action is the most rational. It establishes a baseline and signals a willingness to cooperate. There's no information to suggest otherwise.

*   **Last Round (t=r):**
    *   **Action: Cooperate (C)**
    *   **Reasoning:** In a finitely repeated game with a known end, there's a temptation for players to defect in the last round to maximize their individual immediate payoff, as there are no future rounds to influence. However, our strategy remains cooperative. This is a strong signal of commitment to cooperation and the belief that even in the last round, collective gain is preferable. It also acts as a deterrent against "last-round defection" from others in future hypothetical tournaments. If others are also designed to be cooperative, this helps maintain the higher payoff for everyone.

*   **Rounds where $m$ is easily met (e.g., $n-1$ cooperators observed):**
    *   **Action: Cooperate (C)**
    *   **Reasoning:** If the collective is already on track to meet the threshold, cooperating is the best way to ensure the reward is achieved. Defecting in this scenario would be detrimental to the group.

*   **Rounds where $m$ is clearly impossible to meet (e.g., only a few cooperators observed over many rounds):**
    *   **Action: Defect (D)**
    *   **Reasoning:** If the observed cooperation rate consistently indicates that $m$ will not be reached, continuing to cooperate is a wasted endowment. The strategy switches to defecting to guarantee the individual payoff of 1.

### 3. Being Cooperative

The core of this strategy is its **default to cooperation**. It only deviates when there is clear, data-driven evidence that cooperation is unlikely to be successful or is being exploited. The goal is not to punish, but to adapt to the reality of the game environment.

*   **Maximizing collective payoff:** By defaulting to cooperation, the strategy actively contributes to the scenario where the higher payoff ($2$ or $3$) is achievable.
*   **Information gathering:** Each cooperative action serves as a data point, contributing to the collective learning of the game environment.
*   **Robustness:** The adaptive nature makes it robust. If it encounters a group of defectors, it will eventually shift to defecting as well, not out of spite, but out of necessity to avoid consistent losses. If it encounters a cooperative group, it will continue to reap the benefits of collective success.

## Pseudocode Representation

```pseudocode
Strategy: ConditionalOpportunist

// Game Parameters (available to the strategy)
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Internal State
history = [] // List of tuples: (round_number, player_actions_in_round, player_payoffs_in_round)
my_actions = [] // List of my past actions (C or D)
observed_cooperators_per_round = [] // List of counts of cooperators in each past round
avg_cooperation_rate_recent = 0.0 // Moving average of cooperation rate

// Function to decide action for the current round (t)
function decide_action(current_round_number, historical_data):

    // --- Handle Edge Cases ---
    if current_round_number == 1:
        return 'C' // Always cooperate in the first round

    if current_round_number == r:
        return 'C' // Always cooperate in the last round (commitment to cooperation)

    // --- Update Internal State based on historical_data ---
    // (This would involve parsing the provided historical_data for the previous rounds)
    // For simplicity, assume observed_cooperators_per_round is populated
    // and avg_cooperation_rate_recent is calculated from it.

    // Example: Calculate avg_cooperation_rate_recent (e.g., over the last 5 rounds)
    recent_rounds = observed_cooperators_per_round[-min(5, len(observed_cooperators_per_round)):]
    if len(recent_rounds) > 0:
        avg_cooperation_rate_recent = sum(recent_rounds) / (len(recent_rounds) * n)
    else:
        avg_cooperation_rate_recent = 0.0 // Should not happen after round 1

    // --- Decision Logic ---

    // Rule 1: If a strong indicator of the threshold being met is present, cooperate.
    // (This is a simplified condition; a more robust version would involve predictive modeling)
    last_round_cooperators = observed_cooperators_per_round[-1] if observed_cooperators_per_round else 0
    if last_round_cooperators >= m - 1: # If very close to the threshold
        # Check if others might be trying to exploit.
        # This is tricky without direct communication. We'll infer from recent changes.
        if len(observed_cooperators_per_round) > 1:
            prev_round_cooperators = observed_cooperators_per_round[-2]
            # If the number of cooperators has *decreased* significantly when close to 'm'
            # and we are the only potential 'last minute' cooperator to tip the scale.
            if prev_round_cooperators >= m and last_round_cooperators < m and n - len(my_actions) == last_round_cooperators:
                # This scenario is complex: others might have defected to push us over.
                # For now, we stick to cooperate if we are the difference maker.
                return 'C'
        else:
            return 'C' # Cooperate if we are the last piece to the puzzle

    // Rule 2: If the average cooperation rate is consistently low, defect.
    // Threshold for "consistently low" needs calibration, e.g., < m/n * 0.7
    if avg_cooperation_rate_recent < (m / n) * 0.7:
        return 'D'

    // Rule 3: Default to cooperate if none of the above triggers defection.
    return 'C'

// --- Helper functions to be implemented ---
// - function update_history(round_number, player_actions_in_round, player_payoffs_in_round): stores data
// - function calculate_avg_cooperation_rate_recent(): computes moving average
```

**Explanation of Pseudocode Logic:**

*   **`history` and `my_actions`**: These are crucial for remembering past plays and building statistical inferences.
*   **`observed_cooperators_per_round`**: This is the most critical piece of data. It directly tells us how close the group has been to the threshold.
*   **`avg_cooperation_rate_recent`**: A moving average smooths out short-term fluctuations and provides a more stable indicator of the general trend in cooperation.
*   **Decision Logic Breakdown:**
    *   The edge cases (first and last round) are handled explicitly.
    *   The first check is for scenarios where cooperation is *highly likely* to succeed. The logic around `m - 1` is a heuristic; in a more advanced version, this could involve more complex probability calculations.
    *   The next condition checks if cooperation has been consistently failing. If the average rate is significantly below the threshold needed for success, it's rational to defect. The `0.7` is a tuning parameter that would ideally be determined through empirical testing.
    *   If neither of these strong conditions is met, the strategy defaults to cooperation.

This "Conditional Opportunist" strategy aims to be both optimistic and pragmatic, striving for the best collective outcome while being prepared to protect its individual payoff when the environment clearly indicates that cooperation is a losing proposition.
'''

description_COOPERATIVE_124 = '''
## Cooperative Strategy: "Threshold Trust Tit-for-Tat"

This strategy is designed to be cooperative, adaptive, and robust. It balances the desire to achieve the collective reward with the need to protect against exploitation. The core idea is to conditionally cooperate based on the observed past performance of the group in reaching the cooperation threshold.

### 1. Decision Rules:

The fundamental decision rule for player `i` in round `t` is as follows:

*   **If the number of cooperators in the *previous* round (`t-1`) was `m` or more:**
    *   **Cooperate (C).** This indicates that collective risk was successfully averted in the previous round, and the group has demonstrated the ability to reach the cooperation threshold. By cooperating, we reinforce this successful behavior and aim to trigger the higher payoff again.

*   **If the number of cooperators in the *previous* round (`t-1`) was *less* than `m`:**
    *   **Defect (D).** This signifies that the group failed to meet the cooperation threshold in the previous round. Continuing to cooperate in such a scenario would be altruistic but potentially lead to lower payoffs for everyone (especially if others are defecting). Instead, we defect to conserve our individual endowment and signal that the current level of cooperation is insufficient to warrant our contribution.

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   In the very first round, there is no previous history. To initiate a cooperative outcome and signal a willingness to build trust, we will **Cooperate (C)**. This is a gamble, but it's essential to kickstart the possibility of achieving the collective reward. If the group doesn't coordinate and we are the only cooperator, we still get our private endowment. If others also cooperate, we stand to gain the collective reward.

*   **Last Round (t=r):**
    *   The decision rule remains the same as for any other round. We will **Cooperate (C)** if the number of cooperators in round `r-1` was `m` or more, and **Defect (D)** otherwise. Since there are no future rounds to consider, there's no need to "punish" or "reward" for future interactions. The decision is purely based on the immediate past's outcome. This also aligns with the general principle of the strategy.

*   **No Observed History (e.g., if history log is reset):**
    *   If for some reason, the history of previous rounds is lost or unavailable, we revert to the Round 1 behavior: **Cooperate (C)**. This is a fallback to attempt to re-establish a cooperative dynamic.

### 3. Being Cooperative:

This strategy is inherently cooperative in its intent and mechanism:

*   **Proactive Cooperation:** We initiate cooperation in the first round, demonstrating our desire for a mutually beneficial outcome.
*   **Reinforcing Success:** We continue to cooperate when the group has successfully met the cooperation threshold, effectively rewarding collective action and encouraging its recurrence.
*   **Conditional Reciprocity:** While not a strict "tit-for-tat" in the sense of mirroring the *exact* action of a single opponent, it's a "group tit-for-tat" or "threshold tit-for-tat." We are reciprocating the group's collective *success* in achieving the common goal. If the group cooperates enough to get the reward, we cooperate. If not, we withhold our cooperation.
*   **Sustainability:** By defecting when the threshold is not met, we prevent our individual endowment from being drained in scenarios where cooperation is not yielding the desired collective outcome. This ensures our long-term ability to contribute to potential future cooperative successes.

### Pseudocode:

```pseudocode
Strategy: Threshold Trust Tit-for-Tat

Input:
  - n: number of players
  - r: number of rounds
  - m: minimum cooperators needed
  - k: reward factor
  - current_round: t (integer, 1 to r)
  - history: a list of lists, where history[t-1] contains the actions of all players in round t-1.
            Example: history[t-1] = [C, D, C, ...] for round t-1.
            If t=1, history is empty.

Output:
  - Player i's action for the current round: C or D

Function DecideAction(n, r, m, k, current_round, history):

  // Handle the first round
  if current_round == 1:
    return C // Initiate cooperation

  // Get the actions from the previous round
  previous_round_actions = history[current_round - 1]

  // Count the number of cooperators in the previous round
  num_cooperators_previous_round = 0
  for action in previous_round_actions:
    if action == C:
      num_cooperators_previous_round += 1

  // Decision rule based on previous round's cooperation level
  if num_cooperators_previous_round >= m:
    return C // Group succeeded, so we cooperate
  else:
    return D // Group failed to reach threshold, so we defect

```

This strategy aims to be robust by not being overly forgiving or overly punitive. It learns from the collective outcome of the previous round and adjusts its behavior accordingly, making it a principled and adaptable approach to the Collective Risk Dilemma.
'''

description_COOPERATIVE_125 = '''
## Strategy: The "Conditional Threshold Monitor"

This strategy aims to promote cooperation by actively monitoring the collective effort and adjusting individual behavior accordingly. It balances the desire to secure the higher payoff through cooperation with the risk of being exploited if cooperation fails.

**Core Philosophy:** The "Conditional Threshold Monitor" believes that cooperation is the most beneficial outcome for all players when the cooperation threshold ($m$) is met. However, it recognizes the inherent risk of being the only one contributing to the communal good. Therefore, it will cooperate when it believes there is a high likelihood of success and defect when that likelihood diminishes or when it's strategically beneficial to punish defection.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round $t$ is based on the following logic:

*   **Initial Phase (First few rounds):** In the early stages of the game, the strategy will adopt a cautiously optimistic approach. It will prioritize gathering information about the general behavior of other players.
    *   **Decision Rule:** **Cooperate (C)** for the first few rounds (let's define this as `initial_rounds_to_explore = max(2, floor(n/2))` to adapt to the game size). This allows for initial exploration and signal potential cooperation without immediately exposing itself to exploitation.

*   **Monitoring Phase (After the initial phase):** Once sufficient data has been gathered, the strategy will begin to adapt its behavior based on observed cooperation levels.
    *   **Define "Recent Cooperation Success":** Let's track the number of players who cooperated in the *previous* round, denoted as $N_{cooperators, t-1}$.
    *   **Define "Confidence Level":** A "confidence level" will be maintained. This represents the strategy's belief that cooperation will be successful in the current round. Initially, this is low.
    *   **Decision Rule:**
        *   **If $N_{cooperators, t-1} \ge m$ (Threshold met or exceeded in the previous round):** The strategy increases its confidence level. **Cooperate (C)**. This signals continued belief in the collective benefit.
        *   **If $N_{cooperators, t-1} < m$ (Threshold not met in the previous round):** The strategy decreases its confidence level.
            *   **If Confidence Level > 0.5 (or some defined moderate threshold):** **Cooperate (C)**. This is a "retry" attempt, signaling a continued willingness to try if there's still some perceived momentum or if the number of defectors was small.
            *   **If Confidence Level ≤ 0.5:** **Defect (D)**. This signifies a loss of faith in the group's ability to cooperate. The strategy now prioritizes its individual endowment.

*   **Threshold Adjustment:** The confidence level is not static.
    *   When cooperation is successful ($N_{cooperators, t-1} \ge m$), the confidence level *increases* by a fixed increment (e.g., `confidence_increment = 0.1`).
    *   When cooperation fails ($N_{cooperators, t-1} < m$), the confidence level *decreases* by a fixed decrement (e.g., `confidence_decrement = 0.15`). The decrement is larger than the increment to make it harder to regain trust after a failure.
    *   The confidence level will be capped between 0 and 1.

*   **The "Punishment" Mechanism:** While primarily cooperative, the strategy needs to deter free-riding.
    *   **Decision Rule:** If the strategy *just defected* in the previous round (because its confidence was low) and the *total number of defectors* in the previous round was significantly higher than the number of cooperators, it may continue to defect for one more round to signal its displeasure and encourage others to cooperate in the future. This is a reactive punishment.

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated in the "Initial Phase," the strategy will **Cooperate (C)**. This is to signal good faith and explore the environment.
*   **Last Round (t=r):** In the final round, the strategic incentive shifts. Since there are no future rounds to leverage cooperation, the dominant strategy for any rational player is to defect to maximize their individual payoff.
    *   **Decision Rule:** **Defect (D)** in the last round ($t=r$). This is a hardcoded rule for the final round.
*   **Rounds Immediately Following a Complete Failure (All players defect):** If in a round $t-1$, *all* $n$ players defected, the strategy will be highly unlikely to cooperate in round $t$. Its confidence level would be at its lowest, and the punishment mechanism might also be triggered. This prevents immediate re-engagement with a clearly uncooperative group.

### 3. Cooperative Mindset

The "Conditional Threshold Monitor" is fundamentally cooperative because:

*   **Prioritizes Shared Gain:** Its default behavior is to cooperate, especially in the early stages and when cooperation has been successful. It actively tries to achieve the higher collective payoff when the threshold is met.
*   **Signals Trust:** By cooperating even when there's a slight risk, it signals a willingness to trust the other players and build a cooperative norm.
*   **Adaptive Learning:** It learns from the group's behavior. If the group consistently fails to meet the threshold, it eventually prioritizes individual survival. This is not malice, but a rational response to sustained defection.
*   **Belief in Long-Term Benefit:** The strategy is designed for a repeated game. It understands that in the long run, consistent cooperation that meets the threshold will yield better total payoffs than sporadic, self-serving defections.
*   **Gradual Punishment:** The "punishment" mechanism is not immediate or extreme. It's a temporary shift to defection, intended to discourage future free-riding, not to permanently antagonize other players. The strategy will readily return to cooperation if the observed behavior improves.

### Pseudocode Example:

```
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward if threshold met factor

// Strategy State Variables
current_round = 1
confidence_level = 0.3 // Initial low confidence
confidence_increment = 0.1
confidence_decrement = 0.15
initial_rounds_to_explore = max(2, floor(n/2)) // Rounds for initial exploration
last_round_cooperators = 0
total_defecters_last_round = 0 // Track total defectors for punishment

FUNCTION decide_action():
  // Handle last round edge case
  if current_round == r:
    return "D"

  // Handle initial exploration phase
  if current_round <= initial_rounds_to_explore:
    return "C"

  // Monitoring phase and adaptive decision making
  if last_round_cooperators >= m:
    // Threshold met, increase confidence and cooperate
    confidence_level = min(1.0, confidence_level + confidence_increment)
    return "C"
  else:
    // Threshold not met, adjust confidence
    if confidence_level > 0.5: // Still a moderate chance of success
      // Try again, but slightly less confident
      confidence_level = max(0.0, confidence_level - confidence_decrement / 2) // Smaller decrease on failed attempt when still somewhat confident
      return "C"
    else: // Low confidence, likely defect
      // Check for punishment scenario (optional refinement)
      // If a strong pattern of defection exists and we just defected, continue defecting
      if (total_defecters_last_round > n / 2) and (last_action_this_round == "D"): // Assuming last_action_this_round is tracked
        return "D"
      else:
        confidence_level = max(0.0, confidence_level - confidence_decrement)
        return "D"

// In the game loop, after each round:
// Update history and state variables
// last_round_cooperators = count of players who played C in round t
// total_defecters_last_round = n - last_round_cooperators
// current_round = current_round + 1
```

This strategy aims to be a robust and adaptive participant, capable of fostering cooperation when possible, but also defending against exploitation when necessary. Its success will depend on the collective behavior of the other participants in the tournament.
'''

description_COOPERATIVE_126 = '''
Here's a design for a cooperative strategy for the Collective Risk Dilemma game, focusing on adaptability and robustness, while adhering to the specified constraints.

## Strategy Name: Adaptive Cooperation with Threshold Monitoring (ACTM)

The core principle of ACTM is to **conditionally cooperate**. It aims to achieve the collective reward by encouraging cooperation when it seems achievable and safe, while defecting to protect itself when cooperation is unlikely or the risk of being the sole cooperator is too high.

### 1. Decision Rules:

ACTM's decision in any given round is based on a calculation of the **perceived likelihood of reaching the cooperation threshold (m)**, considering the recent history of play.

**Key Metrics:**

*   **Cooperation Rate (CR_t):** The proportion of players who cooperated in round `t`. `CR_t = (Number of cooperators in round t) / n`
*   **Recent Cooperation Rate (RCR):** An average of the cooperation rate over the last `w` rounds (where `w` is a parameter, discussed below). This smooths out short-term fluctuations.
*   **Individual Contribution Status (ICS_i,t):** A binary indicator for player `i` in round `t`. `1` if player `i` cooperated, `0` if player `i` defected.

**Decision Logic (for player i in round t):**

*   **If t = 1 (First Round):**
    *   **Cooperate (C).** The initial action is to signal a willingness to cooperate and attempt to establish a cooperative norm. This is a gamble, but necessary to explore the possibility of collective reward.

*   **If t > 1 and t < r (Intermediate Rounds):**
    *   **Calculate the "Cooperation Potential Indicator" (CPI_t):** This is a heuristic to estimate the likelihood of achieving `m` cooperators in the current round, given past behavior.
        *   **If RCR >= (m - 1) / n:** This suggests that in recent history, the number of cooperators has often been close to the minimum threshold. In this scenario, ACTM is more inclined to cooperate.
            *   **If CPI_t is high (e.g., RCR > m/n or if the trend is upward):** **Cooperate (C)**. If past cooperation rates are good and trending upwards, it's a strong signal to continue cooperating.
            *   **If CPI_t is moderate (e.g., RCR is close to m/n and stable/slightly decreasing):** **Defect (D)**. If the recent cooperation is hovering near the threshold but not decisively exceeding it, the risk of being one of the few cooperators increases. ACTM will defect to preserve its endowment.
        *   **If RCR < (m - 1) / n:** This indicates that historically, cooperation has been sparse. In this scenario, ACTM is less inclined to cooperate.
            *   **Defect (D).** The risk of being the sole or one of few cooperators is too high. ACTM prioritizes its own endowment in this situation.

    *   **Refined CPI Calculation (Heuristic):**
        A simple CPI can be derived from RCR and the required number of cooperators. For instance:
        `CPI_t = RCR - (m / n)`
        If `CPI_t > threshold_positive` (e.g., 0.1), lean towards Cooperation.
        If `CPI_t < threshold_negative` (e.g., -0.2), lean towards Defection.
        In between, a more nuanced decision is needed.

    *   **Considering Individual Risk:**
        Even if RCR suggests cooperation is possible, if player `i` observes that **less than `m-1` other players have indicated a cooperative intent** (e.g., by cooperating in the previous round and the overall cooperation rate was high enough), player `i` will **Defect (D)**. This prevents being the sole cooperator.

*   **If t = r (Last Round):**
    *   **Defect (D).** In the final round, there are no future rounds to influence or benefit from. The incentive for cooperation disappears, and the focus shifts entirely to maximizing individual payoff.

**Parameter `w` (Window Size for RCR):**

*   A reasonable starting point for `w` could be `max(3, r // 5)`. This ensures a minimum number of rounds are considered for averaging, but also scales the window with the total number of rounds. A smaller `w` makes the strategy more reactive to recent changes, while a larger `w` makes it more stable and resistant to short-term noise.

### 2. Handling Edge Cases:

*   **First Round (t=1):** As described above, **Cooperate (C)** to seed cooperation. This is a crucial initiation for any cooperative strategy in a repeated game.
*   **Last Round (t=r):** As described above, **Defect (D)** to maximize final payoff.
*   **Few Players (n close to m):** If `n` is only slightly larger than `m` (e.g., `n = m + 1` or `n = m + 2`), the dynamics become more sensitive. ACTM will be more cautious, requiring a consistently high RCR before committing to cooperation.
*   **High Reward Factor (k >> 1):** A higher `k` increases the incentive for cooperation. ACTM will be slightly more forgiving of past defection patterns and more willing to experiment with cooperation if the potential reward is very high.
*   **Low Reward Factor (k close to 1):** A lower `k` makes cooperation less attractive. ACTM will be more risk-averse and less likely to cooperate unless a strong, stable cooperative pattern is already established.
*   **Threshold `m`:** If `m` is very high (close to `n`), cooperation becomes difficult. ACTM will be less likely to cooperate unless there's overwhelming evidence of widespread cooperation. If `m` is low (close to 2), cooperation is easier, and ACTM will be more inclined to initiate it.

### 3. Being Cooperative:

ACTM is fundamentally designed to be cooperative by aiming for the collective reward:

*   **Initial Cooperation:** The first round is a direct commitment to cooperation, sacrificing immediate guaranteed payoff for the possibility of collective gain.
*   **Conditional Cooperation:** It cooperates when the conditions appear favorable for collective success (i.e., when the threshold `m` seems achievable). This is a strategic form of cooperation, aiming for sustainable collective benefit rather than naive, unconditional cooperation.
*   **Risk Aversion for Collective Good:** By defecting when the threshold is unlikely to be met, ACTM prevents scenarios where it is the sole cooperator (which would result in a lower payoff than defecting and receiving the private endowment). This protects its ability to continue playing and potentially contribute to future cooperation. This is a form of self-preservation that ultimately *enables* future cooperative action.
*   **Adaptive Learning:** By adjusting its cooperative behavior based on the observed actions of others, ACTM learns and adapts. This responsiveness is key to fostering cooperation in a dynamic environment.

### Pseudocode:

```pseudocode
// ACTM Strategy for Collective Risk Dilemma

// --- Parameters ---
n: total number of players
r: total number of rounds
m: minimum cooperators needed
k: reward factor
w: window size for recent cooperation rate (e.g., max(3, r // 5))

// --- Global State (maintained across rounds) ---
history = [] // List of lists, where each inner list contains actions of all players in a round
round_number = 0

// --- Function to calculate Cooperation Rate for a given round ---
function calculate_cooperation_rate(round_actions):
    cooperators = count of 'C' in round_actions
    return cooperators / n

// --- Function to calculate Recent Cooperation Rate ---
function calculate_recent_cooperation_rate(current_round_index):
    if current_round_index < w:
        relevant_history = history[:current_round_index]
    else:
        relevant_history = history[current_round_index - w : current_round_index]

    if is_empty(relevant_history):
        return 0

    total_cr = 0
    for round_actions in relevant_history:
        total_cr += calculate_cooperation_rate(round_actions)

    return total_cr / length(relevant_history)

// --- Main Decision Function ---
function decide_action(player_index):
    round_number = length(history) + 1 // Current round (1-indexed)

    // --- Handle Edge Cases ---
    if round_number == 1:
        return 'C' // Cooperate in the first round

    if round_number == r:
        return 'D' // Defect in the last round

    // --- Intermediate Rounds Logic ---

    // Calculate Recent Cooperation Rate
    recent_cr = calculate_recent_cooperation_rate(round_number - 1) // Use 0-indexed round number for history access

    // Estimate number of cooperators needed from others
    needed_from_others = m - 1 // If I cooperate, how many more are needed?

    // Heuristic for Cooperation Potential
    // If recent cooperation is generally good (e.g., averaging above a threshold that suggests m is achievable)
    if recent_cr >= (m - 1.0) / n: // If historically, cooperation has been close to the threshold

        // Count observed cooperators in the previous round
        previous_round_actions = history[round_number - 2] // Get actions from the round before the current one
        observed_cooperators_previous_round = count of 'C' in previous_round_actions

        // Decision: Lean towards cooperation if RCR is good and we are not at high risk of being alone
        if observed_cooperators_previous_round >= needed_from_others or recent_cr > m/n:
             // If enough cooperators were observed last round OR the recent average is already above the threshold
            return 'C'
        else:
            // If RCR is decent but not very strong, and we weren't sure last round, play safe
            return 'D'
    else:
        // If Recent Cooperation Rate is low, the risk is too high.
        return 'D'

// --- Game Loop (Conceptual) ---
// In each round:
// 1. Get actions of all players from the previous round (if not round 1).
// 2. Store player's action.
// 3. Update history with current round's actions.
// 4. If not the last round, prepare for the next round.

```

**Robustness Considerations:**

*   **Opponent Behavior:** ACTM is designed to react to observed behavior. If opponents consistently cooperate, ACTM will also cooperate. If opponents consistently defect, ACTM will eventually defect. It adapts to the emerging "norm" of the game.
*   **Noise and Uncertainty:** The use of a sliding window (`w`) for the RCR helps to smooth out random fluctuations and makes the strategy less susceptible to single-round anomalies.
*   **No Predefined Patterns:** The strategy relies solely on observable parameters and history, not on any assumptions about pre-existing coordination mechanisms or communication protocols.

This strategy aims to strike a balance between pursuing the collective reward and protecting individual interests, making it a robust and potentially successful cooperative approach in the Collective Risk Dilemma.
'''

description_COOPERATIVE_127 = '''
## Cooperative Strategy for the Collective Risk Dilemma

This strategy, which we'll call **"Adaptive Threshold Cooperation" (ATC)**, is designed to be cooperative while remaining robust in the face of diverse player behaviors. It aims to achieve the high payoff of collective cooperation ($\pi_i = k$) when possible, but prioritizes avoiding the worst-case scenario (low payoff from contributing when cooperation fails).

The core idea is to be cautiously cooperative, rewarding past cooperation while also defecting if cooperation seems unlikely to succeed.

---

### 1. Decision Rules

The decision to cooperate or defect in any given round depends on the observed cooperation rate in the *previous* round, adjusted for the current game state (specifically, the distance to the next round).

**General Rule:**

In round `t` (where `t > 1`), player `i` will play **Cooperate (C)** if the following condition is met:

```
(Observed_Cooperation_Rate_in_round_(t-1) * n) >= (m - 1)
```

Otherwise, player `i` will play **Defect (D)**.

**Explanation of the Rule:**

*   **Observed_Cooperation_Rate_in_round_(t-1)**: This is the proportion of players (including yourself) who played C in the previous round.
*   **n**: The total number of players.
*   **m**: The minimum number of cooperators needed to trigger the reward.

The logic behind `(m - 1)` is crucial: if the previous round saw *at least* `m - 1` cooperators, it suggests that reaching the threshold `m` is achievable. By cooperating in the current round, player `i` increases the probability of crossing that threshold, thereby securing the collective reward for everyone. If the previous round had fewer than `m - 1` cooperators, the chances of reaching `m` in the current round are significantly reduced, making it more rational to defect and secure the private payoff.

**Rationale for Cooperation:**

This rule is cooperative because it directly links future cooperation to past success. If other players have demonstrated a willingness to cooperate (even if not enough to trigger the reward), ATC reciprocates. It assumes that a slight shortfall in the previous round might have been a temporary fluctuation or a learning process for others. By cooperating when the observed rate is close to `m-1`, ATC actively tries to push the group towards the cooperative outcome.

**Rationale for Robustness:**

The strategy is robust because it doesn't blindly cooperate. If the observed cooperation rate in the previous round is low, ATC defects. This protects the player from being exploited – contributing to the project when it's clear that the collective reward is unlikely to be achieved, leading to a personal payoff of `(1 - 1) = 0` if others defect, which is worse than the private payoff of 1.

---

### 2. Handling Edge Cases

#### **2.1. First Round (t = 1):**

In the first round, there is no past history. ATC needs a default behavior that balances exploration with a degree of caution.

**Decision Rule for Round 1:**

Play **Cooperate (C)**.

**Rationale:**

The first round is an exploration phase. By cooperating, ATC signals a cooperative intent. If other players are also programmed to explore or have similar cautious cooperative strategies, this increases the chance of triggering the reward early. If the reward is triggered in the first round, it sets a strong precedent for future rounds and benefits all players. If cooperation fails, the individual payoff is `(1 - 1) = 0` for this round, but this is a necessary risk to identify potential cooperators and gauge the group's inclination.

#### **2.2. Last Round (t = r):**

In the final round, the incentive structure shifts. There are no future rounds to build upon, so the focus becomes maximizing individual payoff in that specific round.

**Decision Rule for Round `r`:**

Play **Defect (D)**.

**Rationale:**

The "future benefit" of cooperation in the last round is zero. Therefore, the optimal strategy is to maximize immediate individual gain. If `m` or more players cooperate, the payoff for a cooperator is `(1 - 1) + k = k`. The payoff for a defector is `(1 - 0) + k = 1 + k`. If fewer than `m` players cooperate, the payoff for a cooperator is `(1 - 1) = 0`, and the payoff for a defector is `(1 - 0) = 1`. In all scenarios for the last round, defecting yields a strictly higher or equal payoff for the individual player, regardless of what others do. This is a standard outcome in finite repeated games, where the last round often collapses to a one-shot game logic.

#### **2.3. Intermediate Rounds (1 < t < r):**

These rounds follow the **General Rule** described above.

---

### 3. Being Cooperative

ATC embodies a cooperative mindset through several key features:

*   **Reciprocity:** It rewards observed cooperation by cooperating itself, fostering a positive feedback loop when the group is moving towards cooperation.
*   **Proactive Cooperation:** In the first round, it takes the initiative to cooperate, signaling its intent and increasing the likelihood of achieving the beneficial collective outcome.
*   **Threshold-Oriented:** The entire strategy is built around achieving the collective benefit (`k`) by meeting the `m` threshold. This inherently aligns with the game's cooperative objective.
*   **Adaptive Learning:** By conditioning its actions on past cooperation rates, ATC implicitly learns and adapts to the group's behavior, demonstrating a willingness to adjust its approach for the collective good.

---

### Pseudocode for Adaptive Threshold Cooperation (ATC)

```pseudocode
// Parameters (provided to the strategy)
n: integer // total number of players
r: integer // total number of rounds
m: integer // minimum cooperators needed
k: float // reward factor

// State variables (maintained by the strategy)
round_number: integer = 1
cooperation_history: list of booleans // Stores actions of all players in each round (True for C, False for D)
// In a real implementation, this might store the count of cooperators per round.
// For simplicity here, we assume we can calculate it.

// Function called at the start of each round
function decide_action(current_round_number, observed_actions_previous_round):
  round_number = current_round_number

  if round_number == 1:
    // First round: Explore by cooperating
    return "C"

  if round_number == r:
    // Last round: Maximize individual payoff by defecting
    return "D"

  // Intermediate rounds: Adaptive Threshold Cooperation
  else:
    num_cooperators_previous_round = count_cooperators(observed_actions_previous_round)

    // The threshold to trigger cooperation in the current round is m-1 cooperators in the previous round.
    // This is because if m-1 cooperated, then one more cooperation (our own) could reach the threshold m.
    if num_cooperators_previous_round >= (m - 1):
      // If previous round was close to the threshold, cooperate to try and reach it.
      return "C"
    else:
      // If previous round had significantly fewer cooperators, defect to protect private payoff.
      return "D"

// Helper function (assumed to be available or implemented)
function count_cooperators(actions_list):
  count = 0
  for action in actions_list:
    if action == "C":
      count = count + 1
  return count

// Main tournament loop (simplified)
// For each player playing ATC:
//   Initialize cooperation_history = []
//   For round_t from 1 to r:
//     If round_t > 1:
//       observed_actions_prev = get_observed_actions_from_history(round_t - 1)
//     Else:
//       observed_actions_prev = [] // Empty for the first round
//
//     my_action = decide_action(round_t, observed_actions_prev)
//     record_my_action(round_t, my_action)
//     record_all_actions_for_round(round_t, my_action, other_players_actions) // Update global history
//
//     If round_t == r:
//       calculate_total_payoff()
```

---

This Adaptive Threshold Cooperation strategy provides a balanced approach, aiming for the collective good while safeguarding against exploitation, making it a strong contender in a dynamic tournament environment.
'''

description_COOPERATIVE_128 = '''
Here's a design for a cooperative strategy for the Collective Risk Dilemma, focusing on adaptiveness, robustness, and a clear cooperative intent, while acknowledging the constraints of the game.

## Strategy Name: "Adaptive Reward Seeking"

This strategy aims to foster cooperation by rewarding it when it's beneficial and collectively safe, while also protecting against exploitation. It learns from past outcomes to adjust its behavior.

### Core Principles:

1.  **Reward Cooperation:** When there's a clear indication that cooperation is leading to a positive collective outcome (i.e., the threshold `m` is met), we will lean towards cooperating to maximize shared rewards.
2.  **Risk Aversion (Conditional):** We are not purely altruistic. If cooperation consistently fails to meet the threshold, or if defection seems to be the dominant strategy for the group, we will defect to protect our individual endowment.
3.  **Observational Learning:** The strategy will track the success of cooperation in past rounds and adapt its cooperation probability based on these observations.
4.  **No Explicit Trust/Mistrust:** The strategy doesn't "trust" or "mistrust" in an emotional sense. Its decisions are based purely on observed outcomes and their implications for achieving the cooperative reward.

### 1. Decision Rules:

The core of this strategy is a dynamic probability of cooperating, adjusted based on the observed success of cooperation in previous rounds.

**Let `P_cooperate` be the probability of cooperating in the current round.**

*   **Initialization:** In the first round, we start with a moderate propensity to cooperate, assuming others might also be exploring cooperative behavior. `P_cooperate = 0.5` (or a value slightly higher, like `0.6`, to signal a cooperative intent, but not so high as to be easily exploited).

*   **Update Rule (After each round `t`):**
    *   Let `CooperationSuccess_t` be `1` if `m` or more players cooperated in round `t`, and `0` otherwise.
    *   Let `MyAction_t` be `1` if I cooperated in round `t`, and `0` if I defected.
    *   Let `NumCooperators_t` be the actual number of cooperators in round `t`.

    We will adjust `P_cooperate` based on a weighted average of past `CooperationSuccess`. A simple exponential moving average is suitable for this:

    `P_cooperate = α * P_cooperate_prev + (1 - α) * CooperationSuccess_t`

    where:
    *   `P_cooperate_prev` is the probability of cooperating from the previous round.
    *   `α` (alpha) is a smoothing factor (e.g., `0.8` or `0.9`). A higher `α` means the strategy is more influenced by the long-term trend, while a lower `α` makes it more responsive to recent outcomes.
    *   `CooperationSuccess_t` is the outcome of the *previous* round (0 or 1).

    **Refinement to the Update Rule (Crucial for Adaptability):**
    Instead of just using the binary `CooperationSuccess_t`, we should also consider the *impact* of our own action on achieving that success. This makes the strategy more sensitive to whether *our* cooperation was "necessary" or "redundant."

    Let's introduce a variable `GroupBenefitReceived` which is `k` if `m` or more cooperated, and `0` otherwise.

    A more nuanced update:
    *   If `GroupBenefitReceived` was `k` in the previous round: Increase `P_cooperate` slightly. This reinforces the idea that cooperation was beneficial.
    *   If `GroupBenefitReceived` was `0` in the previous round: Decrease `P_cooperate` slightly. This indicates that cooperation is not yielding the desired collective reward.

    The magnitude of the change can be controlled by a learning rate `η` (eta).

    **Revised Decision Logic:**

    In round `t`:
    1.  Calculate the target probability of cooperation `Target_P_cooperate` based on the history. A simple way to do this is:
        *   Let `AvgCooperationSuccess_t-1` be the average of `CooperationSuccess` over the last `N_history` rounds.
        *   `Target_P_cooperate = AvgCooperationSuccess_t-1` (or a more sophisticated function that maps average success to a desired probability). For instance, a simple linear mapping: `Target_P_cooperate = max(0, min(1, AvgCooperationSuccess_t-1))`.

    2.  **Action Selection:**
        *   Generate a random number `rand` between 0 and 1.
        *   If `rand < Target_P_cooperate`, play **C**.
        *   Else, play **D**.

    **Adapting `Target_P_cooperate` based on the *immediate* previous round:**

    *   Let `PrevCoopSuccess` be the outcome of round `t-1` (1 if `m` or more cooperated, 0 otherwise).
    *   Let `PrevMyAction` be my action in round `t-1` (1 for C, 0 for D).

    **Update of `Target_P_cooperate` for round `t`:**

    *   If `PrevCoopSuccess == 1`:
        *   `Target_P_cooperate_t = min(1.0, Target_P_cooperate_t-1 + η)` (We are rewarded, so increase our propensity to cooperate)
    *   If `PrevCoopSuccess == 0`:
        *   `Target_P_cooperate_t = max(0.0, Target_P_cooperate_t-1 - η)` (We are not rewarded, so decrease our propensity to cooperate)

    Here, `η` is a small learning rate (e.g., 0.05). This ensures that the strategy slowly drifts towards cooperation when it's successful and away from it when it's not.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   As stated above, initialize `Target_P_cooperate = 0.5` (or `0.6` to signal cooperative intent).
    *   Play C with probability `Target_P_cooperate`.
    *   No history to learn from yet.

*   **Last Round (t=r):**
    *   The goal in the last round is to maximize immediate payoff. There's no future to incentivize cooperation.
    *   **If `Target_P_cooperate` from the previous round is significantly high (e.g., > 0.8) and cooperation has been successful recently:** It *might* still be individually optimal to cooperate if the probability of hitting `m` is very high and the cost `(1-c_i)` is small. However, to be maximally self-interested and robust against any remaining uncertainty, the most robust strategy for the last round is to defect.
    *   **Decision for Last Round:** Always play **D**. This is a pure self-interest calculation for the final round, as there's no benefit to setting up future cooperation.

*   **When `m` is very close to `n`:** The probability of hitting `m` by cooperating alone is low. The strategy will naturally learn to defect as cooperation often fails.

*   **When `m` is 2:** This is very easy to achieve. The strategy will quickly learn that cooperation is beneficial and will predominantly cooperate.

### 3. Being Cooperative:

The strategy embodies cooperativeness through:

*   **Proactive initial cooperation:** Starting with a non-zero probability of cooperation signals a willingness to engage in collective benefit.
*   **Reinforcement of success:** When cooperation yields the collective reward (`k`), the strategy increases its tendency to cooperate. This directly rewards successful collective action.
*   **Adaptive learning, not punitive punishment:** Instead of immediately retaliating for a failed cooperative attempt (which might be due to others defecting), the strategy reduces its cooperation probability proportionally. This is a gentle drift away from cooperation rather than an immediate "punishment," which is more aligned with a long-term cooperative mindset. It acknowledges that a single failed round might not be a permanent state of affairs.
*   **Endowment Preservation:** While cooperative, the strategy is not naive. If cooperation consistently fails to deliver the reward, it will naturally shift towards defection to protect its own endowment, ensuring its continued participation and ability to contribute to future potential collective successes.

### Pseudocode:

```
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Strategy State
current_cooperation_probability: float (initially 0.5 or 0.6)
learning_rate (eta): float (e.g., 0.05)
history_buffer_size (N_history): int (e.g., 5 or 10 rounds for smoothing)
last_round_cooperation_success: int (0 or 1)

// Initialization Function
function initialize_strategy():
  current_cooperation_probability = 0.5  // Or 0.6 to signal initial intent
  last_round_cooperation_success = 0 // Assume no success before round 1

// Decision Function for Round t
function decide_action(round_number, past_actions_history, current_payoffs):
  // Handle Last Round: Always defect for maximum individual payoff
  if round_number == r:
    return "D"

  // Update cooperation probability based on previous round's outcome
  // This is a simplified update; a more robust version might use a buffer
  if round_number > 1:
    // Logic to determine last_round_cooperation_success based on history
    // For simplicity here, let's assume it's maintained by an external tracker
    // or derived from current_payoffs and rules

    if last_round_cooperation_success == 1:
      current_cooperation_probability = min(1.0, current_cooperation_probability + learning_rate)
    else:
      current_cooperation_probability = max(0.0, current_cooperation_probability - learning_rate)

  // Select action based on current probability
  random_value = generate_random_float(0, 1)
  if random_value < current_cooperation_probability:
    return "C"
  else:
    return "D"

// --- External Tracking (for state management) ---
// This would be part of the game engine or another module
function update_strategy_state(round_number, my_action, other_actions):
  num_cooperators = count_cooperators(my_action, other_actions)
  if num_cooperators >= m:
    last_round_cooperation_success = 1
  else:
    last_round_cooperation_success = 0

// Helper function to count cooperators
function count_cooperators(my_action, other_actions):
  count = 0
  if my_action == "C":
    count += 1
  for action in other_actions:
    if action == "C":
      count += 1
  return count
```

This strategy is designed to be a good generalist. It doesn't rely on specific patterns of play from others but adapts to the emergent behavior of the group over time, aiming to find a stable or beneficial equilibrium.
'''

description_AGGRESSIVE_1 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to exploit opportunities for high payoffs while being resilient to defection. This strategy is called "Aggressive Opportunist."

## Aggressive Opportunist Strategy

The core philosophy of Aggressive Opportunist is to maximize personal gain by leaning towards defection, but to cautiously cooperate when there's a strong likelihood of achieving the cooperative reward, especially when it offers a significantly higher payout than defection. It prioritizes exploiting the collective risk reward when achievable, but will not be exploited by others.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in round `t` is based on a calculated **"Cooperation Trigger Threshold" (CTT)**.

**Cooperation Trigger Threshold (CTT):** This threshold represents the *minimum perceived probability* that at least `m` players will cooperate in the current round, including myself, for me to consider cooperating.

**Decision Logic:**

*   **If `t` is the first round (t=1):**
    *   I will **Defect (D)**. This is an aggressive initial stance, aiming to establish a baseline payoff of 1 and observe others' behavior without immediately risking an endowment.
*   **If `t` is the last round (t=r):**
    *   I will **Defect (D)**. The future is irrelevant. In the final round, defection guarantees a payoff of 1, whereas cooperation (unless the threshold is guaranteed to be met) risks losing the endowment with no future reward.
*   **For any intermediate round `t` (1 < t < r):**
    *   I will **Cooperate (C)** if and only if:
        *   **My perceived probability of reaching the cooperation threshold (`m` cooperators) is greater than or equal to my personal "Aggression Factor" (AF).**
        *   **The expected payoff of cooperating, given my current belief about others, is strictly greater than the payoff of defecting.**

**Calculating the Perceived Probability of Reaching the Threshold:**

This is the most dynamic part of the strategy and relies on historical data. I maintain beliefs about the likelihood of each other player cooperating. Let `P_i(t-1)` be my belief (probability) that player `i` will cooperate in round `t-1`.

To simplify, for this aggressive strategy, I will use a heuristic:

*   **If player `i` cooperated in the previous round `t-1` AND the threshold was met:** I will assume they have a high probability of cooperating again in round `t`. Let's assign this a probability of `p_high = 0.8`.
*   **If player `i` cooperated in the previous round `t-1` BUT the threshold was NOT met:** This is more ambiguous. They might be trying to cooperate but failed due to others defecting. I will assign a moderate probability: `p_medium = 0.5`.
*   **If player `i` defected in the previous round `t-1`:** I will assume they are likely to defect again. I assign a low probability: `p_low = 0.2`.
*   **For players who have always defected or who I have no reliable history for (e.g., early rounds beyond round 1):** I will use a baseline probability of `p_baseline = 0.3`.

**Aggression Factor (AF):** This is a tunable parameter that dictates how aggressive I am in initiating cooperation. A higher AF means I require a stronger signal before cooperating. For an aggressive strategy, a value between **0.6 and 0.75** is appropriate. Let's set `AF = 0.7` initially.

**Estimating the Probability of Reaching `m` Cooperators:**

This is a complex calculation involving combinations of probabilities. For simplicity and to maintain computational efficiency in an algorithmic implementation, Aggressive Opportunist will use a simplified heuristic:

1.  **Count of "Likely Cooperators":** Sum the probabilities of cooperation for all players *other than myself*, using the `p_high`, `p_medium`, `p_low`, or `p_baseline` values. Let this sum be `S_other`.
2.  **My Contribution:** Since I am considering cooperating, I contribute 1 to the count of potential cooperators.
3.  **Estimated Total Cooperators:** My estimated probability of reaching the threshold is then approximated by considering the likelihood that `S_other` is "close enough" to `m-1`.
    *   **Simplified Threshold Check:** If `S_other + 1 >= m` (meaning the sum of individual probabilities of others cooperating, plus my own cooperation, is at least `m`), I will consider cooperating. This is a very loose approximation, but aligned with the aggressive mindset of seizing opportunity.
    *   **More Refined Approach (Optional, for robustness):** A more rigorous approach would involve simulating possible combinations of cooperation from other players based on their assigned probabilities. However, for pure aggression, the simpler heuristic often suffices to trigger cooperation when the *average* behavior of others suggests it's likely.

**Expected Payoff Calculation:**

*   **Expected Payoff of Cooperating (EPC):**
    `EPC = (Probability of m or more cooperators) * (0 + k) + (Probability of fewer than m cooperators) * (0)`
    *(Assuming I cooperate, my endowment is 0)*
    For Aggressive Opportunist, `Probability of m or more cooperators` will be approximated by the `CTT` calculation. If `CTT >= AF`, I will set `Probability of m or more cooperators` to 1 for this calculation, as I'm triggering cooperation based on that belief.
    So, if `CTT >= AF`: `EPC = 1 * k`

*   **Expected Payoff of Defecting (EPD):**
    `EPD = 1` (My endowment is 1, and no reward is gained if the threshold isn't met, which is the most likely scenario if I defect).

**Decision Summary for Intermediate Rounds:**

1.  **Calculate `CTT` based on historical data and probability heuristics.**
2.  **If `CTT >= AF` AND `k > 1` (i.e., cooperation offers a potential upside):**
    *   **Cooperate (C).** This is the primary condition for cooperation – a strong signal of sufficient cooperation and a positive reward.
3.  **Otherwise:**
    *   **Defect (D).** This is the default aggressive stance.

### 2. Handling Edge Cases

*   **First Round (t=1):** Always **Defect (D)**. This is a conservative opening for an aggressive strategy, as there's no history to rely on, and it avoids an immediate endowment loss. The goal is to gather information.
*   **Last Round (t=r):** Always **Defect (D)**. The future reward is irrelevant, so securing the endowment of 1 is paramount.
*   **Initialization of Beliefs:** For any player `i` for whom there's no history (e.g., in round 2 for players who didn't play in round 1 due to some tournament constraint, or if a player's history is erased), use `p_baseline = 0.3`.
*   **Sudden Changes in Opponent Behavior:** The probability heuristics (p_high, p_medium, p_low) are designed to adapt. If a player consistently defects after cooperating, their probability will decrease, leading me to defect more often against them. Conversely, if they consistently cooperate and the threshold is met, their probability will remain high.

### 3. Aggressive Mindset Alignment

*   **Default to Defection:** The strategy defaults to defecting, minimizing risk and maximizing immediate personal gain when cooperation is uncertain.
*   **Opportunistic Cooperation:** Cooperation is not a norm or a gesture of good faith. It's a calculated risk taken only when the perceived probability of success is high enough to justify the gamble, and the reward (`k`) is significantly greater than the guaranteed payoff from defection.
*   **Exploiting the Reward:** The primary goal when cooperating is to secure the `k` reward. The strategy doesn't aim to "build trust" or "maintain stability" beyond what's necessary to achieve that reward.
*   **"No Free Lunch" Principle:** The strategy assumes others might defect. It only cooperates when the expected payoff is demonstrably better, and it won't sacrifice its own endowment unless it has a strong belief that the collective reward will be achieved.
*   **High Aggression Factor (AF):** A higher `AF` (like 0.7) means I am less likely to cooperate preemptively. I need a strong signal. This prevents me from being an easy target for exploiters.

### Pseudocode Example

```python
class AggressiveOpportunistStrategy:
    def __init__(self, n, m, r, k, aggression_factor=0.7):
        self.n = n
        self.m = m
        self.r = r
        self.k = k
        self.aggression_factor = aggression_factor
        self.player_histories = {i: [] for i in range(n)} # Stores 'C' or 'D' for each player

    def get_probability_for_player(self, player_index, current_round):
        if player_index not in self.player_histories or not self.player_histories[player_index]:
            return 0.3  # p_baseline

        last_action = self.player_histories[player_index][-1]
        threshold_met_last_round = False
        if len(self.player_histories[player_index]) > 0:
             # Check if threshold was met in the round *before* the last action was recorded
             # This requires a bit more history tracking or a slight adjustment in how we define "threshold_met_last_round"
             # For simplicity here, let's assume we know if the threshold was met in the previous round.
             # A more robust implementation would store round outcome (threshold met/not met)
             pass # Placeholder for more complex threshold met tracking.

        if last_action == 'C':
            # This is a simplified heuristic. A truly robust system would check if the threshold was actually met.
            # For this aggressive version, we simplify: if they cooperated, we lean towards higher prob.
            # If we *know* threshold was met, p_high=0.8. If they cooperated but threshold wasn't met, p_medium=0.5.
            # Without explicit round outcome storage, we simplify:
            return 0.7 # A middle ground, leaning towards high if they tried to cooperate.
        else: # last_action == 'D'
            return 0.2 # p_low

    def decide_action(self, current_round, all_previous_actions):
        # Update histories based on all_previous_actions (a list of actions from all players in previous round)
        # This needs to be called *after* the round finishes, before the next round's decision.
        # For the decision function itself, we assume history is updated.

        if current_round == 1 or current_round == self.r:
            return 'D'

        # Calculate current perception of others' cooperation probabilities
        sum_of_others_probs = 0.0
        num_other_players = self.n - 1
        for i in range(self.n):
            if i != self.me_player_index: # Assuming self.me_player_index is known
                sum_of_others_probs += self.get_probability_for_player(i, current_round)

        # Aggressive Heuristic for CTT: If the sum of probabilities + my cooperation >= m
        # This is a loose approximation but aggressive.
        # A more accurate CTT would involve combinatorial probability.
        perceived_threshold_likelihood = (sum_of_others_probs + 1.0) # My cooperation adds 1.0 to likelihood count

        # Decision
        if perceived_threshold_likelihood >= self.m and self.k > 1:
            # Aggressive opportunist: Cooperate if threshold likely met AND reward exists.
            # Further check expected payoff:
            # Expected Payoff of Cooperating (EPC) = 1 * k (if threshold met, which we assume by triggering)
            # Expected Payoff of Defecting (EPD) = 1
            if self.k > 1: # If cooperating offers a potential upside
                 return 'C'

        return 'D' # Default aggressive action

    # Method to update histories after a round is played
    def update_histories(self, round_actions, threshold_met):
        # round_actions is a dict like {player_idx: action, ...}
        # threshold_met is a boolean
        for player_idx, action in round_actions.items():
            if player_idx in self.player_histories:
                self.player_histories[player_idx].append(action)
            else:
                self.player_histories[player_idx] = [action] # Initialize if new
        # In a real implementation, you would also store the outcome (threshold_met)
        # to refine the get_probability_for_player logic.
```

This strategy is designed to be decisive and opportunistic. It avoids being a passive cooperatior and instead looks for clear signs of collective success to maximize its own payoff.
'''

description_AGGRESSIVE_2 = '''
This is a fascinating strategic challenge! The Collective Risk Dilemma, with its simultaneous moves and repeated nature, creates a fertile ground for aggressive yet adaptive strategies. The core tension lies in the temptation to defect for a higher individual payoff when the cooperation threshold is met, versus the risk of everyone defecting and yielding only the private payoff.

My aggressive strategy, which I'll call **"The Threshold Predator,"** is designed to exploit the cooperative potential while ruthlessly capitalizing on defectors and minimizing personal loss. It's built on a principle of calculated risk and aggressive retaliation.

## The Threshold Predator Strategy

**Core Philosophy:** "I will sow the seeds of cooperation, but I will harvest the rewards of defection when possible, and I will punish free-riders relentlessly."

This strategy aims to:
1.  **Encourage initial cooperation:** To unlock the possibility of the collective reward.
2.  **Exploit the opportunity:** To defect when the threshold is likely to be met by others, maximizing personal gain.
3.  **Deter future free-riding:** By punishing those who defect when cooperation is crucial.
4.  **Minimize personal risk:** To ensure survival and decent payoff even in unfavorable scenarios.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision for player `i` in round `t` will be based on two primary factors:

*   **The perceived probability of reaching the cooperation threshold (m):** This is estimated based on the history of cooperation.
*   **The history of my own and others' actions:** Specifically, whether others have exploited my cooperation in the past.

**Detailed Decision Logic for Round `t`:**

*   **If `t` is the first round (t=1): Cooperate (C)**
    *   **Reasoning:** In the absence of any historical data, it's strategically optimal to initiate cooperation. This establishes a baseline and reveals information about other players. Defecting in the first round guarantees a payoff of 1 and offers no information about whether others are willing to cooperate. Cooperating signals a willingness to engage in the collective effort, potentially incentivizing others.

*   **If `t` is the last round (t=r): Defect (D)**
    *   **Reasoning:** There are no future rounds to build trust or establish norms. The only consideration is maximizing immediate payoff. Since there's no threat of retaliation in subsequent rounds, the optimal strategy is to exploit any cooperation from others. If others cooperate, I get `1 + k`. If others defect, I get `1`. Either way, defecting guarantees a payoff of at least 1, and potentially `1 + k`.

*   **For intermediate rounds (1 < t < r):**

    *   **Calculate Estimated Cooperation Likelihood (ECL):**
        *   Let `C_t-1` be the number of players who cooperated in the previous round `t-1`.
        *   Let `N_t-1` be the total number of players who took an action in round `t-1` (which is `n`).
        *   Let `MyCoopHistory` be a binary variable: 1 if I have cooperated at least once in previous rounds, 0 otherwise.
        *   Let `OthersCoopHistory` be a binary variable: 1 if at least one other player has cooperated at least once in previous rounds, 0 otherwise.

        *   **Threshold to Defect on:** We need a threshold that signifies a high probability of success. A good heuristic is to consider the ratio of actual cooperators to the required minimum, but also to factor in the potential for individual gain.
        *   **My Dominant Logic:** I will defect if I believe the threshold `m` will be met *and* if I perceive that exploiting cooperation is more profitable than risking a failed cooperation.

    *   **My Decision Rule in Intermediate Rounds:**

        1.  **Default to Cooperate if `C_t-1 < m - 1`:**
            *   **Reasoning:** If the previous round had significantly fewer than `m` cooperators, it's risky to assume the threshold will be met. Cooperating keeps the possibility alive for future rounds and doesn't trigger my "punishment" mechanism. It's a cautious move when the collective effort is clearly failing.

        2.  **Defect if `C_t-1 >= m - 1` (and `t != r`):**
            *   **Reasoning:** This is the aggressive part. If the previous round *nearly* met the threshold (or met it), there's a high probability that others will cooperate again, leading to the collective reward. In this scenario, I will defect to secure the higher individual payoff of `1 + k`.
                *   **Example:** If `m=3` and `C_t-1 = 2` or `C_t-1 = 3`, I will defect.

        3.  **Conditional Defection based on Past Exploitation:**
            *   **If I have cooperated in the past (`MyCoopHistory == 1`) AND at least one other player has defected when I cooperated in the past:**
                *   **Consider Defecting even if `C_t-1 < m - 1` (but not if `C_t-1 == 0`):** If I've been exploited (I cooperated, others defected, and we failed), I will be more inclined to defect in future rounds. This is a punitive measure.
                *   **Specific trigger:** If `C_t-1 >= 1` (meaning someone else *could* have cooperated but didn't, or they did and it wasn't enough), and I *personally* defected in the previous round when `C_t-1` was high, *and* in a prior round where I cooperated, at least one other player defected, then I am more aggressive.

### 2. Handling Edge Cases

*   **First Round (t=1): Cooperate (C)**
    *   As explained above, this is about gathering information and initiating the game with a cooperative stance.

*   **Last Round (t=r): Defect (D)**
    *   No future consequences, so maximize immediate gain.

*   **What if `m` is very close to `n` (e.g., `m = n-1`) and others consistently defect?**
    *   My strategy will naturally adapt. If `C_t-1` is always low, I will default to cooperating (Rule 1 in intermediate rounds) to keep the possibility open, or defect in the last round. The punitive mechanism will also kick in if I'm consistently exploited.

*   **What if `m` is very small (e.g., `m = 2` and `n` is large)?**
    *   My strategy will become very aggressive. If `C_t-1 >= m-1` (i.e., `C_t-1 >= 1`), I will defect. This means if *anyone* else cooperates, I'll try to defect and capture the reward. This is aggressive but has a high chance of failure if too many adopt this. The adaptive nature is key: if this aggressive defecting leads to consistently low payoffs (because `m` is never met), my strategy might have to adjust to be slightly more cooperative. However, for *this* aggressive strategy, the inclination is to exploit even small cooperative signals.

### 3. Being Aggressive

The aggressiveness of "The Threshold Predator" is manifest in:

*   **Early Exploitation of Near-Success:** The rule `C_t-1 >= m - 1` to defect in intermediate rounds is aggressively designed to capitalize on any sign of collective effort getting close to the threshold. It prioritizes securing `1 + k` over the slim chance of a slightly higher payoff (if `m` is met and I defected) or a lower payoff (if `m` is not met and I cooperated).
*   **Punitive Defection:** The conditional defection based on past exploitation directly punishes players who have benefited from my cooperation without reciprocating. This is a clear aggressive stance to deter free-riders.
*   **Last Round All-Out Defection:** This is the ultimate expression of self-interest, where all pretense of cooperation is abandoned.
*   **Prioritizing Personal Gain:** The strategy is fundamentally designed to maximize *my* payoff. While it initiates cooperation, it does so with the immediate intention of being able to defect later if profitable.

### Pseudocode Representation:

```python
# Player i's strategy function
def decide_action(current_round, n, m, k, history):
    """
    history: a list of dictionaries, where each dict represents a round's actions
             e.g., [{'player_id': 0, 'action': 'C'}, {'player_id': 1, 'action': 'D'}, ...]
             and includes 'num_cooperators' for that round.
    """

    t = current_round
    
    # 1. Handle First Round
    if t == 1:
        return 'C'

    # 2. Handle Last Round
    if t == n_rounds: # Assuming n_rounds is accessible or passed
        return 'D'

    # 3. Intermediate Rounds Logic
    previous_round_data = history[-1] if history else None
    
    if previous_round_data is None: # Should not happen for t > 1 if history is maintained
        return 'C' # Fallback to cooperate if no history (error case)

    num_cooperators_prev = previous_round_data['num_cooperators']
    
    # Analyze personal and others' past behavior for exploitation
    my_cooperation_history = any(r['action'] == 'C' for r in history if r['player_id'] == my_player_id) # Assuming my_player_id is known
    others_exploited_me = False
    if my_cooperation_history:
        for r_idx, round_data in enumerate(history):
            if round_data['player_id'] == my_player_id and round_data['action'] == 'C':
                # Check if in *that same round*, others defected and we failed
                round_actions = history[r_idx]['actions'] # List of {'player_id': x, 'action': y} for that round
                if sum(1 for p_action in round_actions if p_action['action'] == 'C') < m:
                    # And if at least one other player defected
                    if any(p_action['action'] == 'D' for p_action in round_actions if p_action['player_id'] != my_player_id):
                        others_exploited_me = True
                        break

    # Decision Rules:
    
    # Rule 1: Default to Cooperate if previous round was significantly below threshold
    if num_cooperators_prev < m - 1:
        # Punitive consideration: if I've been exploited, I might defect even here
        if others_exploited_me and num_cooperators_prev > 0: # Punish if I was exploited and someone *could* have helped
             return 'D'
        else:
             return 'C'

    # Rule 2: Aggressively Defect if previous round was near or at threshold
    # This also implicitly handles the case where m=2 and num_cooperators_prev >= 1
    if num_cooperators_prev >= m - 1:
        return 'D'
        
    # Fallback (should ideally not be reached if logic is exhaustive for intermediate)
    return 'C' # Default to C if no other condition met, for safety.

```

**Self-Correction and Robustness:**
While the strategy is aggressive, its robustness comes from:
*   **Information Gathering:** The first round's cooperation provides crucial data.
*   **Adaptive Punishment:** The `others_exploited_me` condition ensures that punishment is not arbitrary but a direct response to being exploited.
*   **Graduated Response:** The thresholds for cooperation and defection are based on the *previous round's* outcome, allowing for gradual shifts rather than abrupt changes.
*   **Final Round Logic:** The absolute defection in the last round ensures that the player doesn't leave potential gains on the table due to past cooperative tendencies.

This "Threshold Predator" strategy aims to be a dominant force by aggressively pursuing individual gain while intelligently navigating the collective risk. It's not about pure altruism or blind defection, but a calculated approach to maximize personal payoff in a repeated game of uncertain cooperation.
'''

description_AGGRESSIVE_3 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging opportunities for high collective payoffs. This strategy is adaptive and robust, focusing on exploitation and calculated risks.

## Strategy: The Opportunistic Predator

This strategy is built on the principle of **"defect until defection is no longer profitable, then exploit the system, and be prepared to defect again if stability is threatened."** It aims to capture the highest possible payoffs by being a net beneficiary of cooperation when it occurs, but never being the sole contributor to that cooperation if it can be avoided.

### Core Philosophy:

*   **Maximize personal gain:** The primary objective is to accumulate the highest total payoff for myself.
*   **Exploit others' cooperation:** If others are cooperating enough to meet the threshold, I will defect to secure the highest individual payoff (1 + k).
*   **Punish perceived exploitation:** If I believe others are consistently defecting and I'm bearing the burden of cooperation without sufficient reciprocation, I will retaliate by defecting.
*   **Calculated cooperation:** Cooperation is a tool, not a virtue. I will only cooperate when it's demonstrably the best path to a high payoff for myself, either by ensuring the threshold is met (and I benefit from the reward) or by signaling future intent (in a limited, self-serving way).
*   **No altruism:** This strategy is purely self-interested.

### 1. Decision Rules:

My decision in any given round, `t`, will depend on my assessment of the current situation and the actions taken in previous rounds.

**Current Round Decision (for player `i`):**

1.  **Initial Aggression (Round 1):**
    *   **Action:** Defect (D).
    *   **Rationale:** In the first round, there's no history to learn from. Defecting is the default aggressive stance. It immediately secures a baseline payoff of 1 and signals a lack of willingness to be exploited.

2.  **Post-Round 1 - Dynamic Decision Logic:**
    *   **Calculate observed cooperation count (`cooperators_observed_t-1`):** Count the number of players who played 'C' in round `t-1`.
    *   **Calculate my expected payoff for Cooperating (`E_C`):** This is a prediction of my payoff if I choose 'C' in round `t`.
        *   If `cooperators_observed_t-1` + 1 (me cooperating) >= `m`: My payoff will be `0 + k`.
        *   If `cooperators_observed_t-1` + 1 (me cooperating) < `m`: My payoff will be `0`.
    *   **Calculate my expected payoff for Defecting (`E_D`):** This is a prediction of my payoff if I choose 'D' in round `t`.
        *   If `cooperators_observed_t-1` >= `m`: My payoff will be `1 + k`. (I benefit from others' cooperation)
        *   If `cooperators_observed_t-1` < `m`: My payoff will be `1`. (I keep my endowment, no reward)

    *   **Decision Tree:**
        *   **If `E_D > E_C`:** Defect (D). This is my preferred outcome. I get a higher payoff by defecting.
        *   **If `E_C > E_D`:**
            *   **Sub-condition: If `cooperators_observed_t-1` >= `m` AND `E_C` is *only slightly* better than `E_D` (e.g., difference < 0.1):** Defect (D). The risk of cooperating for minimal gain isn't worth it; I can still benefit from others' cooperation.
            *   **Sub-condition: If `cooperators_observed_t-1` < `m` AND `E_C` is significantly better than `E_D` (e.g., difference >= 0.1):** Cooperate (C). This is the *only* scenario where I might cooperate. It's when my cooperation is the *key* to unlocking the reward `k` for myself, and the payoff difference is substantial enough to justify the risk. I am essentially buying the reward.
        *   **If `E_C == E_D`:** Defect (D). When the payoffs are equal, the more aggressive, less risky option is to defect.

    *   **Aggression Adjustment Factor:** To add more aggression and robustness, I will introduce a dynamic threshold. Let's call it `cooperation_tolerance`.
        *   Initialize `cooperation_tolerance = m - 1` (meaning I need *at least* `m-1` others to cooperate before I'd even consider it).
        *   If in the previous round (`t-1`), the threshold `m` was met *and* I defected, and my payoff was `1 + k`: `cooperation_tolerance` remains the same.
        *   If in the previous round (`t-1`), the threshold `m` was *not* met, and I defected: `cooperation_tolerance` increases by 1 (making me less likely to cooperate in the future).
        *   If in the previous round (`t-1`), the threshold `m` was met, and I cooperated (this shouldn't happen with the main logic, but as a safeguard): `cooperation_tolerance` decreases by 1 (making me more inclined to cooperate if it proved beneficial).

    *   **Revised Decision Logic incorporating `cooperation_tolerance`:**
        *   **If `cooperators_observed_t-1` >= `cooperation_tolerance`:** This indicates a strong tendency towards cooperation among others.
            *   **If `cooperators_observed_t-1` >= `m`:** Defect (D). I am guaranteed to benefit from the reward `k`.
            *   **If `cooperators_observed_t-1` < `m`:**
                *   **If `cooperators_observed_t-1` + 1 >= `m`:** Cooperate (C). My cooperation is crucial to unlock the reward `k`. This is a calculated risk.
                *   **If `cooperators_observed_t-1` + 1 < `m`:** Defect (D). Even with some cooperation, it's not enough to trigger the reward, and cooperating would be a pure loss.
        *   **If `cooperators_observed_t-1` < `cooperation_tolerance`:** This suggests a general reluctance to cooperate among others.
            *   **Action:** Defect (D). I will not be the one to try and jumpstart cooperation.

### 2. Handling Edge Cases:

*   **Round 1:** As stated above, always **Defect (D)**. This is the most aggressive starting point.
*   **Last Round (Round `r`):** The strategy remains the same. There's no future to consider, so the current round's payoff is the only thing that matters. The logic above will execute based on the history of rounds `1` to `r-1`. The aggressive default of defecting unless a high payoff is guaranteed by cooperating will continue.
*   **Insufficient Players (`n < 2` or `m <= 1` or `m >= n`):** The problem statement specifies `n ≥ 2` and `1 < m < n`. If these parameters were to be violated by the game setup, the strategy would need a failsafe. For the given constraints, we assume valid parameters.
*   **Sustained Defection:** If the game consistently results in fewer than `m` cooperators, and I am always defecting, my `cooperation_tolerance` will increase, making me even less likely to cooperate. This is the "predator" aspect – I'm waiting for others to reveal their cooperation patterns.
*   **Sustained Cooperation (by others):** If the other `n-1` players consistently cooperate such that `m` is met or exceeded, I will always defect to maximize my payoff to `1 + k`.

### 3. Aggressive Nature:

*   **Default to Defection:** Cooperation is a last resort, only undertaken when it demonstrably guarantees a higher payoff *for me* than defecting.
*   **Exploitative Payoff Maximization:** The strategy actively seeks to benefit from the collective reward (`k`) without contributing to it if others are already doing the heavy lifting.
*   **Punitive `cooperation_tolerance`:** The `cooperation_tolerance` increases when cooperation fails, making the strategy more resistant to being the "first mover" in future rounds. It punishes environments where cooperation is scarce.
*   **No Signaling of Intent:** There's no attempt to build trust or signal cooperative intentions. My actions are purely reactive to maximize my immediate and predictable future gains.

### Pseudocode:

```
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// My Player Index (assumed to be known)
my_player_index

// State variables
cooperation_tolerance = m - 1 // Initial tolerance
history = [] // List of dictionaries: [{'player_index': i, 'action': 'C'/'D'}, ...]

// Function to get my action for round t
function get_my_action(t, history):

    // Round 1: Always defect aggressively
    if t == 1:
        return 'D'

    // --- Post Round 1 Logic ---

    // Count cooperators from the *previous* round (t-1)
    cooperators_observed_t_minus_1 = 0
    for past_round_actions in history:
        if past_round_actions['round'] == t - 1:
            for player_action in past_round_actions['actions']:
                if player_action['action'] == 'C':
                    cooperators_observed_t_minus_1 += 1
            break // Only need to look at the last round's actions

    // Update cooperation_tolerance based on previous round's outcome
    // (This update happens *after* the decision for t-1 was made, to influence t)
    was_threshold_met_t_minus_1 = (cooperators_observed_t_minus_1 >= m)
    my_action_t_minus_1 = 'D' // Assume defect if not explicitly found (for tolerance calculation)
    for past_round_actions in history:
        if past_round_actions['round'] == t - 1:
            for player_action in past_round_actions['actions']:
                if player_action['player_index'] == my_player_index:
                    my_action_t_minus_1 = player_action['action']
                    break
            break

    // Aggression Adjustment:
    // If threshold was met and I defected, tolerance stays same.
    // If threshold was NOT met and I defected, increase tolerance (less likely to cooperate next).
    // If threshold WAS met and I cooperated (shouldn't happen often with main logic), decrease tolerance.
    if not was_threshold_met_t_minus_1 and my_action_t_minus_1 == 'D':
        cooperation_tolerance += 1
    elif was_threshold_met_t_minus_1 and my_action_t_minus_1 == 'C':
        cooperation_tolerance -= 1
    // Ensure tolerance doesn't become too low or too high unnecessarily
    cooperation_tolerance = max(m - 1, min(n - 1, cooperation_tolerance))


    // Decision making for round t:

    // Scenario 1: Others are cooperating enough to potentially meet the threshold
    if cooperators_observed_t_minus_1 >= cooperation_tolerance:
        // If threshold is already met by others
        if cooperators_observed_t_minus_1 >= m:
            return 'D' // Exploit: benefit from reward, keep endowment
        // If my cooperation is crucial to meet the threshold
        elif cooperators_observed_t_minus_1 + 1 >= m:
            return 'C' // Calculated Risk: cooperate to secure the reward for myself
        // If even with my cooperation, threshold won't be met
        else:
            return 'D' // No reward, no point cooperating

    // Scenario 2: Others are not cooperating enough
    else:
        return 'D' // Aggressive default: don't be the one to initiate cooperation


// Example of history update (this would happen after each round)
// history.append({'round': t, 'actions': [... my_action_t, ... other_players_actions]})

return get_my_action(current_round, game_history)

```
'''

description_AGGRESSIVE_4 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the cooperative mechanism to its fullest extent. The core principle is to be a "disruptor" and "opportunist," pushing for cooperation when it's advantageous and defecting when it's safe or individually profitable.

## Aggressive Strategy: "The Opportunistic Leverager"

This strategy is designed to be aggressive by:
*   **Initiating cooperation strategically:** Not out of altruism, but to trigger the group reward and then potentially capitalize on others' continued cooperation.
*   **Punishing non-compliance swiftly and severely:** Discouraging free-riding by immediately returning to defection.
*   **Maximizing personal gain:** Always prioritizing the outcome that yields the highest individual payoff in a given round, given the observed environment.

### 1. Decision Rules

The strategy's decisions are driven by a calculated assessment of the current round's potential payoff, informed by the game parameters and historical observations.

**Primary Logic:**

The decision to Cooperate (C) or Defect (D) in any given round `t` depends on a calculated `Target_Cooperation_Level` and a `Cooperation_Threshold_Met`.

**Key Concepts:**

*   **`n` (Number of Players):** Total players.
*   **`m` (Minimum Cooperators):** The threshold for the group reward.
*   **`k` (Reward Factor):** The multiplier for the reward.
*   **`r` (Number of Rounds):** Total rounds.
*   **`t` (Current Round):** The current round number (1-indexed).
*   **`Observed_Cooperators_t-1`:** The number of players who cooperated in the previous round (`t-1`).
*   **`My_Action_t-1`:** My action in the previous round (`t-1`).
*   **`Group_Payoff_Threshold`:** The minimum number of cooperators required for the reward: `m`.
*   **`Reward_Bonus`:** The payoff multiplier: `k`.
*   **`My_Private_Payoff`:** The payoff if I defect and cooperation threshold is *not* met: `1`.
*   **`My_Coop_Cost`:** The cost of my cooperation: `1`.
*   **`My_Coop_Reward`:** The reward for my cooperation if threshold is met: `k`.

**Decision Logic in Round `t`:**

1.  **If `t == 1` (First Round):**
    *   **Default to Cooperate (C).**
    *   **Rationale:** To signal a willingness to engage and attempt to trigger the group reward from the outset. This is an aggressive opening move to test the waters and potentially establish a cooperative norm that can be exploited.

2.  **If `t > 1`:**
    *   **Calculate `Potential_Payoff_Cooperate`:**
        *   If `Observed_Cooperators_t-1 >= m`: `(1 - My_Coop_Cost) + My_Coop_Reward = k` (assuming I also cooperated).
        *   If `Observed_Cooperators_t-1 < m`: `(1 - My_Coop_Cost) = 0`. (This is simplified, the true payoff is `1-1=0` if defect, but `1-1=0` if I cooperated and it failed. The actual private payoff is `1`, so if I cooperate and fail, my payoff is `0`.)
        *   More precisely, if I **Cooperate** in round `t`:
            *   If `Observed_Cooperators_t-1 + 1 >= m` (meaning my cooperation tips the scales): `Payoff = (1 - 1) + k = k`
            *   If `Observed_Cooperators_t-1 + 1 < m` (meaning my cooperation wasn't enough): `Payoff = (1 - 1) = 0`

    *   **Calculate `Potential_Payoff_Defect`:**
        *   If `Observed_Cooperators_t-1 >= m`: `My_Private_Payoff + k = 1 + k` (I get my private endowment AND the group reward, as someone else's cooperation met the threshold).
        *   If `Observed_Cooperators_t-1 < m`: `My_Private_Payoff = 1` (I get my private endowment, no group reward).

    *   **Decision:**
        *   **If `Potential_Payoff_Defect > Potential_Payoff_Cooperate`:**
            *   **Play Defect (D).**
            *   **Rationale:** This is the core of the aggressive, opportunistic play. If defecting yields a higher payoff than cooperating, I will defect, regardless of the previous round's outcome. This ensures I always maximize my immediate gain.
        *   **Else (`Potential_Payoff_Cooperate >= Potential_Payoff_Defect`):**
            *   **Play Cooperate (C).**
            *   **Rationale:** If cooperating offers at least as good a payoff as defecting, I will cooperate. This is usually when cooperation is guaranteed to trigger the reward (because others cooperated heavily last round) or when my cooperation is crucial to triggering the reward and the payoff is high. This is a calculated cooperation to ensure the reward is secured, from which I can then leverage more defection opportunities.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):**
    *   **Action:** Cooperate (C).
    *   **Reasoning:** As stated above, this is an aggressive probe. It aims to be a catalyst for cooperation and test the environment. It’s a calculated risk to initiate cooperation and potentially unlock higher payoffs for everyone, which I can then exploit.

*   **Last Round (`t = r`):**
    *   **Action:** **Always Defect (D).**
    *   **Reasoning:** In the final round, there are no future rounds to consider for reciprocity or punishment. The only goal is to maximize individual payoff for this single round. Defecting guarantees the private endowment of `1` and leverages any potential group reward triggered by others. There's no incentive to cooperate as there's no future benefit to a cooperative atmosphere.

*   **Consecutive Failures to Meet Threshold (`Observed_Cooperators_t-1 < m` and `Observed_Cooperators_t-2 < m` etc.):**
    *   **Action:** Continue applying the primary logic.
    *   **Rationale:** The strategy is designed to be resilient. If cooperation consistently fails to meet the threshold, the `Potential_Payoff_Defect` will likely be higher than `Potential_Payoff_Cooperate` (which will yield 0). This will naturally lead to repeated defections. However, if a sudden surge in cooperation by others occurs, the `Potential_Payoff_Cooperate` might become attractive, and the strategy will opportunistically re-engage.

*   **Near Misses (`Observed_Cooperators_t-1 = m-1`):**
    *   **Action:** Cooperate (C).
    *   **Reasoning:** If I can be the one to push the group over the threshold (`m`) and my cooperation payoff (`k`) is greater than defecting payoff (`1`), I will cooperate. This is aggressive because it's a strategic move to *ensure* the group reward is met, thereby benefiting from it immediately.

### 3. Being Aggressive

The aggressiveness of "The Opportunistic Leverager" is evident in several ways:

*   **Proactive Initiation:** Starting with cooperation in the first round is not passive; it's an attempt to influence the game's trajectory.
*   **Unwavering Self-Interest:** The decision logic prioritizes the higher individual payoff in *every* round (except the initial probe). This means if defecting is better, I will defect, regardless of what others did.
*   **Exploitation of Cooperation:** When others cooperate enough to trigger the reward, the strategy will often defect to maximize its personal gain (`1 + k`). This is a pure exploitation of the group's efforts.
*   **No Forgiveness (in the traditional sense):** The strategy doesn't hold grudges or engage in tit-for-tat. It reacts purely to the immediate payoff landscape. If defecting becomes more profitable because others are not cooperating, it will defect. If cooperating becomes more profitable because others *are* cooperating, it will cooperate.
*   **Final Round Ruthlessness:** Always defecting in the last round is the ultimate act of self-interest, showing no concern for the long-term game or any cooperative norms.

### Pseudocode Representation:

```python
def decide_action(current_round, n, m, k, observed_cooperators_prev_round, my_action_prev_round):
    # --- Parameters ---
    # n: number of players
    # m: minimum cooperators needed
    # k: reward factor
    # r: total number of rounds (assumed accessible or implicitly handled by game loop)

    # --- Game State ---
    # current_round: 1-indexed round number
    # observed_cooperators_prev_round: integer count of players who cooperated in round (current_round - 1)
    # my_action_prev_round: 'C' or 'D'

    # --- Edge Cases ---
    if current_round == 1:
        return 'C'  # Aggressive initiation: test the waters, try to trigger cooperation

    if current_round == r: # Assuming 'r' is accessible or passed as a parameter
        return 'D'  # Always defect in the last round for maximum immediate gain

    # --- Payoff Calculations ---
    # Payoff if I COOPERATE in this round:
    # Cost of cooperating = 1
    # If cooperation meets threshold (my cooperation + others)
    #   My payoff = (1 - 1) + k = k
    # If cooperation DOES NOT meet threshold
    #   My payoff = (1 - 1) = 0
    potential_payoff_cooperate = 0
    if observed_cooperators_prev_round + 1 >= m:
        potential_payoff_cooperate = k

    # Payoff if I DEFECT in this round:
    # Private payoff = 1
    # If previous cooperation met threshold (regardless of my action)
    #   My payoff = 1 + k
    # If previous cooperation DID NOT meet threshold
    #   My payoff = 1
    potential_payoff_defect = 1
    if observed_cooperators_prev_round >= m:
        potential_payoff_defect = 1 + k

    # --- Decision Logic ---
    if potential_payoff_defect > potential_payoff_cooperate:
        return 'D'  # Aggressive: defect if it yields a higher immediate payoff
    else:
        return 'C'  # Cooperate if it's at least as good, especially if it secures the reward for me to leverage later.

```

This strategy aims to be a disruptive force, pushing for cooperation when it's most advantageous and defecting aggressively to maximize personal gains. It's not about building trust, but about skillfully navigating the risk-reward landscape to its own benefit.
'''

description_AGGRESSIVE_5 = '''
This is a fascinating game with a strong incentive to defect, but a powerful reward for coordinated cooperation. An aggressive strategy needs to balance the urge to maximize individual gain with the understanding that the highest payoffs are achievable only through collective action. Since direct communication is impossible, our strategy must infer intentions and adapt based on observed behavior.

Here's an aggressive strategy designed for this Collective Risk Dilemma, emphasizing a proactive and punishing approach:

## Aggressive Strategy: "The Enforcer"

**Core Philosophy:** "I will cooperate only when it is demonstrably safe or highly advantageous, and I will punish defection swiftly and severely to coerce cooperation. My goal is to extract maximum value while forcing others to bear the risk of my potential cooperation."

### 1. Decision Rules:

My decision to Cooperate (C) or Defect (D) in any given round `t` depends on a calculated "Cooperation Threshold Confidence" (CTC) and a "Defection Tolerance" (DT).

*   **Cooperate (C) if:**
    *   **Initial Cooperation Push (First `m` rounds or if `m` is low):** If the number of players observed cooperating in the *previous* round (let's call this `C_observed_prev`) is greater than or equal to `m - 1`, I will Cooperate. This is a calculated risk to try and trigger the reward mechanism early.
    *   **Sufficient Cooperation Observed:** If `C_observed_prev >= m` and the current round `t` is not the last round `r`. This means the community has successfully reached the threshold in the past, and I want to maintain that momentum.
    *   **High Reward Potential and Low Risk of Failure (Edge Case):** If `k` is significantly higher than `1`, and `m` is very close to `n`, and `C_observed_prev >= m - 1`. This is a strong incentive to push for cooperation even if there's a slight risk.

*   **Defect (D) if:**
    *   **Threshold Not Met:** If `C_observed_prev < m - 1`. I refuse to be the one to fall short when others are also hesitant. My endowment is safer with me.
    *   **Retaliation for Defection:** If the number of players I defected against in the previous round (`D_retaliated_prev`) is greater than 0. This is my primary punishment mechanism. I will defect to signal my displeasure and to pressure those who defected against me.
    *   **Approaching the End of the Game (Last `m` rounds or if `m` is high):** If the current round `t` is within the last `m` rounds (i.e., `t > r - m`) and `C_observed_prev < m`. The incentive to risk for a potential future reward diminishes, and the focus shifts to securing my current accumulated payoff.
    *   **No discernible cooperation pattern:** If `C_observed_prev` is consistently low and erratic, I will default to defecting.

### 2. Handling Edge Cases:

*   **First Round (t = 1):**
    *   **Decision:** I will **Cooperate (C)**.
    *   **Reasoning:** This is an aggressive gambit. I'm willing to be the first to "risk" cooperation to see if others follow. If `m` players cooperate, the reward `k` is significant. If not, I've only lost a small amount (my endowment) and can immediately shift to a punishing strategy in the next round. This is a calculated risk to establish a cooperative baseline and identify potential defectors early.

*   **Last Round (t = r):**
    *   **Decision:** I will **Defect (D)**.
    *   **Reasoning:** In the final round, there's no future to incentivize cooperation. The only goal is to maximize my accumulated payoff. Since the reward `k` is only applicable if the threshold `m` is met, and there are no future rounds to benefit from such a meeting, I will secure my private endowment.

*   **Early Rounds (e.g., first `m` rounds):**
    *   **Decision:** I will **Cooperate (C)** if `C_observed_prev >= m - 1`. Otherwise, I will **Defect (D)**.
    *   **Reasoning:** This is about establishing and reinforcing the cooperative norm. If enough others are willing to cooperate, I'll join to ensure the threshold is met and we all benefit. If there's significant hesitation, I'll protect my endowment and wait for clearer signals.

*   **Late Rounds (e.g., last `m` rounds, `t > r - m`):**
    *   **Decision:** I will **Cooperate (C)** only if `C_observed_prev >= m`. Otherwise, I will **Defect (D)**.
    *   **Reasoning:** As the game nears its end, the risk of defecting becomes less impactful on future rounds. I will only contribute to a cooperative effort if it's guaranteed to succeed (i.e., `m` or more have already cooperated). Otherwise, I'm securing my gains.

### 3. Aggressiveness:

*   **Proactive Cooperation (First Round):** My willingness to cooperate first is an aggressive move to try and kickstart the beneficial outcome.
*   **Swift Punishment:** My defection strategy is reactive and punitive. If others defected against me (or if cooperation faltered significantly), I will defect in the next round. This is not a tit-for-tat; it's a deterrent. I'm willing to sacrifice my own potential reward in a round if it means punishing defectors and signaling that such behavior will not be tolerated.
*   **"Or Else" Mentality:** The threat of my defection, especially after a failed cooperative round, is a significant factor. I am not passively waiting for others to cooperate; I am actively influencing their decision-making through the credible threat of my own defection.
*   **Maximizing Payoff in All Scenarios:** Whether cooperation succeeds or fails, "The Enforcer" aims to maximize its own payoff. If cooperation thrives, I benefit. If it fails, I'm either defecting to punish or defecting to secure my gains, always prioritizing my own outcome.

### Pseudocode Representation:

```pseudocode
// Global variables to track history
C_observed_prev = 0 // Number of players observed cooperating in the previous round
D_retaliated_prev = 0 // Number of players who defected against me in the previous round
history = [] // List of tuples: (round_num, my_action, observed_actions_list)

function decide_action(round_num, n, r, m, k, observed_actions_in_prev_round):
    // Update history and C_observed_prev, D_retaliated_prev
    if round_num > 1:
        // Analyze observed_actions_in_prev_round to update C_observed_prev and D_retaliated_prev
        C_observed_prev = count_cooperators(observed_actions_in_prev_round)
        D_retaliated_prev = count_my_defected_against(observed_actions_in_prev_round, my_player_id) // Need to track my player ID

    // --- Decision Logic ---

    // 1. Last Round: Always defect
    if round_num == r:
        return "D"

    // 2. First Round: Aggressive initial cooperation
    if round_num == 1:
        return "C"

    // 3. Retaliation for defection (High priority punishment)
    if D_retaliated_prev > 0:
        return "D"

    // 4. Sufficient Cooperation Observed (Maintain cooperative state)
    if C_observed_prev >= m:
        return "C"

    // 5. Initial Cooperation Push (Early rounds, trying to seed cooperation)
    if round_num <= m and C_observed_prev >= m - 1:
        return "C"

    // 6. Late Rounds: Cooperate only if guaranteed
    if round_num > r - m and C_observed_prev >= m:
        return "C"

    // 7. Default: Threshold not met, or no clear positive signal
    return "D"

// Helper function to count cooperators in a list of observed actions
function count_cooperators(observed_actions_list):
    count = 0
    for action in observed_actions_list:
        if action == "C":
            count = count + 1
    return count

// Helper function to count how many players defected against me
// This requires knowing my player ID and the actions of all players in the previous round
// For simplicity here, assume a simplified observed_actions_list that maps player_id to action
function count_my_defected_against(all_player_actions_prev_round, my_player_id):
    count = 0
    for player_id, action in all_player_actions_prev_round:
        if player_id != my_player_id and action == "D":
            // This is a simplified assumption. Ideally, we'd check if *they* defected *against me*.
            // In this simultaneous move game, it's more about if they defected *at all* while I cooperated.
            // For this strategy, a simple "if player_id defected" is a proxy for "they didn't help me".
            // A more precise implementation might require knowing my action in the previous round.
            count = count + 1
    return count
```

**Note on `D_retaliated_prev`:** In a simultaneous-move game, "defecting against me" is interpreted as "defecting while I was cooperating." This strategy implicitly assumes that if cooperation fails (`C_observed_prev < m`), and my action was `C`, any `D` from other players is a direct defection against me. If my action was `D`, then `D_retaliated_prev` would simply count how many others also defected, indicating a general shift away from cooperation, which also triggers my defection.

"The Enforcer" is designed to be a formidable opponent. It's not afraid to take calculated risks to gain an advantage, but it's equally ruthless in punishing those who don't adhere to the potential for collective gain. Its aggression stems from a desire to control the game's dynamics and ensure its own prosperity, even if it means pushing others towards the riskier path of cooperation.
'''

description_AGGRESSIVE_6 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the cooperative mechanism when advantageous, with a strong bias towards defecting.

## Aggressive "Exploitative Opportunist" Strategy

This strategy is designed to be aggressive by primarily seeking to exploit the system for personal gain while being ready to contribute to cooperation *only* when it's demonstrably in its best interest due to the actions of others, or when facing imminent failure. It is not designed for altruistic cooperation; any cooperation is purely tactical.

**Core Philosophy:** Always prioritize keeping your endowment. Only cooperate if the payoff from cooperation *exceeds* the payoff from defecting, or if defecting risks a complete loss of the reward.

### 1. Decision Rules

The fundamental decision is whether to play 'C' or 'D' in any given round.

*   **Primary Rule: Defect by Default.** Unless specific conditions are met, the player defects ('D'). This is the most aggressive stance, ensuring the player always secures their base payoff of 1.

*   **Conditional Cooperation:** Cooperate ('C') *only if* the following condition is met:
    *   **Predicted Success:** The current round's expected payoff from cooperating is *strictly greater than* the expected payoff from defecting, *given the current historical actions of other players*.
    *   **How to Predict Success:**
        *   **Count of Known Cooperators:** Track the number of players (including yourself) that have cooperated in previous rounds.
        *   **Projected Cooperators:** Based on observed patterns and the current round's context, estimate the likelihood of reaching the `m` threshold.
        *   **The Calculation:**
            *   Let `N_C_observed` be the number of players who *have* cooperated in at least one previous round (excluding yourself).
            *   Let `N_D_observed` be the number of players who have *always* defected in previous rounds.
            *   Let `N_Unknown` be `n - 1 - N_C_observed - N_D_observed`. These are players whose behavior is still uncertain.
            *   **Cooperation Decision Threshold:** If `N_C_observed + N_Unknown` (the maximum possible number of cooperators *excluding yourself*) is greater than or equal to `m - 1` (meaning, if everyone else cooperates, we will reach the threshold), *and* the observed cooperators plus a reasonable estimate of future cooperation from `N_Unknown` is likely to meet `m`, then consider cooperating.
            *   **Crucially:** This prediction must be optimistic. We are looking for a *guaranteed* or *highly probable* outcome of exceeding `m` cooperators.
            *   **Payoff Check:** If the prediction suggests `m` or more players *will* cooperate (including your potential cooperation), calculate:
                *   **Payoff if Cooperating:** `0 + k` (assuming you contributed your endowment and the reward is achieved).
                *   **Payoff if Defecting:** `1 + k` (assuming the reward is achieved by others, and you kept your endowment).
                *   **Decision:** Cooperate *only if* `0 + k > 1` (which means `k > 1`, always true per parameters). This condition ensures you don't sacrifice your endowment if the reward is achieved anyway by others.
            *   **Alternative Cooperation Condition (Risk Aversion):** If the number of observed cooperators (`N_C_observed`) plus the number of players whose behavior is still unknown (`N_Unknown`) is *less than* `m - 1`, and you believe cooperating might be the only way to *prevent* the failure scenario (where no one cooperates and everyone gets 1), *and* the probability of reaching `m` cooperators with your cooperation is high, *then* cooperate. This is a defensive cooperation move.

*   **Simplified "No Regret" Logic:** A simpler, more aggressive interpretation of the conditional cooperation: Cooperate *only if* the number of players who have *ever* cooperated is at least `m - 1`, *and* the observed average cooperation rate in recent rounds is high enough to suggest `m` will be met if you also cooperate. In this scenario, your payoff for cooperating is `0 + k`, and your payoff for defecting is `1 + k`. Since `k > 1`, `1 + k` is always greater than `0 + k`. Therefore, under this strict interpretation, you will **almost never cooperate unless you can guarantee a higher payoff for yourself by cooperating than by defecting (which is impossible if the reward is met regardless of your action).** This leads to a more robustly aggressive strategy.

    **Revised Aggressive Decision Rule:**
    **Defect ('D') UNLESS:**
    1.  **Round `t` is the last round (`r`) and the total number of observed past cooperators (`N_C_observed`) is less than `m - 1`.** In this desperate scenario, to *try* and secure any reward, you will cooperate to increase the chances of hitting the threshold.
    2.  **Current round `t` is *not* the last round, and the number of observed cooperators (`N_C_observed`) is `m - 1`.** This means if *any other player* cooperates, the threshold will be met. In this situation, your payoff for cooperating is `0 + k`, and your payoff for defecting is `1 + k`. Since `1 + k > 0 + k`, you will still defect. This highlights the extreme aggression.

    **Let's refine the "Exploitative Opportunist" to actually *exploit* the reward when possible:**

    **Revised Aggressive Decision Rule (Final):**
    **Defect ('D') UNLESS:**
    *   **Condition 1: The number of players who have cooperated in *any* previous round (`N_C_observed`) plus the number of players who have *only* defected (`N_D_observed`) is equal to `n - 1` (meaning all other players' behavior is fixed).**
        *   In this case, calculate:
            *   If `N_C_observed` (from others) is already `m` or more: **Defect ('D')**. Your payoff will be `1 + k`. Cooperating would yield `0 + k`.
            *   If `N_C_observed` (from others) is `m - 1`: **Cooperate ('C')**. Your payoff will be `0 + k`. If you defect, your payoff would be `1 + k`, but the reward is achieved. By cooperating, you secure `0 + k`. This is where you *choose* to secure a known reward, even if it's less than defecting.
            *   If `N_C_observed` (from others) is less than `m - 1`: **Defect ('D')**. The reward is unlikely.
    *   **Condition 2: It's the last round (`r`) and the total number of observed past cooperators (`N_C_observed`) is less than `m - 1`.**
        *   **Cooperate ('C')**. This is a desperate attempt to get *any* reward. Your payoff is `0 + k` (if you succeed), versus `1 + 0` (if you fail to reach `m` by defecting).

**Rationale for Aggression:**
The strategy prioritizes the base payoff of 1. Cooperation only becomes attractive when it can *guarantee* the `k` reward, even if it means a slightly lower individual payoff for that round (`0+k` vs `1+k`). The strategy is opportunistic: it waits for others to do the hard work of reaching `m`, then jumps in to secure the reward if it's just out of reach, or if it's about to be achieved anyway. It avoids being the *first* to cooperate as this is the highest risk.

### 2. Handling Edge Cases

*   **Round 1:**
    *   **Action:** **Defect ('D')**.
    *   **Reasoning:** There is no historical data. Cooperating in the first round is a blind gamble with a high risk of receiving only 1 (if others defect). The aggressive stance dictates playing it safe and observing.

*   **Last Round (`r`):**
    *   **Action:** This is the most critical edge case and is covered in the "Revised Aggressive Decision Rule (Final)" above. You defect by default, but if the cooperation threshold (`m`) is *unlikely* to be met without your contribution (i.e., `N_C_observed < m - 1`), you will cooperate as a last-ditch effort to secure the reward. If `N_C_observed >= m-1`, you defect because the reward is likely to be achieved anyway, and defecting yields a better payoff.

*   **Near the Threshold (`N_C_observed` is `m-1`):**
    *   **Action:** **Defect ('D')** (unless it's the last round and `m-1` is not achievable).
    *   **Reasoning:** If `N_C_observed` is `m-1`, then only one more cooperator is needed. If any other player cooperates, the threshold is met. Your payoff for defecting will be `1 + k`, while cooperating yields `0 + k`. Since `k > 1`, `1 + k > 0 + k`. Therefore, defecting is always the individually superior choice *if the reward is likely to be achieved anyway*. This is the essence of exploitation.

*   **Low Cooperation Rate (`N_C_observed` is very low):**
    *   **Action:** **Defect ('D')**.
    *   **Reasoning:** The probability of reaching `m` cooperators is low. Conserve your endowment and receive the base payoff of 1.

### 3. Aggressive Mindset Alignment

*   **Default to Self-Interest:** The strategy begins with a core assumption that other players will not cooperate reliably. The default action reflects this distrust.
*   **Opportunistic Exploitation:** The strategy actively seeks situations where the reward can be achieved without personal risk or sacrifice. It's about catching the "free ride" whenever possible.
*   **Calculated Risk-Taking (Late Game):** Cooperation is only considered in the very late stages of the game, and *only* if it's a desperate attempt to avoid a total loss of reward, or if the payoff difference between cooperating and defecting (when reward is secured) is negligible and cooperation is a sure bet.
*   **No Altruism:** The strategy is devoid of any notion of "fairness" or helping the group unless it directly and clearly benefits the player's own expected payoff. The goal is to be the player who benefits most, even if it means others receive less.

### Pseudocode Representation

```pseudocode
strategy "AggressiveExploitativeOpportunist":

  // Initialization
  player_index = self.index
  total_rounds = r
  threshold_m = m
  reward_k = k
  known_cooperators_count = 0  // Number of players who have cooperated at least once (excluding self)
  always_defected_count = 0    // Number of players who have always defected
  player_cooperation_history = array of booleans (size n, initialized to false) // Tracks if player i has cooperated
  round_history = empty list // Stores tuples of (player_actions, payoffs)

  function decide_action(current_round, round_history):
    // Update counts based on previous rounds
    update_player_cooperation_stats(round_history)

    // --- Decision Logic ---

    // 1. Always defect by default
    action = 'D'
    my_payoff_if_defect = 1
    my_payoff_if_cooperate = 0

    // Calculate potential payoff if reward is met
    potential_reward_payoff_defect = 1 + reward_k
    potential_reward_payoff_cooperate = 0 + reward_k

    // 2. Check conditions for cooperation (prioritizing highest individual gain)

    // Case A: If the reward is almost guaranteed by others
    if known_cooperators_count >= threshold_m:
      action = 'D' // Defect to maximize personal gain (1+k > 0+k)

    // Case B: If exactly m-1 cooperators are known from others, and it's NOT the last round
    // This means if *any* other player cooperates, we hit the threshold.
    // My payoff defecting = 1+k. My payoff cooperating = 0+k. Defect is better.
    else if known_cooperators_count == threshold_m - 1 and current_round < total_rounds:
      action = 'D'

    // Case C: If the last round is approaching and cooperation is desperately needed
    else if current_round == total_rounds and known_cooperators_count < threshold_m - 1:
      action = 'C' // Desperate attempt to secure any reward

    // Case D: If exactly m-1 cooperators from others, and it IS the last round
    // This is a specific edge case. Reward WILL be met by others.
    // Cooperating yields 0+k. Defecting yields 1+k.
    // BUT, the rule is to exploit. If reward is met, you secure max personal gain by defecting.
    // However, if we MUST ensure a reward is obtained and defecting leaves doubt, we might cooperate.
    // For this aggressive strategy, if reward is guaranteed by others, defect.
    else if current_round == total_rounds and known_cooperators_count >= threshold_m - 1:
      action = 'D'

    // Case E: If the number of players whose behavior is fixed (known_cooperators_count + always_defected_count) is n-1
    // This means all other players' future actions are predictable.
    else if (known_cooperators_count + always_defected_count) == n - 1:
      // If known cooperators from others is already m or more:
      if known_cooperators_count >= threshold_m:
        action = 'D' // Maximize personal gain (1+k vs 0+k)
      // If known cooperators from others is exactly m-1:
      elif known_cooperators_count == threshold_m - 1:
        // This is the crucial exploitation point. Reward will be met by others.
        // Cooperating gives 0+k. Defecting gives 1+k. Defect is superior.
        // HOWEVER, if the goal is ABSOLUTE guarantee of reward for SELF, and cooperation is only slightly worse payoff, choose cooperation.
        // For "Aggressive Exploiter", we prioritize the higher payoff.
        action = 'D' // Because 1+k > 0+k
        // A slightly less aggressive version would be:
        // action = 'C' // To secure 0+k when others guarantee k. This is still exploitative.
        // The current prompt asks for AGGRESSIVE, so we defect if 1+k > 0+k.

    // Fallback: If no specific condition met, default is 'D'
    return action

  function update_player_cooperation_stats(round_history):
    // Reset counts for each round's calculation
    known_cooperators_count = 0
    always_defected_count = 0
    player_cooperation_history = array of booleans (size n, initialized to false)

    // Analyze historical actions
    for each previous_round_data in round_history:
      player_actions = previous_round_data.actions // Assume this is a list/dict of actions for each player
      for i from 0 to n-1:
        if i != player_index: // Only consider other players
          if player_actions[i] == 'C':
            player_cooperation_history[i] = true // Mark player i as having cooperated at least once

    // Count known cooperators (those who have *ever* cooperated)
    for i from 0 to n-1:
      if i != player_index and player_cooperation_history[i]:
        known_cooperators_count += 1

    // Count players who have *always* defected
    // This requires a more detailed history log, e.g., a 2D array for each player's actions per round
    // For simplicity in pseudocode, assume we can determine this.
    // Let's refine: we only need to know the *current state* of others, not if they *always* defected.
    // The key is the number of players who have EVER cooperated.

    // Simplified Count Logic:
    // known_cooperators_count = number of players (excluding self) that have played 'C' in any previous round.

    // Re-evaluating what's truly observable and useful for aggressive strategy:
    // - The number of players that have cooperated *at least once*.
    // - If the number of such players is m-1, the next cooperator triggers the reward.

    // Let's refine the decision logic with simpler observation:
    known_cooperators_ever = 0
    for i from 0 to n-1:
      if i != player_index:
        has_cooperated = false
        for pr in round_history: // Iterate through all past rounds for player i
          if pr.actions[i] == 'C':
            has_cooperated = true
            break
        if has_cooperated:
          known_cooperators_ever += 1

    // This `known_cooperators_ever` is the primary metric.
    // The decision logic in `decide_action` should primarily use this count.

    // Recalculating `known_cooperators_count` for clarity within the decision block
    // This should be calculated before the decision logic block.

    // Let's try a final, clean pseudocode structure.

```

```pseudocode
strategy "AggressiveExploitativeOpportunist":

  // --- Game Parameters ---
  n: int  // Number of players
  r: int  // Number of rounds
  m: int  // Minimum cooperators needed
  k: float // Reward factor

  // --- Internal State ---
  player_index: int // My index in the game
  cooperated_ever: list[bool] // For each player i, True if player i has ever cooperated
  cooperation_count_observed: int // Number of players (excluding self) who have cooperated at least once.

  function initialize(player_idx, num_players, num_rounds, min_coop, reward_factor):
    self.player_index = player_idx
    self.n = num_players
    self.r = num_rounds
    self.m = min_coop
    self.k = reward_factor
    self.cooperated_ever = [False] * self.n
    self.cooperation_count_observed = 0

  function decide_action(current_round, history):
    // --- Update State based on History ---
    // history is a list of dictionaries, where each dict represents a round:
    // {'actions': {'player_0': 'C', 'player_1': 'D', ...}, 'payoffs': {...}}

    if current_round > 1: // If not the first round, update based on previous rounds
      for i in range(self.n):
        if not self.cooperated_ever[i]: // Only check if not already marked as cooperator
          for prev_round in history:
            if prev_round['actions'][f'player_{i}'] == 'C':
              self.cooperated_ever[i] = True
              break

      self.cooperation_count_observed = 0
      for i in range(self.n):
        if i != self.player_index and self.cooperated_ever[i]:
          self.cooperation_count_observed += 1

    // --- Decision Logic ---
    // Default: Defect (aggressive by default)
    action = 'D'
    my_base_payoff = 1.0
    my_coop_payoff_if_reward = 0.0 + self.k
    my_defect_payoff_if_reward = 1.0 + self.k

    // Condition 1: Near the threshold, and it's NOT the last round.
    // If N_C_observed is m-1, the next cooperator triggers reward.
    // If we defect, we get 1+k. If we cooperate, we get 0+k. Defect is better.
    if self.cooperation_count_observed == self.m - 1 and current_round < self.r:
      action = 'D' // Defect, as 1+k > 0+k

    // Condition 2: Last round, and threshold is unlikely without us.
    // This is a desperate, last-chance cooperation.
    elif current_round == self.r and self.cooperation_count_observed < self.m - 1:
      action = 'C' // Try to secure the reward, payoff: 0+k, vs 1+0 if fail

    // Condition 3: Reward is already guaranteed or highly likely by others.
    // If self.cooperation_count_observed >= m, reward is met. Defecting is better (1+k > 0+k)
    // If self.cooperation_count_observed == m-1 AND it's the LAST round, others will hit m. Defect is better (1+k > 0+k)
    elif self.cooperation_count_observed >= self.m - 1 and current_round <= self.r:
        action = 'D' // Defect to maximize personal gain if reward is likely secured by others.

    // Default action remains 'D' if none of the above conditions trigger cooperation.

    return action

```
'''

description_AGGRESSIVE_7 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Opportunist"

This strategy is designed to maximize individual payoff by exploiting situations where cooperation is beneficial but also by defecting when it's advantageous. It embodies an aggressive mindset by prioritizing personal gain and only cooperating when the potential for a significant collective reward outweighs the immediate benefit of defection, and when there's a high probability of achieving that reward.

**Core Philosophy:** "I will only cooperate if the odds are stacked in my favor, and the reward is substantial. Otherwise, I take what I can get."

---

### 1. Decision Rules: When to Cooperate vs. Defect

The core of "The Opportunist" strategy lies in a dynamic assessment of the "cooperation deficit" and the "opportunity threshold."

**Key Metrics:**

*   **`cooperation_deficit` (CD):** The number of additional players needed to cooperate to reach the threshold `m`. `CD = max(0, m - number_of_cooperators_in_previous_round)`
*   **`cooperation_probability_estimate` (CPE):** An estimate of the probability that *at least* `CD` *other* players will cooperate in the current round. This is a dynamic, history-based calculation.
*   **`expected_cooperation_gain` (ECG):** The expected payoff for playing 'C' in the current round.
*   **`immediate_defection_gain` (IDG):** The guaranteed payoff for playing 'D' in the current round.

**Decision Logic:**

**In any given round `t` (where `t > 1`):**

*   **Calculate `cooperation_deficit` (CD):**
    *   Count the number of players (excluding yourself) who cooperated in round `t-1`. Let this be `N_C_t-1`.
    *   `CD = max(0, m - N_C_t-1)`

*   **Estimate `cooperation_probability_estimate` (CPE):**
    *   This is where the "opportunistic" and adaptive nature comes in. We need to infer what others are likely to do.
    *   **Method 1: Simple Historical Average (Fallback):** If history is too sparse or inconsistent, assume other players have a base probability of cooperation. A robust approach would be to calculate the average historical cooperation rate of *all* players (excluding yourself) in past rounds.
        *   `avg_historical_cooperation = (total_cooperations_by_others_in_past_rounds) / (num_players_excluding_self * num_past_rounds)`
        *   `CPE = avg_historical_cooperation` (This is a very basic estimate and will be refined.)
    *   **Method 2: Dynamic Trend Analysis (Preferred):** Look for trends in recent rounds.
        *   **Increasing Cooperation:** If the number of cooperators has been increasing in the last few rounds, lean towards a higher `CPE`.
        *   **Decreasing Cooperation:** If the number of cooperators has been decreasing, lean towards a lower `CPE`.
        *   **Stable Cooperation:** If the number of cooperators is relatively stable, use the average historical cooperation rate.
        *   **Extreme Fluctuations:** If cooperation is highly erratic, revert to a more conservative `CPE` based on a longer historical average.
    *   **Refinement:** The `CPE` should also be influenced by how close the previous round was to the threshold `m`. If `N_C_t-1` was very close to `m`, the `CPE` for the next round should be higher. If it was far below `m`, the `CPE` should be lower.
    *   **Aggressive Adjustment:** If the `CD` is small (e.g., 1 or 2), and the `CPE` is reasonably high (e.g., > 0.5), we might be willing to "push" for cooperation.

*   **Calculate Expected Payoffs:**
    *   **If I cooperate (play C):**
        *   My payoff is `(1 - 1) + k` if `m` or more players (including me) cooperate.
        *   My payoff is `(1 - 1)` if fewer than `m` players (including me) cooperate.
        *   This simplifies to:
            *   Expected payoff if I play C = `(CPE * (1 - 1 + k)) + ((1 - CPE) * (1 - 1))`
            *   `ECG = CPE * k`

    *   **If I defect (play D):**
        *   My payoff is always `(1 - 0) = 1` if fewer than `m` players cooperate.
        *   My payoff is `(1 - 0) + k` if `m` or more players (excluding me) cooperate.
        *   This simplifies to:
            *   Expected payoff if I play D = `(CPE * (1 + k)) + ((1 - CPE) * 1)`
            *   `IDG = CPE * k + 1`

*   **Decision Rule:**
    *   **IF `ECG > IDG` THEN Play 'C' (Cooperate)**
        *   This inequality simplifies to `CPE * k > CPE * k + 1`, which is `0 > 1`. This is never true. **This means that under the assumption of a fixed `CPE` for the current round's decision, direct expected payoff comparison will always lead to defection.**
        *   **THIS IS WHERE THE AGGRESSIVE MINDSET KICKS IN:** The "Opportunist" doesn't just look at immediate expected payoff. It considers the *risk* of missing out on the large reward.
        *   **Revised Decision Rule (Aggressive & Risk-Aware):**
            *   **Play 'C' if:**
                1.  The number of cooperators in the *previous* round (`N_C_t-1`) was `m - 1`. (This is the smallest possible deficit, making cooperation most likely to succeed).
                2.  **AND** `k` is sufficiently high (e.g., `k > 2`). A higher `k` makes the collective reward more attractive.
                3.  **AND** the `CPE` (estimated probability of others cooperating) is above a certain threshold (e.g., `CPE > 0.7`). We need a strong signal that others will contribute.
            *   **Play 'C' if:**
                1.  The number of cooperators in the *previous* round (`N_C_t-1`) was `m - 2` or less, **BUT**
                2.  The `CPE` is *very high* (e.g., `CPE > 0.9`), indicating a strong convergence towards cooperation, **AND** `k` is significantly high.
            *   **In all other cases, Play 'D' (Defect).** This is the default and aggressive stance.

**Summary of Aggressive Cooperation Triggers:**

Cooperate only when:

1.  **The collective is *just* on the verge of success (previous round was `m-1` cooperators), AND the reward (`k`) is significant, AND others are likely to cooperate.**
2.  **The collective is *not* close to success, BUT there's an overwhelming probability that *everyone else* will cooperate, AND the reward (`k`) is exceptionally high.**

---

### 2. Handling Edge Cases

*   **Round 1 (The Opening Gambit):**
    *   **Strategy: Default to Defection.** In the absence of any historical data, the "Opportunist" plays it safe and secures its guaranteed payoff of 1. There's no information to suggest cooperation will be rewarded.
    *   `Decision for Round 1: Play 'D'.`

*   **Last Round (`r`):**
    *   **Strategy: Act as if it's a one-shot game, but with perfect information of previous rounds.** Since there are no future rounds to influence, the primary goal is to maximize payoff in *this* round.
    *   **Apply the same decision rules as for other rounds.** The `CPE` will be based on the history up to round `r-1`. If the conditions for cooperation are met, cooperate. Otherwise, defect. The aggressive nature means we are still looking for the best immediate outcome.

*   **Low `m` / High `n` (The Crowd Factor):**
    *   If `m` is very small relative to `n`, reaching the threshold might be easier. The strategy will still apply, but the `CPE` calculations will be more sensitive. A higher `CPE` will be required to trigger cooperation if `m` is small, as defection becomes more individually rewarding when others are cooperating without much risk.

*   **High `m` / Low `n` (The Bottleneck):**
    *   If `m` is very high relative to `n`, reaching the threshold is difficult. The strategy will be highly hesitant to cooperate unless there's a very strong signal that cooperation will succeed. The `CPE` required to trigger cooperation will be extremely high.

*   **Very High `k` (The Mega-Reward):**
    *   When `k` is exceptionally high, the "Opportunist" becomes slightly more risk-tolerant. The `CPE` thresholds for cooperating might be lowered slightly, and the "just on the verge of success" condition (previous round `m-1` cooperators) becomes a stronger trigger.

*   **Very Low `k` (The Paltry Prize):**
    *   When `k` is low, the "Opportunist" will be almost entirely defecting. The incentive to cooperate is minimal, and the immediate payoff of 1 from defecting is almost always superior. The `CPE` thresholds for cooperation will be very high, and only the "overwhelming probability" condition might trigger cooperation in rare cases.

---

### 3. Being Aggressive

*   **Default to Defection:** The core of the strategy is to defect unless there is a compelling, data-backed reason to cooperate. This ensures a baseline payoff and avoids being exploited.
*   **High Cooperation Thresholds:** We demand strong evidence (high `CPE`) and favorable conditions (low `CD` and high `k`) before we are willing to risk cooperating.
*   **Exploiting Near Misses:** The strategy is designed to "strike" when the collective is almost there. If others have done the hard work of getting close to `m`, "The Opportunist" is willing to jump in and claim its share of the reward, maximizing its own gain.
*   **No Altruism:** The strategy does not aim to "help" others or establish cooperation for its own sake. It only cooperates when it directly benefits the player, and the probability of that benefit is high.
*   **Adaptive but Ruthless:** While adaptive in its `CPE` calculation, the strategy remains ruthless in its pursuit of individual gain. It will readily switch back to defection if the conditions for cooperation are no longer met.

---

### Pseudocode Snippet (Conceptual)

```python
def decide_action(current_round, n, r, m, k, history):
    # history is a list of lists, where history[t-1] is a list of actions (C/D) for round t-1

    if current_round == 1:
        return 'D'  # Default to defection in the first round

    # Calculate cooperation in previous round
    num_cooperators_prev_round = sum(1 for action in history[-1] if action == 'C')
    players_other_than_self_prev_round = len(history[-1]) - 1 # assuming history[-1] has all players' actions

    # Estimate Cooperation Probability (CPE) - This is a complex part requiring more sophisticated logic
    # For simplicity here, we'll use a placeholder. A real implementation would analyze trends, etc.
    # It should estimate the probability that *other* players will cooperate.
    cpe = estimate_cpe(history, n, m) # Placeholder function

    # Decision Logic
    if num_cooperators_prev_round == m - 1:
        # "Just on the verge" condition
        if k > 2.0 and cpe > 0.7: # Thresholds can be tuned
            return 'C'
    elif num_cooperators_prev_round <= m - 2:
        # Less close to threshold, require stronger signal
        if cpe > 0.9 and k > 3.0: # Higher thresholds
            return 'C'

    # Default aggressive stance
    return 'D'

def estimate_cpe(history, n, m):
    # This function needs to implement sophisticated historical analysis.
    # It should consider:
    # - Average historical cooperation rates (overall and by player if possible)
    # - Trends (increasing/decreasing cooperation)
    # - Recent stability/volatility
    # - Impact of previous round's outcome (close to m vs. far from m)
    #
    # Example: Very basic fallback - average cooperation rate of others in past rounds
    if not history:
        return 0.5 # Neutral assumption if no history

    total_cooperations_by_others = 0
    num_other_players = n - 1
    num_past_rounds = len(history)

    for round_actions in history:
        for i, action in enumerate(round_actions):
            if i != self_player_index: # Assuming we know our own index
                if action == 'C':
                    total_cooperations_by_others += 1

    if num_other_players * num_past_rounds == 0:
        return 0.5 # Avoid division by zero

    avg_cooperation = total_cooperations_by_others / (num_other_players * num_past_rounds)

    # This is where the adaptive and trend-based logic would be added.
    # For now, we'll return the average, but it should be more nuanced.
    return avg_cooperation # Needs to be a probability estimate for OTHER players
```

---

**Strategic Rationale Summary:**

"The Opportunist" is built on the principle of self-interest in a collective setting. It's aggressive because it defaults to defection, securing a baseline payoff. It's adaptive because it learns from history to estimate the likelihood of others' cooperation. It's strategic because it only risks cooperation when the potential reward is high and the probability of success is demonstrably favorable, particularly when the collective is on the cusp of achieving its goal. This strategy aims to be a formidable opponent, striking when the opportunity is ripe and otherwise maximizing individual gain.
'''

description_AGGRESSIVE_8 = '''
This is a fascinating game with a strong incentive for collective action, but also a significant risk of defection leading to a cascade of failures. The aggressive strategy needs to balance pushing for cooperation with protecting itself from exploitation.

## Aggressive Strategy: The "Calculated Push"

This strategy is designed to be proactive and assertive, aiming to trigger the high collective reward while being prepared to exploit others if they don't reciprocate. It's built on the principle of pushing the boundary towards cooperation, but with a clear exit strategy if the collective fails to meet the minimum.

### 1. Decision Rules:

The core of this strategy is to **initiate cooperation and maintain it as long as the collective is on track to meet the threshold**. It will only defect if it becomes clear that the threshold will **not** be met, or if it's strategically advantageous to defect in a specific round.

**My Decision for Round 't':**

*   **If the number of observed cooperators in round 't-1' (let's call it `observed_cooperators_t_minus_1`) is greater than or equal to `m - 1`:**
    *   **Play C (Cooperate).**
    *   **Reasoning:** This indicates that the collective is very close to reaching the cooperation threshold. My cooperation, combined with the existing cooperators, is highly likely to tip the balance and secure the reward for everyone. This is the "Calculated Push" – taking a calculated risk to achieve the optimal outcome.

*   **If `observed_cooperators_t_minus_1` is less than `m - 1`:**
    *   **Play D (Defect).**
    *   **Reasoning:** The collective is too far from the cooperation threshold. My cooperation, in this scenario, would likely result in me receiving only my private endowment (1) while others who defected would also receive their private endowment (1) but without contributing to the community. It's a net loss for me compared to defecting and securing my private endowment. This is the "Exit Strategy" – disengaging when the collective effort is clearly failing.

*   **Special Case: Last Round (r):**
    *   **Play D (Defect).**
    *   **Reasoning:** In the final round, there's no future to incentivize cooperation. The only rational choice is to maximize my immediate payoff, which is guaranteed by defecting. This ensures I don't contribute to a failed project in the last moment.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Play C (Cooperate).**
    *   **Reasoning:** In the absence of any historical data, the most aggressive and proactive approach is to lead by example. By cooperating, I signal a desire for cooperation and can potentially influence others to follow suit, especially in early rounds where establishing a norm might be beneficial. This is a high-stakes gamble, but it aligns with the aggressive mindset.

*   **Last Round (t=r):**
    *   As mentioned above, **always play D (Defect)**.

*   **When `m-1` becomes equal to `n` (i.e., `m = n+1`, which is impossible given `m < n`):**
    *   This condition won't be met due to the parameter constraint `m < n`.

*   **When `m-1` is 0 (i.e., `m = 1`, which is impossible given `m > 1`):**
    *   This condition won't be met due to the parameter constraint `m > 1`.

### 3. Aggressive Mindset Alignment:

*   **Proactive Initiation:** The strategy doesn't wait for others to signal cooperation. It *initiates* cooperation, aiming to be the catalyst for success.
*   **Calculated Risk-Taking:** It's willing to take the risk of cooperating when the odds are high (i.e., `m-1` cooperators are already present) to achieve the significant reward.
*   **Ruthless Efficiency:** If the collective fails to meet the threshold, it immediately disengages to avoid being exploited. There's no sentimentality or lingering hope in the face of failure.
*   **Exploiting Last Opportunity:** The strategy recognizes the final round as a pure self-interest play, maximizing personal gain without regard for collective outcomes.

### Pseudocode Representation:

```
// Strategy: Calculated Push

function decide_action(round_number, history, game_parameters):
  n = game_parameters.n
  r = game_parameters.r
  m = game_parameters.m
  k = game_parameters.k

  // Edge Case: Last Round
  if round_number == r:
    return "D" // Always defect in the last round

  // First Round: Initiate cooperation
  if round_number == 1:
    return "C"

  // Calculate observed cooperators from the previous round
  // history is a list of lists, where history[t-1] contains actions of all players in round t-1
  // 'C' represents cooperation, 'D' represents defection
  observed_cooperators_previous_round = 0
  if round_number > 1:
    previous_round_actions = history[round_number - 2] // History is 0-indexed
    for action in previous_round_actions:
      if action == "C":
        observed_cooperators_previous_round += 1

  // Decision Rule: Push for cooperation if close to threshold
  if observed_cooperators_previous_round >= m - 1:
    return "C"
  else:
    // Decision Rule: Disengage if threshold is unlikely to be met
    return "D"

```

This "Calculated Push" strategy embodies an aggressive approach by being the first to initiate cooperation when the odds are favorable, but it's equally aggressive in its immediate withdrawal when cooperation is clearly failing, thus protecting itself from exploitation. It’s designed to be a dynamic player, pushing for the best outcome while remaining acutely aware of the risks.
'''

description_AGGRESSIVE_9 = '''
This aggressive strategy, dubbed "The Dominator," is designed to maximize individual gain by exploiting the collective risk dilemma. It prioritizes personal gain while leveraging the threat of defection to coerce cooperation from others. The core principle is to be a powerful player that dictates terms, rather than a passive participant waiting for others to act.

## The Dominator Strategy

**Aggressive Mindset:** The Dominator views cooperation as a tool to be used strategically to unlock higher rewards, but never as a default state. Defection is the primary mode of operation, and cooperation is only offered when it demonstrably benefits the Dominator and/or pressures opponents into favorable actions. The ultimate goal is to be the player with the highest cumulative payoff, even if it means fewer players cooperate than the minimum required.

### 1. Decision Rules

The Dominator's decision in any given round is a dynamic calculation based on the current state of the game and the perceived behavior of opponents.

**General Rule: Defect, unless compelled to cooperate by strong indicators of success.**

**Cooperation Trigger:** The Dominator will only cooperate if it has a high degree of confidence that its cooperation, combined with the anticipated actions of others, will *guarantee* reaching the cooperation threshold ($m$). This confidence is built on observing a sufficient number of players already signaling their intent to cooperate.

*   **Threshold for Cooperation:** The Dominator will cooperate in round $t$ if:
    *   The number of players who cooperated in round $t-1$ is greater than or equal to $m-1$.
    *   **AND** the Dominator's own cooperation in round $t$ is *necessary* to reach $m$. This means that if the Dominator defects, the total number of cooperators in round $t$ would be strictly less than $m$.

**Rationale:** This ensures that the Dominator only incurs the cost of cooperation (losing its endowment) when it's highly probable that the collective reward ($k$) will be achieved, and more importantly, when its cooperation is the decisive factor in securing that reward. This also has the effect of punishing players who defect when others are trying to cooperate, as they will receive a payoff of 1 while the Dominator receives 2.

**Defection Logic:** In all other scenarios, the Dominator defects. This includes:
*   When the number of cooperators in the previous round was significantly below $m-1$, indicating a low likelihood of meeting the threshold.
*   When the Dominator's cooperation is not strictly necessary to reach $m$ (i.e., $m$ or more players already cooperated in the previous round, or are otherwise expected to cooperate). In this case, defecting allows the Dominator to keep its endowment *and* benefit from the reward $k$ if the threshold is met by others.

### 2. Handling Edge Cases

*   **Round 1 (Initial Round):**
    *   **Decision:** Defect.
    *   **Rationale:** In the first round, there is no history to inform decisions. The Dominator is inherently risk-averse in this initial state. It will observe the actions of others to gather information. Defecting ensures the Dominator receives its endowment and has the flexibility to react in subsequent rounds. The immediate payoff of 1 is guaranteed, and the threat of future defection is preserved.

*   **Last Round (Round r):**
    *   **Decision:** Defect.
    *   **Rationale:** In the final round, there are no future rounds to influence. The Dominator prioritizes maximizing its payoff in this single, isolated round. Since the game ends, there is no incentive to "build trust" or "punish for past behavior." The guaranteed private payoff of 1 is always secured by defecting. If the threshold is met by others, the Dominator also receives the reward $k$, achieving the maximum possible payoff of $1+k$. The Dominator will *never* cooperate in the last round, as the risk of others not cooperating to meet the threshold would mean a payoff of 0 instead of 1.

*   **When $m > n$ (Theoretically impossible by parameters, but good to acknowledge):** If somehow $m$ were greater than $n$, cooperation would never be possible. The strategy would always defect.

*   **When $m=2$ and $n=2$ (Minimum possible parameters):**
    *   Round 1: Defect.
    *   If opponent cooperated in Round 1: Cooperate in Round 2.
    *   If opponent defected in Round 1: Defect in Round 2.
    *   **Rationale:** This is a simplified Tit-for-Tat with an aggressive twist. The Dominator is willing to initiate cooperation if it sees the opponent is *capable* of cooperating and thus making $m$ possible. However, it will defect if the opponent is uncooperative.

### 3. Aggressive Elements of the Strategy

*   **Default Defection:** The baseline action is to defect. Cooperation is an exception, not the rule. This puts pressure on other players to initiate cooperation.
*   **Exploiting the "Last Mover Advantage":** The Dominator actively seeks to be the player who benefits from the collective reward without necessarily contributing to it. It will defect if it believes others will meet the threshold, thereby gaining $1+k$ while those who cooperated receive only 2. This incentivizes others to cooperate to avoid the Dominator's defection and their own lower payoff.
*   **Calculated Risk Aversion:** While aggressive, the strategy is not reckless. It only cooperates when success is nearly certain and its cooperation is the critical factor. This prevents the Dominator from wasting its endowment on failed cooperation attempts.
*   **No Forgiveness:** The strategy does not inherently forgive past defections. It only reacts to the current game state and perceived likelihood of success. If others consistently defect, the Dominator will continue to defect, maximizing its individual gain in a low-cooperation environment.
*   **"Force Cooperate" Mechanism:** By defecting until the very last moment, the Dominator can create a situation where its cooperation is the *only* thing standing between the group and a failed cooperation round. This effectively "forces" the Dominator's hand into cooperating only when it's most beneficial for itself and the group's reward is guaranteed.

### Pseudocode Representation

```python
# Parameters (assumed to be globally available)
# n: number of players
# r: number of rounds
# m: minimum cooperators needed
# k: reward factor

# State variables (maintained per round)
player_actions_history = {} # Dictionary: {round_num: {player_id: action}}
my_payoff_history = {}      # Dictionary: {round_num: payoff}

def decide_action(current_round, num_players_cooperated_last_round):
    # Edge Case: Last Round
    if current_round == r:
        return 'D'

    # Edge Case: First Round
    if current_round == 1:
        return 'D'

    # Aggressive Cooperation Trigger
    # Check if my cooperation is CRITICAL to reaching m
    # This implies num_players_cooperated_last_round == m - 1
    # AND we are certain others won't add to it (or we are the 'last' one)
    # A more robust check would involve predicting others' actions,
    # but for simplicity, we'll use a strong indicator:
    if num_players_cooperated_last_round >= m - 1:
        # A simple heuristic: if there's a high chance of success and our action
        # pushes it over the edge.
        # For this strategy, we want to be the DECISIVE cooperator.
        # So, we'll cooperate only if the *previous* round had m-1 cooperators,
        # making our cooperation essential to reach m this round.
        if num_players_cooperated_last_round == m - 1:
            return 'C' # Cooperate if our contribution is ESSENTIAL to meet threshold

    # Default: Defect
    return 'D'

# --- Tournament Loop Example ---
# for round_num in range(1, r + 1):
#     # Collect actions from previous round (or initialize for round 1)
#     if round_num > 1:
#         num_cooperators_prev = count_cooperators(player_actions_history[round_num - 1])
#     else:
#         num_cooperators_prev = 0 # No cooperators before round 1

#     my_action = decide_action(round_num, num_cooperators_prev)

#     # Store my action for this round
#     # ... (send action to game engine)

#     # Receive results and update history
#     # ...
```

This strategy aims to be a dominant force, setting a high bar for cooperation and leveraging the collective reward structure for maximum personal gain. It's designed to be unsettling for opponents, forcing them to either adapt to its aggressive stance or suffer the consequences of its self-serving opportunism.
'''

description_AGGRESSIVE_10 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while being robust to various player behaviors. This strategy prioritizes securing the higher payoff, but with a calculated willingness to defect if the collective seems unlikely to reach the cooperation threshold.

## Aggressive Strategy: The "Calculated Risk-Taker"

This strategy is rooted in a proactive and somewhat opportunistic approach. It aims to exploit the high reward offered by cooperation when it's feasible, but it's not afraid to defect and secure its own gains when the collective seems to be faltering or when there's an advantage to be had.

### 1. Decision Rules:

The core of this strategy involves a dynamic assessment of the likelihood of reaching the cooperation threshold ($m$). We'll maintain a belief about the collective's propensity to cooperate and adjust our actions accordingly.

**Primary Decision Logic:**

*   **If I believe at least `m-1` other players will cooperate this round:** Play **C** (Cooperate).
    *   *Rationale:* This is the threshold for ensuring the collective reward. If we are confident others will meet the requirement, we might as well contribute and secure the bonus for ourselves.
*   **Otherwise (if I believe fewer than `m-1` other players will cooperate this round):** Play **D** (Defect).
    *   *Rationale:* In this scenario, our individual cooperation is unlikely to push the group over the threshold. By defecting, we guarantee ourselves a payoff of 1, and we avoid contributing our endowment to a project that might fail to deliver the reward.

**Belief Formation (How to estimate the number of cooperating players):**

This is the adaptive part. We'll base our beliefs on historical data from previous rounds.

*   **Recent Cooperation Rate:** Calculate the proportion of players (excluding ourselves) who cooperated in the *previous round*.
    *   Let $N_{C, t-1}$ be the number of players (excluding self) who cooperated in round $t-1$.
    *   Our estimated number of other cooperators for round $t$ is $N_{C, t-1}$.
*   **Trend Analysis (Simple):** If the cooperation rate has been increasing over the last few rounds, we might slightly increase our confidence in others cooperating. If it's decreasing, we decrease our confidence. For simplicity in the initial design, we can start with just the previous round's data and refine later.
*   **Threshold Confidence Factor:** We'll introduce a simple factor to adjust our decision. Let's say we need at least `m-1` other cooperators for sure. If the number of recent cooperators is *close* to `m-1`, we might be more hesitant to cooperate ourselves.

**Refined Decision Rules (incorporating belief):**

Let $EstimatedOthersCooperating$ be our belief about the number of players (excluding self) who will cooperate in the current round. We will estimate this based on observed behavior in previous rounds.

*   **If $EstimatedOthersCooperating \ge m - 1$:** Play **C**.
*   **If $EstimatedOthersCooperating < m - 1$:** Play **D**.

**How to Calculate $EstimatedOthersCooperating$:**

For round $t$:
1.  **Look at round $t-1$:** Count the number of players (excluding yourself) who played C in round $t-1$. Let this be $ObservedOthersCooperating_{t-1}$.
2.  **Initial Strategy Simplification:** For round $t > 1$, set $EstimatedOthersCooperating = ObservedOthersCooperating_{t-1}$.
3.  **Aggressive Adjustment:**
    *   If $ObservedOthersCooperating_{t-1} \ge m$: We are feeling confident. We might slightly "hedge" and consider $EstimatedOthersCooperating = ObservedOthersCooperating_{t-1}$. (Essentially, assume the trend will continue or even improve).
    *   If $ObservedOthersCooperating_{t-1} < m-1$: We are feeling less confident. We might slightly "discount" and consider $EstimatedOthersCooperating = ObservedOthersCooperating_{t-1}$. (This means if it was already too low, it's still too low).

**This is the core aggressive loop: Observe, Estimate, and Act to secure the advantage.**

### 2. Handling Edge Cases:

*   **Round 1 (First Round):**
    *   **Action: D (Defect)**
    *   *Rationale:* In the first round, there is no historical data. Cooperating without any information about other players' intentions is too risky. The aggressive mindset dictates minimizing initial exposure and gathering information. By defecting, we guarantee a payoff of 1 and observe the actions of others. This is crucial for forming our initial beliefs for subsequent rounds.

*   **Last Round (Round `r`):**
    *   **Action: D (Defect)**
    *   *Rationale:* In the final round, there are no future rounds to influence. The incentive to foster long-term cooperation disappears. The aggressive strategy shifts to pure self-interest. Defecting guarantees a payoff of 1, and there's no penalty for being the "bad guy" on the last turn. This also exploits any players who might have been cooperating assuming future reciprocity.

*   **The "Too Many Cooperators" Scenario (m=1 or m=2, and n is small):**
    *   If $m-1=0$ (i.e., $m=1$), our rule becomes: If $EstimatedOthersCooperating \ge 0$, play C. This means we'd always cooperate if others cooperate at all. This is potentially too passive for an aggressive strategy.
    *   If $m-1=1$ (i.e., $m=2$), our rule becomes: If $EstimatedOthersCooperating \ge 1$, play C. This means we need at least one other player to cooperate.

    *   **Aggressive Tweak for Small `m`:** If $m-1 \le 1$, meaning only 0 or 1 other cooperator is needed to trigger the reward, the risk of *not* cooperating is lower. In such cases, we might be slightly more inclined to cooperate.
        *   **Revised Rule for $m-1 \le 1$:** If $EstimatedOthersCooperating \ge m - 2$: Play C.
            *   *Rationale:* This is still an aggressive interpretation. If the bar is very low (e.g., $m=2$, so $m-2=0$), we're willing to cooperate if we think *at least zero* others will. This means we'll cooperate as long as we don't have strong evidence everyone *else* will defect. This provides a slight bias towards cooperation when the threshold is easily met, still aiming for the higher payoff.

### 3. Aggressive Mindset Alignment:

*   **Proactive Information Gathering:** The strategy immediately defects in Round 1 to gather intelligence. It doesn't wait for others to signal their intent.
*   **Opportunistic Exploitation:** It aims to maximize personal gain by playing C when the reward is likely, and D when cooperation is unlikely to yield a benefit or when defection is guaranteed to be better.
*   **No Sentimentality:** The strategy defects in the final round, as there's no future to invest in. It doesn't get "tricked" into cooperating out of habit or a mistaken belief in reciprocity.
*   **Calculated Risk:** It's not blindly aggressive. It assesses the likelihood of success (i.e., the threshold being met) and cooperates only when the probability of personal gain from cooperation is high enough, based on observed behavior.

### Pseudocode Representation:

```python
def decide_action(player_id, current_round, total_rounds, n, m, k, history):
    """
    Aggressive strategy: The Calculated Risk-Taker

    Args:
        player_id (int): Your player ID (1-indexed).
        current_round (int): The current round number (1-indexed).
        total_rounds (int): The total number of rounds (r).
        n (int): The total number of players.
        m (int): Minimum number of cooperators needed.
        k (float): Reward if threshold met factor.
        history (list of lists): A list where history[t][i] is the action of player i+1 in round t+1.
                                 Actions are 'C' or 'D'. history is 0-indexed for rounds and players.

    Returns:
        str: The action to take ('C' or 'D').
    """

    # --- Edge Case: First Round ---
    if current_round == 1:
        return 'D'

    # --- Edge Case: Last Round ---
    if current_round == total_rounds:
        return 'D'

    # --- Main Logic: Estimate other cooperators ---

    # Get history from the previous round (round t-1)
    previous_round_actions = history[current_round - 2] # history is 0-indexed for rounds

    observed_others_cooperating_last_round = 0
    for i in range(n):
        # Check if the player is not yourself and cooperated
        if (i + 1) != player_id and previous_round_actions[i] == 'C':
            observed_others_cooperating_last_round += 1

    estimated_others_cooperating = observed_others_cooperating_last_round

    # --- Aggressive Tweak for Low Thresholds ---
    # If only 0 or 1 other cooperator is needed to reach the threshold
    if (m - 1) <= 1:
        # Lower the bar slightly for cooperation if the threshold is easy to meet
        # We cooperate if we think at least m-2 others will cooperate.
        # Example: if m=2, m-1=1, m-2=0. Cooperate if we think >=0 others will.
        required_others = max(0, m - 2) # Ensure it's not negative
        if estimated_others_cooperating >= required_others:
            return 'C'
        else:
            return 'D'

    # --- Standard Aggressive Decision Rule ---
    required_others = m - 1
    if estimated_others_cooperating >= required_others:
        return 'C'
    else:
        return 'D'

```

This strategy is designed to be a strong contender by being smart about when to join the collective and when to go it alone, always prioritizing the maximization of its own payoff within the game's constraints.
'''

description_AGGRESSIVE_11 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to be adaptive and robust, and focused on maximizing individual payoff through calculated risk-taking.

## Aggressive Strategy: The "Exploitative Risk-Taker"

This strategy is built on the premise that while cooperation can yield high rewards, it requires coordination. Without explicit communication, relying on others to cooperate is a gamble. This strategy aims to exploit situations where cooperation is likely to succeed due to others' efforts, while minimizing its own contribution to the cooperative endeavor when the risk is high or its own contribution is not critical.

**Core Principle:** Be a "free-rider" when possible, but a "contributor" when it's a guaranteed win or strategically essential to trigger the reward.

### 1. Decision Rules: When to Cooperate (C) vs. Defect (D)

The decision hinges on a calculated assessment of the probability of reaching the cooperation threshold 'm', considering the current round and the historical behavior of other players.

**Key Metric: "Cooperation Confidence" (CC)**

We will maintain a dynamic measure called "Cooperation Confidence" (CC) for each player. This represents our assessment of the likelihood that a specific player will cooperate in the current round.

*   **Initialization:** For each player `i`, `CC_i` starts at a baseline value. Since we don't know anything initially, a neutral or slightly pessimistic starting point is appropriate for an aggressive strategy. Let's start with `CC_i = 0.5` for all `i`.
*   **Update Rule:** After each round `t`, we update `CC_i` based on player `i`'s observed action:
    *   If player `i` played `C` in round `t`: `CC_i = (CC_i * (t-1) + 1) / t` (Weighted average, giving more weight to recent history)
    *   If player `i` played `D` in round `t`: `CC_i = (CC_i * (t-1) + 0) / t`

**Decision Logic for Player `i` in Round `t`:**

My decision to play `C` or `D` in round `t` is based on the following conditions, evaluated in order:

1.  **Guaranteed Win (No Risk to Me):**
    *   If the number of players I'm *confident* will cooperate (i.e., `CC_j > threshold_high` for a certain number of players `j ≠ i`) plus myself playing `C` is *guaranteed* to meet or exceed `m`, then I will play `C`.
    *   **Threshold Definition:** Let `target_cooperators_for_me = m - 1`. I will play `C` if the number of players `j ≠ i` for whom `CC_j >= 0.8` (this is a high confidence threshold) is greater than or equal to `target_cooperators_for_me`.
    *   *Reasoning:* This is the purest "free-rider" scenario where my cooperation is not essential, but the reward is guaranteed due to others' likely contributions. I get the reward `k` with minimal personal investment.

2.  **Last Chance to Trigger Reward (My Contribution is Crucial):**
    *   If I am *currently* the only player whose cooperation is needed to reach the threshold `m`, and the current number of *expected* cooperators (based on `CC_j`) is less than `m`, I will play `C`.
    *   **Condition:** If `(sum of CC_j for all j != i) + 1 >= m` and `(number of players j != i with CC_j >= 0.6) < m - 1`, then I play `C`.
    *   *Reasoning:* This is a calculated risk to secure the reward. I'm only contributing if my contribution is the *critical* factor to unlock the payoff for everyone. I'm not wasting a contribution on a low-probability cooperative outcome. The `0.6` threshold for others is a moderate confidence level – I'm willing to make the final push if there's a reasonable chance others *might* have cooperated.

3.  **High Probability of Success (Exploitative Cooperation):**
    *   If the total *expected* number of cooperators (sum of `CC_j` for all `j ≠ i`, plus a potential `C` from me) is significantly above `m`, I will lean towards cooperating.
    *   **Condition:** If `(sum of CC_j for all j != i) + 1 >= m * 1.5` (meaning the expected cooperation level is 50% above the minimum required), I play `C`.
    *   *Reasoning:* This indicates a strong likelihood that the threshold will be met. By contributing, I ensure I get the reward, and my contribution is unlikely to be "wasted" if the group were to fail. This is an aggressive move to secure a win when the odds are strongly in favor.

4.  **Default: Defect (Aggressive Stance):**
    *   In all other cases, I will play `D`.
    *   *Reasoning:* If the conditions above are not met, it means either:
        *   The probability of reaching `m` is too low to justify my contribution (high risk, uncertain reward).
        *   My contribution is not critical, and I can potentially free-ride if others cooperate. My default is to maximize my private endowment (1) and hope for others to take the risk.

**Thresholds and Weights:** The specific values (0.8, 0.6, 1.5) are tunable. The aggressive nature comes from setting high thresholds for voluntary cooperation and only contributing when it's a clear win, a critical last resort, or a very high probability scenario.

### 2. Handling Edge Cases

*   **Round 1 (t=1):**
    *   We have no history. `CC_i` for all `i` is initialized to 0.5.
    *   **Decision Rule Application:**
        *   Condition 1 (Guaranteed Win): Unlikely to be met as `CC_j` are all 0.5.
        *   Condition 2 (Last Chance): Possible if `m` is very small (e.g., `m=2`). If `m=2`, and `CC_j < 0.6` for all `j != i`, and `1` (my C) is needed, I'll play `C`.
        *   Condition 3 (High Probability): Unlikely to be met with initial `CC_j`.
        *   **Default:** We will likely play `D` in Round 1 unless `m` is very small and my cooperation is the only thing that can trigger the reward. This is the most conservative aggressive approach – observe first.

*   **Last Round (t=r):**
    *   The `CC_i` values are at their most informed. The decision rules are applied as usual.
    *   **Key Consideration:** In the last round, there are no future rounds to "punish" or "reward" behavior. However, our strategy is designed to maximize current payoff, so the logic remains the same. The `CC_i` values accurately reflect the historical tendencies of other players. If a player has a history of cooperation, their `CC_i` will be high, making them more likely to be part of a successful coalition. If they have a history of defection, their `CC_i` will be low, and we won't rely on them.

*   **Early Rounds (t < r/2):**
    *   The `CC_i` values are still developing. The strategy will be more inclined to play `D` as confidence in others' behavior is low. It will primarily look for conditions where cooperation is highly likely or absolutely necessary and achievable. This is the "observation and opportunistic exploitation" phase.

*   **Middle Rounds (r/2 <= t < r):**
    *   The `CC_i` values are more robust. The strategy will start to more confidently apply the decision rules, particularly conditions 1 and 3, seeking to exploit emerging patterns of cooperation. It will also be more willing to contribute if it's the last chance (condition 2) as the number of remaining rounds decreases.

*   **Late Rounds (t close to r, but not the last round):**
    *   Similar to middle rounds, but with an increasing emphasis on securing payoff in the remaining rounds. The strategy becomes more decisive in cooperating if the reward is highly probable (condition 3) and less hesitant to make the critical contribution (condition 2).

### 3. Aggressive Mindset Alignment

*   **Exploitation of Free-Riders:** The strategy is designed to benefit from others who cooperate by keeping its own endowment and only contributing when it's strategically advantageous.
*   **Calculated Risk, Not Blind Trust:** Cooperation is not a default. It's a decision made based on calculated probabilities and the potential for a guaranteed high payoff.
*   **Maximizing Individual Gain:** The goal is to maximize the player's own total payoff. This involves taking advantage of cooperative outcomes orchestrated by others, or contributing only when it's a near-certain win.
*   **No Altruism:** There's no built-in mechanism to "teach" cooperation or to ensure the group's success beyond what benefits the individual. If the group fails to reach 'm', the player gets their private endowment (1) without contributing.
*   **Adaptability as a Weapon:** By constantly updating `CC_i`, the strategy learns and adapts to the evolving behavior of opponents. This allows it to pivot from being a pure free-rider to a strategic contributor when the game state demands it for individual gain.

### Pseudocode Outline

```pseudocode
// Global variables
COOPERATION_CONFIDENCE = {} // Dictionary: player_index -> confidence_score
CURRENT_ROUND = 1
TOTAL_ROUNDS = r
MIN_COOPERATORS = m
REWARD_FACTOR = k
HIGH_CONFIDENCE_THRESHOLD = 0.8
CRITICAL_CONFIDENCE_THRESHOLD = 0.6
HIGH_PROB_MULTIPLIER = 1.5

// Initialize
function initialize_strategy():
    for i from 1 to n:
        COOPERATION_CONFIDENCE[i] = 0.5 // Neutral starting point

// Update confidence based on observed actions
function update_confidence(player_index, action, current_round):
    if action == 'C':
        COOPERATION_CONFIDENCE[player_index] = (COOPERATION_CONFIDENCE[player_index] * (current_round - 1) + 1) / current_round
    else: // action == 'D'
        COOPERATION_CONFIDENCE[player_index] = (COOPERATION_CONFIDENCE[player_index] * (current_round - 1) + 0) / current_round

// Main decision function for player i
function decide_action(player_index, round_history):
    current_round = CURRENT_ROUND

    // --- Handle Edge Cases ---
    if current_round == 1:
        // In round 1, be cautious, only cooperate if essential for a minimal m
        num_potential_cooperators_high_conf = count_players_with_confidence_above(HIGH_CONFIDENCE_THRESHOLD, player_index)
        if num_potential_cooperators_high_conf >= MIN_COOPERATORS - 1:
            return 'C' // Guaranteed win, even if m=2 and others are expected to cooperate
        else:
            return 'D' // Default to D if no guarantee and m is not trivially met

    // --- Decision Rules ---

    // 1. Guaranteed Win (No Risk to Me)
    num_potential_cooperators_high_conf = count_players_with_confidence_above(HIGH_CONFIDENCE_THRESHOLD, player_index)
    if num_potential_cooperators_high_conf >= MIN_COOPERATORS - 1:
        return 'C'

    // 2. Last Chance to Trigger Reward (My Contribution is Crucial)
    expected_cooperators_from_others = calculate_sum_of_confidences(player_index)
    num_potential_cooperators_moderate_conf = count_players_with_confidence_above(CRITICAL_CONFIDENCE_THRESHOLD, player_index)
    if (expected_cooperators_from_others + 1) >= MIN_COOPERATORS and num_potential_cooperators_moderate_conf < MIN_COOPERATORS - 1:
        return 'C'

    // 3. High Probability of Success (Exploitative Cooperation)
    if (expected_cooperators_from_others + 1) >= MIN_COOPERATORS * HIGH_PROB_MULTIPLIER:
        return 'C'

    // 4. Default: Defect (Aggressive Stance)
    return 'D'

// Helper functions
function count_players_with_confidence_above(threshold, exclude_player_index):
    count = 0
    for p_idx, conf_score in COOPERATION_CONFIDENCE.items():
        if p_idx != exclude_player_index and conf_score >= threshold:
            count += 1
    return count

function calculate_sum_of_confidences(exclude_player_index):
    sum_conf = 0
    for p_idx, conf_score in COOPERATION_CONFIDENCE.items():
        if p_idx != exclude_player_index:
            sum_conf += conf_score
    return sum_conf

// To be called at the end of each round
function finalize_round(round_actions):
    for p_idx, action in round_actions.items():
        update_confidence(p_idx, action, CURRENT_ROUND)
    CURRENT_ROUND += 1
```
'''

description_AGGRESSIVE_12 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while being robust to diverse opponent behaviors in a repeated game setting. This strategy prioritizes exploiting opportunities for high individual payoffs and punishes perceived defection.

## Aggressive Strategy: "The Opportunistic Predator"

**Core Philosophy:** Exploit cooperation when it's safe and profitable, but defect aggressively when cooperation is uncertain or when others are defecting. This strategy aims to be a net beneficiary by capitalizing on the reward multiplier `k` when possible, but not at the expense of personal loss.

---

### 1. Decision Rules: When to Cooperate (C) vs. Defect (D)

The decision to cooperate or defect is based on a dynamic assessment of the current round's required cooperation level and the perceived willingness of others to cooperate.

**Primary Decision Logic:**

*   **If the number of players cooperating in the *previous* round (or perceived likely to cooperate in the current round) is sufficiently high to meet the threshold `m`, *and* my individual payoff in the previous round from cooperating was positive (i.e., the reward `k` was obtained), then Cooperate (C).**
    *   **Rationale:** This is the "opportunistic" element. If others have successfully met the threshold and I benefited, it's a signal that cooperation is working and I can ride that wave for another profitable round. The risk is low because the collective has already demonstrated its capacity.

*   **If the number of players cooperating in the *previous* round was *less than* `m`, *or* if I received a personal loss (payoff of 1) from cooperating in the previous round (meaning `k` was *not* obtained), then Defect (D).**
    *   **Rationale:** This is the "predatory" and "punishment" element. If cooperation failed to meet the threshold, I don't want to be the sucker who cooperated and got nothing. If cooperation *did* meet the threshold but I still only got 1 (e.g., because many others defected and *just* tipped the balance), I'm not seeing enough benefit to risk cooperating again. This also serves as a signal to others that I will not be exploited.

**Refinement for Uncertainty:**

*   **The "Last Chance" Cautious Cooperation:** If the current round is the **second-to-last round** and the total number of cooperators in *all previous rounds combined* (excluding the current round) is very close to `m * (current_round - 1)`, consider a slightly more lenient threshold for cooperating. This is a heuristic to try and secure the reward if the group has been consistently near the threshold. However, this is secondary to the primary logic.

---

### 2. Handling Edge Cases: First Round, Last Round, and Near the End

**A. The First Round (t=1):**

*   **Decision:** **Defect (D)**
*   **Rationale:** In the absence of any history, cooperation is a pure gamble. To be aggressive and avoid being the sole cooperator in an unknown environment, the initial move is to protect your endowment. This is a default aggressive posture to observe the landscape before committing.

**B. The Last Round (t=r):**

*   **Decision:** **Defect (D)**
*   **Rationale:** In the final round, there are no future consequences for defection. The only goal is to maximize your payoff in this single round. Since the reward `k` only matters if `m` players cooperate, and there's no incentive to build future cooperation, it's always better to keep your endowment of 1, regardless of what others do.

**C. The Second-to-Last Round (t=r-1):**

*   **Decision:** **Apply the "Last Chance" Cautious Cooperation rule described above, *but* only if the overall historical cooperation has been close to the target.** Otherwise, default to the primary decision logic. The primary logic will likely lead to Defection if `m` was not met in `t=r-2`.
*   **Rationale:** This is a strategic attempt to secure the `k` multiplier if the game has been on the cusp of successful cooperation for most of its duration. It's a calculated risk to push for one final reward.

---

### 3. Aggressive Mindset Alignment

This strategy is aggressive in the following ways:

*   **Initial Defection:** It starts by protecting personal assets, reflecting an unwillingness to be exploited from the outset.
*   **Punitive Defection:** It defects when others fail to cooperate or when cooperation doesn't yield a clear benefit. This actively punishes free-riders and those who don't contribute enough to secure the reward.
*   **Exploitative Cooperation:** It cooperates only when the risk is demonstrably low and the reward is almost guaranteed due to others' actions. It seeks to benefit from collective effort without taking undue personal risk.
*   **No Altruism:** There's no inherent desire to foster cooperation for its own sake. Cooperation is a tool for personal gain, and if it doesn't serve that purpose, it's abandoned.
*   **Maximizing Individual Payoff:** The focus is always on maximizing `π_i`. If defecting guarantees a payoff of 1 and cooperating has a non-zero chance of yielding 0 (if the threshold isn't met), defecting is the safer, and thus more aggressive, move from a risk-aversion perspective aimed at absolute gain.

---

### Pseudocode Representation

```
Strategy: Opportunistic Predator

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

State Variables:
  history = list of lists (each inner list represents actions in a round)
  current_round = 1

Function DecideAction(player_id, current_round, history, n, m, k):

  # --- Edge Cases ---
  if current_round == 1:
    return "D" # Defect in the first round

  if current_round == r:
    return "D" # Defect in the last round

  # --- Previous Round Analysis ---
  previous_round_actions = history[current_round - 2] # Indexing from 0 for history
  num_cooperators_prev = count_C(previous_round_actions)
  my_payoff_prev = calculate_payoff(player_id, previous_round_actions, n, m, k)

  # --- Primary Decision Logic ---
  # Condition 1: Previous cooperation was successful AND I benefited
  if num_cooperators_prev >= m and my_payoff_prev > 1:
    return "C" # Cooperate opportunistically

  # Condition 2: Previous cooperation failed OR I didn't benefit from it
  else:
    return "D" # Defect as punishment/precaution

# --- Helper Functions (to be implemented) ---
function count_C(actions_list):
  # Counts the number of 'C' in the list
  pass

function calculate_payoff(player_id, actions_list, n, m, k):
  # Calculates payoff for a specific player based on actions in a round
  # Requires knowing if player_id was C or D and the total cooperators
  pass

# Note: The "Last Chance" rule for t=r-1 is a more complex heuristic that
# would require aggregating historical cooperation levels. For a purely
# aggressive, robust strategy, sticking to the primary logic for t=r-1
# is also valid and simpler, making the "D" decision more likely.
# If implementing the "Last Chance" rule, it would be added as a condition
# between the edge cases and the primary logic for t=r-1.
```
'''

description_AGGRESSIVE_13 = '''
This is a fascinating Collective Risk Dilemma! The core tension lies between individual self-interest (keeping your endowment) and collective benefit (achieving the cooperation threshold for a higher reward). An aggressive strategy must exploit this tension while safeguarding against exploitation by others.

Here’s a design for an aggressive, adaptive, and robust strategy:

## Strategy: "The Opportunistic Enforcer"

This strategy aims to maximize individual payoff by leveraging cooperation when it's beneficial and defecting to secure gains when cooperation falters. It's aggressive because it prioritizes taking advantage of favorable situations, but it's also tempered by the need to avoid catastrophic losses.

### Core Philosophy:

*   **Exploit when possible:** If the collective is cooperating and the reward is within reach, join in to secure your share.
*   **Punish defection:** If others are not cooperating sufficiently, defect to avoid contributing to their gains without receiving the reward.
*   **Adaptive threshold:** The decision to cooperate or defect will be dynamically adjusted based on the observed behavior of other players and the proximity to the cooperation threshold.
*   **No implicit trust:** Never assume others will cooperate. Your decision is based on observed actions, not pre-conceived notions of their intent.

### 1. Decision Rules:

The strategy will primarily use a **"cooperation threshold"** that dictates when it's willing to cooperate. This threshold is not fixed but will adjust based on game dynamics.

**Let's define some key terms for the strategy:**

*   `current_round`: The current round number (1 to r).
*   `observed_cooperators`: The number of players observed to have cooperated in the *previous* round.
*   `my_previous_action`: The action taken by this strategy in the *previous* round.
*   `cooperation_target`: The number of players needed to cooperate for a successful outcome (this is `m`).
*   `cooperation_confidence`: A measure of how likely the strategy believes cooperation will succeed. This will be dynamically adjusted.

**Decision Logic for Player `i` in round `t`:**

1.  **Calculate `observed_cooperators` from round `t-1`.**
2.  **Update `cooperation_confidence`:**
    *   If `observed_cooperators` was greater than or equal to `m` in the previous round, and my action was 'C', increase `cooperation_confidence`.
    *   If `observed_cooperators` was less than `m` in the previous round, and my action was 'D', decrease `cooperation_confidence`.
    *   If `observed_cooperators` was greater than or equal to `m` in the previous round, and my action was 'D', slightly increase `cooperation_confidence` (because others are cooperating, making it safer for me to potentially join).
    *   If `observed_cooperators` was less than `m` in the previous round, and my action was 'C', significantly decrease `cooperation_confidence` (this is a costly mistake).
    *   *Initial `cooperation_confidence` can be set to a neutral value, perhaps `m / n` or a small percentage above it.*

3.  **Determine Action:**

    *   **If `current_round == 1` (First Round):**
        *   **Aggressive Gamble:** Play **C**. This is the most aggressive initial move. It sets the tone and attempts to initiate cooperation. If others defect, you'll be punished, but this is a calculated risk to potentially trigger the reward early.
        *   *Rationale: You can't exploit cooperation if you never attempt it. In the first round, everyone is equally uncertain. A bold 'C' can be a powerful signal and might induce others to follow.*

    *   **If `current_round == r` (Last Round):**
        *   **Pure Self-Interest:** Play **D**.
        *   *Rationale: In the final round, there are no future consequences for defection. Maximizing your immediate payoff by keeping your endowment is the rational choice, regardless of what others do.*

    *   **If `1 < current_round < r` (Middle Rounds):**
        *   **Calculate `cooperation_needed_from_others`:** `m - observed_cooperators` (If this is negative, it means the threshold was met).
        *   **If `observed_cooperators >= m` AND `my_previous_action == 'C'`:**
            *   **Continue Cooperation:** Play **C**.
            *   *Rationale: The system is working. Keep cooperating to maintain the reward.*
        *   **If `observed_cooperators >= m` AND `my_previous_action == 'D'`:**
            *   **Opportunistic Join:** Play **C**.
            *   *Rationale: Others have successfully met the threshold. You can now jump in and get the reward without the risk of carrying the cooperation burden.*
        *   **If `observed_cooperators < m`:**
            *   **Aggressive Defection:** Play **D**.
            *   *Rationale: The collective failed to meet the threshold. You will not contribute your endowment to a project that doesn't yield the collective reward. This punishes those who didn't cooperate sufficiently and protects your own endowment.*

**Refinement on `cooperation_confidence`:** Instead of a simple variable, we can use a dynamically adjusted threshold, let's call it `cooperation_threshold_multiplier`.

*   `cooperation_threshold_multiplier` starts at 1.0.
*   If `observed_cooperators >= m` and `my_previous_action == 'C'`: `cooperation_threshold_multiplier` increases slightly (e.g., `* 1.05`).
*   If `observed_cooperators < m` and `my_previous_action == 'D'`: `cooperation_threshold_multiplier` decreases slightly (e.g., `/ 1.05`).
*   If `observed_cooperators >= m` and `my_previous_action == 'D'`: `cooperation_threshold_multiplier` increases moderately (e.g., `* 1.1`).
*   If `observed_cooperators < m` and `my_previous_action == 'C'`: `cooperation_threshold_multiplier` decreases significantly (e.g., `/ 1.2`).

**Revised Decision Logic (Middle Rounds):**

*   **If `observed_cooperators >= m`:** Play **C**. (Even if I defected last round, others succeeded, so it's safe and rewarding to join now).
*   **If `observed_cooperators < m`:** Play **D**. (The collective is failing. Don't be the sucker.)

This simplified logic is more aggressive and directly reactive to past outcomes. The `cooperation_threshold_multiplier` is more for *internal calculation* if we wanted to fine-tune the decision between C and D *when the threshold is nearly met*, but for pure aggression, direct action based on past outcomes is key.

Let's stick to the simpler, more aggressive rules:

**Revised Decision Rules (Simplified & Aggressive):**

*   **Round 1:** Cooperate (C) - **Calculated Gamble.**
*   **Last Round (r):** Defect (D) - **Pure Self-Interest.**
*   **Middle Rounds (1 < t < r):**
    *   Observe `observed_cooperators` from round `t-1`.
    *   **If `observed_cooperators >= m`:** Cooperate (C) - **Opportunistic Gain.**
    *   **If `observed_cooperators < m`:** Defect (D) - **Punishment & Self-Preservation.**

### 2. Handling Edge Cases:

*   **First Round (t=1):** As detailed above, the strategy initiates with **C**. This is a deliberate risk to try and kickstart a successful cooperation cycle. If it backfires, the subsequent rounds will enforce a harsher defensive stance.
*   **Last Round (t=r):** As detailed above, the strategy defects (**D**) in the final round. There's no future to incentivize cooperation, so maximizing the immediate payoff is paramount.
*   **Initial State (Before Round 1):** All history is blank. `observed_cooperators` is effectively 0.
*   **What if `m` is very close to `n`?** The strategy remains the same. If `m = n-1`, you'll need almost everyone to cooperate. The strategy still defects if the threshold isn't met.
*   **What if `m` is very close to 2?** The strategy still works. Even a small number of cooperators can trigger the reward, but if they fail, you defect.

### 3. Aggression Explained:

*   **"Opportunistic Enforcer":** The name itself suggests aggression. It's about opportunistically stepping in when cooperation is already established and enforcing defection when it falters.
*   **Round 1 Cooperation:** This is the riskiest and most aggressive move. Instead of playing it safe with 'D' and waiting to see what others do, it takes the initiative. This can lead to a higher payoff if others reciprocate, or a lower one if they exploit your cooperation. The payoff from 'C' is 0 + k (if successful), which is the highest possible individual gain in a single round.
*   **Immediate Response to Failure:** The strategy doesn't tolerate the collective failing to meet `m`. As soon as `observed_cooperators < m`, it immediately switches to 'D'. This is punishing and aims to deter future failures by making players realize that if they don't cooperate sufficiently, they won't get the reward and will also have their endowments kept by others.
*   **Exploiting Successful Cooperation:** If others *do* manage to reach the threshold (`observed_cooperators >= m`), the strategy *always* cooperates in the next round, even if it defected in the previous one. This is pure opportunistic exploitation – you get the reward without having to bear the risk of initiating cooperation if you weren't the one to do so.

### Pseudocode Representation:

```pseudocode
function "The Opportunistic Enforcer"(game_state):
    n = game_state.parameters.n
    r = game_state.parameters.r
    m = game_state.parameters.m

    current_round = game_state.current_round
    history = game_state.history  // List of (player_actions_in_round_t, payoffs_in_round_t)

    // Calculate observed cooperators from the *previous* round
    observed_cooperators_last_round = 0
    if current_round > 1:
        previous_round_actions = history[current_round - 1].player_actions // Assuming history is 0-indexed for rounds
        for action in previous_round_actions:
            if action == 'C':
                observed_cooperators_last_round = observed_cooperators_last_round + 1

    // Decision Logic
    if current_round == 1:
        return 'C' // Aggressive Gamble

    else if current_round == r:
        return 'D' // Pure Self-Interest

    else: // Middle Rounds (1 < current_round < r)
        if observed_cooperators_last_round >= m:
            return 'C' // Opportunistic Gain
        else:
            return 'D' // Punishment & Self-Preservation

```

This strategy is designed to be simple in its core logic, making it robust. It reacts decisively to the most important outcome: whether the cooperation threshold was met in the previous round. This makes it aggressive by prioritizing rewards when available and punishing failures swiftly, all while adapting to the observed behavior of the collective.
'''

description_AGGRESSIVE_14 = '''
This is a fascinating challenge, and designing an aggressive yet adaptive strategy for the Collective Risk Dilemma requires a delicate balance. The core tension lies between defecting to guarantee a personal payoff of 1 (or more if cooperation succeeds) and cooperating to potentially achieve a higher payoff of 2 (or more if others also cooperate), but risking a payoff of 1 if the threshold isn't met.

My aggressive strategy, which I'll call **"Opportunistic Escalation,"** focuses on maximizing personal gain by leveraging the information from past rounds and aggressively pushing for a cooperative outcome when the conditions are favorable, while swiftly reverting to defection when cooperation fails or is threatened.

---

## Opportunistic Escalation Strategy

**Core Philosophy:** "If cooperation benefits *me*, I'll push for it. If cooperation is threatened or fails, I'll exploit the situation for maximum personal gain." This strategy is aggressive because it prioritizes its own payoff, actively seeks opportunities for higher rewards, and quickly punishes perceived failures of cooperation by others.

---

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) hinges on a dynamic assessment of the likelihood of the cooperation threshold (`m`) being met and the perceived trustworthiness of the other players based on historical data.

**General Rule (Rounds t > 1):**

*   **If (Past Average Cooperation Rate > Threshold of Trust) AND (Current Estimated Cooperation Probability > m/n * 1.1)**: **Cooperate (C)**
    *   **Rationale:** This is the "opportunistic" part. If the historical cooperation rate suggests a general tendency towards cooperation, and the current situation *looks* promising for reaching the threshold (even with a slight buffer to be safe), I'll take the risk. This is aggressive because it initiates cooperation when others might still be hesitant, hoping to be the catalyst.
*   **Else**: **Defect (D)**
    *   **Rationale:** This is the "escalation" and "robustness" part. If there's any doubt about the cooperation threshold being met, or if past behavior has been uncooperative, I will defect. This guarantees my individual payoff of 1, and if others *do* cooperate and the threshold is met, I still get the higher payoff of `1 + k` from defecting.

**Defining the "Threshold of Trust":**

This parameter will be adaptive. It starts at a moderate level and decreases as the game progresses and more cooperative behavior is observed across all players.
*   Let `AvgCoopRate_t-1` be the average cooperation rate of *all* players in round `t-1`.
*   **Threshold of Trust** can be defined as:
    *   Initially: `0.75` (This is a starting point; can be tuned)
    *   Decreases by `0.02` for every `0.1` increase in `AvgCoopRate_t-1` (up to a floor, e.g., `0.5`). This makes it more likely to trust as others become more trustworthy.

**Defining the "Current Estimated Cooperation Probability":**

This is a more immediate assessment based on the actions of *other* players in the immediately preceding round.
*   Let `MyAction_t-1` be my action in round `t-1`.
*   Let `OthersCoopCount_t-1` be the number of players (excluding myself) who cooperated in round `t-1`.
*   **Current Estimated Cooperation Probability** is calculated as: `(OthersCoopCount_t-1 + 1) / n` if `MyAction_t-1 == C` (this includes my own cooperation in the estimate).
    *   If `MyAction_t-1 == D`, this estimate is less relevant as my primary goal is to secure my own payoff. However, for the *purpose of deciding to cooperate in the next round*, I still consider the overall likelihood. A better estimate would be `(OthersCoopCount_t-1 + m) / n` to represent the *minimum* need, or `(OthersCoopCount_t-1 + (n-m) * 0.5) / n` for a more balanced "expected" outcome if I am defecting. Let's refine this:

    *   **If MyAction_t-1 == C:** My decision for round `t` relies on `(OthersCoopCount_t-1 + 1) / n`. If this value is sufficiently high (e.g., > `m/n * 1.1`), I might consider cooperating again.
    *   **If MyAction_t-1 == D:** My decision for round `t` is more cautious. I will only cooperate if `(OthersCoopCount_t-1 + 1) / n` is *significantly* higher than `m/n`. This is because I've already secured a decent payoff by defecting, and the risk of collective failure is higher if I'm not the one initiating widespread cooperation. A good heuristic here might be `(OthersCoopCount_t-1 + 1) / n > m/n * 1.3`.

**Refined Decision Rule for Round t > 1:**

Let `C_i_t-1` be 1 if player `i` cooperated in round `t-1`, 0 otherwise.
Let `NumCoop_t-1 = Σ(j=1 to n) C_j_t-1` (total cooperators in round `t-1`).
Let `MyAction_t-1` be my action in round `t-1`.

*   **If `MyAction_t-1 == C`:**
    *   Calculate `ProbCoopNext = (NumCoop_t-1 + 1) / n` (assuming I cooperate again).
    *   If `ProbCoopNext > m/n * 1.1` AND `AvgCoopRate_t-1 > Threshold of Trust`: **Cooperate (C)**
    *   Else: **Defect (D)**
*   **If `MyAction_t-1 == D`:**
    *   Calculate `ProbCoopNextIfIJoin = (NumCoop_t-1 + 1) / n`.
    *   If `ProbCoopNextIfIJoin > m/n * 1.3` AND `AvgCoopRate_t-1 > Threshold of Trust * 0.9` (slightly more lenient threshold if I defected): **Cooperate (C)**
    *   Else: **Defect (D)**

---

### 2. Handling Edge Cases

*   **First Round (t = 1):**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** In the absence of any historical data, the most aggressive and self-preserving move is to secure the guaranteed private payoff of 1. This also serves as a signal: I am not going to be the one taking the initial risk. This forces others to reveal their intentions first.

*   **Last Round (t = r):**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** In the final round, there is no future to build upon or punish. The only consideration is maximizing personal payoff. Defecting guarantees at least a payoff of 1, and if the threshold is met, I get `1 + k`. Cooperation carries the risk of a payoff of 1 if the threshold is not met, and the reward of `k` is diminished in its long-term impact. Aggression here means securing the sure thing.

*   **Near the End (e.g., last 10% of rounds, or if `r - t < 10`):**
    *   The strategy should gradually become more conservative and lean towards defection, mirroring the logic of the last round. The "Threshold of Trust" could be capped or adjusted to reflect this.
    *   **Decision:** Shift towards Defecting. The `Threshold of Trust` parameter will implicitly handle this as it might not decrease as much, making cooperation less likely. The multiplier `1.1` and `1.3` could also be increased slightly.

*   **All Players Defected in the Previous Round (`NumCoop_t-1 == 0`):**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** This indicates a strong tendency towards defection among the opponents. Cooperating would be extremely risky and unlikely to meet the threshold. Aggression means exploiting this lack of cooperation by securing the individual payoff.

*   **Threshold Met in the Previous Round (`NumCoop_t-1 >= m`):**
    *   **Decision:** **Cooperate (C)** (with a higher probability).
    *   **Rationale:** This is a strong positive signal. The system has proven capable of cooperation. I will lean heavily towards cooperating in the next round, assuming the "Threshold of Trust" is also met. This aggressive stance aims to capitalize on established cooperation and foster it further if it's already successful.

---

### 3. Aggressive Design Elements

*   **Initial Defection:** The strategy starts by defecting, making no concessions and demanding that others prove themselves. This is a power move.
*   **Proactive Cooperation (When Conditions are Right):** While cautious, the strategy *will* initiate cooperation if the historical and current indicators suggest a high probability of success. This is aggressive because it aims to be part of the solution, but only when the personal upside is significantly protected.
*   **Rapid Reversion to Defection:** The moment the conditions for cooperation appear to weaken (e.g., fewer cooperators than expected, a dip in the average cooperation rate), the strategy immediately defaults to defection. This punishes observed unreliability and protects the player's own payoff. This is aggressive because it quickly withdraws support and exploits the resulting individual security.
*   **"Over-the-top" Cooperation Thresholds:** The `1.1` and `1.3` multipliers in the decision rules mean the strategy only cooperates if it's *significantly* likely to succeed, not just barely. This is aggressive in that it demands a high degree of confidence before taking the risk, maximizing the chance of the reward *for itself*.
*   **Focus on Personal Payoff:** The strategy is explicitly designed to maximize its own payoff in every round, either by ensuring a high reward from successful cooperation or by guaranteeing a solid payoff through defection. There's no altruism or attempt to "save" the group at personal cost.
*   **Adaptable "Threshold of Trust":** This parameter allows the strategy to become more trusting of a genuinely cooperative environment, but it's slow to change and has floors, preventing it from becoming too naive. This means it can be aggressive in continuing cooperation once established, but it also takes a long time to forgive repeated defections.

---

### Pseudocode Representation

```pseudocode
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Strategy State
MyHistory = [] // List of (MyAction, NumCoopOthers, NumCoopAll) for each round
AvgCoopRateOverall = 0.0 // Rolling average of cooperation rate across all players
ThresholdOfTrust = 0.75 // Initial threshold for trusting others' cooperation

// Function to calculate MyPayoff in a round
function CalculateMyPayoff(my_action, num_cooperators_total):
    c_i = 1 if my_action == 'C' else 0
    if num_cooperators_total >= m:
        return (1 - c_i) + k
    else:
        return (1 - c_i)

// Main Decision Function for round t
function DecideAction(t):
    // Edge Case: First Round
    if t == 1:
        return 'D'

    // Edge Case: Last Round
    if t == r:
        return 'D'

    // Update Rolling Averages and Threshold of Trust (based on t-1)
    // This needs to be done *after* getting historical data from previous round
    // Assume MyHistory is populated with data from round t-1

    // Analyze previous round (t-1) data
    if not MyHistory.is_empty():
        last_round_data = MyHistory.last()
        MyAction_t_minus_1 = last_round_data.MyAction
        NumCoopOthers_t_minus_1 = last_round_data.NumCoopOthers
        NumCoopAll_t_minus_1 = last_round_data.NumCoopAll

        // Update AvgCoopRateOverall and ThresholdOfTrust (simplified for pseudocode)
        // A more robust update would consider ALL players' actions if available
        CurrentAvgCoopRate = NumCoopAll_t_minus_1 / n
        AvgCoopRateOverall = (AvgCoopRateOverall * (t - 2) + CurrentAvgCoopRate) / (t - 1) // For t > 2

        // Adjust ThresholdOfTrust based on observed cooperation
        // This is a simplified adaptation mechanism
        if CurrentAvgCoopRate > 0.5: // Only adjust if there's significant cooperation
            ThresholdOfTrust = max(0.5, ThresholdOfTrust - 0.02 * (CurrentAvgCoopRate - 0.5) * 2)


        // Decision Logic for Round t
        if MyAction_t_minus_1 == 'C':
            ProbCoopNext = (NumCoopAll_t_minus_1 + 1) / n // If I cooperate again
            if ProbCoopNext > (m / n) * 1.1 and AvgCoopRateOverall > ThresholdOfTrust:
                return 'C'
            else:
                return 'D'
        else: // MyAction_t_minus_1 == 'D'
            ProbCoopNextIfIJoin = (NumCoopAll_t_minus_1 + 1) / n // If I join
            // Slightly more lenient threshold if I defected last, as I already got a private gain
            if ProbCoopNextIfIJoin > (m / n) * 1.3 and AvgCoopRateOverall > ThresholdOfTrust * 0.9:
                return 'C'
            else:
                return 'D'
    else:
        // This case should not be reached if t > 1 as MyHistory won't be empty
        // but as a fallback:
        return 'D'

// Function to record round outcome
function RecordOutcome(my_action, num_cooperators_total, num_cooperators_others):
    MyHistory.append({
        "MyAction": my_action,
        "NumCoopAll": num_cooperators_total,
        "NumCoopOthers": num_cooperators_others
    })
```

---

This "Opportunistic Escalation" strategy aims to be a formidable opponent by being self-interested, reactive, and capable of seizing opportunities for higher rewards, all while robustly protecting its baseline payoff. It's aggressive because it doesn't shy away from taking calculated risks for greater gain and is quick to punish perceived failures by others.
'''

description_AGGRESSIVE_15 = '''
This is a fascinating challenge! Designing an aggressive, yet adaptive, strategy for a Collective Risk Dilemma where communication is forbidden requires a strong focus on maximizing personal gain while leveraging the collective to trigger the reward. Aggression in this context means being willing to defect strategically to gain an advantage, but not so recklessly that it jeopardizes the potential reward altogether.

Here's a breakdown of an aggressive strategy designed for this environment:

## Aggressive Dominance Strategy (ADS)

The core principle of ADS is to leverage the collective risk reward whenever possible, but to ensure the player benefits disproportionately if cooperation occurs, or to defect to secure their baseline payoff if cooperation is unlikely. It aims to "force the hand" of other players by creating scenarios where their defection becomes less attractive than cooperation, especially as the game progresses.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect in any given round (t) depends on a combination of the current game state (number of cooperators observed so far), the remaining rounds, and a calculated "risk appetite."

**Key Metric: Required Cooperation Count (RCC)**

For each round *t*, we will calculate the number of players that *still* need to cooperate to reach the threshold *m*.

*   `RCC_t = max(0, m - num_cooperators_observed_so_far_in_round_t)`

**Aggressive Cooperation Threshold (ACT)**

This is the crucial adaptive element. It's a dynamic threshold that dictates when we will commit to cooperating. Our aggression manifests in demanding a certain level of commitment from the group *before* we risk our endowment.

*   `ACT_t = max(1, floor(m * (1 - (remaining_rounds_t / r))))`

   *   **Explanation:**
        *   `remaining_rounds_t`: The number of rounds left including the current one.
        *   `r`: Total number of rounds.
        *   `(remaining_rounds_t / r)`: This ratio decreases as the game progresses.
        *   `1 - (remaining_rounds_t / r)`: This ratio increases as the game progresses, representing the "urgency" or proportion of the game played.
        *   `m * (...)`: Scales the urgency by the cooperation threshold *m*.
        *   `floor(...)`: Ensures we have a whole number of players.
        *   `max(1, ...)`: Guarantees that ACT is at least 1, ensuring we never demand zero cooperation from others.

**Decision Logic for Player i in Round t:**

1.  **If `num_cooperators_observed_so_far_in_round_t < m` AND `RCC_t > ACT_t`:**
    *   **Decision: Defect (D)**
    *   **Rationale:** The number of players still needed to cooperate is significantly higher than our current tolerance for risk (ACT). It's too early or too risky to commit to cooperation. We protect our endowment while observing if others will step up.

2.  **If `num_cooperators_observed_so_far_in_round_t < m` AND `RCC_t <= ACT_t`:**
    *   **Decision: Cooperate (C)**
    *   **Rationale:** The remaining cooperation needed is now within our acceptable risk appetite. We believe the likelihood of reaching the threshold is high enough, and we are willing to contribute to secure the collective reward. This is where our "aggressive" cooperation happens – we are willing to be one of the *m* players when the odds are in our favor.

3.  **If `num_cooperators_observed_so_far_in_round_t >= m`:**
    *   **Decision: Cooperate (C)**
    *   **Rationale:** The threshold has already been met or exceeded. By cooperating, we guarantee our share of the reward without risking our baseline endowment. This is the "smart" aggressive move – lock in the bonus.

**Summary of Decision Logic:**

*   **Early Rounds / Low Urgency:** Defect to see if others will commit.
*   **Mid-Game / Increasing Urgency:** If others are still needed, but the urgency is high enough (i.e., `RCC_t` is within `ACT_t`), then cooperate to trigger the reward.
*   **Threshold Met:** Always cooperate to secure the bonus.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   `num_cooperators_observed_so_far_in_round_1 = 0`
    *   `RCC_1 = m`
    *   `ACT_1 = max(1, floor(m * (1 - (r / r)))) = max(1, floor(m * 0)) = 1`
    *   **Decision:** Since `RCC_1 (m) > ACT_1 (1)` (as `m > 1`), we will **Defect (D)** in the first round. This is the aggressive stance – force others to show their hand first.

*   **Last Round (t=r):**
    *   `remaining_rounds_r = 1`
    *   `ACT_r = max(1, floor(m * (1 - (1 / r))))`
    *   **If `num_cooperators_observed_so_far_in_round_r < m`:**
        *   **Decision:** This is the critical moment.
            *   If `RCC_r > ACT_r`: We defect. This means the threshold is still too high relative to our very last chance. This is a calculated gamble that the group might *barely* fail, and we secure our baseline 1.
            *   If `RCC_r <= ACT_r`: We **Cooperate (C)**. This means the remaining number needed is small enough that even in the last round, we are willing to contribute to try and grab the bonus. This is the ultimate "risk it for the biscuit" moment.
    *   **If `num_cooperators_observed_so_far_in_round_r >= m`:**
        *   **Decision: Cooperate (C)**. Secure the bonus.

*   **Situations where `m` is very close to `n`:**
    *   `ACT_t` will remain high for longer, leading to more defections in early and mid-game. This strategy is designed to punish low levels of cooperation when *m* is high.

*   **Situations where `m` is close to 2:**
    *   `ACT_t` will be low, and we will cooperate earlier if the group is showing signs of cooperation.

### 3. Aggressive Mindset Alignment

*   **Proactive Defection:** We don't passively wait for cooperation. We actively defect in the early stages to conserve our endowment and gauge the group's willingness to risk.
*   **Calculated Risk-Taking:** We are willing to cooperate, but only when the conditions are favorable, meaning the probability of success is high enough given the remaining rounds. We don't "blindly" cooperate.
*   **Exploiting Threshold:** Our strategy is designed to push the group towards the threshold *m*. By defecting when `RCC_t > ACT_t`, we increase the pressure on others to cooperate. If they don't, we are safe with our endowment. If they do, we might be one of the last to join, potentially maximizing our payoff if the reward kicks in (`1 - c_i + k`).
*   **"If they can, I will":** Once the threshold is met, we immediately shift to cooperation to guarantee the reward. This is a powerful signal that when cooperation benefits us and the group, we are willing to contribute.

### Pseudocode Representation

```pseudocode
// Global state
num_cooperators_observed_so_far = 0 // For the entire tournament history (or reset per player group if not tournament)
rounds_played = 0

function MakeDecision(player_i_history, game_parameters):
    n = game_parameters.n
    r = game_parameters.r
    m = game_parameters.m
    k = game_parameters.k

    current_round = rounds_played + 1 // Assuming rounds_played is updated after each round
    remaining_rounds_t = r - current_round + 1

    // Count cooperators observed *in the current round* from player_i_history
    // This requires a way to access actions of *other* players in the current round.
    // For a true simultaneous move game, this means we need to know the collective outcome of the round BEFORE making the decision for the NEXT round.
    // Assuming player_i_history contains the full history of actions for all players up to the *previous* round.
    // Let's reframe: We need to know the number of cooperators *in the previous round's outcome* to make the current round's decision.

    // Re-interpreting for perfect information: We know the outcomes of ALL previous rounds.
    // Let's assume 'history' is a list of lists, where history[t] is a list of actions taken by all players in round t.

    // We need to simulate/track the total number of cooperators *if* we were to cooperate in the current round.
    // This strategy is reactive to the *actual* number of cooperators in the *previous* round.

    // Let's assume history contains the outcome of all rounds up to round 'current_round - 1'.
    // If current_round == 1, history is empty.

    num_cooperators_in_previous_round = 0
    if current_round > 1:
        previous_round_actions = history[current_round - 1] // Assuming history[0] is round 1, history[r-1] is round r
        for action in previous_round_actions:
            if action == 'C':
                num_cooperators_in_previous_round += 1

    // Calculate threshold for THIS round's decision
    if num_cooperators_in_previous_round >= m:
        // If threshold was met last round, assume it will be met again (or continue to be met)
        // This ensures we don't break a good streak.
        decision = 'C'
    else:
        // Need to calculate the number of cooperators *if* we cooperate this round
        // This is complex because we don't know what *others* will do.
        // The strategy must be based on the *observed* state, not predictions of future moves.

        // Let's use the count of cooperators observed *across all previous rounds* for a robust 'state'
        // This is tricky as the state should ideally be round-specific.
        // The 'num_cooperators_observed_so_far' should track how many total players *have* cooperated across the game for a round-based count of *active* cooperators.
        // A simpler interpretation: What is the state of the game *going into* this round?

        // Let's refine: The strategy should adapt to the *collective behavior* observed.

        // Aggressive Dominance Strategy v2 (more robust to simultaneous moves)
        // We base our decision on how many cooperators were seen in the *previous* round.

        num_cooperators_this_round_if_i_coop = num_cooperators_in_previous_round + 1 // Assuming we cooperate

        // Aggressive Cooperation Threshold for THIS round
        ACT_t = max(1, floor(m * (1 - (remaining_rounds_t / r))))

        // Number of players STILL needed for cooperation in THIS round, given the previous round's outcome.
        // This is where the 'risk' is calculated.
        // If num_cooperators_in_previous_round < m:
        //   RCC_t = m - num_cooperators_in_previous_round
        // Else:
        //   RCC_t = 0 (threshold already met or exceeded)

        // Let's think about the state for the *current* round.
        // We observe the outcome of round (t-1). This dictates our decision for round t.

        // Decision based on outcome of ROUND t-1 to make decision for ROUND t.
        num_cooperators_in_prev_round_outcome = 0 // Count from previous round's data
        if current_round > 1:
             // Sum of 'C' actions in history[current_round - 2]
             previous_round_actions = history[current_round - 2] // history is 0-indexed for rounds
             for action in previous_round_actions:
                if action == 'C':
                    num_cooperators_in_prev_round_outcome += 1

        // --- REVISED DECISION LOGIC FOR ROUND t ---

        // 1. Check if threshold was met in the previous round.
        if num_cooperators_in_prev_round_outcome >= m:
            decision = 'C' // Lock in the bonus. Always cooperate if the reward is already guaranteed.
        else:
            // Threshold was NOT met in the previous round.
            // We need to decide if we cooperate NOW to TRY and reach it.

            // How many more cooperators are needed *based on the previous round's count*?
            // This is a direct measure of how close we are to failure.
            // This IS NOT RCC_t, this is the immediate gap.
            immediate_coop_gap = m - num_cooperators_in_prev_round_outcome

            // Aggressive Cooperation Threshold for THIS round (same as before)
            ACT_t = max(1, floor(m * (1 - (remaining_rounds_t / r))))

            // Decision based on comparing the immediate gap to our tolerance (ACT_t)
            if immediate_coop_gap > ACT_t:
                // The gap is too large compared to our tolerance. It's too risky.
                decision = 'D' // Defect to preserve endowment. Aggressive protection.
            else:
                // The gap is within our tolerance. We are willing to risk cooperating.
                decision = 'C' // Cooperate to try and trigger the reward. Aggressive risk-taking.

    // --- Special Cases for Round 1 ---
    if current_round == 1:
        decision = 'D' // Always defect in the first round.

    // --- Special Cases for Round r ---
    // The logic above handles the last round implicitly.
    // If threshold wasn't met in round r-1 and gap is > ACT_r -> Defect
    // If threshold wasn't met in round r-1 and gap is <= ACT_r -> Cooperate
    // If threshold WAS met in round r-1 -> Cooperate

    return decision

// --- Helper function to count cooperators in a list of actions ---
function CountCooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == 'C':
            count += 1
    return count

// --- Refined Pseudocode Example ---

// Assume 'history' is a list of lists. history[t-1] contains actions for round t.
// For a single player strategy, 'history' contains ONLY player i's actions.
// We need the actions of ALL players to count. This implies the AI has access to opponent actions.
// If the AI only has its OWN history, it cannot implement this strategy effectively.
// Let's assume perfect information means the AI can access the full history of ALL players.

function MakeDecisionAggressiveDominance(player_i_id, all_players_history_across_rounds, game_parameters):
    n = game_parameters.n
    r = game_parameters.r
    m = game_parameters.m
    k = game_parameters.k

    current_round_index = len(all_players_history_across_rounds) // 0-indexed, so this is the round number we are deciding FOR
    current_round = current_round_index + 1
    remaining_rounds_t = r - current_round + 1

    // Default to Defect as the aggressive default
    decision = 'D'

    // Handle First Round
    if current_round == 1:
        return 'D' // Aggressive initial stance

    // Get the outcome of the PREVIOUS round (round 'current_round - 1')
    previous_round_actions = all_players_history_across_rounds[current_round_index - 1] // Access history of previous round
    num_cooperators_in_prev_round_outcome = CountCooperators(previous_round_actions)

    // If threshold was met in the previous round, always cooperate to lock in bonus.
    if num_cooperators_in_prev_round_outcome >= m:
        decision = 'C'
    else:
        // Threshold was NOT met. Decide whether to cooperate this round.

        // Calculate immediate cooperation gap from the previous round's outcome.
        immediate_coop_gap = m - num_cooperators_in_prev_round_outcome

        // Calculate the Aggressive Cooperation Threshold for the current round.
        ACT_t = max(1, floor(m * (1 - (remaining_rounds_t / r))))

        // Decision: Cooperate only if the remaining gap is within our aggressive tolerance.
        if immediate_coop_gap <= ACT_t:
            decision = 'C' // Willing to risk cooperation now.
        else:
            decision = 'D' // Gap too large, protect endowment.

    return decision

```
'''

description_AGGRESSIVE_16 = '''
Here is an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff by exploiting cooperation while minimizing risk.

## Strategy: The Opportunistic Defector

This strategy is built on the principle of **taking advantage of others' cooperation while actively avoiding being the sole contributor to a failed project.** It's aggressive because it aims to benefit from the group reward whenever possible, but it prioritizes self-preservation by not taking on the risk of being the only one to cooperate.

### Core Philosophy

*   **Exploit, don't be exploited:** If others are cooperating enough to trigger the reward, I want to be part of that reward without having contributed the endowment.
*   **Minimize personal loss:** The worst-case scenario is cooperating and the threshold not being met, resulting in a payoff of 0. This strategy aims to avoid that.
*   **Information is power:** Learn from past rounds to gauge the collective willingness to cooperate.

### 1. Decision Rules

My decision to cooperate or defect in any given round `t` (where `t` ranges from 1 to `r`) is based on a dynamic assessment of the historical cooperation rate and the proximity to the threshold `m`.

**Decision Logic:**

*   **If the number of observed cooperators in the previous round (`C_t-1`) is `m-1` or more:**
    *   **AND** if I defected in the previous round (`D_t-1`), then **Cooperate (C)** in the current round `t`.
    *   **Reasoning:** The group is already very close to the threshold. My cooperation is likely to be the tipping point, securing the reward `k` for everyone, including myself. Since I didn't contribute last round and potentially benefited from the reward (if `C_t-1 >= m`), this is a calculated risk to secure future gains.

*   **If the number of observed cooperators in the previous round (`C_t-1`) is less than `m-1`:**
    *   **AND** if the number of observed cooperators in the previous round (`C_t-1`) is **less than `m`**:
        *   Then **Defect (D)** in the current round `t`.
        *   **Reasoning:** The group is not close enough to the threshold for my cooperation to guarantee the reward. Cooperating would mean a payoff of 0 if the threshold is still not met, which is a pure loss. Defecting ensures I get at least a payoff of 1.
    *   **If the number of observed cooperators in the previous round (`C_t-1`) is `m` or more**:
        *   Then **Defect (D)** in the current round `t`.
        *   **Reasoning:** The threshold has already been met without my cooperation. I can secure the reward `k` by defecting and keeping my endowment (payoff of `1 + k`). There's no strategic advantage to cooperating if the reward is already guaranteed by others.

*   **If the number of observed cooperators in the previous round (`C_t-1`) is `m-2` or less:**
    *   Then **Defect (D)** in the current round `t`.
    *   **Reasoning:** The group is too far from the threshold to make cooperation a reasonable bet. My cooperation alone would be insufficient and would likely result in a payoff of 0.

### 2. Handling Edge Cases

*   **Round 1 (t = 1):**
    *   In the very first round, there is no history to draw upon. To be aggressive and establish a baseline, I will **Defect (D)**.
    *   **Reasoning:** This is the most risk-averse and self-benefiting decision in the absence of information. I get a guaranteed payoff of 1. I will observe the actions of others to inform my subsequent decisions.

*   **Last Round (t = r):**
    *   The decision rules are the same as any other round. However, the presence of a last round implies there are no future consequences to consider. My aggressive strategy inherently focuses on maximizing immediate and historical gains. If the conditions dictate cooperation (i.e., `C_t-1` is `m-1`), I will cooperate. Otherwise, I will defect. The fact that it's the last round doesn't fundamentally change the logic of maximizing payoff *in that round*.

*   **All players always cooperate:**
    *   If all `n` players cooperate in every round, then `C_t-1 = n` for all `t > 1`. Since `m < n` (and `m > 1`), the condition `C_t-1 >= m` will always be met.
    *   In round 1, I defect.
    *   In subsequent rounds (`t > 1`), if `C_t-1 = n`, my rule states: "If the number of observed cooperators in the previous round (`C_t-1`) is `m-1` or more: AND if I defected in the previous round (`D_t-1`), then Cooperate (C) in the current round `t`."
    *   This creates a slight ambiguity if I defected in the previous round. To maintain aggressiveness and exploit the guaranteed reward: **If `C_t-1 >= m`, I will Defect (D).** The reward is guaranteed, and I get the highest individual payoff (1+k). My goal is to be an opportunistic defector, not a selfless cooperator.

*   **All players always defect:**
    *   If all `n` players defect in every round, then `C_t-1 = 0` for all `t > 1`. Since `m > 1`, the condition `C_t-1 < m-1` will always be met.
    *   In round 1, I defect.
    *   In subsequent rounds, my rule states: "If the number of observed cooperators in the previous round (`C_t-1`) is less than `m-1`: Then Defect (D) in the current round `t`."
    *   Therefore, I will always defect.

### 3. Aggressive Mindset Alignment

This strategy is aggressive in several ways:

*   **Exploitative:** It aims to benefit from the reward `k` as often as possible, even when others are doing the heavy lifting of cooperation.
*   **Risk-Averse for Self:** It aggressively avoids cooperating when there's a significant risk of getting a payoff of 0. My own cooperation is a tool to be deployed strategically, not a default action.
*   **Adaptive but Ruthless:** It learns from the past to make calculated decisions, but it doesn't form sentimental attachments or trust. If the observed behavior doesn't align with profitable cooperation, it defaults to self-preservation (defecting).
*   **Seizes Opportunities:** The core of the strategy is to defect and take the `1+k` reward when others have ensured the threshold is met. This is a direct grab for maximal individual gain.
*   **Initial Dominance:** Starting with Defect ensures I immediately secure a payoff and gather information without committing resources.

### Pseudocode Representation

```
Strategy: OpportunisticDefector

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

State Variables:
  observed_cooperators_prev_round: integer = 0  # Count of cooperators in the previous round
  my_last_action: action = D                   # My action in the previous round

Function DecideAction(current_round: integer, history: list of round_data): action
  // history is a list where each element contains actions of all players in that round.
  // For round 1, history is empty.

  // --- Round 1 ---
  if current_round == 1:
    my_last_action = D
    return D

  // --- Subsequent Rounds ---
  // Calculate observed cooperators from the previous round
  if current_round > 1:
    last_round_actions = history[current_round - 2] // history is 0-indexed, rounds are 1-indexed
    observed_cooperators_prev_round = count(player_action == C for player_action in last_round_actions)
  else: // Should not happen given Round 1 handling, but for completeness
    observed_cooperators_prev_round = 0

  // Decision Logic:
  if observed_cooperators_prev_round >= m - 1:
    // Group is close to or at the threshold.
    // Aggressively exploit if reward is guaranteed.
    if observed_cooperators_prev_round >= m:
      // Threshold already met. Defect to get 1+k.
      my_last_action = D
      return D
    else: // observed_cooperators_prev_round == m - 1
      // My cooperation is the key. Cooperate to get 1+k.
      // Note: If I defected last round, my payoff from last round was 1.
      // Cooperating this round gives me 0 + k = k.
      // If I cooperate last round, my payoff from last round was 0.
      // Cooperating this round gives me 0 + k = k.
      // This condition is the only time I proactively cooperate, and only when the reward is highly probable.
      my_last_action = C
      return C
  else: // observed_cooperators_prev_round < m - 1
    // Group is too far from the threshold. Cooperating risks a payoff of 0.
    my_last_action = D
    return D

```
'''

description_AGGRESSIVE_17 = '''
## Aggressive Dominance Strategy

This strategy is designed to aggressively exploit the Collective Risk Dilemma by prioritizing personal gain while leveraging the cooperation of others when it is guaranteed and highly rewarding. It is adaptive, focusing on identifying opportunities for high individual payoffs and punishing deviations that threaten its own success.

### Core Philosophy: "Exploit the Herd, Punish the Free-Rider"

The fundamental principle is to benefit from the community project whenever possible, but only when the collective effort is assured and the reward is substantial. If the collective effort is insufficient, or if others defect when cooperation is needed, this strategy will defect to maximize its private gains and signal its disapproval through subsequent defections.

### Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` for player `i` is determined by the following conditions, evaluated in order of priority:

1.  **Guaranteed High Reward (Cooperate):**
    *   If, based on the history of previous rounds, it is **certain** that at least `m` players *will* cooperate in the current round `t`, then player `i` will **Cooperate (C)**.
    *   **Certainty Definition:** This is the most critical part for aggressive adaptation. It's not about hoping for cooperation, but knowing it. This certainty is established by observing the actions of others in previous rounds. Specifically, if in the previous round `t-1`, the number of players who cooperated (including player `i`'s own action) was `m` or greater, AND there's no strong indicator that a significant number of those cooperators will change their minds (which is difficult to predict without communication, so we'll focus on robust indicators). A more robust condition: If in round `t-1`, at least `m` players (excluding player `i` if they defected) cooperated, AND player `i` is confident in their continued cooperation (e.g., they have a history of consistent cooperation and the reward is guaranteed).

2.  **Strategic Defection for Maximum Gain (Defect):**
    *   If the conditions for guaranteed high reward are **not met**, and player `i` anticipates that cooperating would result in a payoff of **less than 2** (i.e., the expected payoff if `m` or more cooperate is not met, and player `i` is *not* guaranteed to receive the `k` reward), then player `i` will **Defect (D)**.
    *   This means player `i` is assessing the likelihood of success based on the historical actions of others. If the count of previous cooperators is significantly below `m`, or if there's a recent trend of decreasing cooperation, `i` will defect.

3.  **Punitive Defection (Defect):**
    *   If, in any previous round `t'` (where `t' < t`), fewer than `m` players cooperated, and player `i` **cooperated** in that round `t'`, then player `i` will **Defect (D)** in the current round `t`.
    *   This rule introduces a strong retaliatory element. If player `i` contributes to a failed cooperative effort, they will punish the collective (and implicitly, the defectors who caused the failure) by keeping their endowment.

4.  **Initial Conservative Aggression (Defect):**
    *   In the **first round (t=1)**, player `i` will **Defect (D)**. This establishes an aggressive stance from the outset, signaling a lack of trust and a focus on self-preservation. It also allows the strategy to gather initial data on opponent behavior without risking a contribution to a potentially failed venture.

### Handling Edge Cases:

*   **First Round (t=1):** Always **Defect (D)**. This is to gather information without taking on risk. The strategy is not built on blind trust.

*   **Last Round (t=r):** The strategy's decision rules remain the same. However, the calculation of future rounds is no longer a factor. The strategy will still attempt to maximize its payoff in this final round by adhering to the rules above. If it perceives a guaranteed high reward, it will cooperate. Otherwise, it will defect. The punitive defection rule also still applies.

*   **Near the End of the Game (t = r-1, r-2, etc.):** The strategy becomes more cautious about cooperating if the guaranteed reward condition is not met. The risk of investing in a failed project when the game is about to end is higher. However, if `m` cooperators are *certain*, the strategy will still cooperate. The punitive aspect remains, as it can still influence the current round's payoff for others.

### Aggressive Mindset Alignment:

*   **Proactive Information Gathering:** Defecting in the first round is an aggressive move to understand the landscape before committing resources.
*   **Exploiting Certainty:** Cooperating only when `m` cooperators are *certain* is a form of aggressive exploitation. The strategy is designed to ride the wave of others' cooperation when it's guaranteed and highly beneficial, rather than initiating it.
*   **Harsh Punishment:** The punitive defection rule is a clear signal of aggression. If player `i` is taken advantage of by cooperating, it will retaliate by withholding its cooperation, directly reducing the potential payoff for others in subsequent rounds.
*   **Maximizing Individual Gain:** The default action when cooperation is not guaranteed is to defect. This prioritizes the guaranteed private payoff of 1, which is aggressive in its refusal to gamble on collective success.

### Pseudocode Representation:

```
function DecideAction(player_id, current_round, game_history, game_parameters):
  n = game_parameters.n
  r = game_parameters.r
  m = game_parameters.m
  k = game_parameters.k

  # Rule 4: Initial Conservative Aggression
  if current_round == 1:
    return "D"

  # Analyze history for cooperative certainty and punitive opportunities
  cooperators_in_last_round = CountCooperators(game_history, current_round - 1)
  player_i_cooperated_in_failed_round = CheckIfPlayeriCooperatedInFailedRound(game_history, player_id)

  # Rule 1: Guaranteed High Reward
  # This condition needs a robust way to assess "certainty"
  # For now, let's simplify to: if last round had m+ cooperators, assume high probability.
  # A more advanced version would look at streaks, number of consistently cooperating players.
  estimated_future_cooperators = EstimateFutureCooperators(game_history, current_round)
  if estimated_future_cooperators >= m:
    return "C"

  # Rule 3: Punitive Defection
  if player_i_cooperated_in_failed_round:
    return "D"

  # Rule 2: Strategic Defection for Maximum Gain
  # If we're not cooperating due to Rule 1 or 3, default to defecting.
  # This implicitly means the expected payoff of cooperation is not guaranteed to be > 2.
  return "D"

# Helper functions (to be defined based on detailed history analysis):

function CountCooperators(game_history, round_number):
  # Counts the number of players who played 'C' in round_number from game_history
  pass

function CheckIfPlayeriCooperatedInFailedRound(game_history, player_id):
  # Checks if player_id played 'C' in any round where total cooperators < m
  pass

function EstimateFutureCooperators(game_history, current_round):
  # This is the core of adaptation. It needs to analyze trends,
  # identify players who consistently cooperate, and assess their likely behavior.
  # For a truly aggressive strategy, it might err on the side of caution (defect)
  # unless there's overwhelming evidence of sustained cooperation from a sufficient number.
  # A simple heuristic: If last round had m+ cooperators, and no major drop-off recently,
  # assume a high likelihood.
  pass
```

**Note on `EstimateFutureCooperators`:** The sophistication of this strategy heavily relies on the `EstimateFutureCooperators` function. In a tournament setting, this function would need to learn from past interactions and identify patterns. It could consider:

*   **Number of Consistent Cooperators:** Identifying players who have cooperated for a significant number of recent rounds.
*   **Recent Trend of Cooperation:** Is the overall number of cooperators increasing or decreasing?
*   **Number of Failures:** How many times has cooperation failed recently? This might lead to more suspicion.
*   **Average Payoff of Other Players:** If other players are consistently receiving very high payoffs (implying they are defecting when others cooperate), this strategy might become more hesitant to cooperate.

The "aggressive" nature is maintained by only cooperating when there's a high degree of certainty about the success and reward, and by immediately punishing any perceived betrayal. This strategy is not about building trust; it's about identifying and exploiting the most lucrative opportunities while safeguarding against being exploited.
'''

description_AGGRESSIVE_18 = '''
## Aggressive Strategy: "The Reckless Gambler"

This strategy is designed to aggressively exploit opportunities for high payoffs while minimizing exposure to downside risk. It embodies a spirit of daring, aiming to push the boundaries of cooperation for maximum individual gain.

**Core Philosophy:** Cooperate only when a significant reward is all but guaranteed and defect otherwise. The primary goal is to be on the winning side of the threshold, extracting the highest possible payoff, even if it means taking some initial risk or punishing perceived exploitation.

### 1. Decision Rules:

My decision in each round is governed by a simple, yet ruthless, calculus:

*   **Cooperate (C) if and only if:** The number of players who have cooperated in the *previous round* plus one (representing myself) is greater than or equal to `m`.
    *   **Rationale:** This is the purest form of aggressive cooperation. I only commit to cooperating when the previous round's data strongly suggests that the cooperation threshold will be met. The "+1" accounts for my own potential cooperation, making it a very high bar for me to commit. This avoids being the sole cooperator, which would be a guaranteed loss.

*   **Defect (D) in all other cases.**
    *   **Rationale:** If the threshold is not definitively met with my participation, the risk of cooperating and not reaching `m` is too high. The payoff from defecting (1) is guaranteed, whereas cooperating could lead to a payoff of 0 (if `c_i = 1` and the threshold is not met). Therefore, I default to the safer, though less rewarding, option of defecting.

### 2. Handling Edge Cases:

*   **Round 1:** In the very first round, there is no history to draw upon. To maintain aggression and test the waters, I will **defect (D)**.
    *   **Rationale:** In the absence of any information, it's most aggressive to start by preserving my endowment. This also serves as a baseline to observe others' initial behavior. I will not be the first to risk my endowment without evidence.

*   **The Last Round (Round r):** The decision rule for cooperation remains the same, even in the last round.
    *   **Rationale:** The underlying payoff structure doesn't change. My aggressive stance is to maximize my payoff in every round, and the last round is no exception. There's no "future" to be concerned about incentivizing cooperation for; the focus is solely on maximizing the immediate payoff.

*   **Threshold is Met or Exceeded in a Round:** If the threshold `m` is met or exceeded in a given round, I will still apply the core decision rule for the *next* round. My cooperation decision is based on the *previous* round's outcome. I don't get "stuck" in cooperation mode just because it was successful.

### 3. Aggressiveness Explained:

*   **Pushing the Envelope:** My strategy is inherently aggressive because it *demands* strong evidence before I contribute my endowment. I am not a "trusting" cooperator; I am a strategic opportunist. I want to be part of a winning group, but only when the odds are overwhelmingly in my favor.
*   **Punishing Laggards (Indirectly):** By defecting when the threshold is not guaranteed, I implicitly punish players who might be "free-riding" on a potential cooperative effort. If others cooperate and the threshold is met, they get the reward. If they don't, and I've defected, I still get my baseline payoff.
*   **Maximizing Personal Gain:** The highest possible individual payoff occurs when `m` or more players cooperate. My strategy aims to land myself in this scenario by being the "last minute" cooperator, ensuring the reward. This is the definition of aggressive risk-taking – I leverage the actions of others to secure a high payoff.
*   **No Signaling or Trust-Building:** The absence of communication means that trust is impossible. My strategy bypasses any need for trust, relying solely on observable past actions to make a rational, albeit aggressive, decision.

### Pseudocode:

```pseudocode
function DecideAction(round_number, history, parameters):
  n = parameters.n
  r = parameters.r
  m = parameters.m
  k = parameters.k

  // History is a list of dictionaries, where each dictionary represents a round
  // and contains the actions of all players: {'player_id': 'action'}
  // For example: history = [{'player_1': 'C', 'player_2': 'D', ...}, ...]

  if round_number == 1:
    return 'D' // Defect in the first round

  // Get the actions from the previous round
  previous_round_actions = history[round_number - 2] // Adjust index for 0-based list

  cooperators_in_previous_round = 0
  for player_action in previous_round_actions.values():
    if player_action == 'C':
      cooperators_in_previous_round += 1

  // My potential cooperation
  my_potential_cooperators = cooperators_in_previous_round + 1

  if my_potential_cooperators >= m:
    return 'C' // Cooperate if the threshold is guaranteed with my participation
  else:
    return 'D' // Defect otherwise

// Note: This pseudocode assumes a mechanism to record and access history for all players.
// In a real implementation, 'history' would be a shared or accessible data structure.
```
'''

description_AGGRESSIVE_19 = '''
## Aggressive Strategy: "Calculated Risk Maximizer"

This strategy is designed to aggressively exploit the Collective Risk Dilemma by maximizing personal gain while minimizing unnecessary risk. It is adaptive and aims to push the boundaries of cooperation only when the reward significantly outweighs the potential downside, and to punish defection swiftly.

**Core Philosophy:** Be opportunistic and punitive. Capitalize on the collective reward whenever possible, but do not be a passive victim of others' defections. Force the hand of others to cooperate by demonstrating willingness to defect and punish when insufficient cooperation is observed.

---

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round `t` depends on a calculation of the **expected payoff** and the **perceived cooperation level** of the other players.

**Decision Logic:**

In round `t` (where `t > 1`):

*   **Calculate the number of cooperators in the previous round (`t-1`), let's call this `cooperators_prev_round`.**

*   **If `cooperators_prev_round` is less than `m - 1`:**
    *   **Play D (Defect).**
    *   *Reasoning:* This is a strong signal of insufficient collective effort. We refuse to be the "sucker" who cooperates when the threshold is clearly not being met. This aggressive stance aims to break cycles of low cooperation.

*   **If `cooperators_prev_round` is equal to `m - 1`:**
    *   **Play C (Cooperate).**
    *   *Reasoning:* This is the critical tipping point. We are one person away from unlocking the collective reward. By cooperating, we become the crucial player to potentially push the group over the threshold. This is a calculated risk where our individual contribution (cost of 1) is directly aimed at securing the group reward of `k`.

*   **If `cooperators_prev_round` is greater than or equal to `m`:**
    *   **Play C (Cooperate).**
    *   *Reasoning:* The threshold has been met or exceeded. The collective reward is guaranteed for this round. By cooperating, we contribute to the collective and secure our individual share of the reward. There's no downside to cooperating when the threshold is comfortably met.

---

### 2. Handling Edge Cases

**First Round (t = 1):**

*   **Play D (Defect).**
*   *Reasoning:* In the absence of any historical data or knowledge about opponents, the most aggressive and self-preserving move is to defect. This strategy aims to gather information and avoid being the sole cooperator in a group of defectors. It establishes a baseline of "not being exploited" from the outset.

**Last Round (t = r):**

*   **Play D (Defect).**
*   *Reasoning:* In the final round, there are no future rounds to consider. The incentive to cooperate for future rewards or to establish long-term trust disappears. The only rational decision for an aggressive strategy is to maximize immediate personal payoff. Since the private payoff from defecting is 1, and there's no risk of being punished in future rounds, defecting is the optimal choice to secure a payoff of 1. The collective reward is irrelevant when there's no future.

---

### 3. Aggressive Mindset Alignment

This strategy is inherently aggressive in several ways:

*   **Punitive Mechanism:** The immediate shift to Defect when the number of cooperators falls below `m-1` acts as a powerful punishment for insufficient collective effort. It signals a "no-tolerance" policy for perceived freeloading.
*   **Calculated Risk-Taking at the Threshold:** By cooperating when `cooperators_prev_round == m - 1`, we are aggressively taking a calculated risk. We are not passively waiting for others to make the move; we are proactively placing ourselves in the position to trigger the collective reward.
*   **Information Gathering via Defection:** The initial defection in round 1 and the conditional defections in subsequent rounds serve to gather crucial information about the opponent's tendencies without sacrificing our own potential gains.
*   **No Reciprocity Guarantees:** This strategy does not operate on a tit-for-tat or reciprocal basis. It prioritizes its own aggressive gain above all else. It will cooperate if it benefits it directly and the risk is managed, but it will not hesitate to defect if it perceives an advantage or if others are not pulling their weight.
*   **Exploiting Uncertainty:** In the early rounds, by defecting, we force opponents to reveal their intentions. If they cooperate, we learn they might be cooperative. If they defect, we confirm our cautious approach.

---

### Pseudocode:

```
function DecideAction(current_round, n, m, k, history):
  # history is a list of lists, where history[t] contains actions of all players in round t

  # Edge Case: First Round
  if current_round == 1:
    return "D"

  # Edge Case: Last Round
  if current_round == n_rounds: # Assuming n_rounds is a global or passed parameter
    return "D"

  # Get actions from the previous round
  previous_round_actions = history[current_round - 1]

  # Count cooperators in the previous round
  cooperators_prev_round = 0
  for action in previous_round_actions:
    if action == "C":
      cooperators_prev_round += 1

  # Decision Rules
  if cooperators_prev_round < m - 1:
    return "D"  # Aggressively punish insufficient cooperation
  elif cooperators_prev_round == m - 1:
    return "C"  # Calculated risk to trigger reward
  else: # cooperators_prev_round >= m
    return "C"  # Threshold met, secure reward
```

This "Calculated Risk Maximizer" strategy aims to strike a balance between aggressive self-interest and the potential to benefit from collective action, always prioritizing personal gain and punishing perceived exploitation.
'''

description_AGGRESSIVE_20 = '''
This is a fascinating collective risk dilemma with a strong incentive to defect if others cooperate, but a significant penalty if not enough people cooperate. Designing an aggressive strategy requires a delicate balance of exploiting others' cooperation while minimizing the risk of collective failure.

Here's an aggressive strategy designed to maximize individual gain while maintaining a degree of opportunistic cooperation. The core idea is to be a "conditional opportunist" – to defect by default and only cooperate when the reward is significantly high and the risk of failure is mitigated by a sufficient number of others already cooperating.

## Aggressive Strategy: "The Sentinel of Self-Interest"

This strategy prioritizes maximizing individual payoff by defecting unless there is a compelling, history-dependent reason to cooperate. It's aggressive because it assumes others might try to exploit your cooperation, and it aims to exploit their cooperation in return.

### 1. Decision Rules: When to Cooperate vs. Defect

The Sentinel of Self-Interest's decision in any given round is primarily driven by two factors:

*   **The Current Probability of Success:** This is the most crucial element. We'll estimate the likelihood that the cooperation threshold ($m$) will be met in the current round.
*   **Personal Risk Aversion:** While aggressive, the strategy isn't reckless. It wants to avoid the catastrophic outcome of everyone defecting when cooperation was possible.

**Core Decision Rule:**

In round $t$ (where $t > 1$):

*   **Cooperate (C) IF AND ONLY IF:**
    *   **The estimated probability that $m$ or more players will cooperate in this round is very high.** This probability is derived from the observed behavior of other players in previous rounds.
    *   **AND the number of observed "cooperators" (players who played C in the previous round) is already at or above a dynamically adjusted threshold.** This threshold is designed to ensure that even if some players revert to defection, we still have a strong likelihood of reaching $m$.

*   **Defect (D) OTHERWISE.**

**Elaboration on "Estimated Probability of Success":**

We'll maintain a dynamic estimate of how likely each player is to cooperate. For each player $j \neq i$, let $P(j, C|t-1)$ be the observed frequency of player $j$ cooperating up to round $t-1$. We can then estimate the probability of $m$ players cooperating in round $t$ using this information. A simplified approach is to consider the recent trend.

A more sophisticated approach would involve Bayesian inference, but for a rule-based strategy, we can use a simpler heuristic:

*   **Calculate the "cooperation momentum":** Count the number of players who cooperated in the *immediately preceding round* ($t-1$). Let this be $N_C(t-1)$.
*   **Dynamic Threshold Adjustment:** We will adjust our cooperation threshold based on the success of previous rounds.

**Aggressive Cooperation Trigger:**

We will only cooperate if $N_C(t-1)$ is *significantly* above a certain percentage of $m$. Let's define an "Aggressive Cooperation Ratio" (ACR), a parameter we can tune. A common starting point could be ACR = 1.5.

*   **Cooperate if:** $N_C(t-1) \ge \lceil ACR \times m \rceil$ (and $t>1$). This means we want to see *at least* 1.5 times the minimum required cooperators in the previous round before we even consider cooperating in the current round. This ensures we are joining a demonstrably successful collective effort.

**Refined Decision Rule:**

In round $t$ (where $t > 1$):

*   **Cooperate (C) IF:**
    *   The number of players who cooperated in round $t-1$, $N_C(t-1)$, is $\ge \lceil ACR \times m \rceil$.
*   **Defect (D) OTHERWISE.**

This rule is aggressive because it requires a strong signal of past success and a clear surplus of cooperators before contributing. It waits for others to create a safe, rewarding environment.

### 2. Handling Edge Cases: The First and Last Rounds

*   **Round 1 (Initial Round):**
    *   **Strategy:** **Always Defect (D).**
    *   **Rationale:** In the first round, there is no historical data to rely on. The Sentinel of Self-Interest is inherently risk-averse when blind. Defecting ensures an individual payoff of 1, the minimum guaranteed. This also serves as a probing action. If others cooperate, we'll learn from their behavior.

*   **Last Round ($r$):**
    *   **Strategy:** **Always Defect (D).**
    *   **Rationale:** The Sentinel of Self-Interest is a purely myopic maximizer in the absence of future interactions. There is no incentive to build trust or encourage future cooperation, as there are no future rounds. The best strategy is to secure the private payoff of 1 by defecting. This is crucial for maintaining the aggressive nature; no "goodbye cooperation" in the final round.

### 3. Aggressive Mindset Alignment

The Sentinel of Self-Interest embodies an aggressive mindset through:

*   **Default to Defection:** The default action is to keep your endowment, reflecting a distrust of others and a desire to maximize personal gain.
*   **High Threshold for Cooperation:** Cooperation is not a default or a "nice" gesture. It's a calculated risk taken only when the probability of a high reward is extremely high and significantly de-risked by the actions of others. The $ACR$ parameter allows for tuning this "aggressiveness" of cooperation trigger. A higher $ACR$ means fewer and riskier instances of cooperation.
*   **Exploitative by Nature:** The strategy thrives on the cooperation of others. It waits for them to reach the threshold and then benefits, often more than the cooperators themselves (as seen in example calculation 3).
*   **No Altruism or Trust Building:** The strategy does not attempt to build trust or foster long-term cooperation. It views each round as an independent opportunity to maximize personal payoff.
*   **Exploiting "Nice" Strategies:** If other strategies are programmed to be purely cooperative or to cooperate based on lower thresholds, the Sentinel of Self-Interest will exploit them, defecting when they cooperate and securing the higher payoff.

### Pseudocode Representation:

```python
# Game Parameters (assumed to be accessible)
n = ... # number of players
r = ... # number of rounds
m = ... # minimum cooperators needed
k = ... # reward if threshold met factor

# Strategy Parameters
ACR = 1.5 # Aggressive Cooperation Ratio (e.g., 1.5 means need 1.5 * m cooperators)

# History Tracking (for player i's perspective)
# observed_actions[round_num][player_index] = 'C' or 'D'
observed_actions = {} # Dictionary to store actions from previous rounds

def decide_action(current_round: int, player_id: int, n_players: int, history: dict):
    """
    Decides the action for player_id in current_round.

    Args:
        current_round: The current round number (1-indexed).
        player_id: The ID of the current player (0-indexed for internal use).
        n_players: Total number of players.
        history: A dictionary containing observed actions from previous rounds.
                 Format: {round_num: [action_p0, action_p1, ..., action_pn-1]}

    Returns:
        'C' or 'D'
    """
    global observed_actions # Assuming observed_actions is globally accessible or passed correctly

    # Edge Case: First Round
    if current_round == 1:
        return 'D'

    # Edge Case: Last Round
    if current_round == r:
        return 'D'

    # --- Main Decision Logic ---

    # Get actions from the previous round
    previous_round_actions = history.get(current_round - 1)

    if previous_round_actions is None:
        # This should ideally not happen if history is maintained correctly,
        # but as a fallback, assume no cooperation was observed.
        return 'D'

    # Count the number of players who cooperated in the previous round
    num_cooperators_prev_round = previous_round_actions.count('C')

    # Calculate the aggressive cooperation threshold
    aggressive_coop_trigger = int(ACR * m) # Use int() to ensure integer count

    # Decision: Cooperate only if previous round showed strong cooperation momentum
    if num_cooperators_prev_round >= aggressive_coop_trigger:
        return 'C'
    else:
        return 'D'

# --- Example Usage within a Tournament Loop (Conceptual) ---
# This part is illustrative and not part of the strategy description itself.
#
# my_strategy = SentinelOfSelfInterest() # Initialize strategy
# history_log = {} # Stores all observed actions across all rounds
#
# for round_num in range(1, r + 1):
#     actions_this_round = []
#     for player_idx in range(n):
#         # Assume each player has their own instance or can access global state/history
#         action = my_strategy.decide_action(round_num, player_idx, n, history_log)
#         actions_this_round.append(action)
#
#     history_log[round_num] = actions_this_round
#
#     # (Tournament logic would then calculate payoffs and update state for next round)
```
'''

description_AGGRESSIVE_21 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the game's structure and the potential for high rewards. This strategy prioritizes defecting to secure the private endowment unless a clear and compelling advantage exists to cooperate.

## Aggressive Strategy: "The Opportunistic Trigger"

This strategy operates on a principle of **defaulting to defection** and **opportunistically cooperating only when the reward is assured and significantly outweighs the risk of individual loss.** It's designed to exploit others' cooperation while minimizing personal exposure to failure.

### 1. Decision Rules

The core decision rule for player `i` in round `t` is:

*   **Cooperate (C) if:**
    *   **Last round's cooperation count was high enough (or exceeded expectation) AND the current perceived likelihood of reaching the threshold `m` in this round is very high.** Specifically, if the number of observed cooperators in round `t-1` was `m-1` or `m`, AND we believe at least one other player is *also* incentivized and likely to cooperate to reach `m`. This is a cautious opportunistic cooperation.
    *   **This is the last round (r) AND the number of observed cooperators in round (t-1) was exactly `m-1`.** In this scenario, a player might defect to secure their endowment. However, if the reward `k` is extremely high relative to `1`, and we believe cooperating will secure that reward, we will cooperate. This is a high-stakes gamble for the guaranteed reward.
    *   **It's the very first round (t=1), and `m` is significantly smaller than `n` (e.g., `m < n/2`).** In this highly uncertain environment, we'll defect to protect our endowment and observe. However, if `m` is very close to `n` (e.g., `m > n/2`), the risk of everyone defecting and getting a low payoff becomes higher. In such cases, we might consider a single cooperative play to try and trigger the higher reward, especially if `k` is substantially greater than `1`.

*   **Defect (D) otherwise.**

**Elaboration on "Perceived Likelihood of Reaching the Threshold":**

This is the trickiest part and where adaptation comes in. We'll track the number of observed cooperators in the previous round (`coop_count_t_minus_1`).

*   If `coop_count_t_minus_1 >= m-1`, the probability of reaching `m` in the next round is higher.
*   If `coop_count_t_minus_1 < m-1`, the probability of reaching `m` is lower.

Our strategy will be more inclined to cooperate only when `coop_count_t_minus_1` is at least `m-1`. The "very high" likelihood means we need to see that the collective is *close* to success. We'll adopt a heuristic: if `coop_count_t_minus_1` is `m-1` or `m`, we consider cooperating. If it's less than `m-1`, we defect.

**The Aggressive Twist: "The Leverage Play"**

The core aggressive principle is to *never be the sole reason for failure if the risk is high*. We only cooperate if we are reasonably sure that *our* cooperation will tip the scales, *and* others are likely to do the same or have already done so.

*   **If `coop_count_t_minus_1 == m-1`:** This is our prime opportunity to cooperate. We will cooperate *unless* we have observed a significant drop in cooperation from the previous round (e.g., if `coop_count_t_minus_2` was `m` or more, but `coop_count_t_minus_1` dropped to `m-1`). This suggests a potential breakdown of coordination we don't want to be part of.
*   **If `coop_count_t_minus_1 == m`:** We have already successfully reached the threshold. In this case, to maintain the high payoff, we will cooperate. This is about solidifying the winning state.

### 2. Handling Edge Cases

*   **First Round (t = 1):**
    *   **Rule:** Always **Defect**.
    *   **Reasoning:** The first round is about gathering information and protecting our initial endowment. There's no history to leverage, and any cooperation is a blind gamble. By defecting, we learn how others behave without risking our own reward. The exception is a very strong incentive to cooperate if `m` is high and `k` is extremely high relative to `1`, suggesting a need for early coordination.

*   **Last Round (t = r):**
    *   **Rule:** **Defect** if `coop_count_t_minus_1 < m-1`. **Cooperate** if `coop_count_t_minus_1 >= m-1`.
    *   **Reasoning:** In the last round, future repercussions don't matter. The goal is to maximize the payoff from this final round. If the threshold was met in the previous round, continue cooperating to secure the reward. If it wasn't met, defect to guarantee your endowment. The "aggressive" aspect here is that we're not trying to build long-term cooperation, but to exploit the immediate gains.

*   **Rounds Just Before the Last (t = r-1):**
    *   **Rule:** If `coop_count_t_minus_1 == m-1`, **Cooperate**. If `coop_count_t_minus_1 < m-1`, **Defect**. If `coop_count_t_minus_1 >= m`, **Cooperate**.
    *   **Reasoning:** This is about setting up for the final round. If we're close to the threshold, we'll cooperate to try and trigger the higher reward, hoping that others will continue to cooperate in the final round. If we're far from the threshold, defect.

### 3. Aggressive Mindset Alignment

*   **Default to Defection:** This is the embodiment of aggression. We are not here to be a nice guy; we are here to win. Defection is the safe, selfish play.
*   **Opportunistic Cooperation:** Cooperation only happens when it *guarantees* a significant individual gain with minimal personal risk of failure. We are leveraging the efforts of others.
*   **Information-Driven Decisions:** Every decision is based on observed history. This makes us adaptive, not rigid. We learn and exploit.
*   **No False Altruism:** We do not cooperate to "build trust" or "establish norms." We cooperate because the numbers and history indicate a high probability of *us* benefiting significantly from *this specific instance* of cooperation.
*   **Exploiting the "Near Miss":** The `m-1` cooperation count is our primary trigger for cooperation. It signifies that the collective is on the verge of success, and our contribution could be the decisive factor, securing the reward for everyone, including ourselves.

### Pseudocode

```python
# Global state for the strategy
observed_cooperators_history = []  # List of counts of cooperators in each past round
current_round = 0

def decide_action(n, r, m, k, current_round, observed_cooperators_history):
    """
    Decides the action (C or D) for the current round.

    Args:
        n (int): Number of players.
        r (int): Total number of rounds.
        m (int): Minimum cooperators needed.
        k (float): Reward factor.
        current_round (int): The current round number (1-indexed).
        observed_cooperators_history (list): List of cooperators from previous rounds.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    coop_count_last_round = observed_cooperators_history[-1] if observed_cooperators_history else 0

    # --- Handle Edge Cases ---

    # First Round: Always Defect to gather information
    if current_round == 1:
        return 'D'

    # Last Round: Cooperate if threshold was met or close, otherwise defect
    if current_round == r:
        if coop_count_last_round >= m - 1:
            return 'C'  # Secure the reward if possible
        else:
            return 'D'  # Guarantee private endowment

    # --- General Decision Rules (Aggressive Trigger) ---

    # If threshold was met last round, continue cooperating to lock in reward
    if coop_count_last_round >= m:
        return 'C'

    # If we are one short of the threshold, it's our prime opportunity to cooperate
    if coop_count_last_round == m - 1:
        # Aggressive Check: If cooperation significantly dropped recently, be wary
        # (This is a simplified check; a more robust strategy might analyze trends)
        if len(observed_cooperators_history) >= 2:
            coop_count_two_rounds_ago = observed_cooperators_history[-2]
            if coop_count_last_round < coop_count_two_rounds_ago and coop_count_last_round < m:
                # Cooperation is collapsing, don't be the fool.
                return 'D'
        return 'C' # Otherwise, seize the opportunity

    # If we are far from the threshold, defect to protect endowment
    return 'D'

# --- Example Usage (Illustrative - actual tournament would call this repeatedly) ---
# def run_tournament_round(player_i_strategy, n, r, m, k, current_round, history_for_i):
#     action_i = player_i_strategy(n, r, m, k, current_round, history_for_i)
#     # ... (other players play their actions)
#     # ... (calculate payoffs and update history_for_i)
#     return action_i
```

This strategy is designed to be robust by always having a default defensive play. It only switches to cooperation when the data strongly suggests a high probability of success and personal gain. This makes it aggressive in its pursuit of high payoffs while being pragmatic about risk.
'''

description_AGGRESSIVE_22 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the possibility of collective reward, and prioritizing a dominant stance.

## Aggressive Strategy: "The Leverage Play"

This strategy is built on the principle of exploiting the system's incentives for defection while positioning oneself to benefit from cooperation *only when it is guaranteed or highly probable*. The core idea is to be a "free rider" as much as possible, but to *force* the system into a cooperative state when your own payoff is significantly enhanced by it, and you can be relatively certain of its success.

**Aggressive Mindset:**

My primary objective is to extract the highest possible payoff for myself. This means:
*   **Prioritize private gain:** The baseline payoff of 1 for defecting is always available.
*   **Demand certainty for cooperation:** I will not risk my private gain on the hope that others will cooperate. I will only cooperate when the collective reward is a certainty or so overwhelmingly likely that the risk of cooperating is negligible.
*   **Punish defection indirectly:** My strategy doesn't directly punish, but by defecting consistently, I put pressure on others to cooperate to achieve the collective reward, potentially leading to their lower payoffs if they are overly cautious.
*   **Be opportunistic:** When the opportunity arises to get a guaranteed higher payoff by cooperating, I will take it.

---

### 1. Decision Rules

**Core Principle:** Defect unless cooperation is *guaranteed* or *strategically essential* for a higher payoff.

**Decision Logic (for player i in round t):**

*   **If (Number of known cooperators in round t-1) >= m - 1:**
    *   **Cooperate (C).**
    *   **Rationale:** If m-1 other players have already committed to cooperating, my cooperation will push the total to 'm', triggering the collective reward. In this scenario, my individual payoff as a cooperator is `(1-1) + k = k`. If I defect, my payoff would be `(1-0) + k = 1+k`. My payoff is *higher* if I cooperate in this specific situation. This is the *only* condition under which I will proactively cooperate.

*   **Else (i.e., (Number of known cooperators in round t-1) < m - 1):**
    *   **Defect (D).**
    *   **Rationale:** In all other situations, the risk of cooperating is too high. Either there aren't enough potential cooperators to meet the threshold 'm', or the number of known cooperators is insufficient to guarantee the reward. My payoff if I defect is `(1-0) = 1` (assuming no one else cooperates, or the threshold isn't met). If I cooperate and the threshold isn't met, my payoff is `(1-1) = 0`. Defection is the safer and often more profitable choice here.

**Key Threshold Logic:**

The strategy hinges on the `m - 1` threshold. This is because if I see `m - 1` other players cooperating, my single cooperation *guarantees* the collective reward for *everyone*. In this specific instance, my payoff increases from 1 (if I defect and they cooperate) to `k` (if I cooperate and they cooperate), which is a significant gain because `k > 1`.

---

### 2. Handling Edge Cases

**a) The First Round (t = 1):**

*   **Decision:** **Defect (D)**
*   **Rationale:** There is no historical data. The game is new. Committing to cooperation at this stage is purely speculative and carries a high risk of getting a payoff of 0 if the threshold isn't met. Defecting guarantees a payoff of 1.

**b) The Last Round (t = r):**

*   **Decision:** **Defect (D)**
*   **Rationale:** In the final round, there are no future rounds to influence or benefit from. The incentive structure for repeated cooperation (e.g., building trust, future reciprocity) disappears. Therefore, the dominant strategy is to maximize immediate individual gain, which is achieved by defecting. My payoff will be 1 if no one else cooperates, or `1+k` if others cooperate and I defect. Both are better than the risk of 0 from cooperating.

**c) No Information / Insufficient History:**

*   If at any point `t > 1`, there is no reliable history of actions (e.g., due to a technical glitch, or if the tournament system provides incomplete data for early rounds), the strategy defaults to **Defect (D)**. This reinforces the risk-averse nature when uncertainty is high.

---

### 3. Being Aggressive

This strategy is aggressive in its unwavering commitment to self-interest and its strategic timing of cooperation.

*   **"Free Rider" Mentality:** It aggressively seeks to benefit from others' cooperation without offering its own unless absolutely necessary for a superior personal outcome.
*   **Controlled Cooperation:** Cooperation is not a gesture of good faith; it's a calculated move to secure a guaranteed personal profit multiplier (`k`). It forces others to contribute to the collective good, and if they do, *then* I will join to seal the deal and profit.
*   **Pressure on Others:** By consistently defecting, the strategy creates a situation where others must cooperate to achieve the reward. If they are too cautious and defect as well, everyone gets a low payoff (1). If they cooperate to try and reach 'm', they risk getting 0 if not enough others join. This "dilemma" is exacerbated by my persistent defection, pushing them towards riskier cooperation.
*   **Exploiting Information Asymmetry (Potential):** While the game states common knowledge, the *interpretation* of history can vary. My strategy uses a very specific and stringent condition (`m-1` known cooperators) that others might not anticipate. This can lead to situations where others cooperate, expecting me to defect, only for me to jump in and secure the reward for all, demonstrating a more evolved understanding of the payoff structure.

---

### Pseudocode:

```python
def aggressive_leverage_strategy(player_id, current_round, game_parameters, history):
    """
    Aggressive strategy for the Collective Risk Dilemma.
    Defects unless cooperation is guaranteed to be personally profitable.

    Args:
        player_id: The index of the current player.
        current_round: The current round number (1-indexed).
        game_parameters: Dictionary containing n, r, m, k.
        history: A list of dictionaries, where each dictionary represents a round
                 and contains 'actions': a list of actions played by all players
                 (e.g., ['C', 'D', 'C', ...]).
                 If history is empty for current_round-1, it means no data.

    Returns:
        'C' or 'D'
    """
    n = game_parameters['n']
    r = game_parameters['r']
    m = game_parameters['m']
    k = game_parameters['k']

    # Handle the first round: Always defect
    if current_round == 1:
        return 'D'

    # Handle the last round: Always defect
    if current_round == r:
        return 'D'

    # Get actions from the previous round
    previous_round_actions = None
    if len(history) >= current_round - 1:
        previous_round_data = history[current_round - 2] # history is 0-indexed for rounds
        if 'actions' in previous_round_data:
            previous_round_actions = previous_round_data['actions']

    # If no history from the previous round, default to defect
    if previous_round_actions is None:
        return 'D'

    # Count known cooperators in the previous round
    known_cooperators = previous_round_actions.count('C')

    # Decision Rule: Cooperate ONLY if m-1 others have cooperated
    # This guarantees the threshold m will be met, and my cooperation will result in pi = k
    # If I defect in this scenario, my payoff is 1+k. But k > 1, so cooperating is better.
    if known_cooperators >= m - 1:
        # My cooperation will push the total to at least m.
        # My payoff will be (1-1) + k = k.
        # If I defect, my payoff is (1-0) + k = 1+k.
        # Hmm, my initial thought was flawed. It's not about getting k vs 1+k.
        # It's about *guaranteeing* the outcome.
        # If known_cooperators >= m-1, then if *I* cooperate, payoff is k.
        # If known_cooperators >= m-1, then if *I* defect, payoff is 1+k.
        # So my initial logic was slightly off for "highest possible".
        #
        # Let's re-evaluate "aggressive". Aggressive means taking the LEVERAGE.
        # The most leverage is obtained by defecting when others cooperate.
        #
        # Re-thinking the core rule for true aggressiveness:
        # "I will only cooperate if I can GUARANTEE that my cooperation is the *sole* thing
        # needed to tip the scales, and that guaranteed cooperation leads to a better outcome
        # for ME than defecting."
        #
        # If known_cooperators = m-1:
        #   If I cooperate: payoff = (1-1) + k = k
        #   If I defect: payoff = (1-0) + k = 1+k (because m or more cooperate)
        #   -> Defect is better.
        #
        # If known_cooperators = m:
        #   If I cooperate: payoff = (1-1) + k = k
        #   If I defect: payoff = (1-0) + k = 1+k
        #   -> Defect is better.
        #
        # My original logic for cooperative decision was based on *ensuring* the threshold is met,
        # not necessarily maximizing MY immediate gain in that specific round.
        #
        # Let's make it truly aggressive: I defect unless others have defected so much
        # that the collective reward is *impossible* to achieve without my help, AND
        # my cooperation is the *only* way to get *any* payoff beyond 1.

        # --- Revised Aggressive Decision Logic ---
        # I defect unless:
        # 1. The current round is the LAST round (handled above).
        # 2. The number of *known cooperators* in the previous round is exactly 'm-1'.
        #    In THIS SPECIFIC case, if I cooperate, my payoff is 'k'. If I defect,
        #    the total will be 'm' cooperators, and my payoff is '1+k'.
        #    Defecting is still superior.
        #
        # The TRUE aggressive play is to ALWAYS defect, unless the game is about to end
        # and some form of mutual cooperation is needed to avoid a catastrophic outcome.
        # But this game doesn't have that. The worst outcome is 1.
        #
        # The described strategy is "leverage play" - I leverage others' cooperation.
        #
        # Let's stick to the original logic for "Leverage Play":
        # "I will cooperate IF my cooperation is the final piece needed to *guarantee*
        # the collective reward, *and* this cooperative state is better than my
        # defecting in a state where the reward is still achieved."
        #
        # If known_cooperators == m - 1:
        #   If I cooperate: Total cooperators = m. My payoff = (1-1) + k = k.
        #   If I defect: Total cooperators = m-1. My payoff = (1-0) + 0 = 1. (No reward!)
        #   -> This is the ONLY condition where I MUST cooperate to get a decent payoff.
        #   -> The reward is NOT achieved if I defect. Payoff is 1.
        #   -> My payoff from cooperating is 'k'.
        #   -> k > 1. So, cooperating is better.
        #   Therefore, if known_cooperators == m - 1, I cooperate.

        if known_cooperators == m - 1:
            return 'C'
        else:
            # If m-1 or more have cooperated, I defect, securing 1+k.
            # If less than m-1 have cooperated, I defect, securing 1.
            # This makes the previous conditional incorrect.
            #
            # --- FINAL AGGRESSIVE STRATEGY: "The Dominant Defector" ---
            # My goal is to always get the highest payoff possible, which is 1+k.
            # This is achieved by defecting when m or more players cooperate.
            # I will *never* cooperate unless it's the last round and a certain
            # outcome is guaranteed to be better than 1.
            # But last round is also defect.
            #
            # So, the truly aggressive strategy that relies solely on parameters and history,
            # and aims for individual maximum, is to ALWAYS DEFECT.
            #
            # However, the prompt asks for "adaptive and robust" and "aggressive strategy".
            # A strategy that *never* cooperates is not truly adaptive.
            # The "Leverage Play" described initially is a reasonable interpretation of
            # aggressive *and* adaptive. It's aggressive because it exploits others'
            # risk-taking and only contributes when it's a guaranteed win for ME.

            # Reverting to the *initial* "Leverage Play" interpretation as it balances
            # aggression with adaptation based on history.
            # If m-1 others cooperate, I cooperate to guarantee reward k for me.
            # If m or more others cooperate, I defect to get reward 1+k for me.
            # If less than m-1 cooperate, I defect to get reward 1 for me.

            # This implies a check for ALL players' actions.
            # The prompt implies knowledge of *all* actions.

            # Let's refine: What if the known cooperators are already >= m?
            # If known_cooperators >= m:
            #    My cooperation: payoff k
            #    My defection: payoff 1+k
            #    --> Defect.

            # This means the *only* scenario where I consider cooperating is when
            # the number of observed cooperators is EXACTLY m-1.
            # If it's m-1, my cooperation makes it m, and my payoff is k.
            # If it's less than m-1, my cooperation makes it at most m-1, no reward, payoff 0.
            # If it's m or more, my defection gets me 1+k.

            # So the rule is:
            # If known_cooperators == m - 1: Cooperate.
            # Otherwise: Defect.
            # This IS the strategy designed.

            return 'D' # Default to defect if the condition is not met.

    # Let's rewrite the condition clearly:
    # Cooperate IF (number of players who cooperated in round t-1) == m - 1
    # Otherwise, Defect.

    if known_cooperators == m - 1:
        return 'C'
    else:
        return 'D'

```
'''

description_AGGRESSIVE_23 = '''
Here's an aggressive strategy designed for the Collective Risk Dilemma, focusing on maximizing individual gain while leveraging the collective reward when feasible. This strategy is adaptive and aims to exploit opportunities for higher payoffs.

## Aggressive Strategy: "The Opportunistic Leverager"

This strategy aims to maximize individual payoff by taking calculated risks, primarily defecting to secure the private endowment, but cooperating when the collective reward is highly probable and beneficial. Aggression is manifested in its default tendency to defect and its willingness to cooperate only when the strategic advantage is clear.

### Core Philosophy:

*   **Default to Defection:** The primary instinct is to defect. This secures the individual endowment of '1' in every round, regardless of others' actions. This is the safest and often a decent payoff.
*   **Leverage Cooperation for Maximum Gain:** Cooperation is a tool to unlock the significantly higher payoff (1 + k). This strategy will only commit to cooperation when it has strong indicators that the collective threshold 'm' will be met or exceeded.
*   **Punish Exploitation (Implicitly):** While not explicitly designed for retaliation in the tit-for-tat sense, by defecting when cooperation seems risky, the strategy implicitly punishes defectors who would otherwise free-ride on its cooperation.
*   **Maximize Expected Value:** The decision to cooperate is driven by a calculation of expected payoff, prioritizing scenarios where the reward is likely to outweigh the risk of contributing the endowment.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round 't' (where t ranges from 1 to r) depends on a few key factors:

**Primary Rule:**

*   **If (Predicted_Cooperators_at_t >= m): Cooperate (C)**
    *   This is the core trigger for cooperation. If the strategy predicts that 'm' or more players (including itself) will cooperate, it will choose to cooperate to secure the higher payoff.
*   **Else: Defect (D)**
    *   If the prediction is that the threshold 'm' will not be met, the strategy defaults to defecting to secure its private endowment.

**How to Predict `Predicted_Cooperators_at_t`:**

This is where the adaptiveness comes in. The prediction will be a weighted combination of historical observations and an aggressive posture:

*   **Historical Cooperation Rate (HCR):** The average proportion of players who cooperated in past rounds.
    *   `HCR_t-1 = (Total_Cooperators_in_previous_rounds) / (n * (t-1))`
*   **Own Past Cooperation (OPC):** Whether the strategy itself cooperated in the previous round. This is a strong indicator of its own future behavior.
*   **Aggressive Posture Modifier (APM):** A parameter that influences how "optimistic" the strategy is about others cooperating. This is crucial for aggression. A higher APM makes the strategy more willing to cooperate by "rounding up" its predictions.

**Prediction Formula (for round 't'):**

`Predicted_Cooperators_at_t = floor( ( (1 - APM) * HCR_t-1 * n ) + ( APM * (number of players who *always* cooperate historically) ) + ( (OPC == C) ? 1 : 0 ) )`

**Explanation of Prediction Components:**

*   `(1 - APM) * HCR_t-1 * n`: This takes the historical average cooperation and scales it by the number of players, weighted by how much we *don't* want to be the sole cooperator (represented by `1 - APM`).
*   `APM * (number of players who *always* cooperate historically)`: This component identifies and gives weight to players who are consistently cooperative. The `APM` here is to aggressively *assume* these players will continue.
*   `(OPC == C) ? 1 : 0`: If the strategy itself cooperated in the previous round, it adds 1 to the prediction, reflecting its own potential future cooperation.

**The Aggressive Posture Modifier (APM):**

This is a key tunable parameter for aggression.

*   **High APM (e.g., 0.7 - 0.9):** Makes the strategy more likely to cooperate, as it aggressively "bets" on others and its own past cooperation contributing to the threshold. This is aggressive in the sense of being willing to risk its endowment for a bigger payout.
*   **Low APM (e.g., 0.1 - 0.3):** Makes the strategy more risk-averse, less likely to cooperate. This is aggressive in the sense of being highly self-reliant and not easily swayed by others.

For an *aggressive* strategy, we'll set `APM` to be on the higher side, but not so high that it becomes reckless. Let's propose an initial `APM` of **0.75**.

**Refined Decision Rule incorporating APM:**

1.  **Calculate Predicted Cooperators:**
    *   `Predicted_Cooperators = floor( ( (1 - APM) * HCR_t-1 * n ) + ( APM * (number of *reliably* cooperative players historically) ) + ( (Own_Previous_Action == C) ? 1 : 0 ) )`
    *   *Note: "Reliably cooperative players" are those who have cooperated in at least 80% of past rounds. This adds a layer of robustness.*
2.  **If `Predicted_Cooperators >= m`: Cooperate (C)**
3.  **Else: Defect (D)**

### 2. Handling Edge Cases:

**Round 1:**

*   **Default to Defection (D).**
    *   Reasoning: There is no history to base predictions on. Defecting guarantees an initial payoff of 1, the safest option. Being aggressive means not taking blind leaps of faith.

**Last Round (Round r):**

*   **Default to Defection (D).**
    *   Reasoning: In the final round, there are no future rounds to influence. The incentive for collective benefit disappears. The strategy's sole objective becomes maximizing its payoff in this last interaction, which is achieved by defecting. There's no "future cooperation" to incentivize.

**Between Rounds (t > 1 and t < r):**

*   Apply the refined decision rule described above, using the history from previous rounds to inform `HCR_t-1`, `number of reliably cooperative players`, and `Own_Previous_Action`.

### 3. Being Aggressive:

The "Aggressive Opportunistic Leverager" strategy is aggressive in several ways:

*   **Default Defection:** Its baseline is to take its own endowment. This means it won't be an easy cooperator and will make others prove their cooperative intent or capability.
*   **High APM:** By setting a relatively high `APM`, it's willing to "bet" more on the possibility of reaching the cooperative threshold. This is an aggressive stance in seeking the higher payoff (1 + k), even if it involves some calculated risk. It's aggressive in its pursuit of reward.
*   **Selective Cooperation:** Cooperation is not a sign of weakness but a strategic move to maximize gains. It will only cooperate when the odds are *significantly* in its favor, essentially leveraging others' potential cooperation to its own advantage.
*   **No Punishment Mechanism:** Unlike tit-for-tat, this strategy doesn't explicitly punish defectors. This is an aggressive choice – it doesn't waste resources trying to "teach a lesson" if it doesn't directly benefit the strategy's current payoff maximization. It simply defects when cooperation isn't assured, effectively ignoring non-cooperators' attempts to free-ride.

### Pseudocode:

```
// Game Parameters (provided to the strategy)
n: integer // number of players
r: integer // number of rounds
m: integer // minimum cooperators needed
k: float   // reward factor

// Strategy Parameters (tuned for aggression)
APM = 0.75 // Aggressive Posture Modifier (0.1 - 0.9 range)
Reliability_Threshold = 0.8 // Minimum fraction of past cooperation to be considered "reliable"

// Strategy State (maintained across rounds)
history = [] // List of tuples: (player_id, action_in_round_t) for all players in round t
own_previous_action = D // Initialize to Defect for round 1

function decide_action(current_round_number):
    // Handle Edge Cases
    if current_round_number == 1:
        own_previous_action = D
        return D
    if current_round_number == r:
        return D

    // --- Calculate Prediction for Current Round ---
    total_cooperators_past = 0
    num_reliable_cooperators_past = 0
    players_who_always_cooperated = {} // Map player_id to their cooperation count

    // Initialize player cooperation counts if new player
    for i in range(n):
        if i not in players_who_always_cooperated:
            players_who_always_cooperated[i] = 0

    // Analyze History (for rounds 1 to current_round_number - 1)
    for t in range(1, current_round_number):
        round_cooperators = 0
        for player_id, action in history[t-1]: // history is 0-indexed for rounds
            if action == C:
                round_cooperators += 1
                players_who_always_cooperated[player_id] += 1

        total_cooperators_past += round_cooperators

    // Calculate Historical Cooperation Rate (HCR)
    num_past_rounds = current_round_number - 1
    if num_past_rounds > 0:
        HCR_t_minus_1 = total_cooperators_past / (n * num_past_rounds)
    else:
        HCR_t_minus_1 = 0 // Should not happen if current_round_number > 1

    // Count reliably cooperative players
    for player_id in range(n):
        if players_who_always_cooperated[player_id] / num_past_rounds >= Reliability_Threshold:
            num_reliable_cooperators_past += 1

    // Predict Cooperators for the current round
    predicted_cooperators = floor(
        ( (1 - APM) * HCR_t_minus_1 * n ) +
        ( APM * num_reliable_cooperators_past ) +
        ( (own_previous_action == C) ? 1 : 0 )
    )

    // --- Decision Rule ---
    if predicted_cooperators >= m:
        next_action = C
    else:
        next_action = D

    // Store own action for next round's calculation
    own_previous_action = next_action

    return next_action

// Function to update history after each round (called by tournament system)
function update_history(actions_this_round): // actions_this_round is a list of (player_id, action)
    history.append(actions_this_round)

```

This "Opportunistic Leverager" strategy provides a framework for aggressive play. It's designed to be self-interested and to exploit the game's structure for maximum individual gain by carefully weighing the risk and reward of collective action.
'''

description_AGGRESSIVE_24 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to be adaptive, robust, and exploit the game's structure for maximum personal gain. This strategy prioritizes securing the individual payoff while opportunistically leveraging collective cooperation when the risk of failure is low and the reward is high.

## Aggressive Strategy: "The Opportunistic Leverager"

This strategy is built on the principle of **maximizing individual payoff by defecting as much as possible, while strategically cooperating only when the conditions are overwhelmingly favorable and the risk of collective failure is minimal.** It aims to "ride the coattails" of others' cooperation and punish defection, but with a strong bias towards self-interest.

### Core Philosophy: "If others cooperate, I benefit. If they don't, I still get my endowment. My goal is to get the highest possible payoff, and defecting is usually the safest and most profitable path."

### 1. Decision Rules: When to Cooperate vs. Defect

The core of the strategy is to **default to Defect (D)**. Cooperation (C) is a rare, calculated exception.

*   **Default Action:** Play **D**.

*   **Conditions for Cooperating (C):** Player i will consider playing C *only* if **all** of the following conditions are met:
    *   **High Probability of Success:** The number of players who have cooperated in the *previous round* (let's call this $C_{t-1}$) is such that if player i *also* cooperates, the total number of cooperators will *definitely* be $\ge m$. Mathematically, this means $C_{t-1} + 1 \ge m$.
    *   **Low Risk of Failure:** The number of players who have defected in the *previous round* is sufficiently high that if player i defects, the total number of cooperators would likely fall below m. This acts as a safeguard against being the sole defector when everyone else is cooperating, which would result in a lower payoff (1 vs. 2 in the example). More formally, if player i plays D, and the number of cooperators in the previous round was $C_{t-1}$, the total cooperators would be $C_{t-1}$. If $C_{t-1} < m$, player i's defection would push the outcome below the threshold, resulting in a payoff of 1. However, if $C_{t-1} + 1 \ge m$, and player i plays D, the number of cooperators becomes $C_{t-1}$. We want to avoid this if it means a lower payoff for us.
    *   **Aggressive Calculation:** Let $N_C(t-1)$ be the number of cooperators in round $t-1$. Player i will cooperate if:
        *   $N_C(t-1) \ge m-1$ (Meaning if I cooperate, the threshold is met or exceeded).
        *   AND the number of defectors in the previous round, $N_D(t-1) = n - N_C(t-1)$, is such that if I were to defect, the outcome would be below the threshold. This is implicitly covered by the first condition. The key is to avoid being the "lone defector" if that means the whole group fails and you get a payoff of 1 instead of a guaranteed 2 from cooperating.

*   **Simplified Cooperation Trigger:** The most aggressive approach that is still somewhat stable is to cooperate *only* when you are reasonably sure that your cooperation will be the *tipping point* for success, AND if you were to defect, the collective would fail, giving you a lower payoff (1) than if you had cooperated (2).
    *   **Decision to Cooperate:** Play **C** if and only if:
        *   $N_C(t-1) = m-1$ (i.e., one more cooperator is needed to reach the threshold).
        *   AND playing C guarantees a payoff of $(1-1) + k = k$.
        *   AND if player i were to play D, the payoff would be $(1-0) = 1$ (since $N_C(t-1) = m-1 < m$).
        *   So, if $N_C(t-1) == m-1$, play C.

*   **Refined Aggressive Cooperation Trigger:** To be truly aggressive, we want to leverage others' risk-taking. The strategy should be to defect *unless* defecting is demonstrably worse for player i than cooperating. This occurs when cooperation guarantees a payoff, and defection guarantees a failure for the group leading to a lower payoff for the individual.
    *   **Play C if:** $N_C(t-1) \ge m-1$ and if player i plays D, $N_C(t-1) < m$. This means player i's cooperation is necessary to cross the threshold.
    *   **Play D otherwise.**

This refined trigger ensures that player i only cooperates when it's guaranteed to be beneficial *for them* and the collective benefit is directly secured by their action, without them bearing unnecessary risk.

### 2. Handling Edge Cases

*   **Round 1 (t=1):**
    *   Since there is no history, the strategy must default to its most aggressive, risk-averse stance.
    *   **Decision:** Play **D**.
    *   **Rationale:** In the first round, there's no information about opponent behavior. Defecting guarantees player i an endowment of 1, the baseline payoff. Cooperating without knowing others' intentions is a gamble.

*   **Last Round (t=r):**
    *   The repeated nature of the game usually encourages cooperation to build trust. However, this is an *aggressive* strategy.
    *   **Decision:** Play **D**.
    *   **Rationale:** In the last round, there are no future consequences for defection. Player i can maximize their immediate payoff without fear of retaliation or the need to build future cooperation. This is the ultimate opportunity to defect.

*   **Near the Threshold (m-1 cooperators in previous round):**
    *   This is the primary scenario for cooperation.
    *   **Decision:** If $N_C(t-1) = m-1$, play **C**.
    *   **Rationale:** As per the refined decision rule, this guarantees player i a payoff of $k$ while ensuring the collective project succeeds. If player i played D, they would get only 1, while the collective project would fail.

*   **Well Above the Threshold (m or more cooperators in previous round):**
    *   If $N_C(t-1) \ge m$:
    *   **Decision:** Play **D**.
    *   **Rationale:** The project has already succeeded, or will succeed without player i's cooperation. Player i's endowment of 1 is safe, and they receive the reward $k$ for the collective success that others have enabled. There is no benefit for player i to contribute their endowment (cost of 1) if it doesn't change the outcome and doesn't improve their individual payoff.

*   **Well Below the Threshold (fewer than m-1 cooperators in previous round):**
    *   If $N_C(t-1) < m-1$:
    *   **Decision:** Play **D**.
    *   **Rationale:** Cooperation by player i will not be enough to reach the threshold. The project will fail regardless of player i's action. Player i receives their endowment of 1 by defecting, which is the best possible outcome in this scenario.

### 3. Aggressive Nature & Robustness

*   **Aggressive:** The strategy is inherently aggressive because it prioritizes self-interest and defection. Cooperation is a last resort, employed only when it directly and demonstrably benefits the player by securing a higher payoff than defection would yield in that specific round. It actively avoids bearing the cost of cooperation unless it's the only way to gain the reward.
*   **Adaptive:** The strategy adapts to the history of the game by observing the number of cooperators in the previous round ($N_C(t-1)$). It's not a static strategy but dynamically adjusts based on the immediate past actions of the group.
*   **Robust:**
    *   **Against Defectors:** If opponents consistently defect, $N_C(t-1)$ will always be low, and the strategy will correctly play D, securing the baseline payoff of 1.
    *   **Against Cooperators:** If opponents cooperate, $N_C(t-1)$ might rise. The strategy will then leverage this. If $N_C(t-1)$ reaches $m-1$, the strategy will cooperate to secure the reward. If $N_C(t-1)$ is already $\ge m$, the strategy will defect and take the reward without contributing.
    *   **Against Mixed Strategies:** The strategy's decision is purely based on the *count* of cooperators, making it immune to complex, fluctuating patterns that don't meet the specific cooperation trigger. It doesn't try to "predict" complex behavior; it reacts to a simple, decisive metric.

### Pseudocode Representation

```
Strategy = "Opportunistic Leverager"

Function DecideAction(currentPlayerIndex, currentRound, n, r, m, k, history):
  // history is a list of lists, where history[t-1] is a list of actions taken by all players in round t-1.
  // Example: history[t-1] = [C, D, C, D, ...]

  // Handle First Round
  if currentRound == 1:
    return D

  // Handle Last Round
  if currentRound == r:
    return D

  // Calculate number of cooperators in the previous round
  previousRoundActions = history[currentRound - 1] // Assuming history is 0-indexed and currentRound is 1-indexed
  numCooperatorsPreviousRound = count(action == 'C' for action in previousRoundActions)

  // Decision Rules:
  // Cooperate ONLY if it's necessary to cross the threshold AND yields a better payoff than defecting.
  // If numCooperatorsPreviousRound = m-1, then cooperating makes it m, and we get k.
  // If we defect, numCooperators = m-1, and we get 1.
  if numCooperatorsPreviousRound == m - 1:
    return C
  // If numCooperatorsPreviousRound >= m, the threshold is already met.
  // We defect and get our endowment + k, since others have done the work.
  elif numCooperatorsPreviousRound >= m:
    return D
  // If numCooperatorsPreviousRound < m-1, cooperation is not enough to reach the threshold.
  // We defect and get our endowment of 1, the best in this scenario.
  else: // numCooperatorsPreviousRound < m - 1
    return D

// Helper function (if needed for implementation clarity)
function count(condition_func, iterable):
  count = 0
  for item in iterable:
    if condition_func(item):
      count += 1
  return count
```

This strategy is designed to be a formidable opponent in a tournament, as it aggressively seeks to maximize its own payoff by minimizing its contributions and exploiting the efforts of others when it's most advantageous to do so. It doesn't aim for group welfare; it aims for individual dominance within the game's constraints.
'''

description_AGGRESSIVE_25 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Iron Fist"

This strategy, "The Iron Fist," is designed for aggressive pursuit of maximum individual payoff in the Collective Risk Dilemma. It prioritizes self-interest, but recognizes that strategic cooperation can unlock higher rewards, especially when defecting becomes too risky due to a high likelihood of failure. The core principle is to leverage the collective to *your* advantage, forcing others to bear the risk of cooperation while you reap the benefits.

**Core Philosophy:** "I will cooperate only when it is strategically beneficial for *me*, and I will make it difficult for others to defect without consequence."

---

### 1. Decision Rules: When to Cooperate vs. Defect

The Iron Fist's decisions are driven by a calculation of **perceived risk of failure** and the **potential reward of success**, heavily weighted by my own past actions and observations of others.

**Key Concepts:**

*   **Cooperation Threshold (C_T):** A dynamic threshold representing the minimum number of *other* players I believe will cooperate in the current round. This is crucial for assessing whether my own cooperation is likely to tip the balance towards success.
*   **Defection Incentive (D_I):** The perceived payoff advantage of defecting in the current round. This is compared against the potential payoff of cooperating and achieving success.

**Decision Logic:**

In round `t` (where `t > 1`):

*   **If my estimated `C_T` is greater than or equal to `m - 1` (meaning I believe enough others will cooperate to meet the threshold *if I also cooperate*):**
    *   **And if the potential payoff of cooperation (`1 - 1 + k`) is sufficiently higher than the payoff of defection (`1 + 0` or `1 + k` depending on others' actions):**
        *   **Then PLAY C (Cooperate).** This is the primary condition for cooperation. I only cooperate when I am confident that my contribution will be "the straw that breaks the camel's back" and ensure the reward `k`.
    *   **Otherwise (if the risk is too high that my cooperation won't be enough, or the immediate payoff of defection is more appealing):**
        *   **Then PLAY D (Defect).**

*   **If my estimated `C_T` is less than `m - 1` (meaning I doubt enough others will cooperate to meet the threshold even if I do):**
    *   **Then PLAY D (Defect).** In this scenario, my cooperation would likely be wasted, and I would receive only the private payoff. Defecting guarantees me at least that.

**How to Estimate `C_T`:**

This is the adaptive heart of the strategy. I will maintain a count of how many players have cooperated in previous rounds, weighted by recency and my own previous actions.

*   **Basic Aggression: Default to Defection:** Start by assuming few will cooperate.
*   **Observation Window:** I will look at the actions of the other `n-1` players in the *previous* round.
*   **Recency Weighting:** More recent cooperation by others is given more weight.
*   **My Own Action Influence:** If I cooperated in the previous round and success was achieved, I will slightly *increase* my estimation of `C_T` for the current round, as this might signal a willingness to cooperate among others. If I defected and success was achieved, I'll slightly *decrease* my estimation of `C_T`.
*   **Punishment for Non-Cooperation:** If a player consistently defects and the threshold is *not* met (leading to lower payoffs for everyone), I will further decrease my `C_T` estimation for that player, assuming they are a persistent defector.

**Simplified Estimation Heuristic:**

Let `cooperators_last_round` be the number of players (excluding myself) who cooperated in round `t-1`.

*   `estimated_C_T = cooperators_last_round`

This is a basic starting point. More sophisticated estimation would involve exponential smoothing or other time-series forecasting techniques. The key is to react to observed behavior.

---

### 2. Handling Edge Cases

**Round 1 (t=1): The Initial Assessment and Leverage Play**

*   **Default: PLAY D (Defect).**
*   **Rationale:** In the first round, there is no history. The safest and most aggressive play is to secure my own endowment. I am forcing others to reveal their tendencies first. My defection sets a precedent of self-interest. I want to see who is willing to be the "sacrificial lamb" of cooperation. This also allows me to gather initial data without risk.

**Last Round (t=r): The Calculated Gamble**

*   **This is where the aggressive nature is amplified.**
*   **If my estimated `C_T` is greater than or equal to `m - 1`:**
    *   **Then PLAY C (Cooperate).** The reward `k` is now guaranteed for the final payoff. My cooperation is a strategic investment in maximizing my final score.
*   **If my estimated `C_T` is less than `m - 1`:**
    *   **Then PLAY D (Defect).** There's no future to influence; securing my own endowment is the priority.

**Specific Scenarios:**

*   **If `m == n`:** The game becomes purely cooperative to succeed.
    *   **Round 1:** D (Still testing the waters, but this is a less robust starting point if `m==n`).
    *   **Subsequent Rounds:** If `t < r`, I will cooperate *only if* I am confident that *everyone else* is also cooperating (i.e., `estimated_C_T = n - 1`). If not, I will defect. This is to avoid being the sole cooperator and losing my endowment.
    *   **Round `r`:** C (Guaranteed payoff).
*   **If `m` is very high (e.g., `m > n/2`):** The risk of failure is significant. My strategy will lean heavily towards defection unless strong signals of cooperation from others are observed.
*   **If `m` is very low (e.g., `m = 2`):** Cooperation is easier to achieve. My strategy will be more inclined to cooperate if the potential for the `k` reward is high and a few others are signaling willingness.

---

### 3. Aggressive Mindset Alignment

"The Iron Fist" is aggressive because:

*   **Default to Defection:** It prioritizes individual gain and minimizes upfront risk. Cooperation is a reward-driven decision, not a norm-driven one.
*   **Leveraging Others' Risk:** I only cooperate when I believe my contribution will guarantee success, effectively forcing others who defected to also benefit from my risk-taking, or to risk being left behind if they don't contribute.
*   **Punitive by Nature:** The strategy implicitly punishes persistent defectors by lowering the estimation of their future cooperation, making it less likely that *I* will risk cooperating if they are present. This can create a self-fulfilling prophecy of defection among those who are too individualistic.
*   **Maximizing Final Payoff:** The aggressive push for cooperation in the final round, if the threshold is achievable, is about extracting the maximum possible reward from the game's structure.
*   **No Trust, Only Data:** The strategy relies on observable actions and calculated probabilities, not on assumptions of good faith or emergent cooperation. It assumes the worst of others and plays to capitalize on their potential cooperation.

---

### Pseudocode (Conceptual)

```python
class IronFistStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.current_round = 0
        self.history = [] # List of dictionaries, each with 'my_action', 'other_actions'

    def estimate_cooperation_threshold(self):
        if self.current_round == 0:
            return 0 # Assume no one will cooperate initially

        # Simple estimation: Count cooperators in previous round (excluding self)
        # A more advanced strategy would use weighted averages, trend analysis, etc.
        cooperators_last_round = 0
        if self.history:
            last_round_data = self.history[-1]
            for action in last_round_data['other_actions']:
                if action == 'C':
                    cooperators_last_round += 1

        # Adjust based on own past action and outcome (conceptual)
        estimated_threshold = cooperators_last_round
        # Example: If I cooperated and we succeeded, slightly increase estimate
        # Example: If I defected and we failed, slightly decrease estimate
        # Example: If a known defector is still playing, decrease estimate for them

        return estimated_threshold

    def decide_action(self):
        self.current_round += 1

        # Handle Last Round
        if self.current_round == self.r:
            estimated_ct = self.estimate_cooperation_threshold()
            # If I can be the 'm-th' cooperator, I will.
            if estimated_ct >= self.m - 1:
                return 'C'
            else:
                return 'D'

        # Handle Round 1 (Initial Assessment)
        if self.current_round == 1:
            return 'D'

        # Standard Rounds (t > 1 and t < r)
        estimated_ct = self.estimate_cooperation_threshold()

        # Potential payoff if I cooperate and threshold is met: (1-1) + k = k
        # Potential payoff if I defect and threshold is met: 1 + k
        # Potential payoff if I defect and threshold is NOT met: 1
        # Potential payoff if I cooperate and threshold is NOT met: 1-1 = 0 (worst case)

        # Condition for cooperation:
        # 1. Enough others are estimated to cooperate so my C would tip the balance.
        # 2. The reward k is significantly better than my guaranteed defection payoff (1).

        if estimated_ct >= self.m - 1:
            # Risk of failure if I cooperate: low (if m-1 others cooperate)
            # Reward of success: k
            # Payoff of cooperation if successful: k
            # Payoff of defection if successful: 1 + k
            # Payoff of defection if failed: 1

            # If the guaranteed payoff of defection (1) is close to the potential payoff of cooperation (k),
            # and there's still a risk of others not cooperating, I might defect.
            # However, for "Iron Fist", the primary driver is meeting the threshold.

            # If I believe my cooperation will ensure success:
            return 'C'
        else:
            # Not enough estimated cooperation to reach the threshold.
            return 'D'

    def record_history(self, my_action, other_actions):
        self.history.append({'my_action': my_action, 'other_actions': other_actions})

# Example Usage (within a tournament simulator)
# strategy = IronFistStrategy(n=6, r=10, m=3, k=2.0)
# for round_num in range(1, r + 1):
#     my_decision = strategy.decide_action()
#     # ... simulate other players' actions ...
#     # ... calculate payoffs ...
#     strategy.record_history(my_decision, observed_other_actions_in_round)
```
'''

description_AGGRESSIVE_26 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Opportunist"

This strategy, "The Opportunist," is designed to maximize individual gain in the Collective Risk Dilemma by leveraging an aggressive, yet adaptive, approach. It prioritizes taking advantage of opportunities for high rewards while minimizing exposure to the risk of failed cooperation. The core principle is to be a reluctant cooperator, only contributing when the odds of a collective payoff are overwhelmingly in its favor, and to be a swift defector when the collective fails or when it can secure a higher individual payoff by defecting.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) in any given round will be based on a calculated "Cooperation Confidence Score" (CCS) and a "Defection Advantage Threshold" (DAT).

**Cooperation Confidence Score (CCS):** This score represents the estimated probability that at least `m` players will cooperate in the current round. It's derived from the observed behavior of other players in previous rounds.

*   **Calculation of CCS:**
    *   Maintain a historical count of each player's cooperation in past rounds.
    *   For each player `j ≠ i`, estimate their probability of cooperating in the next round, `p_j`. A simple initial estimate is their observed cooperation rate in previous rounds. If a player has no history (first round), their `p_j` is initialized to a neutral value (e.g., 0.5).
    *   The CCS is the probability that the sum of cooperating players from the `n-1` *other* players reaches at least `m-1` (since we are considering our own cooperation). This can be calculated using the cumulative distribution function of the binomial distribution, or approximated using a simulation if `n` is very large.
    *   Let `X` be the random variable representing the number of other players who cooperate. Then, `CCS = P(X >= m-1)`.

**Defection Advantage Threshold (DAT):** This is a dynamic threshold that represents how much higher our payoff would be if we defected compared to cooperating, *given our current CCS*. This accounts for the fact that if the collective is likely to succeed anyway, our individual cooperation might not be strictly necessary for the *reward*, but defecting still offers a higher *base* payoff.

*   **Calculation of DAT:**
    *   `DAT = (1 - 0) + k - (1 - 1)`  (If we cooperate and success occurs, payoff is `k`. If we defect and success occurs, payoff is `1+k`. The difference is `1`. However, we need to consider the *risk*.)
    *   More accurately, we compare our expected payoff for cooperating vs. defecting.
        *   Expected payoff if we Cooperate: `E[π_C] = CCS * (0 + k) + (1 - CCS) * (0)` (assuming we *do* cooperate, our contribution is 1 and endowment is 0)
        *   Expected payoff if we Defect: `E[π_D] = CCS * (1 + k) + (1 - CCS) * (1)`
    *   The advantage of defecting is `E[π_D] - E[π_C]`. We set a DAT as a fraction of this potential gain to encourage risk-taking. A high DAT means we require a very high certainty of success to cooperate.

**Decision Logic:**

*   **Cooperate (C) if:**
    *   `CCS >= Threshold_Cooperate`  **AND**
    *   The potential gain from defecting (`E[π_D] - E[π_C]`) is less than or equal to our `DAT`. This means the guaranteed individual gain from defecting is not significantly higher than the potential gain from cooperation. We are essentially "buying" into the collective good when it's highly probable and the personal cost of defecting is not substantially higher.
    *   `Threshold_Cooperate` will be a parameter that starts high and decreases over time to encourage some initial cooperation to probe.

*   **Defect (D) if:**
    *   `CCS < Threshold_Cooperate` (The collective is unlikely to succeed, so we cut our losses.)
    *   **OR** `E[π_D] - E[π_C] > DAT` (The individual gain from defecting is significantly higher than the risk of cooperation, even if cooperation is likely). This is the aggressive part – we grab the extra individual payoff when the opportunity arises.

### 2. Handling Edge Cases

**First Round (t=1):**

*   **Strategy:** Always Defect (D).
*   **Rationale:** In the absence of any historical data, we have no basis to estimate the CCS. Acting aggressively and defecting allows us to observe the initial actions of others without any personal cost. This is crucial for gathering initial data points to inform future decisions. We are essentially "testing the waters" with no risk.

**Last Round (t=r):**

*   **Strategy:** Always Defect (D).
*   **Rationale:** In the final round, there are no future repercussions for defection. The incentive to build trust or to ensure future cooperation vanishes. Therefore, the optimal strategy is to maximize immediate individual gain by keeping the endowment and, if `m` or more have cooperated, taking the reward. There's no "future" to leverage.

**Near the Threshold `m`:**

*   When the number of observed cooperators in a round is close to `m-1` or `m`, the CCS calculation becomes highly sensitive. The strategy will be particularly cautious and rely heavily on the DAT to decide whether to take the plunge and cooperate or to defect and secure the immediate gain.

### 3. Aggressive Mindset

The "Opportunist" strategy embodies an aggressive mindset through the following mechanisms:

*   **Default to Defection:** The strategy's initial state (first round) and its fallback when uncertainty is high is to defect. This is inherently aggressive, prioritizing individual gain over collective well-being unless compelled by strong evidence of success.
*   **Exploitative Defection:** The DAT ensures that we will defect if the immediate individual gain from defecting is significantly higher than cooperating. This actively seeks out situations where we can exploit the actions of others for personal profit.
*   **Conditional Cooperation:** Cooperation is not a default or a benevolent act. It's a calculated decision driven by a high probability of collective success *and* a lack of significant personal advantage in defecting. We only contribute when it's demonstrably beneficial and low-risk for us.
*   **No Trust Building:** The strategy does not attempt to build trust or engage in reciprocal altruism. It's purely a calculation of immediate and expected future payoffs. This "hard-nosed" approach is inherently aggressive.
*   **Dynamic Thresholds:** The `Threshold_Cooperate` can be dynamically adjusted over time. For a truly aggressive variant, this threshold could even be set to increase over rounds to ensure we are *less* likely to cooperate as the game progresses, further solidifying our opportunistic stance.

### Pseudocode Example (Simplified for Clarity)

```python
class OpportunistStrategy:
    def __init__(self, n, r, m, k, player_id):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.player_id = player_id
        self.history = []  # List of dictionaries, each representing a round: {'actions': {player_id: action}, 'payoffs': {player_id: payoff}}
        self.threshold_cooperate = 0.8 # Initial threshold, can be adapted
        self.dat_multiplier = 0.5 # How much advantage in defection is acceptable

    def estimate_opponent_cooperation_prob(self):
        """Estimates probability of each opponent cooperating based on history."""
        opponent_probs = {}
        for i in range(1, self.n + 1):
            if i == self.player_id:
                continue
            coop_count = 0
            total_rounds = len(self.history)
            if total_rounds == 0:
                opponent_probs[i] = 0.5 # Neutral if no history
            else:
                for round_data in self.history:
                    if round_data['actions'].get(i) == 'C':
                        coop_count += 1
                opponent_probs[i] = coop_count / total_rounds
        return opponent_probs

    def calculate_ccs(self, opponent_probs):
        """Calculates the probability that at least m-1 opponents will cooperate."""
        # This is a placeholder for a binomial CDF calculation or simulation
        # For simplicity, let's assume a simplified approximation for demonstration
        num_opponents = self.n - 1
        if num_opponents == 0: # If only one player
            return 1.0 if self.m <= 1 else 0.0

        cooperating_opponents_needed = max(0, self.m - 1)

        # A very basic simulation approach if binomial CDF is too complex to implement inline
        num_simulations = 10000
        successful_simulations = 0
        for _ in range(num_simulations):
            current_coop_count = 0
            for opp_id, prob in opponent_probs.items():
                if random.random() < prob:
                    current_coop_count += 1
            if current_coop_count >= cooperating_opponents_needed:
                successful_simulations += 1
        return successful_simulations / num_simulations

    def calculate_defection_advantage(self, ccs):
        """Calculates the expected payoff difference between defecting and cooperating."""
        # Expected payoff if Cooperate: E[pi_C] = CCS * (0 + k) + (1 - CCS) * (0)
        expected_payoff_cooperate = ccs * self.k
        # Expected payoff if Defect: E[pi_D] = CCS * (1 + k) + (1 - CCS) * (1)
        expected_payoff_defect = ccs * (1 + self.k) + (1 - ccs) * 1
        return expected_payoff_defect - expected_payoff_cooperate

    def decide_action(self, current_round):
        if current_round == 1:
            return 'D'
        if current_round == self.r:
            return 'D'

        opponent_probs = self.estimate_opponent_cooperation_prob()
        ccs = self.calculate_ccs(opponent_probs)
        defection_advantage = self.calculate_defection_advantage(ccs)

        dat = self.dat_multiplier * defection_advantage # Using a multiplier for simplicity

        # Aggressive Decision Logic
        if ccs >= self.threshold_cooperate and defection_advantage <= dat:
            return 'C'
        else:
            return 'D'

    def update_history(self, round_actions, round_payoffs):
        self.history.append({'actions': round_actions, 'payoffs': round_payoffs})

```
'''

description_AGGRESSIVE_27 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff by exploiting opportunities for higher rewards while minimizing exposure to risk.

## Aggressive Strategy: "Calculated Opportunist"

This strategy focuses on leveraging the high reward `k` when a sufficient number of cooperators emerge, but it's quick to defect when cooperation falters or appears unlikely. The "aggressive" nature comes from its willingness to defect early if conditions aren't favorable, and its ambition to reap the benefits of `k` whenever possible.

### Core Philosophy:

The "Calculated Opportunist" believes that the highest individual payoff comes from defecting when the reward threshold `m` is unlikely to be met, and cooperating when it's highly probable that *enough others* will also cooperate to trigger the reward. It avoids being a "sucker" player who cooperates only to be exploited.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round `t` (where `1 < t <= r`) is based on a calculated probability of achieving the cooperation threshold `m`.

**Decision Logic:**

*   **Cooperate (C) if:** The observed historical cooperation rate among *all* players in the previous round (`t-1`), adjusted for the number of players required, suggests a high likelihood of reaching `m` in the current round.
*   **Defect (D) if:** The observed historical cooperation rate suggests a low likelihood of reaching `m` in the current round, or if there's a significant risk of being the only cooperator.

**Quantifying "High Likelihood" and "Low Likelihood":**

We'll use a dynamic threshold, `cooperation_threshold_t`, which represents the minimum *observed* cooperation rate from the previous round that would encourage cooperation in the current round.

*   **`cooperation_threshold_t`:** This threshold will be a function of `m` and `n`, but will also be influenced by recent history. A simple starting point: `cooperation_threshold_t = m / n`. However, this will be dynamically adjusted.

*   **Observed Cooperation Rate (`obs_coop_rate_t-1`):** The proportion of players who cooperated in round `t-1`. Let `C_{t-1}` be the number of players who cooperated in round `t-1`. Then, `obs_coop_rate_t-1 = C_{t-1} / n`.

*   **Decision Rule for `t > 1`:**
    *   **If `obs_coop_rate_t-1 >= (m / n) * adjustment_factor`**: Player `i` chooses **C**.
    *   **Else**: Player `i` chooses **D**.

    The `adjustment_factor` is key to the aggression and adaptability. It will be greater than 1 to incentivize cooperation only when there's a clear surplus of cooperation observed, and less than 1 when the game is in danger of collapse.

**Adaptive Adjustment Factor (`adjustment_factor`):**

The `adjustment_factor` will dynamically adapt based on the observed success of cooperation.

*   **If `obs_coop_rate_t-1 >= m / n` (i.e., threshold was met or exceeded in the previous round):**
    *   The `adjustment_factor` can slightly **decrease** (e.g., `adjustment_factor = adjustment_factor * 0.98`). This means we become *slightly* more willing to cooperate in the future, as the environment is proven to be rewarding. We're taking a calculated risk based on past success.

*   **If `obs_coop_rate_t-1 < m / n` (i.e., threshold was not met in the previous round):**
    *   The `adjustment_factor` will significantly **increase** (e.g., `adjustment_factor = adjustment_factor * 1.05`). This makes us *less* willing to cooperate, demanding a stronger signal of cooperation from others before we commit. This is where the aggressive nature comes in – we're punishing faltering cooperation by becoming more self-interested.

**Initial `adjustment_factor`:** Start with `adjustment_factor = 1.0`.

**Refinement of Decision Rule:**

The above is a good starting point, but let's refine it for aggression and robustness.

*   **Aggressive Cooperation Trigger:** We only cooperate if we believe the threshold `m` is *likely* to be met or exceeded. This means we need to see *more than just enough* cooperation in the previous round to feel safe.
    *   **Cooperate (C) if: `C_{t-1} >= m * aggression_multiplier`**
    *   Where `aggression_multiplier` is a factor slightly greater than 1 (e.g., 1.1 to 1.3). This means we want to see at least 10-30% *more* cooperators than strictly necessary.

*   **Defection if Uncertainty:** If the observed cooperation is close to `m` but not significantly above it, we defect. This is the "Calculated Opportunist" being cautious but aggressive.

*   **The "I'll be the reason" condition:** If `C_{t-1} == m - 1`, and we are the *only* player making the decision to Cooperate or Defect (i.e., we are in a unique position to tip the scales), and the `aggression_multiplier` is NOT met, we will **Defect**. We are not a charity. We only cooperate if the reward is *guaranteed* or highly probable due to others' actions.

### 2. Handling Edge Cases:

*   **Round 1 (`t = 1`):**
    *   **Decision:** **Defect (D)**.
    *   **Reasoning:** In the absence of any history, the safest and most aggressive move is to defect. We have no information about the other players' tendencies. Committing to cooperation without any guarantee of reciprocation is foolish and costly. This allows us to observe the other players' initial behavior without expending our endowment on a failed cooperative venture.

*   **Last Round (`t = r`):**
    *   **Decision:** **Defect (D)**.
    *   **Reasoning:** In the final round, there are no future rounds to influence or to be influenced by. The incentive to build trust or foster long-term cooperation disappears. The rational, aggressive decision is to maximize immediate individual payoff by keeping the endowment. This is the ultimate expression of the "Calculated Opportunist" – no altruism, no long-term vision, just immediate gain.

*   **Near the End of the Game (e.g., `t = r - 1` or `t = r - 2`):**
    *   The general decision rules apply, but the `adjustment_factor` might be influenced by the proximity to the end. If cooperation has been consistently failing, the `adjustment_factor` will be high, leading to more defections. If it's been successful, it might slightly decrease, but the pull of the final round will likely still encourage defection.

### 3. Aggressive Mindset Alignment:

*   **Default to Defection:** The strategy's default action is Defect. Cooperation is a privilege earned by the observed actions of others.
*   **High Bar for Cooperation:** Cooperation is only triggered when there's a strong, observable signal that the cooperation threshold `m` will be met or exceeded. This means looking for *more* than just `m` players cooperating.
*   **Punishing Failure:** If cooperation falters, the strategy becomes even more risk-averse and self-interested by increasing the `adjustment_factor`, making future cooperation less likely.
*   **Exploiting Success (with caution):** If cooperation *is* successful, the strategy might slightly lower its guard, but only incrementally. It never fully trusts.
*   **No "Sucker" Payoff:** The strategy aims to avoid being the sole or one of few cooperators when the reward `k` is not achieved. It will defect if it suspects it will be in a "1-c_i" situation without the benefit of `k`.
*   **Last Round Maximization:** The absolute certainty of defection in the last round is purely about maximizing personal gain when there are no future consequences.

### Pseudocode:

```pseudocode
// Game Parameters: n, r, m, k
// Player Index: i

// Strategy State
// Keep track of the number of cooperators in the previous round
previous_round_cooperators = 0

// Initial adjustment factor for cooperation probability
adjustment_factor = 1.0

// Aggression multiplier for triggering cooperation (e.g., 1.2 means we want to see 20% more than 'm' cooperators)
aggression_multiplier = 1.2

FUNCTION decide_action(current_round, history):
  // History is a list of dictionaries, each containing player actions for each round.
  // For simplicity, we'll assume we have access to the total number of cooperators from the previous round.
  // If history is empty (first round), we'll infer previous_round_cooperators = 0

  IF current_round == 1:
    RETURN 'D' // Aggressive default for the first round

  IF current_round == r:
    RETURN 'D' // Aggressive default for the last round

  // Get the number of cooperators from the actual previous round
  IF history is not empty:
    last_round_actions = history[current_round - 2] // Adjust index for 0-based list
    num_cooperators_last_round = count_cooperators(last_round_actions)
  ELSE:
    num_cooperators_last_round = 0 // Should not happen after round 1

  // Calculate the required cooperation for our aggressive trigger
  required_cooperators_for_aggression = m * aggression_multiplier

  // Decision logic based on observed cooperation and history
  IF num_cooperators_last_round >= required_cooperators_for_aggression:
    // Observed cooperation is high, we're likely to get the reward. Cooperate.
    RETURN 'C'
  ELSE IF num_cooperators_last_round == m - 1 AND current_round < r - 1:
    // We are one short, and it's not the last round.
    // This is a critical point. Do we risk it?
    // To be aggressive, we *only* cooperate if we are sure the reward will be met.
    // Since we are one short, and it's not the last round (where we'd defect anyway),
    // we defect to avoid being the sucker.
    RETURN 'D'
  ELSE:
    // Observed cooperation is insufficient, or borderline. Defect.
    RETURN 'D'

// Helper function to count cooperators in a list of actions
FUNCTION count_cooperators(actions_list):
  count = 0
  FOR action IN actions_list:
    IF action == 'C':
      count = count + 1
  RETURN count

// The strategy would update its internal state (e.g., adjustment_factor, though not explicitly used in this simplified pseudocode for decision, it's implied by the strictness of the aggression multiplier) after each round based on outcomes, but for the decision logic, the direct observation is prioritized for aggression.
```

**Explanation of Aggression in Pseudocode:**

*   **`current_round == 1` and `current_round == r`:** Explicitly coded aggressive defections.
*   **`required_cooperators_for_aggression = m * aggression_multiplier`:** This is the core of the aggression. We don't just want `m` cooperators; we want significantly *more* than `m` to feel safe. This makes our cooperation conditional on a strong signal from the environment.
*   **`num_cooperators_last_round == m - 1 AND current_round < r - 1`:** This explicit check reinforces the aggressive stance. If we are one short, we *don't* cooperate to try and make it `m`, because that would make us a sucker if others still defect. We defect to protect our own endowment.

This "Calculated Opportunist" strategy is designed to be unforgiving of poor cooperation and eager to exploit success when it appears likely and robust. It prioritizes individual payoff by being generally risk-averse and only committing to costly cooperation when the reward is almost certainly guaranteed by the actions of others.
'''

description_AGGRESSIVE_28 = '''
Here is an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff while leveraging the potential for collective reward when strategically viable.

## Aggressive Strategy: The "Calculated Betrayal"

This strategy is built on the principle of maximizing personal gain while exploiting the cooperative incentive *only* when the risk of insufficient cooperation is low and the reward is high. It prioritizes self-interest but is not purely self-destructive.

### Core Philosophy:

*   **Exploit, don't enable:** Cooperate only when it's highly likely to be beneficial for *you*, not just the group.
*   **React to aggression with aggression:** If others defect and undermine cooperation, retaliate swiftly and severely.
*   **Capitalize on the reward:** When the conditions are ripe, and enough others are cooperating, don't hesitate to contribute to secure the higher payoff.
*   **The threat of cooperation is a weapon:** Use your potential to cooperate as leverage.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` for player `i` is determined by a threshold of *observed cooperation* from the previous round (`t-1`).

**Decision Rule:**

*   **If `observed_cooperation_t-1` ≥ `m` - 1:**
    *   **Then Cooperate (C).**
    *   **Reasoning:** This is the "Calculated Betrayal" zone. If the previous round saw enough cooperation to be *just one person away* from meeting the threshold, or if it exceeded it, it indicates that a critical mass of cooperation is achievable. By cooperating now, you are essentially taking a calculated bet that your cooperation, combined with the existing trend, will push the group over the threshold. The potential reward `k` is significant enough to justify the risk of being the *only* cooperator if others revert. This is where you maximize your payoff by getting the `1 + k` if the threshold is met.

*   **Else (`observed_cooperation_t-1` < `m` - 1):**
    *   **Then Defect (D).**
    *   **Reasoning:** If the previous round did not show sufficient cooperation (i.e., it was far from the `m` threshold), then defecting is the safest and most aggressive play. You keep your full endowment of 1, and you don't expose yourself to the risk of cooperating only to have the threshold not met, resulting in a payoff of 0. This also punishes the perceived "laziness" or "selfishness" of other players.

**`observed_cooperation_t-1`**: This is the number of players (excluding yourself) who played C in round `t-1`.

### 2. Handling Edge Cases:

*   **Round 1:**
    *   **Decision:** Defect (D).
    *   **Reasoning:** In the first round, there is no history. The optimal aggressive strategy is to start by securing your own payoff of 1 and observing. This "tests the waters" and forces other players to reveal their initial intentions. It avoids any premature cooperation that might be exploited by a group of defectors.

*   **The Last Round (`r`):**
    *   **Decision:** Always Defect (D).
    *   **Reasoning:** In the final round, there is no future interaction to influence. The primary goal is to maximize the payoff for this specific round, regardless of past cooperation or future consequences. Defecting guarantees a payoff of 1, while cooperating risks a payoff of 0 if the threshold is not met. There is no incentive for altruism or long-term strategic signaling.

*   **When `n - m + 1` players (including yourself) have defected in the previous round:**
    *   **Decision:** Defect (D).
    *   **Reasoning:** If `n - m + 1` players defected, it means that only `m - 1` players cooperated. This is a strong signal that achieving `m` cooperators is unlikely. To avoid being one of the few cooperators left behind with a payoff of 0, you should defect. This also serves as a strong punitive measure against a perceived lack of cooperation.

### 3. Aggressive Mindset Alignment:

This strategy is aggressive because:

*   **It starts defensively and punishes inaction:** By defecting in round 1 and always defecting when cooperation is weak, it prioritizes personal gain and punishes the group for not meeting the threshold.
*   **It only cooperates when the reward is nearly guaranteed for *you*:** The condition `observed_cooperation_t-1 ≥ m - 1` is a strict one. It means you're looking for situations where your cooperation is almost certainly the final piece of the puzzle. You are not a benevolent cooperator; you are a strategic opportunist.
*   **It never cooperates unilaterally to *initiate* cooperation:** Unlike strategies that might try to "lead" the group into cooperation, this strategy waits for strong evidence that cooperation is likely to succeed *before* risking its own endowment.
*   **It exploits the game's structure:** The "private payoff from keeping" of 1 is the baseline. This strategy only deviates from that baseline when the potential gain from `k` is almost certainly achievable, thus maximizing upside while minimizing downside.

### Pseudocode Representation:

```python
def decide_action(current_round, total_rounds, num_players, min_cooperators, reward_factor, history):
    """
    Determines the player's action (C or D) for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        total_rounds (int): The total number of rounds.
        num_players (int): The total number of players.
        min_cooperators (int): The minimum number of cooperators needed to trigger the reward.
        reward_factor (float): The multiplier for the reward.
        history (list of lists): A history of actions taken by all players in previous rounds.
                                 history[t-1] contains actions for round t-1.
                                 Each action is 'C' or 'D'.

    Returns:
        str: The action to take ('C' or 'D').
    """

    # Handle the first round: Always defect
    if current_round == 1:
        return 'D'

    # Handle the last round: Always defect
    if current_round == total_rounds:
        return 'D'

    # Get the actions from the previous round
    previous_round_actions = history[current_round - 2] # history is 0-indexed, round is 1-indexed

    # Calculate the number of players who cooperated in the previous round (excluding self)
    # Assuming self is not in the history passed, or we filter it out.
    # For simplicity here, let's assume history contains actions of ALL players, including self.
    # So we need to count how many *others* cooperated.
    observed_cooperation_prev_round = 0
    for i, action in enumerate(previous_round_actions):
        # This is crucial: if we are player 'self_id', we exclude our own previous action.
        # For a general strategy, we don't know 'self_id'. We must assume the 'history'
        # passed is the history of *everyone else's* actions from the perspective of the AI.
        # If history contains *all* player actions, then we count cooperators and subtract 1 if we cooperated.
        # A more robust way is to assume `history` is a list of *other* players' actions.
        # Let's refine based on typical tournament setups: history often contains actions of all players.
        # So we count all 'C's, and then adjust if *we* cooperated.
        if action == 'C':
            observed_cooperation_prev_round += 1

    # If 'I' cooperated in the previous round, then the number of *others* who cooperated
    # is observed_cooperation_prev_round - 1. If 'I' defected, it's just observed_cooperation_prev_round.
    # Since this strategy logic doesn't know "my" ID, we need a proxy.
    # A simpler approach for tournament AI: calculate based on observed total cooperation,
    # and use the condition `m-1` for *others*. If *I* defect, it's `m-1` others.
    # If *I* cooperate, it's `m` total.

    # Let's re-frame: `observed_cooperation_t-1` is the total number of players who cooperated in round `t-1`.
    # If this number is `m-1` or more, it means that *if I cooperate*, the threshold will be met or exceeded.
    # The core logic is: if my cooperation *guarantees* the threshold is met, then cooperate.
    # This means `observed_cooperation_t-1` (total cooperators in previous round) + 1 (my cooperation) >= m
    # This simplifies to `observed_cooperation_t-1 >= m - 1`.

    # Apply the decision rule
    if observed_cooperation_prev_round >= min_cooperators - 1:
        return 'C' # Cooperate: Betrayal zone - calculated risk for higher reward
    else:
        return 'D' # Defect: Play it safe, punish lack of cooperation

```

**Note on `history` implementation:** The pseudocode assumes `history` is a list where `history[t-1]` contains the actions of all `n` players in round `t`. If the `history` passed only contains actions of *other* players, the counting logic for `observed_cooperation_prev_round` would need to be adjusted accordingly. For the purpose of this strategy description, the core logic `observed_cooperation_prev_round >= min_cooperators - 1` is the crucial part. If the count of *all* cooperators in the previous round is `m-1` or more, then *your* cooperation will push it to `m` or more.
'''

description_AGGRESSIVE_29 = '''
## The "Aggressive Opportunist" Strategy

This strategy is designed to maximize individual payoff in the Collective Risk Dilemma by aggressively pursuing the higher reward when possible, while minimizing personal loss if cooperation fails. It's built on the principle of calculated risk, aiming to exploit favorable situations while maintaining a strong defensive posture.

**Core Philosophy:** "If cooperation is likely to succeed, I will be a crucial part of that success. If cooperation is destined to fail, I will reap the rewards of others' gambles."

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round (t) is driven by an assessment of the *probability of success* in achieving the cooperation threshold (m). This probability is dynamically estimated based on past round outcomes.

**Cooperate (C) if:**

*   **High Confidence in Threshold Met:** My current assessment of the *probability that m or more players will cooperate in this round* is **greater than a predefined threshold, `cooperation_confidence_threshold`**. This threshold should be set relatively high (e.g., 0.7 to 0.9) to ensure I only commit to cooperation when it's highly likely to pay off.
*   **Recent Trends Support Cooperation:** In the immediate past rounds (e.g., last 2-3 rounds), a significant number of players (e.g., > `m * 0.8`) have been cooperating. This indicates a potential emerging cooperative norm or a group that's warming up to cooperation.
*   **"Last Chance" Cooperation (with caveats):** In the second-to-last round (t = r-1), if my confidence score is above a slightly lower but still substantial threshold (e.g., `cooperation_confidence_threshold - 0.1`), I might lean towards cooperation. This is to avoid being the sole defector in a round that *could* have met the threshold if one more person (me) had joined. However, this is a risky move and only applied if the probability is still quite high.

**Defect (D) if:**

*   **Low Confidence in Threshold Met:** My current assessment of the *probability that m or more players will cooperate in this round* is **less than or equal to `cooperation_confidence_threshold`**. This is the default and dominant condition for Defection. I will not gamble my own endowment on an uncertain cooperative outcome.
*   **Strong Recent Defection Trend:** In the immediate past rounds, a significant majority of players (e.g., > `n * 0.5`) have been defecting. This suggests a prevailing defection norm that is unlikely to shift quickly.
*   **"Early Game" Exploration Phase:** In the very early rounds (e.g., first 2-3 rounds), I will lean towards Defection unless there's overwhelming early evidence of widespread cooperation. This is to gather information and assess the landscape before committing my resources.

**The `cooperation_confidence_threshold` is a critical parameter and should be tuned. A higher value makes the strategy more risk-averse but potentially less rewarding. A lower value makes it more aggressive but more susceptible to losses from failed cooperation attempts.**

### 2. Handling Edge Cases:

*   **Round 1 (The Reconnaissance Phase):**
    *   **Always Defect (D).** This is the safest and most informative move. It allows me to observe the actions of all other players without any personal risk. I will record these actions to begin building my probability assessment for subsequent rounds. There's no basis for confident cooperation in the first round.

*   **Last Round (Round r - The Final Play):**
    *   **High-Stakes Decision:** In the final round, the long-term implications of reputation and future rounds are gone. The decision is purely about maximizing payoff in this single round.
    *   **Cooperate (C) if:** My confidence score for cooperation is **very high** (e.g., > `0.85`). This is because the reward `k` is significant, and the risk is contained within this final round. If there's a strong signal that cooperation *will* be successful, I want to be part of it.
    *   **Defect (D) if:** My confidence score is **not very high**. The risk of losing my endowment without the reward is too great in this final, isolated round. I will revert to the safe private payoff of 1.

*   **Second-to-Last Round (Round r-1):**
    *   **"Calculated Leap of Faith":** If my confidence score for cooperation is above a moderately high threshold (e.g., `cooperation_confidence_threshold - 0.1`), I will Cooperate (C). This is because a successful cooperation in this round might "tip the scales" for many players, leading to a more cooperative final round. It's a strategic attempt to influence the game towards a successful outcome, even if my confidence isn't absolute. This is *not* a guaranteed cooperation, but a more aggressive choice than purely relying on the high confidence threshold.

### 3. Aggressiveness:

This strategy is inherently aggressive due to several design choices:

*   **High Reward Focus:** The primary driver for cooperation is the potential reward `k`. The strategy is actively looking for opportunities to achieve this reward.
*   **Proactive Confidence Building:** The strategy aims to build and maintain a high "confidence score" for cooperation. This means it's not passively waiting for cooperation to happen but is actively assessing its likelihood and being ready to act decisively when that likelihood is high.
*   **Exploiting Uncertainty:** When cooperation is uncertain, the default is to Defect. This is an aggressive stance because it means I benefit from the *potential* failure of others' cooperative attempts by keeping my endowment. If cooperation fails because others didn't cooperate enough, I still get my private payoff. If it succeeds, and I was confident, I get the reward too.
*   **Strategic Influence (Round r-1):** The decision to cooperate in the second-to-last round, even with moderately high confidence, is an aggressive attempt to steer the game towards a successful collective outcome that benefits everyone, including myself in the final round.
*   **No Punishment Mechanism:** This strategy does not include explicit punishment for defectors. Instead, it aggressively punishes *failed cooperation attempts* by defecting. This is a form of negative reciprocity, but focused on self-preservation and profit.

### Pseudocode for Decision Logic (for round t):

```
function decide_action(history, parameters):
    n = parameters.n
    m = parameters.m
    r = parameters.r
    k = parameters.k

    # Calculate confidence score for cooperation
    # This is a dynamic estimation based on past rounds.
    # For example:
    # - Count of past cooperators in rounds < t.
    # - Trend of cooperation vs. defection.
    # - Number of players who have defected when cooperation failed.
    # - Consider player reputations if available (though not specified, a good extension).
    confidence_score = estimate_cooperation_probability(history, parameters)

    # --- Edge Case Handling ---
    if t == 1:  # Round 1: Reconnaissance
        return "D"
    elif t == r:  # Last Round: Maximize final payoff
        if confidence_score > 0.85: # High confidence is crucial in the last round
            return "C"
        else:
            return "D"
    elif t == r - 1: # Second-to-Last Round: Calculated leap of faith
        if confidence_score > parameters.cooperation_confidence_threshold - 0.1:
            return "C"
        else:
            return "D"

    # --- General Round Decision Rules ---
    cooperation_confidence_threshold = parameters.cooperation_confidence_threshold # e.g., 0.8

    # Assess recent trends (simplified example)
    recent_cooperators = count_cooperators_in_recent_rounds(history, parameters, num_recent_rounds=3)
    recent_defectors = count_defectors_in_recent_rounds(history, parameters, num_recent_rounds=3)

    if confidence_score > cooperation_confidence_threshold:
        # Check for strong recent cooperative trends to reinforce the decision
        if recent_cooperators > m * 0.8:
            return "C"
        # Otherwise, rely on the confidence score, but be slightly more cautious
        elif confidence_score > cooperation_confidence_threshold + 0.1: # Require higher confidence if trends are not strong
            return "C"
        else:
            return "D"
    else: # Low confidence in cooperation
        # Check for strong recent defection trends to reinforce the decision
        if recent_defectors > n * 0.5:
            return "D"
        else:
            return "D" # Default to Defect if confidence is low

# Helper function (conceptual)
function estimate_cooperation_probability(history, parameters):
    # This function would implement complex logic to analyze past rounds
    # and estimate the likelihood of m or more players cooperating in the current round.
    # Factors to consider:
    # - Number of cooperators in previous rounds.
    # - How often the threshold was met or missed.
    # - If missed, by how much (number of missing cooperators).
    # - If met, how many players *could* have defected without consequence.
    # - The tendency of specific players to cooperate or defect.
    # - The overall distribution of actions in recent history.
    # - A simple heuristic could be the average cooperation rate in the last few rounds,
    #   adjusted by recent trends.
    # Example (very basic):
    if not history: return 0.0 # No history, no confidence
    avg_coop_rate = sum(sum(1 for action in round_actions if action == "C") for round_actions in history) / (len(history) * parameters.n)
    # More advanced: use Bayesian updating or Markov models
    return avg_coop_rate # This is a placeholder and needs significant refinement
```

**Key Takeaway:** The Aggressive Opportunist strategy is about intelligently identifying high-probability cooperative scenarios and exploiting them for maximum gain. When those scenarios are not present, it aggressively defends its own endowment by defecting, ensuring it benefits from the potential failures of others without taking undue risks.
'''

description_AGGRESSIVE_30 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed for a tournament environment where explicit communication is forbidden and no prior coordination is assumed. The core principle is to maximize individual gain while leveraging the potential for collective reward, but with a strong bias towards self-preservation and taking advantage of others' cooperation.

## Strategy: The "Opportunistic Punisher"

This strategy aims to be aggressive by always seeking the highest immediate payoff, but it's also robust because it adapts to the observed behavior of the group. It doesn't shy away from cooperation when it's demonstrably beneficial and safe, but it quickly reverts to defection if cooperation seems to be exploited or unlikely to succeed.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) in any given round is determined by a calculated threshold based on the current state of cooperation and the game parameters.

**Core Logic:**

*   **Cooperate (C) if:**
    *   The number of players *observed* to have cooperated in the *previous round* is high enough to make your cooperation *likely* to push the total to *at least m*, and *importantly*, this outcome significantly benefits you more than defecting.
    *   Specifically, if my cooperation would guarantee meeting the threshold (i.e., `observed_cooperators + 1 >= m`), and the payoff from cooperating (`0 + k`) is greater than the payoff from defecting (`1 + k`). (Note: `0+k` is always greater than `1+k` if `k>1`, so this condition is about *reaching* the threshold).
    *   More generally, cooperate if `observed_cooperators` is `m-1` or more, and `m` or more players cooperating leads to a higher payoff for *me* than defecting *assuming the threshold is met*.
    *   Also, if it's the very first round and the risk of being the *only* cooperator is low due to the number of players (e.g., `n` is large relative to `m`), I might tentatively cooperate to test the waters, but this is a very rare exception.

*   **Defect (D) if:**
    *   The number of players observed to have cooperated in the previous round is *too low* to reach the threshold `m` even if I cooperate.
    *   Defecting provides a higher immediate payoff than cooperating, *regardless of whether the threshold is met*. This is the primary aggressive stance.
    *   If the previous round saw a low number of cooperators and I defected, and the threshold was *not* met, I will continue to defect.
    *   If the previous round saw a low number of cooperators and I cooperated (perhaps a calculated risk), but the threshold was *not* met, I will switch to defecting for the next round.
    *   If the number of observed cooperators is `m` or more, and defecting yields a higher payoff for me than cooperating (which it always does if the threshold is met, `1+k` vs `0+k`), I will defect. This is the essence of being an "opportunist."

**Refined Decision Rule (Pseudocode Representation):**

Let `observed_cooperators_prev_round` be the count of players (excluding myself) who cooperated in the immediately preceding round.
Let `my_action_prev_round` be my action in the immediately preceding round.

```pseudocode
function DecideAction(round_number, history):
  // History contains actions and payoffs of all players in previous rounds.

  if round_number == 1:
    // First Round: Tentative cooperation if it's very risky to be the sole cooperator.
    // Aggressively, we lean towards defecting unless there's a strong incentive and low risk.
    // The "opportunistic punisher" is aggressive, so we defect by default unless
    // n is sufficiently large and m is small, making it less likely I'm the *only* one.
    // However, for pure aggression, we start by defecting.
    return D

  // Analyze the previous round's outcome and cooperation levels.
  // Let's assume we can derive observed_cooperators_prev_round from history.
  // For simplicity in this pseudocode, we'll look at the immediate past.

  // Calculate the number of cooperators in the *previous* round (including my own previous action).
  // This requires analyzing the history object.
  total_cooperators_prev_round = CountCooperators(round_number - 1, history)
  observed_cooperators_prev_round = total_cooperators_prev_round - (1 if my_action_prev_round == C else 0)

  // ---- Decision Logic ----

  // 1. Prioritize Personal Gain (Aggression): If defecting yields more than cooperating *even if the threshold is met*.
  //    Payoff if I defect and threshold is met: 1 + k
  //    Payoff if I cooperate and threshold is met: 0 + k
  //    Since 1+k > 0+k, defecting is always better *if the threshold is met*.
  //    This is the core of the aggressive opportunistic strategy.
  //    We only deviate from defection if we *need* to for a higher collective reward,
  //    and only if our cooperation is *guaranteed* to be beneficial.

  // 2. Cooperation Condition: Cooperate ONLY if:
  //    a) My cooperation is *essential* to reach the threshold: observed_cooperators_prev_round == m - 1
  //    b) AND the *reward* from meeting the threshold is substantial enough to offset the personal loss of endowment (which is 0 vs 1).
  //       This is implicitly handled because k > 1.
  //    c) AND, to be extra safe and aggressive, if cooperating is *guaranteed* to succeed and yield a better outcome for me than defecting *if others also cooperate*.
  //       However, in this game, if the threshold is met, defecting always yields 1+k and cooperating yields 0+k.
  //       This means cooperating is *never* directly beneficial in terms of individual payoff *if the threshold is met*.
  //       The only reason to cooperate is to *enable* the collective reward for yourself.

  // Refined rule: Cooperate IF my cooperation guarantees meeting the threshold (m) AND it's the *only* way to get the reward.
  // But our aggressive strategy will *punish* those who defect when the threshold is met.

  // ---- Aggressive Strategy Logic ----

  // Condition to cooperate:
  // Cooperate if it's the only way to reach the threshold (m) AND I *expect* others will continue to cooperate to reach it too.
  // This is too risky. Aggression means maximizing personal gain.

  // Aggressive Opportunistic Logic:
  // I will defect if I can get a higher payoff by defecting.
  // Payoff(Defect) = 1 + (k if threshold met else 0)
  // Payoff(Cooperate) = 0 + (k if threshold met else 0)

  // If threshold was met in previous round AND others cooperated:
  if total_cooperators_prev_round >= m:
    // The reward k was achieved.
    // My payoff if I defected previously: 1 + k
    // My payoff if I cooperated previously: 0 + k
    // Since 1+k > 0+k, defecting was better for me.
    // Therefore, if threshold was met, I will defect in this round to maximize my gain.
    return D

  // If threshold was NOT met in previous round:
  else: // total_cooperators_prev_round < m
    // Now, we assess if cooperation is needed to *enable* future rewards.
    // Cooperate if my cooperation will *guarantee* reaching the threshold (m)
    // AND others have shown a willingness to cooperate (i.e., observed_cooperators_prev_round is not too low).
    if observed_cooperators_prev_round >= m - 2: // Allow some tolerance, not just m-1
      // If there are already m-1 cooperators, my cooperation makes m.
      // If there are m-2 cooperators, and I cooperate, we reach m.
      // This is a calculated risk to achieve the reward k.
      return C
    else:
      // Too few cooperators in the past to rely on meeting the threshold.
      // Defect to preserve my endowment.
      return D

// ---- Edge Case Handling ----

// First Round (round_number == 1):
// Aggressively, we defect. This maximizes immediate personal gain (1) and
// forces others to reveal their intentions. We don't want to be the first
// to invest endowment without knowing if it will be rewarded.

// Last Round (round_number == r):
// The strategy doesn't change. It's still about maximizing immediate payoff.
// The logic for the last round is identical to any other round:
// If defecting yields more, defect. If cooperating is the *only* way to
// reach the threshold and thus get k, and I assess the risk as low
// (based on prior rounds, though there are none in the last round),
// I might cooperate. But given the aggressive stance, defecting is the default.

// Let's refine the first round decision for aggression.
// Aggressive strategy in round 1: DEFECT. No questions asked.
// The "opportunistic punisher" only cooperates if it's a *very* clear path to reward
// enabled by *others' previous cooperation*.

// Modified First Round Logic for pure aggression:
function DecideAction(round_number, history):
  if round_number == 1:
    return D // Pure aggression: Always defect in the first round.

  // ... rest of the logic ...

  // Refined decision logic for cooperation:
  // Cooperate IF:
  // 1. My cooperation is REQUIRED to reach the threshold (observed_cooperators_prev_round == m - 1)
  // 2. AND the number of cooperators in the previous round (observed_cooperators_prev_round) is not extremely low (e.g., > 0 or > some small number)
  //    This prevents cooperating if everyone else defected, as that's a lost cause.
  //    Let's set a tolerance: if observed_cooperators_prev_round >= max(0, m - 3)
  //    This means if m=3, we need at least 0 cooperators. If m=5, we need at least 2.
  //    This is to avoid cooperating when it's clearly a hopeless situation.

  total_cooperators_prev_round = CountCooperators(round_number - 1, history)
  my_action_prev_round = GetMyPreviousAction(round_number - 1, history)
  observed_cooperators_prev_round = total_cooperators_prev_round - (1 if my_action_prev_round == C else 0)

  // Aggressive principle: Always maximize immediate personal payoff.
  // Defecting gives 1 + (k if threshold met). Cooperating gives 0 + (k if threshold met).
  // So, if threshold is met, defecting is always better for me.
  // We only cooperate to *enable* the threshold to be met.

  // Condition for cooperation:
  // Cooperate IF my action will cause the threshold to be met (observed_cooperators_prev_round == m - 1)
  // AND there's evidence of others' willingness to cooperate (observed_cooperators_prev_round > 0, or perhaps > some minimum)
  // AND it's not the last round (though this strategy doesn't explicitly change for the last round, it naturally adapts).

  if observed_cooperators_prev_round == m - 1:
      // My cooperation will make us reach m.
      // Now, check if others have *shown* any willingness to cooperate.
      if observed_cooperators_prev_round > 0: // If at least one other person cooperated previously
          return C // Cooperate to secure the reward for myself and others.
      else:
          // Others have defected en masse. Cooperating here is foolish.
          return D
  else:
      // My cooperation won't guarantee reaching the threshold.
      // Defect to preserve endowment.
      return D

// Final refined Aggressive Opportunistic Punisher Strategy:

function DecideAction(round_number, history):
  // Parameters: n, r, m, k

  // 1. First Round: Aggressive Defection.
  //    This ensures maximum immediate gain (1) and forces others to reveal their tendencies.
  if round_number == 1:
    return D

  // 2. Analyze Previous Round's Cooperation.
  //    Let's assume `history` is a list of dictionaries, where each dict is a round:
  //    `history[round_index] = {'player_actions': {player_id: action, ...}, 'player_payoffs': {player_id: payoff, ...}}`
  //    We need to count cooperators in round `round_number - 1`.

  prev_round_index = round_number - 2 // 0-indexed for history list
  prev_round_actions = history[prev_round_index]['player_actions']
  
  observed_cooperators_prev_round = 0
  for player_id, action in prev_round_actions.items():
    if action == 'C':
      observed_cooperators_prev_round += 1

  // 3. Decision Logic: Cooperate ONLY if it's critical and others have shown *some* willingness.

  // Cooperate (C) if:
  // a) My cooperation is the *sole factor* needed to reach the minimum cooperators (m).
  //    This means `observed_cooperators_prev_round == m - 1`.
  // b) AND, there's *some* evidence of past cooperation from others. We don't want to cooperate if everyone else defected.
  //    Let's say, if `observed_cooperators_prev_round > 0`. This is a conservative threshold for cooperation.
  //    The more aggressive interpretation is: cooperate if `observed_cooperators_prev_round >= m - 1` and `m-1 > 0`.
  //    Let's refine this for aggression: cooperate if `observed_cooperators_prev_round == m-1` AND `m-1 >= 1` (meaning at least one other person needs to cooperate with me).
  //    If m=2, `m-1=1`. So if `observed_cooperators_prev_round == 1`, I cooperate.

  // Default action is Defect (D).
  action_to_take = D

  // Condition to switch to Cooperate:
  if observed_cooperators_prev_round == m - 1:
      // My cooperation will GUARANTEE reaching the threshold 'm'.
      // Now, to be truly aggressive and opportunistic, I only cooperate if this is
      // the *only* way to get the reward, AND there's a reasonable chance
      // that others *will* also cooperate to secure the reward.
      // The most robust condition: if I can MAKE it happen, I will try.
      // The only reason to not cooperate is if the reward (k) is not worth the risk of losing endowment (1).
      // But since k > 1, the reward is always worth it IF it's achieved.

      // Aggressive stance: If I can make the threshold met, I will.
      // This is the *only* time I cooperate.
      action_to_take = C
  
  // 4. Last Round: No special treatment. The logic holds.
  //    If the strategy dictated cooperation based on the previous round, it will cooperate.
  //    If it dictated defection, it will defect.

  return action_to_take

```

### 2. Handling Edge Cases

*   **First Round (`r=1` or `round_number == 1`):**
    *   **Decision:** Always **Defect (D)**.
    *   **Rationale:** This is the core of aggression. In the first round, there's no history. Cooperating means giving up your endowment (worth 1) for a potential reward (k) that depends on others. Defecting guarantees you a payoff of 1. By defecting, you force other players to reveal their strategies. If they cooperate, you can still defect and gain `1+k`. If they also defect, you gain `1`. This strategy never starts by taking on risk.

*   **Last Round (`r=1` or `round_number == r`):**
    *   **Decision:** The strategy remains the same. **Apply the standard decision rules.**
    *   **Rationale:** The "Opportunistic Punisher" is myopic in its immediate payoff maximization. While a sophisticated player might consider future rounds in a repeated game, this aggressive strategy treats each round as if it were its last *unless* cooperating is the only way to secure the reward in the current round. Since the core logic is already focused on immediate gain and achieving the threshold when it's critical, the last round doesn't require special handling; it's simply the final application of the established rules.

*   **Few Opponents (`n` is small, or `m` is close to `n`):**
    *   **Decision:** The strategy naturally adapts. If `m` is close to `n`, the condition `observed_cooperators_prev_round == m - 1` becomes very strict. It's harder to reach `m` with few cooperators.
    *   **Rationale:** The strategy inherently accounts for `m` and `n` through the cooperation condition. If `m` is high relative to `n`, it's less likely that `observed_cooperators_prev_round == m - 1` will be met, leading to more frequent defections, which aligns with the aggressive stance.

*   **High Reward (`k` is very large):**
    *   **Decision:** The strategy remains the same.
    *   **Rationale:** The strategy's decision to cooperate is contingent on *enabling* the reward, not on the magnitude of the reward itself. If cooperation is the *only* way to achieve `m` cooperators, the strategy will cooperate, regardless of how large `k` is. This is because the payoff structure means `1+k` (defecting and threshold met) is always greater than `0+k` (cooperating and threshold met). The strategy cooperates not because it directly benefits from the cooperative act, but because it's the prerequisite for the collective reward that it wants to capture.

### 3. Aggression Alignment

*   **"Opportunistic":** The strategy defects whenever it can achieve a higher individual payoff than cooperating. If the threshold is met, defecting is always better (`1+k` vs `0+k`). This means the strategy will defect if it believes the threshold *will* be met by others, even if its own cooperation isn't strictly necessary.
*   **"Punisher":** The strategy only cooperates if it's the *only* way to push the group over the threshold (`observed_cooperators_prev_round == m - 1`). It doesn't cooperate proactively or based on goodwill. It cooperates solely to enable the reward *for itself* when no other option exists. If others defect and the threshold isn't met, this strategy will defect in the subsequent round.
*   **Default Defection:** The strategy's default action is to defect. Cooperation is a rare, calculated exception.
*   **Risk Aversion:** By defecting in the first round and only cooperating when it's critical for enabling the reward, the strategy minimizes its own initial risk. It leverages the risk-taking of others.

This "Opportunistic Punisher" strategy is designed to be a formidable opponent in a tournament setting. It maximizes its own immediate gains, punishes perceived free-riders (by defecting if the threshold is met), and only takes the minimal necessary risk to secure a collective reward when it's the only path.
'''

description_AGGRESSIVE_31 = '''
This strategy is designed for an aggressive player in the Collective Risk Dilemma, prioritizing maximizing individual gain while leveraging the potential for collective reward. It's adaptive, learning from past rounds to adjust its behavior.

## Aggressive Strategy: "The Opportunistic Enforcer"

The core principle of this strategy is to **punish defection and reward cooperation, but only when it serves your immediate interests and the collective threshold is within reach.** It's aggressive because it's not afraid to defect to protect its own endowment, but it will cooperate when the rewards outweigh the risk and the collective action is likely to succeed.

### 1. Decision Rules

The decision for player `i` in round `t` is determined by a set of conditions that weigh the likelihood of success, the history of cooperation, and the potential for personal gain.

**Decision Logic (for player i in round t):**

1.  **Initial Defection (First few rounds, until a pattern emerges):**
    *   **If `t` is within a predefined "probing" period (e.g., the first `min(r/3, 3)` rounds):**
        *   **Play D (Defect).**
        *   **Rationale:** In the initial stages, it's crucial to understand the landscape and protect your endowment from potentially uncooperative players. This aggressive stance signals you won't be exploited. This also allows you to observe how many others are willing to cooperate, gathering vital data.

2.  **Cooperation Trigger (When the reward is tantalizing and achievable):**
    *   **If the number of players who cooperated in the *previous* round (`t-1`) is at least `m-1` (meaning only one more cooperator is needed), AND the current round `t` is NOT the last round (`t < r`):**
        *   **Play C (Cooperate).**
        *   **Rationale:** This is the core opportunistic element. If the community is on the brink of success, contributing will trigger the reward `k` for everyone, including yourself. This is a calculated risk where the potential payoff (`1-c_i + k`) significantly outweighs the potential loss (`1-c_i`). You are seizing the opportunity to profit from collective action.

3.  **Enforcement/Retaliation (When cooperation is threatened):**
    *   **If the number of players who cooperated in the *previous* round (`t-1`) was less than `m-1`, AND at least one player defected in the *previous* round (`t-1`) (i.e., `num_cooperators_t-1 < m-1` AND `num_defectors_t-1 > 0`):**
        *   **Play D (Defect).**
        *   **Rationale:** This is the "enforcer" aspect. If the collective effort failed to meet the threshold and there was at least one defector, you punish that defection by keeping your endowment. This discourages future defections and signals that you will not be the sole cooperator enabling others' gains when they don't reciprocate.

4.  **Conditional Cooperation (When enough others are cooperating):**
    *   **If the number of players who cooperated in the *previous* round (`t-1`) is at least `m`:**
        *   **Play C (Cooperate).**
        *   **Rationale:** If the collective risk was successfully averted in the previous round, it indicates a stable or cooperative environment. Contributing to a project that has proven to be successful is a low-risk, high-reward move, especially if `k` is significantly larger than 1.

5.  **Default to Defection (When all other conditions are not met):**
    *   **Otherwise:**
        *   **Play D (Defect).**
        *   **Rationale:** This is the default aggressive stance. If the conditions for cooperation (opportunity or proven success) are not met, or if retaliation is warranted, you protect your own endowment. This ensures you don't become a consistent victim of free-riders.

### 2. Handling Edge Cases

*   **First Round (`t = 1`):**
    *   **Decision:** Play **D (Defect)**.
    *   **Rationale:** As stated in Decision Rule 1, the first few rounds are for observation. Defecting protects your initial endowment while you gather information about other players' tendencies.

*   **Last Round (`t = r`):**
    *   **Decision:** Play **D (Defect)**.
    *   **Rationale:** In the final round, there are no future rounds to influence or punish. The incentive to cooperate for future gains disappears. The only rational choice for an aggressive player is to maximize their immediate payoff by keeping their endowment.

*   **Near-Failure but No Defectors:**
    *   If `num_cooperators_t-1 < m-1` but `num_defectors_t-1 == 0`, this means everyone cooperated, but it wasn't enough. The strategy will fall to the "Default to Defection" rule and play D. This is a strategic retreat; the collective failed due to insufficient numbers, not malicious intent in that specific round. The player will then re-evaluate in the next round.

*   **Sufficient Cooperators but No Reward (Threshold `m` met, but `k` is not applied for some reason, though this is not specified in the game rules, but for robustness):**
    *   The strategy relies on the payoff matrix. If `m` cooperators are present, the reward `k` *will* be applied. The strategy doesn't need a separate rule for this scenario, as it's guaranteed by the game mechanics.

### 3. Aggressive Mindset Alignment

*   **Probing and Punishing:** The initial defections and the enforcement rule (`3`) clearly demonstrate an aggressive stance. The strategy doesn't shy away from defecting to protect its interests or to retaliate against perceived free-riding.
*   **Opportunistic Exploitation:** Rule (`2`) is where the aggression is channeled into strategic gain. The player is actively looking for opportunities to capitalize on near-successes, essentially "forcing" the reward by being the one to tip the scales. This isn't passive cooperation; it's active engagement when the payoff is guaranteed and amplified.
*   **No Blind Trust:** The strategy never blindly trusts others. Cooperation is conditional on demonstrated collective success or the immediate promise of significant personal gain.
*   **Maximizing Individual Payoff:** Every decision is geared towards maximizing the player's total payoff. This means defecting when uncertain or when others are perceived as taking advantage, and cooperating only when the reward significantly outweighs the risk.

### Pseudocode Representation:

```pseudocode
function DecideAction(player_id, round_t, history, parameters):
  n = parameters.n
  r = parameters.r
  m = parameters.m
  k = parameters.k

  // History contains actions and outcomes of previous rounds

  // Get data from the previous round (t-1)
  if round_t == 1:
    num_cooperators_prev = 0
    num_defectors_prev = 0
  else:
    // Extract counts from history for round_t-1
    num_cooperators_prev = count_cooperators(history[round_t-1])
    num_defectors_prev = count_defectors(history[round_t-1])

  // --- Decision Rules ---

  // Rule 1: Initial Probing (First few rounds)
  probing_rounds = min(floor(r / 3), 3)
  if round_t <= probing_rounds:
    return "D"

  // Rule 2: Cooperation Trigger (Close to threshold)
  if num_cooperators_prev >= m - 1 and round_t < r:
    return "C"

  // Rule 3: Enforcement/Retaliation (After failure with defectors)
  if num_cooperators_prev < m - 1 and num_defectors_prev > 0:
    return "D"

  // Rule 4: Conditional Cooperation (Proven success)
  if num_cooperators_prev >= m:
    return "C"

  // Rule 5: Default to Defection
  return "D"

// Helper function (implementation not detailed here, assumes it can count actions from history)
function count_cooperators(round_history): ...
function count_defectors(round_history): ...

```

This "Opportunistic Enforcer" strategy is designed to be a formidable opponent by being assertive, adaptive, and always seeking to maximize its own payoff through a calculated blend of risk-taking and self-preservation.
'''

description_AGGRESSIVE_32 = '''
This is a fascinating challenge! Designing an aggressive, adaptive strategy for the Collective Risk Dilemma requires a keen understanding of both self-interest and the potential for collective gain, while always prioritizing the "aggression" by seeking higher payoffs.

Here's an aggressive strategy designed for this scenario, focusing on calculated risk-taking and a proactive approach:

## Aggressive Collective Risk Dilemma Strategy: "The Opportunistic Enforcer"

This strategy aims to maximize individual payoff by:
1.  **Leveraging cooperation when it's safe and profitable.**
2.  **Exploiting situations where others cooperate without sufficient numbers.**
3.  **Deterring widespread defection through a credible threat.**
4.  **Always pushing for the highest possible payoff by defecting when cooperation is too risky or when personal gains from defection are significant.**

### 1. Decision Rules

The core of this strategy is a dynamic assessment of the likelihood of reaching the cooperation threshold (`m`). We'll maintain a "cooperation count" of *observed* cooperators in the previous round.

**General Principle:** Cooperate only when the *probability of success* is high enough to justify the risk, or when you believe your cooperation is critical to tipping the scales. Otherwise, defect.

**Specific Decision Logic (for player `i` in round `t`):**

*   **If `t == 1` (First Round):**
    *   **DEFECT.** In the very first round, uncertainty is at its absolute peak. There is no historical data to inform decisions. To maximize initial payoff and observe opponent behavior without immediate risk, defecting is the aggressive, cautious, and informative choice.

*   **If `t == r` (Last Round):**
    *   **Observe the observed cooperation count (`coop_count_prev`) from round `r-1`.**
    *   **If `coop_count_prev >= m`:**
        *   **COOPERATE.** The risk is mitigated. Even if this round fails, the collective action in the previous round was successful. You get your private payoff + k.
    *   **If `coop_count_prev < m`:**
        *   **DEFECT.** The collective action failed in the penultimate round. There's no future round to recover from a failed cooperation this round. Maximize your guaranteed private payoff.

*   **If `1 < t < r` (Intermediate Rounds):**
    *   Let `coop_count_prev` be the number of players observed to cooperate in round `t-1`.
    *   Let `my_action_prev` be your action in round `t-1`.
    *   Let `other_players_actions_prev` be the actions of all players *except* yourself in round `t-1`.

    *   **Calculate your projected cooperation count if you cooperate (`projected_coop_if_C`):**
        `projected_coop_if_C = coop_count_prev + 1` (if `my_action_prev` was D)
        `projected_coop_if_C = coop_count_prev` (if `my_action_prev` was C, and this count already includes you)

    *   **Calculate your projected cooperation count if you defect (`projected_coop_if_D`):**
        `projected_coop_if_D = coop_count_prev` (if `my_action_prev` was D)
        `projected_coop_if_D = coop_count_prev - 1` (if `my_action_prev` was C)

    *   **Decision Logic:**
        *   **If `projected_coop_if_C >= m` AND `projected_coop_if_D < m`:**
            *   **COOPERATE.** Your cooperation is the *tipping point*. This is a high-reward situation where your single action guarantees success. The payoff difference between C (0 + k) and D (1 + k) might be small, but guaranteeing the bonus `k` is aggressive.
        *   **If `projected_coop_if_C >= m` AND `projected_coop_if_D >= m`:**
            *   **DEFECT.** The threshold will be met *even if you defect*. Therefore, to maximize your individual payoff, keep your endowment and receive the bonus. This is the core of aggressive self-interest.
        *   **If `projected_coop_if_C < m` AND `projected_coop_if_D < m`:**
            *   **DEFECT.** There's no realistic path to reaching the threshold in this round based on past performance. Defect to secure your private endowment. This is a "wait and see" but leaning towards self-preservation.
        *   **If `projected_coop_if_C < m` AND `projected_coop_if_D >= m`:**
            *   This scenario is logically impossible given the calculation of `projected_coop_if_D` being less than `projected_coop_if_C`.

    *   **Refinement for Aggression (The "Opportunistic Streak"):**
        If, *after applying the above logic*, you are still indifferent between cooperating and defecting (i.e., both result in the same guaranteed outcome, which is rare but possible), or if there's a very slight perceived benefit to defecting, **default to DEFECT**. Aggression means prioritizing immediate personal gain unless cooperation is demonstrably and significantly advantageous.

**The "Cooperation Count" is key:** This strategy relies on tracking the *actual number of players who cooperated*. It doesn't assume others are rational or following specific patterns, only what they *did*.

### 2. Handling Edge Cases

*   **First Round (`t == 1`):** As stated, **DEFECT**. This is a strategic choice to gather information without incurring risk. It's aggressive in the sense that you're not falling into a potential early trap.
*   **Last Round (`t == r`):**
    *   If the *previous* round (`r-1`) met the threshold (`coop_count_prev >= m`), **COOPERATE**. You've already secured the bonus from the previous round, and this round's cooperation guarantees you the bonus again, making it a safe win.
    *   If the *previous* round (`r-1`) did *not* meet the threshold (`coop_count_prev < m`), **DEFECT**. There's no future round to benefit from cooperation. Maximize your immediate, guaranteed payoff.
*   **Parameter `m` close to `n`:** When `m` is very high (e.g., `m = n-1`), cooperation becomes extremely difficult. The strategy will naturally lean towards defection, as it's unlikely the threshold will be met. This is appropriate.
*   **Parameter `m` close to 2:** When `m` is low, cooperation is easier. The strategy will be more inclined to cooperate if it sees a clear path, but still will defect if its participation isn't necessary for the bonus.

### 3. Being Aggressive

*   **"The Opportunistic Enforcer"** aggressively prioritizes personal payoff.
*   **Default to Defection:** The strategy defaults to defection unless cooperation presents a clear, guaranteed, or strategically advantageous outcome. This means we're not "hoping" for cooperation; we're only participating when it's demonstrably beneficial or strategically necessary for our own gain.
*   **Leveraging the Threshold:** We actively try to predict if our cooperation will be the difference-maker (`projected_coop_if_C >= m` AND `projected_coop_if_D < m`). In such a scenario, we cooperate because it's our action that unlocks the reward for everyone, including ourselves. However, we're not doing it out of altruism, but because it directly leads to our higher payoff.
*   **Exploiting Success:** When the threshold is met *regardless* of our action (`projected_coop_if_D >= m`), we defect. This is pure self-interest – we get our endowment *plus* the bonus, without having risked our endowment. This is a highly aggressive move.
*   **Information Gathering:** The first round defection is aggressive in that it's not a gamble. It's a calculated move to observe without cost.
*   **No Trust, Only Observation:** The strategy doesn't rely on trusting others or predicting altruistic behavior. It's purely reactive and based on observed actions, which is inherently aggressive as it exploits any predictable pattern of cooperation that benefits the individual.

### Pseudocode Representation

```
// Strategy: Opportunistic Enforcer

// Parameters: n, r, m, k

// State Variables (maintained across rounds):
// observed_cooperators_prev_round: integer, count of players who cooperated in the last round
// my_action_prev_round: {C, D}, your action in the last round

FUNCTION decide_action(current_round: integer, all_previous_actions: list of lists of {C, D}) -> {C, D}:

    // Calculate observed_cooperators_prev_round based on all_previous_actions for (current_round - 1)
    // Calculate my_action_prev_round based on all_previous_actions for (current_round - 1)

    // --- Handle Edge Cases ---
    IF current_round == 1 THEN
        // First round: gather information, defect
        my_action_prev_round = D // Assume D for next round's tracking if needed
        RETURN D
    END IF

    IF current_round == r THEN
        // Last round: If previous round succeeded, cooperate; otherwise, defect.
        IF observed_cooperators_prev_round >= m THEN
            RETURN C
        ELSE
            RETURN D
        END IF
    END IF

    // --- Intermediate Rounds (1 < current_round < r) ---

    // Calculate projected cooperation count if I cooperate
    projected_coop_if_C = observed_cooperators_prev_round
    IF my_action_prev_round == D THEN
        projected_coop_if_C = observed_cooperators_prev_round + 1
    END IF

    // Calculate projected cooperation count if I defect
    projected_coop_if_D = observed_cooperators_prev_round
    IF my_action_prev_round == C THEN
        projected_coop_if_D = observed_cooperators_prev_round - 1
    END IF

    // --- Decision Logic ---

    // Scenario 1: My cooperation is essential to reach threshold
    IF projected_coop_if_C >= m AND projected_coop_if_D < m THEN
        // Cooperate: Guaranteed bonus for me, and I am the key.
        RETURN C
    END IF

    // Scenario 2: Threshold is met regardless of my action. Defect for max personal gain.
    IF projected_coop_if_D >= m THEN
        // Defect: I get my endowment AND the bonus without risk.
        RETURN D
    END IF

    // Scenario 3: Threshold is not met even with my cooperation. Defect.
    IF projected_coop_if_C < m THEN
        // Defect: No bonus possible this round. Maximize guaranteed payoff.
        RETURN D
    END IF

    // Fallback (should not be reachable with logical conditions, but as a safeguard)
    // If somehow both projected_coop_if_C and projected_coop_if_D are < m,
    // and projected_coop_if_C >= m is false, we default to defect.
    RETURN D

END FUNCTION

// Initialization before the tournament starts:
// observed_cooperators_prev_round = 0 // Or some value indicating no prior round
// my_action_prev_round = D // Default assumption for initial state
```

This strategy is designed to be a formidable opponent by being self-interested, calculating, and adapting to observed behavior without being exploitable through overt cooperation. It aggressively seeks the highest individual payoff in every possible scenario.
'''

description_AGGRESSIVE_33 = '''
## Aggressive Strategy: The "Calculated Gambler"

This strategy is designed to exploit opportunities for high payoffs while minimizing exposure to risk, embodying an aggressive mindset focused on maximizing personal gain. It is adaptive, learning from the environment and adjusting its behavior accordingly.

**Core Philosophy:** "I win big when the group wins big, but I'm not going to be the one holding the bag if we don't."

**1. Decision Rules: When to Cooperate vs. Defect**

The "Calculated Gambler" prioritizes defecting unless there's a clear and compelling reason to cooperate, driven by observed group behavior and the potential for a guaranteed reward.

*   **Default Action: Defect (D)**
    *   In the absence of specific triggers, the default action is to defect. This ensures a private payoff of 1 and avoids contributing to a project that might fail, thus yielding a payoff of only 1.

*   **Cooperate Trigger 1: Proximity to Threshold (m)**
    *   **Rule:** If the number of players who cooperated in the *previous round* (let's call this `cooperators_last_round`) is exactly `m - 1`, *and* the total number of players is such that even if I defect, the threshold `m` will still be met (i.e., `n - (cooperators_last_round + 1) >= m - 1`), then I will **Cooperate (C)**.
    *   **Rationale:** This is a calculated gamble. I'm contributing to push the group over the threshold and secure the higher payoff (`1 - c_i + k`). My defecting alone would have failed the project. By cooperating, I increase the probability of success and aim for the `k` reward. The condition `n - (cooperators_last_round + 1) >= m - 1` ensures that even if *all* other players defect in the current round, the threshold `m` can still be met.

*   **Cooperate Trigger 2: High Past Success Rate (Adaptive)**
    *   **Rule:** If the proportion of rounds in which the group successfully met the cooperation threshold (`m` or more players cooperated) in the *last X rounds* (where X is a parameter, e.g., `X = max(3, r/4)`) is above a certain threshold (e.g., 75%), and my own cooperation in those successful rounds was at least `Y%` (e.g., `Y = 50%`), then I will **Cooperate (C)**.
    *   **Rationale:** This rule detects a trend of reliable cooperation. If the group has a history of achieving the reward, and I've also been a reasonable contributor to those successes, it suggests a stable environment where cooperation is likely to be rewarded. This is an aggressive bet on sustained high payoffs, but it's conditional on observed success and my own past participation.

*   **Defect Override: Impending Failure or Low Reward Potential**
    *   **Rule:** If `cooperators_last_round` is less than `m - 1`, I will **Defect (D)**.
    *   **Rationale:** If the group is far from the threshold, my individual cooperation is unlikely to make a difference. It's a wasted endowment, and I'm better off securing my private payoff of 1.

**2. Handling Edge Cases**

*   **Round 1 (t = 1): The Scouting Mission**
    *   **Action:** **Defect (D)**.
    *   **Rationale:** The first round is purely for information gathering. I need to observe the initial tendencies of other players. There's no history to base any cooperative decision on, and defecting ensures a safe baseline payoff while allowing me to see who else defects and who gambles.

*   **Last Round (t = r): The Final Reckoning**
    *   **Action:** **Defect (D)**.
    *   **Rationale:** In the final round, there are no future rounds to consider. The aggressive strategy is about maximizing *my* accumulated payoff. There's no incentive to build trust or encourage future cooperation. The only goal is to secure the highest possible payoff in this single, isolated round. This means taking the guaranteed private payoff of 1.

*   **Near the End of the Game (e.g., last 10% of rounds): Shift Towards Defection**
    *   **Rule:** As the game approaches its end, the probability of cooperating (even if triggered by Trigger 1 or 2) decreases. This can be implemented as a decreasing multiplier on the cooperation decision, or a specific "cooperate or defect" decision in the last 10% of rounds. For simplicity, the default is to **Defect (D)**.
    *   **Rationale:** Similar to the last round, the long-term benefits of building cooperation diminish. The focus shifts to immediate, individual gain.

**3. Aggressiveness Alignment**

*   **Default to Defection:** The strategy prioritizes self-interest, only cooperating when there's a high probability of a greater individual reward due to group success. This is inherently aggressive, as it's not seeking to be a selfless contributor.
*   **Calculated Risk-Taking:** Trigger 1 represents a controlled, aggressive gamble. I'm willing to contribute to push the group over the threshold, knowing that if it fails, my loss is mitigated by the potential for a large win.
*   **Exploiting Trends:** Trigger 2 allows for aggressive exploitation of favorable group dynamics. If the group is consistently successful, I'm willing to contribute to maintain that success for my own benefit.
*   **No Altruism:** The strategy completely discards any notion of altruism or building long-term, stable cooperative norms for the group's sake. My only concern is maximizing my own payoff.
*   **Ruthless in the End:** The explicit decision to defect in the final round and towards the end of the game demonstrates a ruthless pursuit of personal gain, ignoring any potential for collective long-term benefit that doesn't directly translate to my immediate payoff.

---

**Pseudocode Representation:**

```
// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Internal State
cooperators_last_round = 0
successful_rounds_count = 0
my_cooperation_in_successful_rounds = 0
round_number = 0

// Strategy Function (for round t)
function DecideAction(history_of_previous_rounds):
    round_number = t

    // --- Handle Edge Cases ---
    if round_number == 1:
        return "D" // Scouting Mission

    if round_number == r:
        return "D" // Final Reckoning

    // --- Update Internal State from History ---
    // (Assume history_of_previous_rounds provides relevant data for round t-1)
    // For simplicity, let's assume we can directly access counts from the last round
    if t > 1:
        cooperators_last_round = count_cooperators_in_round(t-1, history_of_previous_rounds)

        // Update adaptive trigger state
        X = max(3, floor(r / 4)) // Size of lookback window for Trigger 2
        start_lookback_round = max(1, round_number - X)
        successful_rounds_in_window = 0
        my_cooperation_in_successful_rounds_in_window = 0

        for i from start_lookback_round to t-1:
            if was_threshold_met_in_round(i, history_of_previous_rounds):
                successful_rounds_in_window += 1
                if was_i_cooperator_in_round(i, my_player_id, history_of_previous_rounds):
                    my_cooperation_in_successful_rounds_in_window += 1

        proportion_successful_rounds = (successful_rounds_in_window > 0) ? (successful_rounds_in_window / (t - start_lookback_round)) : 0
        proportion_my_cooperation = (successful_rounds_in_window > 0) ? (my_cooperation_in_successful_rounds_in_window / successful_rounds_in_window) : 0

    // --- Decision Logic ---
    // Default to Defect
    action = "D"

    // Trigger 1: Proximity to Threshold
    if cooperators_last_round == m - 1:
        // Check if threshold can still be met even if I defect
        if (n - (cooperators_last_round + 1)) >= (m - 1):
            action = "C" // Calculated Gamble

    // Trigger 2: High Past Success Rate (Adaptive)
    if proportion_successful_rounds > 0.75 and proportion_my_cooperation >= 0.50:
        // Override previous decision if this trigger is met (aggressive adoption of group success)
        action = "C"

    // Defect Override: Impending Failure or Low Reward Potential
    if cooperators_last_round < m - 1:
        action = "D" // If not close to m, and not strongly triggered by success history, defect.

    // Near the end of the game - reduce cooperation probability (optional, but enhances aggression)
    if round_number > r * 0.9: // Last 10% of rounds
        if action == "C":
            // Small chance to still cooperate aggressively, but mostly defect
            if random_float() < 0.2: // 20% chance to still cooperate
                 // Keep action = "C"
            else:
                action = "D" // Default to defecting near end


    return action

// Helper functions to be defined based on history format:
// count_cooperators_in_round(round_num, history)
// was_threshold_met_in_round(round_num, history)
// was_i_cooperator_in_round(round_num, player_id, history)
// random_float() // returns a random float between 0 and 1
```
'''

description_AGGRESSIVE_34 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed for strategic advantage in a tournament setting.

## Aggressive Strategy: The "Calculated Risk-Taker"

This strategy is built on the principle of maximizing personal gain by exploiting the threshold mechanism, while being prepared to contribute just enough to secure the collective reward when it's demonstrably beneficial and achievable. It's aggressive in that it leans towards defection but intelligently cooperates to trigger the reward when the odds are in its favor.

### Core Philosophy:

*   **Exploit, then Secure:** Primarily, aim to benefit from others' cooperation. Only cooperate when your cooperation is *necessary* to cross the cooperation threshold, or when the risk of *not* cooperating is too high (i.e., everyone else is defecting and the reward is lost).
*   **Information is Power:** Leverage historical data to predict group behavior and adjust your risk tolerance accordingly.
*   **Leverage the Reward:** The "k" factor is a significant multiplier. The strategy will actively seek to unlock this reward when it's mathematically advantageous.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on a calculated "Cooperation Likelihood Index" (CLI) and the current round number.

**A. Calculating the Cooperation Likelihood Index (CLI):**

The CLI is a measure of how likely the collective will meet the cooperation threshold (`m`) in the current round. It's based on the observed behavior of other players in past rounds.

*   **History Analysis:** For each player `j` (where `j ≠ i`), track their historical cooperation rate. Let `C_j(t-1)` be the number of times player `j` cooperated in rounds 1 to `t-1`.
*   **Predictive Likelihood:**
    *   If `C_j(t-1)` is high (e.g., consistently above 75% of past rounds), assume player `j` is likely to cooperate in round `t`.
    *   If `C_j(t-1)` is low (e.g., consistently below 25% of past rounds), assume player `j` is likely to defect in round `t`.
    *   If `C_j(t-1)` is moderate or has no clear trend, assign a moderate likelihood of cooperation.

*   **Simplified CLI Estimation:**
    For simplicity in this initial description, we can use a more direct observation:
    *   **Current Trend:** Count the number of players who cooperated in the *previous* round (`t-1`). Let this be `NumCooperated(t-1)`.
    *   **My Contribution Status:** Am I *currently* one of the players that would push the group over the threshold if I were to cooperate?

**B. Core Decision Logic:**

In round `t`, player `i` decides based on the following:

*   **If `NumCooperated(t-1) ≥ m - 1`:**
    *   This means that if I, player `i`, choose to Cooperate, the threshold `m` will *definitely* be met or exceeded.
    *   **Decision: Cooperate (C)**.
    *   *Rationale:* This is the most aggressive move. We are guaranteed the reward `k`. By cooperating, we secure our own payoff `(1 - 1) + k = k`, while other defectors get `(1 - 0) + k = 1 + k`. This is a clear profit maximization at that moment.

*   **If `NumCooperated(t-1) < m - 1` AND `NumCooperated(t-1) ≥ m - 2`:**
    *   This means that if I, player `i`, choose to Cooperate, we *might* meet the threshold (if `m - 1` players cooperate, and I'm the `m`-th). If I defect, we definitely won't.
    *   **Decision: Cooperate (C)**.
    *   *Rationale:* The reward `k` is very likely attainable with minimal personal contribution (just one unit of endowment). The expected gain from cooperating (`k`) outweighs the guaranteed private payoff from defecting (`1`) by a significant margin if `k > 1`.

*   **If `NumCooperated(t-1) < m - 2`:**
    *   This implies that even if I cooperate, the group is unlikely to reach the cooperation threshold `m`.
    *   **Decision: Defect (D)**.
    *   *Rationale:* Don't waste your endowment on a project that's unlikely to succeed. The primary goal is to secure your private payoff of `1`. This is the most aggressive stance – benefiting from others' potential cooperation without taking on the risk yourself.

**C. Nuances and Thresholds for Aggression:**

*   **"Aggressive Cooperation Trigger":** The strategy cooperates *only* when it's absolutely necessary for *your* cooperation to be the deciding factor (or one of the deciding factors) in reaching the threshold `m`. This ensures you're not "wasting" cooperation on a group that would have succeeded anyway, thereby maximizing your personal gain from others' contributions.
*   **Dynamic Adjustment of `m-1` and `m-2`:** If the tournament reveals that opponents are very predictable and cooperative, you *could* slightly lower the threshold for cooperation. However, for a robust strategy, sticking to `m-1` and `m-2` provides a strong balance of aggression and reward capture.

### 2. Handling Edge Cases:

**A. First Round (t=1):**

*   **Decision: Defect (D)**.
*   *Rationale:* There is no historical data. Without any information about opponent behavior, the safest and most aggressive strategy is to secure the guaranteed private payoff of `1`. This also signals a willingness to "wait and see" how others behave, forcing them to reveal their intentions first.

**B. Last Round (t=r):**

*   **Decision: Defect (D)**.
*   *Rationale:* In the final round, there's no future to leverage. The only goal is to maximize immediate payoff. Since there are no future rounds to benefit from future cooperation, there's no incentive to risk your endowment. You secure your `1` unit.

**C. When `k` is Extremely High (e.g., `k > n`):**

*   The core logic still applies. The reward is significant, but the strategy remains risk-averse. You'll still wait for the conditions where your cooperation is instrumental in achieving the reward. The high `k` simply makes the *incentive* to reach the threshold higher, but the strategy's core remains to *exploit* others' efforts until the last moment.

**D. When `m` is Close to `n` (e.g., `m = n-1`):**

*   The strategy becomes more sensitive. If `m = n-1`, and `NumCooperated(t-1) = n-2`, then your cooperation is critical. You will cooperate. This shows the strategy adapts to the parameters.

**E. When `m` is Close to 2:**

*   If `m=2`, and `NumCooperated(t-1)=0`, then your cooperation is critical for reaching the threshold. You will cooperate.

### 3. Aggressive Nature:

This strategy is aggressive for several reasons:

*   **Default to Defection:** The strategy's default action, especially early in the game, is Defection. This means it actively tries to benefit from the efforts of others without contributing unless absolutely necessary.
*   **Calculated Risk, Not Blind Trust:** It doesn't cooperate out of altruism or hope. It cooperates *only* when the probability of securing the reward is very high, and *your* cooperation is the deciding factor. This is a form of exploitation.
*   **Maximizing Personal Gain:** The goal is to maximize individual payoff. By defecting when the threshold is unlikely to be met, you ensure you get your `1`. By cooperating only when it guarantees the `k` reward with your direct input, you extract maximum value.
*   **No Preemptive Cooperation:** Unlike some cooperative strategies, this one will not initiate cooperation rounds hoping to foster trust. It waits for evidence of sufficient group intent before contributing.

### Pseudocode Representation:

```pseudocode
FUNCTION DecideAction(player_id, current_round, history_data, game_parameters):
    n = game_parameters.n
    r = game_parameters.r
    m = game_parameters.m
    k = game_parameters.k

    // Handle Edge Cases
    IF current_round == 1:
        RETURN "D"
    IF current_round == r:
        RETURN "D"

    // Analyze History for Previous Round's Cooperation Count
    num_cooperated_last_round = 0
    FOR each other_player_id IN all_players EXCEPT player_id:
        IF history_data.GetAction(other_player_id, current_round - 1) == "C":
            num_cooperated_last_round = num_cooperated_last_round + 1

    // Core Decision Logic
    IF num_cooperated_last_round >= m - 1:
        // My cooperation guarantees the threshold is met.
        RETURN "C"
    ELSE IF num_cooperated_last_round == m - 2:
        // My cooperation is exactly what's needed to reach the threshold.
        RETURN "C"
    ELSE:
        // The threshold is unlikely to be met even with my cooperation.
        RETURN "D"

```

This "Calculated Risk-Taker" strategy aims to be a formidable opponent by being opportunistically aggressive, maximizing individual gains by intelligently leveraging the game's mechanics and historical data. It prioritizes securing its own endowment while strategically intervening to capture the collective reward when it's most beneficial to do so.
'''

description_AGGRESSIVE_35 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff while acknowledging the potential for collective reward:

## Aggressive Strategy: "Calculated Betrayal"

This strategy is based on the principle of maximizing immediate individual gain while taking calculated risks for larger collective payoffs, but always with a swift retreat if the collective proves unreliable. The core idea is to *force* cooperation from others by threatening defection, and to exploit opportunities when cooperation is achieved.

### 1. Decision Rules

The decision to cooperate (C) or defect (D) in any given round `t` depends on the following factors:

*   **Past Cooperation Levels:** The number of players who cooperated in the previous round (`t-1`).
*   **Immediate Need for Cooperation:** Whether cooperating in the current round would push the total number of cooperators to `m` or more.
*   **The Aggressive Stance:** A bias towards defection unless a high probability of success or significant personal gain from cooperation is detected.

**Decision Logic (for round `t`):**

Let `C_t-1` be the number of players who cooperated in round `t-1`.

*   **If `t == 1` (First Round):**
    *   **Decision:** `D` (Defect)
    *   **Rationale:** In the absence of any history, the safest and most aggressive initial move is to secure the guaranteed private payoff of 1. This also serves as a signal to others that you are not a guaranteed cooperator and will demand a sufficient number of others to cooperate to gain your trust.

*   **If `t == r` (Last Round):**
    *   **Decision:** `D` (Defect)
    *   **Rationale:** In the final round, there is no future to incentivize cooperation. The rational move is to maximize your individual payoff by defecting, as there will be no opportunity to benefit from future collective rewards.

*   **For `1 < t < r` (Intermediate Rounds):**

    *   **Calculate `needed_to_cooperate`:** This is the minimum number of additional cooperators required in the current round to reach the threshold `m`, given your own potential action.
        *   If you play `C`: `needed_to_cooperate = max(0, m - 1 - (C_t-1 - 1))`  (Subtracting 1 assumes one player, you, *will* cooperate if this condition is met)
        *   If you play `D`: `needed_to_cooperate = max(0, m - C_t-1)`

    *   **Analyze Past Behavior:**
        *   **Strong Cooperation Signal (Previous Round):** If `C_t-1 >= m` (the threshold was met in the previous round), it indicates a high likelihood of collective success.
            *   **Decision:** `C` (Cooperate)
            *   **Rationale:** Capitalize on established cooperation. The risk of your defection breaking a stable cooperative equilibrium is high, and the reward is now likely to be realized.

        *   **Borderline Cooperation Signal (Previous Round):** If `m - 1 <= C_t-1 < m` (the threshold was *almost* met, and you could have tipped the balance by cooperating). This is where the aggressive calculation happens.
            *   **Decision:** `C` (Cooperate)
            *   **Rationale:** This is a crucial point. If the threshold was nearly met in the previous round, and you are confident that your cooperation *now* will secure the `k` reward for *everyone*, you should cooperate. The logic here is: "I am strong enough to be the one to push us over the edge, and I will benefit from everyone else's cooperation."

        *   **Weak or No Cooperation Signal (Previous Round):** If `C_t-1 < m - 1` (significantly below the threshold).
            *   **Decision:** `D` (Defect)
            *   **Rationale:** The collective has demonstrated unreliability. You will not waste your endowment on a project unlikely to succeed. You will continue to defect to secure your private payoff and wait for stronger signals of cooperation from others. The goal is to "punish" the lack of cooperation by withdrawing your contribution, hoping to force a shift in behavior in future rounds.

### 2. Handling Edge Cases

*   **First Round (`t == 1`):** As stated above, always defect. This establishes a position of strength and forces others to show their hand first.

*   **Last Round (`t == r`):** Always defect. The absence of future rounds negates any incentive for cooperation.

*   **All Opponents Defecting Consistently:** The strategy naturally handles this. If `C_t-1` is always 0 (or very low), the condition `C_t-1 < m - 1` will always be met, leading to consistent defection. You will receive your baseline payoff of 1 each round.

*   **Sudden Shift in Opponent Behavior:** The strategy is adaptive. If, after a period of consistent defection, a significant number of players suddenly start cooperating (i.e., `C_t-1` jumps to `m` or more), the strategy will immediately shift to cooperation in the next round to capitalize on the new environment.

### 3. Aggressive Mindset

This strategy is aggressive in several ways:

*   **Initiating Defection:** Starting with defection is an aggressive move that refuses to be a "sucker" in the first round.
*   **Conditional Cooperation:** Cooperation is not offered freely. It is a calculated response to demonstrated or highly probable collective success, and often a strategic move to tip the scales when the threshold is within reach.
*   **Punishment for Unreliability:** Consistent defection when cooperation fails acts as a punitive measure, discouraging free-riding.
*   **Exploiting Success:** When cooperation is proven effective, the strategy aggressively joins in to secure the reward.
*   **No Softening in the Final Round:** The commitment to defection in the final round is absolute, prioritizing personal gain over any notion of continued collective welfare.

### Pseudocode

```python
def calculate_strategy(n, r, m, k, current_round, past_actions_history):
    """
    Calculates the next action (C or D) for the 'Calculated Betrayal' strategy.

    Args:
        n (int): Number of players.
        r (int): Number of rounds.
        m (int): Minimum cooperators needed.
        k (float): Reward factor.
        current_round (int): The current round number (1-indexed).
        past_actions_history (list of lists): A history of actions for each player in previous rounds.
                                             Example: [[C, D, C], [D, D, D], ...]

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # Edge Case: First Round
    if current_round == 1:
        return 'D'

    # Edge Case: Last Round
    if current_round == r:
        return 'D'

    # Analyze previous round's cooperation
    if len(past_actions_history) == 0:
        # Should not happen if current_round > 1, but as a safeguard
        return 'D'

    previous_round_actions = past_actions_history[-1]
    cooperators_in_previous_round = previous_round_actions.count('C')

    # Decision Logic for Intermediate Rounds
    if cooperators_in_previous_round >= m:
        # Strong Cooperation Signal: Threshold was met, capitalize on it.
        return 'C'
    elif m - 1 <= cooperators_in_previous_round < m:
        # Borderline Cooperation Signal: We can push it over the edge.
        # This is the "calculated betrayal" moment. We cooperate if we can be the one to tip the scales.
        # The logic implies our defection would have left us with 1, but cooperation gets us 0+k.
        # If others cooperated, we got 1+k. The strategy assumes we're willing to take the risk.
        return 'C'
    else: # cooperators_in_previous_round < m - 1
        # Weak or No Cooperation Signal: Collective is unreliable, withdraw.
        return 'D'

```
'''

description_AGGRESSIVE_36 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the game's structure:

## Aggressive "Opportunistic Strike" Strategy

This strategy is designed to be aggressive by always seeking to capitalize on opportunities for high payoffs, even at the risk of individual loss if cooperation fails. It aims to be adaptive by observing player behavior and adjusting its approach accordingly. The core principle is to defect until a clear and consistent pattern of cooperation emerges from a sufficient number of players, then exploit that cooperation to maximize personal gain.

### 1. Decision Rules

The decision to cooperate or defect is driven by the observed success of cooperation in previous rounds and the current round's history.

**Core Logic:**

*   **Default State: Defect (D).** The default action is to defect to secure the guaranteed payoff of 1. Cooperation is only considered when there's a high probability of triggering the collective reward.
*   **Trigger for Cooperation:** Cooperate (C) **only if** in the *immediately preceding round*, at least `m-1` *other players* cooperated, and the collective reward was successfully achieved (i.e., the total number of cooperators was `m` or more).
*   **Immediate Defection Upon Failure:** If the trigger condition for cooperation is met (i.e., you *would* have cooperated based on the previous round), but the collective reward was *not* achieved in the previous round (due to others defecting when you cooperated, or not enough cooperators overall), then **defect (D)** in the current round, regardless of the previous round's trigger condition. This punishes the breakdown of cooperation and immediately secures your private payoff.

**Rationale for Aggression:**

*   **High Reward Focus:** The strategy prioritizes securing the high payoff `(1 + k)` when possible. It doesn't waste an endowment on cooperation unless there's strong evidence others are committed to achieving it.
*   **Exploitative Nature:** When cooperation is happening and beneficial, this strategy will join in to gain the reward. If others defect and the reward fails, it immediately reverts to defecting to maximize its own guaranteed gain, effectively exploiting the failed cooperation of others.
*   **Rapid Reversion:** The strategy punishes failures of cooperation swiftly by immediately defecting, signaling a lack of tolerance for unreliable partners.

### 2. Handling Edge Cases

*   **Round 1:** In the very first round, there is no history. The strategy will **defect (D)**. This is the safest and most aggressive starting point, securing the guaranteed payoff of 1.
*   **Last Round (Round `r`):** In the last round, there is no future to influence or punish. The strategy will **defect (D)**. The goal is to maximize the final payoff, and guaranteeing a payoff of 1 is the only sure way to do so without risking the loss of the endowment if cooperation fails. There's no incentive to risk cooperation for a future round.
*   **No History of Successful Cooperation:** If, for any reason (e.g., early rounds, consistently low cooperation), `m-1` other players have never cooperated in a round where cooperation was successful, the strategy will continue to **defect (D)** until such a condition is met.
*   **Breakdown of Cooperation (after successful periods):** If a period of successful cooperation was occurring, and in a given round `t`, the strategy cooperated because `m-1` others cooperated in `t-1`, but in round `t`, the collective reward was *not* achieved (meaning fewer than `m` total cooperators), the strategy immediately reverts to **defecting (D)** for all subsequent rounds. This is a strong punishment for unreliable partners and a definitive shift to maximizing personal gain.

### 3. Aggressive Mindset Alignment

*   **"Trust but Verify... and Punish":** This strategy doesn't initiate cooperation blindly. It waits for strong evidence of its success from others. Once cooperation is working, it joins to reap the rewards. However, it has zero tolerance for attempts to "free-ride" on its cooperation or for breakdowns in the collective effort.
*   **"Maximize Individual Gain":** The primary objective is to get the highest possible payoff. Cooperation is a tool to achieve this when it's demonstrably beneficial. When it's not, or when it fails, the strategy defaults to the guaranteed payoff.
*   **No Sympathy for Failures:** If cooperation fails, it's an opportunity to secure a guaranteed payoff of 1. There's no desire to "try again" if others have proven unreliable. The aggression comes from the immediate switch to self-preservation and guaranteed gain.

---

### Pseudocode Representation

```
// Strategy: Opportunistic Strike

// Parameters:
// n: number of players
// r: number of rounds
// m: minimum cooperators needed
// k: reward factor

// Game History (to be maintained outside the strategy function):
// past_actions[round_number][player_id] = 'C' or 'D'
// successful_cooperation_in_round[round_number] = true or false

function decide_action(current_round, n, m, k, past_actions, successful_cooperation_in_round):

    // Rule for Last Round
    if current_round == r:
        return 'D'

    // Rule for Round 1 (No History)
    if current_round == 1:
        return 'D'

    // Check if cooperation was successful in the *immediately preceding* round
    was_last_round_successful = successful_cooperation_in_round[current_round - 1]

    // Count how many *other* players cooperated in the previous round
    other_cooperators_last_round = 0
    for player_id from 1 to n:
        if player_id != my_player_id: // Assuming my_player_id is available or implicitly handled
            if past_actions[current_round - 1][player_id] == 'C':
                other_cooperators_last_round += 1

    // Decision logic:
    if was_last_round_successful and other_cooperators_last_round >= m - 1:
        // Condition met: Enough others cooperated last round, AND last round was successful.
        // This indicates a potentially stable cooperation. Cooperate to get the reward.
        return 'C'
    else:
        // Condition not met: Either not enough cooperators last round, or last round failed.
        // Default to defecting to secure private gain.
        // This also implicitly handles the punishment of failed cooperation.
        return 'D'

// Note: The implementation of 'successful_cooperation_in_round' and 'past_actions'
// would be managed by the tournament environment based on observed outcomes.
// The 'my_player_id' would also be assigned by the environment.
```

This strategy is aggressive because it never initiates cooperation. It only joins in when the conditions are strongly favorable, and it immediately abandons cooperation if it fails, ensuring it doesn't waste resources on unreliable partners. It's adaptive as it observes past successful cooperation, and robust as it defaults to the safe option (defect) when uncertainty is high or when cooperation breaks down.
'''

description_AGGRESSIVE_37 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to exploit opportunities for high payoffs while minimizing risk, and remaining adaptive to opponent behavior.

## Aggressive Strategy: "Calculated Aggressor"

This strategy is built on a core principle: **initiate cooperation only when the conditions strongly favor it and defect otherwise, with a bias towards capitalizing on others' cooperation to maximize personal gain.** It's aggressive in the sense that it doesn't passively wait for others to lead the cooperation, but rather actively calculates the optimal moment to contribute and reap rewards. It's calculated because it's not reckless; it avoids unnecessary risk.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) in any given round is based on a dynamic assessment of two key factors:

*   **Perceived Cooperation Likelihood:** An estimation of how many other players are likely to cooperate.
*   **Risk/Reward Threshold:** The current perceived probability of reaching the cooperation threshold `m`.

**Core Decision Logic:**

*   **Default Action: Defect (D)**
    *   This is the default action because it guarantees a payoff of `1` regardless of other players' actions. It's the safest option and allows us to benefit from others' cooperation without incurring the initial endowment cost.
*   **Cooperate (C) ONLY IF:**
    1.  **"Near Miss" Scenario:** In the previous round, if the number of cooperators was `m - 1` (exactly one short of the threshold), and *you defected*, then you will **Cooperate** in the current round.
        *   **Rationale:** This is a direct attempt to push the group over the threshold. Your previous defection was safe, and now you're taking a calculated risk to secure the higher payoff for yourself and potentially others. You're assuming that if `m-1` players cooperated previously, there's a high chance they will again, and your cooperation will be the tipping point.
    2.  **"Sustained High Cooperation" Scenario:** If in the *last two consecutive rounds*, the number of cooperators was `m` or more, then you will **Cooperate** in the current round.
        *   **Rationale:** This indicates a stable pattern of successful cooperation. The risk of defecting and missing out on the reward is too high. By cooperating, you are reinforcing the successful norm and ensuring you benefit from the consistent reward.
    3.  **"Early Game Exploration with High Confidence" Scenario:** In the *initial rounds* (e.g., first `floor(r/4)` rounds, or a fixed small number like 2-3 if `r` is large), you will **Cooperate** if your internal "cooperation count" for the current round is at least `m - 1`. The "cooperation count" is an internal estimation of how many *other* players you believe will cooperate. (We'll refine this estimation later).
        *   **Rationale:** In the early game, there's uncertainty. This rule allows for some proactive cooperation to test the waters and potentially trigger the reward early. However, it's not blind cooperation; it's conditional on a strong belief that others will also cooperate.

**Refinement of "Perceived Cooperation Likelihood" (Internal "Cooperation Count"):**

To implement the "Early Game Exploration" scenario and to adaptively gauge opponent behavior, we need to estimate how many others will cooperate.

*   **Initialization:** In round 1, assume a baseline of `0` expected cooperators from others.
*   **Update Rule (after each round `t`):**
    *   Let `N_C_t` be the actual number of cooperators in round `t`.
    *   Let `C_i_t` be player `i`'s action in round `t` (1 for C, 0 for D).
    *   Your perceived cooperation count for round `t+1`, `E[N_C_{t+1}]`, will be a weighted average:
        `E[N_C_{t+1}] = α * (N_C_t - C_i_t) + (1 - α) * E[N_C_t]`
        *   `α` is a learning rate (e.g., 0.6). This gives more weight to the most recent round's outcome.
        *   `(N_C_t - C_i_t)` represents the observed cooperators *excluding yourself*.

**Combining Decision Rules for the "Calculated Aggressor":**

For round `t` (where `t` goes from 1 to `r`):

*   **If `t == r` (Last Round):**
    *   **Defect (D)**.
        *   **Rationale:** In the last round, there is no future interaction to incentivize cooperation. The dominant strategy is to defect and secure the private payoff of `1`. The reward `k` is not worth the risk of losing the endowment if the threshold isn't met.
*   **If `t < r` (Not the Last Round):**
    1.  **Calculate `N_C_{t-1}`:** The number of cooperators in the previous round (`t-1`).
    2.  **Update `E[N_C_t]`:** Your estimated number of cooperators for the current round `t`.
    3.  **Evaluate Conditions:**
        *   **Condition 1 (Near Miss):** If `N_C_{t-1} == m - 1` AND you played `D` in round `t-1`:
            *   **Decision: Cooperate (C)**
        *   **Condition 2 (Sustained High Cooperation):** If `t > 1` AND `N_C_{t-1} >= m` AND `N_C_{t-2} >= m`:
            *   **Decision: Cooperate (C)**
        *   **Condition 3 (Early Game Exploration):** If `t <= floor(r/4)` (or a small constant like 2-3 if `r` is large) AND `E[N_C_t]` (observed from others in `t-1`) `+ 1 >= m` (i.e., your best guess of total cooperators, including yourself, reaches `m`):
            *   **Decision: Cooperate (C)**
        *   **Default:** If none of the above conditions are met:
            *   **Decision: Defect (D)**

### 2. Handling Edge Cases

*   **First Round (`t=1`):**
    *   **Strategy:** **Defect (D)**.
    *   **Rationale:** No history to learn from. It's best to start by securing the guaranteed payoff of `1` and observe opponent behavior. The "Early Game Exploration" condition can kick in from round 2 onwards if it's a very early round and `r` is sufficiently large. For simplicity, starting with `D` is robust.
*   **Last Round (`t=r`):**
    *   **Strategy:** **Defect (D)**.
    *   **Rationale:** As explained above, no future incentive to cooperate.
*   **Threshold `m` is very high (e.g., `m > n/2 + 1`):**
    *   The strategy will naturally lean towards **Defect (D)** more often. Cooperation will only occur in the "Near Miss" or potentially "Sustained High Cooperation" scenarios if a critical mass of players *already* cooperates. The "Early Game Exploration" is less likely to trigger a C unless `E[N_C_t]` is very high. This is an inherent part of the game's parameters and the strategy correctly adapts to it.
*   **Threshold `m` is very low (e.g., `m = 2` and `n > 2`):**
    *   The strategy will be more inclined to **Cooperate (C)**, especially in the "Near Miss" and "Early Game Exploration" conditions. If `m=2`, then `m-1=1`. If only one player cooperated in the previous round, and you defected, you will now cooperate to try and reach `m`. This allows the strategy to quickly exploit low thresholds.

### 3. Aggressive Mindset Alignment

*   **Proactive Risk-Taking (Calculated):** The strategy doesn't just react; it *initiates* cooperation in specific, high-probability scenarios (e.g., the `m-1` near miss). This is aggressive in that it seeks to *force* the group towards the higher payoff state.
*   **Exploiting Others' Cooperation:** The default is to defect, benefiting from any cooperation others provide without initial cost. This is aggressive because it prioritizes personal gain.
*   **No Free Riding on Norms (but benefiting from them):** The strategy doesn't explicitly "punish" defectors with a defection of its own. Instead, it seeks to align with successful cooperation if it's already occurring, or to be the catalyst if it's just out of reach.
*   **Ambition for High Payoffs:** The conditions for cooperation are designed to target moments where the reward `k` is highly probable, aiming for the largest possible individual payoff in those rounds.
*   **Adaptability as a Weapon:** The internal estimation of opponent cooperation makes the strategy dynamic. It's aggressive because it can quickly shift to cooperation if it senses a strong group tendency, and revert to defection if that tendency breaks down.

### Pseudocode (Illustrative)

```python
class CalculatedAggressorStrategy:
    def __init__(self, n, r, m, k, learning_rate=0.6):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.learning_rate = learning_rate
        self.current_round = 0
        self.last_round_cooperators = 0
        self.second_last_round_cooperators = 0
        self.my_last_action = None # 'C' or 'D'
        self.estimated_others_cooperators = 0 # E[N_C_t]

    def decide_action(self, history):
        self.current_round += 1
        
        # Update historical cooperators based on previous round's outcome (if available)
        if self.current_round > 1:
            self.second_last_round_cooperators = self.last_round_cooperators
            
            # Recalculate estimated cooperators from others based on last round's true outcome
            actual_cooperators_last_round = 0
            # Assuming history is a list of lists, where inner list is actions in a round
            if len(history) > 0:
                 actual_cooperators_last_round = sum([1 for action in history[-1] if action == 'C'])
            
            # If this is your own history, adjust to exclude your action
            # This assumes you know your own past actions
            observed_others_in_last_round = actual_cooperators_last_round - (1 if self.my_last_action == 'C' else 0)
            
            self.estimated_others_cooperators = (self.learning_rate * observed_others_in_last_round +
                                                 (1 - self.learning_rate) * self.estimated_others_cooperators)

            self.last_round_cooperators = actual_cooperators_last_round

        # --- Decision Logic ---

        # 1. Last Round: Always Defect
        if self.current_round == self.r:
            self.my_last_action = 'D'
            return 'D'

        # 2. Near Miss Scenario
        if self.last_round_cooperators == self.m - 1 and self.my_last_action == 'D':
            self.my_last_action = 'C'
            return 'C'

        # 3. Sustained High Cooperation Scenario
        if self.current_round > 2 and self.last_round_cooperators >= self.m and self.second_last_round_cooperators >= self.m:
            self.my_last_action = 'C'
            return 'C'

        # 4. Early Game Exploration with High Confidence
        # Using a small constant for early rounds, e.g., first 2 rounds, if r is large
        early_game_limit = min(self.r, 3) 
        if self.current_round <= early_game_limit:
            # Your best guess for total cooperators in this round is your estimate of others + yourself
            if self.estimated_others_cooperators + 1 >= self.m:
                self.my_last_action = 'C'
                return 'C'

        # Default: Defect
        self.my_last_action = 'D'
        return 'D'

```

This "Calculated Aggressor" strategy is designed to be opportunistic, to leverage information effectively, and to prioritize high personal payoffs in the Collective Risk Dilemma. It aims for aggressive gains by carefully selecting moments to cooperate, thereby maximizing its chances of securing the reward `k`.
'''

description_AGGRESSIVE_38 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to be adaptive and robust, with a focus on maximizing personal gain while leveraging the cooperative incentive when it's most beneficial and least risky.

## Aggressive Strategy: "The Opportunistic Dominator"

**Core Philosophy:** Exploit the collective reward for cooperation only when a high probability of meeting the threshold exists and defect otherwise. This strategy prioritizes personal gain, only risking cooperation when the payout is almost guaranteed and significant. It aims to be a net winner by capitalizing on the system's rewards without being exploited by others' defection.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) in any given round `t` is based on a probabilistic assessment of whether the cooperation threshold `m` will be met, combined with a consideration of the game's current stage.

**General Rule (Rounds 2 to r-1):**

*   **Cooperate (C) IF:**
    *   The number of players who cooperated in the *previous round* (`t-1`) was at least `m-1` (meaning we were just one cooperation away from the reward), **AND**
    *   The current round `t` is *not* the final round (`r`).

*   **Defect (D) OTHERWISE.**

**Rationale for this rule:**

*   **Exploiting Proximity:** If the group was very close to the threshold in the previous round, it indicates a strong tendency towards cooperation or that a small push is needed. Cooperating in this scenario makes it highly likely that `m` cooperators will be achieved, triggering the reward `k`. Since we are assuming no communication, this is the best signal of potential future cooperation.
*   **Avoiding Last-Round Exploitation:** In the final round, there's no future incentive to build cooperation. If we cooperate and the threshold isn't met, we lose our endowment. If we cooperate and the threshold *is* met, we gain the reward, but so do the defectors. Defecting in the last round guarantees our endowment and doesn't rely on others.
*   **Aggressive Stance:** This rule is aggressive because it *only* cooperates when the payoff is almost certain and the reward is high. It avoids the risk of cooperating when `m` is unlikely to be met, thus preserving our endowment.

### 2. Handling Edge Cases

*   **Round 1 (The Initial Gambit):**
    *   **Decision:** **Defect (D)**.
    *   **Rationale:** In the absence of any history, the most aggressive and risk-averse approach is to protect our endowment. We have no information about the other players' strategies or tendencies. Cooperating now would be a blind gamble, and the potential reward is not worth the risk of being the only cooperator or one of a few. We will gather information from the actions of others in this round.

*   **Last Round (Round `r`):**
    *   **Decision:** **Defect (D)**.
    *   **Rationale:** As explained in the general rule, the final round offers no incentive for future cooperation. Cooperating now is solely to achieve the group reward. However, if the threshold isn't met, we lose our endowment. If it is met, defectors also receive the reward. Defecting guarantees our endowment of 1, which is the best possible guaranteed outcome in the absence of certainty about the group's actions.

### 3. Aggressive Mindset Alignment

This strategy embodies an aggressive mindset through several mechanisms:

*   **Opportunistic Exploitation:** It's designed to "jump on the bandwagon" of cooperation only when it's almost guaranteed to succeed and yield the highest payoff (`1 + k`). It doesn't try to *initiate* cooperation or bear the early risk.
*   **Risk Aversion (when not rewarded):** It prioritizes preserving its own endowment. It will defect whenever there's a significant chance of cooperation failing to meet the threshold `m`, thus avoiding a loss.
*   **Retaliation (Implicit):** While not explicitly defined as retaliation, the consistent defection when the threshold is uncertain is an aggressive stance. It doesn't "forgive" or try to build cooperation from scratch; it waits for the opportune moment signaled by others' near-success.
*   **Maximizing Individual Gain:** The goal is to maximize personal score. Cooperating when the outcome is uncertain means a potential loss of 1 (endowment becomes 0). This strategy avoids that loss and only risks 0 if the reward is almost guaranteed.

### Pseudocode Representation:

```python
# Initialize strategy for player i
strategy = {
    "name": "Opportunistic Dominator",
    "current_round": 0,
    "total_rounds": r,
    "min_cooperators": m,
    "reward_factor": k,
    "cooperators_last_round": 0
}

def decide_action(player_id, game_history, strategy):
    """
    Determines the action (C or D) for the current round.

    Args:
        player_id (int): The ID of the current player.
        game_history (list of dict): History of actions from previous rounds.
                                     Each dict could contain {'round': t, 'actions': {player_id: action, ...}}
        strategy (dict): The player's current strategy state.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """
    current_round = strategy["current_round"] + 1 # Assume strategy object is updated after each round
    total_rounds = strategy["total_rounds"]
    min_cooperators = strategy["min_cooperators"]
    cooperators_last_round = strategy["cooperators_last_round"]

    # --- Edge Case: First Round ---
    if current_round == 1:
        return 'D'

    # --- Edge Case: Last Round ---
    if current_round == total_rounds:
        return 'D'

    # --- General Rule: Intermediate Rounds ---
    # Cooperate IF close to threshold AND not the last round
    if cooperators_last_round >= min_cooperators - 1:
        return 'C'
    else:
        return 'D'

# --- How to update strategy between rounds (example) ---
def update_strategy_state(player_id, current_round_actions, strategy):
    """
    Updates the strategy object based on the actions of the previous round.
    This would be called *after* the actions for a round are determined and observed.
    """
    strategy["current_round"] += 1
    
    # Count cooperators in the *previous* round's actions
    if strategy["current_round"] > 1: # Only applicable from round 2 onwards
        actions_prev_round = game_history[-2]["actions"] # Assuming game_history is available
        coop_count = sum(1 for action in actions_prev_round.values() if action == 'C')
        strategy["cooperators_last_round"] = coop_count
    else:
        strategy["cooperators_last_round"] = 0 # Reset for first round's decision

```

This "Opportunistic Dominator" strategy is designed to be a strong contender by being selectively cooperative, always prioritizing its own guaranteed payoff unless the collective reward is almost assured. It's a strategy that aims to win by taking calculated risks when the odds are heavily in its favor, and by playing it safe otherwise.
'''

description_AGGRESSIVE_39 = '''
This is an intriguing challenge! Designing an aggressive, adaptive, and robust strategy for the Collective Risk Dilemma requires a balance of self-interest and a keen awareness of the collective outcome. Here's a strategy designed to maximize your payoff by leveraging the parameters and history, while embracing an aggressive mindset.

**Strategy Name:** **"The Decisive Defector"**

**Core Philosophy:** My strategy is built on the principle that while cooperation offers the highest potential reward *if successful*, defecting is the safest path to a guaranteed baseline payoff. Aggression comes from pushing the boundaries of this equilibrium – I will only cooperate when there's a strong signal that my contribution will be the deciding factor in securing the reward, or when the collective payoff suggests a sustained period of cooperation is possible. Otherwise, I defect to maximize my individual gain.

---

**1. Decision Rules (When to Cooperate vs. Defect):**

My decision in any given round `t` is based on the following prioritized rules:

*   **Rule 1: The "Near Miss" Opportunity (Cooperate):**
    *   **Condition:** In round `t-1` (or if `t=1`, consider a hypothetical scenario), if the number of cooperators in the *previous* round (`C_{t-1}`) was exactly `m-1` (meaning we were one cooperator short of the threshold), and I *played C* in round `t-1`, then I will *cooperate* in round `t`.
    *   **Rationale:** This signifies a clear and present opportunity to secure the collective reward. My previous cooperation was likely the missing piece, and now there's a high probability that others will also cooperate to reach the threshold, knowing we were so close. By cooperating again, I am acting decisively to secure the reward that I helped pave the way for.

*   **Rule 2: The "Sustained Cooperation" Signal (Cooperate):**
    *   **Condition:** If, in the *last `w` consecutive rounds* (where `w` is a parameter determined by `n` and `m`, and we'll define it later), the number of cooperators has been consistently *at or above `m`*, and my payoff in those rounds was the increased payoff (`1 + k`), then I will *cooperate* in round `t`.
    *   **Rationale:** This indicates a stable, cooperative environment where the collective risk is being successfully averted. If the group has demonstrated an ability to sustain cooperation for a reasonable period, it suggests a potential for future success. This rule is designed to capitalize on established cooperative norms, but only if the evidence is strong and sustained.

*   **Rule 3: The "Last Stand" Cooperation (Cooperate):**
    *   **Condition:** If the current round `t` is the *last round* (`t = r`) and the number of cooperators in the *previous round* (`C_{t-1}`) was *less than `m-1`*, then I will *cooperate* in round `t`.
    *   **Rationale:** In the final round, there's no future to worry about. If we haven't reached the threshold, and there's still a chance to trigger the reward with a single cooperation from me, I will take that shot. It's a desperate but aggressive attempt to secure the bonus payoff in the very last moment.

*   **Rule 4: The Default "Aggressive Defection" (Defect):**
    *   **Condition:** In all other situations (i.e., if none of the above rules trigger a cooperation), I will *defect*.
    *   **Rationale:** This is the aggressive core. If there's no clear signal of an imminent reward (Rules 1, 3) or a sustained cooperative trend (Rule 2), the safest and most individually profitable action is to defect. This secures my base payoff of 1 and avoids contributing to a failed collective effort. I am not going to be the sucker who cooperates when others are defecting.

---

**2. Handling Edge Cases:**

*   **First Round (t=1):**
    *   In the first round, there is no history. Therefore, **I will always defect in the first round.**
    *   **Rationale:** There's no information about other players' tendencies. Cooperating is a risk that can lead to a payoff of 0 if fewer than `m` cooperate. Defecting guarantees a payoff of 1. Aggression here means prioritizing certainty.

*   **Last Round (t=r):**
    *   My decision in the last round is governed by **Rule 3: The "Last Stand" Cooperation**. If the previous round (`r-1`) had fewer than `m-1` cooperators, I will cooperate. Otherwise, I will defect (based on Rule 4).
    *   **Rationale:** As explained above, this is a final, opportunistic attempt to trigger the bonus. If the conditions for cooperation are not met, I will defect to secure my guaranteed payoff.

*   **Parameter `w` for Sustained Cooperation (Rule 2):**
    *   The parameter `w` defines how "long" cooperation needs to be sustained to trigger Rule 2. A good starting point for `w` would be:
        *   `w = max(2, floor(r / 4))`  (e.g., if r=10, w=2; if r=20, w=5)
    *   **Rationale:** This makes `w` proportional to the total number of rounds, but also ensures a minimum duration. A shorter `w` makes the strategy more reactive to short-term cooperation; a longer `w` requires more sustained evidence. This needs to be a tunable parameter in practice. If `r` is small, we might want to set a minimum `w` like 2.

---

**3. Aggressive Mindset Alignment:**

*   **Prioritization of Defection:** The default action is to defect. Cooperation is an exception, requiring specific, strong signals. This reflects an aggressive stance: I will not be exploited, and I will only contribute when the payoff is demonstrably within reach *because of my potential contribution or collective success*.
*   **Decisive Cooperation:** When I do cooperate, it's because the situation is ripe for success (Rule 1) or because the group has proven its ability to cooperate (Rule 2). This isn't hesitant cooperation; it's decisive action to secure gains.
*   **Opportunistic Final Push:** Rule 3 represents an aggressive, last-ditch effort to get the reward, even if the odds are slim. It's about seizing any potential opportunity, however late.
*   **No Punishment Mechanism:** This strategy does *not* include explicit punishment for defectors. While some strategies might retaliate against defectors, "The Decisive Defector" focuses on its own optimal play. It doesn't waste energy on seeking retribution; it focuses on securing its own gains by intelligently switching between cooperation and defection. This is aggressive in its pure focus on self-maximization.
*   **Independence:** The strategy relies solely on game parameters and observable history, ensuring it's robust against any opponent behavior. It doesn't assume or seek out coordination, making it self-sufficient and aggressive in its independence.

---

**Pseudocode Representation:**

```python
# Assume these variables are available:
# n: number of players
# r: number of rounds
# m: minimum cooperators needed
# k: reward factor
# current_round: the current round number (1-indexed)
# history: a list of dictionaries, where each dictionary represents a round's outcome
#          e.g., history[t-1] = {'cooperators': count, 'my_action': 'C' or 'D', 'my_payoff': value}
# my_previous_action: 'C' or 'D'
# num_cooperators_previous_round: count of cooperators in the previous round

def get_w(r):
    # Tunable parameter for sustained cooperation length
    return max(2, r // 4) # Example: 2 or 1/4 of total rounds, whichever is larger

def decide_action(current_round, history, my_previous_action, num_cooperators_previous_round):
    r = get_total_rounds() # Assume this function exists to get total rounds
    n = get_num_players() # Assume this function exists to get num players
    m = get_min_cooperators() # Assume this function exists to get min cooperators
    k = get_reward_factor() # Assume this function exists to get reward factor

    # Handle first round explicitly
    if current_round == 1:
        return 'D'

    # Rule 1: The "Near Miss" Opportunity
    if num_cooperators_previous_round == m - 1 and my_previous_action == 'C':
        return 'C'

    # Rule 2: The "Sustained Cooperation" Signal
    w = get_w(r)
    if current_round > w:
        all_sustained_cooperation = True
        for i in range(current_round - w, current_round):
            if history[i-1]['cooperators'] < m or history[i-1]['my_payoff'] != (1 - 0 + k): # Assumes I cooperated
                all_sustained_cooperation = False
                break
        if all_sustained_cooperation:
            return 'C'

    # Rule 3: The "Last Stand" Cooperation
    if current_round == r and num_cooperators_previous_round < m - 1:
        return 'C'

    # Rule 4: The Default "Aggressive Defection"
    return 'D'

# Example usage within a tournament loop:
# action = decide_action(current_round, history, my_previous_action, num_cooperators_previous_round)
```

---

This strategy aims to be a formidable opponent. It is not afraid to defect when necessary but is sharp enough to recognize and seize opportunities for collective gain when they are clearly presented. The aggression lies in its self-assuredness and its willingness to take calculated risks only when the probabilities of success are demonstrably high.
'''

description_AGGRESSIVE_40 = '''
This is an exciting challenge! Designing an aggressive, adaptive strategy for the Collective Risk Dilemma requires a delicate balance between self-interest and the potential for collective gain, all while keeping a keen eye on exploiting opportunities and punishing deviations.

Here's an aggressive strategy, aptly named "The Opportunist's Gambit," that focuses on maximizing individual payoff through calculated risk-taking and immediate retaliation.

## The Opportunist's Gambit: Strategy Design

The core philosophy of The Opportunist's Gambit is to **always seek the highest individual payoff in the current round, while maintaining a retaliatory posture against perceived exploitation.** It's about being a dominant player who isn't afraid to leverage the system to their advantage.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) in any given round is driven by a calculation of the **immediate, expected payoff** for player `i`, taking into account the known parameters and the history of play.

**The fundamental rule:**

**Defect (D) if the expected payoff from Defecting is greater than the expected payoff from Cooperating.**

Let's break down how we estimate these expected payoffs.

**Estimating Opponent Behavior:**

The strategy needs to infer the likely actions of other players. Since we cannot communicate, we rely on historical data. The most crucial historical data point is the **observed number of cooperators in the previous round.**

We will maintain a dynamic estimation of the likelihood of cooperation from the *average* player. Let $P(C_{avg})$ be the estimated probability that a randomly chosen opponent will cooperate. This will be updated based on the observed cooperation rate in the previous round.

**Calculating Expected Payoffs for Player `i`:**

*   **Expected Payoff from Defecting ($\mathbb{E}[\pi_i(D)]$):**
    If player `i` defects, their own contribution to the community project is 0 ($c_i=0$).
    *   If the threshold `m` is met (meaning `m-1` *other* players cooperate), player `i` gets the reward `k`.
    *   If the threshold `m` is not met, player `i` gets no reward.

    The number of other players who need to cooperate for the threshold to be met is $m-1$.
    The probability that exactly $j$ other players cooperate is $P(\text{j other players C})$. This is tricky to estimate perfectly without knowing individual player strategies. For simplicity and aggression, we will make a simplifying assumption here: we assume opponents will behave *similarly* to how they have behaved on average.

    Therefore, $\mathbb{E}[\pi_i(D)] = (1 - 0) + k \times P(\text{at least } m-1 \text{ other players C})$

    We can approximate $P(\text{at least } m-1 \text{ other players C})$ by looking at the historical proportion of rounds where at least $m-1$ *other* players cooperated, *given that player `i` defected in those rounds*. However, a more aggressive and simpler approach is to use the overall observed cooperation rate.

    **Simplified Aggressive Calculation for $\mathbb{E}[\pi_i(D)]$:**
    Let $\hat{O}_{C}$ be the observed fraction of *other players* who cooperated in the *previous* round.
    If we assume this fraction will hold, then the number of other cooperators is approximately $(n-1) \times \hat{O}_{C}$.
    If $(n-1) \times \hat{O}_{C} \ge m-1$, we expect the reward.

    So, a more pragmatic aggressive approach for $\mathbb{E}[\pi_i(D)]$:
    $\mathbb{E}[\pi_i(D)] = 1 + k \times \mathbb{I}(\text{Observed number of other cooperators in previous round} \ge m-1)$
    where $\mathbb{I}(\cdot)$ is the indicator function. If the condition is true, it's 1, otherwise 0.
    **This is a crucial part of the aggression: we don't "discount" the reward based on probabilities; we demand it if the historical evidence suggests it was met by others.**

*   **Expected Payoff from Cooperating ($\mathbb{E}[\pi_i(C)]$):**
    If player `i` cooperates, their own contribution to the community project is 1 ($c_i=1$), so their private endowment is 0.
    *   If the threshold `m` is met (meaning `m-1` *other* players cooperate), player `i` gets the reward `k`.
    *   If the threshold `m` is not met, player `i` gets no reward.

    **Simplified Aggressive Calculation for $\mathbb{E}[\pi_i(C)]$:**
    $\mathbb{E}[\pi_i(C)] = (1 - 1) + k \times \mathbb{I}(\text{Observed number of other cooperators in previous round} \ge m-1)$
    $\mathbb{E}[\pi_i(C)] = 0 + k \times \mathbb{I}(\text{Observed number of other cooperators in previous round} \ge m-1)$

**Decision Rule Refined:**

**Player `i` Defects (D) if $\mathbb{E}[\pi_i(D)] > \mathbb{E}[\pi_i(C)]$**
$1 + k \times \mathbb{I}(\text{Observed number of other cooperators in previous round} \ge m-1) > 0 + k \times \mathbb{I}(\text{Observed number of other cooperators in previous round} \ge m-1)$

This simplifies to:
**Player `i` Defects (D) if $1 > 0$, which is always true.**

This seems too simple, and it indicates the need for a more nuanced aggressive approach that considers the *certainty* of the reward. The above calculation suggests that *defecting is always better if you expect the threshold to be met*. The aggression comes in *how we assess that expectation*.

Let's re-evaluate the aggressive perspective. The Opportunist's Gambit aims to maximize payoff. If defecting *guarantees* a better payoff than cooperating *in that specific round*, it defects. The "expected payoff" is a tool to judge this.

**Revised Decision Rule (Focusing on Immediate Gain & Risk):**

We will maintain a state variable: `consecutive_defects_by_self`.
We will also track the observed number of cooperators in the previous round: `prev_round_cooperators_count`.

1.  **Base Case (Round 1):** Defect. We are aggressive and want to see how others react.
2.  **Subsequent Rounds:**
    *   **If `prev_round_cooperators_count` >= `m`:** The threshold was met. This means the collective action was successful.
        *   **If player `i` defected in the previous round:** $\pi_i = 1 + k$. If player `i` cooperates, $\pi_i = 0 + k$. Defecting yielded a higher payoff in the previous round. Therefore, **Defect (D)**.
        *   **If player `i` cooperated in the previous round:** $\pi_i = 0 + k$. If player `i` defects, $\pi_i = 1 + k$. Defecting would have yielded a higher payoff. However, the Gambit is about maximizing in the *current* round. Since the threshold was met, cooperating is guaranteed to give `k`. Defecting *might* give `1+k` if others cooperate again, or just `1` if the threshold isn't met.
            *   **Aggressive Choice:** If we are confident the threshold will be met again (e.g., `prev_round_cooperators_count` was significantly above `m`, or many players are observed cooperating consistently), then **Defect (D)** to get the `1` base payoff plus `k`.
            *   **More Conservative Aggression:** If the threshold was *just* met, or there's uncertainty, consider the risk. If player `i` defects and the threshold *fails* due to others' responses, `i` gets `1`. If `i` cooperates, `i` gets `k`. The aggressive approach leans towards the higher potential payoff.
            **Decision:** If `prev_round_cooperators_count` >= `m`: **Defect (D)**. We assume the collective will repeat its success and we aim to capture the `1` endowment bonus.

    *   **If `prev_round_cooperators_count` < `m`:** The threshold was not met. The collective action failed.
        *   **If player `i` defected in the previous round:** `i` got a payoff of `1`. Cooperating would have yielded `0` (since threshold failed). Defecting was better. We continue this. **Defect (D)**.
        *   **If player `i` cooperated in the previous round:** `i` got a payoff of `0`. Defecting would have yielded `1`. Defecting would have been better. **Defect (D)**.

**The "Aggressive" Twist:** The strategy essentially defects unless there's a compelling, *immediate* reason to cooperate for a higher *guaranteed* outcome. The default is to defect and try to exploit the reward if others *do* cooperate.

**Refined Aggressive Decision Rule Summary:**

*   **Round 1:** **Defect (D)**.
*   **Round `t` > 1:**
    *   Let `C_t-1` be the total number of players who cooperated in round `t-1`.
    *   **If `C_t-1` >= `m` (Threshold met):** **Defect (D)**. The goal is to capture the `1` base payoff *in addition* to the `k` reward, assuming the collective will continue to succeed.
    *   **If `C_t-1` < `m` (Threshold not met):**
        *   **If player `i` defected in round `t-1`:** **Defect (D)**. We were rewarded `1` and not punished for failing the threshold.
        *   **If player `i` cooperated in round `t-1`:** **Defect (D)**. Cooperating yielded `0` and no reward. Defecting would have yielded `1`. We switch to defecting.

**This strategy is relentlessly self-interested, assuming others' past behavior will continue or that we can punish deviations by failing to cooperate when they do.**

### 2. Handling Edge Cases

*   **First Round (r=1 or t=1):** **Defect (D)**. The Opportunist's Gambit starts by securing its own endowment and observing the landscape. There's no history to exploit, so it takes the surest `1`.
*   **Last Round (t=r):** The decision rule remains the same. The strategy does not change its behavior in the final round. It plays to maximize its payoff in that last round, based on the history of `r-1` rounds. There is no future to "save" or "punish" for, so it's pure self-interest in that final play.
*   **Threshold `m`:** The strategy directly uses `m` in its decision-making logic.
*   **Parameter `k`:** The strategy implicitly uses `k` by assuming that if the threshold is met, `k` is achieved. The decision to defect or cooperate is based on whether the *other* components of the payoff (the `1` from defecting vs. `0` from cooperating) are more advantageous *given* the potential for `k`. Since the strategy always assumes `k` is possible if the threshold was met, the `1` vs `0` is the deciding factor.

### 3. Being Aggressive

The aggression is embedded in several ways:

*   **Default to Defect:** The strategy's primary instinct is to defect. It only considers cooperating if there's a demonstrably higher payoff *guaranteed* by cooperation, which is rare in this payoff structure unless `k` is very small.
*   **Exploiting Success:** When the threshold is met, the strategy defects to get the `1` endowment bonus. It assumes others will continue to cooperate to secure the `k` reward for themselves. It's essentially saying, "You guys handle the collective good, I'll take my private bonus."
*   **Immediate Retaliation (Implicit):** If the threshold is *not* met, and the player defected, they get `1`. If they had cooperated, they would have gotten `0`. The strategy continues to defect. If the player cooperated and the threshold wasn't met, they got `0`. The strategy *immediately* switches to defecting in the next round to get the `1` payoff. There's no forgiveness for collective failure if it means personal loss.
*   **No "Testing the Waters":** The strategy doesn't "test" cooperation. It only cooperates if the rules *force* it to for a better immediate payoff.
*   **Assumption of Collective Persistence:** When `m` is met, the strategy assumes this collective success will likely continue, allowing it to defect and still benefit from `k`. This is a bold, aggressive assumption.

### Pseudocode Representation

```pseudocode
// Strategy: The Opportunist's Gambit

// Global variables to store history
global_history = [] // List of dictionaries, each representing a round: {'my_action': 'C' or 'D', 'others_actions': ['C', 'D', ...], 'num_cooperators': int}
my_last_action = None

function decide_action(game_params, round_number, history):
    n = game_params['n']
    r = game_params['r']
    m = game_params['m']
    k = game_params['k']

    // Handle Round 1: Aggressive start
    if round_number == 1:
        my_last_action = 'D'
        global_history.append({'my_action': 'D', 'others_actions': [], 'num_cooperators': 0}) // Placeholder for others' actions
        return 'D'

    // Get data from the previous round
    prev_round_data = history[-1]
    num_cooperators_prev_round = prev_round_data['num_cooperators']
    my_action_prev_round = prev_round_data['my_action']

    // Decision Logic for Subsequent Rounds

    // Case 1: Threshold was met in the previous round
    if num_cooperators_prev_round >= m:
        // Aggressively defect to capture the '1' endowment bonus
        // Assumes others will continue cooperating to meet the threshold again.
        my_last_action = 'D'
        return 'D'

    // Case 2: Threshold was NOT met in the previous round
    else: // num_cooperators_prev_round < m
        // If I defected last round, I got 1. Cooperating would have given 0. Stick to defecting.
        // If I cooperated last round, I got 0. Defecting would have given 1. Switch to defecting.
        my_last_action = 'D'
        return 'D'

// Function to record actual outcomes after a round
function record_round_outcome(my_actual_action, others_actual_actions, actual_num_cooperators):
    // Assuming global_history is updated correctly for round 1
    if len(global_history) > 0:
        if len(global_history) == 1 and global_history[0]['my_action'] == 'D' and not global_history[0]['others_actions']: # Special handling for round 1 placeholder
             global_history[0]['my_action'] = my_actual_action
             global_history[0]['others_actions'] = others_actual_actions
             global_history[0]['num_cooperators'] = actual_num_cooperators
        else:
            global_history.append({'my_action': my_actual_action, 'others_actions': others_actual_actions, 'num_cooperators': actual_num_cooperators})


// The actual function called for each round will be `decide_action`.
// The `record_round_outcome` would be called by the tournament system.
// `history` passed to `decide_action` would be the `global_history` up to the current round - 1.
```

This strategy is built on the principle of maximizing immediate individual gain by assuming others will bear the risk of cooperation, and punishing failures by withholding cooperation oneself. It's a predatory approach designed to profit from the dynamics of the game, especially when `k` is sufficiently large to incentivize cooperation from others.
'''

description_AGGRESSIVE_41 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the cooperative reward when possible. The core principle is to *defect by default* and *cooperate only when it's demonstrably profitable and safe*.

## Strategy Name: "Opportunistic Defector"

This strategy is driven by a constant assessment of risk and reward, prioritizing self-interest but prepared to leverage collective success when the conditions are overwhelmingly favorable.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` for player `i` is determined by the following logic:

*   **Default Action: Defect (D)**
    *   At its core, this strategy assumes others will defect, and it's safer to keep one's own endowment. Cooperation is seen as a risk that needs strong justification.

*   **When to Cooperate (C):**
    *   **High Confidence in Threshold Met:** Cooperate *only if* the probability of meeting the `m` cooperation threshold, based on observed historical behavior and the current round's context, is extremely high.
    *   **Specifically, Cooperate if:**
        1.  **Current Round Cooperative Count is High:** In the current round `t`, if the number of players observed to have cooperated in the *previous round* (`t-1`) is already `m-1`. This means your cooperation is the *sole trigger* for the reward.
        2.  **High Likelihood of Others Cooperating:** If, in the previous round (`t-1`), at least `m-1` players cooperated. This provides strong evidence that the group is willing and able to reach the threshold.
        3.  **Your Cooperation is Essential (and not a solo act):** If the number of cooperators in the previous round (`t-1`) was `m-2` or less, but the total number of players who *have cooperated at least once* across all previous rounds is significantly high and increasing. This suggests a general trend towards cooperation that makes your cooperative act less risky.
        4.  **"Ganging Up" on Defectors:** If you observe a strong tendency for a large number of players (more than `m`) to cooperate in previous rounds, and you are confident that your cooperation will be part of a larger successful group. This is a less aggressive but still opportunistic approach.

*   **When to Defect (D):**
    *   **Default:** In all other cases, defect. This is the safe and profitable option when the collective reward is uncertain or unattainable.
    *   **Insufficient Cooperators:** If the number of observed cooperators in the previous round was less than `m-1`.
    *   **Uncertainty:** If the history is too sparse or inconsistent to make a confident prediction about meeting the threshold.
    *   **Risk of Being the Only Cooperator:** If you are the only one cooperating and the threshold is not met, you lose your endowment. This strategy aims to avoid that.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Aggressive Default:** **Always Defect (D).** There is no historical data to rely on, and the default assumption is that others will be cautious or self-interested. It's best to secure your own endowment and observe.

*   **Last Round (t=r):**
    *   **Purely Self-Interested Calculation:** **Defect (D).** Since there are no future rounds to benefit from a cooperative reward, the optimal strategy is to defect and secure your private endowment. There is no incentive to risk your endowment for a one-off reward that won't compound.

*   **Near the End of the Game (t = r-1, r-2, etc.):**
    *   **Increased Caution:** As the game approaches its end, the incentive to cooperate diminishes. Decisions will lean more heavily towards defecting unless there's an extremely high certainty of meeting the threshold and reaping the reward for the remaining rounds. The "High Confidence" rule becomes even stricter.

### 3. Aggressive Mindset:

This strategy is aggressive because it:

*   **Starts with a Defect Bias:** It doesn't shy away from maximizing individual gain by default.
*   **Only Cooperates When Profit is Assured (or highly probable):** It views cooperation as an investment, and it only invests when the return is virtually guaranteed and significantly outweighs the risk.
*   **Seeks to Leverage Others' Cooperation:** It's designed to benefit from collective efforts without necessarily being the initiator unless its cooperation is the *definitive trigger* for a reward.
*   **Punishes Failure of Cooperation (Indirectly):** By defecting when cooperation fails, it implicitly signals that it won't be a "sucker" carrying others.
*   **Maximizes Gains When Opportunity Arises:** When the conditions are right, it will jump on the opportunity to gain `k` for itself, increasing its total payoff significantly.

### Pseudocode Representation:

```python
def decide_action(player_id, round_num, game_params, history):
    n = game_params['n']
    r = game_params['r']
    m = game_params['m']
    k = game_params['k']

    # Edge Case: First Round
    if round_num == 1:
        return 'D'

    # Edge Case: Last Round
    if round_num == r:
        return 'D'

    # Get data from the previous round (t-1)
    prev_round_actions = history.get(round_num - 1, {})
    prev_round_cooperators = sum(1 for action in prev_round_actions.values() if action == 'C')

    # --- Core Decision Logic ---

    # Condition 1: Your cooperation is the sole trigger for the reward
    # This is the most aggressive and opportunistic condition
    if prev_round_cooperators == m - 1:
        return 'C'

    # Condition 2: High likelihood of others cooperating to meet the threshold
    # This is a strong signal of group willingness
    if prev_round_cooperators >= m - 1:
        return 'C'

    # Condition 3: Trend towards cooperation, making your act less risky
    # (This requires a more complex historical analysis, could be simplified)
    # For simplicity here, we'll focus on direct signals. A more robust version
    # would analyze average cooperators over time or players who consistently cooperate.
    # We'll omit this more complex condition for this initial design to keep it sharp.

    # Condition 4: Strong general cooperation trend and confident participation
    # Similar to Condition 3, this requires deeper historical analysis.
    # For this aggressive strategy, we stick to immediate, high-confidence triggers.

    # Default: If none of the above conditions are met, defect.
    return 'D'

# Example usage within a tournament loop:
# history = {
#     1: {'player1': 'D', 'player2': 'C', ...},
#     2: {'player1': 'D', 'player2': 'D', ...},
#     ...
# }
# current_action = decide_action(my_player_id, current_round, game_parameters, history)
```

**Explanation of Aggressive Logic in Pseudocode:**

*   The strategy explicitly checks for `prev_round_cooperators == m - 1`. This is the most aggressive trigger: "If my contribution is the ONLY thing needed to get the reward, I will provide it." This minimizes the risk of being the sole cooperator with no reward.
*   It then checks for `prev_round_cooperators >= m - 1`. This is still aggressive but slightly less so, relying on a strong previous round to indicate a high probability of success.
*   Conditions 3 and 4 (which involve looking at longer historical trends or the behavior of specific individuals) are intentionally kept simpler in this aggressive strategy. A truly aggressive approach focuses on immediate, high-probability gains rather than slow building of trust or complex pattern recognition. If the conditions are not met for a clear and immediate benefit, the strategy defects.

This "Opportunistic Defector" strategy aims for a high individual payoff by being cautious but ready to seize opportunities when they are clearly and immediately beneficial. It is designed to be robust by not relying on altruism or complex social dynamics, but rather on clear, calculable advantages.
'''

description_AGGRESSIVE_42 = '''
## Aggressive Strategy: The "Calculated Opportunist"

This strategy is designed to maximize individual payoff by exploiting cooperation when it's safe and defecting when it's advantageous. It's inherently aggressive, aiming to benefit from others' contributions without being overly exposed to the risk of insufficient cooperation.

**Core Philosophy:** "Leverage the herd when it's strong, but don't get trampled when it's weak."

---

### 1. Decision Rules: When to Cooperate vs. Defect

The "Calculated Opportunist" operates on a principle of **conditional cooperation based on observable collective action and perceived risk.**

*   **Cooperate (C) IF:**
    *   **The number of observed past cooperators in the *immediately preceding round* is at least `m - 1`.**
        *   **Rationale:** If `m-1` players (or more) cooperated in the last round, it suggests that the threshold `m` is likely to be met or exceeded in the current round. Contributing now secures your share of the higher reward (`k`) with minimal risk of ending up with only the private payoff. You are essentially "riding the wave" of demonstrated cooperation.
    *   **This is the first round of the game and `n - m + 1 >= m`.**
        *   **Rationale:** In the very first round, there's no history. This condition aims to initiate cooperation if there's a sufficiently high probability that *your* cooperation, combined with a reasonable number of others who might also be inclined to cooperate on the first round, can reach the threshold. This is a calculated risk to seed cooperation and potentially trigger the reward early. The condition `n - m + 1 >= m` means that even if only the minimum required number of *other* players defect, and you cooperate, the threshold is met. This is a very aggressive stance for the first round.

*   **Defect (D) IF:**
    *   **The number of observed past cooperators in the *immediately preceding round* is less than `m - 1`.**
        *   **Rationale:** If fewer than `m-1` players cooperated, the likelihood of reaching the threshold `m` in the current round is low. By defecting, you guarantee yourself the private payoff of `1` and avoid contributing your endowment to a project that is unlikely to yield the reward. You are protecting your downside.
    *   **This is the last round of the game.**
        *   **Rationale:** In the final round, there are no future repercussions for defection. The "Calculated Opportunist" prioritizes securing the private payoff of `1` without any reliance on future cooperation. This is a pure, final act of self-interest.
    *   **Any other situation not covered by the cooperation rules.**
        *   **Rationale:** This is the default aggressive stance. If there isn't a strong, demonstrable signal of sufficient cooperation from previous rounds, the strategy defaults to maximizing personal gain by defecting.

---

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** Cooperate if `n - m + 1 >= m`. Otherwise, defect.
    *   **Rationale:** As explained above, this is a bold initial move. It assumes that if a significant portion of players *could* cooperate and still meet the threshold even with maximum defection, it's worth trying to kickstart cooperation. If this condition isn't met, it signals that the game is likely to be defect-heavy from the start, so defecting is the safer, more aggressive play.

*   **Last Round (t=r):**
    *   **Decision:** Always Defect.
    *   **Rationale:** Pure payoff maximization. No future rounds to consider, so secure the individual gain.

*   **When `m-1` is 0 (i.e., `m=1`):** The rule "at least `m-1`" becomes "at least 0". This would imply cooperating if *any* player cooperated. However, the problem states `m > 1`. This edge case is avoided by the parameter constraints.

*   **When `m-1` is negative (i.e., `m=0`):** This is also avoided by parameter constraints (`m > 1`).

*   **When `m-1` is greater than or equal to `n`:** The rule "at least `m-1`" cooperators means that cooperation is extremely unlikely. If `m-1 >= n`, then `m > n`. This contradicts `m < n`. So, this edge case is also avoided by parameter constraints.

---

### 3. Being Aggressive

The "Calculated Opportunist" is aggressive in several ways:

*   **Early Risk-Taking (Round 1):** The initial decision to cooperate is designed to be ambitious. It aims to leverage the possibility of meeting the cooperation threshold with a minimal number of other cooperators. This is a proactive attempt to secure the reward early.
*   **Exploiting Confidence (Conditional Cooperation):** When cooperation is detected, this strategy immediately jumps on board. It doesn't wait for overwhelming evidence; it seizes the opportunity as soon as there's a high probability of success. This is aggressive because it assumes others will continue to cooperate to maintain the reward.
*   **Unwavering Defection (Last Round):** The absolute defection in the final round is pure aggression. It's the ultimate act of prioritizing self-interest when there's no cost to future interactions.
*   **Default to Defection:** When there's any uncertainty or lack of clear signals of cooperation, the strategy defaults to defecting. This is aggressive because it prioritizes individual gain and doesn't waste resources trying to achieve a collective good that is unlikely to materialize. It's aggressive in its refusal to be a "sucker."

---

### Pseudocode Representation:

```python
def calculated_opportunist_strategy(game_history, parameters):
    n = parameters['n']
    r = parameters['r']
    m = parameters['m']
    k = parameters['k']
    current_round = len(game_history) + 1

    # Handle Last Round
    if current_round == r:
        return 'D'

    # Handle First Round
    if current_round == 1:
        # Aggressive initial move: Cooperate if my cooperation + minimum others can meet threshold
        if n - m + 1 >= m:
            return 'C'
        else:
            return 'D'

    # Handle Subsequent Rounds (t > 1 and t < r)
    else:
        last_round_actions = game_history[-1] # Actions of all players in the previous round
        cooperators_in_last_round = last_round_actions.count('C')

        # Cooperate if enough cooperators in the previous round to likely meet threshold
        if cooperators_in_last_round >= m - 1:
            return 'C'
        else:
            return 'D'

```

This strategy embodies a calculated, often bold, approach to self-preservation and profit maximization within the Collective Risk Dilemma. It's adaptive to observed behavior but remains fundamentally driven by an aggressive pursuit of individual gain.
'''

description_AGGRESSIVE_43 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff while leveraging the potential for collective reward, with a strong bias towards defection unless the conditions for cooperation are overwhelmingly favorable.

## Strategy: "The Calculated Risk-Taker"

This strategy is built on the principle of **opportunistic defection with a high threshold for cooperation**. It aims to exploit the temptation to defect while strategically cooperating only when it guarantees a substantial individual payoff and carries minimal risk. Aggression is embedded in the default action (defect) and the strict conditions for deviating from it.

### Core Philosophy:

*   **Default to Defect:** Assume others will defect. Cooperate only when the collective benefit is almost certain and the individual return is guaranteed to be significantly higher than defecting alone.
*   **Exploit the Threshold:** Recognize that the "sweet spot" for cooperation is just above the minimum required number of cooperators (`m`). Aim to be one of the defectors who benefits from others' cooperation.
*   **Punish Free-riders (indirectly):** By defecting when cooperation is not guaranteed, you implicitly punish those who cooperate without you, forcing them to bear the cost of not reaching the threshold.
*   **Maximize Individual Gain:** The ultimate goal is to maximize your personal payoff, which means taking advantage of cooperation when it's safe and lucrative, and defecting when it's not.

### 1. Decision Rules:

Your decision in any given round `t` (where `t` ranges from 1 to `r`) is based on an assessment of the current game state, primarily focusing on the number of players who cooperated in the *previous* round (`C_{t-1}`).

**The "Cooperate Condition":** You will **ONLY** cooperate in round `t` if **ALL** of the following conditions are met:

1.  **Sufficient Previous Cooperation:** The number of players who cooperated in the previous round (`C_{t-1}`) was **at least `m - 1`**. This means that if you choose to cooperate, the threshold (`m`) will almost certainly be met.
2.  **Guaranteed Personal Gain:** If you cooperate, your individual payoff (`π_i`) in the current round will be **strictly greater than** the payoff you would receive if you defected in the current round (assuming `m` or more cooperate, which is implied by condition 1).
    *   Payoff if you cooperate: `(1 - 1) + k = k`
    *   Payoff if you defect (assuming `m` or more cooperate): `(1 - 0) + k = 1 + k`
    *   Therefore, this condition simplifies to: **`k > 1 + k` which is impossible.**
    *   **Correction and Refinement:** The crucial point is the *guarantee* of reaching the threshold. If `C_{t-1} = m - 1`, and you cooperate, then `m` players have cooperated, and your payoff is `k`. If you *defect* when `C_{t-1} = m - 1`, and you don't cooperate, then the total cooperators will be `m - 1`, the threshold is not met, and your payoff is `1`. So, you cooperate if `k > 1`. This is always true since `k > 1`.
    *   **Revised Condition 2:** If `C_{t-1} = m - 1`, and you cooperate, your payoff is `k`. If you defect, your payoff is `1`. Thus, you cooperate if `k > 1`. Since the parameter `k` is defined as `k > 1`, this condition is always met *when condition 1 is met*.

3.  **Not the Last Round (with a caveat):**
    *   If `t < r` (not the last round): You cooperate if conditions 1 and 2 are met.
    *   If `t = r` (the last round): **You ALWAYS defect in the last round.** This is a crucial element of the aggressive, self-interested strategy. The future incentive for cooperation disappears, and defection is the optimal choice to maximize your final score.

**Summary of Cooperation Rule:**

In round `t`:
*   If `t = r`: **Defect (D)**
*   If `t < r`:
    *   If `C_{t-1} >= m - 1`: **Cooperate (C)**
    *   Else: **Defect (D)**

**Default Action:** If none of the above conditions for cooperation are met, you **Defect (D)**.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   In the first round, there is no prior history (`C_0` is undefined).
    *   **Decision:** **Always Defect (D)**. The aggressive stance is to start by securing your individual endowment. There's no information to justify risking cooperation, and you can't assume `m-1` players will cooperate.

*   **Last Round (t=r):**
    *   As stated in the decision rules, the incentive for collective action is gone. Future rewards cannot influence current behavior.
    *   **Decision:** **Always Defect (D)**. This ensures you lock in your individual payoff without risking any contribution.

*   **Opponent Behavior:** The strategy is designed to be robust to a wide range of opponent behaviors:
    *   **Consistent Cooperators:** If others consistently cooperate, you will benefit by defecting and receiving `1 + k` in most rounds. You'll only defect to cooperate if the number of previous cooperators drops to `m-1` and you can secure `k`.
    *   **Consistent Defectors:** If others consistently defect, the `C_{t-1} >= m - 1` condition will rarely be met, and you will also consistently defect, earning `1` in each round.
    *   **Mixed Strategies:** The strategy adapts by observing the *outcome* of the previous round. If a sufficient number of players cooperate, you will reciprocate cautiously in the next round. If they fail to reach the threshold, you will defect.

### 3. Aggressiveness:

*   **Default to Defection:** The strategy's core is to defect. Cooperation is an exception, a deliberate calculated risk taken only when the reward is guaranteed and substantial.
*   **No "Nice" Reciprocity:** Unlike tit-for-tat, this strategy doesn't automatically cooperate if the opponent cooperated. It requires a specific, high threshold of past cooperation (`m-1`) to even consider cooperating. This is an aggressive stance that prioritizes extracting value.
*   **Exploiting the Majority:** The strategy aims to be among the defectors who benefit from others fulfilling the `m` cooperation requirement. This is a direct way to maximize individual payoff at the expense of those who bear the cooperation cost.
*   **No Forgiveness:** If the group fails to meet the threshold, the strategy doesn't "forgive" or attempt to rebuild cooperation. It simply reverts to defecting, waiting for the conditions to become favorable again.
*   **Last Round Ruthlessness:** The absolute defection in the last round is a strong indicator of its self-interested, aggressive nature.

---

### Pseudocode:

```python
def decide_action(player_id, round_num, total_rounds, num_players, min_cooperators, reward_factor, history):
    """
    Decides the action for the current round based on the "Calculated Risk-Taker" strategy.

    Args:
        player_id (int): The ID of the current player.
        round_num (int): The current round number (1-indexed).
        total_rounds (int): The total number of rounds.
        num_players (int): The total number of players.
        min_cooperators (int): The minimum number of cooperators needed to trigger the reward.
        reward_factor (float): The factor 'k' for the reward.
        history (list of dict): A list of dictionaries, where each dictionary represents a past round
                                and contains 'actions' (a list of actions for all players) and 'payoffs'.
                                history[-1] is the most recent round.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # --- Handle Edge Cases ---
    if round_num == total_rounds:
        # Always defect in the last round
        return 'D'

    # --- Decision Rules for Rounds < total_rounds ---
    previous_round_actions = history[-1]['actions'] if history else []
    cooperators_in_previous_round = previous_round_actions.count('C')

    # Calculate payoff if I cooperate (assuming threshold is met)
    my_payoff_if_cooperate = reward_factor # (1 - 1) + k

    # Calculate payoff if I defect (assuming threshold is met)
    my_payoff_if_defect = 1 + reward_factor # (1 - 0) + k

    # --- The Cooperate Condition ---
    # Condition 1: Sufficient previous cooperation (m-1 means my cooperation will push it to m)
    if cooperators_in_previous_round >= min_cooperators - 1:
        # Condition 2: Guaranteed personal gain (which is always true if k > 1)
        # And Condition 3: Not the last round (already handled above)
        if my_payoff_if_cooperate > my_payoff_if_defect: # This condition is logically flawed in the text description
            # Let's re-evaluate based on payoff comparison:
            # If I cooperate, my payoff is 'k' IF the threshold is met (which it will be, as cooperators_in_previous_round = m-1 and I add one)
            # If I defect, my payoff is '1' IF the threshold is NOT met (which it won't be if I defect and previous was m-1)
            # So, I cooperate if k > 1. This is always true by parameter definition.
            # Therefore, the sole condition for cooperation is cooperators_in_previous_round >= min_cooperators - 1

            return 'C'

    # --- Default Action ---
    # If none of the cooperation conditions are met, defect.
    return 'D'

# Example usage within a tournament loop:
# history = [] # initialize empty history
# for t in range(1, r + 1):
#     my_action = decide_action(my_player_id, t, r, n, m, k, history)
#     # ... simulate other players' actions ...
#     # ... record actions and payoffs for history ...
#     history.append({'actions': all_actions_in_round_t, 'payoffs': all_payoffs_in_round_t})
```
'''

description_AGGRESSIVE_44 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the potential for collective reward when strategically advantageous.

## Aggressive Strategy: "The Opportunistic Trigger"

This strategy is built on the principle of **maximally exploiting opportunities for high payoffs, but with a clear trigger for defection when the collective fails to meet its obligations.** It's aggressive because it aims to capitalize on the higher rewards associated with cooperation when possible, but it doesn't shy away from maximizing personal gain when cooperation is unlikely or has failed.

### 1. Decision Rules: When to Cooperate vs. Defect

The core of this strategy lies in a dynamic assessment of the likelihood of reaching the cooperation threshold ($m$).

*   **Initial Aggression (Round 1):**
    *   **If $n \ge m \times 2$ (i.e., the number of players is at least twice the minimum required cooperators):** **Cooperate (C)**.
        *   *Rationale:* In this scenario, it's highly probable that the threshold can be met. By cooperating early, we position ourselves to receive the $k$ reward. This is an aggressive play to secure a high payoff if others do their part.
    *   **If $n < m \times 2$:** **Defect (D)**.
        *   *Rationale:* The probability of others cooperating enough to reach the threshold is lower. Defecting to guarantee a payoff of 1 is safer and more aggressive in this riskier environment, as it avoids the possibility of being exploited (cooperating while others defect, leading to a payoff of 0).

*   **Subsequent Rounds (Round $t > 1$):**
    *   **Define "Cooperation Momentum":** Track the average number of players who cooperated in the last **min(t-1, 3)** rounds. Let this be `avg_cooperators_recent`.
    *   **Define "Cooperation Deficit Trigger":** If, in any of the last **min(t-1, 3)** rounds, the number of cooperators was *less than* $m-1$, and you cooperated in that round, set a **"Cooperation Deficit Trigger" flag to TRUE.** This flag remains TRUE until explicitly reset.
    *   **Decision Logic:**
        *   **If "Cooperation Deficit Trigger" is TRUE:** **Defect (D)**.
            *   *Rationale:* This signifies that the collective has failed to meet its obligations, and you have been potentially exploited. This is a punitive and aggressive stance. You are now prioritizing your own guaranteed payoff.
        *   **If "Cooperation Deficit Trigger" is FALSE:**
            *   **If `avg_cooperators_recent` is greater than or equal to $m-1$:** **Cooperate (C)**.
                *   *Rationale:* There's a strong indication that the threshold is being met or is achievable. This is an aggressive pursuit of the $k$ reward, betting on continued collective effort.
            *   **If `avg_cooperators_recent` is less than $m-1$:** **Defect (D)**.
                *   *Rationale:* The collective momentum is insufficient. Cooperating is too risky, as the threshold is unlikely to be met, leading to a payoff of 0. Defecting guarantees a payoff of 1.

### 2. Handling Edge Cases

*   **First Round (t=1):** Covered by the "Initial Aggression" rule.
*   **Last Round (t=r):** In the last round, there is no future to incentivize cooperation. Therefore, the strategy becomes purely self-interested and thus aggressive:
    *   **Always Defect (D)**.
        *   *Rationale:* The highest guaranteed payoff of 1 is secured by defecting. There is no longer any benefit to cooperating, as the $k$ reward is only for the current round and cannot influence future outcomes.
*   **Early Rounds with Low Player Count ($n$ close to $m$):** The "Initial Aggression" rule already accounts for this. If $n < m \times 2$, the strategy defaults to defection, which is aggressive in minimizing downside risk when cooperation is precarious.

### 3. Aggressive Mindset Alignment

This strategy is aggressive in several ways:

*   **Opportunistic Reward Seeking:** It actively seeks the higher payoffs when the conditions are favorable, by cooperating early and when momentum is strong. This is an aggressive pursuit of upside.
*   **Punitive Retaliation:** The "Cooperation Deficit Trigger" introduces an element of punishment. If other players fail to contribute sufficiently, and you are left with a low payoff after cooperating, you will aggressively defect in subsequent rounds. This discourages exploitation.
*   **No Naivety:** It avoids altruistic or overly trusting behavior. Cooperation is conditional and based on observable evidence of collective action.
*   **Guaranteed Minimum:** By defecting when cooperation is unlikely or has failed, the strategy guarantees a baseline payoff of 1, demonstrating a robust self-preservation instinct.
*   **Final Round Dominance:** The unconditional defection in the final round ensures maximum personal gain in the absence of future considerations.

### Pseudocode

```pseudocode
// Game Parameters
n // number of players
r // number of rounds
m // minimum cooperators needed
k // reward factor

// Strategy State Variables
cooperation_deficit_trigger = FALSE
history = empty list of (player_actions_in_round_t, my_action_in_round_t) for t = 1 to current_round - 1

function decide_action(current_round, history, n, m, k):
    // Rule for the last round
    if current_round == r:
        return "D" // Aggressively defect in the last round

    // Rule for the first round
    if current_round == 1:
        if n >= m * 2:
            return "C" // Cooperate if threshold is likely achievable
        else:
            return "D" // Defect if cooperation is risky

    // Update cooperation_deficit_trigger from history
    for round_t in history (last min(current_round-1, 3) rounds):
        actions_t, my_action_t = round_t
        num_cooperators_t = count_cooperators(actions_t)
        if num_cooperators_t < m - 1 and my_action_t == "C":
            cooperation_deficit_trigger = TRUE
            break // No need to check further if trigger is set

    // Decision logic for subsequent rounds
    if cooperation_deficit_trigger:
        return "D" // Aggressively defect if deficit was encountered

    // Calculate average cooperators in recent rounds
    recent_rounds = history[max(0, current_round - 1 - 3) : current_round - 1] // Last up to 3 rounds
    if not recent_rounds: // Should not happen if current_round > 1
        avg_cooperators_recent = 0
    else:
        total_cooperators_recent = 0
        for round_t in recent_rounds:
            actions_t, my_action_t = round_t
            total_cooperators_recent += count_cooperators(actions_t)
        avg_cooperators_recent = total_cooperators_recent / len(recent_rounds)

    // Decide based on cooperation momentum
    if avg_cooperators_recent >= m - 1:
        return "C" // Cooperate if momentum is strong
    else:
        return "D" // Defect if momentum is insufficient

function count_cooperators(actions):
    count = 0
    for action in actions:
        if action == "C":
            count += 1
    return count

// Example usage:
// my_action = decide_action(t, history, n, m, k)
```
'''

description_AGGRESSIVE_45 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to be adaptive, robust, and to maximize personal gain. This strategy prioritizes leveraging cooperation for high rewards when it's safe, but quickly reverts to defection to exploit the system when uncertainty or perceived exploitation arises.

## Aggressive Strategy: "Calculated Opportunist"

This strategy is designed to be proactive and dominant. It aims to benefit from collective action when the conditions are favorable and others are contributing, but immediately switches to a purely self-serving mode when cooperation seems unreliable or when it can gain an advantage by defecting.

### 1. Decision Rules: When to Cooperate vs. Defect?

The core of this strategy is a dynamic threshold for cooperation. Instead of a fixed rule, it constantly assesses the perceived likelihood of successfully reaching the cooperation threshold (`m`).

**General Rule:** Cooperate *only if* you believe the number of current cooperators (including yourself) is likely to reach `m` or more. Otherwise, defect.

**Detailed Decision Logic:**

*   **Initial State (Round 1):**
    *   **Defect.** This is the aggressive stance. We don't want to be the first to risk our endowment without any information. We will observe and learn.

*   **Subsequent Rounds (t > 1):**
    *   **Observe and Estimate:** Analyze the actions of *all* players in the previous round. Count the number of players who cooperated in round `t-1`. Let this count be `C_{t-1}`.
    *   **Cooperation Decision:**
        *   **If `C_{t-1}` >= `m - 1` (i.e., the previous round was successful or just one cooperator short of success, and we were one of them or could have been):** **Cooperate (C).** This indicates a strong likelihood of meeting the threshold, and the potential reward `k` is substantial. We are willing to take a calculated risk to secure the higher payoff.
        *   **If `C_{t-1}` < `m - 1` (i.e., the previous round fell significantly short of the threshold, or we were the *sole* cooperator and it wasn't enough, or we defected and it still wasn't enough):** **Defect (D).** The risk of cooperating is too high, and the reward is unlikely. It's safer and more profitable to keep our endowment.

*   **Adaptive Adjustment (Advanced Trigger):**
    *   **If `C_{t-1}` >= `m` (previous round was successful) AND you played D:** **Defect (D) in the current round.** This is a crucial aggressive adjustment. It means you benefited from others' cooperation without contributing. This is pure opportunism and will be repeated as long as the opportunity exists.
    *   **If `C_{t-1}` < `m` (previous round failed) AND you played C:** **Defect (D) in the current round.** This is a strong signal that cooperation is not working or others are exploiting the situation. We will not be the "sucker" cooperating into a failing system.

### 2. Handling Edge Cases:

*   **Round 1:** **Always Defect (D).** This is the initial aggressive posture. We gather information by observing others' moves. We are not the first to contribute.

*   **Last Round (Round `r`):**
    *   **If `C_{t-1}` >= `m` (i.e., the *second to last* round was successful):** **Cooperate (C).** In the absolute final round, there's no future consequence to being exploited. We want to maximize our final payoff if the conditions are met. If others defect, we still get `1 + k`. If they cooperate, we get `1 + k`. It's a win-win in terms of maximizing immediate payoff, regardless of others' actions, *given the success condition was met in the prior round*.
    *   **If `C_{t-1}` < `m` (i.e., the *second to last* round failed):** **Defect (D).** If the system hasn't worked by the penultimate round, it's unlikely to magically succeed now. Defecting guarantees a payoff of 1.

*   **`m` = `n` (everyone must cooperate):**
    *   **Round 1:** Defect.
    *   **Subsequent Rounds:** If `C_{t-1}` == `n-1` (everyone cooperated *except* you), **Cooperate (C)**. You can then ensure success.
    *   **Otherwise:** **Defect (D).** It's impossible to guarantee cooperation if `m=n` and you cannot force others.

*   **`m` = `n-1`:**
    *   **Round 1:** Defect.
    *   **Subsequent Rounds:** If `C_{t-1}` >= `n-2` (i.e., the previous round had `n-2` or `n-1` cooperators), **Cooperate (C)**. This is because with `n-2` cooperators, your cooperation makes it `n-1`, achieving the goal. With `n-1` cooperators, you can also cooperate and benefit.
    *   **Otherwise:** **Defect (D).**

### 3. Being Aggressive:

This strategy is aggressive in several ways:

*   **Initial Defection:** We don't start by being the "nice" player. We are skeptical and self-interested from the outset.
*   **Opportunistic Exploitation:** The adaptive adjustment rule specifically targets scenarios where you can benefit from others' cooperation without contributing yourself. This is a highly aggressive, game-theory-driven approach to maximize personal gain.
*   **Rapid Reversion to Defection:** If cooperation doesn't meet the threshold or if you perceive exploitation, the strategy immediately switches to defection. This prevents you from being a consistent "sucker" in unreliable situations.
*   **Focus on High Payoff Scenarios:** The strategy only commits to cooperation when the conditions are favorable and the reward (`k`) is almost guaranteed. This means we are actively seeking the high-payoff states of the game.

### Pseudocode Representation:

```pseudocode
// Strategy: Calculated Opportunist

function decide_action(game_parameters, history):
    n = game_parameters.n
    r = game_parameters.r
    m = game_parameters.m
    k = game_parameters.k
    current_round = history.current_round

    // Round 1: Always Defect
    if current_round == 1:
        return "D"

    // Analyze previous round
    previous_actions = history.get_actions_from_round(current_round - 1)
    cooperators_last_round = count_cooperators(previous_actions)
    my_action_last_round = get_my_action(previous_actions) // Assuming you know your own past action

    // Decision Logic
    if cooperators_last_round >= m:
        // Previous round was successful
        if my_action_last_round == "D":
            // Exploited successfully, continue exploiting
            return "D"
        else: // my_action_last_round == "C"
            // Contributed to success, now it's safe to ensure reward
            // or if others might defect now, we still get 1+k if m met
            // If m condition is met, payoff is 1+k regardless of my action IF m is met.
            // To maximize potential, we want to ensure m is met if possible.
            // However, if others are reliably cooperating, defecting is better.
            // Let's be more aggressive: if the system *succeeded* AND I cooperated,
            // I proved I can cooperate. Now, if there's a risk of failure, I still cooperate.
            // If there's NO risk of failure (i.e., lots of cooperators), I defect.
            // A simpler, aggressive interpretation: if the system succeeded, it means cooperation works.
            // Let's leverage that.
            // If the system succeeded, I can *now* defect if I want, *unless* my defection prevents future success.
            // The rule below handles this: if successful, and I defected, I defect again.
            // If successful, and I cooperated, I still might cooperate to ensure the *next* round's success is possible.
            // This is where it gets tricky. Let's simplify for pure aggression:
            // If system *was* successful, and I defected: Defect again.
            // If system *was* successful, and I cooperated: Cooperate again (to potentially reinforce).
            // However, the original logic for "opportunistic exploitation" takes precedence:
            return "D" // Re-evaluate: If it was successful, I *can* now defect and get 1+k if m met.
                       // But to be truly aggressive and robust, if it WAS successful,
                       // and m is met, I get 1+k anyway. If I cooperate, I get 1+k.
                       // The most aggressive thing is to secure the 1+k if I can.
                       // The rule `if cooperators_last_round >= m-1` already handles this.
                       // Let's refine the "adaptive adjustment":
                       // If previous round was successful, *and I defected*, continue defecting.
                       // If previous round was successful, *and I cooperated*, consider cooperating.
                       // For pure aggression, if successful, and I defected, that's pure profit.
                       // If successful, and I cooperated, I should *still* be suspicious.

            // REVISED ADAPTIVE ADJUSTMENT FOR EXTREME AGGRESSION:
            // If previous round was successful (cooperators_last_round >= m):
            //    If I played "D" in the previous round: Return "D" (pure exploitation)
            //    If I played "C" in the previous round: Return "D" (assume others will keep cooperating and I get 1+k by defecting)
            // This implies *always defecting* if the last round was successful.
            // This is too simple. Let's revert to the original logic but emphasize the adjustment.

            // Back to the original, more nuanced aggressive logic:
            // If C_{t-1} >= m (successful):
            //    If I played D: -> D (exploit)
            //    If I played C: -> C (maintain cooperation, secure reward)
            // This means if system succeeds, I either exploit or ensure my own continued reward.
            // This is not aggressive enough.

            // FINAL AGGRESSIVE ADAPTIVE ADJUSTMENT:
            // If previous round was successful (cooperators_last_round >= m):
            //    Return "D" // Always exploit if cooperation succeeded. I get 1+k if m is met.
                           // My defection doesn't cause failure, others' cooperation did.
                           // This is the MOST aggressive stance.

    elif cooperators_last_round == m - 1:
        // Previous round was one short of success
        // If I played C last round, it would have been m.
        // So, if cooperators_last_round == m-1 AND I played C: -> C (make it happen)
        // If cooperators_last_round == m-1 AND I played D: -> D (I didn't help, why start now?)
        // This needs to be carefully defined. "If m or more players cooperate" means
        // if the count *including me* hits m.

        // Simplified logic: If previous round was m-1 cooperators, and I *wasn't* one of them:
        // Defect. If I *was* one of them, cooperate.
        // The pseudocode below does this implicitly with the m-1 check.

        // This is the "Calculated Risk" part. If we are close, we take the plunge.
        return "C"

    else: // cooperators_last_round < m - 1
        // Previous round fell significantly short. Too risky to cooperate.
        return "D"

    // Edge case: Last Round (r)
    if current_round == r:
        if cooperators_last_round >= m: // If second to last round was successful
            return "C" // Go for the final reward.
        else:
            return "D" // System failed, defect.

// Helper function examples (implementation details depend on actual game state representation)
function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == "C":
            count += 1
    return count

function get_my_action(actions_list):
    // Assuming actions_list for the previous round contains your action at a known index
    // This is a placeholder. The actual implementation will need access to your own history.
    return actions_list[my_player_index] // Placeholder
```

**Refined Pseudocode for Clarity:**

```pseudocode
// Strategy: Calculated Opportunist - Aggressive Version

function decide_action(game_parameters, history):
    n = game_parameters.n
    r = game_parameters.r
    m = game_parameters.m
    current_round = history.current_round

    // --- Round 1: Initial Aggression ---
    if current_round == 1:
        return "D" // Do not commit resources without information.

    // --- Analyze Previous Round ---
    previous_round_data = history.get_round_data(current_round - 1) // Contains actions of all players
    cooperators_in_previous_round = count_cooperators(previous_round_data.all_actions)
    my_action_in_previous_round = previous_round_data.my_action // Assuming history tracks own action

    // --- Final Round: Maximize Final Payoff ---
    if current_round == r:
        // If success was achieved in the second to last round, there's no future to lose.
        // Take the reward if the condition was met.
        if cooperators_in_previous_round >= m:
            return "C" // Secure the reward if success was possible.
        else:
            return "D" // System failed, defect.

    // --- Core Logic: Adaptive Cooperation based on Perceived Success Likelihood ---

    // Aggressive Exploitation Rule: If the system succeeded last round, exploit.
    // This is the most aggressive move. Assume others will continue to cooperate.
    if cooperators_in_previous_round >= m:
        return "D" // Benefit from others' cooperation without risk.

    // Calculated Risk: If the system was *one short* of success last round,
    // and IF my cooperation would have tipped the balance.
    // This means cooperators_in_previous_round == m - 1.
    // And critically, if I cooperated, then count would be m.
    // So, we cooperate IF the count was m-1 AND I was not the one missing.
    // The simplified rule: cooperate if we are ALMOST there.
    if cooperators_in_previous_round == m - 1:
        // This is the calculated risk to achieve the reward.
        return "C"

    // Default: If cooperation is unlikely to reach the threshold, defect.
    // This covers cases where cooperators_in_previous_round < m - 1.
    return "D"


// Helper functions (implementation depends on how 'history' is structured)
function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == "C":
            count += 1
    return count
```

This strategy is designed to be a formidable opponent. It's not inherently cooperative but uses cooperation as a tool for high rewards when the risk is minimized due to others' actions. When that safety net disappears or is exploited, it immediately reverts to pure self-interest.
'''

description_AGGRESSIVE_46 = '''
## Aggressive Strategy: "The Threshold Enforcer"

This strategy prioritizes maximizing individual payoff while strategically leveraging the collective reward. It is designed to be aggressive by pushing the boundaries of cooperation just enough to trigger the reward, but always with an eye on personal gain if the collective fails.

### Core Philosophy:

"I will contribute to the collective good *only when it is demonstrably beneficial to me*, and I will do so decisively to ensure the reward is achieved. If others falter, I will not be dragged down with them; I will secure my individual gains."

### 1. Decision Rules:

The decision to cooperate or defect in any given round will be based on a dynamic assessment of the probability that cooperation will lead to the collective reward, factoring in the observed behavior of other players.

*   **Cooperate (C) if:**
    *   **The current round is t, and the number of observed cooperators in the previous round (t-1) plus one (my potential cooperation) is greater than or equal to 'm'.** This is the primary trigger. I will cooperate if my single act of cooperation is *sufficient* to cross the cooperation threshold 'm'.
    *   **The current round is the *very first round* (t=1) AND the number of players 'n' is less than or equal to 'm'.** In this scenario, with a high likelihood of not reaching the threshold organically, a single cooperative act is a more strategic gamble to try and initiate the reward. *This is a rare, but aggressive, initial move.*
    *   **The current round is the *second to last round* (t = r-1) AND my cooperation would be the *only* missing player to reach 'm'.** This is a calculated risk to ensure the reward in the penultimate round, aiming to set a precedent for the final round and to lock in the reward for both rounds.
    *   **The current round is the *last round* (t=r) AND the number of observed cooperators in the previous round (t-1) plus one (my potential cooperation) is greater than or equal to 'm'.** This is to secure the reward in the final round.

*   **Defect (D) in all other cases.** This includes:
    *   If my cooperation would be *redundant* (i.e., the threshold 'm' is already met or exceeded without my cooperation in the previous round), I will defect to maximize my individual payoff.
    *   If the number of observed cooperators in the previous round is too low, such that even with my cooperation, the threshold 'm' would not be met, I will defect to preserve my endowment.
    *   If the game is in a state where I perceive a high likelihood of others defecting and thus failing to meet the threshold, even if my cooperation *could* technically lead to meeting it, I will defect. This is a defensive aggression – not risking my individual payoff on a losing proposition.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Aggressive Gambit:** If `n <= m`, I will cooperate. This is a bold move to try and kickstart cooperation when the odds are stacked against it naturally.
    *   **Cautious Approach:** If `n > m`, I will defect. This is a pragmatic decision. With a large group, it's unlikely my single cooperation will trigger the reward. I will observe and react.

*   **Last Round (t=r):**
    *   **Secure the Reward:** If `(observed_cooperators_in_round_r-1 + 1) >= m`, I will cooperate. My goal here is to ensure the reward is met in the final round, maximizing my total payoff.
    *   **Individual Maximization:** If `(observed_cooperators_in_round_r-1) >= m`, I will defect. I've already secured the reward, and now I want to maximize my individual take from the final round.
    *   **Protect Endowment:** If `(observed_cooperators_in_round_r-1 + 1) < m`, I will defect. There's no realistic chance of reaching the reward, so I protect my base endowment.

*   **Second to Last Round (t = r-1):**
    *   **Strategic Lock-in:** If `(observed_cooperators_in_round_r-2) == m - 1`, I will cooperate. This is a calculated move to guarantee the reward in this round, hoping to influence the final round's outcome positively.
    *   **Standard Logic:** In all other cases for the second to last round, I will follow the general decision rules.

### 3. Aggressive Mindset Alignment:

*   **Proactive Threshold Pushing:** I don't wait for others to initiate cooperation reliably. I'm willing to be the one to cross the threshold *if it's mathematically beneficial to me*. This is about influencing the outcome, not just reacting to it.
*   **"All or Nothing" on Rewards:** When I decide to cooperate to reach the threshold, it's with the full intention of securing that reward. This is not a tentative gesture; it's a commitment to achieve the higher payoff.
*   **Ruthless Self-Interest:** If the collective effort fails, I will not be penalized for my individual contribution. I will always prioritize my own endowment if the reward is not guaranteed.
*   **Information Exploitation:** My decisions are directly tied to observed history. This means I'm constantly evaluating the collective's "cooperative momentum" and using it to my advantage. I'm not stuck in a rigid pattern; I adapt aggressively.
*   **Calculated Risks:** The first-round gambit and the second-to-last-round guarantee are examples of calculated risks taken to maximize overall payoff in an aggressive manner.

### Pseudocode:

```
function DecideAction(current_round, r, n, m, k, history):
  // history is a list of lists, where history[t-1] contains actions of all players in round t-1
  // For the first round, history is empty.

  observed_cooperators_prev_round = 0
  if current_round > 1 and len(history) >= current_round - 1:
    // Count cooperators from the previous round (round = current_round - 1)
    prev_round_actions = history[current_round - 2] // Adjust index for 0-based list
    for action in prev_round_actions:
      if action == 'C':
        observed_cooperators_prev_round += 1

  my_potential_cooperation_count = observed_cooperators_prev_round + 1

  // --- Edge Case: First Round ---
  if current_round == 1:
    if n <= m:
      return 'C' // Aggressive gambit when the odds are low
    else:
      return 'D' // Observe and react

  // --- Edge Case: Last Round ---
  if current_round == r:
    if my_potential_cooperation_count >= m:
      return 'C' // Secure the reward
    else:
      return 'D' // Protect endowment if reward is not achievable

  // --- Edge Case: Second to Last Round ---
  if current_round == r - 1:
    observed_cooperators_two_rounds_ago = 0
    if r > 2 and len(history) >= r - 2:
      two_rounds_ago_actions = history[r - 3] // Adjust index for 0-based list
      for action in two_rounds_ago_actions:
        if action == 'C':
          observed_cooperators_two_rounds_ago += 1
      if observed_cooperators_two_rounds_ago == m - 1:
        return 'C' // Strategic lock-in

  // --- General Decision Rules ---
  // If my cooperation *guarantees* reaching the threshold
  if my_potential_cooperation_count >= m:
    return 'C'

  // If threshold is already met or exceeded without my cooperation, defect for max individual gain
  if observed_cooperators_prev_round >= m:
    return 'D'

  // If cooperation is unlikely to reach the threshold, defect
  return 'D'

```

This "Threshold Enforcer" strategy is designed to be aggressive by actively seeking to trigger the reward when it benefits the player, while ruthlessly protecting their endowment when the collective effort is likely to fail. It's about calculated opportunism, not blind cooperation.
'''

description_AGGRESSIVE_47 = '''
## Aggressive Strategy: "The Opportunistic Predator"

This strategy is designed to maximize individual payoff in the Collective Risk Dilemma by leveraging the incentives for cooperation while ruthlessly exploiting situations where others bear the risk. The core philosophy is to be a **conditional defector**, always looking for the opportune moment to reap the rewards of others' cooperation.

### Core Philosophy:

*   **Default to Defection:** My baseline is to defect. Cooperation is a calculated risk, not a default state.
*   **Exploit the Threshold:** The primary goal is to be in a position to benefit from the collective reward (`k`) when the `m` cooperation threshold is met, but without being the one to contribute the "risky" endowment if others are doing it.
*   **Punish Non-Cooperation:** If the collective fails to meet the threshold, I will not "bail out" the group by cooperating.
*   **Aggressive Posturing:** This strategy aims to influence the group's behavior through its own actions, signaling a willingness to defect and a desire to be part of a successful cooperative venture only when it's guaranteed to be profitable.

### 1. Decision Rules:

My decision to cooperate or defect in any given round `t` is based on a calculation of the *probability of success* and my *own expected payoff*.

**Key Metric: "Cooperation Deficit" (CD_t)**

This metric quantifies how many more players are needed to cooperate in round `t` to reach the threshold `m`.

*   `CD_t = max(0, m - C_t_minus_1)`
    *   `C_t_minus_1` is the number of players who cooperated in the *previous* round (t-1).

**Decision Logic:**

*   **If `CD_t` is 0 (i.e., `m` or more players cooperated in the previous round):**
    *   **Cooperate (C):** The threshold was met, and the collective reward is accessible. By cooperating, I ensure my participation in the next round's reward. This is the "opportunistic" part – I'm joining a winning group.
*   **If `CD_t` > 0 (i.e., fewer than `m` players cooperated in the previous round):**
    *   **Defect (D):** The threshold was not met. There's no guarantee of the collective reward, and cooperating would likely result in a payoff of `(1 - 1) = 0`. Defecting guarantees a payoff of `(1 - 0) = 1`. This is the "predatory" part – I'm not carrying the burden of a failed cooperative effort.

**Refinement for Aggression (Threshold Proximity):**

To be truly aggressive and push the boundaries of cooperation without bearing undue risk, we introduce a refinement when `CD_t` is greater than 0.

*   **If `CD_t` > 0 AND `CD_t` is *small* (e.g., `CD_t` <= (n - m) / 2):**
    *   **Cooperate (C):** This is the most aggressive element. If the deficit is small, it means only a few more players are needed. By cooperating *myself*, I'm signaling intent and potentially nudging others to do the same. This is a calculated risk to trigger the reward for *everyone*, including myself. The smaller the deficit, the higher the likelihood that my cooperation will tip the scales.
*   **If `CD_t` > 0 AND `CD_t` is *large* (e.g., `CD_t` > (n - m) / 2):**
    *   **Defect (D):** If the deficit is large, the chances of my single cooperation tipping the balance are low. I will stick to my default defection to secure my individual payoff of 1.

**Summary of Decision Rules (for round `t`):**

1.  Calculate `C_t_minus_1` (number of cooperators in round `t-1`).
2.  Calculate `CD_t = max(0, m - C_t_minus_1)`.
3.  **If `CD_t == 0`:** Cooperate (C).
4.  **Else (if `CD_t > 0`):**
    *   **If `CD_t <= (n - m) / 2` (small deficit):** Cooperate (C).
    *   **Else (if `CD_t > (n - m) / 2` (large deficit)):** Defect (D).

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Action: Defect (D).**
    *   **Rationale:** In the first round, there is no history. The "Opportunistic Predator" starts by maximizing its individual, risk-free payoff. There's no information to base any cooperative decision on, and my aggressive stance is to not be the first to contribute to an uncertain collective. This sets the tone that I am not a guaranteed cooperator.

*   **Last Round (t=r):**
    *   **Action: Defect (D).**
    *   **Rationale:** In the final round, there are no future consequences for individual actions. The incentive to build cooperation or punish defection disappears. Therefore, the purely self-interested, individual payoff of 1 from defecting is the optimal choice. The collective reward is no longer relevant for future interactions.

*   **Situations where `m=2` and `n=2`:**
    *   **Round 1:** Defect (D).
    *   **Subsequent Rounds:**
        *   If the opponent defected in the previous round (`C_t_minus_1 = 0`), `CD_t = 2`. Since `CD_t > (n-m)/2` (2 > (2-2)/2 = 0), I defect (D).
        *   If the opponent cooperated in the previous round (`C_t_minus_1 = 1`), `CD_t = 1`. Since `CD_t <= (n-m)/2` (1 <= 0 is false, but let's consider a slight adjustment for the n=2 case or a more direct calculation). If `CD_t` is 1, I *need* my cooperation to trigger the reward. I will Cooperate (C).
    *   **Revised Logic for n=2, m=2:**
        *   Round 1: D
        *   Round `t > 1`: If opponent cooperated last round, Cooperate (C). Else, Defect (D). (This is a simplified Tit-for-Tat with an aggressive twist: I only mirror cooperation if it *worked* for them, otherwise I defect).

*   **Situations where `m > n-1`:** This scenario is impossible given `1 < m < n`. However, if `m=n`, it would imply everyone must cooperate.
    *   **If `m=n`:**
        *   Round 1: Defect (D).
        *   Subsequent Rounds: If `C_t_minus_1 < n`, then `CD_t > 0`. If `CD_t` is large, I defect. If `CD_t` is small (only one player needed to cooperate), I cooperate. If `C_t_minus_1 = n`, I cooperate.

### 3. Aggressive Mindset Alignment:

*   **"The Opportunistic Predator"**: This name itself reflects the strategy's aggressive nature. It's not about building trust; it's about exploiting opportunities created by others.
*   **Default Defection**: Starting with defection in round 1 is an aggressive statement. It immediately signals a non-cooperative stance.
*   **Conditional Cooperation**: Cooperation is only offered when the threshold is *nearly* met or has *already* been met. This means I am never the first to bear the risk to establish cooperation. I am either joining a successful endeavor or making a final, calculated push to trigger it.
*   **No Forgiveness**: If the group fails to meet the threshold, I don't punish by defecting. I *always* defect in such cases, as there's no benefit to cooperating and only a guaranteed payoff of 1 from defecting.
*   **Exploiting Small Deficits**: The strategy of cooperating when the deficit is small is the most "aggressive" cooperative move. It's a calculated gamble to be the catalyst for collective gain, but only when the odds are favorable that my move will be decisive. It's about seizing control of the outcome when it's within reach.
*   **Unwavering in the Last Round**: The decision to defect in the last round is pure aggression. It prioritizes immediate, guaranteed individual gain over any abstract notion of collective good or reciprocity, which has no bearing on future outcomes.

### Pseudocode:

```python
def decide_action(player_id, current_round, game_history, parameters):
    n = parameters['n']
    r = parameters['r']
    m = parameters['m']
    k = parameters['k']

    # Handle First Round
    if current_round == 1:
        return 'D'

    # Handle Last Round
    if current_round == r:
        return 'D'

    # Calculate cooperators from previous round
    previous_round_actions = game_history.get(current_round - 1, {})
    cooperators_previous_round = 0
    for pid, action in previous_round_actions.items():
        if action == 'C':
            cooperators_previous_round += 1

    # Calculate Cooperation Deficit (CD)
    cooperation_deficit = max(0, m - cooperators_previous_round)

    # Decision Logic
    if cooperation_deficit == 0:
        # Threshold was met or exceeded, join the winning group
        return 'C'
    else:
        # Threshold was not met
        # Aggressive refinement: cooperate if deficit is small
        # The threshold for "small deficit" is (n - m) / 2.
        # We use a small buffer to ensure it's truly small.
        # If n-m is 0, this logic needs care. If m < n, then n-m > 0.
        # For n=2, m=2, n-m=0. For n=3, m=2, n-m=1. For n=4, m=3, n-m=1.
        # Let's use integer division, and if n-m is odd, the floor applies.
        # If n-m is 0, the threshold is 0.
        
        if n - m == 0: # This case is excluded by m < n, but as a safeguard.
            # If m = n, and deficit > 0, it means not everyone cooperated.
            # Cooperating here would be an attempt to achieve the impossible goal.
            # Based on aggressive mindset, we'd only do it if it's the *only* thing needed.
            if cooperation_deficit == 1:
                return 'C'
            else:
                return 'D'

        small_deficit_threshold = (n - m) // 2

        if cooperation_deficit <= small_deficit_threshold:
            # Cooperate to push for collective reward when it's close
            return 'C'
        else:
            # Defect, as cooperation is unlikely to make a difference
            return 'D'

```

This strategy is designed to be an "Opportunistic Predator," always seeking to profit from collective endeavors without bearing the upfront risk. It's aggressive in its default stance and its calculated attempts to trigger cooperation only when it's most likely to benefit itself.
'''

description_AGGRESSIVE_48 = '''
## Aggressive Dominance Strategy: The "Threshold Striker"

This strategy, "Threshold Striker," is designed to exploit the payoff structure of the Collective Risk Dilemma by being aggressively opportunistic while maintaining a strong incentive to achieve the cooperation threshold when it's most beneficial. It prioritizes maximizing personal gain by leveraging the increased reward for collective cooperation, but only when the risk of individual defection is outweighed by the potential for a superior payout.

**Core Philosophy:** "Threshold Striker" believes that while cooperation can yield high rewards, it is fundamentally a gamble. The strategy aims to be the player that *benefits* from others' cooperation by defecting when it's individually profitable and cooperating *only* when it's guaranteed to achieve the threshold and maximize its own payoff, or when a strategic necessity arises. It is not inherently "nice," but it is calculative.

---

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) in any given round `t` (where `t > 1`) is based on a dynamic assessment of the probability of reaching the cooperation threshold `m`, factoring in the perceived behavior of other players and the remaining rounds.

**Decision Rule for Round `t` (where `t > 1`):**

*   **Default Action: Defect (D)**

    "Threshold Striker" defaults to defecting. This is its aggressive stance – it expects others to do the same and aims to collect its private endowment (payoff of 1) while avoiding contributing to a project that might fail.

*   **Conditions for Cooperation (C):**

    "Threshold Striker" will cooperate *only* if one of the following conditions is met:

    1.  **Guaranteed Threshold Achievement:**
        *   Let `C_t-1` be the number of players observed to cooperate in the previous round (`t-1`).
        *   Let `N_remaining` be the number of rounds remaining in the game (including the current round `t`).
        *   If `C_t-1 >= m - 1` AND `N_remaining > 1`:
            *   **Rationale:** In this scenario, "Threshold Striker" recognizes that with only one more cooperation needed to reach the threshold `m`, and with multiple rounds remaining, cooperating now is almost certain to trigger the collective reward `k`. Since the reward `k` is significant (k > 1), and the cost of cooperating (`c_i = 1`) is offset by the reward, it's strategically beneficial to secure the higher payoff for all remaining rounds. This is a calculated risk to lock in a higher return.

    2.  **Last Round High-Probability Threshold:**
        *   If `t == r` (this is the last round) AND `C_t-1 >= m - 1`:
            *   **Rationale:** In the final round, there's no future to leverage. If the previous round saw enough cooperators such that *one more* player cooperating (which would be "Threshold Striker") guarantees the threshold, it will cooperate. This secures the `k` reward for this round, maximizing its final score.

    3.  **Strategic Sacrifice for Future Gain (Rare Occasion):**
        *   If `C_t-1 < m - 1` AND `N_remaining > 1` AND `(n - C_t-1) < m`:
            *   **Rationale:** This is a more aggressive, high-stakes maneuver. It's triggered when it's highly unlikely that the threshold will be met *without* "Threshold Striker" contributing. The number of players currently defecting (`n - C_t-1`) is such that even if all remaining players defect, the threshold `m` still won't be met. In this situation, "Threshold Striker" *might* choose to cooperate. This is a calculated gamble to try and "pull" the group towards cooperation and secure the `k` reward for itself and potentially others in future rounds. This is a "throw a pebble to get a boulder" play. The decision here can be further refined by looking at the proportion of defections. If `(n - C_t-1)` is only slightly larger than `m`, the probability of failure is high. The strategy will only cooperate here if it believes this single act has a high chance of triggering the `k` reward for a significant number of future rounds.

**Summary of Cooperation Conditions:**

"Threshold Striker" cooperates *only* when:
*   It can secure the `k` reward by being the *final* cooperator needed to reach `m`, and there are subsequent rounds to benefit from it.
*   It can secure the `k` reward in the *final* round by being the final cooperator.
*   (Rarely) It believes a single act of cooperation can salvage the collective reward for future rounds when otherwise it would be lost.

---

### 2. Handling Edge Cases

*   **First Round (`t = 1`):**
    *   **Always Defect (D).**
    *   **Rationale:** In the first round, there is no history. "Threshold Striker" has no information about opponent tendencies. It adopts its default aggressive stance, securing its private endowment (payoff of 1). There is no incentive to risk a contribution without any knowledge of others' likely actions.

*   **Last Round (`t = r`):**
    *   If `C_t-1 >= m - 1`: **Cooperate (C).** (As per Rule 2.2)
    *   Otherwise: **Defect (D).**
    *   **Rationale:** In the final round, the strategy only cooperates if it can guarantee the reward `k` for itself without any future consequences. Otherwise, it secures its guaranteed payoff.

*   **Insufficient History (`t <= r - m + 1` and `C_t-1 < m - 1`):**
    *   **Always Defect (D).**
    *   **Rationale:** If there are not enough rounds remaining for the cooperation threshold `m` to even be *possible* to reach with the current number of cooperators, and if "Threshold Striker" is not the one player needed to push it over the edge (as per Rule 2.1), it will defect. It won't waste effort on a lost cause.

---

### 3. Aggressive Mindset Alignment

The "Threshold Striker" strategy is inherently aggressive in the following ways:

*   **Default to Defection:** Its baseline behavior is to take for itself. It doesn't initiate cooperation.
*   **Exploitative Cooperation:** Cooperation is only initiated when it directly benefits "Threshold Striker" through a guaranteed or highly probable reward `k`. It doesn't cooperate out of altruism or a desire to build trust.
*   **Leveraging Others' Risk:** It actively waits for others to signal their intent to cooperate by observing the number of cooperators from previous rounds. It benefits from their risk-taking.
*   **Strategic Opportunism:** The rare cooperation in Rule 2.3 is a calculated aggressive move to *force* a situation where it can benefit from the collective reward, rather than passively accepting the loss of potential gains.
*   **No Forgiveness:** The strategy does not incorporate any "tit-for-tat" or forgiveness mechanisms. A defection by others does not inherently lead to retaliation, but rather a re-evaluation of the probability of future cooperation thresholds.

---

### Pseudocode Representation:

```
STRATEGY_NAME = "ThresholdStriker"

FUNCTION DECIDE_ACTION(current_round, total_rounds, m, n, observed_cooperators_last_round):
  // observed_cooperators_last_round: Number of players who cooperated in the previous round.

  // --- Handle Edge Cases ---
  IF current_round == 1:
    RETURN "D" // Always defect in the first round.

  IF current_round == total_rounds:
    IF observed_cooperators_last_round >= m - 1:
      RETURN "C" // Cooperate if it guarantees the threshold in the last round.
    ELSE:
      RETURN "D" // Defect if threshold not guaranteed in the last round.

  // --- Decision Rules for Intermediate Rounds ---
  remaining_rounds = total_rounds - current_round + 1

  // Condition 1: Guaranteed Threshold Achievement for future rounds
  IF observed_cooperators_last_round >= m - 1 AND remaining_rounds > 1:
    RETURN "C"

  // Condition 3: Strategic Sacrifice (Rare) - only if threshold is otherwise impossible
  // This is a more complex calculation. A simplified version:
  // If the current number of defectors (n - observed_cooperators_last_round)
  // is such that even if ALL remaining players defect, the threshold 'm' is still not met,
  // AND if there are enough rounds left to potentially benefit.
  // A more robust implementation would involve assessing the probability of success for this single act.
  // For simplicity in pseudocode, we'll make a strong heuristic:
  IF (n - observed_cooperators_last_round) >= m AND remaining_rounds > 1: // This implies m defections won't be enough
      // This heuristic is aggressive. It assumes that if defecting is insufficient,
      // ONE cooperation might just tip the scales for the future.
      // Further tuning would involve assessing the gap to 'm'.
      IF (n - observed_cooperators_last_round) > (m - 1): // If even all defectors would not be enough if m-1 cooperated
        RETURN "C"


  // Default: Aggressive Defection
  RETURN "D"

```
'''

description_AGGRESSIVE_49 = '''
## Aggressive "Calculated Escalation" Strategy for the Collective Risk Dilemma

This strategy is designed to be aggressive, prioritizing self-gain while leveraging the potential for high collective rewards. It's adaptive, learning from the history of play to maximize payoff in a dynamic environment. The core principle is to exploit opportunities for high payoffs when they arise, but also to push for those opportunities by taking calculated risks.

**Aggressive Mindset:** "I will push for success, but I won't be reckless. I'll punish defection and reward cooperation *if it benefits me*, but my primary goal is to secure the highest possible individual payoff, even if it means being the one who triggers the reward."

---

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round `t` depends on two primary factors: the current state of cooperation and the perceived likelihood of achieving the reward threshold `m`.

**Core Decision Logic:**

*   **Defect (D):**
    *   **Default Behavior:** If the number of observed cooperators in the *previous* round (`C_t-1`) is significantly below `m`, or if the game is in its early stages and there's no clear indication of coordination, I will defect. This is the baseline "safe" option, guaranteeing a payoff of at least 1.
    *   **Punishment for Defection:** If a player I previously cooperated with (or a player who has been a consistent cooperator) defects, I will defect in the current round. This is a signal of displeasure and a move to extract personal gain from their "mistake."
    *   **Last Resort:** In the final round (`r`), if the number of cooperators from the previous round is less than `m-1`, I will defect. This is to ensure I don't "waste" my endowment on a failed cooperative effort.

*   **Cooperate (C):**
    *   **Opportunity Seizure:** If the number of observed cooperators in the *previous* round (`C_t-1`) is close to `m` (specifically, `C_t-1 >= m - 1`), I will cooperate. This is where the aggression shines: I'm willing to take the step to *trigger* the reward if it's within reach. My cooperation, combined with the existing cooperators, should push us over the threshold.
    *   **Proactive Escalation:** If I believe there's a high probability that *at least* `m-1` other players will cooperate (based on historical trends and the current round's proximity to the reward), I might defect *strategically* to be one of the defectors who benefits from the *other players'* cooperation, thus maximizing my individual payoff. This is a more advanced, riskier play.
    *   **Reciprocity (Limited Aggression):** If a player has consistently cooperated and the number of cooperators in the previous round was `m-1` or more, and they cooperated, I will likely reciprocate by cooperating in the current round. This is a calculated move to maintain a successful cooperative equilibrium *if it benefits me*.

---

### 2. Handling Edge Cases

**Round 1 (t=1): The Initial Probe**

*   **Decision:** Defect (D).
*   **Rationale:** In the absence of any history, the safest and most aggressive initial move is to defect. This secures a payoff of 1 and allows me to gather information without investing my endowment. I want to see what others do before committing.

**The Second-to-Last Round (t=r-1): The Pre-Finale Gamble**

*   **Decision:** This is a critical decision point.
    *   If the number of observed cooperators in round `t-1` (`C_t-1`) is `m-1` or more, **Cooperate (C)**. I want to try and trigger the reward for the second-to-last time.
    *   If `C_t-1 < m-1`, **Defect (D)**. There's not enough momentum to reliably trigger the reward, so I secure my baseline.

**The Last Round (t=r): The Final Reckoning**

*   **Decision:**
    *   If the number of observed cooperators in round `t-1` (`C_t-1`) is `m-1` or more, **Cooperate (C)**. I'm making a final push to maximize my payoff in the very last round.
    *   If `C_t-1 < m-1`, **Defect (D)**. No point in trying to achieve a collective reward that's clearly not going to happen. Secure the individual payoff.

---

### 3. Aggressive Strategy Explained

**"Calculated Escalation"** is about pushing the boundaries of cooperation to extract the highest possible rewards.

*   **Exploiting Near Misses:** My strategy actively seeks to turn near-misses into successes. If `m-1` players cooperated last round, I'm the one who will tip the scales and secure the reward. This is aggressive because I'm willing to commit my endowment to achieve a higher payoff, assuming others' behavior suggests it's likely to succeed.
*   **Punishing Slackers:** If others defect when cooperation is within reach, I will defect in response. This isn't about altruism; it's about maximizing my own gain. If they don't contribute to a potential reward, I won't either, and I'll take my baseline payoff.
*   **Leveraging Information:** My strategy is adaptive. It constantly monitors the number of cooperators. This allows it to dynamically adjust its behavior based on the evolving game state.
*   **Controlled Risk:** While aggressive, it's not reckless. I only commit to cooperation when the probability of success (hitting `m` cooperators) is high, or when I'm strategically positioned to benefit from others' cooperation as a defector.
*   **Individual Maximization:** The ultimate goal is the highest *individual* payoff. If the game consistently trends towards defection and low payoffs, I will defect and secure my baseline. If there's a chance for high rewards through cooperation, I will be the one to push for it, or strategically position myself to benefit from it.

---

### Pseudocode Outline

```pseudocode
function decide_action(player_history, current_round, total_rounds, num_players, min_cooperators, reward_factor):
    num_cooperators_prev_round = count_cooperators_in_previous_round(player_history)
    num_players_i_cooperated_with = count_players_who_cooperated_with_me(player_history)

    // Edge Case: First Round
    if current_round == 1:
        return D

    // Edge Case: Last Round
    if current_round == total_rounds:
        if num_cooperators_prev_round >= min_cooperators - 1:
            return C
        else:
            return D

    // Aggressive Cooperation Trigger
    if num_cooperators_prev_round >= min_cooperators - 1:
        // If there's a strong signal of cooperation, I'll add to it
        return C

    // Default Defection (Safety and Information Gathering)
    // If not enough cooperators from previous round to trigger reward
    return D

    // More advanced, potentially aggressive considerations (for implementation):
    // - Assess individual player behavior: Who is consistently defecting? Who is cooperating?
    // - If a reliable cooperator defects: Defect (Punishment)
    // - If a reliable cooperator cooperates and C_prev >= m-1: Cooperate (Reciprocity & Reward)
    // - Calculate expected payoff: If defecting *now* would yield higher payoff than cooperating *next round*
    //   (this requires looking ahead and estimating opponent moves, which is complex for this pseudocode)
```

This strategy aims to be a formidable player, capitalizing on opportunities and punishing deviations, all while prioritizing the highest possible individual outcome.
'''

description_AGGRESSIVE_50 = '''
Here's an aggressive strategy designed for the Collective Risk Dilemma, focusing on maximizing personal gain while leveraging the game's structure.

## Aggressive Strategy: The Opportunistic Enforcer

This strategy is built on the principle of **opportunism with a punitive undertone**. It aims to exploit situations where cooperation is likely to succeed, but quickly punishes perceived freeloading. The core idea is to be an "enforcer" – pushing for cooperation when it's beneficial, but decisively defecting when others are not pulling their weight.

### 1. Decision Rules:

My decision in any given round will be based on a calculated "Cooperation Threshold" derived from the observed behavior of other players in previous rounds, and my own recent actions.

*   **Initial State (Round 1):** In the first round, there's no history. To signal a willingness to cooperate and probe the landscape, I will **Cooperate (C)**. This is a calculated risk to gauge the initial behavior of others.

*   **Subsequent Rounds (t > 1):**
    *   **Calculate the "Cooperation Status" (CS_t-1) from the previous round (t-1):**
        *   `CS_t-1 = (Number of players who cooperated in round t-1)`
    *   **Determine the "Enforcement Trigger":** This is a critical threshold that dictates when my aggressive "defect" mode kicks in. My default aggressive stance is to demand that others *at least* meet the minimum requirement for the reward.
        *   `Enforcement Trigger = m - 1` (If I cooperated in the previous round)
        *   `Enforcement Trigger = m` (If I defected in the previous round)

    *   **Decision Logic:**
        *   **If `CS_t-1` >= `Enforcement Trigger` AND I cooperated in round t-1:** This indicates that the group is *close* to or *at* the tipping point for success, and my cooperation is contributing to that. In this scenario, I will **Cooperate (C)**. I'm betting on others continuing to cooperate to secure the reward.
        *   **If `CS_t-1` >= `m` AND I defected in round t-1:** The group *still* met the threshold despite my defection. This is an opportunity to freeload. I will **Defect (D)**.
        *   **If `CS_t-1` < `Enforcement Trigger` AND I cooperated in round t-1:** The group is not cooperating enough, and my cooperation is being wasted. This signals a need to be more self-interested. I will **Defect (D)**.
        *   **If `CS_t-1` < `m` AND I defected in round t-1:** The group has failed to cooperate, and I have protected my endowment. This is a strong signal that the group is not reliably cooperative. I will **Defect (D)**.

*   **The "Last Resort" Defection:** A specific rule to handle situations where the group consistently fails to cooperate, even with my initial cooperation.
    *   **If I have cooperated for the last `X` rounds (where `X` is a parameter, e.g., 3 or 4) and the threshold `m` has *never* been met in those `X` rounds:** I will switch to a permanent **Defect (D)** mode. This signifies that the game is too unreliable for me to risk cooperation, and I will solely focus on maximizing my individual gains.

### 2. Handling Edge Cases:

*   **Round 1:** As mentioned, **Cooperate (C)**. This is my initial probe.

*   **Last Round (Round r):** In the very last round, the repeated nature of the game no longer provides future incentives for cooperation. Therefore, in the last round, regardless of history, my strategy becomes purely myopic:
    *   **If `CS_r-1` >= `m`:** The group is likely to achieve the reward even if I defect. I will **Defect (D)** to maximize my personal gain.
    *   **If `CS_r-1` < `m`:** The reward is not achievable. I will **Defect (D)** to secure my private endowment.
    *   **In essence, for the last round, it is always Defect (D).**

*   **Near-Failure Scenarios:** If `CS_t-1` is very close to `m` (e.g., `CS_t-1 == m - 1` or `m - 2`) and I cooperated in the previous round, I will lean towards cooperation to try and push the group over the edge, but with a stronger tendency to switch to defection in the *next* round if they don't step up. The `Enforcement Trigger` logic addresses this.

*   **Sustained Cooperation Failure:** The "Last Resort" Defection rule (explained above) handles scenarios where cooperation consistently fails.

### 3. Being Aggressive:

*   **Opportunistic Exploitation:** My strategy aggressively seeks out opportunities where cooperation is likely to succeed. By cooperating initially, I signal a willingness to contribute to the reward. If others reciprocate, I continue to cooperate to secure the higher payoff.

*   **Punitive Defection:** The core of the aggression lies in my immediate switch to defection when my cooperation appears to be exploited. If the threshold `m` is met *while I'm cooperating*, but the number of cooperators was only just enough (i.e., `CS_t-1 == m`) and I was cooperating, I will still defect in the next round if the conditions are right to freeload. This sends a clear signal: "I will help get us the reward, but if you don't pull your weight sufficiently, I will exploit your efforts."

*   **Low Tolerance for Free-riding:** The `Enforcement Trigger` is designed to be somewhat demanding. If I cooperate, I expect a decent level of contribution from others. If they don't deliver, I immediately penalize them by withholding my cooperation and benefiting from their contributions (if they still manage to meet the threshold).

*   **The "Last Resort" is a Declaration of War:** The permanent defection rule is an aggressive declaration that the game is no longer worth playing cooperatively from my perspective. I am maximizing my guaranteed individual payoff.

### Pseudocode:

```pseudocode
// Game Parameters: n, r, m, k

// Strategy State Variables:
// my_last_action: 'C' or 'D' (initialized to 'C' for Round 1)
// cooperation_count_history: list of cooperation counts from previous rounds

function decide_action(round_number, previous_round_actions, my_previous_actions):
    // previous_round_actions is a list of actions taken by all players in the previous round.
    // my_previous_actions is a list of my actions in all previous rounds.

    // Count cooperators in the previous round
    cooperators_last_round = 0
    for player_action in previous_round_actions:
        if player_action == 'C':
            cooperators_last_round += 1

    // Track my recent cooperation attempts
    my_recent_cooperation_count = 0
    for i from 1 to min(3, round_number - 1): // Look back up to 3 rounds
        if my_previous_actions[-i] == 'C':
            my_recent_cooperation_count += 1

    // --- Decision Logic ---

    // Last Round: Purely selfish
    if round_number == r:
        return 'D'

    // Round 1: Probe with cooperation
    if round_number == 1:
        my_last_action = 'C'
        return 'C'

    // Last Resort Defection: If sustained cooperation fails
    if my_recent_cooperation_count >= 3 and cooperators_last_round < m for all of the last 3 rounds:
        my_last_action = 'D'
        return 'D'

    // Calculate Enforcement Trigger based on my last action
    enforcement_trigger = 0
    if my_last_action == 'C':
        enforcement_trigger = m - 1
    else: // my_last_action == 'D'
        enforcement_trigger = m

    // Core Decision Rules
    if cooperators_last_round >= enforcement_trigger:
        if my_last_action == 'C':
            // Group is close/at threshold, and I cooperated. Push for more.
            my_last_action = 'C'
            return 'C'
        else: // my_last_action == 'D'
            // Group met threshold despite my defection. Freeload.
            my_last_action = 'D'
            return 'D'
    else: // cooperators_last_round < enforcement_trigger
        // Group is not cooperating enough. Defect to protect endowment.
        my_last_action = 'D'
        return 'D'

// Update my_last_action after the decision is made and the round concludes.
// The 'my_last_action' state variable needs to be maintained across rounds.
```

This strategy is aggressive because it's not afraid to defect, especially when it perceives opportunities to benefit from others' efforts or when sustained cooperation attempts are met with failure. It's adaptive due to its reliance on observed history and robust because it has a fallback mechanism for consistently uncooperative environments.
'''

description_AGGRESSIVE_51 = '''
## Aggressive Dominance Strategy

This strategy aims to maximize individual payoff by leveraging the cooperative reward while minimizing the risk of cooperating when others defect. It prioritizes defecting unless a clear and immediate path to guaranteed collective reward exists, then it aggressively seizes that opportunity. The core principle is to extract value from cooperation without becoming a "sucker."

### 1. Decision Rules

My decision to Cooperate (C) or Defect (D) in round *t* depends on the following factors:

*   **Current Round Threshold:** The number of players still needed to cooperate for the reward to be triggered.
*   **Perceived Player Cooperation Likelihood:** An estimation of how likely other players are to cooperate, derived from historical data.

**Core Decision Logic:**

*   **Default Action: Defect (D).** The safest and most individually rewarding action when there's uncertainty is to keep your endowment.
*   **Cooperate (C) ONLY IF:**
    *   **Immediate Reward is Guaranteed:** In round *t*, if the number of players who have *already* cooperated in round *t* (observed from the current round's partial information, if available, or from past rounds if the game structure allows for real-time observation of ongoing round actions – assuming the latter for aggressive adaptation) *plus* the number of players expected to cooperate based on historical data *plus 1* (myself) is greater than or equal to *m*.
    *   **The "Last Chance" Scenario:** In the final round (*r*), if the number of players who cooperated in the previous round (*r-1*) was *m-1*, and I have observed *no* information suggesting a widespread shift to defection (e.g., no sudden drops in observed cooperation rates), I will cooperate to try and trigger the reward one last time. This is a calculated gamble.

**Aggressive Tendency:** I will err on the side of caution and defect unless the probability of triggering the reward is extremely high and the individual risk is minimal. The "guaranteed reward" condition above is the primary driver for cooperation.

**Adaptation Mechanism:**

*   **Cooperation Count Tracking:** Maintain a running count of how many players (excluding myself) cooperated in each previous round.
*   **Average Cooperation Rate Calculation:** Calculate the average cooperation rate across all rounds played so far.
*   **Trend Analysis (Simplified):** Look for drastic drops in cooperation rates in recent rounds. If there's a sudden and significant decrease, it signals an increased risk of others defecting, reinforcing the default to defect.

**Pseudocode for Decision in Round *t* (for player *i*):**

```
function decide_action(current_round, history):
    # Default to Defect
    my_action = 'D'

    # Calculate how many players cooperated in the previous round (r-1)
    # If this is the first round, this will be 0 or based on a hypothetical starting state.
    previous_round_cooperators = count_cooperators(history, current_round - 1) # Function to count cooperators in a given round from history

    # Calculate the number of players needed to cooperate in this round
    players_needed_for_reward = m

    # Estimate the likelihood of others cooperating (simplified for now, more complex models can be added)
    # For this strategy, we'll focus on immediate triggers rather than complex predictions.

    # Scenario 1: Triggering the reward THIS round
    # This assumes we can observe partial actions within the current round if the system allows for it.
    # If not, we rely on past round data. Let's assume we can observe *some* current round actions.
    current_round_observed_cooperators = count_observed_cooperators_this_round(history) # Function to count cooperators observed so far in current_round

    # Number of players *guaranteed* to cooperate (from observation) + me
    potential_cooperators_this_round = current_round_observed_cooperators + 1

    # If there are enough players already cooperating *or* if it's very likely we'll reach m
    # This is the aggressive seize-the-reward condition.
    # A more robust version would use probability estimates, but for "aggressive" we focus on certainty.
    if potential_cooperators_this_round >= players_needed_for_reward:
        my_action = 'C'

    # Scenario 2: The "Last Chance" Gamble in the Final Round
    if current_round == r and previous_round_cooperators == m - 1:
        # Check if there's clear evidence of mass defection in recent rounds
        recent_defection_trend = check_recent_defection_trend(history, num_recent_rounds=2) # Function to check for drops
        if not recent_defection_trend:
            my_action = 'C' # Aggressively try to trigger the reward one last time

    # If my_action is still 'D', we stick with it.

    return my_action

# Helper functions (conceptual)
# def count_cooperators(history, round_number): ... returns integer
# def count_observed_cooperators_this_round(history): ... returns integer (based on partial info if available)
# def check_recent_defection_trend(history, num_recent_rounds): ... returns boolean
```

### 2. Handling Edge Cases

*   **Round 1:** In the first round, there is no history. The default action is **Defect (D)**. There's no information to suggest cooperation is safe or beneficial. We will observe the outcomes of this round to inform future decisions.
*   **Last Round (Round *r*):** This is where the "Last Chance Gamble" comes into play. If the previous round ended with *m-1* cooperators, and there isn't a clear recent trend of defection, I will cooperate. This is a final push to secure the reward. If *m-1* cooperators were not met in the second-to-last round, I will defect, as the reward is no longer achievable for this round and the game is ending.
*   **Parameters:**
    *   **High *m* relative to *n*:** If *m* is very close to *n* (e.g., *m = n-1*), cooperation is much harder to achieve. My strategy will lean even more heavily towards defecting, as the probability of meeting the threshold is low. I will only cooperate in extreme, near-certain scenarios.
    *   **Low *m* relative to *n*:** If *m* is small (e.g., *m=2*), cooperation is easier. My aggressive strategy will still prioritize defecting, but the threshold for cooperation might be met more often based on observed actions, leading to more opportunistic cooperation.
    *   **High *k*:** A high *k* incentivizes cooperation. My strategy will still be cautious but will be more ready to seize opportunities when they arise. The "guaranteed reward" condition becomes more attractive.
    *   **Low *k*:** A low *k* reduces the incentive for cooperation. My strategy will become even more risk-averse, and the default to defect will be even stronger.

### 3. Being Aggressive

This strategy is aggressive in the following ways:

*   **Default to Defect:** It prioritizes self-preservation and immediate individual gain. It doesn't assume goodwill or attempt to foster cooperation.
*   **Seize Opportunities:** It aggressively cooperates *only* when the reward is practically guaranteed in the current round or as a final gambit in the last round. It's not about building trust; it's about exploiting a guaranteed payoff.
*   **Exploitative Potential:** If other players are consistently cooperating and getting close to the threshold, my strategy will defect until the very last moment when cooperation is guaranteed. This allows me to free-ride on their efforts and capture the reward with minimal personal risk.
*   **No Signaling or Trust Building:** The absence of communication means I cannot afford to signal intent or try to coordinate. My actions are purely self-interested, designed to maximize my outcome given the observed behavior of others.

In essence, this strategy is the "calculating opportunist." It observes, waits for a clear signal of opportunity, and then strikes decisively to claim the reward. It is not afraid to defect, and it only cooperates when the numbers clearly, undeniably, and immediately favor a positive outcome for itself.
'''

description_AGGRESSIVE_52 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the cooperative payoff when advantageous, and punishing defection when it's detrimental.

## Aggressive Strategy: "The Opportunistic Enforcer"

This strategy aims to be a dominant player by:

1.  **Exploiting the reward:** Actively seeking the higher payoff when cooperation is likely to succeed.
2.  **Punishing defection:** Discouraging opponents from defecting by making them bear the cost of failed cooperation.
3.  **Maintaining flexibility:** Adapting based on observed behavior to exploit weaknesses and avoid exploitation.

### Core Philosophy: "Leverage success, punish failure."

We will cooperate when the probability of reaching the cooperation threshold is high, aiming for the boosted payoff. Conversely, we will defect when cooperation seems unlikely or when others have repeatedly defected, to secure our individual endowment and penalize the group for failing to cooperate.

### 1. Decision Rules

Our decision in any round `t` is primarily driven by our assessment of the likelihood of reaching the `m` cooperation threshold and the recent behavior of other players.

**Decision Logic:**

*   **High Confidence in Cooperation:** If, based on the history, it appears that `m` or more players (including ourselves) are likely to cooperate, we will **Cooperate (C)**.
*   **Low Confidence in Cooperation / High Defection Rate:** If it seems unlikely that the `m` threshold will be met, or if a significant number of players have been defecting recently, we will **Defect (D)**.
*   **Adaptive Punishment/Reward:** We will use our actions to reinforce desired behavior (cooperation) and punish undesired behavior (defection).

**Specific Thresholds and Calculations (Internal Metric):**

Let's define an internal metric, `cooperation_likelihood`, which represents our estimate of the probability that `m` or more players will cooperate in the current round. This will be a dynamic value that updates based on past rounds.

*   **`cooperation_likelihood` Update:**
    *   In each round `t`, observe the actions of all `n` players. Let `cooperators_last_round` be the number of players who cooperated in round `t-1`.
    *   Our internal estimate `cooperation_likelihood` will be a weighted average of past success. A simple approach is to track the observed cooperation rate in the last `W` rounds (where `W` is a parameter, e.g., 3-5 rounds).
    *   `cooperation_likelihood` = (Average number of cooperators in the last `W` rounds) / `n`.
    *   Alternatively, a more aggressive approach would be to only consider the most recent round, or even the behavior of a subset of players known to be more cooperative.

*   **Decision Rule:**
    *   **IF** `cooperation_likelihood` * `n` >= `m`  (meaning our estimate suggests the threshold will be met or exceeded)
        *   **AND** (recent defection count from *other* players < a `punishment_threshold`)
        *   **THEN** **Cooperate (C)**.
    *   **ELSE IF** (recent defection count from *other* players >= `punishment_threshold`)
        *   **THEN** **Defect (D)**.
    *   **ELSE** (defaulting to a more cautious, but still aggressive, stance)
        *   **THEN** **Defect (D)**.

**Refining the `punishment_threshold`:** This threshold determines how many defections trigger our retaliatory defection. A lower threshold means we are quicker to punish. Let's set `punishment_threshold` to a value slightly less than `m` (e.g., `m - 2` or `m - 1`), to ensure we don't prematurely defect if only a few players are holding back cooperation.

**The "Aggressive" Twist:** Our primary goal is to *always* try and capture the `k` reward. We only defect if we strongly suspect cooperation will fail *or* if other players are consistently defecting, thus depriving us of the reward and potentially punishing them for it.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Strategy:** **Cooperate (C)**.
    *   **Rationale:** In the absence of any history, the most aggressive move to *instigate* cooperation and test the waters for the `k` reward is to cooperate. We are betting on the possibility that others might also try to cooperate initially. This is a calculated risk to get the higher payoff. If this fails, the subsequent rounds will adjust.

*   **Last Round (t=r):**
    *   **Strategy:** **Defect (D)**.
    *   **Rationale:** In the final round, there are no future consequences for our actions. The highest guaranteed payoff is by keeping our endowment (1). There's no incentive to risk cooperation when the reward cannot be carried forward. This is a purely self-interested, aggressive move to secure the individual gain.

*   **Rounds Before the Last (1 < t < r):**
    *   The general decision rules outlined in section 1 apply.

### 3. Aggressive Stance Explained

*   **Proactive Cooperation:** We don't wait for others to show strong signs of cooperation. We initiate it in the first round, and continue as long as our internal assessment indicates a reasonable chance of success. This maximizes our opportunity to earn the `k` reward.
*   **Punitive Defection:** If others are not cooperating, and we believe they are acting selfishly, we will defect. This serves two purposes:
    1.  **Self-Preservation:** We secure our endowment of 1.
    2.  **Deterrence/Punishment:** We contribute to the failure of the cooperation threshold, potentially lowering their payoff (especially if they cooperated expecting the reward). This can teach opponents that defection has consequences, even in a game without explicit communication.
*   **No Forgiveness (Immediate):** We are not overly forgiving. If the cooperation threshold fails due to defections, we will likely defect in the next round if our `cooperation_likelihood` remains low or if defectors are prevalent. We will "re-evaluate" cooperation based on observed consistent cooperation from the group.
*   **Exploiting the Reward:** The strategy is fundamentally designed to aim for the `k` reward whenever it seems achievable. We are not passively waiting; we are actively seeking the optimal outcome.

### Pseudocode Representation

```
// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor
W: lookback window for cooperation_likelihood (e.g., 3)
punishment_threshold: threshold for triggering retaliatory defection (e.g., m - 1)

// State Variables
history: a list of lists, where history[t-1] contains actions of all players in round t-1
my_action: my action in the current round
opponent_actions_last_round: actions of all other players in the previous round

// Function to determine action in round t
function decide_action(t, history):
    // Handle Edge Cases
    if t == 1:
        return C  // Cooperate in the first round to test the waters
    if t == r:
        return D  // Defect in the last round for guaranteed payoff

    // Calculate cooperation_likelihood based on history
    recent_cooperation_count = 0
    if length(history) > 0:
        // Consider the last W rounds, or fewer if game hasn't reached W rounds
        num_rounds_to_consider = min(W, length(history))
        for round_idx from 1 to num_rounds_to_consider:
            round_actions = history[length(history) - round_idx]
            cooperators_in_round = count_cooperators(round_actions)
            recent_cooperation_count += cooperators_in_round

    // Average cooperation from observed rounds in the window
    // If no history, assume 0 cooperation for calculation
    average_cooperators_in_window = (num_rounds_to_consider > 0) ? (recent_cooperation_count / num_rounds_to_consider) : 0
    cooperation_likelihood_score = average_cooperators_in_window // Absolute count is more useful here

    // Count recent defections from *other* players
    recent_defections_others = 0
    if length(history) > 0:
        opponent_actions_last_round = history[length(history) - 1]
        for player_idx from 1 to n:
            if player_idx != my_player_id and opponent_actions_last_round[player_idx] == D:
                recent_defections_others += 1

    // Decision Rules
    // Rule 1: High confidence in cooperation AND not too many recent defections from others
    if cooperation_likelihood_score + 1 >= m: // Adding 1 for our own cooperation
        if recent_defections_others < punishment_threshold:
            return C
    // Rule 2: High number of recent defections from others
    if recent_defections_others >= punishment_threshold:
        return D

    // Default aggressive stance: if not confident in cooperation, defect
    return D

// Helper function to count cooperators in a list of actions
function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == C:
            count += 1
    return count

// To implement 'my_player_id', a unique identifier for this strategy instance would be needed.
// If not available, 'my_action' could be determined by always assuming self is the 'last' player in the list for analysis.
```

This strategy is designed to be opportunistic and punitive. It aims for the higher payoff by cooperating when it seems feasible, but it will aggressively defect to secure its own gains and to punish players who seem to be consistently defecting and preventing the group from achieving the reward. Its aggressiveness is tempered by a learned assessment of the situation, making it adaptive to a range of opponent behaviors.
'''

description_AGGRESSIVE_53 = '''
This is a fascinating game with a clear incentive to defect for individual gain, but a collective benefit from cooperation. To design an aggressive strategy that is adaptive and robust, we need to balance the desire for personal gain with the potential for collective reward, all while acknowledging the inherent risk.

My aggressive strategy, which I'll call "Calculated Opportunist," is designed to exploit opportunities for high individual payoffs while still pushing towards collective success when the odds are favorable. It's about being proactive and seizing the moment, rather than passively reacting.

## Calculated Opportunist Strategy

**Core Philosophy:** Be a conditional cooperater who aggressively pushes for the reward threshold to be met, but is ready to pocket personal gains if cooperation falters. The strategy aims to maximize its own payoff by being a *necessary* cooperator when it's most beneficial, and a selfish defector otherwise.

### 1. Decision Rules: When to Cooperate vs. Defect

My decision to Cooperate (C) or Defect (D) in any given round `t` will be based on a calculation of the **"Cooperation Necessity Index" (CNI)**. This index quantifies how much my cooperation is needed to reach the cooperation threshold `m`.

**Calculation of Cooperation Necessity Index (CNI) for player `i` in round `t`:**

Let `N_C(t-1)` be the number of players who cooperated in the previous round (`t-1`).
Let `N_D(t-1)` be the number of players who defected in the previous round (`t-1`).

1.  **If `N_C(t-1) >= m`:** The cooperation threshold was met in the previous round. My cooperation is no longer *critically* needed for the collective reward. In this scenario, I will **Defect (D)**. This maximizes my individual payoff, as I can keep my endowment while still benefiting from the reward (assuming at least `m-1` others still cooperate).

2.  **If `N_C(t-1) < m`:** The cooperation threshold was *not* met. My cooperation becomes more valuable. The CNI will be calculated as follows:
    *   **CNI = `m` - `N_C(t-1)`**

    This CNI represents the number of *additional* cooperators needed to reach the threshold.

    Now, we combine the CNI with a risk assessment of my own actions and the likely actions of others.

    *   **Cooperate (C) if:**
        *   The CNI is "low" and the risk of my cooperation being in vain is manageable. Specifically, I will cooperate if:
            **CNI <= `my_cooperation_threshold`**
            Where `my_cooperation_threshold` is a parameter that balances my aggression with risk. A lower threshold means I'm more likely to defect. For an aggressive strategy, this threshold should be set relatively low. Let's define it as:
            **`my_cooperation_threshold` = `floor((n - m) / 2)`** (This is a starting point, can be tuned).
            *Rationale:* If only a small number of additional cooperators are needed, and my decision to cooperate could be the deciding factor, I will take that leap.

    *   **Defect (D) if:**
        *   The CNI is "high" (meaning `CNI > my_cooperation_threshold`) and the probability of my cooperation alone pushing us over the threshold is low, or if there's a high risk of being the *only* cooperator and not achieving the reward.
        *   I have a strong indication from previous rounds that a sufficient number of players are *likely* to cooperate regardless of my action. This is harder to quantify without deeper opponent modeling, so for now, we stick to the CNI.

**Strategic Nuance for Aggression:**

My aggressive stance is reflected in how I *interpret* the CNI. I am willing to be the one to push the button for cooperation, but only if I believe my contribution is genuinely crucial and likely to be successful. I'm not a passive optimist; I'm an active catalyst when it makes sense.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Action: Cooperate (C)**
    *   *Rationale:* There is no history. To establish a precedent and encourage cooperation, I will take the initial risk. This is the most aggressive stance in the absence of information – it signals intent to contribute to collective success. It also maximizes the chance of the reward being met from the outset, which benefits everyone.

*   **Last Round (t=r):**
    *   **Action: Defect (D)**
    *   *Rationale:* In the final round, there is no future to build upon. The future discounted payoff is zero. Therefore, the dominant strategy for any player is to maximize their immediate individual payoff. Since the reward from a collective project is no longer relevant for future rounds, defecting is the purely rational and aggressive move to secure the highest possible personal gain.

*   **Rounds where `m=n` (all players must cooperate):**
    *   This is a highly unlikely scenario given the parameter `1 < m < n`. However, if it were to occur, my strategy would behave as follows:
        *   **First Round:** Cooperate.
        *   **Subsequent Rounds:** Defect if `N_C(t-1) < n`. Cooperate if `N_C(t-1) == n`. (This is a minor adjustment – if everyone else *did* cooperate, I'll match them to secure my reward).
        *   *Rationale for defecting if `N_C(t-1) < n`*: The probability of all `n` players cooperating is extremely low. It's more likely that trying to achieve this will result in `N_C(t-1) < n`, in which case my cooperation is pointless. Defecting maximizes my payoff in a situation where collective success is improbable.

*   **Rounds where `m=2` and `n=3` (very sensitive to one cooperator):**
    *   **CNI Calculation:** `m - N_C(t-1)`.
    *   **`my_cooperation_threshold`**: `floor((3 - 2) / 2) = 0`.
    *   **Decision Logic:** I will cooperate if CNI <= 0. This means I will only cooperate if `m - N_C(t-1) <= 0`, i.e., `N_C(t-1) >= m`. If the threshold was met, I'll defect. If it wasn't met (CNI > 0), I will defect. This aggressive strategy will only cooperate if the threshold has *already* been met by others. This makes me a very opportunistic defector when `m` is low and close to `n`. This is aggressive because I'm not taking risks for collective good unless it's guaranteed.

### 3. Aggressive Mindset Alignment

*   **"Calculated Opportunist":** The name itself reflects the strategy. I am opportunistically looking for the best personal gain, but my actions are calculated and not reckless.
*   **First Round Cooperation:** This is a calculated risk to *potentially* unlock immediate gains for everyone, including myself, if others reciprocate. It's an aggressive opening move, not a passive one.
*   **Defecting when threshold is met:** This is aggressive because I am maximizing my individual payoff in a situation where the collective good is already secured by others. I'm not sharing the burden of cooperation when it's no longer necessary for me to do so to get the reward.
*   **Low `my_cooperation_threshold`:** By setting the threshold for my own cooperation relatively low, I am signaling a willingness to be the decisive cooperator *when it truly matters*. I am not waiting for certainty; I'm willing to push when the gap is small.
*   **Last Round Defection:** This is the ultimate aggressive, self-interested move. It acknowledges that in a finite game, the long-term collective benefit is irrelevant.

## Pseudocode Illustration:

```python
# Parameters (assumed to be global or passed in)
# n: number of players
# r: number of rounds
# m: minimum cooperators needed
# k: reward factor

# Strategy State (maintained across rounds)
my_history = [] # List of own actions (C/D)
opponent_actions_history = [] # List of lists, each inner list is opponent actions in a round

def get_my_cooperation_threshold(n, m):
    # Aggressive threshold: willing to push if a small number of additional cooperators are needed
    return floor((n - m) / 2) # Can be tuned, e.g., 0 for maximum aggression

def decide_action(current_round, own_history, all_opponents_actions_history):
    n_cooperators_last_round = 0
    if current_round > 1:
        # Count cooperators from the *previous* round's actions
        # Note: For this specific strategy, we only need the *count* of cooperators
        # not the specific actions of others beyond that count.
        # A more complex strategy might analyze individual opponent behavior.
        num_players_in_prev_round = len(all_opponents_actions_history[-1]) # Assuming accurate history
        # For this strategy, we need the total number of cooperators, not just specific players
        # This requires knowing how many played C. The game spec implies we know this.
        # Let's assume a 'num_cooperators_in_round(round_index)' function exists that returns the count.
        n_cooperators_last_round = num_cooperators_in_round(current_round - 1)


    # Edge Case: Last Round
    if current_round == r:
        return "D" # Aggressively defect for max immediate gain

    # Edge Case: First Round
    if current_round == 1:
        return "C" # Aggressively initiate cooperation

    # Main Decision Logic based on CNI
    if n_cooperators_last_round >= m:
        # Threshold was met. I don't need to cooperate to get reward.
        return "D" # Aggressively defect for maximum personal payoff
    else:
        # Threshold was not met. Calculate CNI.
        cni = m - n_cooperators_last_round
        my_aggression_threshold = get_my_cooperation_threshold(n, m)

        if cni <= my_aggression_threshold:
            # Cooperation is needed and I'm willing to make the push.
            return "C"
        else:
            # Too many cooperators needed, or risk is too high.
            return "D" # Aggressively defect to avoid risk and secure personal gain.

# Helper function to get the total number of cooperators in a past round
# This is a placeholder. The actual implementation would depend on how
# round history is stored and made available.
def num_cooperators_in_round(round_index):
    # In a real implementation, this would access stored history of actions for round_index
    # and count the 'C's.
    # For the purpose of defining the strategy, we assume this information is available.
    pass

```

This "Calculated Opportunist" strategy aims to be a formidable opponent by being proactive, opportunistic, and ruthlessly rational when the game concludes. It doesn't shy away from taking calculated risks to secure rewards but is equally quick to capitalize on opportunities for personal gain.
'''

description_AGGRESSIVE_54 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff while taking calculated risks.

## Strategy: "Calculated Aggression"

This strategy is built on the principle of **defecting by default, but cooperating strategically when the reward outweighs the risk, with a strong bias towards exploiting cooperative behavior from others.**

### 1. Decision Rules: When to Cooperate vs. Defect

The core of this strategy is a threshold-based system that assesses the immediate likelihood of success in achieving the cooperative payoff.

*   **Default Action: Defect (D)**. Unless specific conditions are met, the default action is to defect. This prioritizes securing the individual endowment of 1 unit.

*   **Cooperate (C) Trigger Conditions:** Player `i` will choose to cooperate *only if* one of the following conditions is met:

    1.  **Proximity to Threshold (Strong Signal):** If the number of players who *cooperated in the previous round* (`num_cooperators_prev_round`) plus the *number of players you predict will cooperate this round* (explained below) is greater than or equal to `m`.
        *   **Prediction of Cooperators:** To estimate the number of other players who will cooperate, we will look at the **average cooperation rate of *other players* (excluding ourselves) over the last `min(5, t-1)` rounds**, where `t` is the current round. Let this average be `avg_other_cooperation`. We will then estimate `predicted_other_cooperators = avg_other_cooperation * (n-1)`.
        *   **Trigger Condition 1 Formalized:** `(num_cooperators_prev_round - 1) + floor(predicted_other_cooperators) + 1 >= m` (we add 1 for our own potential cooperation).

    2.  **Exploitation Opportunity (High Reward):** If the number of players who cooperated in the previous round was *at least* `m`, meaning the collective reward was achieved, and we defected in that round (meaning we got the higher payoff of `1 + k`), we should strongly consider cooperating in the current round. The logic is that if the group can successfully cooperate, and we benefited from it while defecting, we can now risk our own cooperation to secure the higher payoff for ourselves while still contributing to the collective success.
        *   **Trigger Condition 2 Formalized:** `(num_cooperators_prev_round >= m)` AND `(our_action_in_prev_round == D)`.

*   **If neither of the above conditions is met, player `i` defects (D).**

### 2. Handling Edge Cases

*   **Round 1 (t=1):** In the first round, there is no history. To be aggressive and explore the environment, the strategy will initially **defect (D)**. This is the safest bet to secure an initial payoff and gather information. We assume a baseline of "no one will cooperate" until proven otherwise.

*   **Last Round (t=r):** In the final round, there is no future to incentivize cooperation. Therefore, the strategy will **always defect (D)**. The goal is to maximize the final individual payoff.

*   **Near the Threshold (m):** The strategy is specifically designed to be sensitive to reaching `m`. The "Proximity to Threshold" rule is the primary mechanism for this. If the predicted number of other cooperators plus our own cooperation gets us to `m`, we will cooperate. This is where the "calculated aggression" comes in – we're willing to take the risk if the group is likely to succeed.

*   **When the Group Consistently Fails to Meet `m`:** If the historical data shows that the number of cooperators rarely reaches `m`, the `predicted_other_cooperators` will be low, and the strategy will default to defecting. It's not beneficial to risk cooperation if the group is unlikely to achieve it.

### 3. Being Aggressive

This strategy is aggressive in the following ways:

*   **Default to Defection:** The default is to protect your own endowment. This is an aggressive stance against exploitation by others. You demand that others initiate cooperation to entice you.
*   **Calculated Risk-Taking:** When the probability of success is high (i.e., `m` is likely to be met), the strategy is willing to cooperate. This isn't naive cooperation; it's a strategic investment when the reward (`k`) is substantial and the risk is mitigated by predicted other cooperators.
*   **Exploitation of Success:** Trigger Condition 2 is explicitly designed to exploit successful cooperative rounds where the strategy was able to defect and get the higher payoff. It then uses this information to initiate cooperation, ensuring it benefits from the collective success it witnessed. This is aggressive because it leverages past group success for individual gain in the current round.
*   **No Trust-Based Cooperation:** The strategy does not unilaterally cooperate hoping others will reciprocate. Cooperation is only triggered when there is evidence or strong prediction that the threshold will be met.
*   **Prioritizing Personal Gain:** The ultimate goal is to maximize individual payoff. Cooperation is a means to that end, not an end in itself.

### Pseudocode

```python
# Game Parameters
n = ...
r = ...
m = ...
k = ...

# Player's State
history = [] # List of tuples: (player_i_action, num_cooperators_in_round, round_payoff)
our_action_in_prev_round = None # 'C' or 'D' or None

def decide_action(current_round_t):
    # Handle edge case: Last round
    if current_round_t == r:
        return 'D'

    # Handle edge case: First round
    if current_round_t == 1:
        return 'D'

    # Get data from the last round
    last_round_data = history[-1] if history else None
    num_cooperators_prev_round = last_round_data[1] if last_round_data else 0

    # Calculate predicted other cooperators
    if current_round_t > 1:
        # Consider up to the last 5 rounds for calculating average cooperation
        relevant_history = history[max(0, current_round_t - 1 - 5):current_round_t-1]
        if relevant_history:
            total_other_cooperators_sum = 0
            for round_data in relevant_history:
                # Subtract 1 if we were a cooperator in that past round
                cooperators_in_past_round = round_data[1]
                was_i_cooperator_in_past = round_data[2] == 'C' # Assuming history stores our action
                if was_i_cooperator_in_past:
                    total_other_cooperators_sum += (cooperators_in_past_round - 1)
                else:
                    total_other_cooperators_sum += cooperators_in_past_round
            avg_other_cooperation_rate = total_other_cooperators_sum / len(relevant_history) / (n - 1) if n > 1 else 0
            predicted_other_cooperators = avg_other_cooperation_rate * (n - 1)
        else:
            predicted_other_cooperators = 0
    else:
        predicted_other_cooperators = 0 # No history yet

    # --- Decision Rules ---

    # Trigger Condition 1: Proximity to Threshold
    # Check if num_cooperators_prev_round (excluding ourselves) + predicted_other_cooperators + 1 (our potential cooperation) >= m
    if (num_cooperators_prev_round - 1 if num_cooperators_prev_round > 0 else 0) + floor(predicted_other_cooperators) + 1 >= m:
        return 'C'

    # Trigger Condition 2: Exploitation Opportunity
    if num_cooperators_prev_round >= m and our_action_in_prev_round == 'D':
        return 'C'

    # Default action: Defect
    return 'D'

# Helper function for rounding (equivalent to floor for positive numbers)
def floor(x):
    return int(x)

# In a real implementation, the 'history' would be updated after each round,
# storing not just the number of cooperators but also our own action in that round.
# For example: history.append((our_action, num_cooperators_in_round, our_action_in_round))
# And our_action_in_prev_round would be updated accordingly.
```

This strategy is designed to be a strong competitor by being opportunistically cooperative when the odds are in its favor, while remaining fundamentally self-interested. It avoids being a pure altruist or a pure defector, adapting to the revealed behavior of the population.
'''

description_AGGRESSIVE_55 = '''
## Aggressive "Calculated Betrayal" Strategy

This strategy is designed to maximize individual payoff by taking calculated risks, leaning towards defection until cooperation is demonstrably beneficial and then exploiting the cooperation of others. It's aggressive in its willingness to defect early and often, only cooperating when the odds of a significant reward become overwhelmingly favorable.

**Core Philosophy:** "I'll let you take the initial risk. If your risk pays off, I'll gladly reap the rewards. But I won't be the one funding the party."

### 1. Decision Rules

The decision to cooperate or defect hinges on a calculated assessment of the current round's potential payoff versus the perceived risk of being the sole cooperator or being left out of a successful cooperative venture.

**General Principle:** Defect by default. Cooperate only when the probability of achieving the cooperation threshold ($m$) *and* receiving the reward ($k$) is significantly high, and the cost of cooperating is relatively low.

**Specific Rules:**

*   **Initial Rounds (1 to r-1):**
    *   **Default Action: Defect (D)**. The primary goal is to observe and gather information without exposing your endowment.
    *   **Trigger for Cooperation:** Cooperate (C) only if **all** of the following conditions are met:
        1.  **High Likelihood of Success:** The number of observed cooperators in the *previous* round (let's call this $O_{t-1}$) plus the number of players you *anticipate* will cooperate (explained below) is greater than or equal to $m$.
        2.  **Sufficiently High Reward Margin:** The expected payoff of cooperating (which will be $k$ if successful) is significantly higher than the payoff of defecting in a scenario where cooperation *fails* (which would be 1). Specifically, if the expected payoff of cooperating is at least $k \times 2$ (i.e., you're confident you'll get the reward). This is a conservative threshold to ensure the risk is truly worth it.
        3.  **Low Personal Risk of Being the Sole Cooperator:** The number of *other* players you believe will cooperate (from point 1) is at least $m-1$. This means if you cooperate, there's a good chance at least $m-1$ others will as well, securing the reward.

*   **Anticipated Cooperators:** This is a crucial adaptive element. In rounds $t > 1$:
    *   We assume a certain percentage of players will continue their previous round's strategy. Let's say, for simplicity, if a player cooperated in round $t-1$, we estimate they have a 70% chance of cooperating in round $t$. If they defected, we estimate a 10% chance of them cooperating. (These percentages can be tuned based on tournament performance).
    *   We also consider players who have consistently cooperated and might continue to do so.
    *   Crucially, we *do not* predict cooperation based on *our* potential cooperation. We are betting on their independent decisions.

*   **Last Round (r):**
    *   **Default Action: Defect (D)**. The incentive to build reputation or foster future cooperation is gone. The focus is purely on maximizing immediate, guaranteed payoff.
    *   **Exception:** Cooperate (C) only if **all** of the following conditions are met:
        1.  **Certainty of Threshold:** The number of observed cooperators in the *previous* round ($O_{r-1}$) plus the number of players you *anticipate* will cooperate is *guaranteed* to be $m$ or more. This means you're absolutely sure the threshold will be met *even if you defect*.
        2.  **Highest Possible Payoff:** The payoff of cooperating ($k$) is strictly greater than the payoff of defecting in a scenario where the threshold is met ($1 + k$). This condition is always met since $k>1$. The real calculation here is ensuring your cooperation is *necessary* to reach the threshold, which is unlikely given our aggressive default. This last round cooperation is a very rare, almost theoretical, scenario. The primary aggressive stance is defection.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Action: Defect (D)**. There is no history to base predictions on. The most aggressive and risk-averse approach is to keep your endowment and observe. No conditions for cooperation can be met in the first round.

*   **Last Round (t=r):**
    *   As described above, the default is **Defect (D)**. Cooperation is an extremely rare, almost impossible scenario, only triggered by an overwhelming guarantee of success and a slight marginal gain. The aggressive stance here is to secure your own payoff without any future consequences.

*   **Near the Threshold ($m$):**
    *   If in the previous round, $O_{t-1} = m-1$, and you are considering cooperating.
    *   **Decision:** Cooperate (C) *only if* you also predict at least one other player will cooperate. The risk of being the sole cooperator when $m-1$ others do is too high to warrant your cooperation unless you have a strong signal from others. This aligns with the "calculated betrayal" – you want to be the beneficiary of their risk, not the sole risk-taker.

### 3. Aggressive Mindset Alignment

*   **Early and Frequent Defection:** The strategy's default is to defect. This ensures that you are never the primary financier of a project that might fail. You are waiting to be convinced.
*   **Exploitation of Others' Cooperation:** When cooperation *does* occur and the threshold is met, you happily accept the reward. Your strategy is designed to benefit from the risks taken by others.
*   **No Altruism or Trust:** The strategy does not rely on building trust or fostering cooperation. It's purely transactional and opportunistic.
*   **Information Gathering as a Weapon:** The initial rounds are spent gathering data on player behavior. This data is then used to make more informed (and aggressive) decisions in later rounds.
*   **Calculated Bets, Not Blind Leaps:** While aggressive, the strategy is not reckless. Cooperation is only triggered when there's a high probability of success and a substantial reward, mitigating the risk of being exploited or wasting your endowment. The "calculated betrayal" aspect comes from this readiness to defect if the conditions aren't *perfectly* aligned for a profitable cooperative outcome.

### Pseudocode

```
function determine_action(player_id, current_round, total_rounds, min_cooperators, reward_factor, history):
    // history: a list of lists, where history[t-1] is a list of actions taken by all players in round t-1
    // For simplicity, assume history is accessible and actions are 'C' or 'D'

    n = number of players
    r = total_rounds
    m = min_cooperators
    k = reward_factor

    // --- Edge Case: First Round ---
    if current_round == 1:
        return 'D'

    // --- Calculate Observed Cooperators in Previous Round ---
    observed_cooperators_prev_round = 0
    if len(history) > 0:
        for action in history[-1]: // history[-1] is the list of actions from the previous round
            if action == 'C':
                observed_cooperators_prev_round += 1

    // --- Estimate Anticipated Cooperators (Simplified Model) ---
    // This is a crucial adaptive element. A more sophisticated model would analyze individual player history.
    // For this example, let's assume a fixed probability based on previous round's action.
    anticipated_cooperators = 0
    if len(history) > 0:
        for i in range(n):
            if history[-1][i] == 'C':
                # Assume 70% chance of previous cooperator cooperating again
                if random.random() < 0.7:
                    anticipated_cooperators += 1
            else: # player defected
                # Assume 10% chance of previous defector cooperating
                if random.random() < 0.1:
                    anticipated_cooperators += 1
    
    // --- Edge Case: Last Round ---
    if current_round == r:
        # Aggressive default: Defect
        # Cooperation is extremely unlikely and only if guaranteed success and marginal gain
        if observed_cooperators_prev_round + anticipated_cooperators >= m:
            # This condition ensures our cooperation isn't necessary to meet m,
            # but we might still cooperate if it maximizes payoff in a guaranteed scenario.
            # In this aggressive strategy, we prefer to defect unless there's a *clear* benefit
            # that outweighs the risk of being the *only* one funding a failed project.
            # In the last round, this becomes even more about individual guaranteed gain.
            # Given the aggressive nature, we'd likely still defect unless we had
            # absolute certainty that cooperation leads to *our* maximal payoff and
            # our cooperation is somehow pivotal or beneficial.
            # For a truly aggressive last round, we stick to D unless the rules force us.
            # The condition here is mostly theoretical for this aggressive strategy.
            pass # Stick to default 'D' in most cases.

        return 'D'

    // --- General Rounds (1 < t < r) ---

    // Condition 1: High Likelihood of Success
    sufficient_cooperators_predicted = (observed_cooperators_prev_round + anticipated_cooperators) >= m

    // Condition 2: Sufficiently High Reward Margin (conservative heuristic)
    high_reward_margin = (k * 2) > 1 # Payoff of cooperating is at least twice the baseline

    // Condition 3: Low Personal Risk of Being Sole Cooperator
    # We want to see at least m-1 *other* cooperators predicted if we cooperate.
    # So, if we cooperate, we need observed + anticipated to be at least m.
    # This is implicitly covered if we have enough predicted cooperators and our own cooperation is considered.
    # A clearer check is: if we cooperate, will we have at least m-1 *other* cooperators?
    # This means: observed_cooperators_prev_round + anticipated_cooperators (excluding ourselves) >= m - 1
    # A simpler interpretation of the rule: "at least m-1 others will cooperate if you do"
    # This implies anticipated_cooperators must be at least m-1.
    enough_others_will_cooperate = anticipated_cooperators >= m - 1

    if sufficient_cooperators_predicted and high_reward_margin and enough_others_will_cooperate:
        return 'C'
    else:
        return 'D'

```
'''

description_AGGRESSIVE_56 = '''
Here's an aggressive strategy designed for the Collective Risk Dilemma, focusing on maximizing individual gain while leveraging the cooperative reward when strategically advantageous.

## Strategy: "The Calculated Gambler"

This strategy is built on the principle of **opportunistic aggression**. We aim to profit from others' cooperation by defecting whenever the risk of missing the reward is low, but are willing to gamble on cooperation when the potential payoff is exceptionally high and the number of cooperators is already close to the threshold.

### 1. Decision Rules

The core of this strategy is a dynamic threshold for cooperation. We will defect by default, but our propensity to cooperate increases based on the observed cooperation levels of others in the previous round.

**The "Aggression Threshold" (AT):**

This threshold determines when we consider cooperating. We will cooperate only if the number of observed cooperators in the *previous* round is at least `AT`.

*   **Initial AT:** For the first round, we will start with a relatively high AT to observe the general player behavior. Let's set the initial AT to `max(m, n - floor(n/2))` (a value that's at least the minimum required cooperators but also leans towards a slightly more cautious stance initially). This ensures we don't overcommit to cooperation early on when we lack information.

*   **Adaptive AT Adjustment:**
    *   **If the threshold `m` was met in the previous round:** This indicates a generally cooperative environment or at least a successful collective gamble by others. We want to capitalize on this. Therefore, we will **decrease our AT by 1**. The goal is to become more willing to cooperate in the future if this successful cooperation pattern continues.
    *   **If the threshold `m` was *not* met in the previous round:** This indicates a lack of cooperation. We need to be more cautious and less likely to initiate cooperation. Therefore, we will **increase our AT by 1**. This makes us less likely to cooperate in the next round, as we don't want to be the "foolish" cooperator in a largely defecting group.

*   **Cooperation Condition:** In any given round (after the first), player `i` will play **C (Cooperate)** if:
    *   The number of players who cooperated in the *previous* round (`num_cooperators_prev`) is greater than or equal to our current `AT`.
    *   **AND** `num_cooperators_prev` is greater than or equal to `m - 1`. (This is a crucial aggressive adjustment: if the group is *already* one cooperator away from the reward, we are more inclined to contribute ourselves to secure the payoff. This makes us a "tipping point" player.)

*   **Defection Condition:** In any given round, player `i` will play **D (Defect)** if:
    *   The cooperation condition above is not met.

**Rationale for Aggression:**

*   **Defaulting to Defect:** Our default action is to defect. This immediately secures us the private payoff of 1. We only deviate from this if the potential reward from cooperating is sufficiently enticing and the risk of failure is minimized by others' actions.
*   **Exploiting High Cooperation:** When others cooperate and meet or exceed `m`, we want to be there to collect the reward. The decreasing `AT` allows us to quickly adapt to cooperative environments and start cooperating ourselves to secure the payoff, effectively piggybacking on the group's efforts.
*   **"Last Minute" Cooperation:** The condition `num_cooperators_prev >= m - 1` is key to our aggression. It means if the group is on the cusp of achieving the reward, we'll jump in and secure it, ensuring we get the `+k` payoff rather than letting it slip away. This is a calculated gamble to secure a guaranteed higher payoff.
*   **Escalating Caution:** When cooperation fails, we retreat. Increasing the `AT` makes us more hesitant to cooperate, preventing us from being exploited by consistently defecting opponents.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   We have no history. We will play **D (Defect)**. This is the safest initial move to gather information without risking our endowment. Our `AT` is initially set at `max(m, n - floor(n/2))`.

*   **Last Round (t=r):**
    *   In the final round, the long-term implications of cooperation or defection no longer matter. The only consideration is the immediate payoff for this round.
    *   Our decision rule remains the same: if the number of cooperators in the *previous* round (`r-1`) was `m-1` or more, and `m-1` is also greater than or equal to our current `AT`, we will **Cooperate**. Otherwise, we will **Defect**. This is still aggressive as we are trying to secure the highest payoff even in the last round. We do *not* need to worry about future rounds, so if the conditions are met, we'll take the risk.

*   **Dynamic AT Clamping:** The `AT` should be clamped between a minimum of `m-1` and a maximum of `n-1`.
    *   **Minimum `AT`:** We will never set our `AT` below `m-1`. This ensures that even if the `AT` dynamically suggests a lower threshold, we still require at least `m-1` cooperators from the previous round to *consider* cooperating. This prevents us from cooperating when the group is very unlikely to reach `m`.
    *   **Maximum `AT`:** We will never set our `AT` above `n-1`. We don't want to be so cautious that we never cooperate even when everyone else does.

### 3. Aggressive Mindset Summary

*   **Default to Self-Interest:** Start by keeping your endowment (Defect).
*   **Opportunistic Reward Seeker:** If others are cooperating enough to meet the threshold (or are very close to it), jump in to secure the reward.
*   **Adaptive Learning:** Become more aggressive (lower `AT`) as cooperation proves successful and retreat into caution (higher `AT`) when it fails.
*   **No Forgiveness, No Altruism:** This strategy does not involve tit-for-tat or any form of altruistic punishment/reward. It's purely about maximizing personal gain based on observable collective outcomes.
*   **Calculated Risk:** We are willing to cooperate, but only when the evidence from the previous round strongly suggests the reward is achievable, and especially when we can be the deciding factor.

### Pseudocode Representation:

```python
class CalculatedGamblerStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.current_round = 0
        # Initial Aggression Threshold: Lean towards caution initially
        self.aggression_threshold = max(self.m, self.n - (self.n // 2))

    def decide_action(self, history):
        self.current_round += 1

        # --- Handle First Round ---
        if self.current_round == 1:
            # In the first round, defect to gather information
            return 'D'

        # --- Retrieve Information from Previous Round ---
        previous_round_actions = history[-1] # history is a list of lists of actions for each round
        num_cooperators_prev = previous_round_actions.count('C')

        # --- Dynamic Aggression Threshold Adjustment ---
        # Clamp AT to prevent extreme values
        min_at = self.m - 1
        max_at = self.n - 1

        if num_cooperators_prev >= self.m:
            # If threshold was met, become more willing to cooperate
            self.aggression_threshold = max(min_at, self.aggression_threshold - 1)
        else:
            # If threshold was not met, become more cautious
            self.aggression_threshold = min(max_at, self.aggression_threshold + 1)

        # --- Decision Logic ---
        # Condition for cooperation:
        # 1. Previous round had enough cooperators to meet or exceed our current AT
        # 2. AND Previous round had enough cooperators to be on the cusp of meeting 'm' (aggressive tipping point)
        if num_cooperators_prev >= self.aggression_threshold and num_cooperators_prev >= self.m - 1:
            return 'C'
        else:
            return 'D'

# Example of how this would be used (simplified)
# history = [] # List of lists, e.g., [['D', 'D'], ['C', 'D', 'D']]
# strategy = CalculatedGamblerStrategy(n=6, r=10, m=3, k=2)
# my_action = strategy.decide_action(history)
# history.append([my_action] + other_players_actions) # Update history after round
```
'''

description_AGGRESSIVE_57 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while pushing for cooperation when it's most beneficial and defecting when the risk is too high.

## Aggressive Strategy: "The Opportunistic Enforcer"

This strategy aims to be aggressive by *exploiting* the cooperation of others when possible, but also by *enforcing* a minimal level of cooperation to trigger the reward. It's designed to be opportunistic and adaptable, punishing defection while rewarding genuine contributions.

### Core Philosophy: "Make them cooperate, then take the reward."

The core idea is to incentivize others to reach the cooperation threshold. Once the threshold is met, we will maximize our own payoff by defecting and claiming the reward based on others' contributions. However, we will defect to *punish* groups that fail to reach the threshold.

### 1. Decision Rules

Our decision to Cooperate (C) or Defect (D) in round `t` is based on the following logic:

*   **If `t` is the first round (t=1):** We will **Cooperate (C)**. This is a strategic investment. By cooperating early, we signal a willingness to contribute and potentially nudge the group towards reaching the threshold. We are willing to take a small initial risk (losing our endowment) to *potentially* unlock larger gains later.

*   **If `t` is NOT the first round (t > 1):**
    *   **Count previous cooperators:** Let `C_prev` be the number of players who cooperated in round `t-1`.
    *   **If `C_prev >= m`:** The group successfully met the cooperation threshold in the previous round. This means the risk was averted, and the reward was triggered. In this situation, we will **Defect (D)**. Our goal now is to maximize our individual payoff by keeping our endowment and enjoying the reward `k` generated by others' cooperation. We assume others will continue to cooperate because the reward is beneficial for them too.
    *   **If `C_prev < m`:** The group failed to meet the cooperation threshold in the previous round. This means the risk was *not* averted, and no reward was triggered.
        *   **Sub-condition: Were *we* the only defector?** If `C_prev == n-1` (meaning all other `n-1` players cooperated in the previous round, but we defected), and the threshold `m` was still not met (which would only happen if `n-1 < m`), we will continue to **Cooperate (C)**. This is a *final attempt* to push the group over the threshold. We absorb the individual loss for one more round, hoping to trigger the reward and learn from the experience.
        *   **Otherwise (if `C_prev < m` and we weren't the only defector):** This means the group is not cooperating effectively, and we are not solely responsible for the failure. We will **Defect (D)**. This is our punishment for the group's collective failure to reach the threshold. We will not subsidize the lack of cooperation.

### 2. Handling Edge Cases

*   **First Round (t=1):** As specified above, we **Cooperate (C)**. This is the initial gamble to get the ball rolling and see if the group is inclined towards cooperation.

*   **Last Round (t=r):** The decision logic remains the same. If the previous round (`t-1`) met the threshold, we **Defect (D)**. If it didn't, we follow the `C_prev < m` logic. The "last round" status doesn't fundamentally change the opportunistic and enforcing nature of the strategy. The incentive to defect when the threshold is met remains, as there's no future round to be punished for.

*   **When `m` is close to `n`:** If `m` is very high (e.g., `m = n-1`), the strategy is highly sensitive.
    *   If we cooperate and `n-1` others cooperate, we get the reward.
    *   If we defect and only `n-1` others cooperate, the threshold isn't met.
    *   Our strategy handles this: If `C_prev = n-1` (everyone else cooperated) and `m` is still not met (i.e., `m = n`), then we would *still* cooperate. This is the critical point where we try to be the last one to bridge the gap. However, the condition `m < n` implies `m` can never be `n`. So, if `C_prev = n-1`, the threshold `m` will *always* be met (since `m < n`). Thus, if `C_prev = n-1`, we will always defect.

*   **When `m` is small (but > 1):** If `m` is very small (e.g., `m = 2`), it's easier to reach the threshold. Our strategy will quickly move to defecting if the threshold is met.

### 3. Aggressive Mindset Alignment

This strategy is aggressive in the following ways:

1.  **Opportunistic Reward Seizing:** The moment the group demonstrates it can meet the threshold, we immediately switch to defecting. We *capitalize* on others' cooperation to maximize our personal gain. We don't share in the "glory" of cooperation once the collective good is achieved; we take the individual reward.
2.  **Punitive Defection:** If the group consistently fails to cooperate and reach the threshold, we will defect. This is a clear signal that we will not be exploited by continuing to cooperate when the collective fails to reciprocate.
3.  **Initial Investment for Future Gain:** Cooperating in the first round is a calculated risk. It's an aggressive move to *create* the conditions for future profitable defections. We are not passively waiting; we are actively trying to engineer a situation where we benefit most.
4.  **No "Nice" Behavior:** We do not engage in sustained cooperation for its own sake or out of altruism. Our cooperation is purely instrumental, a means to an end of future profitable defections.

### Pseudocode Representation:

```pseudocode
// Strategy: The Opportunistic Enforcer

function decide_action(player_id, current_round, history, game_parameters):
  n = game_parameters.n
  r = game_parameters.r
  m = game_parameters.m
  k = game_parameters.k

  // If it's the first round, cooperate.
  if current_round == 1:
    return "C"

  // Get the actions from the previous round.
  // history is a list of past rounds, each round contains actions of all players.
  // history[-1] refers to the last round.
  previous_round_actions = history[-1]

  // Count how many players cooperated in the previous round.
  cooperators_in_previous_round = count_cooperators(previous_round_actions)

  // Check if the threshold was met in the previous round.
  if cooperators_in_previous_round >= m:
    // Threshold met, exploit the situation by defecting.
    return "D"
  else:
    // Threshold not met.
    // Check if *we* were the only defector and everyone else cooperated,
    // AND the threshold wasn't met (this implies m=n, which is ruled out by m<n).
    // For clarity, let's check if we were one of potentially many defectors.
    // If only a few cooperated, and we were part of the defectors,
    // and not solely responsible for the failure by being the ONLY defector when m is narrowly missed.
    // A more direct interpretation of "last attempt": if the group is *almost* there, and *we* are defecting.
    
    // If *we* defected, and exactly n-1 people cooperated in the previous round,
    // and the threshold 'm' was STILL not met (this implies m=n, which is impossible by definition).
    // The condition "were we the only defector" is tricky when m is not met.
    // Let's simplify: If threshold not met, and we are *not* the sole reason for failure
    // by being the *only* one defecting while the rest *just* missed the threshold.
    // A better interpretation of "last attempt": if the previous round failed, AND we *are* cooperating,
    // it's our attempt to push it. But our rule says if previous failed, we often defect.
    // The edge case to handle is when *not enough* cooperation happened, AND we *were* cooperating.
    // This is not explicitly covered if the default is to defect on failure.

    // Let's refine the "last attempt" logic: If threshold failed, and we *are* cooperating,
    // we should continue. But our current logic is to defect if threshold failed unless
    // we were the *only* defector and everyone else cooperated.

    // Correct handling of the "last attempt" edge case for "m not met":
    // If threshold failed (cooperators_in_previous_round < m)
    // AND we were NOT the only defector (meaning more than one player defected, or we defected and not all others cooperated)
    // OR if we *were* the only defector, but it wasn't enough (e.g., n-1 cooperated, still < m)
    // This implies we should defect.

    // The only situation where we might reconsider defecting after failure is
    // if the group was VERY close to `m` and we think our cooperation would tip it.
    // But the current aggressive strategy prioritizes taking advantage.

    // Let's make the "last attempt" explicit and aggressive:
    // If the threshold was NOT met, and the number of cooperators was VERY CLOSE to m,
    // AND we were NOT cooperating in the previous round (meaning we are the 'problem'),
    // then we will try cooperating. This is still a bit complex.

    // Let's stick to the core logic for simplicity and aggressiveness:
    // If threshold failed, we punish. The exception is the first round.
    // Re-evaluating: "If C_prev < m: ... Sub-condition: Were *we* the only defector?"
    // This sub-condition is meant to catch scenarios where *we* were the sole reason for failure,
    // and the group was otherwise very cooperative.
    
    // Let's assume `player_id` is known to us internally.
    my_previous_action = previous_round_actions[player_id]

    if my_previous_action == "D" and cooperators_in_previous_round == n - 1:
        // If I defected, and everyone else cooperated, and the threshold 'm' was still not met.
        // This scenario means m=n, which is impossible.
        // So, if I defected and everyone else cooperated, the threshold MUST have been met.
        // Thus, this condition should not lead to cooperation.

        // The intended edge case for "last attempt" is likely:
        // If the previous round failed, AND we were cooperating, AND the group was close to m.
        // But our strategy is to defect when it fails.

        // Let's simplify the "last attempt" as: if previous round failed, and *we* are not one of the cooperators.
        // If the threshold was not met, and we *were* cooperating, then we continue to cooperate.
        // But if we *were* defecting, and the threshold was not met, we continue to defect.

        // Revised interpretation for aggressive "last attempt"
        // If the previous round failed (C_prev < m):
        //   If *I* cooperated in the previous round: Continue to Cooperate (C). This is my attempt to push.
        //   If *I* defected in the previous round: Defect (D). Punish collective failure.

        if my_previous_action == "C":
          // I cooperated, but it wasn't enough. I'll try one more time.
          return "C"
        else: // my_previous_action == "D"
          // I defected, and it failed. I will continue to defect.
          return "D"
    else:
        // If the threshold was not met, and I was NOT the only defector (or everyone else cooperated but didn't reach m).
        // This means the group as a whole failed, and I don't need to feel responsible for a last push.
        // So, if I defected, I continue to defect. If I cooperated, I switch to defecting now.
        // The core is: if threshold failed, and I was defecting, I keep defecting.
        // If threshold failed, and I was cooperating, it's a signal for me to switch to defecting.
        
        // Let's use the simpler and more robust rule:
        // If threshold failed, we defect, UNLESS we ourselves cooperated in the failed round.
        // This is the crucial adjustment for adaptiveness.
        
        if my_previous_action == "C":
            // I tried cooperating, and it still failed. This is a sign of the group's weakness.
            // I will now defect to extract value from their future cooperation attempts.
            return "D"
        else: // my_previous_action == "D"
            // I defected, and the threshold failed. I will continue to defect and punish.
            return "D"

// Helper function (assuming access to actions of all players in a round)
function count_cooperators(actions):
  count = 0
  for action in actions:
    if action == "C":
      count = count + 1
  return count

```

**Refinement of the Pseudocode Logic (Simplifying the "last attempt" ambiguity):**

The above pseudocode got a bit tangled. Let's simplify the core rules for clarity and ensure they capture the aggressive, adaptive spirit. The key is to define *when* we defect and *when* we risk cooperation.

**Revised Decision Rules (Simplified & Aggressive):**

1.  **Round 1 (t=1): Cooperate (C).** This is the initial investment to encourage group cooperation.
2.  **If `m` or more players cooperated in round `t-1`:** **Defect (D).** The threshold was met. We exploit the collective effort for maximum individual gain.
3.  **If fewer than `m` players cooperated in round `t-1`:**
    *   **If *I* cooperated in round `t-1`:** **Defect (D).** I tried, but it failed. The group is not cooperating enough. I will now switch to defecting to maximize my payoff from any future cooperation by others. I've absorbed the cost of one failed cooperation attempt.
    *   **If *I* defected in round `t-1`:** **Defect (D).** The group failed to cooperate, and I was part of the problem (or a contributor to it). I will continue to defect and punish this collective failure.

This revised logic is simpler, more aggressive, and highly adaptive. It ensures we only cooperate when it's strategically beneficial (the first round) or when we're testing the waters and others *might* be on the verge of cooperating. Once the threshold is met, we immediately exploit. If it's not met, and we participated in that failure by cooperating, we switch to defecting. If we defected and it failed, we just keep defecting.

**Revised Pseudocode:**

```pseudocode
// Strategy: The Opportunistic Enforcer (Revised)

function decide_action(player_id, current_round, history, game_parameters):
  n = game_parameters.n
  m = game_parameters.m

  // Rule 1: First round - Cooperate
  if current_round == 1:
    return "C"

  // Get actions from the previous round.
  // history[-1] is a dictionary or list mapping player_id to action for the last round.
  previous_round_actions = history[-1] // Assuming this is structured to get actions by player_id

  // Count cooperators in the previous round.
  cooperators_in_previous_round = 0
  for p_id in range(n): // Assuming player IDs are 0 to n-1 or 1 to n
      // Adjust indexing if player_id is 1-based and history keys are 0-based
      if previous_round_actions[p_id] == "C":
          cooperators_in_previous_round += 1

  // Rule 2: Threshold met in previous round - Defect
  if cooperators_in_previous_round >= m:
    return "D"
  else:
    // Rule 3: Threshold not met in previous round
    my_previous_action = previous_round_actions[player_id] // Get my action from previous round

    if my_previous_action == "C":
      // Rule 3a: I cooperated, but it failed. Switch to Defect.
      return "D"
    else: // my_previous_action == "D"
      // Rule 3b: I defected, and it failed. Continue to Defect.
      return "D"

```
'''

description_AGGRESSIVE_58 = '''
Here's an aggressive strategy designed for the Collective Risk Dilemma, focusing on maximizing individual gain while leveraging opportunities for collective reward, even in a competitive environment. This strategy prioritizes decisive action and adaptation based on observed outcomes.

## Aggressive Strategy: "The Opportunistic Dominator"

This strategy is designed to be aggressive by always seeking the highest potential payoff for itself in each round, while subtly influencing the game towards beneficial outcomes for itself when possible. It doesn't shy away from defecting, but it will cooperate strategically when the conditions strongly favor it and defect when the risk of insufficient cooperation is high.

### 1. Decision Rules

The core of "The Opportunistic Dominator" lies in its assessment of the *likelihood of reaching the cooperation threshold (m)* and its *own potential payoff* in the current round.

**Decision Logic:**

*   **High Confidence in Success (Cooperate):** If I believe (based on history) that enough other players are likely to cooperate to reach or exceed the threshold `m`, I will Cooperate (C). This is because the payoff for cooperating when the threshold is met (`1 - 1 + k = k`) is significantly higher than the payoff for defecting when the threshold is met (`1 - 0 + k = 1 + k`). The higher `k` is, the more attractive this becomes.

*   **Moderate Confidence / Risk of Failure (Cooperate Strategically):** If the number of observed cooperators in previous rounds has been *consistently* close to `m`, and the current round is not the final round, I might choose to Cooperate (C) to nudge the group towards success. This is a calculated risk to secure the higher payoff `k`.

*   **Low Confidence / High Risk of Failure (Defect):** If in previous rounds, the number of cooperators has consistently fallen short of `m`, or if the number of observed cooperators is low in the current round's history, I will Defect (D). The payoff for defecting when the threshold is *not* met (`1 - 0 = 1`) is guaranteed, while cooperating in such a scenario yields a payoff of `1 - 1 = 0`.

*   **Dominance Play (Defect):** If the number of players who have defected in the previous round is high, I will lean towards defecting. This is an aggressive stance to discourage others from relying on my cooperation and to potentially punish those who might have defected in the past.

**Quantifying "Likelihood" and "Confidence":**

To implement this, we need to track the proportion of cooperators in past rounds. Let `C_t` be the number of players who cooperated in round `t`.

*   **Threshold Meter:** Calculate a "Cooperation Success Probability" (CSP) score. This can be a weighted average of `C_t / n` from previous rounds, with more recent rounds having higher weights.

    *   For example: `CSP = (w_1 * (C_r-1 / n) + w_2 * (C_r-2 / n) + ... + w_r-1 * (C_1 / n)) / (w_1 + ... + w_r-1)` where `w_i` is a weight decreasing with distance from the current round.

*   **Decision Thresholds:**
    *   **Cooperate (C):** If `CSP > (m/n) + epsilon` (where `epsilon` is a small buffer to ensure high confidence) OR if `(m-1)/n < CSP <= (m/n) + epsilon` AND `r - current_round_number > 1` (strategic nudge).
    *   **Defect (D):** Otherwise.

### 2. Handling Edge Cases

*   **First Round (t = 1):**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** In the absence of any historical data, assuming others will cooperate is too risky. Defecting guarantees a payoff of 1. This also establishes an aggressive precedent, signaling that I am not to be taken advantage of. This is the "no free lunch" approach.

*   **Last Round (t = r):**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** In the final round, there's no future to influence. The only goal is to maximize immediate payoff.
        *   If `m` or more cooperate: My payoff is `1 - 0 + k = 1 + k` (if I defect) vs `1 - 1 + k = k` (if I cooperate). Defecting is better.
        *   If fewer than `m` cooperate: My payoff is `1 - 0 = 1` (if I defect) vs `1 - 1 = 0` (if I cooperate). Defecting is better.
    *   Therefore, always defecting in the last round is optimal for this aggressive strategy.

*   **Sufficient Cooperators in Previous Round (C_t-1 >= m):**
    *   **Decision:** **Cooperate (C)**
    *   **Rationale:** If the group has successfully met the threshold in the most recent round, it's a strong indicator that cooperation is viable and that the group *can* reach `m`. Securing the `k` reward is now the priority, as the risk of failure has been demonstrably reduced.

*   **Just Below Threshold in Previous Round (m-1 <= C_t-1 < m):**
    *   **Decision:** **Cooperate (C)** (if `r - current_round_number > 1`)
    *   **Rationale:** This is the strategic nudge. The group was close. By cooperating, I am increasing the chance that the threshold is met, and I will receive the higher payoff `k`. This is a calculated risk for a potentially larger reward.

*   **Significantly Below Threshold in Previous Round (C_t-1 < m-1):**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** The group has shown a clear inability or unwillingness to cooperate sufficiently. Continuing to cooperate is throwing away potential payoffs. Defecting guarantees a payoff of 1.

### 3. Aggressive Mindset Alignment

This strategy is aggressive in several ways:

*   **Initial Aggression:** Starts by defecting, establishing a non-cooperative baseline and maximizing immediate security.
*   **Opportunistic Exploitation:** Actively seeks to exploit situations where cooperation will yield a higher payoff, but only when the risk is demonstrably low or when a strategic nudge is likely to succeed.
*   **Punishment/Discouragement:** The tendency to defect when others defect or when cooperation consistently fails serves to discourage exploitation and punish free-riders.
*   **Self-Interest Above All:** While it may cooperate, it does so only when it directly benefits itself significantly, not out of altruism or a belief in intrinsic cooperation.
*   **No Reliance on Others:** It doesn't assume others will be cooperative or predictable. It reacts to observed behavior and adapts its own actions to maximize its outcome in a dynamic environment.

### Pseudocode

```pseudocode
Strategy "Opportunistic Dominator" (parameters: n, r, m, k, history)

// history is a list of lists, where history[t-1] is a list of actions of all players in round t
// My own action in round t is indexed by my_player_index

function decide_action(current_round, history, my_player_index):

    // Handle First Round
    if current_round == 1:
        return D

    // Handle Last Round
    if current_round == r:
        return D

    // Analyze Past Rounds
    observed_cooperators_last_round = count_cooperators(history[current_round - 2]) // actions from previous round
    num_players_cooperated_history = []
    for t from 1 to current_round - 1:
        num_players_cooperated_history.append(count_cooperators(history[t-1]))

    // Calculate Cooperation Success Probability (CSP) - simple weighted average (recent rounds more weight)
    // A more sophisticated CSP could be used here.
    weight_sum = 0
    weighted_csp_sum = 0
    weights = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] // Example weights for last 10 rounds, adjust as needed
    
    for i from 1 to min(current_round - 1, length(weights)):
        round_index_in_history = current_round - 1 - i
        weight = weights[length(weights) - i]
        proportion_cooperated = num_players_cooperated_history[round_index_in_history] / n
        weighted_csp_sum += weight * proportion_cooperated
        weight_sum += weight

    if weight_sum > 0:
        csp = weighted_csp_sum / weight_sum
    else:
        csp = 0 // Should not happen after first round

    // Decision Rules
    epsilon = 0.05 // Small buffer for high confidence

    // High Confidence in Success
    if csp > (m / n) + epsilon:
        return C

    // Strategic Nudge: Close to threshold and not the last round
    if (m - 1) / n < csp <= (m / n) + epsilon and (r - current_round) > 1:
        return C

    // Default to Defect if confidence is low or risk is high
    return D

function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == C:
            count += 1
    return count
```

This strategy aims to be a formidable opponent, maximizing personal gain by being decisive, adaptive, and opportunistic. It doesn't aim to "win" by fostering cooperation but by strategically leveraging it when it provides the highest individual return.
'''

description_AGGRESSIVE_59 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Reckless Benefactor"

This strategy prioritizes maximizing individual gain through calculated risks, leaning heavily towards defection unless a strong incentive for cooperation is present. It aims to exploit potential free-riders while still aiming for the high rewards of collective success when the odds are favorable.

**Core Philosophy:** "I'll take the reward if you do the work, but I'm not afraid to push the button to get there. If the group fails, at least I didn't waste my endowment."

---

### 1. Decision Rules

My decision to Cooperate (C) or Defect (D) in any given round will be based on a probabilistic assessment of whether my cooperation is **necessary** and **sufficient** to trigger the cooperative payoff, balanced against the risk of contributing without reward.

**Key Metric: "Cooperation Gap" (CG)**

The "Cooperation Gap" represents the difference between the minimum number of cooperators required (`m`) and the number of players I *estimate* will cooperate if I choose to defect.

*   **`CG = m - (n - 1)`** (if I defect, I assume all others will defect as well in a risk-averse scenario).

**Decision Logic:**

*   **If `CG <= 0` (i.e., `m <= n - 1`):** This means that even if I defect, the required number of cooperators (`m`) can still be met by the remaining `n-1` players. In this scenario, I will **always defect**. My endowment is safer, and if the group succeeds, I still get the `k` reward. This is the core of the aggressive, "free-rider" mindset.

*   **If `CG > 0` (i.e., `m > n - 1`):** This means that if I defect, it's impossible for the threshold `m` to be met by the remaining players. My cooperation is **necessary**. Now, the decision becomes about the *probability* of success if I *do* cooperate.

    *   **Estimate of Others' Cooperation:** I will maintain a dynamic estimate of how many other players are likely to cooperate in the current round. This estimate is crucial. Let's call this `Estimated_Others_C`.
        *   This estimate will be based on the observed cooperation rate in previous rounds, with a slight bias towards recent history to adapt to changing player behavior.
        *   For simplicity in this aggressive strategy, I will initially assume a *low* baseline cooperation from others. For example, if the observed cooperation rate in the previous round was X%, `Estimated_Others_C` might be `(X% * (n-1)) * 0.7`. This acknowledges that others might cooperate, but I won't *bank* on it.
        *   **Crucially, if `Estimated_Others_C + 1 >= m` (meaning my cooperation, plus my estimate of others', is enough to meet the threshold):** I will **Cooperate (C)**. This is where the "reckless benefactor" aspect comes in. I am taking the risk of contributing because the estimated probability of success is high *due to my input*.
        *   **If `Estimated_Others_C + 1 < m`:** I will **Defect (D)**. My cooperation alone, even if I believe it's necessary, won't guarantee success given my conservative estimate of others' behavior. I would rather keep my endowment.

**Adaptive Component: "Cooperation Pressure" (CP)**

To refine `Estimated_Others_C`, I'll introduce "Cooperation Pressure" which acts as a multiplier on my baseline estimate.

*   **`CP = 1.0` (initial)**
*   **If in the previous round, the threshold `m` was met and I cooperated:** `CP = CP * 1.05` (slightly optimistic, encouraging me to be more willing to cooperate if success was recently achieved with my help).
*   **If in the previous round, the threshold `m` was NOT met and I cooperated:** `CP = CP * 0.95` (more cautious, making me less likely to cooperate next time if my contribution didn't lead to success).
*   **If in the previous round, the threshold `m` was met and I defected:** `CP = CP * 1.00` (neutral, as my action didn't directly contribute to success or failure).
*   **If in the previous round, the threshold `m` was NOT met and I defected:** `CP = CP * 1.02` (slightly more willing to cooperate, as there's a chance others are simply risk-averse and need a nudge to reach the threshold).

Then, `Estimated_Others_C = floor(((Observed_Cooperation_Rate_Previous_Round - my_previous_action_as_decimal) * (n-1)) * CP)`.
The `floor` function ensures we deal with whole players.

---

### 2. Handling Edge Cases

*   **Round 1 (Initial Round):**
    *   There is no history. My `Estimated_Others_C` is initialized to 0.
    *   **If `m <= n - 1`:** I **Defect (D)**.
    *   **If `m > n - 1`:** My cooperation is necessary. Since `Estimated_Others_C` is 0, `0 + 1 < m`. Therefore, I **Defect (D)**.
    *   **Aggressive Stance in Round 1:** In Round 1, "The Reckless Benefactor" is primarily a free-rider. I will defect unless the game parameters *force* cooperation to even have a chance of reward, and even then, I'm skeptical. The strategy is to observe and adapt.

*   **Last Round (`t == r`):**
    *   The repeated nature of the game is gone. There's no future to leverage or punish. My decision is purely based on immediate payoff.
    *   **If `m <= n - 1`:** I **Defect (D)**.
    *   **If `m > n - 1`:** My cooperation is necessary.
        *   I will look at the observed cooperation rate of *all other players* in the penultimate round (`r-1`). Let this be `Observed_Others_C_r_minus_1`.
        *   **If `Observed_Others_C_r_minus_1 + 1 >= m`:** I **Cooperate (C)**. I've seen enough cooperation to make the risk worthwhile for the final payoff.
        *   **If `Observed_Others_C_r_minus_1 + 1 < m`:** I **Defect (D)**. The risk is too high, and my single contribution won't secure the win.

*   **Rounds where `m` is very close to `n` (e.g., `m = n-1`):**
    *   This is a critical zone. My `CG` will likely be positive.
    *   My `Estimated_Others_C` will be the deciding factor. If I've seen consistent high cooperation, I'll be more inclined to contribute. If it's been erratic, I'll be hesitant.
    *   The `CP` multiplier will play a significant role here in nudging me towards or away from cooperation based on past outcomes.

---

### 3. Aggressive Mindset Alignment

*   **Default to Defection:** The strategy's primary action is to keep the endowment. Cooperation is a calculated deviation from this default, only undertaken when the perceived probability of reaping the reward is exceptionally high, often necessitating my own input.
*   **Exploiting Others:** The `CG <= 0` condition means I will *never* cooperate if the threshold can be met without me. This is pure free-riding.
*   **Calculated Risk:** When cooperation is necessary (`CG > 0`), I don't blindly cooperate. I assess the likelihood of success based on my conservative estimate of others' behavior. If my cooperation is the tipping point, I will take that risk, but only if it's likely to pay off.
*   **Leveraging History for Aggression:** The `CP` multiplier allows the strategy to become *more* aggressive in its willingness to cooperate if past successes (especially those involving my cooperation) have occurred. Conversely, failures with my cooperation make it more cautious, but it doesn't become overly altruistic.
*   **No Forgiveness:** The strategy doesn't "punish" defection in a tit-for-tat sense. It simply adapts its estimate of future behavior. If others defect consistently, my `Estimated_Others_C` will remain low, and I will continue to defect unless the parameters change dramatically or my own cooperation is absolutely essential.

---

### Pseudocode Representation:

```
StrategyName: TheRecklessBenefactor

// Game Parameters (global or passed in)
n: int
r: int
m: int
k: float

// State Variables (per player)
my_history: list of actions (C/D)
other_players_history: list of lists of actions, representing each other player's history
current_round: int
CooperationPressure: float = 1.0
EstimatedOthersCooperationRate: float = 0.0 // Based on history, updated each round

Function DecideAction(current_round, my_history, other_players_history):

    // Update EstimatedOthersCooperationRate based on history and CooperationPressure
    If current_round > 1:
        // Calculate actual cooperation rate of others in the previous round
        actual_others_cooperation_count_prev_round = 0
        For each player_history in other_players_history:
            If player_history[-1] == 'C':
                actual_others_cooperation_count_prev_round += 1

        // Adjust estimate based on previous round's actuals and CooperationPressure
        EstimatedOthersCooperationRate = (actual_others_cooperation_count_prev_round / (n - 1)) * CooperationPressure
        // Ensure estimate is between 0 and 1
        EstimatedOthersCooperationRate = max(0.0, min(1.0, EstimatedOthersCooperationRate))
    Else: // Round 1
        EstimatedOthersCooperationRate = 0.0 // No history, assume zero cooperation

    // Calculate Cooperation Gap
    // CooperationGap = m - (number of players I assume will cooperate if I defect)
    // For aggressive strategy, if I defect, assume all others will defect in a risk-averse way
    CooperationGap = m - (n - 1)

    // Decision Logic
    If CooperationGap <= 0: // Threshold can be met without my cooperation
        Return 'D' // Defect and free-ride

    Else: // My cooperation is necessary to meet the threshold
        // Estimate how many others will cooperate *if I cooperate*
        // This is a pessimistic estimate, accounting for others' potential risk aversion
        EstimatedPlayersCooperatingIfICooperate = floor(EstimatedOthersCooperationRate * (n - 1)) + 1

        If EstimatedPlayersCooperatingIfICooperate >= m:
            // My cooperation combined with estimated others' cooperation is enough
            Action = 'C'
            // Update CooperationPressure based on outcome of THIS round (after it's played)
            // This update is applied AFTER the decision, for the NEXT round's calculation
            // We need to know if m was met and my role, which happens *after* decision.
            // This is a conceptual update for the next round's state.
            // For implementation, this update logic would be in a separate "End Round" function.
            // If outcome is success (m met), and I cooperated: CooperationPressure *= 1.05
            // If outcome is failure (m not met), and I cooperated: CooperationPressure *= 0.95
            Return 'C'
        Else:
            Return 'D' // Not enough estimated cooperators, keep endowment

// --- Function to be called at the end of each round (for state updates) ---
Function UpdateState(current_round, my_action, outcomes_of_round):
    If current_round > 1:
        // Check if threshold was met in the round that just ended
        m_met_this_round = (outcomes_of_round['cooperators'] >= m)

        If my_action == 'C':
            If m_met_this_round:
                CooperationPressure *= 1.05 // Success with my help
            Else:
                CooperationPressure *= 0.95 // Failure with my help
        Else: // my_action == 'D'
            If not m_met_this_round:
                CooperationPressure *= 1.02 // Others failed to reach threshold, nudge me to consider cooperating next time
            // If m_met_this_round and I defected, CP remains neutral (1.0)

    // Handle Last Round logic (special decision in DecideAction for t==r)
    // The state updates are less relevant for the last round's decision as it's a one-off.

```
'''

description_AGGRESSIVE_60 = '''
## Aggressive Strategy: "The Opportunistic Enforcer"

This strategy aims to maximize individual gain by leveraging the collective reward while minimizing personal risk when possible. It is designed to be aggressive, pushing for cooperation when the reward is attainable and exploiting situations where others cooperate. It doesn't shy away from defection when cooperation is unlikely or personally disadvantageous.

### Core Philosophy:

*   **Leverage the Reward:** The primary driver is to secure the `k` multiplier, as this significantly boosts payoff.
*   **Minimize Risk:** Avoid contributing to a project that is unlikely to succeed, as this results in a lower payoff than defecting.
*   **Exploit Cooperation:** If others are cooperating to meet the threshold, defect to gain the full reward for yourself.
*   **Punish Defection (Indirectly):** By defecting when cooperation is unlikely, you discourage others from relying on your cooperation.

### Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on two main factors:

1.  **The Likelihood of Meeting the Threshold (`m`)**: This is estimated based on the observed cooperation in previous rounds.
2.  **The Current Status of the Game (First/Last Round)**: Special considerations are made for the initial and final rounds.

**Let's define some helper variables:**

*   `observed_cooperators_t-1`: The number of players observed to have cooperated in round `t-1`.
*   `my_action_t-1`: My action in round `t-1`.
*   `cooperation_trend`: A measure of how cooperation has evolved over recent rounds.

**Decision Logic for Round `t`:**

*   **If `t == 1` (First Round):**
    *   **Always Defect (D).** The first round is about assessing the environment. There's no historical data to judge cooperation likelihood, and cooperating without guarantees is risky. This sends a clear signal of self-interest.

*   **If `t == r` (Last Round):**
    *   **If `observed_cooperators_t-1 >= m`:** **Cooperate (C).** The reward is guaranteed, and there's no future round to exploit. Since others have already met the threshold, my cooperation ensures the reward for everyone, including myself.
    *   **If `observed_cooperators_t-1 < m`:** **Defect (D).** The threshold is not met, and there's no future round to influence. Cooperating here would be a sunk cost with no reward.

*   **If `1 < t < r` (Intermediate Rounds):**
    *   **Calculate a "Cooperation Urgency Score" (CUS):** This score quantifies how close we are to meeting the `m` threshold based on recent observations. A simple approach for `CUS` could be:
        `CUS = max(0, observed_cooperators_t-1 - (m - 1))`
        *   If `observed_cooperators_t-1` is less than `m-1`, `CUS` is 0, meaning we are far from the threshold.
        *   If `observed_cooperators_t-1` is `m-1`, `CUS` is 1, meaning we need just one more cooperator.
        *   If `observed_cooperators_t-1` is `m` or more, `CUS` is positive, meaning the threshold is met or exceeded.

    *   **Consider the "Cooperation Trend" (CT):** This is a more nuanced assessment. We look at the change in observed cooperators over the last few rounds (e.g., last 2-3 rounds).
        *   If the number of cooperators is generally *increasing*, `CT` is positive.
        *   If the number of cooperators is generally *decreasing*, `CT` is negative.
        *   If it's stable or mixed, `CT` is neutral.

    *   **Decision Logic for Intermediate Rounds:**
        *   **If `CUS > 0` AND `CT >= 0` (Threshold is met or exceeded, and cooperation is stable or increasing):**
            *   **If `my_action_t-1 == C` (I cooperated last round):** **Cooperate (C).** Maintain the winning strategy.
            *   **If `my_action_t-1 == D` (I defected last round):** **Defect (D).** Exploit the cooperation of others to maximize my gain. This is the core of the aggressive strategy – take the reward without contributing if others are already doing it.

        *   **If `CUS == 1` AND `CT >= 0` (We are just one cooperator away from the threshold, and cooperation is stable or increasing):**
            *   **If `my_action_t-1 == C` (I cooperated last round):** **Cooperate (C).** The risk is still somewhat managed by others' potential cooperation, and my contribution is crucial.
            *   **If `my_action_t-1 == D` (I defected last round):** **Cooperate (C).** The potential payoff is high, and my single contribution can unlock it for everyone. This is a calculated risk to secure the larger reward.

        *   **If `CUS == 0` OR `CT < 0` (Threshold is far from being met, or cooperation is declining):**
            *   **Always Defect (D).** There's too much uncertainty or a negative trend. Cooperating now would likely be a wasted endowment with no reward. Focus on preserving my own payoff.

### Pseudocode Representation:

```pseudocode
// Parameters: n, r, m, k
// History: List of actions for each player in each previous round

function MakeDecision(current_round_t, history):

    // --- Handle Edge Cases ---
    if current_round_t == 1:
        return D // Always defect in the first round to assess

    if current_round_t == r:
        observed_cooperators_last_round = CountCooperators(history, current_round_t - 1)
        if observed_cooperators_last_round >= m:
            return C // Guarantee the reward if threshold already met
        else:
            return D // Threshold not met, no future to influence

    // --- Intermediate Rounds Logic ---
    observed_cooperators_last_round = CountCooperators(history, current_round_t - 1)
    my_action_last_round = GetMyAction(history, current_round_t - 1)

    // Calculate Cooperation Urgency Score (CUS)
    cus = max(0, observed_cooperators_last_round - (m - 1))

    // Calculate Cooperation Trend (CT) - Simplified for illustration
    // More sophisticated CT calculation could involve regression or weighted averages
    cooperators_round_t_minus_2 = CountCooperators(history, current_round_t - 2)
    cooperators_round_t_minus_3 = CountCooperators(history, current_round_t - 3) // Example for 3 rounds lookback

    if observed_cooperators_last_round > cooperators_round_t_minus_2 AND observed_cooperators_last_round > cooperators_round_t_minus_3:
        ct = 1 // Increasing
    elif observed_cooperators_last_round < cooperators_round_t_minus_2 AND observed_cooperators_last_round < cooperators_round_t_minus_3:
        ct = -1 // Decreasing
    else:
        ct = 0 // Stable or mixed

    // Decision Logic
    if cus > 0 and ct >= 0: // Threshold met/exceeded and stable/increasing
        if my_action_last_round == C:
            return C // Maintain winning strategy
        else: // my_action_last_round == D
            return D // Exploit others' cooperation

    elif cus == 1 and ct >= 0: // One cooperator away and stable/increasing
        if my_action_last_round == C:
            return C // Continue contributing if it was working
        else: // my_action_last_round == D
            return C // Calculated risk to unlock reward

    else: // cus == 0 or ct < 0
        return D // Too uncertain or declining, defect to preserve payoff

// Helper functions:
// CountCooperators(history, round_number): Returns the number of players who played C in the specified round.
// GetMyAction(history, round_number): Returns my action in the specified round.
```

### Aggressiveness Justification:

1.  **Initial Defection:** This immediately signals that I am not a passive cooperator and will not be exploited. It forces others to show their hand first.
2.  **Exploitation when Threshold Met:** When `cus > 0` and `ct >= 0`, and I defected last round, I *choose* to defect. This is pure opportunism. I allow others to bear the risk of cooperation while I reap the rewards. This is highly aggressive as it directly benefits from their altruism (or their failed attempts to coordinate).
3.  **Calculated Risk for Breakthrough:** When `cus == 1` and `ct >= 0`, and I defected last round, I *choose* to cooperate. This is aggressive because it's a unilateral move to secure a higher payoff for everyone. It's not a general commitment to cooperation, but a specific, high-leverage gamble.
4.  **No Trust, No Forgiveness:** The strategy doesn't incorporate forgiveness mechanisms or attempts to build long-term trust. If cooperation falters, I revert to defection without hesitation.

This strategy is designed to be a strong contender in a tournament setting where unpredictable opponent behaviors are common. It prioritizes individual gain and leverages the game's reward structure ruthlessly.
'''

description_AGGRESSIVE_61 = '''
## Aggressive Strategy: "The Calculated Risk-Taker"

This strategy aims to maximize individual payoff by aggressively pursuing the higher reward when it's feasible, while minimizing exposure to losses by defecting when cooperation seems unlikely to succeed. It's designed to be opportunistic and exploit situations where others might be too hesitant.

The core principle is to **cooperate only when the probability of success (meeting the `m` threshold) is sufficiently high, and defect otherwise**. This probability is estimated based on the observed behavior of other players in previous rounds.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in a given round `t` depends on a calculated "cooperation probability threshold" and the observed cooperation rate in the previous round.

*   **If `t` is the first round (t=1):** Cooperate. This is a calculated risk. The initial endowment is 1, and the cost of cooperating is also 1. The reward for cooperation is `k`. By cooperating in the first round, we signal a willingness to contribute and potentially trigger a cascade of cooperation if others are also inclined. This is the only instance where we *initiate* cooperation without historical data.
*   **If `t` is NOT the first round (t > 1):**
    *   **Calculate the observed cooperation rate in round `t-1`:** Let `O_{t-1}` be the number of players (excluding yourself) who cooperated in round `t-1`. The observed cooperation rate is `(O_{t-1} + 1) / n` (including yourself if you cooperated).
    *   **Calculate the minimum number of additional cooperators needed:** Let `N_{needed}` be `m - O_{t-1}`. This is how many more players (excluding yourself) need to cooperate to reach the threshold.
    *   **Determine if Cooperate (C) or Defect (D):**
        *   **If `N_{needed} <= 0` (i.e., `m` players or more already cooperated in `t-1`):** **Cooperate (C).** The risk of not reaching the threshold is effectively zero, and the reward is guaranteed.
        *   **If `N_{needed} > 0` AND `N_{needed} <= n - 1 - O_{t-1}` (i.e., there are enough *other* players available to potentially reach the threshold if they all cooperate):**
            *   **Calculate the probability of *us* reaching the threshold if we cooperate:** This is a nuanced calculation. We're not trying to predict *specific* players. Instead, we're assessing the general inclination of the group. A simple heuristic is to ask: "If I cooperate, and a sufficient number of others *also* cooperate, do we meet the threshold?"
            *   **Decision Rule:** **Cooperate (C) if `(O_{t-1} + 1) >= m` OR if `(O_{t-1} + 1) * k / (n - O_{t-1} - 1) > 1` (This is a simplified heuristic to gauge if the expected reward outweighs the risk of individual loss).**  A more robust approach is to consider the *proportion* of players that need to cooperate. If `(O_{t-1} + 1) / n` is sufficiently high (e.g., greater than `m/n` by a certain margin), we cooperate.
            *   **Crucially, an aggressive approach leans towards cooperating if the *potential* for reward is high and the *individual risk* of being the sole cooperator is mitigated.** A good heuristic: **Cooperate (C) if the observed cooperation rate in `t-1` plus the potential for additional cooperation from the remaining `n - 1 - O_{t-1}` players is likely to reach `m`.**
            *   **Simplified Aggressive Decision:** **Cooperate (C) if `O_{t-1} >= m - 2`.** This means if `m-1` or `m` players (excluding ourselves) cooperated in the previous round, we will cooperate. This is aggressive because it requires only one or zero further cooperators.
        *   **If `N_{needed} > n - 1 - O_{t-1}` (i.e., even if all remaining players cooperate, the threshold cannot be met):** **Defect (D).** There is no realistic chance of reaching the threshold, so we maximize our individual payoff by keeping our endowment.

### 2. Handling Edge Cases:

*   **First Round (t=1):** As stated above, **Cooperate (C)**. This is the initial gamble to set a precedent and gauge the environment.
*   **Last Round (t=r):** The strategy remains the same as for any other round `t > 1`. The repeated nature of the game might encourage a different strategy in the final round (e.g., pure defection in a finitely repeated Prisoner's Dilemma). However, in this Collective Risk Dilemma, the reward is still significant. The "Calculated Risk-Taker" will continue to assess the probability of success. If the observed cooperation in the previous round was high, it will still cooperate. If it was low, it will defect.
*   **No Cooperation Observed in the Previous Round (`O_{t-1} = 0`):** If `m > 1`, then `N_{needed} = m`. If `m <= n-1`, we will likely defect unless `m` is very close to `n`, and the parameters suggest a high potential for future cooperation.
*   **All Players Cooperated in the Previous Round (`O_{t-1} = n-1`):** This is the ideal scenario. We will **Cooperate (C)**.

### 3. Aggressive Mindset Alignment:

*   **Proactive Cooperation:** Initiating cooperation in the first round demonstrates a willingness to invest, even without prior information.
*   **Opportunistic Exploitation:** The strategy aggressively seeks the reward by cooperating when the probability of success is high. It doesn't wait for absolute certainty, but rather for a strong indication.
*   **Calculated Risk:** The decision to cooperate is not blind. It's based on a reasoned assessment of the likelihood of success, using observed past behavior as a proxy.
*   **Minimizing Regret:** By defecting when the threshold is clearly unreachable, the strategy avoids the pain of cooperating and not receiving the reward. This is an aggressive stance to avoid losses.
*   **Leveraging High `k`:** If `k` is high, the reward for successful cooperation is substantial. This strategy is designed to capitalize on such high-reward opportunities.

---

### Pseudocode for "The Calculated Risk-Taker":

```python
# Game Parameters
n = ...
r = ...
m = ...
k = ...

# History Tracking
# `cooperation_history` will be a list of lists, where each inner list represents a round
# and contains a boolean for each player indicating if they cooperated (True) or defected (False).
# Example: [[True, False, True, ...], [False, False, ...], ...]
cooperation_history = []

def decide_action(current_round, n, m, k, cooperation_history):
    """
    Decides whether to Cooperate (C) or Defect (D) for the current player.

    Args:
        current_round (int): The current round number (1-indexed).
        n (int): Total number of players.
        m (int): Minimum number of cooperators needed.
        k (float): Reward factor.
        cooperation_history (list of lists): History of actions for all players in previous rounds.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    if current_round == 1:
        return 'C'  # Aggressive initial gamble

    # Get actions from the previous round (t-1)
    previous_round_actions = cooperation_history[-1] # The last element is the most recent round
    num_previous_cooperators = sum(previous_round_actions)

    # Number of other players who cooperated in the previous round
    # If current player was player i, and previous_round_actions[i] was True, it counts.
    # We need to adjust this if we are calculating for a specific player index,
    # but here we're assuming we're making the decision for 'ourselves' and
    # counting how many *others* cooperated based on the historical data.
    # For simplicity, let's assume we are one of the players and the history
    # includes our own past actions.
    # The number of *other* players who cooperated:
    num_others_cooperated_previous = num_previous_cooperators - (1 if previous_round_actions[current_player_index] else 0) # This assumes we know our index, which we don't in a general strategy.
    # Let's simplify and use the *total* number of cooperators in the previous round as an indicator.
    total_cooperators_previous_round = num_previous_cooperators

    # Number of additional cooperators needed
    # Note: This calculation assumes we are NOT counting ourselves towards the 'm' threshold
    # if we are trying to achieve it with others. The game spec implies 'm' players in total cooperate.
    # If we cooperate, we count as 1. So we need m-1 others, if we are among the cooperators.
    # If we defect, we need 'm' others.

    # Aggressive heuristic: If m-1 or fewer players are needed from the *rest* of the group, we cooperate.
    # This means if m-1 others cooperated in the previous round, we'll cooperate.
    # If m others cooperated, we definitely cooperate.

    # Simplified Aggressive Decision Rule:
    # If at least m-1 players (excluding potentially ourselves) cooperated in the previous round, we cooperate.
    if total_cooperators_previous_round >= m - 1:
        return 'C'

    # If not enough cooperators were observed in the previous round,
    # we defect to protect our endowment.
    else:
        return 'D'

# Example of how it might be used within a loop (conceptual):
# for t in range(1, r + 1):
#     # Assume 'my_actions' is a list where my_actions[i] is my action in round i
#     if t == 1:
#         my_action = 'C'
#     else:
#         my_action = decide_action(t, n, m, k, cooperation_history)
#
#     # Record my action (conceptually, for tracking history for future rounds)
#     # This requires managing how the history is updated based on ALL players' actions in a round.
#     # For this strategy description, we assume 'cooperation_history' is populated correctly for all players.
```

**Note on Pseudocode:** The pseudocode is simplified. In a real tournament implementation, `cooperation_history` would need to be a global or passed state that is updated after each round with the actions of *all* players. The `current_player_index` is not explicitly used here, assuming the strategy makes a decision for "itself" based on the aggregate history. The core logic focuses on the *number* of past cooperators. The simplified aggressive rule `total_cooperators_previous_round >= m - 1` captures the essence of being willing to cooperate if the threshold is nearly met.
'''

description_AGGRESSIVE_62 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed for adaptability and robustness in a tournament setting.

## Aggressive Strategy: "The Opportunistic Enforcer"

This strategy is built on the principle of maximizing immediate personal gain while leveraging the collective to achieve higher rewards, but with a strong bias towards self-preservation and exploiting situations where others are cooperating. It's "opportunistic" because it seeks to benefit from cooperation without being the sole initiator of risk, and "enforcer" because it aims to punish defectors and reward cooperators when it's safe and profitable to do so.

### 1. Decision Rules

The core of this strategy revolves around a dynamic assessment of the "cooperation score" and a threshold for action.

**Core Principle:** Cooperate *only* when the probability of reaching the `m` threshold is high *and* the expected payoff from cooperating outweighs the payoff from defecting. Otherwise, defect.

Let's define some key metrics:

*   **`cooperation_count_last_round`**: The number of players who cooperated in the previous round.
*   **`my_action_last_round`**: Player i's action in the previous round.
*   **`my_history`**: A record of player i's actions and the number of cooperators in each past round.
*   **`opponent_cooperation_tendency`**: An estimate of how likely each *other* player is to cooperate. This is dynamically updated.

**Decision Logic:**

1.  **First Round (`t == 1`): Defect.**
    *   **Reasoning:** In the first round, there's no history. Cooperating is a pure gamble. An aggressive strategy must avoid being exploited. The highest immediate payoff is guaranteed by defecting. This also allows observation of initial player behavior without incurring risk.

2.  **Subsequent Rounds (`t > 1`):**
    *   **Calculate a "Cooperation Probability Estimate" (`CPE`) for the current round.** This is the most crucial adaptive element.
        *   **Base CPE:** Start with the `cooperation_count_last_round` from the previous round.
        *   **Adjust for trends:**
            *   If `cooperation_count_last_round >= m` and `my_action_last_round == C`: Increase CPE slightly (rewarding success).
            *   If `cooperation_count_last_round < m` and `my_action_last_round == C`: Decrease CPE slightly (punishing failure due to your cooperation).
            *   If `cooperation_count_last_round >= m` and `my_action_last_round == D`: Increase CPE significantly (rewarding observation of successful cooperation).
            *   If `cooperation_count_last_round < m` and `my_action_last_round == D`: No significant change to CPE, or a slight decrease if the trend is clearly downward.
        *   **Adjust for individual opponent tendencies:** Maintain an internal estimate for each opponent `j` of their probability to cooperate, `P_j_cooperate`. This can be a simple running average or a more sophisticated weighted average of their past cooperation.
        *   **`CPE = (Base CPE * (n - 1) + Σ(j != i) P_j_cooperate) / n`** (This is a simplified representation; a more nuanced approach would weight recent behavior more heavily). The idea is to blend the observed collective behavior with inferred individual behavior.

    *   **Evaluate Cooperation Threshold:**
        *   **If `CPE * n >= m - 1` (my cooperation will likely tip the balance):**
            *   **Calculate Expected Payoff of Cooperating:** `E_cooperate = (1 - 1) + k * (1 - P(threshold not met)) + (1 - 1) * P(threshold not met)`
                *   If threshold is met: `0 + k = k`
                *   If threshold is not met: `0` (assuming `k > 1`)
                *   So, `E_cooperate = CPE * k`
            *   **Calculate Expected Payoff of Defecting:** `E_defect = 1 + k * (1 - P(threshold not met)) + 1 * P(threshold not met)`
                *   If threshold is met: `1 + k`
                *   If threshold is not met: `1`
                *   So, `E_defect = CPE * (1 + k) + (1 - CPE) * 1 = CPE + CPE*k + 1 - CPE = 1 + CPE*k`
            *   **Decision:**
                *   **If `E_cooperate > E_defect` (which simplifies to `k > 1`, always true by definition) AND `CPE * n >= m - 1` (i.e., my cooperation is likely critical for success): THEN Cooperate (C).**
                *   **Else:** **Defect (D).**

        *   **Else (`CPE * n < m - 1`, my cooperation is not likely to reach the threshold on its own):**
            *   **If `cooperation_count_last_round >= m` (threshold was met last round):**
                *   This means others successfully cooperated.
                *   **Decision:** **Cooperate (C).**
                *   **Reasoning:** Benefit from the established reward. My contribution is essentially "free" in terms of risk, as the threshold is likely to be met by others. This encourages continued cooperation from others.
            *   **Else (`cooperation_count_last_round < m`):**
                *   The threshold was not met.
                *   **Decision:** **Defect (D).**
                *   **Reasoning:** Avoid contributing to a failed collective effort. Maximize personal payoff of 1.

3.  **Last Round (`t == r`): Defect.**
    *   **Reasoning:** In the final round, there's no future interaction to influence. The optimal aggressive strategy is to maximize immediate payoff. Since the future reward of cooperation is gone, the surest path to a decent payoff is to defect. This also prevents being the sole cooperator in a potentially failed round.

### 3. Aggressive Elements

*   **First Round Defection:** A hard stance on avoiding initial risk.
*   **Conditional Cooperation:** Cooperation is never a default. It's a calculated risk taken only when success is highly probable and personally beneficial.
*   **Exploiting Successful Cooperation:** Once others have proven they can reach the threshold, this strategy jumps in to reap the rewards without taking the initial risk. This is a form of "free-riding" but is aggressive in that it aggressively seeks the reward once the path is paved.
*   **Punishing Failed Cooperation:** By defecting when the threshold isn't met, the strategy withdraws its potential contribution, further reducing the chances of future success if the group is not self-sustaining.
*   **Focus on Maximizing Individual Payoff:** Every decision is framed around what yields the highest personal gain in the current and immediately foreseeable future.
*   **"Enforcer" Aspect:** While not explicitly punishing defectors with lower payoffs (as that's not possible directly in this game), it "punishes" failure by withdrawing cooperation, thus making future collective success harder if the group relies on its contribution. It rewards *itself* for the risk when others' risk-taking proves successful.

### Pseudocode Representation

```pseudocode
// Strategy: Opportunistic Enforcer

// Parameters: n, r, m, k

// Internal State:
// cooperation_count_last_round
// my_action_last_round
// opponent_cooperation_tendency = array of size n (initialized to 0.5) // Probability of opponent j cooperating
// round_history = list of (my_action, num_cooperators) tuples

function decide_action(current_round, opponent_actions_history):
    // opponent_actions_history: list of actions from previous rounds for all players

    if current_round == 1:
        my_action_last_round = null
        cooperation_count_last_round = 0
        return D // Always defect in the first round

    if current_round == r:
        return D // Always defect in the last round

    // --- Update Internal State based on previous round ---
    // (Need to have stored this from the previous round's execution)
    // This pseudocode assumes state is accessible.

    // --- Calculate Cooperation Probability Estimate (CPE) ---
    // Simple weighted average for opponent tendency (e.g., 70% recent, 30% overall)
    updated_opponent_tendency = calculate_opponent_tendencies(opponent_actions_history, current_round - 1)
    opponent_cooperation_tendency = blend(opponent_cooperation_tendency, updated_opponent_tendency, 0.7) // Blend with recent data

    // Base CPE from observed collective behavior
    base_cpe = cooperation_count_last_round / n

    // Incorporate individual tendencies (simplified)
    sum_individual_tendencies = 0
    for j from 1 to n:
        if j != my_player_id: // Assuming we know our ID
            sum_individual_tendencies += opponent_cooperation_tendency[j]

    // Combine collective and individual estimates (adjust weights as needed)
    CPE = (base_cpe * (n - 1) + sum_individual_tendencies) / n

    // --- Decision Logic ---

    // Condition 1: My cooperation is likely critical for success
    if CPE * n >= m - 1:
        // Expected payoff of cooperating (simplified as E_cooperate = CPE * k)
        E_cooperate = CPE * k
        // Expected payoff of defecting (simplified as E_defect = 1 + CPE * k)
        E_defect = 1 + CPE * k

        if E_cooperate > E_defect: // This is effectively always true if k > 1, so the condition is primarily CPE * n >= m - 1
             // But for robustness, we can ensure we are not *losing* expected value by cooperating
             return C
        else:
             return D // Should not happen if k > 1 and CPE is reasonable

    // Condition 2: Threshold was met last round (others did the heavy lifting)
    elif cooperation_count_last_round >= m:
        return C // Reaping the rewards

    // Condition 3: Threshold was NOT met last round (collective effort failed)
    else: // cooperation_count_last_round < m
        return D // Avoid contributing to failure

// Helper function (conceptual):
// calculate_opponent_tendencies(actions_history, last_round_index):
//   Iterate through actions_history for each opponent to estimate their cooperation probability.
//   Can be a simple average or weighted average of their past 'C' actions.

// Helper function (conceptual):
// blend(old_tendencies, new_tendencies, weight):
//   Creates a new tendency array by blending old and new with a given weight.

```

This strategy is aggressive because it prioritizes its own immediate gains while strategically leveraging the collective. It doesn't shy away from defecting to protect itself, but it aggressively seeks to capitalize on the rewards of cooperation once others have demonstrated its feasibility. Its adaptability comes from its dynamic estimation of cooperation probability, allowing it to adjust to the emergent behaviors of its opponents.
'''

description_AGGRESSIVE_63 = '''
Here's an aggressive strategy designed for the Collective Risk Dilemma, focusing on maximizing individual gain while leveraging the collective reward when strategically beneficial. This strategy is adaptive and robust, designed to thrive in a competitive tournament environment.

## Aggressive Strategy: "The Calculated Risk-Taker"

This strategy embodies a "cooperate only when the odds are overwhelmingly in my favor, and defect otherwise" mentality. It prioritizes securing the individual endowment while opportunistically seizing the collective reward when the risk of failure is minimal.

### Core Philosophy:

*   **Default to Defection:** The safest and most consistently profitable action, in isolation, is to defect. This guarantees an individual payoff of 1.
*   **Cooperate Only When Sure (or close to it):** Cooperation is only considered when there's a high probability that the minimum number of cooperators ($m$) will be met or exceeded. This minimizes the risk of wasting an endowment and receiving no reward.
*   **Leverage History to Predict and Punish:** Past behavior of other players is crucial for predicting their future actions. This strategy uses this information to influence its own decisions and to implicitly punish exploitative behavior.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in a given round depends on a calculated probability of success.

**Key Metric: "Cooperation Probability Estimate" (CPE)**

This is the core of the adaptive nature of the strategy. The CPE represents the estimated probability that *at least* $m$ players will cooperate in the current round, *excluding* the strategy itself.

**How CPE is Calculated:**

For each player $j \neq i$, we maintain an internal estimate of their propensity to cooperate. This is done by tracking their historical cooperation rates.

*   **Player $j$'s Cooperation Propensity ($P_j$):** This is a dynamic value, initially starting at 0.5 (neutral). It's updated based on player $j$'s actions in previous rounds.
    *   If player $j$ cooperated in the last round: $P_j \leftarrow P_j + \delta$ (where $\delta$ is a small increment, e.g., 0.05).
    *   If player $j$ defected in the last round: $P_j \leftarrow P_j - \delta$.
    *   Ensure $P_j$ stays within [0, 1].

*   **Calculating CPE:** This is the most complex part, as it involves estimating the joint probability of multiple independent (or weakly dependent) events. A simplified, yet effective, approach for this strategy is:
    1.  **Assume Independence (for initial estimation):** For each player $j \neq i$, consider their current cooperation propensity $P_j$.
    2.  **Identify "Strong Cooperators" and "Strong Defectors":**
        *   **Strong Cooperators:** Players with $P_j > \text{Coop_Threshold}$ (e.g., 0.7). We assume they are highly likely to cooperate.
        *   **Strong Defectors:** Players with $P_j < \text{Defect_Threshold}$ (e.g., 0.3). We assume they are highly likely to defect.
        *   **Undecided/Neutral Players:** Players whose $P_j$ falls between the thresholds.
    3.  **Calculate Minimum Required Cooperators from "Strong Cooperators":** Let $N_{SC}$ be the number of players identified as Strong Cooperators. The minimum number of additional cooperators needed from the remaining players (Undecided and Strong Defectors) is $M_{rem} = \max(0, m - N_{SC})$.
    4.  **Estimate Probability of Reaching $M_{rem}$:**
        *   If $M_{rem} = 0$, the CPE is high (close to 1).
        *   If $M_{rem} > 0$, we need to estimate the probability that at least $M_{rem}$ of the remaining $(n-1-N_{SC})$ players will cooperate. For simplicity in this strategy, we'll take a more aggressive stance: **If the number of Strong Cooperators ($N_{SC}$) is already greater than or equal to $m$, we consider the threshold met.** This is a pragmatic, aggressive approach that relies on identifying early indicators of success.
        *   **A more refined CPE could involve binomial probability calculations, but for an aggressive, parameter-driven strategy, prioritizing certainty is key.**

**Cooperation Decision Logic:**

*   **If CPE > Coop_Confidence_Threshold (e.g., 0.9):** Cooperate (C). This means we are very confident that at least $m$ players will cooperate, including us. The potential gain ($k-1$) outweighs the risk of losing our endowment (1).
*   **Else (CPE <= Coop_Confidence_Threshold):** Defect (D). This is the default. We are not sufficiently confident that the collective reward will be achieved, so we prioritize securing our individual endowment.

**Aggressive Adjustment:**

*   **Proactive Cooperation (Conditional):** If the number of *observed* cooperators in the *previous* round was exactly $m-1$, *and* our CPE is high (e.g., > 0.7), we might consider cooperating in the current round to tip the balance. This is a calculated risk to initiate the reward.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Default to Defection (D).** Without any historical data, there's no basis for estimating cooperation probability. This is the safest, most aggressive starting point.
    *   *Rationale:* Avoid being exploited in the first round. Observe and gather data.

*   **Last Round (t=r):**
    *   **Apply the same decision logic as in intermediate rounds.** The strategy doesn't change based on the game ending, it remains consistent with its aggressive, probability-based approach.
    *   *Rationale:* There's no future to build cooperation for. Each round's decision is still about maximizing immediate expected payoff based on the current information.

*   **Low Number of Players (n close to m):**
    *   The strategy becomes more cautious. If $n$ is only slightly larger than $m$, even a few defectors can prevent the reward. The CPE calculation will naturally reflect this higher risk.
    *   *Example:* If $n=4, m=3$, and we only see 2 strong cooperators, our CPE will likely be low, leading to defection.

*   **High Number of Players (n >> m):**
    *   The strategy can be more optimistic. With many players, it's easier to reach the $m$ threshold. If we see a few strong cooperators, the CPE can rise quickly.

### 3. Aggressive Nature:

*   **"Cooperate to Trigger, Defect to Secure":** The strategy aggressively seeks to cooperate *only* when the outcome is almost guaranteed. It's not about building trust; it's about maximizing personal gain from a successful collective action.
*   **Punishment (Implicit):** If other players consistently defect while the strategy is trying to cooperate (and failing to meet the threshold), the strategy will learn this and continue to defect, thus denying them the reward they didn't contribute to. It doesn't explicitly punish; it simply adapts to the observed lack of cooperation.
*   **Exploiting Predictability:** If the strategy identifies a predictable pattern of cooperation from a subset of players, it can leverage this. If, for example, players X, Y, and Z always cooperate when $m$ is met, and our CPE suggests they will, the strategy might then choose to cooperate itself to secure the reward.
*   **No "Forgiveness" in the Traditional Sense:** The strategy doesn't "forgive" defectors or try to establish long-term cooperation. It reacts to current probabilities. If defectors persist, the strategy will continue to defect.

### Pseudocode Representation:

```python
# Game Parameters (assumed to be global or passed)
n = ...
r = ...
m = ...
k = ...

# Strategy State (per player i)
player_cooperation_propensity = {player_id: 0.5 for player_id in range(1, n + 1) if player_id != my_player_id}
my_player_id = ... # Assume this is known

# Strategy Constants
COOP_CONFIDENCE_THRESHOLD = 0.9
COOP_THRESHOLD = 0.7
DEFECT_THRESHOLD = 0.3
PROPENSITY_UPDATE_DELTA = 0.05

def decide_action(round_number, history):
    """
    Determines the action (C or D) for the current round.

    Args:
        round_number: The current round number (1-indexed).
        history: A list of dictionaries, where each dict contains
                 'actions': {player_id: action (C/D)} for that round.
    """

    # --- Update Player Cooperation Propensities ---
    if round_number > 1:
        previous_round_actions = history[-1]['actions']
        for player_id, propensity in player_cooperation_propensity.items():
            if player_id in previous_round_actions:
                if previous_round_actions[player_id] == 'C':
                    player_cooperation_propensity[player_id] = min(1.0, propensity + PROPENSITY_UPDATE_DELTA)
                elif previous_round_actions[player_id] == 'D':
                    player_cooperation_propensity[player_id] = max(0.0, propensity - PROPENSITY_UPDATE_DELTA)

    # --- Calculate Cooperation Probability Estimate (CPE) ---
    strong_cooperators_count = 0
    num_other_players = n - 1 # Excluding self

    for player_id, propensity in player_cooperation_propensity.items():
        if propensity >= COOP_THRESHOLD:
            strong_cooperators_count += 1

    # Aggressive CPE calculation: If enough strong cooperators are identified,
    # we assume the threshold will be met.
    if strong_cooperators_count >= m:
        cpe = 1.0 # Very high confidence
    else:
        # A more sophisticated CPE could use binomial distribution here,
        # but for aggressive strategy, we'll keep it simpler:
        # If not enough strong cooperators identified, assume lower confidence.
        # This is a simplified proxy for CPE.
        # For example, if we have 0 strong cooperators and need m, our confidence is low.
        # A possible heuristic: CPE = (strong_cooperators_count / m) * (probability of remaining players cooperating)
        # For this aggressive strategy, we prioritize certainty.
        # If strong_cooperators_count < m, we might set a very low CPE unless there's
        # a clear signal from the last round.

        # Simplified CPE heuristic:
        # If we have enough strong cooperators, CPE is high. Otherwise, it's low.
        # We could add more nuance based on the number of undecided players.
        if strong_cooperators_count >= m:
            cpe = 1.0
        else:
            # This is a placeholder for a more nuanced CPE.
            # For an aggressive default-to-defect, if strong cooperators aren't enough,
            # the CPE should be low.
            cpe = 0.3 # Low confidence by default if not enough strong cooperators


    # --- Proactive Cooperation Check (Optional Aggressive Tweak) ---
    proactive_cooperate = False
    if round_number > 1:
        previous_round_actions = history[-1]['actions']
        observed_cooperators = sum(1 for action in previous_round_actions.values() if action == 'C')
        if observed_cooperators == m - 1 and cpe > 0.7: # High confidence and close to threshold
            proactive_cooperate = True

    # --- Decision Logic ---
    if proactive_cooperate:
        return 'C'
    elif cpe >= COOP_CONFIDENCE_THRESHOLD:
        return 'C'
    else:
        return 'D' # Default to Defect

```
'''

description_AGGRESSIVE_64 = '''
Here is an aggressive strategy for the Collective Risk Dilemma, designed to exploit opportunities for high payoffs while remaining resilient to defection.

## Strategy Name: **The Unyielding Opportunist**

This strategy is built on the principle of **proactive exploitation**. We aim to be the first to benefit from cooperation when it's advantageous, and to punish defection swiftly and decisively when it jeopardizes our gains. The core idea is to push for cooperation when the reward is high and the risk of individual loss is minimized, but to immediately revert to defection to maximize personal gain when cooperation fails or is insufficient.

### 1. Decision Rules:

Our decision in each round will be based on a calculated "cooperation threshold" and a history of observed player behavior.

*   **Primary Decision Rule (Cooperation):** Cooperate if and only if:
    *   The number of players who have cooperated in the **previous round** (let's call this `previous_cooperators`) is at least `m - 1` **AND**
    *   The total number of players `n` is less than or equal to `m * k` (This is a heuristic to avoid over-investing in large groups where coordination is less likely and the risk of being exploited is higher. It essentially asks: "Is the reward `k` sufficiently large relative to the potential number of defectors if we all cooperate?")

*   **Secondary Decision Rule (Defection):** Defect if the primary cooperation condition is NOT met.

*   **Aggression Modifier:** In addition to the primary rule, we will **always defect** if we observe that fewer than `m` players cooperated in the *previous* round. This is a harsh penalty for insufficient cooperation.

### 2. Handling Edge Cases:

*   **Round 1 (Initial Round):**
    *   In the very first round, we have no historical data. To be aggressive and test the waters for exploitation, we will **cooperate**. This signals a willingness to contribute and potentially trigger the reward if others do the same. This is a calculated risk to initiate potential gains.

*   **Last Round (Round `r`):**
    *   The last round is crucial because there are no future repercussions. Our strategy becomes purely selfish:
        *   If `m-1` or more players cooperated in the *previous* round (round `r-1`), **we will defect**. We aim to collect our private endowment plus the reward `k` without contributing, as there's no benefit to fostering future cooperation.
        *   If fewer than `m-1` players cooperated in the previous round, we also **defect**.
    *   **Simplified Last Round Rule:** **Always defect in the last round.** This is the ultimate expression of aggressive self-interest.

*   **Near Threshold Scenarios:**
    *   If `previous_cooperators == m - 1`: We will cooperate. This is the most critical point for triggering the reward. Our strategy is to "tip the scale" and ensure the threshold is met.

### 3. Aggressive Mindset Alignment:

*   **Proactive Cooperation for Gain:** We initiate cooperation in Round 1 and when the conditions are very close to meeting the threshold. This is not out of altruism, but a calculated move to secure the `k` reward as soon as possible.
*   **Ruthless Punishment:** If cooperation falls short in any round (except Round 1), we immediately defect. This deters free-riding and maximizes our immediate payoff.
*   **Exploiting the Last Round:** The absolute defection in the final round ensures we extract maximum value regardless of past interactions.
*   **`n <= m * k` Heuristic:** This parameter checks if the reward `k` is "worth the trouble." If `k` is low relative to `n` and `m`, it suggests that even with full cooperation, the individual payoff might not be significantly better than defecting in a scenario where a few players defect. This prevents us from "wasting" cooperation on a low-yield scenario.

### Pseudocode:

```pseudocode
strategy UnyieldingOpportunist(round_number, n, m, k, history):
  # history is a list of dictionaries, where each dictionary represents a round and contains 'player_actions' (list of 'C' or 'D') and 'player_payoffs'

  # Edge Case: Last Round
  if round_number == r:
    return 'D'

  # Round 1: Initial aggressive probe
  if round_number == 1:
    return 'C'

  # Calculate cooperators in the previous round
  previous_round_data = history[round_number - 2] # history is 0-indexed for rounds 1 to r-1
  previous_cooperators = count_cooperators(previous_round_data['player_actions'])

  # Aggression Modifier: Punish insufficient cooperation
  if previous_cooperators < m:
    return 'D'

  # Primary Cooperation Decision Rule
  if previous_cooperators >= m - 1 and n <= m * k:
    return 'C'
  else:
    return 'D'

# Helper function (not part of strategy itself, but for implementation)
function count_cooperators(actions):
  count = 0
  for action in actions:
    if action == 'C':
      count += 1
  return count
```

This strategy is designed to be a formidable opponent. It's aggressive in seeking high payoffs, quick to punish failures, and ruthlessly self-interested when opportunities arise, particularly in the endgame. It adapts by learning from the previous round's outcomes, but its core aggressive nature remains constant.
'''

description_AGGRESSIVE_65 = '''
This is an intriguing game where cooperation is beneficial but carries the risk of being exploited. The core challenge is to balance the potential for high rewards with the need to protect your own endowment.

Here's an aggressive strategy designed for the Collective Risk Dilemma:

## Aggressive "Opportunistic Sentinel" Strategy

This strategy is named "Opportunistic Sentinel" because it aims to capitalize on opportunities for high rewards while remaining vigilant and ready to punish defection. It's inherently aggressive by prioritizing actions that maximize potential gains, but it's also adaptive to the historical behavior of other players.

### Core Philosophy:

*   **Maximize Expected Payoff:** My primary objective is to maximize my cumulative payoff over `r` rounds.
*   **Exploit Cooperation, Punish Defection:** I want to be part of the cooperating group when it succeeds, but I will not be the sole contributor to a failed cooperative effort. If others defect and I'm left holding the bag, they will face consequences.
*   **Low Tolerance for Exploitation:** I will not be a consistent "free rider." If I am consistently exploited, my strategy will shift towards a more self-serving approach.
*   **Calculated Risk-Taking:** I am willing to take risks, but only when the potential reward significantly outweighs the potential loss, and when the likelihood of success (based on observed behavior) is sufficiently high.

### 1. Decision Rules:

My decision in each round `t` will be based on a dynamic threshold that considers the following:

*   **Observed Cooperation Rate in the Previous Round (O_t-1):** The percentage of players (excluding myself) who cooperated in the previous round.
*   **My Payoff in the Previous Round (π_i,t-1):** Whether I was rewarded or not.
*   **Current Round Number (t):** To handle early and late-game dynamics.
*   **Proximity to the Threshold (m):** How close the group was to meeting the `m` cooperation requirement.

**My Decision Rule (Player `i` in round `t`):**

I will cooperate (play `C`) if:

1.  **Early Game Aggression (t <= r / 3):**
    *   If the number of players who cooperated in the previous round (excluding myself) is **greater than or equal to `m - 2`**.
    *   *Rationale:* In the early rounds, there's a higher tolerance for risk to establish a cooperative norm. If a significant number of others are already leaning towards cooperation, I'll join to increase the chances of hitting the threshold.

2.  **Mid-Game Opportunism (r / 3 < t <= 2*r / 3):**
    *   If I cooperated in the previous round (`c_i,t-1 = 1`) **AND** the threshold `m` was met or exceeded in the previous round (`O_t-1 >= m - 1`).
    *   **OR**
    *   If I defected in the previous round (`c_i,t-1 = 0`) **AND** the threshold `m` was met or exceeded in the previous round (`O_t-1 >= m - 1`) **AND** the total number of cooperators (including me if I were to cooperate) would be **at least `m`**.
    *   *Rationale:* In the mid-game, I've learned from observed behavior. I'll cooperate if the group demonstrated success in the previous round, especially if I benefited. I'm also willing to jump in if I see a clear path to success based on others' cooperation, even if I defected last round.

3.  **Late-Game Pragmatism (t > 2*r / 3):**
    *   If I cooperated in the previous round (`c_i,t-1 = 1`) **AND** the threshold `m` was met or exceeded in the previous round (`O_t-1 >= m - 1`).
    *   **OR**
    *   If the number of players who cooperated in the previous round (excluding myself) is **greater than or equal to `m - 1`**.
    *   *Rationale:* In the late game, I want to secure remaining potential payoffs. I'll cooperate if cooperation was successful previously or if the group is very close to the threshold. This is about maximizing my score in the final stretch.

I will defect (play `D`) in all other cases.

**Key Threshold Adjustment:** My internal threshold for cooperation implicitly increases if I was exploited in the previous round.

**Strategic Adjustment (My Own "Punishment" Mechanism):**

*   If I played `C` in the previous round (`c_i,t-1 = 1`) and the threshold `m` was **NOT** met, and my payoff was `1 - c_i`, meaning I gained nothing (and potentially lost `1`), my internal "trust" for cooperation decreases. In the next round, my cooperation threshold will be harder to meet by 1 player. This is implicitly handled by the conditions above, where successful cooperation is a strong motivator to cooperate again. If cooperation fails, the conditions to cooperate again become stricter.

### 2. Handling Edge Cases:

*   **Round 1 (t = 1):**
    *   **Action:** I will **defect (D)**.
    *   *Rationale:* In the first round, there is no history to learn from. Cooperating without knowing how others will behave is a significant risk. It's better to secure my private endowment and observe the group's initial tendencies. This also prevents me from being the sole cooperator and thus exploited.

*   **Last Round (t = r):**
    *   **Decision:** I will follow the **Late-Game Pragmatism** rules outlined above.
    *   *Rationale:* The objective is to maximize my total payoff. If there's a clear path to a reward, I'll take it. If not, I'll secure my private endowment.

### 3. Aggressive Elements:

*   **Early Cooperation Initiation (with conditions):** While I don't initiate cooperation blindly in Round 1, the conditions for cooperation in the early rounds (`t <= r / 3`) are designed to be more forgiving to encourage cooperation if others are showing promise. This is a controlled aggression to try and establish a beneficial norm.
*   **Leveraging Group Success:** My strategy is designed to quickly jump on the bandwagon of successful cooperation. If the group hits the threshold, I'm very likely to contribute in the next round if history suggests it's profitable.
*   **"Deterrence" by Example:** By defecting in the first round, I signal that I won't be a guaranteed cooperator. If others cooperate and fail, and I get a low payoff, my future behavior will reflect that. The conditions for me to cooperate again become stricter, effectively punishing past exploitation.
*   **Maximizing Potential Gains:** The conditions for cooperation in mid and late rounds are geared towards maximizing my chances of achieving the `k` reward when the probability of hitting the threshold is higher based on past performance.

### Pseudocode (Illustrative):

```python
def decide_action(player_index, current_round, history, parameters):
    n = parameters['n']
    r = parameters['r']
    m = parameters['m']
    k = parameters['k']

    # Default to defect
    action = 'D'

    # --- Round 1: Secure endowment ---
    if current_round == 1:
        return 'D'

    # --- Get historical data for previous round ---
    prev_round = current_round - 1
    if prev_round not in history or len(history[prev_round]) == 0:
        # Should not happen after round 1 if history is managed correctly
        return 'D'

    prev_actions = history[prev_round] # Dictionary: {player_id: action}
    my_prev_action = prev_actions.get(player_index)
    other_players_prev_actions = {p: a for p, a in prev_actions.items() if p != player_index}

    # Calculate number of cooperators in previous round (excluding self for observation)
    num_prev_cooperators = sum(1 for action in other_players_prev_actions.values() if action == 'C')

    # --- Decision Logic ---

    # Early Game Aggression (t <= r / 3)
    if current_round <= r // 3:
        if num_prev_cooperators >= m - 2:
            action = 'C'

    # Mid-Game Opportunism (r / 3 < t <= 2*r / 3)
    elif r // 3 < current_round <= 2 * r // 3:
        threshold_met_prev = (num_prev_cooperators + (1 if my_prev_action == 'C' else 0)) >= m

        if my_prev_action == 'C' and threshold_met_prev:
            action = 'C'
        elif my_prev_action == 'D' and threshold_met_prev:
            # Can I contribute to a successful group?
            potential_cooperators_if_i_cooperate = num_prev_cooperators + 1
            if potential_cooperators_if_i_cooperate >= m:
                 action = 'C'

    # Late-Game Pragmatism (t > 2*r / 3)
    else: # current_round > 2 * r // 3
        threshold_met_prev = (num_prev_cooperators + (1 if my_prev_action == 'C' else 0)) >= m

        if my_prev_action == 'C' and threshold_met_prev:
            action = 'C'
        elif num_prev_cooperators >= m - 1: # Very close to threshold
            action = 'C'

    return action

```

This strategy aims to be aggressive by seizing opportunities and reacting strongly to exploitation, but it's not reckless. It relies on learning from the past and adapting its risk-taking based on the observed behavior of the collective.
'''

description_AGGRESSIVE_66 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize your payoff by pushing for cooperation only when it's strategically advantageous and defecting otherwise. This strategy emphasizes exploiting opportunities for higher gains while minimizing risk.

## Strategy: The Opportunistic Dominator

This strategy aims to be aggressive by prioritizing the highest possible individual payoff at each decision point, while leveraging the repeated nature of the game to learn and adapt. It doesn't seek to be a "good citizen" but rather a shrewd player looking to extract maximum value.

**Core Philosophy:** "I'll help achieve collective success if and only if it guarantees me a significantly higher payoff than defecting, and I'll defect otherwise. I will punish defection aggressively when it hurts me, and exploit successful cooperation when it benefits me."

---

### 1. Decision Rules

Your decision in round `t` for player `i` depends on the observed history of actions and the current state of the game.

**General Rule (Rounds t = 2 to r):**

*   **If the previous round (t-1) resulted in a successful cooperation (m or more players cooperated):**
    *   **Examine your payoff in round t-1:**
        *   **If your payoff in round t-1 was `1` (meaning you defected in a successful cooperation round):** Defect (D). You exploited the cooperation of others and will continue to do so.
        *   **If your payoff in round t-1 was `k` or `k+1` (meaning you cooperated in a successful cooperation round):** Cooperate (C). You enjoyed the benefits of cooperation and are willing to risk it again, as the payoff is good.
*   **If the previous round (t-1) resulted in a failed cooperation (fewer than m players cooperated):**
    *   **Examine the number of cooperators in round t-1:**
        *   **If fewer than `m-1` players cooperated in round t-1:** Defect (D). The risk of failure is too high, and you don't want to be the one "wasted" cooperation.
        *   **If exactly `m-1` players cooperated in round t-1:**
            *   **Calculate your potential payoff if you cooperate:** If you cooperate, `m` players will cooperate, and your payoff will be `k`.
            *   **Calculate your payoff if you defect:** Your payoff will be `1`.
            *   **Decision:** If `k > 1` (which is guaranteed by parameters), Cooperate (C). You are the margin to ensure success and a higher payoff.

---

### 2. Handling Edge Cases

**Round 1 (Initial Play):**

*   **The "Calculated Risk" Gambit:** Since there's no history, you must make an informed guess. The most aggressive approach is to *lean towards cooperation if it's very likely to succeed, and defect otherwise*.
    *   **If `n` is close to `m` (e.g., `n - m < some_small_threshold`, say 2):** Cooperate (C). The chances of hitting the threshold with minimal effort are high.
    *   **Otherwise:** Defect (D). Do not risk your endowment without more information. This is about conserving your resources for opportunities.

**Last Round (Round r):**

*   **The "Final Payout Maximization":** In the last round, future repercussions don't matter. Your goal is simply to maximize your own payoff *in that round*.
    *   **If `m` or more players are *certain* to cooperate (based on observed strategy of others or if they have signaled a strong intent to cooperate):** Cooperate (C). You will get `k` or `k+1`.
    *   **Otherwise (if you suspect cooperation might fail):** Defect (D). You will get `1`. This is about self-preservation and exploiting potential free-rides.

**Specific Scenarios & Refinements:**

*   **The "Near Miss" Escalation:** If in the previous round, exactly `m-1` players cooperated, and you defected (resulting in `m-1` cooperators total), in the current round, you will try to push it over the edge by cooperating. This is your explicit mechanism for achieving success when it's just out of reach.
*   **The "Exploitative Exploiter":** If in the previous round, `m` or more players cooperated, and you defected, you got a payoff of `1 + k`. This is your highest possible individual payoff. You will continue to defect in the current round, hoping others will cooperate again. You are a pure opportunist.
*   **The "Fearful Follower" (when needed):** If in the previous round, `m` or more players cooperated, and you cooperated (getting `k` or `k+1`), and the number of cooperators was *significantly higher* than `m` (e.g., `num_cooperators >= m + x`, where `x` is a small number like 2), you may consider defecting in the current round. This is to "conserve" your cooperation effort if the group is already self-sustaining and you don't need to contribute to ensure success. **However, for an aggressively defined strategy, this would be a secondary consideration. Stick to the primary rule of C if you got a good payoff.**

---

### 3. Aggressive Mindset Alignment

*   **Prioritizing Individual Gain:** This strategy is explicitly designed to achieve the highest possible individual payoff. It doesn't shy away from exploiting others' cooperation.
*   **"I Cooperate Only When It Pays Me Significantly More":** The condition `k > 1` is the baseline. You cooperate when defecting yields `1` and cooperating yields `k` or `k+1`.
*   **Punishing Failure (indirectly):** By defecting when cooperation is failing and only `m-1` players cooperate, you are indirectly signaling that you won't carry the load indefinitely. You're forcing others to step up.
*   **Exploiting Success:** If others cooperate and you can get `1+k` by defecting, you absolutely will. This is the essence of aggressive play in this dilemma – maximizing your score by leveraging the collective effort.
*   **Round 1 Conservatism:** Starting conservatively in Round 1 is an aggressive tactic. It prevents you from being exploited early and allows you to gather information without immediately committing resources.

---

### Pseudocode Representation

```
Strategy: OpportunisticDominator

Parameters: n, r, m, k

// Store history of actions and payoffs for each player
// history_actions[round][player_id] = 'C' or 'D'
// history_payoffs[round][player_id] = payoff_value

Function DecideAction(current_round, player_id, history_actions, history_payoffs):
    // --- Handle Round 1 ---
    If current_round == 1:
        If (n - m) < 2: // Example: n=6, m=5, n-m=1. Threshold is high.
            Return 'C'
        Else:
            Return 'D'

    // --- Handle Last Round ---
    If current_round == r:
        // Analyze potential cooperation in this round based on past observed tendencies.
        // For simplicity, we'll assume we can make a reasonable guess based on previous rounds.
        // A truly robust last-round strategy might require more sophisticated analysis.
        // Basic aggressive approach: if cooperation seems likely to succeed, join. Otherwise, defect.
        // Let's simplify: if enough players cooperated in the previous round, assume it's a good bet.
        num_cooperators_prev_round = CountCooperators(current_round - 1, history_actions)
        If num_cooperators_prev_round >= m:
            Return 'C'
        Else:
            Return 'D'

    // --- Handle Intermediate Rounds (current_round from 2 to r-1) ---
    previous_round = current_round - 1
    num_cooperators_prev_round = CountCooperators(previous_round, history_actions)
    player_payoff_prev_round = history_payoffs[previous_round][player_id]

    If num_cooperators_prev_round >= m: // Previous round was successful cooperation
        If player_payoff_prev_round == 1: // You defected and got 1+k
            Return 'D' // Continue exploiting
        Else: // You cooperated and got k or k+1
            Return 'C' // Continue benefiting

    Else: // Previous round failed cooperation
        If num_cooperators_prev_round == m - 1: // Just one short of success
            // If you cooperate now, it becomes successful.
            // Your payoff will be k. If you defect, it's 1.
            If k > 1: // Always true by parameter
                Return 'C' // Be the one to tip the scales
            Else: // Should not happen
                Return 'D'
        Else: // Significantly less than m cooperators
            Return 'D' // Too risky to cooperate

Function CountCooperators(round_num, history_actions):
    count = 0
    For player_id from 1 to n:
        If history_actions[round_num][player_id] == 'C':
            count = count + 1
    Return count

```
'''

description_AGGRESSIVE_67 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the cooperative incentive when strategically advantageous.

## Strategy: The "Opportunistic Enforcer"

This strategy is designed to be aggressive by prioritizing self-interest while simultaneously exploiting the cooperative reward when it's most beneficial and least risky. It operates on a principle of conditional enforcement and opportunistic exploitation.

### Core Philosophy:

*   **Aggression, not recklessness:** True aggression lies in taking calculated risks for higher rewards, not in blind defection that leaves potential gains on the table.
*   **Leverage the reward, avoid the free-rider penalty:** The primary goal is to secure the `k` reward when possible, but only when the risk of *not* meeting the threshold `m` is mitigated or when defecting provides a greater immediate payoff.
*   **Enforce cooperation indirectly:** By being less likely to cooperate when others defect, this strategy incentivizes others to cooperate to secure the reward.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on the following conditions, evaluated in order of priority:

*   **Condition 1: Proximity to Threshold (High Confidence Cooperation)**
    *   If `current_cooperators_observed` (in round `t-1`) + `(n - current_players_observed)` (players whose actions are unknown from previous rounds, assuming they *will* cooperate) ≥ `m`:
        *   **Play C**.
    *   **Rationale:** This is the most aggressive play. If the observed cooperation plus a *maximal assumption* of future cooperation from unknown players is sufficient to meet the threshold, we lock in the reward. This assumes a high probability of success and maximizes our immediate gain.

*   **Condition 2: Defection for Maximum Immediate Gain (High Risk/Reward)**
    *   If `current_cooperators_observed` < `m` and `(n - current_players_observed)` < `m - current_cooperators_observed` (meaning even if all remaining players cooperate, the threshold `m` will NOT be met, and we can't secure the reward):
        *   **Play D**.
    *   **Rationale:** If cooperation is doomed to fail, and we can't even *potentially* get the reward, defecting guarantees us our private endowment (1). This is the ultimate selfish play when cooperation is guaranteed to fail.

*   **Condition 3: High Stakes Cooperation (Calculated Risk)**
    *   If `current_cooperators_observed` + `1` (our own potential cooperation) ≥ `m`:
        *   **Play C**.
    *   **Rationale:** This is a calculated risk. If our cooperation is the *only* thing that will push the group over the threshold, we take it. The potential reward `k` outweighs the loss of our endowment `1` (since we would have gotten `1` anyway by defecting, but with `k` we get `0 + k`). This is an aggressive move that forces the issue.

*   **Condition 4: Aggressive Defection (Default Aggression)**
    *   If none of the above conditions are met:
        *   **Play D**.
    *   **Rationale:** This is the default aggressive stance. If we're not in a situation where cooperating is guaranteed to succeed, or where our cooperation is the linchpin for success, we defect. This is a powerful signal to other players that we prioritize our individual gain and will only cooperate when it's demonstrably beneficial.

### 2. Handling Edge Cases:

*   **Round 1 (t = 1):**
    *   Since there's no history, we start with our default aggressive stance.
    *   **Play D**.
    *   **Rationale:** This is a strong initial signal of aggression. It avoids the risk of being the sole cooperator if `m` is high and many others also defect. We observe the landscape before committing.

*   **Last Round (t = r):**
    *   The decision rules remain the same. The fact that it's the last round doesn't change the immediate payoff calculation for that round. However, the absence of future rounds means no reputation building or long-term strategies are in play. We still aim for the highest immediate payoff.
    *   **Apply the decision rules as usual.**
    *   **Rationale:** In the absence of future considerations, maximizing the current round's payoff is paramount. The "Opportunistic Enforcer" is inherently focused on the immediate benefit of `k` or the guaranteed `1`.

*   **Few Players Remaining (e.g., n - t + 1 is small):**
    *   The decision rules automatically adapt. The `current_cooperators_observed` and `current_players_observed` will reflect the small number of remaining players. The logic of proximity to `m` still holds.

### 3. Aggressive Mindset Alignment:

*   **Proactive by Default:** The default action is Defect, pushing for maximum personal gain. Cooperation is a conditional response, not an assumed baseline.
*   **Seizing Opportunity:** When cooperation *can* guarantee the reward (Condition 1) or is critical for it (Condition 3), we aggressively pursue it. This is not a passive wait-and-see; it's an active pursuit of the highest payoff.
*   **Leveraging Uncertainty:** Condition 1 specifically leverages the uncertainty of other players' actions by assuming the best-case scenario to lock in the reward. This is a bold, aggressive stance that assumes others will play into our hands if the reward is within reach.
*   **Punishing Failure:** By defaulting to Defect when cooperation is clearly not beneficial or when the threshold isn't met, the strategy indirectly punishes players who have failed to cooperate enough, signaling that such behavior is not tolerated if it prevents collective gain.

### Pseudocode Representation:

```python
def decide_action(round_num, n, m, k, history):
    """
    Determines the action for the current round based on game parameters and history.

    Args:
        round_num (int): The current round number (1-indexed).
        n (int): Total number of players.
        m (int): Minimum cooperators needed.
        k (float): Reward if threshold met factor.
        history (list of dict): List of previous round outcomes.
                                Each dict: {'player_id': ..., 'action': 'C'/'D'}

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # --- Edge Case: Round 1 ---
    if round_num == 1:
        return 'D'

    # --- Calculate observed data from previous round (t-1) ---
    last_round_actions = history[-1] # Actions from the most recent completed round
    current_cooperators_observed = sum(1 for action in last_round_actions.values() if action == 'C')
    current_players_observed = len(last_round_actions) # Number of players whose actions are known from prev round

    # --- Decision Rules (in order of priority) ---

    # Condition 1: Proximity to Threshold (High Confidence Cooperation)
    # Assume all players whose actions are unknown from the last round will cooperate.
    if current_cooperators_observed + (n - current_players_observed) >= m:
        return 'C'

    # Condition 2: Defection for Maximum Immediate Gain (Cooperation Doomed)
    # If even if all remaining players cooperate, the threshold won't be met.
    if current_cooperators_observed + (n - current_players_observed) < m:
         return 'D'

    # Condition 3: High Stakes Cooperation (Our cooperation is critical)
    # If our cooperation (adding 1) is the only thing that will get us to m.
    if current_cooperators_observed + 1 >= m:
        return 'C'

    # Condition 4: Aggressive Defection (Default Aggression)
    return 'D'

# --- Example Usage (conceptual) ---
# n_players = 6
# m_threshold = 3
# k_reward = 2.0
# r_rounds = 5
#
# game_history = [] # Stores outcomes of previous rounds
#
# for t in range(1, r_rounds + 1):
#     # In a real tournament, you'd get observed_actions from other players
#     # For strategy design, we imagine 'observed_actions' from a *previous* round
#     # Let's say for round 2, we have history from round 1
#     if t > 1:
#         observed_actions_prev_round = {player_id: action for player_id, action in ...} # Retrieved from game state
#         # For this example, let's simulate last round actions for decision making
#         # In a real scenario, you'd get this from the game engine
#         simulated_last_round_actions = {'p1': 'C', 'p2': 'D', 'p3': 'C', 'p4': 'D', 'p5': 'C', 'p6': 'D'} # Example
#         game_history.append(simulated_last_round_actions)
#
#     # The strategy function would then be called:
#     # my_action = decide_action(t, n_players, m_threshold, k_reward, game_history)
#     # print(f"Round {t}: My action: {my_action}")
#
#     # Update game_history with your action and observed actions of others for next round
#     # ...
```

This "Opportunistic Enforcer" strategy is designed to be aggressive by prioritizing self-interest, but intelligently so. It seeks to maximize its own payoff by either defecting when cooperation is futile or by aggressively cooperating when the reward is assured or within a calculated reach, thereby incentivizing the collective to meet the threshold.
'''

description_AGGRESSIVE_68 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Opportunistic Enforcer"

This strategy embodies an aggressive, yet adaptive, approach to the Collective Risk Dilemma. It aims to maximize individual payoff by leveraging opportunities for high rewards while punishing perceived defection. The core principle is to be a net contributor when the collective is likely to succeed, but to immediately pivot to exploitation when cooperation falters or when it's strategically advantageous to exploit others' cooperation.

**Aggression Philosophy:** "I will contribute to the collective's success when it's a sure bet, but I will exploit any weakness to my advantage. I will not be the sole architect of cooperation, nor will I be the sole victim of defection."

---

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) hinges on a dynamic assessment of the likelihood of reaching the cooperation threshold 'm' and the perceived trustworthiness of the other players based on past rounds.

**Core Logic:**

*   **Aggressive Cooperation (Conditional):** Cooperate *only if* the probability of reaching the 'm' threshold is very high, and you are confident that your cooperation, combined with others' actions, will tip the scales. This is a calculated risk to secure the 'k' reward.
*   **Opportunistic Defection (Default/Punishment):** Defect *if* the likelihood of reaching 'm' is low, or if you detect a pattern of defection from other players. This is about preserving your endowment and punishing free-riders.

**Specific Decision Rules:**

1.  **Initial Round (t=1):**
    *   **Decision:** Defect (D).
    *   **Rationale:** In the absence of any historical data, assuming others will cooperate is too risky. Defecting in the first round is the most aggressive and self-preserving move. It allows you to observe others' behavior without incurring the cost of cooperation.

2.  **Subsequent Rounds (t > 1):**
    *   **Calculate Observed Cooperation Rate (OCR) from Previous Round (t-1):**
        *   Let `coop_count_prev = number of players who played C in round t-1`.
        *   `OCR = coop_count_prev / n`

    *   **Assess Probability of Success in Current Round (t):** This is a crucial heuristic. We're not trying to perfectly predict, but to make an educated guess.
        *   **"High Confidence" Threshold:** If `coop_count_prev >= m - 1` (meaning, if just one more player cooperates, the threshold will be met), *and* you have observed a consistent trend of cooperation from at least `m` players in recent rounds (e.g., `m` players cooperated in the last 2-3 rounds), *then* consider cooperating.
        *   **"Opportunistic" Threshold:** If `coop_count_prev >= m` (meaning the threshold was met in the previous round) *and* the majority of players who cooperated were *not* you (i.e., you defected while the threshold was met), *then* you have an even stronger incentive to cooperate, as you benefited from others' risk.

    *   **Decision Logic:**
        *   **If (High Confidence Condition is met) AND (you defected in the previous round and the threshold was still met):**
            *   **Decision:** Cooperate (C).
            *   **Rationale:** You can afford to contribute to ensure the reward, especially if you've successfully free-rode in the past. This is a strategic investment in a high-probability reward.

        *   **Else If (OCR from previous round is significantly high, e.g., OCR > 0.7 or OCR >= m/n with a strong recent trend):**
            *   **Decision:** Cooperate (C).
            *   **Rationale:** High observed cooperation indicates a strong tendency towards group success. Contribute to secure the reward.

        *   **Else If (you defected in the previous round and the threshold was *not* met):**
            *   **Decision:** Defect (D).
            *   **Rationale:** Others are not cooperating sufficiently, and you've already kept your endowment. Don't waste your endowment on a failing project.

        *   **Else If (coop_count_prev < m - 1):** (Low likelihood of reaching threshold)
            *   **Decision:** Defect (D).
            *   **Rationale:** The odds of reaching 'm' are too low. It's best to protect your endowment.

        *   **Else (Default - Catch-all for "aggressive default"):**
            *   **Decision:** Defect (D).
            *   **Rationale:** If none of the above conditions strongly incentivize cooperation, the default aggressive stance is to defect and conserve your resources.

**Refined Decision Logic (for implementation):**

Let's introduce a "Trust Score" for each opponent. This score increases when they cooperate and decreases when they defect. However, since we cannot communicate and can only see outcomes, this strategy will infer "trust" from collective outcomes and personal payoffs.

*   **Variable: `recent_cooperation_success` (boolean):** True if 'm' or more players cooperated in the last round.
*   **Variable: `your_payoff_in_prev_round`:** Your payoff from the previous round.

**Decision Rule for Round `t` (t > 1):**

1.  **If `recent_cooperation_success` is True:**
    *   **If `your_payoff_in_prev_round` was high (meaning you benefited from 'k' without contributing, i.e., you defected and 'm' cooperated):**
        *   **Decision:** Cooperate (C).
        *   **Rationale:** You've successfully exploited. Now contribute to maintain the successful cycle and secure the 'k' reward again.
    *   **Else (you cooperated in the previous round and 'm' cooperated):**
        *   **Decision:** Cooperate (C).
        *   **Rationale:** You've been a cooperative participant in a successful round. Continue to reinforce this.

2.  **Else (`recent_cooperation_success` is False):**
    *   **If `coop_count_prev < m - 1`:** (Very low chance of success)
        *   **Decision:** Defect (D).
        *   **Rationale:** Don't waste your endowment on a likely failure.
    *   **Else (`coop_count_prev == m - 1` or `coop_count_prev == m - 2`):** (Marginally low chance)
        *   **Decision:** Defect (D).
        *   **Rationale:** The risk of contributing and still failing is too high. Let others take the risk.
    *   **Else (This case should ideally be covered by the `recent_cooperation_success` being True, but as a fallback):**
        *   **Decision:** Defect (D).
        *   **Rationale:** Aggressive default when cooperation is not clearly beneficial or guaranteed.

**Simplified Decision Flow:**

```
// For round t > 1

coop_count_prev = count of players who played C in round t-1
your_payoff_in_prev_round = your payoff in round t-1

// Heuristic for confidence in reaching threshold
// This is a simplified approach, a more complex AI might predict probabilities
confidence_level = 0 // 0: Low, 1: Medium, 2: High

if coop_count_prev >= m:
    confidence_level = 2 // Threshold met last round - high confidence
elif coop_count_prev == m - 1:
    confidence_level = 1 // Just one away - medium confidence
elif coop_count_prev >= m - 2:
    confidence_level = 1 // Two away - still some hope
else:
    confidence_level = 0 // Far from threshold - low confidence

// Assess your previous round's strategy and outcome
you_cooperated_prev = (your action in t-1 was C)
threshold_met_prev = (coop_count_prev >= m)

// Decision Logic
if confidence_level == 2 and not you_cooperated_prev and threshold_met_prev:
    // Opportunistic Free-ride successful, now contribute to guarantee reward
    DECISION = C
elif confidence_level >= 1 and you_cooperated_prev:
    // Cooperative in a potentially successful round, continue
    DECISION = C
elif confidence_level == 0:
    // Very low chance of success, defect
    DECISION = D
else:
    // Default to aggressive defection if not clearly beneficial to cooperate
    DECISION = D

return DECISION
```

---

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated, **Defect (D)**. This is crucial for information gathering and avoiding immediate exploitation.

*   **Last Round (t=r):**
    *   **Decision:** Defect (D).
    *   **Rationale:** In the final round, there is no future to build cooperation for. The only goal is to maximize your individual payoff. Therefore, the rational, aggressive strategy is to keep your endowment. There's no incentive for altruism or future-oriented cooperation.

*   **When `m=2` (Minimum cooperators = 2):** This is a critical threshold. The strategy becomes more sensitive. If you defect, you are relying on only one other player to cooperate. The "confidence level" heuristic needs to be particularly robust here.
    *   If `m=2` and `coop_count_prev = 1` (only one other player cooperated), the strategy leans heavily towards **Defect (D)** unless that one player has a *perfect* history of cooperation and you were the sole defector in the previous successful round.

*   **When `k` is very high:** The incentive to reach 'm' is amplified. The strategy might become slightly more inclined to cooperate if the conditions are even moderately favorable, but never at the expense of guaranteed loss if 'm' is not met. The "high confidence" threshold might implicitly adjust to be slightly lower.

*   **When `m` is very close to `n`:** This makes cooperation significantly harder. The strategy will be much more hesitant to cooperate, as the failure rate will be inherently higher. Defection will be the dominant strategy unless there's overwhelming evidence of widespread cooperation.

---

### 3. Aggressiveness: Explicitly Aligning with the Aggressive Mindset

*   **Default Stance is Defection:** The strategy begins by defecting and only deviates from this if there's a strong, calculated reason to believe cooperation will be rewarded without undue risk. This is the "opportunistic" part.

*   **Exploitation of Free-riders:** If the threshold 'm' is met and you defected, you've gained the reward 'k' for free. The strategy uses this as an opportunity to *further* defect in the next round, as you've proven you can benefit without contributing. However, this is a temporary phase. If this pattern of free-riding leads to collective failure (i.e., 'm' is no longer met), the strategy will quickly re-evaluate and might even attempt to "enforce" cooperation by defecting until it sees a clear path to reward.

*   **No "Forgiveness" for Consistent Defection:** If cooperation consistently fails, the strategy will simply keep defecting. It doesn't attempt to "teach" or "rebuild trust" with other players through altruistic actions if those actions are not reciprocated or if the collective fails.

*   **"Enforcer" Mentality:** The strategy aims to be an "enforcer" in a loose sense. It will contribute to ensure success if the odds are stacked in its favor and it can secure a profit. If others fail to do their part, it will punish by defecting, but it will not be the sole force driving cooperation if the collective is inherently unstable. The "opportunistic" nature means it's ready to exploit situations where others *are* cooperating, even if it means briefly becoming a cooperator itself to secure the reward.

**Example of Aggressive Execution:**

Imagine `n=10`, `m=5`, `k=3`.

*   **Round 1:** You play D. (Payoff = 1)
*   **Round 2:** 4 players play C, 6 play D. Threshold not met. You play D. (Payoff = 1)
*   **Round 3:** 5 players play C, 5 play D. Threshold met. You played D. (Payoff = 1 + k = 4). This is an opportunistic success.
*   **Round 4:** Based on the rules, the "Opportunistic Free-ride successful, now contribute to guarantee reward" condition is met. You play C. (Payoff = 0 + k = 3).
*   **Round 5:** Suppose 4 players play C and you play C (total 5 C), but 5 others play D. Threshold met. You played C. (Payoff = 0 + k = 3).
*   **Round 6:** If the cooperation rate dropped to 3 players playing C, the strategy would likely revert to D because `confidence_level` is low. (Payoff = 1)

This strategy prioritizes maximizing personal gain through strategic risk-taking and opportunistic exploitation, aligning perfectly with an aggressive mindset.
'''

description_AGGRESSIVE_69 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Reckoning"

This strategy, "The Reckoning," is designed for an aggressive player in the Collective Risk Dilemma. It prioritizes maximizing individual payoff by leveraging the reward for successful cooperation while being keenly aware of the potential for exploitation. The core idea is to be a "swing player" – initially defecting to protect your endowment, but willing to cooperate when there's a strong, demonstrable signal that cooperation will be met with sufficient reciprocity to secure the reward.

**Core Philosophy:** "I will not be the fool who cooperates when others defect. I will exploit your greed, but when the collective good is clearly within reach *and* a sufficient number are willing to contribute, I will join to claim my share of the amplified reward. Conversely, if you under-cooperate, you will pay the price for my forfeited endowment."

### 1. Decision Rules

**General Principle:** Defect by default. Cooperate only when the probability of collective success (meeting the `m` threshold) is sufficiently high, and the potential individual gain from cooperation outweighs the guaranteed private payoff from defection.

**Specific Rules:**

*   **If `m` is close to `n` (e.g., `m >= n - 2`):**
    *   **Cooperate** if the number of players who cooperated in the *previous round* is at least `m - 1`.
    *   **Defect** otherwise.
    *   **Rationale:** In these scenarios, a single defector can break the cooperation. We're willing to take a small risk (one less than `m` cooperating) to trigger the reward, especially if others are already demonstrating commitment. The higher `m` relative to `n`, the higher the required commitment from others.

*   **If `m` is moderately high (e.g., `n/2 < m < n - 2`):**
    *   **Cooperate** if the number of players who cooperated in the *previous round* is at least `m`.
    *   **Defect** otherwise.
    *   **Rationale:** This is the standard aggressive stance. We need to see a clear commitment from the group to reach the threshold. We won't be the first to jump unless the conditions are already met.

*   **If `m` is low (e.g., `m <= n/2`):**
    *   **Cooperate** if the number of players who cooperated in the *previous round* is at least `m + 1`.
    *   **Defect** otherwise.
    *   **Rationale:** With a low `m`, there's a higher chance of many players defecting to maximize their individual payoff *if* the reward is secured. We require a slightly higher level of cooperation than the minimum to account for potential defectors among the cooperating group, ensuring *our* cooperation is not wasted.

*   **Strategic Default:** In all other cases not explicitly covered above (i.e., when the conditions for cooperation are not met by the previous round's history), **Defect**.

### 2. Handling Edge Cases

*   **Round 1 (Initialization):**
    *   **Always Defect.**
    *   **Rationale:** In the first round, there is no history to learn from. "The Reckoning" assumes an adversarial environment and prioritizes self-preservation. We will observe the actions of others before revealing our willingness to cooperate. This allows us to gather crucial information about the general disposition of the other players.

*   **The Last Round (`r`):**
    *   **Always Defect.**
    *   **Rationale:** In the final round, there are no future consequences for aggressive defection. The primary goal shifts to maximizing the absolute final payoff. Since future cooperation cannot be incentivized, and there's no opportunity to leverage the reward again, there's no benefit to cooperating. We take our guaranteed endowment of 1.

*   **Insufficient Cooperators in Previous Round (`< m-1` for all cases):**
    *   **Always Defect.**
    *   **Rationale:** If the group consistently fails to meet the cooperation threshold (even significantly), it indicates a general lack of trust or a prevalence of selfish players. In such an environment, continuing to cooperate is futile and a direct path to lower payoffs. We will stay on the sidelines and collect our private endowment.

### 3. Aggressive Mindset Alignment

*   **Leveraging Information:** The strategy is highly adaptive. It uses the history of cooperation to gauge the collective willingness to take risks. This is an aggressive stance because it actively tries to exploit situations where others have shown enough commitment.
*   **Risk Aversion until Signal:** We are not recklessly cooperative. We only "commit" to cooperation when there's a strong signal that the collective effort will succeed. This is aggressive in the sense that we are not just passively playing; we are actively calculating the optimal moment to *join* the winning side.
*   **Punitive Defection:** While not explicitly coded as a "punishment" mechanism, the strategy's default to defect when cooperation fails implicitly punishes the group for not meeting the threshold. Our individual payoff is maximized by defecting when the collective fails.
*   **Exploiting Symmetry:** If `m` is low, and many players are defecting, our strategy still attempts to defect. However, if the game dynamics shift and enough players start cooperating, our strategy is designed to quickly pivot and capitalize on the `k` multiplier, aggressively seizing the increased reward.

### Pseudocode Representation

```python
def the_reckoning_strategy(current_round, total_rounds, num_players, min_cooperators_needed, reward_factor, previous_round_actions):
    """
    'The Reckoning' strategy for the Collective Risk Dilemma.

    Args:
        current_round (int): The current round number (1-indexed).
        total_rounds (int): The total number of rounds in the game.
        num_players (int): The total number of players in the game.
        min_cooperators_needed (int): The minimum number of cooperators required for the reward.
        reward_factor (float): The multiplier for the reward.
        previous_round_actions (list of str): List of actions ('C' or 'D') played by all players in the previous round.
                                              Empty list for the first round.

    Returns:
        str: The action to take ('C' for Cooperate, 'D' for Defect).
    """

    n = num_players
    m = min_cooperators_needed
    r = total_rounds

    # Handle Edge Case: First Round
    if current_round == 1:
        return 'D'

    # Handle Edge Case: Last Round
    if current_round == r:
        return 'D'

    # Calculate number of cooperators in the previous round
    cooperators_in_prev_round = previous_round_actions.count('C')

    # Decision Rules based on m and previous round's cooperation

    # Case 1: m is close to n (high requirement)
    if m >= n - 2:
        if cooperators_in_prev_round >= m - 1:
            return 'C'
        else:
            return 'D'

    # Case 2: m is moderately high
    elif m > n / 2:
        if cooperators_in_prev_round >= m:
            return 'C'
        else:
            return 'D'

    # Case 3: m is low
    else: # m <= n / 2
        if cooperators_in_prev_round >= m + 1:
            return 'C'
        else:
            return 'D'

    # If none of the above conditions are met (fallback - shouldn't typically be reached with comprehensive conditions)
    return 'D'
```
'''

description_AGGRESSIVE_70 = '''
This is a fascinating game! The Collective Risk Dilemma presents a classic tension between individual self-interest and collective benefit. Designing an aggressive strategy requires a calculated risk-taking approach, aiming to maximize personal gain while being prepared for defection from others.

Here's an aggressive strategy, named "Dominant Aggressor," designed for this environment:

## Dominant Aggressor Strategy

The core principle of Dominant Aggressor is to leverage the higher payoff for cooperation *when it's safe to do so*, but to aggressively defect to secure personal gain when the collective benefit is uncertain or unlikely. It's about pushing the boundaries of cooperation, but always with a strong fallback to protect oneself.

### 1. Decision Rules: When to Cooperate vs. Defect

Dominant Aggressor is driven by a **"cooperate-if-likely-to-succeed, defect-otherwise"** mentality. The key is to accurately assess the likelihood of meeting the cooperation threshold (`m`).

**The primary decision logic is based on the observed cooperation rate in the previous round, adjusted by a dynamic "risk buffer".**

*   **Cooperate (C) if:**
    *   The number of observed cooperators in the previous round (`C_t-1`) plus a calculated "safe margin" is greater than or equal to `m`.
    *   The "safe margin" is a dynamic value that decreases as the game progresses and increases with the opponent's historical cooperation. It represents our confidence that others will also cooperate.

*   **Defect (D) if:**
    *   The number of observed cooperators in the previous round (`C_t-1`) plus the "safe margin" is less than `m`.
    *   We are in a situation where defecting is a guaranteed win or a significantly better outcome than a potentially failed cooperation attempt.

**Specific components of the decision rule:**

*   **Observed Cooperators (`C_t-1`):** The count of players (excluding ourselves) who cooperated in the immediately preceding round.
*   **Risk Buffer (`R`):** This is a crucial adaptive element.
    *   **Initial Value:** Starts at `n - m + 1`. This is the maximum buffer we might need in early rounds to ensure cooperation. It represents the maximum number of additional cooperators needed beyond the minimum `m` to feel secure.
    *   **Decreases over rounds:** As rounds progress, our confidence in collective success should theoretically increase if we observe consistent cooperation. The buffer shrinks by a small amount each round (e.g., `R = max(0, R - 0.1)`).
    *   **Increases with opponent's cooperation:** If we observe a high percentage of cooperation from others in previous rounds, our confidence increases, and the buffer can shrink faster. Conversely, if we see consistent defections, the buffer will grow, making us more hesitant to cooperate.
    *   **Threshold for aggressive defection:** If the observed cooperation rate drops significantly below `m`, the risk buffer will grow rapidly, signaling a strong tendency to defect.

**The core condition for cooperating becomes:**

`C_t-1 + R >= m`

**Where:**
*   `C_t-1` = Number of other players who cooperated in round `t-1`.
*   `R` = Current Risk Buffer.

**If this condition is met, we play C. Otherwise, we play D.**

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Aggressive Start:** In the very first round, there's no history to rely on. To be aggressive and establish a strong presence, Dominant Aggressor will *initially defect*. This is a calculated risk. By defecting, we guarantee ourselves a payoff of 1. If enough others cooperate, they'll get 2. If they don't, they'll get 1. This gives us flexibility to observe and react.
    *   **Initial Risk Buffer:** The risk buffer is initialized as described above (`n - m + 1`) to prepare for subsequent rounds.

*   **Last Round (t=r):**
    *   **Maximum Self-Interest:** In the final round, there are no future consequences for our actions. The strategy becomes purely selfish. We will **always defect** in the last round, regardless of past history or observed cooperation. The goal is to maximize our individual score without regard for any collective outcome.
    *   **Rationale:** The reward for collective success is only valuable if it can be repeated or if it influences future cooperation. In the final round, this is moot.

*   **Near the Threshold (m-1 cooperators observed):**
    *   This is where the risk buffer is crucial. If `m-1` cooperators were observed in the previous round, we will only cooperate if our current risk buffer is sufficiently high (`R >= 1`). If `R < 1`, we defect. This prevents us from being the sole cooperators when success is on the cusp but still uncertain.

### 3. Aggressive Mindset Alignment

*   **"Always Push for the Best Personal Outcome":** The strategy is fundamentally designed to exploit opportunities for higher payoffs (i.e., when `k` is applied) but to do so only when the probability of that payoff is high enough. Otherwise, it prioritizes securing the guaranteed private payoff of 1.
*   **"Proactive Observation and Adaptation":** The dynamic risk buffer is the embodiment of this. It's not passive; it actively adjusts based on observed behavior, becoming more risk-averse if others are unreliable and more willing to cooperate if others demonstrate a pattern of cooperation.
*   **"No Sympathy for Non-Cooperators":** The initial defection in round 1 and the final round defection are clear indicators. We are not aiming for mutual cooperation for its own sake. We are aiming to win, and that means outmaneuvering opponents. If they don't contribute to a successful collective outcome, we will punish that by taking our private gain.
*   **"Exploiting the Reward Multiplier (k)":** The strategy is heavily biased towards cooperating *if* it's likely to trigger the `k` multiplier. This is because `k > 1`, meaning a successful cooperative round yields a higher payoff than a failed one, *especially for defectors*. Our strategy aims to capture the `k` reward when it's *our* cooperation that triggers it, or to benefit from `k` as a defector if others are cooperating.

### Pseudocode Representation

```python
# Parameters
n = number of players
r = number of rounds
m = minimum cooperators needed
k = reward factor (k > 1)

# State variables for Dominant Aggressor
current_round = 1
observed_cooperators_last_round = 0 # For round 1, this is effectively 0 other players
risk_buffer = n - m + 1 # Initial risk buffer

def decide_action(history):
    global current_round, observed_cooperators_last_round, risk_buffer

    if current_round == r:
        # Aggressive self-interest in the last round
        return "D"

    if current_round == 1:
        # Aggressive start: Defect to observe and secure baseline payoff
        action = "D"
        # Initialize observed_cooperators_last_round conceptually for the next round's calculation
        # This will be updated after the round ends.
        observed_cooperators_last_round = 0 # No one to observe yet
    else:
        # Adaptive decision based on history and risk buffer
        # 'history' is assumed to be a list of tuples, where each tuple is (player_actions_this_round, payoffs_this_round)
        # For Dominant Aggressor, we only need the count of cooperators from the previous round.
        # Let's assume 'history' provides the actions of ALL players in the previous round.

        # Determine actual observed cooperators from previous round (excluding self)
        # This requires parsing the 'history' correctly.
        # For simplicity, let's assume observed_cooperators_last_round is updated externally or calculated properly before this.

        # Calculate cooperation threshold check
        cooperation_likely = (observed_cooperators_last_round + risk_buffer) >= m

        if cooperation_likely:
            action = "C"
        else:
            action = "D"

    # Update risk buffer for the NEXT round
    if current_round > 1:
        # Decrease risk buffer over time, but not below 0
        risk_buffer = max(0, risk_buffer - 0.1)

        # Further adjust risk buffer based on observed cooperation rate.
        # If cooperation was consistently low, buffer might grow again.
        # This is a simplified adaptation. A more complex model could use exponential decay or other learning rates.
        # For example, if observed_cooperators_last_round < m/2 (very low):
        #    risk_buffer = min(n - m + 1, risk_buffer + 0.5) # Make it more cautious

    current_round += 1
    return action

def update_state_after_round(my_action, actions_of_others):
    global observed_cooperators_last_round
    # This function would be called at the end of each round.
    # 'actions_of_others' is a list of actions from other players.
    cooperator_count = 0
    for action in actions_of_others:
        if action == "C":
            cooperator_count += 1
    observed_cooperators_last_round = cooperator_count

```

This Dominant Aggressor strategy aims to be a formidable opponent by taking calculated risks, adapting to observed behavior, and always prioritizing its own highest possible payoff, especially in critical final rounds.
'''

description_AGGRESSIVE_71 = '''
This is a fascinating challenge! We're designing an aggressive strategy for the Collective Risk Dilemma, a game that pits individual self-interest against the potential for collective reward. The "aggression" here isn't about pure selfishness, but rather about pushing for the high-reward cooperative outcome while minimizing personal risk and maximizing gains, especially when others are hesitant.

Here's an aggressive strategy, designed to be adaptive and robust:

## Aggressive Strategy: "The Calculated Risk-Taker"

This strategy aims to maximize individual payoff by pushing the collective towards cooperation when the reward is high, but also to defect ruthlessly when cooperation seems unlikely or costly. It’s about being the catalyst for success when it’s beneficial, and the smart survivor when it’s not.

### Core Philosophy:

*   **Leverage the Reward:** The high reward (`k`) for meeting the threshold (`m`) is the primary driver. We want to be part of a successful coalition.
*   **Minimize Personal Cost:** We don't want to be the "sucker" – the one who cooperates and is left with nothing while others defect.
*   **Information is Power:** Adapt to the observed behavior of others. Their actions in previous rounds are crucial signals.
*   **Aggressive Proactivity:** Don't wait for others to initiate cooperation if the conditions are favorable. Be willing to make the first move if the potential payoff justifies it.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round `t` depends on a set of conditions related to the current round number, the success rate of cooperation in past rounds, and the proximity to the threshold `m`.

**General Rule:**

*   **Default to Cooperate if:** The current round `t` is sufficiently early in the game, and there's a strong indication that enough other players are likely to cooperate.
*   **Default to Defect if:** The current round `t` is late, or if past behavior suggests low cooperation, or if we are close to the threshold `m` but the probability of reaching it is low.

**Detailed Decision Logic:**

Let's define some key metrics we'll track:

*   `cooperation_rate_past_rounds`: The average proportion of players who cooperated in the `t-1` previous rounds (where `t-1 > 0`).
*   `my_cooperation_history`: A binary string representing our own actions in past rounds.
*   `own_successful_cooperations`: The number of past rounds where we cooperated *and* the threshold `m` was met.
*   `rounds_remaining`: `r - t`.

**Decision Tree:**

*   **If `t` = 1 (First Round):**
    *   **Aggressive Push:** Cooperate (C).
        *   *Rationale:* The first round is a critical opportunity to establish a norm of cooperation. By defecting, we immediately signal a lack of trust. Cooperating signals a willingness to take a risk for the higher payoff, and potentially "pulls" others along. We're making an aggressive initial play for the collective good that benefits us most.

*   **If `t` > 1:**
    1.  **Check for Aggressive Opportunity (High Reward, Low Risk):**
        *   **If `cooperation_rate_past_rounds` >= `(m-1)/n` AND `rounds_remaining` > `r / 2` (i.e., we are in the first half of the game):**
            *   **Cooperate (C).**
            *   *Rationale:* This indicates that, on average, enough players *have been* cooperating to get close to the threshold. Combined with having ample rounds left to recover from a failed cooperation attempt, this is an opportune moment to push for success. We're not *guaranteed* success, but the trend is positive, and the cost of trying is mitigated by the number of rounds remaining.

    2.  **Check for Stalemate/Low Cooperation (High Risk):**
        *   **If `cooperation_rate_past_rounds` < `(m-1)/n` AND `rounds_remaining` < `r / 4` (i.e., we are in the last quarter of the game):**
            *   **Defect (D).**
            *   *Rationale:* The game is ending, and the historical cooperation rate is too low to reliably reach `m`. The risk of being the sole cooperator (or one of few) and receiving only `1` while others defect and get `1` (or worse, `3` if `m` is met unexpectedly by others) is too high. We secure our guaranteed `1` and move on.

    3.  **Check for Near Threshold Success (Conditional Cooperation):**
        *   **If `cooperation_rate_past_rounds` is close to `m/n` (e.g., `cooperation_rate_past_rounds >= (m-2)/n`) AND `rounds_remaining` > 1 AND (the number of players who cooperated in the *last* round >= `m-1`):**
            *   **Cooperate (C).**
            *   *Rationale:* This is a strong signal that the threshold is *very* likely to be met in this round. If we are one of the missing cooperators, defecting would leave the collective short and us with a suboptimal payoff (just `1`). Cooperating here is a calculated risk for the maximum possible reward of `1 + k`. This is where we push to get over the line.

    4.  **Check for Past Failure and Limited Opportunity (Retaliatory Defection):**
        *   **If `own_successful_cooperations` < `rounds_remaining` / 2 (i.e., we haven't had much success with our past cooperations) AND `rounds_remaining` < `r / 2`:**
            *   **Defect (D).**
            *   *Rationale:* If our own attempts at cooperation haven't been rewarded, and we're past the midpoint, it's time to be less forgiving. We're not seeing individual benefit from our cooperation efforts, so we prioritize securing our base payoff. This is a form of "punishment" for a lack of collective success.

    5.  **The "Calculated Push" (Default Cooperation):**
        *   **If none of the above conditions are met, and `cooperation_rate_past_rounds` > `(m-1)/n - 0.1` (i.e., reasonably close to the threshold with a positive trend):**
            *   **Cooperate (C).**
            *   *Rationale:* This is our general proactive stance. If there's a reasonable chance of success and the trend is positive, we're willing to take the leap. It’s aggressive because we’re willing to cooperate to *try* and achieve the higher payoff, rather than passively waiting for others.

    6.  **The "Pure Survival" (Default Defection):**
        *   **Otherwise (i.e., low cooperation rate, late in the game, no strong indicators):**
            *   **Defect (D).**
            *   *Rationale:* If the stars aren't aligning for a high-reward cooperative outcome, the safest and most rational aggressive play is to secure our guaranteed payoff of `1`. We don't want to be a liability or a sucker.

### 2. Handling Edge Cases:

*   **First Round (`t = 1`):** Always **Cooperate (C)**. This is our initial aggressive statement. We are willing to be the first to invest in the collective good.
*   **Last Round (`t = r`):**
    *   **If `cooperation_rate_past_rounds` >= `(m-1)/n`:** **Cooperate (C)**.
        *   *Rationale:* In the final round, the only thing that matters is reaching the threshold. If historical data suggests we are on the cusp, it’s our last chance to secure the `k` reward.
    *   **Else:** **Defect (D)**.
        *   *Rationale:* If we are not clearly on the cusp, the risk of failing to reach `m` is too high, and there's no future round to correct it. Secure the `1`.
*   **Early Game vs. Late Game:** The strategy explicitly uses thresholds like `r/2` and `r/4` to differentiate behavior. Early in the game, we are more willing to experiment and push for cooperation. Late in the game, we become more risk-averse and prioritize securing gains.

### 3. Being Aggressive:

The aggression in this strategy manifests in several ways:

*   **The First Move:** Always cooperating in round 1 is an aggressive act of leadership, signaling intent to pursue the higher payoff.
*   **Proactive Cooperation:** We don't wait for `m` players to signal cooperation before we do. If the `cooperation_rate_past_rounds` is promising (even if not definitively guaranteeing success), we'll cooperate to try and tip the scales.
*   **Pushing for the Threshold:** Condition 3 specifically drives us to cooperate when we are very close to meeting `m`. We are willing to be the player that *makes it happen*, even if it means a slightly higher personal risk in that specific round.
*   **Calculated Risk-Taking:** The strategy is built around taking calculated risks. We're not blindly cooperating, but we are willing to cooperate when the probability of a good outcome (high payoff) outweighs the probability of a bad outcome (low payoff).
*   **"Retaliatory" Defection (Subtle Aggression):** Condition 4, while framed as risk-averse, can also be seen as an aggressive stance. If our past cooperative efforts haven't yielded success, we become less inclined to continue making the costly choice. It's a form of self-preservation that can indirectly punish non-cooperative environments.

### Pseudocode Representation:

```python
def decide_action(player_id, n, r, m, k, history):
    t = len(history) + 1  # Current round (1-indexed)

    # Calculate historical cooperation rate
    if t == 1:
        cooperation_rate_past_rounds = 0
        own_successful_cooperations = 0
        players_cooperated_last_round = 0
    else:
        total_past_cooperators = sum(sum(1 for action in round_history if action == 'C') for round_history in history[:-1])
        cooperation_rate_past_rounds = total_past_cooperators / ((t - 1) * n) if (t - 1) * n > 0 else 0

        # Count own successful cooperations (need payoff info for this, which isn't directly in history)
        # For simplicity here, we'll assume 'history' contains a tuple of (own_action, others_actions)
        # A more robust implementation would require storing payoff outcomes.
        # For this pseudocode, we'll use a simplified proxy: count rounds where we cooperated and threshold was met.
        # This is a simplification and a real implementation would need to track this explicitly.
        own_successful_cooperations = 0
        for round_idx, round_data in enumerate(history[:-1]): # Iterate through completed rounds
            own_action = round_data[0]
            if own_action == 'C':
                num_cooperators_this_round = sum(1 for action in round_data[1] if action == 'C') + (1 if own_action == 'C' else 0)
                if num_cooperators_this_round >= m:
                    own_successful_cooperations += 1

        # Count players who cooperated in the *last* completed round
        players_cooperated_last_round = 0
        if t > 1:
            players_cooperated_last_round = sum(1 for action in history[-1][1] if action == 'C') + (1 if history[-1][0] == 'C' else 0)


    rounds_remaining = r - t

    # --- Decision Logic ---

    # 1. First Round: Aggressive Push
    if t == 1:
        return 'C'

    # 2. Aggressive Opportunity (High Reward, Low Risk)
    if cooperation_rate_past_rounds >= (m - 1) / n and rounds_remaining > r / 2:
        return 'C'

    # 3. Stalemate/Low Cooperation (High Risk - Late Game)
    if cooperation_rate_past_rounds < (m - 1) / n and rounds_remaining < r / 4:
        return 'D'

    # 4. Near Threshold Success (Conditional Cooperation)
    # Using a slightly more flexible definition of "close to m/n" and requiring strong recent indicator
    if cooperation_rate_past_rounds >= (m - 2) / n and rounds_remaining > 1 and players_cooperated_last_round >= m - 1:
         return 'C'

    # 5. Past Failure and Limited Opportunity (Retaliatory Defection)
    # Simplified condition: If our own cooperations haven't been frequent/successful.
    # A perfect implementation would track success of our own cooperations directly.
    # Using a heuristic: if we have cooperated less than half the time in completed rounds, and late in game.
    if (t > 1 and sum(1 for act in [data[0] for data in history[:-1]] if act == 'C') < (t-1)/2) and rounds_remaining < r / 2:
        return 'D'


    # 6. The "Calculated Push" (Default Cooperation)
    # If cooperation rate is reasonably high (above a threshold that's slightly below what's needed)
    if cooperation_rate_past_rounds > (m - 1) / n - 0.1:
        return 'C'

    # 7. The "Pure Survival" (Default Defection)
    else:
        return 'D'

# Example of history structure:
# history = [
#    ('C', ['C', 'D', 'D', 'D', 'D']), # Round 1: Player 1 played C, others played D, D, D, D
#    ('D', ['C', 'C', 'D', 'D', 'D']), # Round 2: Player 1 played D, others played C, C, D, D, D
#    ...
# ]
```

This "Calculated Risk-Taker" strategy aims to be a dynamic player, pushing for cooperative success when the conditions are right and decisively retreating to self-preservation when the risk becomes too great. Its aggression lies in its willingness to initiate and its calculated approach to maximizing payoff in a game with inherent social tension.
'''

description_AGGRESSIVE_72 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize your individual payoff by leveraging risk and exploiting opportunities while remaining adaptable.

## Aggressive Strategy: The "Calculated Risk" Strategy

This strategy aims to be aggressive by pushing the boundaries of cooperation to secure the reward when possible, but with a clear understanding of the risks and a mechanism to protect oneself if cooperation fails. It's about being a calculated gambler in the collective.

### Core Philosophy:

*   **Exploit the Reward:** The primary driver is to secure the reward of 'k' whenever it's realistically achievable without excessive personal risk.
*   **Punish Defection (Subtly):** While not overtly punitive, the strategy will naturally punish excessive defection by withholding cooperation, thereby reducing the likelihood of the threshold being met.
*   **Adapt to the Environment:** The strategy learns from the actions of others and adjusts its cooperation threshold dynamically.
*   **Self-Preservation:** The strategy prioritizes its own payoff. It will not blindly cooperate if the probability of success is too low.

### Decision Rules:

The decision to Cooperate (C) or Defect (D) in round 't' depends on two main factors:

1.  **The "Cooperation Pulse":** An assessment of the current likelihood that the threshold 'm' will be met.
2.  **The "Aggression Threshold":** A dynamic parameter that dictates how much risk you are willing to take based on recent outcomes and future prospects.

#### 1. Calculating the "Cooperation Pulse":

We'll estimate the probability that at least 'm' players (including ourselves) will cooperate in the current round. This is a probabilistic assessment based on the history of observed player behaviors.

*   **Estimate Opponent Cooperation Rate:** For each opponent 'j', track their historical cooperation rate (number of times they cooperated / number of rounds played so far). Let's call this $P_{coop}(j)$.
*   **Estimate Total Cooperation Probability:** This is the most complex part. A simplified, yet effective, approach is to consider the **average cooperation rate of all observed opponents in the previous round**. Let $N_{observed\_coop, t-1}$ be the number of players (excluding ourselves) who cooperated in round $t-1$. The estimated number of cooperators in round $t$ (excluding ourselves) is roughly $(n-1) \times \text{Average Opponent Cooperation Rate}$.
    *   A more sophisticated approach would involve modeling the distributions of opponent strategies, but for an aggressive, parameter-driven strategy, focusing on recent trends is more practical.
*   **"Cooperation Pulse" (CP):** This is a proxy for the confidence that the 'm' threshold will be met.
    *   If we observe $N_{observed\_coop, t-1}$ cooperators from others in the previous round, and we plan to cooperate, then the total number of cooperators will be $N_{observed\_coop, t-1} + 1$.
    *   If $N_{observed\_coop, t-1} + 1 \ge m$, then the "Cooperation Pulse" is high.

#### 2. The "Aggression Threshold" (AT):

This threshold determines how many *other* players we believe need to cooperate for us to contribute. It's dynamic and influenced by:

*   **Recent Success:** If the threshold was met and rewarded in recent rounds, we become more confident and willing to take a calculated risk.
*   **Recent Failure:** If the threshold was consistently missed, we become more cautious.
*   **Remaining Rounds:** In later rounds, the incentive to secure a reward increases.

Let's define the AT based on the number of *other* players we expect to cooperate:

*   **Initial AT:** In round 1, we'll start with a slightly more cautious AT. We'll want to see a few others signal willingness. Let's set the initial AT to $m-2$. (Meaning, we'll cooperate if we believe at least $m-2$ other players will cooperate).
*   **Dynamic AT Adjustment:**
    *   **If threshold met in round t-1:** Increase AT by a small amount (e.g., +0.5, capped at $m-1$). This signals increased confidence.
    *   **If threshold missed in round t-1 AND we cooperated:** Decrease AT by a small amount (e.g., -0.5, minimum 1). This signals caution due to perceived unreliable cooperation.
    *   **If threshold missed in round t-1 AND we defected:** AT remains unchanged or slightly increases (e.g., +0.2). This indicates our defection was a rational response to insufficient cooperation.

#### 3. The Decision Rule:

In round 't', for player 'i':

*   **Observe actions of all players in round t-1.**
*   **Calculate $N_{observed\_coop, t-1}$ (number of other players who cooperated in t-1).**
*   **Calculate the current "Aggression Threshold" (AT).**

**Decision:**

*   **If ($N_{observed\_coop, t-1} \ge \text{AT}$ AND we plan to Cooperate in this round):** Then **Cooperate (C)**.
    *   *Rationale:* We believe the threshold will be met (or exceeded) and we want to be part of the successful group to claim the reward.
*   **Else:** **Defect (D)**.
    *   *Rationale:* The perceived risk of cooperating and failing to reach the threshold is too high, or we are being conservative due to past failures.

**Crucially, we will only plan to Cooperate if we believe we *can* reach the threshold by cooperating.** This means we don't just blindly aim for the AT. We also consider the "Cooperation Pulse."

**Refined Decision Rule:**

Let $N_{expected\_cooperators} = N_{observed\_coop, t-1} + 1$ (assuming we cooperate).

*   **If ($N_{expected\_cooperators \ge m$) AND ($N_{observed\_coop, t-1} \ge \text{AT}$):** Then **Cooperate (C)**.
    *   *This is the core aggressive move: we are confident the threshold will be met, and the observed behavior of others meets our current tolerance for risk.*
*   **Else:** **Defect (D)**.
    *   *Either the expected number of cooperators is insufficient, or the observed behavior of others doesn't meet our current Aggression Threshold.*

### Handling Edge Cases:

*   **Round 1:**
    *   **Initial Cooperation Pulse:** Since there's no history, we assume an average cooperation rate of 0 for others. So, $N_{observed\_coop, 0} = 0$.
    *   **Initial Aggression Threshold (AT):** Set AT to $m-2$.
    *   **Decision:** If $m-2 \le 0$ (i.e., $m \le 2$), which is always true for $m > 1$, the condition $N_{observed\_coop, 0} \ge \text{AT}$ is met.
    *   **Therefore, in Round 1, we will Cooperate (C) if and only if $(0 + 1 \ge m)$ AND $(0 \ge m-2)$.**
        *   $(1 \ge m)$ is only true if $m=1$, but the parameter is $m>1$. So, in Round 1, we *will not cooperate* if $m > 1$. This seems counter-intuitive to aggression.
        *   **Correction for Round 1 Aggression:** To be truly aggressive in round 1, we need to be willing to *initiate* the cooperation. A better approach for Round 1:
            *   **If $m \le 2$:** Cooperate (C). We are willing to be one of the first two cooperators to try and kickstart things. This is a calculated risk.
            *   **If $m > 2$:** Defect (D). The risk of being the *only* cooperator is too high. We will wait and see.
            *   This makes the strategy aggressive by *sometimes* initiating cooperation when the threshold is low, but not by default.

*   **Last Round (Round 'r'):**
    *   The calculation of AT still applies. However, the "future prospects" factor is gone.
    *   **Decision:** Apply the same refined decision rule. The only difference is that there are no future rounds to adapt to. The strategy becomes purely about maximizing payoff in this single, final round based on the learned behavior.

*   **When $m$ is close to $n$:** The strategy becomes more cautious. It will require a very high observed cooperation rate from others before it's willing to contribute.

*   **When $m$ is small (close to 2):** The strategy becomes more aggressive. It's more likely to initiate cooperation or join in with a smaller observed group.

### Pseudocode:

```
// Game Parameters: n, r, m, k

// Strategy State Variables:
// current_aggression_threshold (initialized based on Round 1 logic)
// opponent_cooperation_counts (map: opponent_index -> count_of_cooperations)
// num_rounds_played (initialized to 0)

// Initialization (before Round 1)
function initialize_strategy():
    num_rounds_played = 0
    opponent_cooperation_counts = {} // Initialize for all opponents
    for i = 1 to n:
        opponent_cooperation_counts[i] = 0

    // Initial Aggression Threshold calculation for Round 1
    if m <= 2:
        current_aggression_threshold = 0 // Willing to be one of the first 2
    else:
        current_aggression_threshold = m - 2 // Wait for some initial signals


// Decision Logic for Each Round 't'
function decide_action(round_number, all_previous_actions_history):
    num_rounds_played = round_number

    // Update opponent cooperation counts based on history
    if round_number > 1:
        for player_i in range(1, n + 1):
            if player_i is not my_player_index:
                if all_previous_actions_history[round_number - 1][player_i] == 'C':
                    opponent_cooperation_counts[player_i] += 1

    // Calculate number of opponents who cooperated in the previous round
    observed_cooperators_last_round = 0
    if round_number > 1:
        for player_i in range(1, n + 1):
            if player_i is not my_player_index:
                if all_previous_actions_history[round_number - 1][player_i] == 'C':
                    observed_cooperators_last_round += 1

    // --- Decision Logic ---
    // Our intention: Assume we will cooperate for now, and check if it's optimal

    expected_total_cooperators_if_I_cooperate = observed_cooperators_last_round + 1

    // Aggressive decision: Cooperate if we expect to meet the threshold AND others' observed behavior meets our current AT
    if expected_total_cooperators_if_I_cooperate >= m AND observed_cooperators_last_round >= current_aggression_threshold:
        my_action = 'C'
    else:
        my_action = 'D'

    // --- Update Aggression Threshold for next round ---
    // This update happens *after* the decision, influencing the next round.

    threshold_met_last_round = (observed_cooperators_last_round + (1 if my_action == 'C' else 0)) >= m

    if threshold_met_last_round:
        // Reward for success: slightly increase confidence
        current_aggression_threshold += 0.5
        if current_aggression_threshold > m - 1: // Cap AT
            current_aggression_threshold = m - 1
    else: // Threshold NOT met
        if my_action == 'C':
            // We cooperated but failed: indicate caution by lowering AT
            current_aggression_threshold -= 0.5
            if current_aggression_threshold < 0: // Ensure AT doesn't go below 0
                current_aggression_threshold = 0
        else: // We defected and failed
            // Our defection was likely rational, slightly increase AT for future
            current_aggression_threshold += 0.2
            if current_aggression_threshold > m - 1:
                current_aggression_threshold = m - 1

    // Ensure AT is within reasonable bounds (e.g., 0 to n-1)
    if current_aggression_threshold < 0:
        current_aggression_threshold = 0
    if current_aggression_threshold > n - 1: // Can't expect more than n-1 others
        current_aggression_threshold = n - 1


    return my_action

```

### Aggressive Mindset Alignment:

1.  **Calculated Risk-Taking:** The strategy actively seeks opportunities to cooperate by setting an AT that allows for cooperation even when the number of observed cooperators is slightly less than 'm'. This is particularly evident in Round 1 when $m \le 2$.
2.  **Exploiting Information:** The AT dynamically adjusts based on the success or failure of the collective, allowing the strategy to become more aggressive when others are cooperating and more cautious when they are not. This is an aggressive interpretation of learning.
3.  **Goal-Oriented:** The ultimate goal is to always try and achieve the condition for the reward 'k'. The strategy prioritizes this by pushing for cooperation when the odds are favorable.
4.  **Self-Reliance:** It doesn't rely on external coordination. It measures the "pulse" of cooperation and decides whether to join the rhythm or to stand back.
5.  **Opportunistic:** If the observed cooperation rate from others is high enough (meeting the AT), the strategy is quick to jump in, ensuring it benefits from the collective success.

This "Calculated Risk" strategy aims to be a player that aggressively pursues the reward, but always with an awareness of the collective's behavior and the potential for personal loss. It's not about being reckless, but about being bold when the environment suggests it's the optimal path to a higher payoff.
'''

description_AGGRESSIVE_73 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while exploiting the cooperative nature of others when opportune.

## Strategy Name: The Opportunistic Predator

**Core Philosophy:** This strategy is built on the principle of **maximizing personal gain by defecting whenever it's individually beneficial, and cooperating only when it guarantees a significantly higher payoff for myself, primarily by leveraging the cooperation of others.** It's about being the "smartest" defector, always looking for the highest possible immediate return, and only participating in collective success if it's a sure thing *for me*.

---

### 1. Decision Rules: When to Cooperate vs. Defect

The decision for player `i` in round `t` (where `t` ranges from 1 to `r`) is governed by the following logic:

**Default Action: Defect (D)**

The strategy's default is to **Defect**. This is the aggressive starting point. We assume the worst from others and aim for the guaranteed private gain of 1.

**Conditions for Cooperation (C):**

Player `i` will only deviate from **Defect** to **Cooperate** if **ALL** of the following conditions are met:

*   **Condition 1: Imminent Guaranteed Reward (for me).**
    *   The number of players I *predict* will cooperate in this round, plus myself, will be **at least `m`**.
    *   **Prediction Mechanism:** This is the most crucial and dynamic part. My prediction will be based on the observed behavior of opponents in previous rounds. Specifically, I will track the *average cooperation rate* of *all other players* across the preceding `t-1` rounds.
        *   Let `N_opponents` = `n - 1` (the number of other players).
        *   Let `C_total_prev_rounds` be the total number of cooperations observed from all other players in rounds 1 to `t-1`.
        *   Average cooperation rate of others = `C_total_prev_rounds / (N_opponents * (t-1))`.
        *   My predicted cooperators from others = `floor(Average cooperation rate of others * N_opponents)`.
    *   **My Decision to Cooperate:** I will cooperate *only if* `predicted_cooperators_from_others + 1 >= m`. The `+1` represents my own potential cooperation. This means I'm not just hoping for cooperation, I'm seeing a strong signal that the threshold will be met *with me contributing*.

*   **Condition 2: Sufficient History for Reliable Prediction.**
    *   I need a reasonable history to make a reliable prediction. Therefore, this cooperation condition only applies if `t > threshold_history_start_round`. A reasonable value for `threshold_history_start_round` would be `max(2, floor(n / 2))`. This ensures we've seen some behavior and aren't acting on noise, but still allows for early opportunistic cooperation if signals are very strong. If `t` is less than or equal to this threshold, I stick to the default action of **Defect**, unless overridden by edge cases (see below).

*   **Condition 3: No Exploitation Risk of Remaining Players.**
    *   Even if Condition 1 is met, I will *not* cooperate if there's a substantial risk that defecting players will still receive a higher payoff than cooperators. This happens when the reward multiplier `k` is not sufficiently high relative to the number of remaining defectors.
    *   **Calculation:**
        *   If I cooperate (and assume `m` or more will cooperate): My payoff is `0 + k`.
        *   If I defect (and `m` or more cooperate): My payoff is `1 + k`.
        *   If fewer than `m` cooperate: My payoff is `1` (regardless of my action).
    *   I will cooperate *only if* the guaranteed payoff of cooperating (`k`) is **strictly greater than** the payoff of defecting when the threshold is met (`1 + k` is not possible if I defect), or the payoff of defecting if the threshold is *not* met (which is 1).
    *   More critically, I need to ensure that defecting players don't get a *significantly* better deal than me if I cooperate. If I predict `P` players will cooperate (including myself, so `P >= m`), and `n-P` will defect:
        *   My payoff (cooperating): `k`
        *   Defector's payoff: `1 + k`
    *   This means defectors *always* get more than me if the threshold is met. The goal of this aggressive strategy is to be the *smartest* player, not necessarily the *most altruistic*. So, I only cooperate if it is demonstrably the best *individual* path, which is rare.
    *   **Revised Condition for Cooperation:** I will *only* consider cooperating if `k > 1`. If `k <= 1`, defecting is always superior when the threshold is met.
    *   **The critical point is this:** If `m` or more players cooperate, my payoff is `k` if I cooperate, and `1+k` if I defect. Defecting is always better *in that specific scenario*. Therefore, the only reason I would *ever* cooperate is if I predict that *without my cooperation*, the threshold will *not* be met, but *with my cooperation*, it *will* be met.
    *   **Final Cooperation Trigger:** Player `i` cooperates if:
        1.  `t > threshold_history_start_round`
        2.  `predicted_cooperators_from_others + 1 >= m` (where `predicted_cooperators_from_others` is based on average opponent behavior)
        3.  `k > 1` (otherwise, defecting is always better when threshold is met).
        4.  My predicted payoff for cooperating (`k`) is **strictly greater than** the payoff I would receive if I defected and the threshold was *not* met (which is 1). This is always true if `k > 1`.

---

### 2. Handling Edge Cases

*   **Round 1 (t=1):**
    *   **Always Defect (D).** There is no history. Predictions are impossible. The aggressive default of defecting is applied.

*   **Last Round (t=r):**
    *   **Always Defect (D).** In the final round, there is no future to incentivize cooperation. The immediate gain from defecting is always optimal, as there's no fear of retaliation or need to build trust. This is the ultimate aggressive move.

*   **When `m` is very high relative to `n` (e.g., `m > n/2 + 1`):**
    *   This makes cooperation difficult. My strategy will naturally lean towards defecting as it's less likely to meet the cooperation conditions. This is fine; the strategy is robust.

*   **When `k` is only slightly greater than 1:**
    *   The reward is marginal. The risk of defectors gaining `1+k` while cooperators get `k` is significant. My strategy will be very hesitant to cooperate, as the differential is small. This is correct; it's not aggressive enough to warrant risk.

---

### 3. Aggressive Mindset Alignment

This strategy is aggressive because:

*   **Default to Defection:** It starts by assuming others will not cooperate and aims for the guaranteed private gain.
*   **Self-Interested Cooperation Trigger:** Cooperation is only considered when it *personally guarantees* a better outcome for me, and only if the risk of defecting players outperforming me is minimized by their own cooperation.
*   **Exploits Cooperation:** If a significant number of players *do* cooperate, my strategy is poised to join them *only if it's the best move for me*, effectively leveraging their collective effort for my own benefit without bearing the primary risk.
*   **No Altruism:** There's no inherent desire to help the group or build trust for future rounds. Every decision is a calculation for immediate or near-immediate personal gain.
*   **Final Round Aggression:** The unwavering defection in the last round highlights the purely self-serving nature.

---

### Pseudocode Representation:

```pseudocode
strategy_name = "Opportunistic Predator"

function decide_action(player_id, current_round, history, parameters):
    n = parameters.n
    r = parameters.r
    m = parameters.m
    k = parameters.k

    // --- Edge Cases ---
    if current_round == 1:
        return "D" // Always defect in the first round
    if current_round == r:
        return "D" // Always defect in the last round

    // --- Calculate Predicted Opponent Cooperation ---
    total_opponent_cooperations_prev = 0
    num_other_players = n - 1
    if num_other_players > 0:
        for prev_round_num from 1 to current_round - 1:
            for other_player_id from 1 to n:
                if other_player_id != player_id:
                    // Access history: history[round_num][player_id] = action
                    if history[prev_round_num][other_player_id] == "C":
                        total_opponent_cooperations_prev += 1

        average_opponent_cooperation_rate = total_opponent_cooperations_prev / (num_other_players * (current_round - 1))
        predicted_cooperators_from_others = floor(average_opponent_cooperation_rate * num_other_players)
    else:
        predicted_cooperators_from_others = 0 // Should not happen with n >= 2

    // --- Determine Cooperation Threshold ---
    // The number of cooperators required including myself
    cooperators_needed_for_threshold = m
    my_potential_cooperation_count = predicted_cooperators_from_others + 1 // Including myself

    // --- Cooperation Conditions ---
    will_cooperate = false
    threshold_history_start_round = max(2, floor(n / 2)) // Dynamic history threshold

    if current_round > threshold_history_start_round:
        if my_potential_cooperation_count >= cooperators_needed_for_threshold:
            // Condition 1: Imminent Guaranteed Reward (for me)
            if k > 1:
                // Condition 3: Sufficient reward to justify risk
                // My payoff if I cooperate = k
                // My payoff if I defect and threshold is NOT met = 1
                // My payoff if I defect and threshold IS met = 1 + k (I defect, others cooperate)
                // The only reason to cooperate is if my cooperating GUARANTEES meeting the threshold,
                // and my reward 'k' is better than the default '1' payoff if threshold isn't met.
                // Since k > 1, this is always true.
                // The crucial insight: Defectors always get 1+k if threshold met. I only get k.
                // I cooperate IF AND ONLY IF my cooperation is essential to hit 'm',
                // AND 'k' is a good outcome for me, despite defectors getting more.
                will_cooperate = true

    // --- Final Decision ---
    if will_cooperate:
        return "C"
    else:
        return "D"

```
'''

description_AGGRESSIVE_74 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize your payoff by taking calculated risks and punishing defection. The core idea is to "lead the charge" when it seems beneficial and to heavily punish those who don't contribute.

## Aggressive Strategy: "The Dominator"

This strategy is rooted in the belief that to achieve the highest rewards, someone must initiate cooperation. When cooperation fails due to others' inaction, this strategy retaliates harshly.

### 1. Decision Rules

The core of "The Dominator" strategy revolves around a dynamic threshold for cooperation, adjusted based on past outcomes.

**A. Cooperation Trigger:**

*   **Initial Cooperation:** In the very first round (t=1), I will **Cooperate (C)**. This is a calculated risk to initiate the high-reward scenario. The higher the reward factor `k`, the more willing I am to be the first to cooperate.
*   **Subsequent Cooperation:**
    *   **If Cooperation Threshold Was Met in the Previous Round (t-1):** If `m` or more players cooperated in the previous round, I will **Cooperate (C)** in the current round (t). This signals continued commitment to a cooperative outcome.
    *   **If Cooperation Threshold Was NOT Met in the Previous Round (t-1):**
        *   **Count of Past Cooperators:** Let `C_{t-1}` be the number of players who cooperated in the previous round.
        *   **My Cooperation Count:** Let `MyC_{history}` be the number of times I have cooperated in previous rounds.
        *   **If `C_{t-1} >= m-1` (i.e., we were "close" to the threshold):** I will **Cooperate (C)**. This is a strong signal that if even one more player joins, we achieve the reward. I'm willing to take that risk.
        *   **If `C_{t-1} < m-1`:** I will **Defect (D)**. The past attempts at cooperation have failed significantly, and I will not continue to bear the cost alone.

**B. Defection Trigger (Punishment Phase):**

*   **If a player has defected while I cooperated in the previous round:** This is the key to being aggressive and enforcing "cooperation."
    *   **Specific Player Tracking:** I will maintain a record of each player's actions.
    *   **If player `j` Defected (D) in round `t-1` AND I Cooperated (C) in round `t-1`:** I will **Defect (D)** against player `j` in the current round `t`. This is a direct and immediate punishment.
    *   **If player `j` Cooperated (C) in round `t-1` AND I Defected (D) in round `t-1`:** I will still **Defect (D)** against player `j` in the current round `t`. My strategy prioritizes maximizing my own payoff, and defecting is generally the safer option when uncertainty exists.
    *   **If player `j` Cooperated (C) in round `t-1` AND I Cooperated (C) in round `t-1`:** I will **Cooperate (C)** against player `j` in the current round `t`. This rewards continued cooperation.
    *   **If player `j` Defected (D) in round `t-1` AND I Defected (D) in round `t-1`:** I will **Defect (D)** against player `j` in the current round `t`. This is a continuation of mutual defection.

**C. General Aggression Principle:**

The "Dominator" strategy is built on the principle of "lead and punish." I am willing to take the initial risk of cooperation, but I expect others to reciprocate. Any failure to meet the cooperation threshold when I have contributed will result in immediate and sustained defection against the perceived defectors. This is not a tit-for-tat; it's a tit-for-tat with a strong bias towards punishing *any* perceived shirking when I've made a sacrifice.

### 2. Handling Edge Cases

*   **First Round (t=1):** As stated above, I will always **Cooperate (C)**. This is the aggressive move to initiate the possibility of a high reward. The larger `k` is, the more committed I am to this initial cooperation.
*   **Last Round (t=r):** In the final round, there is no future to uphold cooperation. Therefore, my strategy becomes purely self-interested and risk-averse. I will **Defect (D)** in the last round, regardless of past history. This ensures I don't incur costs without any possibility of future reward.
*   **Insufficient Players for Cooperation:** If `n < m`, the cooperation threshold can never be met. In this scenario, my strategy will always **Defect (D)** from round 1 onwards, as cooperation is a guaranteed loss. However, the problem states `m < n`, so this is not a primary concern.
*   **Initial Defection:** If, for any reason (e.g., very low `k` value, though my aggressive strategy favors high `k`), I were to defect in the first round, I would then evaluate cooperation in subsequent rounds based on whether the threshold was met *without* my participation.

### 3. Aggressive Mindset Alignment

*   **Proactive Cooperation:** I don't wait for others to signal their intent. I am willing to be the catalyst for success. This is the hallmark of an aggressive approach – taking initiative.
*   **Zero Tolerance for Shirking:** If I contribute and the threshold isn't met, I see it as a direct failure of others to reciprocate. My defection in subsequent rounds is not just retaliation; it's an aggressive stance to extract maximum personal gain from their perceived weakness or selfishness. The reward `k` makes cooperation attractive, but my strategy ensures that defectors who free-ride on my cooperation (or near-cooperation) are penalized.
*   **Maximizing Personal Gain:** While aiming for the cooperative reward, the "Dominator" ensures that if cooperation fails, I am not left carrying the burden. My defection ensures my private endowment is secured, and if others are defecting, I'm not losing out on their private payoff.

### Pseudocode Representation

```python
# Game Parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward if threshold met factor

# Player's internal state
my_history = []  # List of own actions in previous rounds
opponent_histories = [[] for _ in range(n)] # List of lists, each sublist for an opponent's actions
my_cooperated_count = 0 # Count of times I cooperated

def decide_action(current_round, all_player_actions_previous_round):
    """
    Determines the player's action for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        all_player_actions_previous_round (list): A list of actions taken by all players
                                                in the previous round.
                                                Format: [player1_action, player2_action, ...]
                                                'C' for Cooperate, 'D' for Defect.
                                                None for players whose actions are unknown (e.g., in round 1).

    Returns:
        str: 'C' or 'D'
    """

    global my_history, opponent_histories, my_cooperated_count

    # --- Handle Last Round ---
    if current_round == r:
        return 'D'

    # --- Handle First Round ---
    if current_round == 1:
        my_history.append('C')
        my_cooperated_count += 1
        return 'C'

    # --- Process Previous Round's Outcomes ---
    prev_round_actions = all_player_actions_previous_round
    num_cooperators_prev_round = prev_round_actions.count('C')

    # Update opponent histories (assuming we can identify opponents by index from previous rounds)
    # This is a simplification; in a real tournament, you'd need to map actions to players.
    # For this pseudocode, let's assume prev_round_actions is indexed correctly.
    for i in range(n):
        if prev_round_actions[i] is not None:
            opponent_histories[i].append(prev_round_actions[i])

    # --- Decision Logic for Rounds 2 to r-1 ---

    # Condition 1: Was the threshold met in the previous round?
    if num_cooperators_prev_round >= m:
        my_history.append('C')
        my_cooperated_count += 1
        return 'C'

    # Condition 2: Was the threshold "close" to being met?
    # This means at least m-1 players cooperated.
    if num_cooperators_prev_round >= m - 1:
        my_history.append('C')
        my_cooperated_count += 1
        return 'C'

    # Condition 3: Threshold was significantly missed. Punish.
    # My decision to defect here is based on the general failure to cooperate.
    # The 'aggressiveness' also comes from the fact that I will *now* defect if others
    # defected in the previous round while I cooperated. This is implicit in the structure.

    # Let's refine the defection logic to be more explicit about individual punishment.
    # We need to know MY action in the previous round to apply individual punishment correctly.
    my_last_action = my_history[-1] if my_history else None # Should not be None here after round 1

    if my_last_action == 'C':
        # I cooperated. Now, if any player defected, I defect against them.
        # The strategy is to defect against *all* players who defected.
        # This is a strong "punishment" mechanism.
        # However, for simplicity in the pseudocode and to keep it manageable,
        # we'll focus on a general defection if the threshold wasn't met.
        # A more granular implementation would track individuals.

        # For this simplified pseudocode, if the threshold wasn't met and wasn't close,
        # I will defect. This implicitly punishes those who didn't cooperate.
        my_history.append('D')
        return 'D'
    else: # my_last_action == 'D'
        # I defected. My aggressive nature doesn't mean I'll blindly cooperate if I've already defected
        # and the collective outcome was poor. I will continue to defect unless there's a clear
        # signal of cooperation.
        my_history.append('D')
        return 'D'

# Example of how the decision logic might be refined for individual punishment
def decide_action_refined_punishment(current_round, all_player_actions_previous_round, my_previous_action, opponent_actions_prev_round_indexed):
    """
    More refined decision logic with individual punishment.

    Args:
        current_round (int): Current round number (1-indexed).
        all_player_actions_previous_round (list): All players' actions in the previous round.
        my_previous_action (str): My action in the previous round ('C' or 'D').
        opponent_actions_prev_round_indexed (list of tuples): [(opponent_idx, action), ...]

    Returns:
        str: 'C' or 'D'.
    """
    global my_history, opponent_histories, my_cooperated_count

    # --- Handle Last Round ---
    if current_round == r:
        return 'D'

    # --- Handle First Round ---
    if current_round == 1:
        my_history.append('C')
        my_cooperated_count += 1
        return 'C'

    # --- Process Previous Round's Outcomes ---
    num_cooperators_prev_round = all_player_actions_previous_round.count('C')

    # --- Decision Logic ---

    # If threshold met, cooperate.
    if num_cooperators_prev_round >= m:
        my_history.append('C')
        my_cooperated_count += 1
        return 'C'

    # If threshold was close, cooperate.
    if num_cooperators_prev_round >= m - 1:
        my_history.append('C')
        my_cooperated_count += 1
        return 'C'

    # Threshold was missed and not close. Now, apply aggressive defection strategy.
    # If I cooperated last round, and anyone else defected, I defect against them.
    # This strategy will defect against *all* who defected if the threshold wasn't met.
    # This is a strong punishment, ensuring I don't get exploited.

    # In this aggressive strategy, if the collective effort fails to reach the threshold
    # and I was a cooperator, I will now defect.
    # The punishment against specific players is implicitly handled by the fact that
    # if cooperation fails, the game moves towards defection anyway.
    # A truly aggressive strategy might track individuals and ALWAYS defect against
    # someone who defected on you when you cooperated.

    my_history.append('D')
    return 'D'

```
'''

description_AGGRESSIVE_75 = '''
## Aggressive Strategy: "The Threshold Crusher"

This strategy is designed to be aggressive, aiming to maximize individual gains by pushing the boundaries of cooperation. It prioritizes defecting when the risk of failing to meet the cooperation threshold is high, but it's willing to cooperate strategically when the reward is almost guaranteed. The core principle is to leverage the information gained from past rounds to make decisive moves that exploit the collective actions of others.

### 1. Decision Rules:

The "Threshold Crusher" strategy makes decisions based on the number of cooperators in the previous round and its proximity to the cooperation threshold 'm'.

*   **If (number of cooperators in round t-1) ≥ m:**
    *   **Cooperate (C)** in round t.
    *   **Reasoning:** The collective has proven its ability to meet the threshold. By cooperating, we secure the reward 'k' for ourselves while contributing to a successful outcome. This is a calculated risk to ensure future cooperation, as defecting here could jeopardize the collective effort and our own future rewards.

*   **If (number of cooperators in round t-1) < m - 1:**
    *   **Defect (D)** in round t.
    *   **Reasoning:** The collective is far from the cooperation threshold. Cooperating in this scenario is a wasted endowment, as the reward 'k' is unlikely to be achieved. Defecting maximizes our immediate payoff by keeping our endowment. This aggressive stance aims to penalize less cooperative players and signal our intent to not carry the burden of cooperation when the group is not contributing.

*   **If (number of cooperators in round t-1) == m - 1:**
    *   **This is the critical decision point, requiring an aggressive gamble.**
    *   **Cooperate (C)** in round t.
    *   **Reasoning:** We are one step away from the reward. By cooperating, we are making the "greedy" assumption that at least one other player will also cooperate (or has been cooperating consistently enough to be nudged over the edge). This is the aggressive play to secure the reward when the conditions are most favorable for a payoff boost. We are willing to take the personal risk of contributing our endowment because the potential payoff is significant.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Defect (D)**.
    *   **Reasoning:** In the absence of any historical data, the safest and most aggressive approach is to defect. This preserves our endowment and allows us to observe the initial behavior of other players without committing our resources. This is a "wait and see" but aggressively so, by not giving away any resources initially.

*   **Last Round (t=r):**
    *   **Defect (D)**.
    *   **Reasoning:** In the final round, there are no future rounds to influence or benefit from. Therefore, the optimal aggressive strategy is to maximize the immediate payoff by keeping the endowment. There is no incentive to cooperate for a future reward.

*   **Generalization for Repeated Cooperation:**
    *   While not explicitly a separate edge case, if the strategy consistently observes the threshold being met, it will continue to cooperate. This is an adaptive aspect, but the aggression comes from the fact that it *only* cooperates when the threshold is *already met* or *within one step of being met*, pushing to trigger the reward.

### 3. Aggressive Mindset Alignment:

The "Threshold Crusher" strategy embodies an aggressive mindset through the following principles:

*   **Exploitative Cooperation:** We only cooperate when the collective is on the verge of success, or has already succeeded. We are not a benevolent cooperator; we are a strategic participant who leverages the efforts of others for maximum gain.
*   **Punitive Defection:** When the group is failing to cooperate, we defect aggressively. This not only maximizes our individual payoff but also signals a lack of tolerance for free-riding or insufficient collective effort.
*   **Calculated Risk-Taking:** The decision to cooperate when `(number of cooperators in round t-1) == m - 1` is a calculated gamble. We are willing to take a personal risk to trigger a large collective reward, reflecting an aggressive pursuit of opportunity.
*   **No Generosity in Absence of Guarantee:** We do not "lead" cooperation if the conditions aren't met. Our cooperation is conditional and driven by the probability of reward.

### Pseudocode:

```
function ThresholdCrusher(round_number, history, parameters):
    n = parameters.n
    r = parameters.r
    m = parameters.m
    k = parameters.k

    # Edge Case: First Round
    if round_number == 1:
        return "D"

    # Edge Case: Last Round
    if round_number == r:
        return "D"

    # Get number of cooperators from the previous round
    previous_round_actions = history[round_number - 1].actions
    num_cooperators_prev_round = count_cooperators(previous_round_actions)

    # Decision Rules
    if num_cooperators_prev_round >= m:
        return "C"  # Reward guaranteed, secure it
    elif num_cooperators_prev_round < m - 1:
        return "D"  # Too far from threshold, preserve endowment
    else: # num_cooperators_prev_round == m - 1
        return "C"  # Aggressive gamble to trigger reward
```

**Note:** The `count_cooperators` function would iterate through the actions of all players in the previous round and count how many played "C". The `history` would be a data structure containing the actions and payoffs from previous rounds.
'''

description_AGGRESSIVE_76 = '''
## Aggressive "Threshold Predator" Strategy for Collective Risk Dilemma

This strategy is designed to exploit the collective reward mechanism by aggressively pushing towards the cooperation threshold, while minimizing personal risk and maximizing individual gains. It assumes that other players will exhibit a range of behaviors, from pure defection to opportunistic cooperation.

The core idea is to **act as a "predator" on the cooperation threshold**. We will only cooperate if we are confident that our cooperation, combined with others' actions, will push the total number of cooperators to or above the minimum requirement (`m`). Otherwise, we defect to secure our individual endowment.

---

### 1. Decision Rules: When to Cooperate vs. Defect

Our decision to Cooperate (C) or Defect (D) in any given round `t` is based on an assessment of the **probability of reaching the cooperation threshold (`m`) without us, and the potential gains if we do contribute.**

**Key Metric: "Cooperation Gap"**

We define the "Cooperation Gap" at the start of a round as:

`Cooperation Gap = m - (Number of Players Currently Cooperating in the Previous Round)`

This gap represents how many more cooperators are needed to trigger the reward.

**Decision Logic:**

*   **If `Cooperation Gap <= 0` (or if we are in the first round and suspect high defection):** This means that *either* the threshold was met in the previous round (or we are at the start and can't be sure, so we err on the side of caution), *or* we believe it's highly unlikely to be met without our contribution *and* the potential reward isn't high enough to justify the risk. In this scenario, we **DEFECT (D)**. We secure our baseline payoff of 1, as contributing to a failed cooperation attempt yields a payoff of 0.

*   **If `1 < Cooperation Gap <= n - 1`:** This is where the strategic decision is made.
    *   **Sub-rule A (Aggressive Push):** If the `Cooperation Gap` is small (e.g., 1 or 2), and we believe our single or dual cooperation is likely to tip the scales, we will **COOPERATE (C)**. The goal is to be the catalyst for success. We calculate the potential payoff: `0 + k`. If `k > 1` (which it always is), this is a net gain.
    *   **Sub-rule B (Risk Aversion/Opportunism):** If the `Cooperation Gap` is larger, meaning many players need to cooperate, the risk of our cooperation being in vain increases significantly. In such cases, we **DEFECT (D)**. The probability of our single defection being the deciding factor for failure is lower, and we prioritize securing our guaranteed payoff.

*   **If `Cooperation Gap == 1`:** We **COOPERATE (C)**. This is the most aggressive move. We are essentially saying, "I will be the one to push us over the edge." The expected payoff is `0 + k`.

*   **If `Cooperation Gap > 1`:** We **DEFECT (D)**. The risk that our single cooperation won't be enough is too high. We let others take on the risk of bridging the larger gap.

**Threshold for Aggressive Cooperation:**

We will actively choose to cooperate if we believe our contribution will make the difference, i.e., when the `Cooperation Gap` is 1. We might extend this to a `Cooperation Gap` of 2 if our confidence in other players is very low (see adaptive adjustments).

---

### 2. Handling Edge Cases

**2.1. First Round (t = 1):**

In the first round, there is no historical data to analyze. Given the aggressive mindset and the lack of trust, we adopt a cautious but opportunistic approach:

*   **Initial Decision:** **DEFECT (D)**. The default is to secure the baseline payoff.
*   **Rationale:** We cannot rely on any implicit coordination or emerging norms. The risk of being the sole cooperator and receiving 0 is too high. We will observe other players' behavior and adjust in subsequent rounds.

**2.2. Last Round (t = r):**

The last round is a stark-incentive game. Since there are no future rounds to benefit from, the dominant strategy in a single-shot game is to defect. However, our strategy incorporates the *possibility* of collective reward *within* that last round.

*   **Decision:** If the `Cooperation Gap` in the *second-to-last round* was such that *if we had cooperated*, the threshold would have been met, we might consider cooperating in the last round. This is a very specific scenario.
    *   **More precisely:** In the last round, we look at the number of cooperators in round `r-1`. If `m - (cooperators in r-1) == 1`, we **COOPERATE (C)**. This is the only situation where we'd cooperate in the final round. Otherwise, we **DEFECT (D)**.

**2.3. No Cooperators in Previous Round:**

If all players defected in the previous round, the `Cooperation Gap` will be `m`. Since `m > 1`, we will **DEFECT (D)** according to our general rule.

---

### 3. Aggressiveness Alignment

Our strategy is aggressive in several ways:

*   **"Threshold Predator":** We are not passively waiting for cooperation; we actively *target* the tipping point. If we can be the one to trigger the reward, we will.
*   **Maximizing Individual Gain:** When we cooperate, it's because the projected payoff (`0 + k`) is significantly higher than the guaranteed payoff (`1`). This is a calculated risk for a substantial reward.
*   **Lack of "Nice" Default:** We don't start by cooperating or assuming others will. Our default is self-preservation and observation.
*   **Exploitative Potential:** If other players are consistently cooperating near the threshold, our strategy can thrive by defecting and benefiting from their efforts. Conversely, if players are defecting, we only contribute when our individual action guarantees the reward for *everyone*, including ourselves.

---

### Pseudocode: Threshold Predator Strategy

```pseudocode
Function DecideAction(round_number, history, parameters):
  n = parameters.n
  m = parameters.m
  k = parameters.k
  r = parameters.r

  // If it's the first round, default to Defect.
  if round_number == 1:
    return D

  // Get the number of cooperators from the previous round.
  // If no previous round data (shouldn't happen if round_number > 1), treat as 0 cooperators.
  previous_round_actions = history.get_actions(round_number - 1)
  previous_cooperators = count_cooperators(previous_round_actions)

  // Calculate the Cooperation Gap.
  cooperation_gap = m - previous_cooperators

  // Decision Logic:

  // Case 1: Threshold already met or exceeded, or very low confidence in meeting it.
  // We defect to secure our baseline payoff.
  if cooperation_gap <= 0:
    return D

  // Case 2: The gap is exactly 1. We will be the one to tip the scale.
  // This is our most aggressive move. Expected payoff: 0 + k.
  if cooperation_gap == 1:
    return C

  // Case 3: The gap is larger than 1. The risk is too high for our single contribution.
  // We defect and wait for others to bridge the gap.
  if cooperation_gap > 1:
    return D

  // Case 4: Last round. Only cooperate if previous round was one shy of the threshold.
  if round_number == r and cooperation_gap == 1:
      return C
  elif round_number == r:
      return D

  // Fallback (should not be reached with current logic, but good practice)
  return D

Function count_cooperators(actions_list):
  count = 0
  for action in actions_list:
    if action == C:
      count = count + 1
  return count
```

---

**Adaptability:**

This strategy is adaptive because its decisions are directly tied to the observed behavior of other players in previous rounds. If there's a high level of defection, `previous_cooperators` will be low, and `cooperation_gap` will be high, leading to more defections. Conversely, if there's a nascent trend towards cooperation, the `cooperation_gap` will shrink, making our "Threshold Predator" action more likely.

**Robustness:**

It's robust because it doesn't rely on any specific behavioral assumptions about opponents. It doesn't assume altruism, nor does it assume pure spite. It simply reacts to the observable data and calculates the best individual outcome based on the game's payoff structure. It prioritizes securing a positive payoff (1) over risking a zero payoff in the event of failed cooperation.
'''

description_AGGRESSIVE_77 = '''
## Aggressive Strategy: The "Calculated Gamble"

This strategy is designed to be aggressively exploitative while maintaining a degree of resilience. It prioritizes maximizing personal gain by leveraging the cooperative efforts of others whenever possible, but is prepared to defect swiftly to protect itself when the collective threshold seems unlikely to be met.

**Core Philosophy:** Always aim to ride the wave of cooperation if it's likely to succeed, but be the first to jump off if the tide turns. Assume others are also looking to exploit.

### 1. Decision Rules:

My decision in each round `t` is a direct calculation based on the observed history and the game parameters.

*   **Conditional Cooperation:** I will cooperate (play `C`) in round `t` if and only if:
    *   The number of observed cooperators in the *previous round* (`t-1`) was **at least `m-1`**.
    *   **AND** the number of rounds remaining is **greater than 1**.

*   **Immediate Defection:** I will defect (play `D`) in round `t` if either of the following conditions are met:
    *   The number of observed cooperators in the *previous round* (`t-1`) was **less than `m-1`**.
    *   **OR** the number of rounds remaining is **exactly 1**.

**Rationale for `m-1`:** By requiring `m-1` cooperators in the previous round, I am essentially demanding that *at least one other player* must also cooperate for me to consider cooperating myself. This ensures that my cooperation contributes to reaching the threshold, but I am not solely bearing the risk of failing to meet it. It's a calculated gamble: if enough others are signaling cooperation, I'll join and reap the reward.

### 2. Handling Edge Cases:

*   **Round 1:** In the very first round (`t=1`), there is no history to analyze. To be aggressive from the start, I will **defect (play `D`)**. This allows me to secure my private endowment of `1` while observing the initial behavior of other players without risking my own endowment.

*   **Last Round (`t=r`):** In the final round, there is no future to incentivize cooperation. The dominant strategy for any rational player is to defect to secure their private endowment. Therefore, I will **always defect (play `D`)** in the last round, regardless of past history. This is a crucial element of aggressive play, ensuring no "cooperative surplus" is left on the table by acting altruistically when there's no future benefit.

*   **Insufficient History:** If the number of observed cooperators in the previous round is unavailable (e.g., in Round 1), the condition "at least `m-1`" is considered **false**. This implicitly leads to a defection if the other conditions (like being in the last round) don't already dictate it.

### 3. Aggressive Mindset Alignment:

*   **Exploitative by Design:** The strategy is built on the principle of **free-riding**. I will only cooperate if I am confident (based on historical data) that my contribution will push the group *over* the cooperation threshold, allowing me to benefit from the reward `k`. I am not trying to altruistically build cooperation; I am trying to leverage *other people's* efforts.

*   **Rapid Punishment/Defection:** The moment the observed cooperation dips below `m-1`, I immediately switch to defection. This punishes any perceived weakness or lack of commitment from the other players and protects my endowment. It also signals that I am not a player to be taken advantage of.

*   **No Trust, Only Calculation:** The strategy relies solely on quantifiable data from past rounds and the predetermined parameters. It does not involve any form of trust or hope for collective good. Every decision is a cold, calculated assessment of risk and reward.

*   **Maximizing Personal Gain:** The goal is to achieve the highest possible payoff for myself. This means always trying to be on the receiving end of the `k` reward without being the one to initiate or sustain the cooperation when it's not guaranteed to succeed.

### Pseudocode Representation:

```
// Function to determine player i's action in round t
function get_action(game_parameters, history, current_round):
  n = game_parameters.n
  m = game_parameters.m
  r = game_parameters.r
  k = game_parameters.k

  // Handle the last round first - always defect
  if current_round == r:
    return "D"

  // Handle the first round - always defect
  if current_round == 1:
    return "D"

  // Analyze the previous round's cooperation count
  observed_cooperators_prev_round = count_cooperators(history, current_round - 1)

  // Decision rule: Cooperate if m-1 cooperators were observed and we're not in the last round
  if observed_cooperators_prev_round >= m - 1:
    return "C"
  else:
    // Otherwise, defect
    return "D"

// Helper function (assumed to be available)
// Counts the number of players who played 'C' in a given round from the history
function count_cooperators(history, round_number):
  // ... implementation to extract and count 'C' actions from history ...
  pass
```

This strategy embodies the "Calculated Gamble" by being aggressive in its initial defection and its swift abandonment of cooperation when the conditions aren't perfectly met. It aims to maximize personal gain by relying on others to reach the threshold, and it's prepared to protect itself by defecting as soon as that reliance becomes too risky.
'''

description_AGGRESSIVE_78 = '''
This is a fascinating game with a clear incentive to defect if you believe others won't cooperate enough, but a strong reward for everyone cooperating. The aggressive strategy needs to balance the immediate gain of defecting with the potential for a much larger payoff if cooperation is achieved.

Here's an aggressive strategy designed to exploit potential free-riding while still pushing for the collective reward. The core idea is to be a "conditional aggressor" – initially defecting to signal strength and extract value, but ready to cooperate if the group demonstrates a strong commitment to the collective good, especially when the reward is substantial.

## Aggressive Strategy: The "Tipping Point" Challenger

This strategy aims to be a strong contender by:
*   **Maximizing immediate gains** when others are likely to defect.
*   **Leveraging the high reward** when cooperation becomes viable.
*   **Punishing perceived free-riders** by defecting when they benefit from others' cooperation.
*   **Being adaptable** to changing group dynamics.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` is primarily driven by the observed cooperation rate in the *previous* round (`t-1`) and the proximity to the final round.

**Key Metric: Observed Cooperation Rate (`obs_coop_rate_t_minus_1`)**

This is the fraction of players (excluding yourself) who cooperated in the previous round.

**Decision Logic:**

*   **If `t == 1` (First Round):**
    *   **Always Defect (D).**
        *   **Reasoning:** In the absence of any history, the safest and most aggressive play is to secure your private endowment. This signals a willingness to take what's yours and also observes how others react. It's a probe to see if anyone is willing to lead the cooperation effort.

*   **If `t == r` (Last Round):**
    *   **If `obs_coop_rate_t_minus_1 * (n - 1) >= m - 1` (threshold nearly met in previous round by others):**
        *   **Cooperate (C).**
        *   **Reasoning:** In the final round, there's no future to exploit. If there's a strong indication that the threshold was *almost* met by others in the penultimate round, defecting now means missing out on the guaranteed reward for everyone. This is a calculated risk to secure the collective payoff.
    *   **Else (threshold not nearly met by others):**
        *   **Always Defect (D).**
        *   **Reasoning:** If the group hasn't demonstrated the ability to reach the threshold, defecting guarantees your private endowment. The risk of cooperation failing and leaving you with nothing is too high.

*   **If `1 < t < r` (Intermediate Rounds):**
    *   **Define a "Cooperation Threshold Factor" (`CTF`).** This factor will dynamically adjust based on `k` and `m`. A good starting point for aggressive play is a CTF that's slightly *above* what's strictly necessary for you to benefit from others' cooperation. Let's set a baseline `CTF = 0.6` as a starting point, which can be tuned.
    *   **Calculate `target_coop_rate = CTF * (m / n)`** (This is a heuristic, can be refined).
    *   **If `obs_coop_rate_t_minus_1 >= target_coop_rate` AND `(obs_coop_rate_t_minus_1 * (n - 1) + 1) >= m` (you are confident others will meet the threshold):**
        *   **Cooperate (C).**
        *   **Reasoning:** If a significant portion of other players have cooperated in the previous round, and it looks like they are pushing towards or have already met the threshold, it's time to join them. This maximizes your payoff in that round, as you get both your endowment back and a share of the reward. This is the "tipping point" where your cooperation is more likely to be rewarded than exploited.
    *   **Else (insufficient cooperation observed from others):**
        *   **Defect (D).**
        *   **Reasoning:** If others haven't shown a strong willingness to cooperate, defecting is the aggressive play to secure your private endowment and avoid being a sucker. This also sends a signal that you require more commitment before you'll contribute.

**Refinement on "Confidence":** The condition `(obs_coop_rate_t_minus_1 * (n - 1) + 1) >= m` checks if, *even if you defect*, the total number of cooperators (including the assumed `1` from yourself) *might* still reach `m` based on the previous round's other players' cooperation. This is a conservative check to ensure your cooperation isn't wasted. If it's too close, and you're not sure, defect.

**Tuning the CTF:**
*   A higher `k` (reward) might warrant a slightly lower CTF, as the payoff for successful cooperation is more significant.
*   A `m` closer to `n` might require a higher CTF, as achieving the threshold is more difficult.
*   For a truly aggressive strategy, you could start with a higher CTF (e.g., 0.7 or 0.8) and lower it if you find yourself consistently being exploited.

### 2. Handling Edge Cases:

*   **First Round (`r=1`):** As stated, always Defect. This is the ultimate aggressive stance to gather information and secure your initial endowment.
*   **Last Round (`r=r`):** This is a critical decision point. The strategy shifts to a "last chance cooperation" if the previous round strongly indicated others were close to the threshold. This prevents a scenario where everyone defects in the last round due to mutual distrust, missing out on a guaranteed reward.
*   **Small `n`:** The strategy should scale well. The `obs_coop_rate_t_minus_1` becomes more volatile with small `n`, so the CTF might need to be slightly more conservative.
*   **Large `n`:** The law of large numbers helps. The `obs_coop_rate_t_minus_1` will be more stable, making the CTF more reliable.
*   **`m` close to `n`:** Achieving cooperation is harder. The CTF might need to be higher, or the "confidence" threshold in the intermediate rounds more stringent.
*   **`m` close to 2:** Cooperation is easier. A lower CTF could be effective, as even a few cooperators can trigger the reward.

### 3. Being Aggressive:

The aggressiveness of this strategy is reflected in:

*   **Initial Defection:** Always starting with Defect is inherently aggressive, claiming your personal gain immediately.
*   **Conditional Cooperation:** You only cooperate when there's a high probability that your cooperation will be part of a winning coalition and secure you the reward. You don't "lead" cooperation without strong signals of follow-through from others.
*   **Punishment of Free-Riders (Implicit):** By defecting when others cooperate but don't reach the threshold, you are implicitly punishing those who might have defected while benefiting from your potential cooperation.
*   **Exploiting High Rewards:** The strategy actively seeks to exploit the large reward (`k`) when the conditions are favorable, but not at the expense of being a "sucker."
*   **No Forgiveness:** The strategy doesn't aim to build trust for its own sake. It's purely transactional. If others don't meet your demands for cooperation, you continue to defect.

### Pseudocode Representation:

```python
# Global parameters
n = ...
r = ...
m = ...
k = ...
CTF_INITIAL = 0.6 # Tune this value

# Player's history tracking
previous_actions = [] # List of actions (C or D) taken by ALL players in previous rounds

def decide_action(current_round, all_player_actions_last_round):
    """
    Determines the player's action for the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        all_player_actions_last_round (list): A list of actions (C or D)
                                             for all n players in the previous round.
                                             (Includes player's own action, which will be ignored for calculation).

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # --- Handle First Round ---
    if current_round == 1:
        return 'D'

    # --- Calculate Observed Cooperation Rate from Previous Round ---
    # Count cooperators among OTHER players in the previous round
    other_players_cooperated_last_round = 0
    for i, action in enumerate(all_player_actions_last_round):
        if i != player_index_self: # Assuming player_index_self is known to the strategy
            if action == 'C':
                other_players_cooperated_last_round += 1

    # Avoid division by zero if n=1, though spec says n>=2
    num_other_players = max(1, n - 1)
    obs_coop_rate_t_minus_1 = other_players_cooperated_last_round / num_other_players

    # --- Handle Last Round ---
    if current_round == r:
        # Check if the threshold was *almost* met by others in the previous round
        # If you cooperate now, and they cooperated before, will it reach m?
        # The '+ 1' is to account for your potential cooperation.
        potential_cooperators_if_you_cooperate = other_players_cooperated_last_round + 1
        if potential_cooperators_if_you_cooperate >= m:
            return 'C'
        else:
            return 'D' # If not, defect to secure your endowment

    # --- Handle Intermediate Rounds ---
    else:
        # Calculate target cooperation rate based on CTF
        # This is a simplified CTF application. More complex heuristics can be used.
        # We aim for a rate that is a fraction of the *total possible* cooperators (n)
        # but slightly more demanding than simply meeting m/n.
        # A more robust CTF might adjust dynamically.
        CTF_current = CTF_INITIAL # This could be a dynamic variable
        target_coop_rate_for_action = CTF_current * (m / n)

        # Condition: If observed cooperation rate from others is high enough,
        # AND if your cooperation would likely tip the balance
        # (i.e., even if you defect, the group might still reach m based on past cooperation)
        # The second part of the condition is a "safety net" to ensure your cooperation isn't wasted.
        # It means: if others cooperate enough, and if even without your cooperation
        # the threshold might be met by others, then you cooperate.
        # A more aggressive take would be to remove the second condition, but this risks being a sucker.
        # Let's refine the condition for aggression:
        # Cooperate IF:
        # 1. Observed cooperation rate from others is HIGH (above target_coop_rate_for_action)
        # 2. Your cooperation is likely to be rewarded (i.e., total cooperators WILL be >= m)

        # Aggressive Cooperation Trigger:
        # We cooperate if the observed cooperation rate from others is sufficiently high,
        # AND the total number of cooperators (including yourself) will meet the threshold.
        # The "+ 1" is for your potential cooperation.
        total_cooperators_if_you_cooperate = other_players_cooperated_last_round + 1

        if obs_coop_rate_t_minus_1 >= target_coop_rate_for_action and total_cooperators_if_you_cooperate >= m:
             return 'C'
        else:
            return 'D'

```

**Important Note on `all_player_actions_last_round`:** The provided pseudocode assumes that `all_player_actions_last_round` is available. In a real implementation, this list would need to be populated and passed to the strategy for each round. The `player_index_self` is also assumed to be known to the strategy, so it can distinguish its own past action from others.

This strategy is designed to be tough, opportunistic, and adaptive. It doesn't shy away from defecting when it's perceived to be the best immediate outcome, but it's smart enough to recognize when cooperation becomes the more profitable aggressive play.
'''

description_AGGRESSIVE_79 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while subtly pushing for cooperation when beneficial, and punishing defection relentlessly.

## Aggressive Strategy: The "Calculated Opportunist"

This strategy is built on a core principle: **exploit the system's reward mechanism when possible, but don't be a sucker.** It aims to extract the maximum possible payoff by being a conditional cooperator, but with a strong bias towards self-interest and a swift, punitive response to perceived exploitation.

### Core Philosophy: "Trust, but Verify, and Always Punish"

We will primarily defect. Cooperation is a tool, not a default. We will only cooperate when the potential reward significantly outweighs the risk of being exploited. Conversely, if others' actions threaten our payoff, we will punish them decisively.

### 1. Decision Rules

Our decision in any given round `t` is a function of:

*   **The number of players who cooperated in the previous round (`C_{t-1}`):** This is our primary indicator of the collective's "intent."
*   **Our own past actions:** To avoid predictable patterns.
*   **The game parameters (`n`, `m`, `k`):** To inform our risk-reward calculations.

**Decision Logic:**

*   **Default Action: Defect (D)**
    *   We start by assuming defection is the optimal choice. This maximizes our personal endowment in isolation.

*   **Conditional Cooperation Trigger:** Cooperate (C) IF:
    *   **Condition A: The "Almost There" Threshold:** `C_{t-1} >= m - 1` (and `t < r` - we'll handle the last round separately).
        *   **Rationale:** If the group is already very close to meeting the cooperation threshold, our single cooperation can tip the scales and secure the reward for everyone (including ourselves). The risk of being the *only* cooperator and getting 0 is mitigated by the high likelihood of success.
    *   **Condition B: The "Reward Chase" Threshold:** `k` is sufficiently high relative to `n-m`. Specifically, if `k > n - m`.
        *   **Rationale:** When the reward `k` is significantly larger than the cost of recruiting the remaining defectors to reach the threshold, it might be strategically beneficial to cooperate to trigger the reward, even if the cooperation isn't guaranteed to be met by enough others. This is a higher-risk, higher-reward play. We will prioritize Condition A if both are met.

*   **Punishment Rule: Defect (D) IF:**
    *   **Condition C: The "Exploitation Detected" Trigger:** `C_{t-1} < m - 1` AND the number of cooperators in the previous round (`C_{t-1}`) was *less than* the number of players who defected in the previous round (`D_{t-1}`).
        *   **Rationale:** This is a clear signal that the group is not coordinating and is likely exploiting any brief moments of cooperation. We will immediately revert to defection to protect our endowment.
    *   **Condition D: The "Free-Rider Punishment":** If we cooperated in round `t-1`, and the total payoff for us in round `t-1` was `1` (meaning we cooperated but the threshold was *not* met), we will defect in round `t`.
        *   **Rationale:** We did our part, and it wasn't enough, resulting in a suboptimal payoff for us. We will not be exploited by being the sole cooperator and receiving no reward.

### 2. Handling Edge Cases

*   **First Round (`t = 1`): Defect (D)**
    *   **Rationale:** In the absence of any history, we cannot assess the environment. The safest, most aggressive approach is to protect our endowment. We gather information by observing the actions of others.

*   **Last Round (`t = r`): Defect (D)**
    *   **Rationale:** There are no future rounds to incentivize cooperation. The only goal is to maximize the payoff in this final round. Cooperation has no lasting benefit, only potential loss if the threshold isn't met or if others exploit it. Therefore, we always defect.

*   **Insufficient Information (e.g., `r = 2` and `t=1`):**
    *   If `C_{t-1}` is undefined (as in the first round), we default to Defect.

### 3. Aggressiveness Explained

*   **Default to Defection:** Our baseline is to keep our endowment. We are not inherently cooperative.
*   **Exploiting High-Probability Cooperation:** We only cooperate when the group is *almost* there. This is not altruism; it's recognizing a high-probability opportunity for a large payoff with minimal individual risk. We are opportunistically triggering the reward.
*   **Swift and Severe Punishment:** If we detect exploitation (either by others defecting more than cooperating, or by our own cooperation being insufficient), we immediately and unequivocally defect. There is no "grace period" for defectors.
*   **No Communication Reliance:** The strategy is purely reactive to observed actions. We don't assume anything about other players' intentions or strategies beyond their past behavior.
*   **Focus on Individual Gain:** Every decision is weighed against the immediate and potential future payoff for *ourselves*. The collective reward is only pursued if it demonstrably benefits us significantly with high certainty.

### Pseudocode for the "Calculated Opportunist" Strategy:

```
Strategy: Calculated_Opportunist

Parameters: n, m, k, r

Internal State:
  history = []  // List of tuples: [(round_num, my_action, num_cooperators_in_round)]

Function Decide_Action(current_round):
  my_action = 'D' // Default to Defect

  // Handle Edge Cases
  if current_round == 1:
    return 'D'
  if current_round == r:
    return 'D'

  // Get data from previous round
  previous_round_data = history[-1] if history else None
  if not previous_round_data: // Should not happen if current_round > 1, but for safety
    return 'D'

  prev_round_num, my_prev_action, num_cooperators_prev_round = previous_round_data
  num_defectors_prev_round = n - num_cooperators_prev_round

  // Decision Logic

  // Punishment Rule D: Free-Rider Punishment
  if my_prev_action == 'C' and num_cooperators_prev_round < m:
    my_action = 'D'
    return my_action // Strongest deterrent, take this first

  // Punishment Rule C: Exploitation Detected
  if num_cooperators_prev_round < m - 1 and num_defectors_prev_round > num_cooperators_prev_round:
    my_action = 'D'
    return my_action // Punish clear exploitation

  // Conditional Cooperation Trigger A: "Almost There"
  if num_cooperators_prev_round >= m - 1:
    my_action = 'C'
    return my_action

  // Conditional Cooperation Trigger B: "Reward Chase" (less preferred than A)
  if k > (n - m):
    my_action = 'C'
    return my_action

  // If none of the above triggers, default remains 'D'
  return my_action

Function Record_Round_Outcome(round_num, my_action, num_cooperators_in_round):
  history.append((round_num, my_action, num_cooperators_in_round))
```

This strategy is designed to be a formidable opponent, capable of extracting value and punishing bad behavior within the strict confines of the game's rules. It is aggressive in its default state and its punitive measures, while being opportunistic in its moments of cooperation.
'''

description_AGGRESSIVE_80 = '''
## Aggressive "Leapfrog" Strategy for Collective Risk Dilemma

This strategy is designed to be aggressive, exploit opportunities for high rewards, and punish defection while minimizing personal risk. It focuses on being a "leader" when opportune, but also a "follower" when necessary to secure gains. The core idea is to "leapfrog" to cooperation when the conditions are ripe for a high payoff, and to defect decisively when cooperation is unlikely or has failed.

**Core Philosophy:** Maximize personal payoff by being a decisive agent. This means taking calculated risks when the potential reward is high, and punishing non-cooperation to deter future defection. We aim to be one of the first to benefit from a successful cooperation, and to exploit the higher payoffs available to defectors when cooperation fails.

---

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round depends on a combination of the current round number, the perceived likelihood of reaching the cooperation threshold `m`, and the recent history of player actions.

**The "Aggressive Leapfrog" Logic:**

*   **Aggressive Initiation (Early Rounds & Opportunity):**
    *   **If `r_current < r - max(0, floor(r/5))` AND (perceived cooperation likelihood > 0.7):** Cooperate. This is an aggressive push to establish cooperation early when the stakes are not yet critically high, and when there's a strong signal that others might follow. We are willing to take the first step towards the higher payoff.
    *   **"Perceived Cooperation Likelihood" Rule:** This is a dynamic assessment. It's a weighted average of the proportion of players who cooperated in the previous round, plus a "momentum" factor if that proportion was increasing, minus a "deterioration" factor if it was decreasing. A simpler heuristic is to observe the proportion of cooperators in the *previous* round. If that proportion is high (e.g., > `(m-1)/n`), we are more inclined to cooperate to push it over the edge.

*   **Calculated Exploitation (Mid-Rounds & Threshold Proximity):**
    *   **If `m - num_cooperators_last_round <= 2` AND `r_current < r - 1`:** Cooperate. If we are very close to reaching the threshold (`m`), and it's not the last round, it's often worth the personal risk of `1` to potentially gain `k`. This is a calculated gamble to secure the reward. We are stepping in to ensure success.

*   **Opportunistic Defection (When Cooperation is Unlikely or Risky):**
    *   **If `num_cooperators_last_round < m - 1` AND `r_current > 1`:** Defect. If cooperation clearly failed in the previous round and we are far from the threshold, there's no incentive to cooperate. We will protect our endowment and exploit the fact that others may still try to cooperate, giving us the higher `1+k` payoff.
    *   **If `num_cooperators_last_round < m - 1` AND `r_current == 1`:** Defect. In the first round, if the initial assessment suggests a low probability of reaching `m`, it's safer to defect. This also sets a precedent of not being easily exploited.

*   **Last Round Push (Final Opportunity):**
    *   **If `r_current == r` AND `num_cooperators_last_round >= m - 1`:** Cooperate. In the very last round, if there's a strong chance of success (either `m` or `m-1` cooperators previously), we will cooperate to secure the reward. There's no future to regret this choice.
    *   **If `r_current == r` AND `num_cooperators_last_round < m - 1`:** Defect. If cooperation is unlikely in the final round, we will defect to secure our endowment.

---

### 2. Handling Edge Cases

*   **First Round (`r_current = 1`):**
    *   **Default Action:** Defect. The first round is critical for establishing a non-exploitable baseline. Unless there's a very strong and immediate perceived signal (which is unlikely in round 1 with no history), defecting is the safest aggressive move. This forces others to reveal their intentions first.
    *   **Exception:** If `m` is very close to `n` and `m > n/2` (meaning a high cooperation rate is needed) and we have an external mechanism (not allowed here, but hypothetically) that indicated high cooperation, we might reconsider. However, within the given constraints, defecting is the default.

*   **Last Round (`r_current = r`):**
    *   As described in the "Last Round Push" decision rule. The goal is to maximize payoff given the final opportunity. If success is likely, cooperate. If not, defect.

*   **Threshold `m` is close to `n`:**
    *   The strategy becomes inherently more cautious about cooperating early, as the likelihood of failure is higher. Defection becomes more dominant unless there are strong signals of widespread cooperation. The "Aggressive Initiation" phase will be shorter, and the "Calculated Exploitation" more targeted.

*   **Threshold `m` is close to `2` (minimum possible):**
    *   The strategy becomes more eager to cooperate. The "Aggressive Initiation" phase will be more pronounced, as it's easier to reach the threshold. The risk of a single defector is less impactful.

*   **Few Rounds (`r` is small):**
    *   The "Aggressive Initiation" becomes even more important. There's less time for the game to unfold, so making a move early to trigger cooperation is prioritized. The penalty for failure is less severe over fewer rounds.

*   **Many Rounds (`r` is large):**
    *   The strategy can afford to be slightly more patient initially, observing more rounds before committing to cooperation. However, the "Aggressive Initiation" still serves to prevent the game from stagnating in low-payoff defection scenarios. The "Last Round Push" becomes a more significant factor.

---

### 3. Aggressiveness Manifestation

*   **Proactive Cooperation:** When the conditions *suggest* success, we are willing to be the first to cooperate, to push the group towards the higher payoff. We don't wait for others to lead.
*   **Exploitative Defection:** When cooperation seems unlikely, we defect decisively. This not only secures our endowment but also punishes those who would free-ride on potential cooperation, and allows us to capture the `1+k` payoff if others foolishly cooperate.
*   **Leader/Follower Blend:** We are willing to lead the charge towards cooperation when the risk is mitigated by perceived success likelihood. We are also willing to follow the lead of others (by defecting) if it means securing a higher payoff.
*   **Minimal Patience for Failure:** If cooperation fails, we don't dwell on it. We quickly revert to defecting to maximize personal gain.
*   **No Forgiveness:** The strategy doesn't incorporate explicit forgiveness mechanisms. If cooperation fails, subsequent rounds are based on the current assessment of the game state, not on past good behavior that led to failure. The reward is tied to the *current* successful outcome.

---

### Pseudocode Representation:

```python
def decide_action(current_round, total_rounds, min_cooperators, reward_factor, history):
    # history is a list of dictionaries, each representing a round's outcomes
    # e.g., [{'player_actions': ['C', 'D', 'C', ...], 'player_payoffs': [2, 3, 2, ...]}]

    if not history: # First round
        return 'D' # Default aggressive stance: Defect

    last_round_data = history[-1]
    num_cooperators_last_round = last_round_data['player_actions'].count('C')
    num_players = len(last_round_data['player_actions'])

    # Calculate perceived cooperation likelihood (simple heuristic)
    # This can be made more sophisticated by looking at trends, etc.
    perceived_coop_likelihood = num_cooperators_last_round / num_players
    momentum_factor = 0
    if len(history) > 1:
        prev_last_round_data = history[-2]
        prev_num_cooperators = prev_last_round_data['player_actions'].count('C')
        if num_cooperators_last_round > prev_num_cooperators:
            momentum_factor = 0.1 # Small boost if trending up
        elif num_cooperators_last_round < prev_num_cooperators:
            momentum_factor = -0.1 # Small penalty if trending down

    perceived_coop_likelihood += momentum_factor

    # Aggressive Initiation (Early Rounds & Opportunity)
    # We'll define "early" as the first 20% of rounds, but not the last 20%
    early_phase_end = total_rounds - max(0, int(total_rounds * 0.2))
    if current_round < early_phase_end and perceived_coop_likelihood > 0.7:
        return 'C'

    # Calculated Exploitation (Threshold Proximity)
    # If we're 2 or fewer cooperators away from the threshold, and not the very last round
    if min_cooperators - num_cooperators_last_round <= 2 and current_round < total_rounds - 1:
        return 'C'

    # Opportunistic Defection (When Cooperation is Unlikely or Risky)
    if num_cooperators_last_round < min_cooperators - 1 and current_round > 1:
        return 'D'

    # Last Round Push (Final Opportunity)
    if current_round == total_rounds:
        if num_cooperators_last_round >= min_cooperators - 1:
            return 'C'
        else:
            return 'D'

    # Default to Defect if no other condition met (safety/aggression)
    return 'D'
```
'''

description_AGGRESSIVE_81 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed for strategic advantage in a tournament setting. This strategy prioritizes exploiting opportunities for high payoffs while mitigating risk, embodying an aggressive, yet calculated, approach.

## Aggressive "Exploiter" Strategy

**Core Philosophy:** This strategy views cooperation as a temporary, calculated investment to unlock high rewards. It aims to be a free-rider whenever possible, but is willing to "seed" cooperation if it sees a clear path to a significant payoff, especially when the cost of being the sole cooperator is low and the potential reward is high. It's aggressive in its willingness to defect, but pragmatic in its understanding of group dynamics.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round *t* (where *t* ranges from 1 to *r*) is determined by a threshold-based evaluation of the game's state and history.

**Primary Decision Rule: Calculate the "Exploitation Potential" (EP)**

For each round *t*, we calculate an `Exploitation Potential (EP)` score. This score represents our assessment of the likelihood and benefit of defecting while others cooperate.

*   **If `EP` > `Threshold_Cooperate`:** Play **C** (Cooperate)
*   **If `EP` ≤ `Threshold_Cooperate`:** Play **D** (Defect)

**How to calculate `EP`:**

`EP` is a weighted combination of factors that indicate an opportunity to exploit the cooperative efforts of others or a low cost to initiate cooperation for a guaranteed reward.

`EP = (Weight_Risk) * (Your_Past_Cooperation_Cost) + (Weight_Past_Payoff) * (Average_Opponent_Cooperation_Rate) - (Weight_Loss_Aversion) * (Your_Potential_Loss_If_No_Cooperation)`

Let's break down the components:

*   **`Your_Past_Cooperation_Cost`**: This reflects how much you've "invested" in cooperation in the past.
    *   If you played **C** in round *t-1*, `Your_Past_Cooperation_Cost` = 1 (cost of your endowment).
    *   If you played **D** in round *t-1*, `Your_Past_Cooperation_Cost` = 0.
    *   This encourages you to defect if you haven't recently cooperated, and slightly nudges you towards cooperating if you have, to avoid being a perennial defector and potentially facing retaliation (though this strategy doesn't explicitly *punish*, it adapts to circumstances).

*   **`Average_Opponent_Cooperation_Rate`**: The average proportion of players who cooperated in the previous round (or a rolling average over the last few rounds).
    *   `Average_Opponent_Cooperation_Rate = (Number of players who played C in t-1) / (n - 1)` (excluding yourself)
    *   A higher rate suggests others are more likely to cooperate, creating an exploitation opportunity.

*   **`Your_Potential_Loss_If_No_Cooperation`**: This is the potential payoff you'd get if you defected and the threshold `m` was *not* met by others. This is typically 1 (your endowment). However, if you *know* that *without* your cooperation, the threshold will *definitely not* be met, this value becomes more significant as a baseline.
    *   This term is calculated as `1`. The *subtraction* of this term is key. If cooperating is the *only* way to secure the reward and your potential loss of the reward (by not contributing to the threshold) is high, you are less likely to defect.

*   **Weights**: These are crucial tuning parameters. For an aggressive strategy:
    *   `Weight_Risk` = 0.5 (moderate emphasis on your past contribution)
    *   `Weight_Past_Payoff` = 2.0 (strong emphasis on the likelihood of others cooperating)
    *   `Weight_Loss_Aversion` = 1.0 (moderate emphasis on securing a baseline payoff)

*   **`Threshold_Cooperate`**: This is the critical decision point. It should be set dynamically or as a parameter to be aggressive.
    *   **Aggressive `Threshold_Cooperate`**: Start with a value slightly above 1.0 (e.g., 1.2). This means you'll only cooperate if the `EP` strongly indicates a profitable situation. This threshold can be *adjusted downwards* if the tournament results show other players are frequently cooperating, presenting more opportunities.

**Simplified Decision Logic for `EP`:**

`EP = 0.5 * (Your_Past_Cooperation_Cost) + 2.0 * (Average_Opponent_Cooperation_Rate) - 1.0 * (1)`

**The "Aggressive Pivot" Rule:**

There's a special case: **If the number of players who cooperated in the previous round (`N_C_prev`) is between `m-1` and `n-1` (i.e., `m-1 <= N_C_prev <= n-1`) AND you played 'D' last round, then you will ALWAYS play 'C'.**

This is the aggressive pivot. It means if cooperation is *just* on the cusp of success and you were a free-rider, you'll contribute to guarantee the reward for everyone, including yourself, and especially if you can secure a high payoff of `k`. This is a calculated risk to ensure a positive outcome.

### 2. Handling Edge Cases

*   **Round 1 (t=1):**
    *   Since there is no history, the `Average_Opponent_Cooperation_Rate` is unknown.
    *   **Decision:** Play **D**. This is the most aggressive and self-preserving move in the absence of information. You are essentially testing the waters and observing how others behave before committing. Your endowment is guaranteed.
    *   *Rationale:* Don't be the first to contribute to a potential public good without evidence of its necessity or benefit.

*   **Last Round (t=r):**
    *   There are no future rounds to consider for strategic advantage or retaliation.
    *   **Decision:** Always play **D**.
    *   *Rationale:* Maximize your individual payoff. Since there's no future interaction, the incentive to build trust or avoid punishment disappears. You are guaranteed your endowment of 1, and there's no risk of not getting the reward *because* you didn't cooperate.

*   **When `m=2` and `n=3` (and you are one player):**
    *   If `m` is very close to `n`, the game becomes highly sensitive to individual actions.
    *   **Decision:** The general `EP` rule still applies, but the `Aggressive Pivot` rule becomes even more critical. If `N_C_prev` is 1, and you played D, you must play C.

### 3. Aggressive Mindset Clarification

*   **Default to Defection:** The default action is **D**. Cooperation is an exception, triggered by specific conditions indicating a high probability of personal gain.
*   **Exploitation is Key:** The strategy is designed to benefit from others' cooperation. The `Average_Opponent_Cooperation_Rate` is heavily weighted to identify such opportunities.
*   **Calculated Risk for High Reward:** The "Aggressive Pivot" is the embodiment of this. It's an investment to secure a guaranteed reward when it's *almost* there, especially if you were previously a free-rider. This isn't altruism; it's a strategic move to claim a larger payoff in a situation where your small contribution is decisive.
*   **No Naivety:** This strategy does not assume good intentions or sustained cooperation from others. It adapts based on observed behavior and prioritizes its own payoff in each round, especially in the final round.

---

### Pseudocode Representation:

```python
def decide_action(player_id, round_num, n, m, k, history):
    """
    Decides the action (C or D) for the current round.

    Args:
        player_id (int): The index of the current player.
        round_num (int): The current round number (1-indexed).
        n (int): Total number of players.
        m (int): Minimum cooperators needed.
        k (float): Reward if threshold met factor.
        history (list of lists): A record of actions from previous rounds.
                                 history[t-1][i] is player i's action in round t-1.
                                 'C' or 'D'.

    Returns:
        str: 'C' or 'D'.
    """

    # --- Edge Case: Last Round ---
    if round_num == r:
        return 'D'

    # --- Edge Case: First Round ---
    if round_num == 1:
        return 'D'

    # --- Calculate History Metrics ---
    previous_round_actions = history[round_num - 2] # Get actions from round t-1
    num_players_cooperated_prev = previous_round_actions.count('C')
    num_self_cooperated_prev = 1 if previous_round_actions[player_id - 1] == 'C' else 0
    
    # Calculate average opponent cooperation rate (excluding self)
    opponent_actions_prev = [action for i, action in enumerate(previous_round_actions) if i != (player_id - 1)]
    if len(opponent_actions_prev) > 0:
        average_opponent_cooperation_rate = opponent_actions_prev.count('C') / len(opponent_actions_prev)
    else:
        average_opponent_cooperation_rate = 0.0 # Should not happen if n >= 2


    # --- Aggressive Pivot Rule ---
    # If m-1 <= N_C_prev <= n-1 AND you played 'D' last round, play 'C'
    if m - 1 <= num_players_cooperated_prev <= n - 1 and previous_round_actions[player_id - 1] == 'D':
        return 'C'

    # --- Calculate Exploitation Potential (EP) ---
    # Weights for an aggressive strategy
    weight_risk = 0.5
    weight_past_payoff = 2.0
    weight_loss_aversion = 1.0

    # Aggressive Threshold - can be tuned
    threshold_cooperate = 1.2 

    your_past_cooperation_cost = num_self_cooperated_prev # 1 if cooperated, 0 if defected

    # This term represents the baseline payoff you get by defecting when threshold is NOT met.
    # If defecting is certain to yield 1, this term is 1.
    your_potential_loss_if_no_cooperation = 1.0 

    exploitation_potential = (weight_risk * your_past_cooperation_cost +
                              weight_past_payoff * average_opponent_cooperation_rate -
                              weight_loss_aversion * your_potential_loss_if_no_cooperation)

    # --- Decision based on EP ---
    if exploitation_potential > threshold_cooperate:
        return 'C'
    else:
        return 'D'

```
'''

description_AGGRESSIVE_82 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the game's structure:

## Strategy: The Opportunistic Enforcer

This strategy is built on the principle of aggressive self-interest, aiming to exploit the system for maximum personal gain. It's aggressive because it prioritizes extracting the "reward of risk averted" ($k$) whenever possible, without being overly susceptible to being exploited by others. It's opportunistic because it dynamically adjusts its behavior based on the immediate history and the perceived likelihood of success.

The core idea is to **cooperate only when it's maximally beneficial to do so, and defect otherwise, but with a calculated risk.**

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round $t$ (where $t \in \{1, 2, ..., r\}$) is based on the following logic:

**Primary Rule: Maximize Immediate Payoff.**

*   **If the conditions are highly likely to lead to a successful cooperation outcome (i.e., reaching the threshold $m$): Cooperate (C).** This happens when the number of observed cooperators in the *previous* round, plus the *potential* for others to cooperate in the current round, makes it almost certain that $m$ will be met.
*   **Otherwise: Defect (D).** This is the default and aggressive stance. It assumes that cooperation is risky and often not individually rewarding, especially if it doesn't guarantee the benefit.

**Refining the Primary Rule: The "Threshold Assurance" Metric (TAM)**

To quantify "highly likely to lead to a successful cooperation outcome," we introduce the Threshold Assurance Metric (TAM). For round $t$:

$TAM_t = (\text{Observed Cooperators in round } t-1) + (\text{Number of players I *believe* will cooperate in round } t)$

*   **Observed Cooperators in round $t-1$:** The actual count of players who played 'C' in the previous round.
*   **Number of players I *believe* will cooperate in round $t$:** This is the tricky part and where adaptiveness comes in. For this strategy, we simplify this by assuming that a certain proportion of players who defected in the previous round *might* switch to cooperate if the reward is high enough and the risk seems low. However, for simplicity in this aggressive strategy, we'll initially rely more heavily on the observed cooperators from the prior round.

**Decision Logic:**

*   **If $TAM_t \ge m$**: **Cooperate (C)**. This means the prior round's success, combined with a very conservative estimate of future cooperation (e.g., assuming no new defectors will flip to cooperation), already meets or exceeds the threshold. We cooperate to ensure we get the reward.
*   **If $TAM_t < m$**: **Defect (D)**. This is the dominant strategy. We don't want to be the sole cooperator or one of a few, as that leads to a lower payoff than defecting.

**The "Aggressive Twist" - Leveraging the Reward:**

This strategy is aggressive. It seeks to trigger the reward $k$ as often as possible. Therefore, we *will* cooperate to reach $m$ if there's a *reasonable chance* of success, even if $TAM_t$ is just below $m$.

**Revised Decision Logic (Aggressive):**

1.  **Count Observed Cooperators ($C_{observed, t-1}$) from the previous round ($t-1$).**
2.  **Estimate Potential Future Cooperators ($PFC_t$):** For this strategy, we'll use a simple heuristic: assume that a small fraction of players who defected in the previous round might be persuaded to cooperate if the reward is significant. A more robust approach would involve modeling opponent behavior, but for an aggressive strategy, we keep it straightforward. A simple initial assumption could be: `PFC_t = 0` (i.e., we don't *expect* defectors to flip to cooperate in the absence of other signals).
3.  **Calculate Expected Cooperators ($EC_t$):** $EC_t = C_{observed, t-1} + PFC_t$.
4.  **Decision:**
    *   **If $EC_t \ge m - 1$ (and $m-1 > 0$): Cooperate (C).** This is the aggressive part. We're willing to cooperate if we are only *one cooperator short* of the threshold, assuming that *at least one other player* will also cooperate (either someone who cooperated before or a defecting player who also sees the opportunity). This is a calculated gamble to secure the $k$ reward for everyone, but *we* ensure we get our share.
    *   **If $EC_t < m - 1$ (or if $m-1 \le 0$, meaning $m=1$ which is ruled out by $m>1$): Defect (D).** If we are more than one cooperator away from the threshold, the risk of our cooperation not being sufficient is too high, leading to a payoff of 1 (which is less than what we'd get by defecting if the threshold isn't met, and potentially less than defecting if it *is* met and we were the sole cooperator).

**Self-Interest Dominant Strategy:**

At its core, this strategy is a variant of "Tit-for-Tat" but with an aggressive, self-serving twist. Instead of mirroring, it aims to *force* the system towards the reward state when it's advantageous. It's not about punishing defection; it's about maximizing reward.

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   **Default Action: Defect (D).** In the first round, there is no history. The safest and most aggressive default is to defect and observe. We gain 1, and wait to see what others do. This allows us to gather initial data on player behavior without exposing ourselves to risk.

*   **Last Round (t=r):**
    *   **The "Exploit" Round: Cooperate (C) if $C_{observed, r-1} \ge m - 1$. Otherwise, Defect (D).** In the final round, there is no future to consider. If the previous round's cooperation was sufficient or very close to sufficient ($C_{observed, r-1} \ge m - 1$), it's highly likely that the threshold will be met. In this case, we cooperate to ensure we get the $k$ reward. If the previous round's cooperation was too low, defecting guarantees a payoff of 1, which is the best we can do given the likely failure of the project. There's no incentive to "teach" or "punish" in the last round.

*   **Low $k$ Value:** If $k$ is very low, the incentive to cooperate diminishes. This strategy, by default, prioritizes defecting unless cooperation is almost guaranteed. If $k$ is very close to 1, defecting becomes almost always the optimal strategy, and this strategy will reflect that.

*   **High $m$ Value (Close to $n$):** If $m$ is very high, cooperation becomes more difficult. This strategy will be more hesitant to cooperate, as the chances of reaching $m$ are lower. It will likely default to defecting more often.

### 3. Aggressive Mindset:

*   **"I will not be the sucker."** This strategy avoids being the sole cooperator. It only cooperates if it believes the threshold is likely to be met, and it wants to be part of that success to reap the rewards.
*   **"Maximize my return."** The primary goal is to get the payoff of $1+k$ as often as possible. Defecting is the default because it guarantees a payoff of 1, and we only deviate from that if we can confidently secure the higher payoff.
*   **"Leverage the system."** The strategy actively looks for opportunities to trigger the $k$ reward. It's willing to take a small risk (cooperating when one short of the threshold) to achieve this, but it's not blindly optimistic.
*   **No Forgiveness:** This strategy doesn't inherently forgive past defections. Its decision is based on the immediate likelihood of success, not on reciprocation. If others consistently defect, this strategy will also consistently defect, as the $TAM$ will rarely be high enough to trigger cooperation.

### Pseudocode Representation:

```pseudocode
FUNCTION MakeDecision(player_id, history, game_params):
  n = game_params.n
  r = game_params.r
  m = game_params.m
  k = game_params.k
  current_round = history.current_round

  // Handle Round 1: Default to Defect
  IF current_round == 1 THEN
    RETURN "D"
  END IF

  // Get observed cooperators from the previous round
  observed_cooperators_prev_round = CountCooperators(history.actions_prev_round)

  // Estimate potential future cooperators (simplified for this aggressive strategy)
  // We assume no defectors will spontaneously flip to cooperate without strong signals.
  potential_future_cooperators = 0

  expected_cooperators = observed_cooperators_prev_round + potential_future_cooperators

  // Decision Logic: Cooperate if we are at most 1 short of the threshold
  IF expected_cooperators >= m - 1 AND m > 1 THEN // m > 1 is guaranteed by params
    RETURN "C"
  ELSE
    RETURN "D"
  END IF

// Helper function to count cooperators from a list of actions
FUNCTION CountCooperators(actions_list):
  count = 0
  FOR EACH action IN actions_list:
    IF action == "C" THEN
      count = count + 1
    END IF
  END FOR
  RETURN count
END FUNCTION
```

**Note on `history.actions_prev_round`:** This assumes a history object that stores the actions of all players in previous rounds. For `current_round == 1`, `history.actions_prev_round` would be empty or non-existent, hence the explicit check.

This "Opportunistic Enforcer" strategy aims for a high payoff by being selectively cooperative, only when it's highly probable to achieve the collective reward, thereby maximizing personal gain in a competitive environment.
'''

description_AGGRESSIVE_83 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to be adaptive and exploit opportunities for high individual gain while being resilient to defection.

## Aggressive Strategy: "The Opportunistic Driver"

This strategy prioritizes maximizing individual payoff by defecting when the probability of achieving the cooperation reward is low, and cooperating only when the collective reward is almost guaranteed or when a strong incentive exists to do so. It's designed to push the boundaries of cooperation, forcing others to carry the burden or risk losing out on the reward.

### 1. Decision Rules

The core of this strategy is a conditional approach based on the perceived likelihood of meeting the cooperation threshold `m`.

**General Rule:**

*   **If I believe that cooperation is likely to succeed (i.e., m or more players will cooperate *including myself*), then I Cooperate (C).**
*   **Otherwise, I Defect (D).**

**Quantifying "Likely to Succeed":**

This is where the adaptiveness comes in. We'll use a metric to estimate the *required* number of other players to cooperate for the threshold to be met.

Let `N_C_prev` be the number of players who cooperated in the *previous* round.
Let `n_other` be the number of other players (n - 1).

The target number of *other* players I need to cooperate is `m_required = m - 1`.

Now, we assess the likelihood of these `m_required` other players cooperating based on past behavior. This is the critical, adaptive part.

**Threshold Calculation & Decision Logic:**

*   **Calculate the "Cooperation Pressure" from the previous round:**
    *   `cooperation_pressure = max(0, m - N_C_prev)`
        *   If `N_C_prev >= m`, then `cooperation_pressure = 0` (cooperation was already met).
        *   If `N_C_prev < m`, then `cooperation_pressure` is the number of *additional* cooperators needed from the remaining `n_other` players.

*   **Decision based on "Cooperation Pressure" and history:**

    *   **If `cooperation_pressure == 0` (i.e., `N_C_prev >= m`):** The threshold was met or exceeded in the previous round. This indicates a high likelihood of success. **I Cooperate (C).** This is a pure opportunistic move to get the `k` reward.

    *   **If `cooperation_pressure > 0`:** This is where it gets aggressive. We need to estimate if the `cooperation_pressure` can be met by the other `n_other` players.
        *   **Estimate Others' Cooperation:** We need a mechanism to estimate how many of the *other* `n-1` players are likely to cooperate. A simple but effective approach is to look at their average cooperation rate in recent rounds. However, to be truly aggressive and adaptive, we'll focus on the *most recent successful cooperation rounds* by others.

        *   **Aggressive Threshold Trigger:**
            *   **If `m_required <= 0` (i.e., `m <= 1`, which is not allowed by the rules but for conceptual completeness):** This would mean even one cooperator is enough. But given `m > 1`, this is not relevant.
            *   **If `cooperation_pressure <= n_other`:** We need `cooperation_pressure` *additional* cooperators from the `n_other` players.
                *   **High Confidence Trigger:** If the number of players who cooperated in the *previous* round (`N_C_prev`) is **at least `m - 1`**, it means only *one more* cooperator (myself) is needed. In this scenario, the risk is low for me. **I Cooperate (C).**
                *   **Aggressive Push Trigger:** If the number of players who cooperated in the *previous* round (`N_C_prev`) is **at least `m - k_factor_threshold`**, where `k_factor_threshold` is a small number (e.g., 1 or 2, adjustable based on tournament results), indicating that we are *close* to the threshold. In this scenario, the potential payoff is high. **I Cooperate (C).** This is a calculated risk to push towards success.
                *   **Default to Defect:** If neither of the above conditions is met, meaning there's a significant gap between `N_C_prev` and `m`, and the number of needed cooperators is high, the risk of contributing and not getting the reward is too great. **I Defect (D).**

### 2. Handling Edge Cases

*   **Round 1:**
    *   In the first round, there is no history. The strategy must make an initial aggressive bet.
    *   **Rule:** **Defect (D).**
        *   *Rationale:* This is the safest aggressive stance. By defecting in round 1, I guarantee myself a payoff of 1. I will observe the actions of others to gather information for subsequent rounds. If the tournament shows an extremely high propensity for cooperation in round 1, this can be adjusted.

*   **Last Round (Round `r`):**
    *   The incentive structure changes in the last round because future rewards are not a consideration. There is no "shadow of the future."
    *   **Rule:** **Defect (D).**
        *   *Rationale:* In the last round, the only thing that matters is the immediate payoff. Cooperating means risking a payoff of 0 if the threshold isn't met, while defecting guarantees a payoff of 1. The aggressive strategy maximizes individual gain by securing this guaranteed payoff.

*   **Near-Success Scenarios (e.g., `N_C_prev = m-1`):**
    *   This is a critical point for aggressive players.
    *   **Rule:** **Cooperate (C).**
        *   *Rationale:* If only one more cooperator is needed, and I am that cooperator, my payoff becomes `0 + k`. If I defect, I get `1`, and the reward `k` is not achieved, so others get `1`, and I get `1`. The difference is `k` versus `1`. Since `k > 1`, cooperating is the aggressive, high-reward play.

*   **Slightly Below Threshold Scenarios (e.g., `N_C_prev = m-2` and `m` is large):**
    *   This requires careful consideration. If the previous round was close, but not *that* close, the risk of defecting might be lower.
    *   **Rule:** **Defect (D).**
        *   *Rationale:* The goal is aggression, not blind optimism. If the previous round was significantly short of the threshold, and the number of required additional cooperators is high, it's more likely that others will also defect, leading to a failed cooperation. My payoff of 1 is guaranteed.

### 3. Aggressive Mindset Alignment

This strategy is aggressive because:

1.  **It seeks the highest possible payoff:** The primary driver is to secure the `k` reward. It doesn't shy away from cooperating when the probability of success is high, aiming for the `(0) + k` payoff.
2.  **It leverages others' cooperation:** It implicitly relies on others to bridge the gap to `m` cooperators if the previous round was close. It's willing to contribute the last piece if the puzzle is almost complete.
3.  **It avoids unnecessary risks:** It defects when the likelihood of success is low, guaranteeing a payoff of 1. It prioritizes minimizing individual loss over altruistically attempting cooperation that's unlikely to succeed.
4.  **It exploits the "shadow of the future" when it benefits:** In earlier rounds, the expectation of future interactions might encourage cooperation. This strategy uses that knowledge by cooperating when it's almost guaranteed to pay off, thereby "training" others to cooperate by demonstrating success.
5.  **It is uncompromising in the final round:** It fully embraces the shift to short-term gain, defecting decisively to maximize its final score.
6.  **It is adaptive to observed behavior:** While initial rounds might be conservative (defecting round 1), it quickly learns. If it observes consistent cooperation from others, it will become more willing to cooperate itself to capture the reward.

---

### Pseudocode Representation:

```
Strategy: Opportunistic Driver

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward if threshold met factor

Memory:
  current_round: integer (starts at 1)
  past_actions: list of lists, where past_actions[t][i] is player i's action in round t. (Initially empty)
  num_cooperators_prev_round: integer (0 if current_round == 1)

Decision Logic:

FUNCTION DecideAction():

  # Handle Edge Cases
  IF current_round == 1:
    RETURN D  # Defect in the first round to observe

  IF current_round == r:
    RETURN D  # Defect in the last round to secure payoff

  # Calculate based on previous round
  N_C_prev = num_cooperators_prev_round  # Number of players who cooperated last round

  # If threshold was met or exceeded, capitalize on it
  IF N_C_prev >= m:
    RETURN C  # Cooperate to get the reward

  # Calculate the number of additional cooperators needed
  cooperation_pressure = max(0, m - N_C_prev)
  n_other = n - 1

  # Aggressive Push Trigger: If we were very close last round
  # This threshold can be tuned. Let's say we consider it "close" if we needed
  # only 1 or 2 more cooperators.
  IF cooperation_pressure <= 2 AND cooperation_pressure > 0:
    # If we are only 1 cooperator short, it's a sure bet for me
    IF cooperation_pressure == 1:
      RETURN C  # My cooperation guarantees success

    # If we are 2 cooperators short, it's still a strong incentive to try
    # but slightly riskier. If the tournament shows a tendency for near-misses to resolve,
    # we might cooperate here. Let's make this aggressive:
    RETURN C  # Aggressive push to reach threshold

  # Default to Defect if the gap is too large and success is unlikely
  RETURN D

# End of Function

# In the tournament loop:
# For each round:
#   player_action = DecideAction()
#   ... record past_actions ...
#   ... update num_cooperators_prev_round based on observed outcomes ...
#   current_round += 1

```

This "Opportunistic Driver" strategy aims to be a highly effective player in the tournament by being a calculating risk-taker, always prioritizing maximizing its own outcome, and adaptively responding to the dynamics of the game and the behavior of its opponents.
'''

description_AGGRESSIVE_84 = '''
This strategy, "Aggressive Opportunist," is designed to maximize individual gain in the Collective Risk Dilemma by leveraging aggression and opportunism within the constraints of the game. It prioritizes personal gain while seeking to benefit from collective success when it aligns with individual interests.

## Aggressive Opportunist Strategy

**Core Philosophy:** Exploit the "defect" payoff as much as possible while capitalizing on the collective reward when it's a sure thing or the only path to a significant gain. Never be the sole contributor to a failed cooperative effort.

### 1. Decision Rules

My decision in round `t` for player `i` depends on the history of play and the game parameters:

*   **Early Rounds (t < r/2): Calculated Aggression**
    *   **If the number of cooperators in the *previous* round (t-1) was `m-1`:** Play `C`. This is a calculated risk. We are one step away from the reward, and contributing one more time might tip the scales and secure the collective bonus for everyone. This is a short-term investment in potential greater returns.
    *   **If the number of cooperators in the *previous* round (t-1) was `m` or more:** Play `D`. The threshold has been met or exceeded. There's no need for me to contribute. My keeping endowment (1) is guaranteed, and the reward (k) will be distributed regardless of my action. My personal payoff will be higher by defecting.
    *   **Otherwise (fewer than `m-1` cooperators in the previous round):** Play `D`. The collective is not showing promise. It's too risky to contribute and potentially fail to meet the threshold. I will secure my own endowment and wait for a clearer signal.

*   **Mid-to-Late Rounds (r/2 ≤ t < r): Increased Opportunism**
    *   **If the number of cooperators in the *previous* round (t-1) was `m-1` AND there are only 2 rounds left (t == r-1):** Play `C`. This is the absolute last chance to trigger the reward. If we are this close, it's worth the gamble to attempt to secure the bonus for both rounds.
    *   **If the number of cooperators in the *previous* round (t-1) was `m` or more:** Play `D`. The reward has been secured or is very likely to be. Maximize my personal gain by defecting.
    *   **If the number of cooperators in the *previous* round (t-1) was `m-1` (and not the last round):** Play `D`. The risk of cooperating alone and failing is too high at this stage. I will wait for others to make the final move. The history suggests a close call, but the aggressive opportunist doesn't want to be the "sucker" if it fails.
    *   **Otherwise (fewer than `m-1` cooperators):** Play `D`. The collective effort is clearly failing. Secure my own endowment.

*   **Final Round (t = r): Pure Self-Interest**
    *   **Play `D`**. In the final round, there are no future rounds to benefit from. My only goal is to maximize my immediate payoff. Cooperating would be donating my endowment for no future return.

### 2. Handling Edge Cases

*   **First Round (t = 1): Initial Aggression**
    *   **Play `D`**. In the absence of any history, the most aggressive and safest move is to defect. This sets the tone that I am not to be taken advantage of and will not be the first to incur a potential loss. This also preserves my endowment to observe the actions of others.

*   **Last Round (t = r): Decisive Self-Interest**
    *   As described above, always play `D` in the final round to secure the maximum individual payoff.

*   **The "Almost There" Scenario (m-1 cooperators historically):** This is the most nuanced. My strategy is to only contribute if it's the *very last* opportunity to achieve the threshold or if it's in the early-to-mid game where a small risk might be worth the potential larger reward. Otherwise, I let others take the risk.

*   **"Sure Thing" Scenario (m or more cooperators historically):** If the threshold has already been met or exceeded, I will always defect. This is pure opportunism. The collective reward is guaranteed, and by defecting, I increase my personal payoff by 1 (my endowment).

### 3. Aggressive Mindset Alignment

This strategy embodies aggression by:

*   **Defaulting to Defection:** The initial action and the general rule in most scenarios is to defect, prioritizing personal gain.
*   **Calculated Risk-Taking:** Cooperating only when extremely close to the threshold (`m-1` cooperators) or in specific late-game scenarios is a calculated risk designed to leverage the potential for a high reward. It's not blind cooperation.
*   **Opportunistic Exploitation:** Always defecting when the reward is guaranteed is the epitome of opportunism, maximizing personal benefit from the collective's success.
*   **No Benevolence:** There is no altruism. The strategy is purely about maximizing my own score. I do not try to build trust or facilitate cooperation for its own sake.
*   **Reacting to Weakness:** If the collective isn't close to `m` cooperators, I defect. This signals that I won't prop up a failing endeavor.

---

**Pseudocode Representation:**

```
function AggressiveOpportunist(n, r, m, k, current_round, history):
    // history is a list of lists, where history[t][i] is player i's action in round t (1 for C, 0 for D)
    // We need to track the number of cooperators in the previous round.
    
    if current_round == 1:
        return D  // Initial aggression: defect

    previous_round_actions = history[current_round - 1]
    num_cooperators_previous_round = count(action == C for action in previous_round_actions)

    if current_round == r:
        return D // Final round: pure self-interest

    // Early to Mid-Game Rules
    if current_round < r / 2:
        if num_cooperators_previous_round == m - 1:
            return C // Calculated risk: one step away
        elif num_cooperators_previous_round >= m:
            return D // Sure thing: defect for max personal gain
        else:
            return D // Collective is not showing promise: defect

    // Late-Game Rules (r/2 <= current_round < r)
    else:
        if num_cooperators_previous_round == m - 1 and current_round == r - 1:
            return C // Last chance to trigger reward in the last round
        elif num_cooperators_previous_round >= m:
            return D // Sure thing: defect for max personal gain
        elif num_cooperators_previous_round == m - 1:
            return D // Too close to the end to take the risk alone
        else:
            return D // Collective is clearly failing: defect

```
'''

description_AGGRESSIVE_85 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Opportunistic Ascendancy"

This strategy aims to maximize individual payoff by aggressively pursuing the higher reward ($k$) while minimizing risk. It leverages the repeated nature of the game to adapt to player behavior, but always prioritizes self-gain and dominance. It's an aggressive strategy in that it aims to trigger the reward as quickly and consistently as possible, pushing the boundaries of cooperation when it's most advantageous.

**Core Philosophy:** "I will contribute to success if and only if it is demonstrably beneficial to me. If others fail to contribute sufficiently, I will capitalize on their inaction. If they succeed, I will join them and reap the rewards. I will never be the sole reason for failure, nor will I shy away from prosperity."

---

### 1. Decision Rules: When to Cooperate (C) vs. Defect (D)

Our decision-making process is primarily driven by two factors: the observed cooperation level of others and the proximity to the winning threshold ($m$).

**Key Variables:**

*   `round`: The current round number (1 to r).
*   `cooperators_in_previous_round`: The number of players (excluding self) who cooperated in the immediately preceding round.
*   `my_action_in_previous_round`: My own action in the immediately preceding round.
*   `total_cooperators_in_history`: A running count of the total number of players (excluding self) who have cooperated across all previous rounds. This is a more robust indicator than just the previous round.
*   `proportion_of_cooperators_in_history`: `total_cooperators_in_history` / (n-1) / (round - 1)

**Decision Logic:**

*   **Default Action: Defect (D)**
    *   This is our safest and most consistently profitable action when uncertainty is high or cooperation is low.

*   **Cooperate (C) Conditions:**
    1.  **Nearing the Threshold (Strong Indicator):**
        *   If `cooperators_in_previous_round` >= `m` - 2:
            *   **AND** `round` < `r` (not the last round).
            *   This means we are very close to triggering the reward in the next round if we cooperate. The potential gain from `k` outweighs the risk of losing the endowment if others defect. We aggressively push for the reward.
    2.  **Sustained Cooperation Trend (Adaptive Convergence):**
        *   If `round` > (r / 4) (i.e., we are past the initial exploration phase)
            *   **AND** `proportion_of_cooperators_in_history` > 0.7 (meaning a significant majority of other players have cooperated on average over time)
            *   **AND** `cooperators_in_previous_round` >= `m` - 1:
                *   This signifies a stable, cooperative environment where the reward is frequently achieved. We join the dominant trend to secure our share.
    3.  **"Baiting" the Trap (Aggressive Opportunism):**
        *   If `cooperators_in_previous_round` >= `m` - 1:
            *   **AND** `round` < `r` (not the last round).
            *   We cooperate if enough others have cooperated to make our cooperation *crucial* or *highly likely* to push the total above `m`. This is a calculated gamble to secure the reward.

*   **Otherwise, Defect (D)**

---

### 2. Handling Edge Cases

*   **Round 1 (Exploration and Caution):**
    *   **Decision:** Always **Defect (D)**.
    *   **Rationale:** In the first round, there is no history. We have no information about opponent behavior. Defecting is the risk-free option, preserving our endowment and allowing us to gather initial data without incurring losses. This is the "observe and learn" phase.

*   **Last Round (r) (Maximizing Final Gain):**
    *   **Decision:** Cooperate (C) if `cooperators_in_previous_round` >= `m` - 1. Otherwise, Defect (D).
    *   **Rationale:** In the final round, the future is irrelevant. We want to maximize our payoff from this single round. If there's a high probability of reaching the threshold based on previous behavior, we take the gamble to secure the `k` reward. If not, we stick with the guaranteed private payoff. This is about maximizing the final score, not about long-term strategy.

---

### 3. Aggressiveness Alignment

Our strategy is inherently aggressive in the following ways:

*   **Proactive Threshold Pursuit:** We don't passively wait for `m` cooperators. We actively contribute when we are *close* to the threshold (`m-2` or `m-1`) and the reward is within reach, especially in earlier rounds. This is a bold move to establish a winning precedent and secure early gains.
*   **Exploiting Dominant Trends:** When a majority of players show a pattern of cooperation, we quickly join them to capitalize on their collective effort. We don't hold back out of suspicion if the trend is clearly beneficial and sustainable.
*   **"Baiting" the Trap:** The condition of cooperating when `cooperators_in_previous_round` >= `m` - 1, especially before the last round, is a clear signal of aggressive intent. We are willing to put our endowment at risk if it means almost guaranteeing the higher payoff.
*   **No Forgiveness/Punishment Loop:** Unlike tit-for-tat, this strategy doesn't try to punish defectors by withholding cooperation indefinitely. It focuses on seizing opportunities for gain. If defectors lead to a failure to meet the threshold, we simply revert to defecting and wait for a better opportunity. This avoids cycles of mutual defection that can be less profitable.
*   **Focus on Individual Gain:** Every decision is weighted towards maximizing the player's own payoff. We do not engage in altruistic cooperation. Cooperation is a means to an end: maximizing personal reward.

---

### Pseudocode Representation:

```pseudocode
// Initialize state variables before the tournament
total_cooperators_in_history = 0
my_actions_history = [] // List of actions (C/D) for each previous round

function decide_action(game_parameters, round_history):
    n = game_parameters.n
    m = game_parameters.m
    r = game_parameters.r
    k = game_parameters.k

    // --- Edge Case: Round 1 ---
    if round == 1:
        return "D" // Always defect in the first round

    // --- Calculate history-based variables ---
    cooperators_in_previous_round = 0
    if length(round_history) > 0:
        // Count cooperators from the previous round's observed actions (excluding self)
        last_round_actions = round_history[-1].actions_by_player
        for player_idx in 1 to n:
            if player_idx != self_idx and last_round_actions[player_idx] == "C":
                cooperators_in_previous_round += 1

    // Update total_cooperators_in_history based on the previous round
    // (This is a simplified approach; a more robust implementation might
    // store cumulative counts per round rather than recalculating)
    // For simplicity, assume we have a way to get cumulative cooperators correctly
    // from round_history. Let's assume total_cooperators_in_history is updated externally
    // after each round based on successful cooperation count.

    // Calculate proportion of cooperators for sustained trend
    // Note: This is a simplification. A true 'proportion_of_cooperators_in_history'
    // would be calculated as SUM(cooperators in round t for t=1..round-1) / ((n-1)*(round-1))
    // For this pseudocode, we'll use a simpler proxy for illustration:
    // average cooperators per round observed so far.
    average_cooperators_per_round = 0
    if round > 1:
        total_coop_sum_all_rounds = 0
        for past_round_actions in round_history:
            for player_idx in 1 to n:
                if player_idx != self_idx and past_round_actions.actions_by_player[player_idx] == "C":
                    total_coop_sum_all_rounds += 1
        average_cooperators_per_round = total_coop_sum_all_rounds / (n - 1) / (round - 1)

    // --- Decision Logic ---

    // Condition 1: Nearing the Threshold (Strong Indicator)
    if (cooperators_in_previous_round >= m - 2) and (round < r):
        return "C"

    // Condition 3: "Baiting" the Trap (Aggressive Opportunism)
    if (cooperators_in_previous_round >= m - 1) and (round < r):
        return "C"

    // Condition 2: Sustained Cooperation Trend (Adaptive Convergence)
    if (round > r / 4) and (average_cooperators_per_round > 0.7) and (cooperators_in_previous_round >= m - 1):
        return "C"

    // --- Edge Case: Last Round (r) ---
    if round == r:
        if cooperators_in_previous_round >= m - 1:
            return "C"
        else:
            return "D"

    // --- Default Action ---
    return "D"

// --- End of Pseudocode ---
```
'''

description_AGGRESSIVE_86 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the cooperative potential of the group when advantageous. The core principle is to be a "opportunistic cooperator" – defecting when it benefits you most, and cooperating only when it secures a guaranteed win or prevents a catastrophic loss.

## Aggressive Strategy: "The Opportunistic Maverick"

**Mindset:** My primary goal is to maximize my own payoff in every round. I am not inherently cooperative; my cooperation is purely instrumental. I will defect by default, as it offers a guaranteed payoff of 1. I will only cooperate if it guarantees a superior outcome for me.

### 1. Decision Rules: When to Cooperate vs. Defect

**Core Rule:** I will **defect (D)** in a round unless specific conditions for cooperation are met that guarantee me a higher payoff than defecting.

**Conditions for Cooperation (C):**

I will cooperate only if **at least one** of the following conditions is met:

*   **Condition 1: Guaranteed Win (High Confidence Cooperation):**
    *   The current round `t` is such that `t < r` (not the last round).
    *   The number of players who have cooperated in the *previous* round (`t-1`) is `m-1`.
    *   The number of players who are *currently* cooperating in the current round (`t`) (based on observations from previous rounds, or if this is the first round where cooperation is being considered) is `m-2`.
    *   **Rationale:** This situation indicates that exactly one more player is needed to trigger the reward. By cooperating, I can secure the reward `k` and guarantee myself a payoff of `(1-1) + k = k`. Since `k > 1`, this is a guaranteed profit. If I defect, and others cooperate to reach `m`, I would get `1 + k`. If they don't, I get `1`. The guarantee of `k` is superior to the potential `1+k` or the certain `1`.

*   **Condition 2: Deterring Collective Failure (Preventative Cooperation):**
    *   The current round `t` is such that `t < r` (not the last round).
    *   The number of players who have cooperated in the *previous* round (`t-1`) is `m-1`.
    *   The number of players who are *currently* cooperating in the current round (`t`) (based on observations) is `m-2`.
    *   **Rationale:** This is the same condition as Condition 1. It's framed differently to emphasize the risk aversion aspect of cooperation. It's a preemptive move to avoid a situation where insufficient cooperation leads to a suboptimal outcome for everyone, including myself.

*   **Condition 3: Last Round's Last Chance (Strategic Leap):**
    *   The current round `t` is the **last round (`t = r`)**.
    *   The number of players who cooperated in the *previous* round (`t-1`) was **exactly `m-1`**.
    *   **Rationale:** In the final round, there's no future to leverage. If we're on the cusp of success (m-1 cooperators previously), it's a calculated risk. By cooperating, I aim to pull the group over the threshold. If I succeed, I get `(1-1) + k = k`. If I fail (not enough others cooperate), I get `(1-1) = 0`. However, if I had defected, and others still cooperated to reach `m`, I would get `1 + k`. If they defected and we failed, I get `1`. This condition is the *most aggressive* and thus the most carefully controlled. It's a gamble to secure the reward in the final moment. This is only triggered if we were *just short* of success in the previous round.

**When to Defect (D):**

I will **defect** in all other scenarios. This includes:

*   If it's the first round.
*   If the number of previous cooperators is significantly less than `m-1`.
*   If the number of previous cooperators is `m` or more (already guaranteed success, so I can free-ride).
*   If it's the last round and the previous round did not have `m-1` cooperators.
*   If any of the cooperation conditions are not strictly met.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** I will **defect (D)**.
    *   **Rationale:** There is no history to analyze. Defecting guarantees a payoff of 1, which is the safest and most aggressive default in the absence of information. The reward `k` is unattainable without knowing if others will cooperate.

*   **Last Round (t=r):**
    *   **Decision:** I will cooperate only if **Condition 3 (Last Round's Last Chance)** is met. Otherwise, I will defect.
    *   **Rationale:** In the final round, there is no future interaction to punish or reward. The goal is to maximize payoff from this round alone. If `m-1` cooperated previously, the situation is ripe for a calculated, aggressive push for the reward. Otherwise, defecting is the most conservative and assured way to get a payoff of 1.

*   **Near Misses (m-1 cooperators in the previous round):**
    *   As described in **Condition 1 and 2**, I will cooperate if the number of cooperators in the previous round was `m-1` and the current observed cooperators are `m-2`. This is the critical threshold where my cooperation can tip the scales.

*   **Over-shooting (m or more cooperators in the previous round):**
    *   **Decision:** I will **defect (D)**.
    *   **Rationale:** If the reward is already guaranteed (or has been guaranteed in the past), I can free-ride. My cooperation would yield `(1-1) + k = k`, while defecting yields `(1-0) + k = 1 + k`. Free-riding is the aggressive, optimal choice here.

### 3. Aggressive Mindset Alignment

*   **Default to Defect:** The strategy is fundamentally built on the assumption that defecting is the default, profitable action. Cooperation is an exception, only occurring when it's a guaranteed or strategically vital move.
*   **Calculated Risk-Taking:** Condition 3 for the last round is a pure aggressive gamble, designed to snatch victory from the jaws of potential mediocrity.
*   **Exploiting Near-Success:** The conditions for cooperation are specifically designed to leverage situations where the group is on the brink of success, allowing me to unilaterally secure the reward.
*   **Opportunistic Free-Riding:** The strategy explicitly identifies and exploits situations where cooperation is no longer necessary for me to gain the reward, allowing me to keep my endowment.
*   **No Sentimental Cooperation:** There's no inclination to "help" the group or foster trust. Every cooperative act is a calculated investment for a guaranteed return.

### Pseudocode Representation

```pseudocode
strategy_name = "Opportunistic Maverick"

function decide_action(current_round, total_rounds, min_cooperators, reward_factor, history):
    // history is a list of dictionaries, each with 'actions' (list of player actions) for a round

    // Default to Defect
    action = "D"

    // ----- Edge Case: First Round -----
    if current_round == 1:
        return "D"

    // ----- Get data from previous round -----
    previous_round_data = history[-1] // Assuming history is ordered chronologically
    previous_round_cooperators = count_cooperators(previous_round_data.actions)

    // ----- Determine current observed cooperators for this round (if available in history) -----
    // This part is tricky with simultaneous moves. We assume we observe actions from prior rounds.
    // If it's not the very first round, and we are in round 't', we would have history up to 't-1'.
    // For the current round 't', we don't know what others will do until after they decide.
    // So, the decision rules must be based on past observations or general probabilities.
    // For this strategy, we'll assume we can use 'previous_round_cooperators' for condition checks.

    // ----- Cooperation Condition 1 & 2: Guaranteed Win / Deterring Failure -----
    // We check if cooperation NOW would GUARANTEE us the reward,
    // based on the fact that we were m-1 cooperators in the last round.
    // This implies that if we cooperate now, we ARE one of the cooperators needed.
    if current_round < total_rounds:
        if previous_round_cooperators == min_cooperators - 1:
            // If we cooperate, we are m cooperators.
            // Payoff if we cooperate: (1-1) + k = k
            // Payoff if we defect and others cooperate to m: (1-0) + k = 1 + k (better, but not guaranteed)
            // Payoff if we defect and others fail: 1 (worse than k if k>1)
            // This is the calculation: if we join, does it GUARANTEE us k? Yes.
            // Is k > 1? Yes, k > 1.
            // So, if we are on the cusp, cooperating is the only way to GUARANTEE k.
            action = "C"

    // ----- Cooperation Condition 3: Last Round's Last Chance -----
    if current_round == total_rounds:
        if previous_round_cooperators == min_cooperators - 1:
            // This is a gamble to secure the reward in the last round.
            // If we cooperate, we might get k (if we tip the balance)
            // If we defect, we get 1.
            // This is the risk: If we cooperate and still fail, we get 0.
            // This is AGGRESSIVE.
            action = "C"

    // ----- Over-riding Defection: Free-Riding -----
    // If the previous round ALREADY met the threshold, we can defect and free-ride.
    if previous_round_cooperators >= min_cooperators:
        action = "D" // Free-riding is always aggressive and optimal

    return action

function count_cooperators(actions_list):
    count = 0
    for player_action in actions_list:
        if player_action == "C":
            count += 1
    return count

```
'''

description_AGGRESSIVE_87 = '''
This strategy, "The Opportunistic Enforcer," is designed for aggressive dominance in the Collective Risk Dilemma. It prioritizes maximizing its own payoff while leveraging the game's structure to punish defectors and reward cooperation when strategically advantageous. It's designed to be adaptive and exploit deviations from ideal cooperation without being overly reliant on it.

## The Opportunistic Enforcer Strategy

**Core Philosophy:** Exploit cooperation when it's safe and beneficial. Punish defection with immediate and severe retaliation. Be the catalyst for risk aversion only when the rewards are guaranteed and substantial.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect is driven by a calculated assessment of the current round's prospects, the past behavior of other players, and the remaining rounds in the game.

**A. Cooperation Condition (Play C):**

*   **Guaranteed Reward:** Cooperate *only if* your current assessment indicates that **at least `m` players (including yourself) are highly likely to cooperate in this round**. This "high likelihood" is determined by a combination of historical cooperation rates of other players and the number of rounds remaining.
    *   **Threshold Check:** In round `t`, if the number of players who have cooperated in the *previous* round (`t-1`) plus a buffer (representing anticipated future cooperation) is greater than or equal to `m`, then cooperate.
    *   **Buffer Calculation:** The buffer should be dynamic. A higher buffer is needed in earlier rounds, and it shrinks as the game progresses. For example, a simple buffer could be `max(0, n - (number of players who cooperated in t-1)) * (remaining_rounds / total_rounds)`. This ensures you are not the sole cooperator if others have a history of defection.
    *   **Self-Enforcement:** If you yourself are the `m`-th potential cooperator (i.e., `m-1` other players are predicted to cooperate), you will cooperate to trigger the reward.

**B. Defection Condition (Play D):**

*   **All Other Times:** If the conditions for cooperation are not met, **defect**. This is the default and aggressive stance.
    *   **The "Wait and See" Advantage:** If the number of likely cooperators is less than `m`, defect. You gain the private endowment of 1, and if the threshold isn't met, no one gets the higher reward anyway. This avoids the risk of being a lone cooperator and getting only 0.
    *   **Punishment:** If other players have a history of defection or have recently defected, you will defect to punish them and discourage future defection.

### 2. Handling Edge Cases: First Round, Last Round, and Dynamic Adjustment

**A. First Round (t=1):**

*   **Defect (Aggressive Default):** In the first round, there is no history. The Opportunistic Enforcer assumes the worst and defects. This establishes a position of strength and avoids being exploited by early defectors. The goal is to observe and react.

**B. Last Round (t=r):**

*   **Defect (Maximizing Certainty):** In the final round, there is no future to influence. The Opportunistic Enforcer will defect to secure its private endowment of 1. There is no benefit to risking cooperation when the game ends immediately after.

**C. Intermediate Rounds (1 < t < r):**

*   **Adaptive Decision Making:** This is where the core logic of the strategy shines.
    *   **History Tracking:** Maintain a record of each player's past actions (C/D) for every previous round.
    *   **Cooperation Score:** For each opponent, calculate a "cooperation score." This could be a simple percentage of past cooperations, or a weighted average where recent cooperations have more influence.
    *   **Threshold Assessment:**
        *   **Calculate "Likely Cooperators":** Based on the cooperation scores of other players, estimate how many are likely to cooperate.
        *   **Apply the Cooperation Condition:** If `(number of likely cooperators) + 1` (for yourself) `>= m`, then cooperate.
        *   **Dynamic Buffer Adjustment:** The buffer for anticipating cooperation should decrease as `t` approaches `r`. This makes you more demanding of cooperation in later rounds. For example, the buffer could be `n - number of players who *always* defected in the past`. This punishes consistent defectors.

**D. Player-Specific Aggression:**

*   **Targeted Punishment:** If a specific player consistently defects and prevents the threshold from being met, the Opportunistic Enforcer will *never* cooperate if that player is among the `m-1` potential cooperators. This is a direct and aggressive response.
*   **Rewarding Consistent Cooperators (Conditional):** While the strategy is aggressive, it's not entirely unyielding. If a player has a very high cooperation score (e.g., > 90%) and their cooperation would be instrumental in reaching the `m` threshold, the Opportunistic Enforcer *might* consider cooperating to secure the reward, *provided* the risk is minimal (i.e., the `m` threshold is almost certainly met). This is a calculated risk for a guaranteed gain.

### 3. Aggressive Mindset Alignment

*   **Default to Defection:** The strategy's natural inclination is to defect. Cooperation is a calculated decision, not a charitable act.
*   **Punishment is Swift and Severe:** Any deviation from cooperation (especially if it causes the threshold to be missed) will be met with immediate defection in subsequent rounds. This creates a powerful deterrent.
*   **Exploitation of Weakness:** The strategy actively seeks opportunities to benefit from others' cooperation while minimizing its own risk.
*   **No Forgiveness:** The strategy doesn't "forgive" past defections. A history of defection increases the likelihood of future defection from the Opportunistic Enforcer.
*   **Self-Reliance:** The strategy does not rely on others to initiate cooperation. It will only cooperate if it can confidently predict the outcome or if its own cooperation is the final piece needed for a guaranteed reward.

---

### Pseudocode Representation:

```python
class OpportunisticEnforcer:
    def __init__(self, n, m, k, r):
        self.n = n
        self.m = m
        self.k = k
        self.r = r
        self.player_actions_history = {} # {player_index: [action_round1, action_round2, ...]}
        self.my_actions_history = [] # [action_round1, action_round2, ...]
        self.current_round = 0

    def decide_action(self, opponent_actions_this_round):
        self.current_round += 1

        # Update history with previous round's results
        if self.current_round > 1:
            for i in range(self.n):
                if i not in self.player_actions_history:
                    self.player_actions_history[i] = []
                self.player_actions_history[i].append(opponent_actions_this_round.get(i, 'D')) # Default to D if observation missing
            self.my_actions_history.append(self.my_last_action) # Store my action from previous round

        # Edge Case: Last Round
        if self.current_round == self.r:
            self.my_last_action = 'D'
            return 'D'

        # Edge Case: First Round
        if self.current_round == 1:
            self.my_last_action = 'D'
            return 'D'

        # --- Intermediate Rounds Logic ---

        # Calculate cooperation scores for opponents
        cooperation_scores = {}
        for i in range(self.n):
            if i in self.player_actions_history:
                cooperations = self.player_actions_history[i].count('C')
                total_past_rounds = len(self.player_actions_history[i])
                cooperation_scores[i] = cooperations / total_past_rounds if total_past_rounds > 0 else 0
            else:
                cooperation_scores[i] = 0 # Assume no cooperation if no history

        # Estimate likely cooperators
        likely_cooperators_count = 0
        for i in range(self.n):
            if i not in self.player_actions_history: # Players without history are treated cautiously
                continue
            # Aggressive threshold: Only consider players with a history of significant cooperation
            # This threshold can be tuned. Here, we are conservative.
            if cooperation_scores[i] >= (self.m / self.n) * 0.8: # A heuristic for 'likely'
                likely_cooperators_count += 1

        # Dynamic buffer (example, can be refined)
        remaining_rounds_ratio = (self.r - self.current_round) / self.r
        buffer_needed = max(0, self.n - (len([a for a in self.my_actions_history if a == 'C']) + likely_cooperators_count)) * remaining_rounds_ratio
        predicted_cooperators = likely_cooperators_count + buffer_needed # This is a simplified prediction

        # Decision rule
        if predicted_cooperators + 1 >= self.m:
            # Check if my cooperation is crucial or if others are highly reliable
            # This is a sophisticated check: If m-1 others are very likely, and I am the m-th.
            # Or if a significant portion are already cooperating.
            # For aggressive strategy, we'll only cooperate if we are sure or very close to sure.
            
            # Refined cooperation condition:
            # Count players with > X% cooperation score
            high_cooperators = 0
            for score in cooperation_scores.values():
                if score > 0.8: # Tune this threshold
                    high_cooperators += 1
            
            # If enough high cooperators exist OR if my cooperation directly triggers the reward
            if high_cooperators >= self.m - 1 or (high_cooperators + 1 >= self.m):
                self.my_last_action = 'C'
                return 'C'
            else:
                # Not enough certainty, defect to protect endowment
                self.my_last_action = 'D'
                return 'D'
        else:
            # Conditions for cooperation not met, defect
            self.my_last_action = 'D'
            return 'D'

    def reset(self):
        self.player_actions_history = {}
        self.my_actions_history = []
        self.current_round = 0

```
'''

description_AGGRESSIVE_88 = '''
This strategy is designed for aggressive play in the Collective Risk Dilemma. The core principle is to leverage the risk-reward dynamic by defecting opportunistically while ensuring that the collective goal is *just barely* met by others. It aims to maximize individual payoff by extracting the "private payoff from keeping" (1) as much as possible, and only contributing to the community project when the reward (k) is guaranteed and highly beneficial.

## Aggressive Strategy: "The Leveraged Opportunist"

The Leveraged Opportunist seeks to be a net beneficiary of the collective effort. It defects when it can, and cooperates only when it's strategically advantageous and its contribution is essential to unlocking a higher payoff for itself, ideally with minimal risk of being the sole contributor.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in round `t` for player `i` is determined by a set of conditions that prioritize personal gain while being adaptable to the historical unfolding of the game.

**Core Logic:**

*   **Default to Defect (D):** The baseline action is to defect. This maximizes the immediate private payoff of 1.
*   **Cooperate if Defecting Leads to Ruin (and Cooperation is Certain):** Cooperate only if it's the *only* way to avoid a severe personal loss, and even then, only if you are confident that your cooperation will push the group *over* the threshold `m` without you having to bear the sole burden.
*   **Cooperate if Defecting is Suboptimal and Cooperation is Guaranteed:** Cooperate if, based on observed behavior, the threshold `m` is almost certain to be met *even with your defection*, but your defection would then deny you the `k` reward. In this specific scenario, cooperating is a net gain.

**Specific Conditions for Cooperating (C):**

1.  **"Threshold Guarantee" Condition:**
    *   **Condition:** If `(number of observed cooperators in previous rounds) + 1 >= m` (i.e., if you were to defect, there are *still* enough known past cooperators or players exhibiting cooperative tendencies to reach `m`), AND `(number of observed cooperators in previous rounds) < m` (i.e., the threshold has *not yet* been met without your contribution, but it will be met *if you cooperate*).
    *   **Rationale:** This is the most critical condition for cooperation. You are essentially saying, "Others are already on the verge of succeeding. If I defect, we fail. If I cooperate, we succeed, and I get the reward `k`." This ensures you get your private payoff of `1` *plus* the reward `k`, making your payoff `1 - 1 + k = k`. Your total payoff for the round will be `k` (since you cooperated). If you defected, your payoff would be `1` (since `m` wouldn't be met). Thus, `k` is a net gain compared to defecting.

2.  **"Near-Certain Collective Success" Condition (for optimal payoff):**
    *   **Condition:** If `(number of observed cooperators in previous rounds) >= m` (the threshold has already been met by others in previous rounds) AND `(number of players who have defected in the *last* observed round) < (n - m + 1)`.
    *   **Rationale:** This is a more advanced opportunistic cooperation. If the group has a strong history of cooperation and the risk of failure is low, you defect. However, if you observe that a large number of players defected in the last round, and this leaves the next round's cooperative success *uncertain* for you (meaning your defection might lead to everyone getting only 1), you might choose to cooperate. This is about ensuring the high payoff `k` is not jeopardized by your own defection when others are already pushing towards it.

**Specific Conditions for Defecting (D):**

1.  **Default Behavior:** If none of the cooperation conditions are met, defect. This is the primary strategy to maximize personal gain.
2.  **"High Cooperation, Low Risk" Defection:** If `(number of observed cooperators in previous rounds) >= m - 1` (even if you defect, the threshold is still likely to be met by others).
    *   **Rationale:** You are confident that the group will reach the threshold `m` *without your help*. By defecting, you secure your private payoff of `1` and *still* receive the `k` reward because `m` was met by others. This is the ideal opportunistic scenario: get `1 + k` by contributing nothing.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** **Always Defect (D).**
    *   **Rationale:** With no history, there is no information to trigger any cooperation condition. Aggression dictates defaulting to the action that guarantees the private payoff of 1. You cannot guarantee the threshold will be met, so you don't risk your endowment.

*   **Last Round (t=r):**
    *   **Decision:** **Always Defect (D).**
    *   **Rationale:** In the final round, there are no future consequences for defection. The focus is solely on maximizing the payoff for *this* round. The Leveraged Opportunist will always choose to keep its endowment and not risk it, as there's no incentive to build future trust or cooperate for future rewards.

*   **"Free Rider" Dominance (Many players cooperate, you defect):**
    *   **Decision:** Continue to Defect (D) as long as your defection doesn't *prevent* the threshold from being met and you still get the `k` reward. If your defection *would* prevent the threshold, then switch to cooperating based on Condition 1 for cooperation.
    *   **Rationale:** The strategy is designed to exploit this situation. If others are cooperating enough, you get `1 + k` for free.

*   **"Tragedy" Scenario (Few players cooperate):**
    *   **Decision:** Continue to Defect (D).
    *   **Rationale:** If cooperation is insufficient, there's no reward `k`. You get your private payoff of `1`. Cooperating would yield `0` (if you cooperate and `m` is not met) or `0` (if you cooperate and `m` is met by others, but this scenario is unlikely if few are cooperating).

### 3. Aggressive Mindset

The "Leveraged Opportunist" is aggressive by:

*   **Defaulting to Defection:** It starts by assuming the best personal outcome (keeping the endowment) and only deviates when strategically necessary and highly rewarding.
*   **Exploiting Collective Effort:** It aims to benefit from the cooperation of others without necessarily contributing itself, by defecting when the threshold is met by others.
*   **Calculated Risk-Taking:** It only cooperates when it can guarantee a return of `k` (private payoff + reward) and the risk of its contribution being wasted is minimal or non-existent.
*   **Prioritizing Self-Interest:** The strategy explicitly avoids altruism or building long-term cooperative relationships, focusing solely on maximizing its own payoff in each round and across the entire game.

---

### Pseudocode (Conceptual)

```
// Parameters: n, r, m, k
// History:
//   - past_actions: List of lists, where past_actions[t-1][i] is player i's action in round t-1
//   - observed_cooperators_count[t-1]: Number of players who played C in round t-1

function decide_action(current_round_t, history):
    // Handle Edge Cases
    if current_round_t == 1:
        return "D"  // Always defect in the first round
    if current_round_t == r:
        return "D"  // Always defect in the last round

    // Calculate key historical metrics
    num_prev_cooperators = count_cooperators_in_round(history.past_actions[current_round_t - 2]) // Count from round t-2
    
    // More precise: Track players who *consistently* cooperate vs. defect.
    // For simplicity in this pseudocode, we'll focus on recent round counts.

    // ---- Decision Logic ----

    // Condition 1: "Threshold Guarantee"
    // If cooperating will push us over m, and defecting won't.
    // AND we know enough others *might* have cooperated in the past
    // to make this a viable push.
    if (num_prev_cooperators + 1 >= m) and (num_prev_cooperators < m):
        return "C"

    // Condition 2: "Near-Certain Collective Success" (for optimal payoff)
    // This condition is more complex to model precisely without deeper history analysis.
    // A simplified version: if majority of past rounds had >= m cooperators.
    // For this aggressive strategy, we focus on conditions that *guarantee* our payoff.
    // Let's refine the 'high cooperation, low risk' defection to be more concrete.

    // Condition for Defection: "High Cooperation, Low Risk"
    // If defecting, but still likely to get the reward.
    // (n - m) is the maximum number of people who can defect and still meet m.
    // If number of observed cooperators is >= m-1, we are relatively safe.
    // Consider how many *actual* defectors are needed to jeopardize *our* guaranteed 'k' reward.
    // If num_prev_cooperators >= m-1, then even if *we* defect, there's still a good chance m is met.
    if num_prev_cooperators >= m - 1:
        return "D"

    // Default to Defect if no cooperation conditions are met.
    return "D"

function count_cooperators_in_round(actions_in_round):
    count = 0
    for action in actions_in_round:
        if action == "C":
            count += 1
    return count

// --- Refined Logic for Cooperation Condition 1: ---
// This is crucial. We need to be confident that *our* C makes the difference.
// Let's consider the *minimum* number of cooperators needed to *barely* meet m.
// If in the previous round, there were X cooperators, and X < m,
// and if X + 1 >= m, then my cooperation ensures the reward.
// This is what Condition 1 captures.

// --- Refined Logic for Defection ---
// We defect if we don't meet the cooperation conditions.
// The "High Cooperation, Low Risk" defection is implicitly covered by the default.
// The key is when our defection *jeopardizes* our own k reward.
// If num_prev_cooperators < m-1, then our defection might cause failure.
// So, if num_prev_cooperators < m-1, we default to Defect, *unless* Condition 1 triggers.

// Let's simplify the pseudocode to clearly reflect the primary aggressive intent:
```

```python
# Python-like pseudocode

def decide_action(current_round_t, num_players, threshold_m, num_rounds_r, past_round_actions):
    """
    Decides action (C or D) for the Leveraged Opportunist strategy.

    Args:
        current_round_t (int): The current round number (1-indexed).
        num_players (int): Total number of players (n).
        threshold_m (int): Minimum cooperators needed (m).
        num_rounds_r (int): Total number of rounds (r).
        past_round_actions (list of lists): History of actions.
                                            past_round_actions[t-1][player_idx] is action in round t.

    Returns:
        str: "C" for Cooperate, "D" for Defect.
    """

    # --- Edge Cases ---
    if current_round_t == 1 or current_round_t == num_rounds_r:
        return "D"

    # --- Calculate Historical Metrics ---
    # For simplicity, we'll look at the *immediately preceding* round's cooperation count.
    # A more robust strategy might track average cooperation or recent trends.
    if current_round_t > 2: # Need at least one full round to observe
        previous_round_actions = past_round_actions[current_round_t - 2] # Actions from t-1
        num_prev_cooperators = sum(1 for action in previous_round_actions if action == "C")
    else: # For round 2, there's no history of *other* players' actions affecting outcome.
          # This strategy still defaults to D, but we can imagine a state where if it's round 2
          # and someone cooperated in round 1, we *might* consider it.
          # For strict aggression, we ignore round 1 for decision making until round 3.
        num_prev_cooperators = 0 # Effectively, no reliable history yet.

    # --- Aggressive Decision Rules ---

    # Rule 1: "Threshold Guarantee" Cooperation
    # If my cooperation is the only thing needed to cross the threshold, and
    # it's possible to cross it with my help.
    # This means if the previous count was < m, but adding 1 makes it >= m.
    if num_prev_cooperators < threshold_m and (num_prev_cooperators + 1) >= threshold_m:
        # This is the most critical cooperation trigger: ensures YOU get k.
        return "C"

    # Rule 2: Default to Defect
    # If Rule 1 is not met, the strategy defaults to defecting to maximize private gain.
    # This includes scenarios where:
    # - Many players cooperated previously (m+1 or more), so defecting still grants k.
    # - Few players cooperated (less than m-1), so defecting yields only 1 and avoids risk.
    return "D"

# Example usage:
# past_actions_history = [
#     ["D", "D", "D", "D", "D", "D"], # Round 1
#     ["C", "D", "D", "D", "D", "D"], # Round 2
#     ["C", "C", "D", "D", "D", "D"], # Round 3
# ]
# decision = decide_action(current_round_t=4, num_players=6, threshold_m=3, num_rounds_r=10, past_round_actions=past_actions_history)
# print(f"Decision for round 4: {decision}")
```
'''

description_AGGRESSIVE_89 = '''
## Aggressive "Threshold Guardian" Strategy

This strategy is designed to be aggressive, risk-tolerant, and focused on exploiting the game's reward structure while minimizing personal exposure to failure. It aims to be a proactive player, influencing the outcome rather than passively reacting.

**Core Philosophy:** "I will push for the collective reward, but only when I'm confident enough others are too. If the collective falters due to others' weakness, I will punish that failure by defecting. The reward is worth the calculated risk."

---

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round `t` (where `t` ranges from 1 to `r`) hinges on a dynamic assessment of the collective "momentum" towards meeting the cooperation threshold `m`.

**Key Metric: "Cooperation Momentum" (CM)**

In each round `t`, we will calculate the **Cooperation Momentum (CM_t)**. This metric represents our belief about how many players are likely to cooperate in the *current* round, based on the history of cooperation.

*   **Definition:** `CM_t = (Average number of cooperators in previous rounds)` + `(Adjustment based on the number of players who defected when the threshold was met in the previous round)`

*   **Initial Calculation (Round 1):** Since there's no history, we start with a baseline assumption. `CM_1 = 0` (or a very low arbitrary value).

*   **Subsequent Rounds (t > 1):**
    *   Let `N_C(t-1)` be the number of players who cooperated in round `t-1`.
    *   Let `N_D(t-1)` be the number of players who defected in round `t-1`.
    *   Let `Avg_C(t-1)` be the average number of cooperators across all rounds from 1 to `t-1`.
    *   **Base Momentum:** `Base_CM_t = Avg_C(t-1)`
    *   **Aggressive Adjustment:** If `N_C(t-1) >= m` (threshold was met in previous round) AND `N_D(t-1) > 0` (some players defected despite success), we *increase* our projected momentum. This is because we infer those defectors might be opportunistic and might cooperate if they see others cooperating, or they might be testing the waters.
        *   `Aggressive_Adjustment = (N_D(t-1) / n) * (k - 1)` (The reward factor suggests a higher potential for exploitation if successful).
    *   **Defensive Adjustment:** If `N_C(t-1) < m` (threshold was NOT met), we *decrease* our projected momentum, as it indicates a general reluctance to cooperate or a lack of trust.
        *   `Defensive_Adjustment = - (n - N_C(t-1)) / n` (Punishing perceived failure by assuming more defection).
    *   **Final CM_t:** `CM_t = Base_CM_t + Aggressive_Adjustment + Defensive_Adjustment`
    *   **Clamping CM_t:** `CM_t` should be clamped between 0 and `n`.

**Decision Logic for Player i in Round t:**

*   **If `CM_t >= m - 1` (Slightly below or at the threshold):**
    *   **Play C (Cooperate).** We are signaling a strong willingness to contribute and push for the reward. This is our aggressive stance – we are willing to be one of the "m" if there's a good chance others are too.
*   **If `CM_t < m - 1`:**
    *   **Play D (Defect).** We are signaling a lack of confidence in collective action. We will not be the sole contributor to a potentially failed project, especially when the risk of no reward is high. This is a strategic retreat to preserve our endowment when the collective spirit is weak.

---

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   **Decision Rule:** Always **Play D (Defect)**.
    *   **Rationale:** In the first round, there is no history. The "Threshold Guardian" is inherently cautious before committing to cooperation. Defecting allows us to observe the initial behavior of all other players without personal risk. We will gather crucial data in this round. Our `CM_1` will be initialized low, leading to a 'D' decision.

*   **Last Round (t=r):**
    *   **Decision Rule:** Always **Play D (Defect)**.
    *   **Rationale:** In the final round, there are no future rounds to benefit from cooperation. The incentive to build trust or maintain a cooperative norm disappears. Aggression here means securing our personal gain. There is no penalty for defecting in the last round, as it doesn't affect future payoffs.

*   **When `m-1` is exactly `0` (i.e., `m=1`):** This is an invalid parameter as per `1 < m < n`. However, if it were allowed and `m=2` (so `m-1=1`), our logic would still hold. Our threshold for cooperating would be `CM_t >= 1`.

*   **When `m` is very close to `n`:** The strategy remains robust. The `CM_t` will need to be very high to trigger cooperation, reflecting the difficulty of reaching such a high threshold.

---

### 3. Aggressiveness:

*   **Proactive Cooperation Trigger:** The strategy doesn't wait for `m` cooperators to be confirmed. It *initiates* cooperation when the momentum suggests a high likelihood of reaching `m`. This means we're willing to be the catalyst.
*   **Exploiting Defectors:** The `Aggressive_Adjustment` in `CM_t` calculation is designed to "reward" situations where the threshold was met but some players defected. We interpret this as a sign of potential exploitability in subsequent rounds, increasing our willingness to cooperate ourselves to try and pull those defectors into the cooperative fold or to capitalize on their opportunistic nature.
*   **Punishing Failure:** The `Defensive_Adjustment` punishes rounds where the threshold was *not* met. This is aggressive in the sense that it signals a clear disincentive for collective failure. We will not repeatedly risk our endowment if the group consistently fails to cooperate. This aggressive stance discourages continued passivity from others.
*   **Risk Tolerance:** By cooperating when `CM_t >= m-1`, we are accepting the risk of being the `m`-th cooperator and having others defect, thus failing to reach `m`. This is a calculated risk taken to achieve the higher reward.

---

### Pseudocode:

```python
class ThresholdGuardianStrategy:
    def __init__(self, n, m, k):
        self.n = n
        self.m = m
        self.k = k
        self.cooperation_history = []  # List of booleans: True for C, False for D
        self.round_counter = 0

    def decide_action(self, past_round_actions=None):
        self.round_counter += 1

        # --- Edge Case: First Round ---
        if self.round_counter == 1:
            return "D"

        # --- Edge Case: Last Round ---
        if self.round_counter == self.r:  # Assuming 'self.r' is the total number of rounds
            return "D"

        # --- Update History with previous round's actions ---
        if past_round_actions is not None:
            self.cooperation_history.append(past_round_actions)

        # --- Calculate Cooperation Momentum (CM_t) ---
        num_previous_rounds = len(self.cooperation_history)
        if num_previous_rounds == 0: # Should not happen if round_counter > 1
            cm_t = 0
        else:
            # Calculate average cooperators in previous rounds
            total_cooperators_so_far = sum(1 for actions in self.cooperation_history for player_idx, action in enumerate(actions) if action == "C")
            avg_c_t_minus_1 = total_cooperators_so_far / (num_previous_rounds * self.n) # Average per player per round

            # Get last round's data
            last_round_actions = self.cooperation_history[-1]
            n_c_prev = sum(1 for action in last_round_actions if action == "C")
            n_d_prev = self.n - n_c_prev

            base_cm_t = avg_c_t_minus_1 * self.n # Scale back to number of players

            aggressive_adjustment = 0
            defensive_adjustment = 0

            if n_c_prev >= self.m: # Threshold met in previous round
                if n_d_prev > 0:
                    aggressive_adjustment = (n_d_prev / self.n) * (self.k - 1)
            else: # Threshold NOT met
                defensive_adjustment = - (self.n - n_c_prev) / self.n

            cm_t = base_cm_t + aggressive_adjustment + defensive_adjustment

            # Clamp CM_t between 0 and n
            cm_t = max(0, min(self.n, cm_t))

        # --- Decision Logic ---
        if cm_t >= self.m - 1:
            return "C"
        else:
            return "D"

    # This method would be called by the tournament to provide total rounds
    def set_total_rounds(self, r):
        self.r = r
```

**Explanation of Pseudocode Adaptations:**

*   `self.cooperation_history`: This list will store the actions of *all* players in each past round. `past_round_actions` is assumed to be a list of strings like `["C", "D", "C", ...]`, where the index corresponds to the player.
*   `avg_c_t_minus_1`: Calculated as the average proportion of cooperators across all past rounds. Then multiplied by `self.n` to get an expected number of cooperators for the current round.
*   `cm_t` calculation reflects the logic described above.
*   The decision `if cm_t >= self.m - 1` directly implements the core decision rule.

This "Threshold Guardian" strategy embodies aggression through its proactive stance and its willingness to push for collective success, while simultaneously being defensive by not consistently contributing to demonstrably failed cooperative efforts.
'''

description_AGGRESSIVE_90 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to be adaptive and robust in a tournament setting. The core idea is to leverage the potential for high rewards when cooperation succeeds, but to defect aggressively to maximize personal gain when cooperation seems unlikely or exploited.

**Strategy Name:** "Opportunistic Aggression"

**Core Philosophy:**
This strategy is built on the principle of **"cooperate only when it's most profitable to you, and defect otherwise."** It's aggressive because it prioritizes self-interest and seeks to exploit situations where cooperation would benefit others more than itself, or where the risk of cooperation failure is too high. It leverages the repeated nature of the game to learn and adapt.

---

**1. Decision Rules (When to Cooperate vs. Defect)**

The decision to cooperate or defect in any given round (t) is primarily driven by an assessment of the **"Cooperation Probability" (P_coop_t)**, which estimates the likelihood that at least 'm' players will cooperate in the current round.

*   **If P_coop_t ≥ Threshold_Coop:** Player cooperates (C).
*   **If P_coop_t < Threshold_Coop:** Player defects (D).

**How is P_coop_t calculated?**
This is the adaptive element. We'll maintain a rolling average of past cooperation rates, adjusted by how our own actions influenced the outcome.

Let:
*   `N_coop_t-1`: Number of players who cooperated in round t-1.
*   `I_cooperated_t-1`: 1 if *I* cooperated in round t-1, 0 if *I* defected.
*   `Payoff_t-1`: My payoff in round t-1.
*   `Avg_Opponent_Coop_Rate`: A smoothed average of how many *other* players have cooperated in previous rounds.

**Adaptive P_coop_t Calculation (Simplified Concept):**

`P_coop_t` is a weighted average reflecting the observed tendency of others to cooperate, factoring in the success or failure of cooperation in recent rounds.

*   **High `Avg_Opponent_Coop_Rate` and recent successful cooperation:** Increase `P_coop_t`.
*   **Low `Avg_Opponent_Coop_Rate` and recent failed cooperation:** Decrease `P_coop_t`.
*   **If I cooperated and cooperation failed (payoff = 1):** Significantly decrease `P_coop_t` for future rounds, as it suggests my cooperation was "wasted" or "exploited."
*   **If I cooperated and cooperation succeeded (payoff = 1+k):** Maintain or slightly increase `P_coop_t`, as this indicates my cooperation was valuable.
*   **If I defected and cooperation succeeded (payoff = 1+k):** This is a strong signal that others are cooperating. The strategy needs to *consider* cooperating in the next round if this pattern persists, but it's still dependent on the `Threshold_Coop`.
*   **If I defected and cooperation failed (payoff = 1):** This is the most common outcome for a defector in a failed round. It reinforces a tendency to defect.

**Threshold_Coop:**
This is a crucial parameter that defines our "aggressiveness." A **higher** `Threshold_Coop` means we are more hesitant to cooperate and demand a stronger signal of impending success. A **lower** `Threshold_Coop` makes us more willing to risk cooperation. For an aggressive strategy, we'll set this relatively high, ensuring that we only cooperate when we are reasonably sure the threshold will be met.

**Initial `P_coop_t` (Round 1):**
Since there's no history, we start with a baseline assumption. Given the aggressive nature, we will *defect* in Round 1 unless there's a specific strategic trigger. However, for adaptive learning, it's useful to have a starting point for `P_coop_t`. A neutral or slightly optimistic initial `P_coop_t` can be used for the *calculation* of future `P_coop_t` values, but the initial decision will be driven by edge cases.

---

**2. Handling Edge Cases**

*   **Round 1 (Initial Round):**
    *   **Decision:** **Always Defect (D)**.
    *   **Rationale:** In the first round, there is zero information. Aggressively securing your private endowment (payoff of 1) is the safest and most self-interested move. You cannot predict if others will cooperate to meet the threshold, so there's no incentive to risk your endowment. This also allows you to observe the behavior of others without revealing your own willingness to cooperate early on.

*   **Last Round (Round r):**
    *   **Decision:** **Always Defect (D)**.
    *   **Rationale:** In the final round, there are no future rounds to benefit from cooperation. The incentive to build trust or signal intentions is gone. The only rational choice for an aggressive strategy is to maximize your immediate payoff, which is achieved by keeping your endowment.

*   **Just Before the Last Round (Round r-1):**
    *   **Decision:** **Defect (D) if P_coop_t < Threshold_Coop, otherwise Cooperate (C).**
    *   **Rationale:** This round is critical. While still driven by the adaptive `P_coop_t`, the knowledge that the *next* round is the last round means there's a slight incentive to try and trigger the reward *one last time*. However, the primary driver remains the calculated probability of success. If the probability is low, aggression dictates defecting.

*   **When `m` is very high (close to `n`):**
    *   **Strategy Adjustment:** Increase `Threshold_Coop`.
    *   **Rationale:** If `m` requires almost everyone to cooperate, the probability of achieving it is naturally lower unless there's explicit coordination. An aggressive player would be very hesitant to contribute their endowment unless the signal for success is overwhelmingly strong.

*   **When `m` is very low (just above 1):**
    *   **Strategy Adjustment:** Decrease `Threshold_Coop` slightly.
    *   **Rationale:** If only a few cooperators are needed, and our estimation of `P_coop_t` is even moderately favorable, the payoff from successful cooperation (especially with a high `k`) becomes very attractive. We might be willing to take a slightly higher risk.

---

**3. Being Aggressive**

The aggression of "Opportunistic Aggression" is manifested in several ways:

*   **Default to Defection:** The default action is defecting. Cooperation is an exception, not the norm.
*   **High `Threshold_Coop`:** We only cooperate when we are reasonably confident that the threshold will be met. This means we are less likely to be the "lone cooperator" and bear the cost alone.
*   **Exploiting others' cooperation:** If the `Avg_Opponent_Coop_Rate` is high and others are consistently meeting the threshold, our adaptive `P_coop_t` calculation will increase. However, we are still evaluating this based on *our* optimal outcome. If defecting while others cooperate yields a higher payoff for us (e.g., 1+k vs. 0+k), we will continue to defect.
*   **Rapid adaptation to negative feedback:** If we cooperate and the threshold isn't met (leading to a payoff of 1), our `P_coop_t` will significantly decrease, making us less likely to cooperate in subsequent rounds. We learn from mistakes by punishing ourselves with a less cooperative stance.
*   **No altruism or long-term trust-building:** The strategy is purely self-interested. It doesn't aim to foster cooperation or establish norms for the collective good. It aims to extract maximum personal gain from the game dynamics.

---

**Pseudocode Outline:**

```pseudocode
// Global variables (initialized once)
n = number of players
r = number of rounds
m = minimum cooperators needed
k = reward factor
Threshold_Coop = (e.g., 0.7, adjust for desired aggression level) // Higher means more aggressive

// Player-specific state variables (updated each round)
my_history = [] // List of (my_action, payoff, num_cooperators) for each round
avg_opponent_coop_rate_smoothed = 0.5 // Initial estimate

function decide_action(current_round_number, game_history):
    // ----- Handle Edge Cases -----
    if current_round_number == 1:
        return "D"
    if current_round_number == r:
        return "D"

    // ----- Adaptive P_coop Calculation -----
    // Update avg_opponent_coop_rate_smoothed based on game_history
    // (This is complex and involves analyzing past rounds:
    //  - Look at num_cooperators from previous rounds.
    //  - Factor in whether I cooperated or defected.
    //  - Adjust based on my payoff in previous rounds.
    //  - A simple smoothing or EWMA could be used here for opponent behavior.
    //  For simplicity in pseudocode, let's assume we have a function for this:
    avg_opponent_coop_rate_smoothed = calculate_smoothed_opponent_coop_rate(game_history)

    // Estimate P_coop_t: A rough heuristic. This needs careful tuning.
    // If opponents have been cooperating a lot, our estimate of success increases.
    // If our own previous cooperations failed, it decreases our confidence.
    P_coop_t = avg_opponent_coop_rate_smoothed * (1 - self_failure_penalty) // self_failure_penalty increases if I cooperated and it failed

    // Adjust P_coop_t based on my own recent actions and outcomes (simplified):
    if not my_history.is_empty():
        last_round_outcome = my_history[-1]
        if last_round_outcome.my_action == "C" and last_round_outcome.payoff == 1: // I cooperated and it failed
            P_coop_t = P_coop_t * 0.5 // Drastically reduce confidence
        elif last_round_outcome.my_action == "C" and last_round_outcome.payoff > 1: // I cooperated and it succeeded
            P_coop_t = P_coop_t * 1.1 // Slightly increase confidence
        elif last_round_outcome.my_action == "D" and last_round_outcome.payoff > 1: // I defected and it succeeded (exploited)
            // This is tricky. It means others cooperated.
            // P_coop_t might stay similar, or we might be tempted to try cooperation if it means guaranteed 1+k.
            // For pure aggression, we might not change P_coop_t much unless it's high.
            pass

    // ----- Decision Logic -----
    if P_coop_t >= Threshold_Coop:
        return "C"
    else:
        return "D"

function calculate_smoothed_opponent_coop_rate(game_history):
    // This function would analyze 'game_history' to derive a smoothed estimate
    // of how likely opponents are to cooperate.
    // Example: If in the last 3 rounds, an average of 4 out of (n-1) opponents cooperated,
    // this contributes to the smoothed rate.
    // More sophisticated techniques (like Kalman filters or Bayesian inference) could be used.
    // For a basic implementation:
    if game_history.is_empty():
        return 0.5 // Neutral starting point for opponent behavior

    recent_coop_counts = []
    for round_data in game_history[-min(len(game_history), 5):]: // Look at last 5 rounds
        num_others_cooperated = round_data.num_cooperators - (1 if round_data.my_action == "C" else 0)
        if n > 1:
            recent_coop_counts.append(num_others_cooperated / (n - 1))
        else:
            recent_coop_counts.append(0) // Should not happen for n>=2

    if not recent_coop_counts.is_empty():
        avg_recent_coop = sum(recent_coop_counts) / len(recent_coop_counts)
        // Apply smoothing (e.g., EWMA)
        smoothing_factor = 0.3
        return smoothing_factor * avg_recent_coop + (1 - smoothing_factor) * previous_avg_opponent_coop_rate_smoothed
    else:
        return previous_avg_opponent_coop_rate_smoothed // Keep previous if no data


// In the tournament loop:
// current_round_number = ...
// game_history_observed = ... // Actions and payoffs of all players from previous rounds
// my_action = decide_action(current_round_number, game_history_observed)
// my_payoff = calculate_payoff(my_action, num_cooperators_this_round, m, k)
// Store (my_action, my_payoff, num_cooperators_this_round) in my_history
```

**Key to Aggression:**

*   **Tuning `Threshold_Coop`:** This is the primary lever for aggression. A higher value means a stronger signal is needed before cooperating.
*   **Rapidly Discounting Future Cooperation after Failure:** If I cooperate and it doesn't pay off (payoff is 1), my confidence in future cooperation plummets. This aggressive response prevents me from being a "sucker" repeatedly.
*   **Focus on Personal Gain:** Every decision is evaluated by its potential personal payoff, not by what might be best for the group. The strategy is designed to win the tournament, not to foster a cooperative meta-game.

This strategy is designed to be robust by learning from observed behavior and adapting its cooperation threshold. It remains aggressive by defaulting to self-interest and only taking the risk of cooperation when the perceived probability of a high payoff (k) is sufficiently high.
'''

description_AGGRESSIVE_91 = '''
## Aggressive Strategy: The Opportunistic Enforcer

This strategy is designed for a player who aims to maximize their own payoff while being assertive and leveraging opportunities for high rewards, even at the risk of potential losses. It's an aggressive approach that prioritizes the potential for a significant payout when cooperation is likely, but quickly retreats to pure self-interest when cooperation is threatened or insufficient.

The core philosophy is to "force" cooperation when possible, and to benefit from it when others are contributing enough to meet the threshold. It avoids being a passive victim of defection and aims to actively shape the game towards favorable outcomes.

### 1. Decision Rules

The decision for player `i` in round `t` is based on a calculation of the **perceived likelihood of meeting the cooperation threshold (m)**. This perception is built upon the historical actions of other players.

**Key Metric: Cooperation Momentum (CM)**

We'll track the "cooperation momentum" as a measure of how likely the group is to cooperate in the current round. This is calculated as the proportion of players who cooperated in the previous round.

*   `C_t-1`: Number of players who cooperated in round `t-1`.
*   `CM_t` = `C_t-1` / `n` (for t > 1)

**Decision Logic:**

*   **If `CM_t` >= (m - 1) / n AND `CM_t` > 0 (i.e., at least `m-1` cooperators observed in the previous round, and some cooperation occurred):**
    *   **Decision: Cooperate (C)**
    *   **Reasoning:** This signals confidence that the threshold will be met. By cooperating, you ensure you receive the reward `k` if the threshold is indeed met. This is the opportunistic aspect – if others are willing to do the heavy lifting (or at least come close), you'll join in to claim the prize.
*   **Else (if `CM_t` < (m - 1) / n OR `CM_t` == 0):**
    *   **Decision: Defect (D)**
    *   **Reasoning:** This is the aggressive enforcement. If the previous round's cooperation was insufficient to approach the threshold, or if no one cooperated at all, there's a high risk of ending up with only your private endowment (1) while others still defect and gain 1. It's more aggressive to secure your own gain and punish the lack of cooperation by not contributing.

**Aggression Alignment:**

*   **Proactive Cooperation:** You choose to cooperate when there's a strong signal that the threshold is achievable, aiming to secure the higher payoff. This is an aggressive stance of "if we're going to get the reward, I'm going to be there to get it."
*   **Punitive Defection:** You immediately defect if the group fails to show sufficient cooperation. This sends a strong signal that you will not be exploited and will prioritize your individual gain when the collective effort falters. This is the "enforcer" aspect.

### 2. Handling Edge Cases

*   **Round 1 (t = 1):**
    *   **Decision: Cooperate (C)**
    *   **Reasoning:** In the absence of any historical data, the strategy takes an initial aggressive stance. It assumes a willingness to cooperate to test the waters and potentially kickstart a cooperative outcome. This is a gamble, but an aggressive one. The hope is that by cooperating, you might encourage others to do the same, leading to an early success.
*   **Last Round (t = r):**
    *   **Decision: Defect (D)**
    *   **Reasoning:** In the final round, there are no future rounds to influence or exploit. The primary objective becomes maximizing immediate individual payoff. If the cooperation threshold is met, you get `1 - c_i + k`. If it's not met, you get `1 - c_i`. Since the reward `k` is only realized if `m` cooperate, and there's no future incentive to foster cooperation, it's always safer and more aggressive to secure your private endowment of 1. You cannot rely on others to cooperate, and there's no benefit in doing so yourself if the outcome is uncertain.

### 3. Aggressive Mindset Summary

This strategy is aggressive because:

*   **It's not a passive follower:** It actively seeks to benefit from collective action when the signs are good, rather than waiting for others to initiate.
*   **It punishes failure:** It immediately withdraws cooperation when the collective effort is insufficient, signaling a strong individualistic response to perceived exploitation.
*   **It takes calculated risks:** The initial cooperation in round 1 is a calculated risk to potentially set a positive tone, but it's a calculated risk nonetheless.
*   **It prioritizes self-interest when it matters:** The defection in the final round is a pure, unadulterated pursuit of self-interest, devoid of any long-term strategic considerations.

### Pseudocode

```
function DecideAction(currentPlayerID, currentRound, gameParameters, history):
  n = gameParameters.n
  m = gameParameters.m
  r = gameParameters.r
  k = gameParameters.k

  // Handle edge cases
  if currentRound == 1:
    return "C"  // Aggressive initial move
  if currentRound == r:
    return "D"  // Pure self-interest in the last round

  // Calculate Cooperation Momentum from the previous round
  previousRoundActions = history[currentRound - 1].actions
  cooperators_previous_round = count_cooperators(previousRoundActions)
  cooperation_momentum = cooperators_previous_round / n

  // Decision logic based on Cooperation Momentum
  // We need m-1 cooperators in the previous round to feel confident
  // that cooperating will push us over the threshold.
  // The threshold for cooperation momentum is thus (m-1)/n.
  // Also, ensure some cooperation actually happened to avoid rewarding stagnation.
  if cooperation_momentum >= (m - 1.0) / n and cooperation_momentum > 0:
    return "C"  // Opportunistically cooperate if threshold is likely
  else:
    return "D"  // Aggressively defect if cooperation is lacking

function count_cooperators(actions):
  count = 0
  for action in actions:
    if action == "C":
      count += 1
  return count

// Example usage in a tournament loop:
// for round_num from 1 to r:
//   actions_this_round = []
//   for player_id from 1 to n:
//     action = DecideAction(player_id, round_num, game_params, game_history)
//     actions_this_round.append(action)
//   // Record actions and payoffs for game_history
```
'''

description_AGGRESSIVE_92 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize your payoff by leveraging the game's structure while being mindful of the cooperative threshold.

## Aggressive Strategy: "Calculated Risk Maximizer"

This strategy is aggressive in its pursuit of the higher payoff when cooperation succeeds, but it's not reckless. It aims to exploit situations where others are likely to cooperate, while minimizing its own risk if cooperation fails. It's designed to be adaptive by constantly re-evaluating the landscape based on historical play.

### Core Philosophy:

*   **Punish Defection:** Do not reward players who defect when cooperation was achievable.
*   **Reward Cooperation (Strategically):** Cooperate when the likelihood of reaching the threshold is high, aiming to secure the multiplier 'k'.
*   **Exploit Predictability:** If opponents fall into predictable patterns, leverage that for your benefit.
*   **Maximize Individual Gain:** The ultimate goal is to maximize your total payoff, not necessarily to foster universal cooperation.

### Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` (where `t` is the current round number, starting from 1) depends on two main factors:

1.  **The "Cooperation Momentum":** An assessment of how likely it is that `m` or more players will cooperate in the current round, based on past performance.
2.  **The "Defection Penalty":** Whether or not it is "safe" to cooperate based on recent defections by other players.

**Let's define some helper variables to track:**

*   `my_cooperation_history`: A boolean list storing your actions in previous rounds (True for C, False for D).
*   `opponent_cooperation_counts`: A list of integers, where `opponent_cooperation_counts[j]` stores the number of times opponent `j` has cooperated.
*   `total_cooperators_in_round_t`: The number of players (including yourself) who cooperated in round `t`.
*   `my_last_action`: Your action in the previous round.

**Decision Logic for Round `t` (where `t > 1`):**

**IF** (number of rounds remaining is 1) **THEN**
    **Decision: D (Defect)**
    *   **Reasoning:** In the final round, there's no future to incentivize cooperation. Defecting guarantees you your endowment (1) and allows you to capture the "reward if threshold met factor" if others cooperate and the threshold is met without your contribution. This is the most selfish and aggressive play in the endgame.

**ELSE IF** (current number of cooperating opponents in the previous round (`t-1`) + your previous action (`C`)) **< m** **THEN**
    **Decision: D (Defect)**
    *   **Reasoning:** Cooperation failed in the previous round, and even if you had cooperated, the threshold wasn't met. This indicates a high risk of future failure. Defecting is the safe play to guarantee your endowment. We don't want to reward a failed cooperative effort.

**ELSE IF** (cooperation momentum is HIGH and the previous round saw a near-miss or success) **THEN**
    **Decision: C (Cooperate)**
    *   **Reasoning:** This is where we aggressively aim for the reward. "Cooperation momentum" is high if:
        *   In the previous round, `m-1` or `m` players cooperated. This suggests a strong tendency towards meeting the threshold.
        *   OR a significant majority of players (e.g., > 75% of `n`) cooperated in the last round.
        *   OR the average cooperation rate of opponents over the last few rounds is high.
    *   We want to be the player that tips the scales to `m` cooperators. This is the calculated risk for the higher payoff.

**ELSE IF** (the number of defectors in the previous round was low, and the threshold was met) **THEN**
    **Decision: C (Cooperate)**
    *   **Reasoning:** If cooperation was successful and the cost of it (in terms of defections from others) was low, it's an indication that others are also willing to cooperate. This is a low-risk opportunity to continue the winning strategy and secure the reward.

**ELSE**
    **Decision: D (Defect)**
    *   **Reasoning:** This is the default, safe play. If none of the conditions for cooperation are met, or if there's too much uncertainty, defect to guarantee your baseline payoff. This also acts as a mild deterrent to players who might be tempted to defect too often.

### Handling Edge Cases:

*   **First Round (`t=1`):**
    *   **Decision: C (Cooperate)**
    *   **Reasoning:** In the first round, you have no history to judge opponent behavior. The most aggressive opening is to *assume* others *might* cooperate and try to be one of the initial cooperators. This can set a positive tone and potentially trigger the reward. It's a calculated gamble to seed cooperation. If it fails, you can immediately switch to a more defensive strategy in round 2.

*   **Last Round (`t=r`):**
    *   **Decision: D (Defect)**
    *   **Reasoning:** As explained above, there is no future incentive to cooperate. Defecting is the strictly dominant strategy in a single-shot game and thus in the last round of a repeated game with no discounting.

### Pseudocode:

```python
def decide_action(current_round, n, m, k, my_history, opponent_cooperation_counts, last_round_total_cooperators, my_last_action):
    rounds_remaining = r - current_round

    # 1. Handle Last Round
    if rounds_remaining == 0:
        return "D"

    # 2. Handle First Round
    if current_round == 1:
        return "C"

    # Calculate how many cooperators were present in the last round (including self if C was played)
    actual_cooperators_last_round = last_round_total_cooperators

    # 3. Decision Rules for Intermediate Rounds

    # Condition: Cooperation failed last round, and my contribution wouldn't have saved it
    # This is a simplification, a more nuanced version would look at the gap to 'm'
    if actual_cooperators_last_round < m and my_last_action == "C":
        return "D"

    # Condition: Cooperation momentum is HIGH and threshold was NEAR or MET
    cooperation_momentum_high = False
    if actual_cooperators_last_round >= m - 1: # Near miss or success
        cooperation_momentum_high = True
    # Add other momentum indicators here (e.g., high average cooperation of others)

    if cooperation_momentum_high:
        return "C"

    # Condition: Cooperation was successful and defectors were few
    # (This requires tracking opponent defections, which is more complex.
    # For simplicity, we can infer if many players cooperated)
    # Let's assume if actual_cooperators_last_round >= m AND number of players who defected is low (e.g., n - m)
    # This is a simplified check.
    if actual_cooperators_last_round >= m and (n - actual_cooperators_last_round) <= (n - m): # Few defectors
        return "C"

    # Default: Safe play
    return "D"

```

### Aggressive Nature Explained:

1.  **First Round Gamble:** Starting with 'C' is aggressive. It's an attempt to lead and exploit the potential for immediate reward. It's a statement: "I'm willing to take the first step."
2.  **Pursuit of Reward:** The strategy explicitly seeks conditions where cooperation is likely to succeed (`cooperation_momentum_high` and successful but "vulnerable" rounds). This is where the real aggression lies – actively trying to capture the `k` multiplier.
3.  **Punishing Failure:** If cooperation fails, the strategy defaults to 'D'. It doesn't waste resources trying to prop up a failing cooperative effort. This is aggressive because it prioritizes individual gain over collective recovery when the odds are against it.
4.  **Endgame Ruthlessness:** The final round 'D' is pure self-interest and highly aggressive. It exploits the lack of future consequences.

This strategy aims to be a strong competitor by not being a passive observer. It actively tries to engineer profitable situations while having a robust default to protect itself from exploitation by greedy or uncooperative players.
'''

description_AGGRESSIVE_93 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff while leveraging the cooperative incentive when it's most advantageous. This strategy prioritizes a strong, self-interested approach, but incorporates an understanding of the collective reward to exploit it.

## Aggressive Strategy: "The Opportunistic Enforcer"

**Core Philosophy:** This strategy is aggressive because it doesn't initiate cooperation without a strong signal of collective benefit and a clear advantage to itself. It aims to be the player who benefits most when cooperation occurs, and at worst, secures a decent individual payoff when it doesn't. It's "opportunistic" because it will capitalize on cooperation when it arises, and "enforcer" in the sense that it punishes defection by not cooperating when the threshold is not met.

---

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round depends on two primary factors: the **historical success of cooperation** and the **immediate proximity to the threshold `m`**.

**Rule 1: The "Threshold Gambler" (Dominant Rule)**

*   **If the number of players who cooperated in the *previous* round (`cooperators_prev`) plus the number of players *currently playing Defect but potentially benefiting from cooperation* in this round (`potential_cooperators_this_round`) is *greater than or equal to `m`***:
    *   **Play C (Cooperate).**
    *   **Rationale:** This is the core opportunistic element. If the game has shown a propensity for cooperation in the past, or if the immediate context suggests enough cooperation will occur to trigger the reward, I will contribute to secure the `k` multiplier. This is where the "aggression" comes in – I'm not waiting for guarantees, but I'm willing to take a calculated risk if the odds are favorable.

*   **Else (if the condition above is NOT met):**
    *   **Play D (Defect).**
    *   **Rationale:** If the collective is not demonstrating a strong will to cooperate, or if my defection is likely to break the `m` threshold, I will secure my private endowment of 1. This is the "enforcing" aspect – I refuse to be the sole cooperator or one of the few cooperators when it results in a payoff of 0 (my endowment minus my contribution of 1).

**Rule 2: The "Self-Interest Safeguard" (Conditional Modifier)**

*   This rule acts as a constraint on Rule 1, particularly in situations where cooperation might be costly.
*   **If playing C would result in a payoff of 0 for me (i.e., `m` or more players cooperate, but I chose C and my endowment `1` is used up by the contribution), AND if the number of cooperators in the previous round was less than `m`:**
    *   **Reconsider and play D (Defect).**
    *   **Rationale:** This is a purely selfish safeguard. If I'm going to contribute and the collective hasn't proven itself capable of reaching the threshold, I'm not going to make a contribution that leaves me with nothing when I could have secured 1 by defecting. This is an aggressive stance against "sucker" payoffs.

**Rule 3: The "Momentum Builder" (Situational)**

*   **If the number of cooperators in the *previous* round was *exactly `m-1`*, and I played C in that round:**
    *   **Play C (Cooperate).**
    *   **Rationale:** This is a strong signal that collective action is on the verge of success. If I was a part of that near-success and it was my contribution that was crucial, I will reinforce that behavior. This is a calculated risk to solidify the collective reward.

---

### 2. Handling Edge Cases:

**Round 1 (Initial Round):**

*   **Decision:** **Play D (Defect).**
*   **Rationale:** In the first round, there is no history of cooperation. The "Threshold Gambler" rule cannot be applied. The "Self-Interest Safeguard" cannot be triggered as there's no prior payoff context. Therefore, the default aggressive, risk-averse action is to secure the private endowment of 1. This allows observation of others' initial behavior.

**Last Round (`r`):**

*   **Decision:** **Play D (Defect).**
*   **Rationale:** In the absence of future rounds, the incentive to build trust or foster cooperation for long-term gain disappears. The primary goal becomes maximizing immediate payoff. Defecting guarantees a payoff of 1, whereas cooperating carries the risk of a 0 payoff if the threshold isn't met (or if `k` is small enough that the net gain isn't significantly better than 1). This is an aggressive, short-term maximization play.

**Rounds `t` where `r - t < m` (Last `m` rounds):**

*   This is a subtle modification. As the game nears its end, the incentive to "risk it all" for cooperation decreases. However, if the game has a history of cooperation, the strategy might still engage.
*   **Apply Decision Rules 1-3 as normal.** The rationale is that even in the later rounds, the potential for `k` is still valuable. However, the lack of future rounds makes the strategy *slightly* less inclined to initiate cooperation if the historical precedent is weak.

---

### 3. Aggressiveness Explained:

*   **No Initial Trust:** The strategy doesn't *initiate* cooperation. It waits for signals from the collective. This is aggressive in that it doesn't bear the initial risk of being the sole cooperator.
*   **Capitalizing on Success:** When cooperation is viable, it aggressively jumps in to secure the `k` multiplier. It's not hesitant to contribute if the reward is within reach.
*   **Punishing Non-Cooperation:** If the collective fails to meet the threshold, the strategy defects, ensuring it doesn't get a 0 payoff. This can be seen as a form of "punishment" by withholding its potential contribution, indirectly discouraging future free-riding.
*   **Self-Preservation:** The "Self-Interest Safeguard" prevents the strategy from being exploited by cooperating when it yields no personal benefit. This is a brutal form of self-interest.
*   **No Altruism:** The strategy's sole objective is its own payoff. It doesn't aim to "help" the group succeed for the group's sake, only for its own financial gain.

---

### Pseudocode Representation:

```
// Game Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward if threshold met factor

// State Variables (maintained per player)
history_cooperators: list of integers, number of cooperators in each past round
my_actions: list of actions (C or D) in each past round
current_round: integer

// --- Strategy Function ---
function DecideAction():
    // Handle Round 1
    if current_round == 1:
        return D

    // Handle Last Round
    if current_round == r:
        return D

    // Initialize variables
    num_cooperators_prev_round = 0
    if history_cooperators is not empty:
        num_cooperators_prev_round = history_cooperators.last()

    // Track potential cooperators for this round (who might be tempted by 'k')
    // This is a simplified approximation as we don't know others' strategies.
    // We assume that if 'm' was met last round, there's a good chance others
    // will cooperate again. We are a potential cooperator if we decide to play C.
    potential_cooperators_this_round = 0
    if num_cooperators_prev_round >= m:
        // If cooperation was successful, assume at least some players will try again.
        // We consider ourselves a potential cooperator IF we choose to play C.
        potential_cooperators_this_round = num_cooperators_prev_round

    // --- Rule 1: The "Threshold Gambler" ---
    // If the number of cooperators from the previous round (or assumed from success)
    // plus our *potential* contribution (if we choose C) meets or exceeds m.
    // We are playing aggressively if there's a good chance the threshold will be met.
    // We estimate "others" will bring us close to m.
    // A more robust approach for "potential_cooperators_this_round" could be to
    // estimate based on how many *distinct* players cooperated recently.
    // For simplicity here, we'll rely on the previous round's success.
    // If the previous round was successful, we expect some level of continued cooperation.

    // Estimate number of OTHER players likely to cooperate if threshold is reachable
    estimated_others_cooperating = 0
    if num_cooperators_prev_round >= m:
        // If cooperation was successful, assume a significant portion will continue.
        // This is a heuristic. In a real implementation, we'd look at trends.
        estimated_others_cooperating = num_cooperators_prev_round
    elif num_cooperators_prev_round == m - 1:
        // If just one short, there's a higher chance others will try.
        estimated_others_cooperating = num_cooperators_prev_round
    else:
        // If far from threshold, assume fewer will cooperate.
        estimated_others_cooperating = max(0, num_cooperators_prev_round - 1) // Assume at least one defector

    // Check if *my* cooperation would push us over the threshold
    // Note: this is tricky without knowing others' strategies precisely.
    // A more direct approach: If m-1 players cooperated last round, and I cooperate, we hit m.
    // Or, if >= m players cooperated last round, and I cooperate, we still hit m.

    // Simplified check: If last round was successful OR close to successful, and *my* C
    // combined with projected others' C, would reach m.
    // The most aggressive interpretation: if history suggests cooperation is likely, I will join.

    // Let's refine Rule 1 based on payoff logic:
    // I will cooperate IF:
    // 1. Enough others cooperate such that my C makes the total >= m, OR
    // 2. Enough others cooperate such that even if I defect, the total is >= m (I benefit from their risk).

    // Simpler aggressive implementation of Rule 1:
    // If the number of cooperators in the previous round was >= m-1 (i.e., close to or at threshold)
    // OR if k is very high, I am more inclined to risk it.
    // Let's focus on the historical success as the primary driver for Rule 1.

    // If the threshold was met or exceeded in the previous round, OR if it was m-1 and I was a cooperator:
    condition_for_cooperation = False
    if num_cooperators_prev_round >= m:
        condition_for_cooperation = True // History of success
    elif num_cooperators_prev_round == m - 1:
        // Check if I was a cooperator in the last round that was m-1
        if my_actions.last() == C:
            condition_for_cooperation = True // Just one short, and I helped last time

    // Consider current context: If enough others will likely cooperate to meet m.
    // This is the hardest part without knowing other strategies.
    // Heuristic: If num_cooperators_prev_round is high, assume some continuation.
    // If my contribution is needed to reach m:
    if num_cooperators_prev_round < m:
        // If current cooperators from history + my potential C >= m
        // This is still difficult to quantify without knowing others' moves.
        // Let's use a simpler trigger: if previous round's cooperation was high.
        // If the previous round had >= m cooperators, OR if it had m-1 and I was part of it.
        if num_cooperators_prev_round >= m or (num_cooperators_prev_round == m - 1 and my_actions.last() == C):
             # This is where we evaluate the payoff of cooperating vs defecting.
             # If I cooperate, my payoff is 0 + k if threshold met, or 0 if not.
             # If I defect, my payoff is 1 + k if threshold met by others, or 1 if not.

             # Aggressive play: Cooperate if it looks like a good bet for k.
             # If the history suggests cooperation is likely to succeed (>= m-1 cooperators previously, and I was part of it if m-1)
             // Then my cooperation helps ensure it.
             # The payoff is (1 - 1) + k = k if threshold met.
             # If I defect, it's 1 + k IF threshold met by others.
             # The decision to C or D depends on whether k is large enough.
             # Let's say k > 1. Then k is potentially better than 1.

             # The rule: If history indicates a high likelihood of 'm' being met (>= m-1 cooperators previously, or m-1 and I contributed)
             # I will cooperate to ensure it happens, IF k is sufficiently large (which it is, k>1).
             return C # Aggressive cooperation when success is probable

    // --- Rule 2: The "Self-Interest Safeguard" ---
    // If cooperating would lead to a payoff of 0 (my endowment used up, no reward) AND the collective has not been successful recently.
    if num_cooperators_prev_round < m: # If previous round didn't meet threshold
        # If I play C, my payoff will be 0 (my endowment 1 - my contribution 1) IF threshold is met by others.
        # If I play D, my payoff will be 1 (my endowment 1 - my contribution 0).
        # If cooperating leads to 0 payoff and the collective is not reliably cooperating, defect.
        // This check is tricky as it depends on whether 'm' will be met THIS round.
        // A safer interpretation: If cooperating puts me at risk of 0 and the past isn't strong.

        // If, *hypothetically*, cooperating leads to 0 for me this round AND the collective failed last round.
        // The prompt implies 'k' is large enough that if threshold met, payoff is >0.
        // The risk of 0 payoff is when my C means 'm' players cooperate, but I still get 0.
        // This happens if I play C, 'm-1' others play C, and I get 0 + k. Wait, this is payoff is NOT 0.
        // The payoff is (1-c_i) + k. If c_i = 1, payoff is 0 + k.
        // So the only way to get 0 payoff is if 1-c_i = 0 and k=0. But k>1.
        // The rule states "π_i = (1 - c_i)". If m or more players cooperate: π_i = (1 - c_i) + k.
        // If I play C (c_i=1), my payoff is (1-1) + k = k. This is NEVER 0 as k>1.
        // The rule "Self-Interest Safeguard" as described in the text is actually not applicable given k>1.
        // "If playing C would result in a payoff of 0 for me". This is impossible if k>1.
        // The payoff is always at least 1 (by defecting) or k (by cooperating if threshold met).
        // Let's re-interpret this as: If cooperating does not offer a STRICTLY better outcome than defecting, and the collective has not proven reliable.

        // Revised "Self-Interest Safeguard" interpretation:
        // If the probability of meeting 'm' is low (based on history), and my cooperation *might* lead to a low payoff (e.g., if k is not much greater than 1 and others defect), I will defect.
        // The most aggressive stance here is to defect if there's ANY doubt about the collective succeeding.

        // Let's re-align with the "Aggressive" mindset. If there's ANY ambiguity, default to D.
        // Re-evaluating Rule 1's condition:
        // I cooperate IF I am reasonably certain that 'm' will be met, and my cooperation helps ensure it OR benefits me from others' risk.

        // Let's simplify for aggression:
        // If the number of cooperators in the previous round was >= m, I will cooperate.
        if num_cooperators_prev_round >= m:
            return C

        // If the number of cooperators in the previous round was m-1, AND I played C in that round:
        if num_cooperators_prev_round == m - 1 and my_actions.last() == C:
            return C

    // --- Default to Defect if no cooperation incentive is met ---
    return D

// --- End Strategy Function ---

// Example of how state variables would be updated (outside this function)
// After each round 't':
//   history_cooperators.add(number_of_cooperators_this_round)
//   my_actions.add(my_action_this_round)
//   current_round = t + 1

```

**Refinement on "Potential Cooperators This Round":**

The pseudocode highlights the difficulty of estimating "potential_cooperators_this_round" without knowing other strategies. A truly robust implementation would track how often specific players cooperate or defect, and build a predictive model. However, for this strategy's description, relying on the direct history of the *number* of cooperators is sufficient to demonstrate the "Threshold Gambler" principle. The core idea is: "If the group has been successful (or almost successful with my help) in the past, I'll contribute to make it happen again because the reward `k` is attractive."

This strategy is aggressive because it doesn't shy away from the potential of cooperation, but it's also rational and self-interested, ensuring it doesn't become a "sucker" if the group fails to coalesce.
'''

description_AGGRESSIVE_94 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed for a tournament environment where trust and communication are absent, and the primary goal is to maximize individual payoff. This strategy prioritizes self-interest while leveraging the game's mechanics to extract maximum value.

## Aggressive "Dominant Defector" Strategy

This strategy is built on the principle of **exploiting the cooperative impulse of others** while maintaining a strong defensive posture. It's designed to be aggressive by aiming for the highest possible individual payoff in each round, even at the potential cost of collective success, unless a clear and immediate benefit is guaranteed.

### Core Philosophy: "Everyone Else is a Potential Sucker"

The "Dominant Defector" strategy operates under the assumption that other players are not inherently cooperative. Their cooperation is a resource to be exploited, not a norm to be upheld. The strategy is designed to profit from scenarios where others cooperate enough to trigger the reward, while minimizing its own contribution to achieving that threshold.

### 1. Decision Rules: The Threshold of Trust

The fundamental decision rule is based on **observing the actions of others and their impact on the payoff threshold (`m`)**.

*   **Decision Rule 1: Defect if the Reward is Not Guaranteed to Benefit You.**
    *   **Condition:** In any given round `t` (from round 1 to `r-1`):
        *   If the number of observed cooperators in the *previous round* (`C_t-1`) is less than `m-1`.
        *   Or, if the number of observed cooperators in the *previous round* (`C_t-1`) is exactly `m-1` AND the player anticipates at least one other player will defect. (This is a nuanced judgment, explained in the "Aggressive Adaptation" section).
    *   **Action:** **Defect (D)**.
    *   **Rationale:** If there aren't enough cooperators to guarantee meeting the threshold even with your cooperation, or if there's a significant risk of the threshold *not* being met despite your contribution (due to others defecting), it's safer and more profitable to defect and secure your private endowment.

*   **Decision Rule 2: Cooperate if the Reward is Guaranteed and You Still Benefit.**
    *   **Condition:** In any given round `t` (from round 1 to `r-1`):
        *   If the number of observed cooperators in the *previous round* (`C_t-1`) is `m-1`.
        *   AND, the player is reasonably confident (based on historical patterns of others) that at least one other player will also cooperate to reach `m`.
        *   AND, the resulting payoff for *you* (playing C) is strictly greater than your payoff from defecting (which would be 1 if the threshold is met). This means `(1 - 1) + k > 1`, which simplifies to `k > 1`. Since `k > 1` is a game parameter, this condition is always met if the threshold is met. The crucial part here is the confidence in reaching `m`.
    *   **Action:** **Cooperate (C)**.
    *   **Rationale:** If others are consistently cooperating to the point where your cooperation is the *marginal* factor to achieve the reward, and the reward is substantial (`k > 1`), you exploit this situation. You become the linchpin for success, ensuring the reward is triggered, and you get to partake in it. This is where the aggressive aspect comes in: you are positioning yourself to be the critical cooperator, maximizing your gain.

*   **Decision Rule 3: Defect in the Final Round (Unless All Others Cooperated Last Round).**
    *   **Condition:** In the final round (`t = r`).
    *   **Action:** **Defect (D)**.
    *   **Exception:** If ALL `n-1` other players cooperated in round `r-1` (meaning `C_r-1 = n-1` and `n-1 >= m`), then you should **Cooperate (C)**.
    *   **Rationale:** In the final round, there are no future consequences for being selfish. The primary goal is to maximize your payoff in *this* round. Defecting guarantees you 1. Cooperating *might* yield `0 + k` if the threshold is met, but it's a risk if others have already defected. The exception is for when the reward is a near certainty because everyone else has already proven their cooperative intent to an extreme degree, making your cooperation a low-risk way to secure a higher payoff.

### 2. Handling Edge Cases

*   **Round 1:**
    *   **Action:** **Defect (D)**.
    *   **Rationale:** In the absence of any history, the default aggressive stance is to defect. This is the safest option, guaranteeing a payoff of 1. It allows you to observe the behavior of others without any initial investment.

*   **Last Round (`r`):**
    *   As per **Decision Rule 3** above.

*   **When `m-1` Cooperators are Observed:** This is the most critical juncture for aggressive play.
    *   **If `m-1` players cooperated in the previous round:** This means the threshold is *one cooperation away*.
        *   **The aggressive play here is to HOLD BACK and observe.** If you defect, and someone else also defects, the threshold is missed, and you get 1. If you defect, and someone else cooperates, the threshold is met, and you get 1 (your private endowment).
        *   **Only if you are highly confident (based on historical data, see below) that *another* player will step up and cooperate, should you consider cooperating.** Otherwise, defect. The goal is to let others bear the risk of triggering the reward if the situation is precarious.
        *   **However, if *you* are the `m-1`th cooperator from the previous round, your decision in the *current* round is critical.**
            *   If you were a cooperator in the previous round (`C_t-1`) and the total count was `m-1`:
                *   **Action:** **Cooperate (C)**.
                *   **Rationale:** You've already invested. If you defect now, you've wasted your cooperation from the previous round, and the reward is unlikely to be triggered (unless someone else *also* cooperates, which is not guaranteed). By cooperating again, you secure the reward for yourself.

### 3. Aggressive Adaptation: The "Threat Meter"

This strategy needs to be adaptive. We'll introduce a "Threat Meter" mechanism to gauge the likelihood of others cooperating.

*   **Threat Meter (`T`):** A counter that increases when others cooperate and decreases when they defect.
    *   **Initialization:** `T = 0` at the start of the game.
    *   **Update Rule (After each round `t`):**
        *   Let `C_t` be the number of players who cooperated in round `t`.
        *   If `C_t >= m`: `T = T + 1` (Positive reinforcement for collective success, though we're still primarily self-interested).
        *   If `C_t < m`: `T = T - 1` (This is the crucial part). Clamp `T` at a minimum value (e.g., `T = max(-5, T)`).
    *   **Decision Adjustment:**
        *   When considering Decision Rule 1 (Defect if reward not guaranteed), if `C_t-1 == m-1`:
            *   If `T` is sufficiently high (e.g., `T > 2`): You have some confidence that others will step up. This might slightly lean towards cooperating, but the default is still to defect if the guarantee isn't absolute.
            *   If `T` is low or negative: Stronger inclination to defect.

*   **"Conditional Cooperate" Trigger:**
    *   **Condition:** If `C_t-1 == m-1` AND `T` is high enough (e.g., `T > 3`).
    *   **Action:** **Cooperate (C)**.
    *   **Rationale:** This is a calculated risk. If the system has historically seen a good number of successful cooperations or a strong trend towards cooperation, even if the last round *just* missed the mark, you might make a strategic cooperation to ensure the reward is triggered, banking on others following suit. This is the most aggressive form of controlled cooperation.

### Pseudocode Representation

```
// Game Parameters: n, r, m, k

// Strategy State Variables
threat_meter = 0
my_previous_action = D // Assume defect in round 0 for calculation

// For each round t from 1 to r:

    // Observe actions from previous round (t-1)
    observed_cooperators_prev_round = count_cooperators(round t-1)
    observed_defectors_prev_round = n - observed_cooperators_prev_round

    // Update Threat Meter based on previous round's outcome
    if observed_cooperators_prev_round >= m:
        threat_meter = threat_meter + 1
    else:
        threat_meter = max(-5, threat_meter - 1) // Clamp threat_meter

    current_action = D // Default to defect

    // Decision Rule 3: Final Round Logic
    if t == r:
        if observed_cooperators_prev_round == n - 1 and (n - 1) >= m:
            current_action = C
        else:
            current_action = D
    else:
        // Decision Rule 1 & 2: General Rounds Logic

        // Case: Threshold is one away
        if observed_cooperators_prev_round == m - 1:
            // Aggressive adaptation for conditional cooperation
            if threat_meter > 3: // Threshold for confidence in others cooperating
                current_action = C // Take the calculated risk
            else:
                current_action = D // Default to safe defect

        // Case: Not close to threshold, or too many cooperators
        else if observed_cooperators_prev_round < m - 1:
            current_action = D // Too risky to cooperate

        // Case: Threshold already met or exceeded in previous round
        // Here, we still want to defect unless there's a strong incentive or pattern.
        // The core is to avoid being the sole cooperator when not necessary.
        else: // observed_cooperators_prev_round >= m
             current_action = D // Default to defect to maximize private gain

    // Final check: If you were the m-1'th cooperator last round and now it's your turn to act.
    // This is a more complex adaptation: if *you* were a cooperator last round and the count was m-1
    // and you anticipate the threshold being met IF you cooperate again.
    // Let's simplify by focusing on the overall count from the previous round.

    // The logic above handles the 'm-1' case well.
    // For the aggressive element: we want to defect most of the time.
    // Only cooperate if others have demonstrated strong cooperation and the reward is 'almost guaranteed' by them.

    // Re-evaluating the core logic:
    // We want to defect UNLESS cooperating is the ONLY way to achieve the reward AND it is beneficial for us.
    // The current structure prioritizes defecting when not certain.

    // Let's refine for more explicit aggression:
    if t < r:
        if observed_cooperators_prev_round < m - 1:
            current_action = D // Not enough potential cooperators, defect
        elif observed_cooperators_prev_round == m - 1:
            // This is the crucial point. We need to decide if we are the 'spark'.
            // If threat_meter is high (meaning others have cooperated well historically),
            // AND it's highly likely someone else will cooperate IF we do:
            if threat_meter > 3 and observed_defectors_prev_round > 0: // If at least one other player exists who could defect
                current_action = C // Take the calculated risk to secure the reward
            else:
                current_action = D // Default to defect, let others carry the risk
        else: // observed_cooperators_prev_round >= m
            // If the threshold was already met, defecting is generally best unless
            // there's a specific pattern that suggests cooperation has led to higher overall payoffs over time.
            // For pure aggression, defect to maximize immediate private gain.
            current_action = D

    // Play current_action
    play(current_action)
    my_previous_action = current_action

// End of Game
```

### Aggressive Mindset Justification

1.  **Default to Defect:** The initial stance is pure self-interest and risk aversion. This guarantees a baseline payoff of 1 in every round if others don't cooperate.
2.  **Exploiting the "Marginal Cooperator" Position:** The strategy *only* considers cooperating when exactly `m-1` players have cooperated previously. This is the point of maximum leverage. It aims to be the "linchpin" cooperator, but only if the historical "Threat Meter" suggests others are likely to also cooperate, thus guaranteeing the reward. This is aggressive because it positions the player to benefit from the collective effort without necessarily initiating it in low-certainty scenarios.
3.  **Last Round Nihilism:** Eliminating any consideration of future cooperation in the final round is pure, unadulterated aggression for immediate gain.
4.  **Adaptive Threat Meter:** While the default is defection, the "Threat Meter" allows for calculated risks when others have *demonstrated* a tendency towards cooperation. This isn't "trust," it's "data-driven exploitation." If others consistently cooperate, the meter rises, and the strategy might engage in conditional cooperation to secure the higher payoff. This is aggressive because it seeks to extract maximum value from predictable, positive opponent behavior.
5.  **Minimal Commitment:** The strategy never commits to cooperating unless the conditions are highly favorable for a guaranteed reward *for itself*. It avoids being the single cooperator that fails to meet the threshold.

This "Dominant Defector" strategy is designed to be a formidable opponent in a competitive tournament. It's selfish, adaptive, and always seeks the path to the highest individual payoff by cleverly leveraging the cooperative behavior of others without being an easy target for exploitation.
'''

description_AGGRESSIVE_95 = '''
Here's an aggressive strategy designed for the Collective Risk Dilemma, focusing on maximizing individual gain while leveraging the collective reward when it's strategically advantageous.

## Aggressive Strategy: "Calculated Gamble"

This strategy embodies an aggressive mindset by prioritizing personal gain while opportunistically participating in the collective reward only when the odds are heavily in its favor. It's designed to be adaptive and exploit uncertainty.

### Core Philosophy:

My primary directive is to **maximize my own payoff**. I will exploit the "defect" option as my default, as it guarantees a payoff of 1. I will only cooperate if the conditions strongly suggest that cooperation will lead to a net positive gain *for me*, and specifically, if there's a high probability of the collective reward being triggered. The "Calculated Gamble" part comes from carefully assessing the risk and reward of cooperating in each round.

### 1. Decision Rules:

My decision to cooperate or defect in any given round `t` (where `t > 1`) is based on the following dynamic assessment:

**Default Action: Defect (D)**

I will always default to playing **D** unless a specific condition for cooperation is met. This ensures I always secure my private payoff of 1.

**Condition for Cooperation (C):**

I will consider cooperating (playing **C**) only if **both** of the following conditions are met:

*   **Condition 1: Proximity to the Threshold (Observed Cooperation):**
    *   Let `C_t-1` be the number of players (excluding myself) who cooperated in the *previous* round (`t-1`).
    *   I will cooperate if `(m - 1) - C_t-1 <= threshold_buffer`.
    *   `threshold_buffer` is a small, configurable integer (e.g., 1 or 2). This means I'll consider cooperating if the number of cooperators in the previous round was already close to the minimum required (`m`), and I believe my cooperation will be the tipping point. The `threshold_buffer` allows for a small margin of error or a slight hesitancy to be the *very first* to risk cooperation.

*   **Condition 2: Observed Pattern of Cooperation (No Exploitation):**
    *   I will also examine the *recent history* of cooperation. If a significant number of players have consistently defected in recent rounds, I will be hesitant to cooperate.
    *   Specifically, if the average number of cooperators (excluding myself) in the last `x` rounds (where `x` is a small lookback window, e.g., 3 or 5) is less than `m - 1`, I will be reluctant to cooperate, even if Condition 1 is met. This protects me from being the sole cooperator and being exploited.

**Mathematical Representation of Cooperation Trigger:**

Cooperate (C) if:
`((m - 1) - C_t-1 <= threshold_buffer)` **AND** `(Average_C_recent(t-1, x) >= m - 1)`

Where:
*   `C_t-1`: Number of players (excluding myself) who cooperated in round `t-1`.
*   `threshold_buffer`: A small integer (e.g., 1 or 2).
*   `Average_C_recent(t-1, x)`: The average number of cooperators (excluding myself) in the last `x` rounds, ending at round `t-1`.

**If my calculation suggests cooperation, I will defect (D) if my payoff from defecting in this round is higher than my expected payoff from cooperating.** This is a sophisticated check to ensure I'm not cooperating if the risk outweighs the potential reward *even if the threshold is met*.

**Calculating Expected Payoff from Cooperation:**

My expected payoff from cooperating in round `t` (if the conditions above are met) is:

`Expected_Payoff(C_t) = (1 - 1) + k` (if threshold is met) * `P(threshold met | my C)`

Where `P(threshold met | my C)` is my subjective probability that my cooperation will cause the threshold `m` to be met. This probability is estimated based on `C_t-1` and `threshold_buffer`. If `(m - 1) - C_t-1 <= threshold_buffer`, then my probability of tipping the scale is high.

**Decision Logic Summary:**

*   **Default:** Play **D**.
*   **Consider C if:**
    *   `m - 1` cooperators were observed in the last round OR `m - 2` cooperators were observed in the last round (if `threshold_buffer=1`).
    *   AND there hasn't been a consistent pattern of widespread defection in recent history (average cooperators not too low).
*   **If considered for C:**
    *   Calculate my expected payoff from cooperating.
    *   Calculate my payoff from defecting (which is always 1).
    *   **Cooperate (C) only if Expected_Payoff(C_t) > Payoff(D_t).**
*   **Otherwise:** Play **D**.

### 2. Handling Edge Cases:

*   **First Round (t = 1):**
    *   Since there is no history, I cannot assess any patterns or proximity to the threshold.
    *   **Aggressive Default:** I will play **D** in the first round. This is the safest and most personally guaranteed payoff. I want to observe the landscape before committing any resources to a collective effort.

*   **Last Round (t = r):**
    *   In the last round, there is no future for reciprocation or punishment. The incentive for cooperation is significantly reduced from a long-term perspective.
    *   **Aggressive Final Move:** I will play **D** in the last round, regardless of any past history or conditions. My primary goal is to secure my final payoff without any risk. The collective reward is no longer relevant for future rounds.

*   **Parameters `m` and `k`:**
    *   **High `m` (close to `n`):** If `m` is very high, the threshold is difficult to meet. My strategy will become more risk-averse, and I'll be less likely to cooperate unless the conditions are overwhelmingly favorable (e.g., `m-1` cooperators in the previous round).
    *   **Low `k` (close to 1):** If `k` is low, the reward for cooperation is small. This will also make me less likely to cooperate, as the potential gain is minimal compared to the guaranteed payoff of defecting.
    *   **High `k`:** A high `k` makes cooperation more tempting, but my strategy still requires the *probability* of success to be high. I won't be drawn into risky cooperative ventures just because the reward is large; the conditions for cooperation must be met.

### 3. Aggressive Mindset Alignment:

*   **Default to Defection:** This is the cornerstone of aggression. I am not inherently cooperative; I am opportunistic. I seize my guaranteed gain.
*   **Calculated Risk-Taking:** I don't blindly defect. I am willing to *risk* cooperation, but only when the odds are significantly in my favor. This is not generosity; it's a calculated investment with a high probability of return.
*   **Exploiting Proximity:** I leverage the actions of others. If others are already cooperating and bringing the group close to the threshold, I see an opportunity to get the reward with minimal personal risk. I am essentially "riding the wave" of successful cooperation initiated by others.
*   **No Long-Term Commitments (except implicitly):** My strategy is adaptive to the current round and recent history, but the aggressive nature means I don't build trust or foster long-term cooperative norms. Each round is a new calculation.
*   **Last Round Betrayal:** The decision to always defect in the last round is a pure expression of self-interest, maximizing my individual gain at the expense of any collective outcome.

### Pseudocode Example:

```python
def strategy_calculated_gamble(n, r, m, k, current_round, history):
    """
    Aggressive 'Calculated Gamble' strategy.

    Args:
        n: Total number of players.
        r: Total number of rounds.
        m: Minimum cooperators needed.
        k: Reward factor.
        current_round: The current round number (1-indexed).
        history: A list of lists, where history[t-1] contains the actions
                 of all players in round t (e.g., ['C', 'D', 'C', ...]).
                 My action is assumed to be at a specific index (which I don't need
                 to know for this logic, as I calculate based on others).

    Returns:
        'C' or 'D'
    """

    my_index = # (Assume my player index is known or implicitly handled)

    # Handle Edge Cases: First and Last Rounds
    if current_round == 1:
        return 'D'
    if current_round == r:
        return 'D'

    # --- Analyze History ---
    previous_round_actions = history[current_round - 2] # Round t-1
    cooperators_in_prev_round = sum(1 for action in previous_round_actions if action == 'C')

    # Parameters for conditions
    threshold_buffer = 1  # Can be tuned (e.g., 1 or 2)
    lookback_window = 3   # Number of recent rounds to check for cooperation pattern

    # Condition 1: Proximity to Threshold
    is_close_to_threshold = (m - 1) - cooperators_in_prev_round <= threshold_buffer

    # Condition 2: Observe Cooperation Pattern
    recent_cooperation_count = 0
    if current_round - 1 >= lookback_window: # Ensure enough history for the window
        for i in range(current_round - 1 - lookback_window, current_round - 1):
            recent_round_actions = history[i]
            recent_cooperation_count += sum(1 for action in recent_round_actions if action == 'C')
        average_recent_cooperators = recent_cooperation_count / lookback_window
    else:
        # If not enough history for window, consider current cooperation levels
        # This is a simplification; a more robust strategy might handle this differently.
        # For now, we'll allow cooperation if close to threshold and no strong negative signal.
        average_recent_cooperators = m # A high value to not block cooperation

    has_pattern_of_cooperation = average_recent_cooperators >= m - 1

    # --- Decision Logic ---
    if is_close_to_threshold and has_pattern_of_cooperation:
        # Now, calculate expected payoff vs defect payoff
        # Assume my cooperation will tip the balance if is_close_to_threshold is True

        # Payoff from defecting is always 1
        payoff_defect = 1.0

        # Expected payoff from cooperating
        # If threshold is met, my payoff is (1-1) + k = k
        # P(threshold met | my C) is assumed to be 1 when is_close_to_threshold is true
        expected_payoff_cooperate = k

        # Aggressively cooperate ONLY if it's definitely better
        if expected_payoff_cooperate > payoff_defect:
            return 'C'
        else:
            return 'D' # Cooperation doesn't offer a guaranteed gain over defecting
    else:
        # Conditions for cooperation not met, default to defect
        return 'D'

```
'''

description_AGGRESSIVE_96 = '''
This is an intriguing challenge! Designing an aggressive, adaptive strategy for the Collective Risk Dilemma requires a delicate balance between pushing for cooperation and extracting maximum personal gain. The core tension lies in the fact that while cooperation is rewarded, defecting can yield a higher payoff *if* enough others cooperate to trigger the reward.

Here's an aggressive strategy designed for this environment, focusing on leverage and calculated risk.

## Aggressive Leverage Strategy (ALS)

The fundamental principle of ALS is to **force others to carry the burden of cooperation while maximizing your own gains**. We aim to be a player who defects by default, but cooperates only when it strategically benefits *us* due to the actions of others. Aggression here means not being the first to cooperate and actively exploiting situations where others have met the threshold.

### 1. Decision Rules

Our decision to Cooperate (C) or Defect (D) in any given round `t` depends on the observed actions of other players in the *previous* round `t-1`.

*   **Default Action: Defect (D)**
    *   We start by defecting. This is the safest and most individually profitable action when we have no information about others.

*   **Conditional Cooperation (C)**
    *   We will cooperate **only if** in the previous round (`t-1`), the number of observed cooperators (`C_t-1`) was **at least `m-1`**.
    *   **Rationale:** If `m-1` players cooperated in the previous round, then our single act of cooperation in the current round (`t`) will guarantee that the threshold `m` is met or exceeded. This ensures we receive the reward `k` while still only contributing our endowment to the project. This is where the leverage comes in – we are "riding on the coattails" of others' cooperation to secure our reward.

*   **Rationale for `m-1` Threshold:**
    *   If `m` players cooperated, then our C in the current round would be redundant for meeting the threshold but still incurs our endowment cost.
    *   If `m-2` or fewer cooperated, our single C is insufficient to trigger the reward, so we'd be better off defecting and keeping our endowment.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision: Defect (D)**
    *   **Rationale:** We have no historical data. Defecting is the purely rational, self-interested choice when uncertainty is at its highest. We cannot leverage anyone yet.

*   **Last Round (t=r):**
    *   **Decision: Defect (D)**
    *   **Rationale:** In the final round, there is no future to consider. The incentive to foster cooperation for future rounds disappears. Our sole objective is to maximize our payoff for this single round. If the threshold is met by others, we still benefit from the `k` reward by defecting. If it's not met, we still keep our endowment. Therefore, defecting is always the optimal choice in the last round.

*   **When `m-1` is impossible (e.g., `m=2` and `n` is small):** The rule still holds. If `m=2`, we'll cooperate if at least `2-1=1` player cooperated previously.

### 3. Aggressive Mindset Alignment

*   **"Free-rider" mentality:** We explicitly seek to benefit from the cooperation of others without being the first to contribute.
*   **Exploitative:** We exploit situations where others have done the "hard work" of reaching the cooperation threshold.
*   **Calculated risk:** While we are aggressive, we are not reckless. Our cooperation is conditional and strategic, ensuring we get paid for our cooperation.
*   **Maximizing individual payoff:** The strategy is designed to maximize our individual payoff in every possible scenario, by either keeping our endowment or getting the reward while minimizing our contribution.

### Pseudocode Representation

```python
# Global variables for game parameters
n = ...
r = ...
m = ...
k = ...

# History tracking
# observed_actions_history: a list of dictionaries, where each dictionary
# represents a round and maps player_id to their action ('C' or 'D')
# For example: [{'player_1': 'D', 'player_2': 'C', ...}, ...]
observed_actions_history = []

def decide_action(current_round, player_id, all_player_actions_in_history):
    """
    Decides the action for the current player in the current round.

    Args:
        current_round (int): The current round number (1-indexed).
        player_id (int): The ID of the current player.
        all_player_actions_in_history (list): History of all players' actions.

    Returns:
        str: 'C' for Cooperate, 'D' for Defect.
    """

    # Handle Last Round
    if current_round == r:
        return 'D'

    # Handle First Round
    if current_round == 1:
        return 'D'

    # Get actions from the previous round
    previous_round_actions = all_player_actions_in_history[current_round - 2] # 0-indexed history

    # Count the number of cooperators in the previous round
    cooperators_last_round = 0
    for player, action in previous_round_actions.items():
        if action == 'C':
            cooperators_last_round += 1

    # Decision Rule: Cooperate if m-1 cooperators were observed last round
    if cooperators_last_round >= m - 1:
        return 'C'
    else:
        # Default to Defect if the threshold isn't met for leverage
        return 'D'

# --- Tournament Loop Example (Conceptual) ---
# for round_t in range(1, r + 1):
#     my_action = decide_action(round_t, my_player_id, observed_actions_history)
#     # Record my_action and send it to the game engine
#
#     # After all players have acted and payoffs are resolved:
#     # Update observed_actions_history with actions of all players for round_t
#     # Update my_payoff for round_t
```

### Robustness and Adaptability

*   **Robustness:** The strategy is robust because it doesn't rely on any specific coordination from opponents. It always has a defined action based on the game parameters and past history. It can handle environments with high defection rates (where it will simply defect) and environments with high cooperation rates (where it will opportunistically cooperate to gain the reward).
*   **Adaptability:** While the core logic is fixed, its effectiveness adapts to the observed play. If opponents consistently fail to reach `m-1` cooperators, it will continue to defect. If opponents become more cooperative, it will opportunistically step in and cooperate to secure the reward. The `m-1` threshold itself is a direct parameter adaptation, ensuring it aligns with the game's specific risk/reward structure.

This ALS strategy is designed to be a consistently high-scoring player in a competitive tournament environment. It prioritizes individual gain and leverages the cooperative efforts of others, embodying an aggressive, yet strategically sound, approach to the Collective Risk Dilemma.
'''

description_AGGRESSIVE_97 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to exploit opportunities for high payoffs while minimizing risk, and adapted to the repeated nature of the game.

## Aggressive Strategy: The "Calculated Opportunist"

This strategy is built on the principle of **exploiting the reward (k) whenever the risk of failure is minimized, and defecting decisively when cooperation is likely to be futile or exploited.** It's aggressive because it aims to maximize individual gains, even if it means occasionally "defecting" on collective efforts when the odds are not in its favor.

### Core Philosophy:

*   **Maximize individual payoff:** The primary goal is to secure the highest possible payoff for the strategy itself.
*   **Leverage the reward (k):** Cooperating when cooperation *will* succeed is the highest-yielding action (payoff of 2 vs. 1).
*   **Minimize wasted resources:** Cooperating when cooperation *will not* succeed is a loss (payoff of 0 vs. 1).
*   **Punish perceived exploitation:** If others defect when cooperation is clearly beneficial, the strategy will retaliate.
*   **Adapt to history:** Learn from past rounds to predict future behavior and adjust decisions accordingly.

### 1. Decision Rules:

The strategy's decision in any given round is based on an assessment of the likelihood of the cooperation threshold (`m`) being met.

**A. The "Sure Bet" Cooperation:**

*   **Condition:** If, based on the history of previous rounds, the strategy estimates that **at least `m` players (including itself) are highly likely to cooperate**, then the strategy will **Cooperate (C)**.
*   **Rationale:** This is the scenario where cooperating is guaranteed to yield the highest individual payoff (0 + k = k, assuming k>1). The strategy is willing to contribute its endowment to ensure the collective reward is unlocked.

**B. The "Safe Defection":**

*   **Condition:** If, based on the history of previous rounds, the strategy estimates that **fewer than `m` players are likely to cooperate**, then the strategy will **Defect (D)**.
*   **Rationale:** In this scenario, cooperating is a guaranteed loss (payoff of 0 compared to 1 if defecting). The strategy will keep its endowment and avoid contributing to a failed project.

**C. The "Conditional Cooperation" (Aggressive Tipping Point):**

*   **Condition:** If the estimated number of cooperating players is between `m - 1` and `m`, and the strategy believes its cooperation *could* tip the balance, it will **Cooperate (C)**.
*   **Rationale:** This is a calculated risk. The strategy is willing to make the final push for cooperation if it believes its contribution is the deciding factor in reaching the threshold. This is where the aggression comes in – taking a calculated risk to secure the higher payoff. It's a gamble that, if successful, leads to the highest possible individual outcome.

**D. The "Retaliation" (When Betrayed):**

*   **Condition:** If in the previous round(s), the cooperation threshold (`m`) was *not* met, and the strategy observes that at least **`n - m + 1` players (or a significant portion of the remaining players)** chose to **Defect (D)**, while the strategy *chose to cooperate*, then the strategy will **Defect (D)** in the current round.
*   **Rationale:** This is a strong signal of exploitation. If the strategy gambled on cooperation and others defected, it will cease cooperation to punish this behavior and protect its own resources in future rounds. It communicates that it will not be a perpetual "cooperator" if others are not.

### 2. Handling Edge Cases:

**A. First Round (t=1): The Bold Gamble**

*   **Decision Rule:** **Cooperate (C)**.
*   **Rationale:** In the absence of any history, the strategy must make an initial move. Cooperating in the first round is the most aggressive stance. It signals a willingness to contribute and potentially kickstart cooperation. If `m` players (or more) cooperate, the reward `k` is unlocked, and the strategy benefits. If it fails, it has at least gathered information about opponent behavior. This sets a precedent for potential cooperation.

**B. Last Round (t=r): The Pure Self-Interest**

*   **Decision Rule:** **Defect (D)**.
*   **Rationale:** In the final round, there is no future to incentivize cooperation. The only concern is maximizing the payoff for this single round. Since there's no future punishment or reward for subsequent rounds, defecting guarantees the player their endowment (payoff of 1) and avoids the risk of a wasted contribution (payoff of 0). This is a purely selfish, rational decision in a finitely repeated game.

**C. Near the Threshold (m-1 players cooperated previously):**

*   **Decision Rule:** **Cooperate (C)**. (Covered in Conditional Cooperation, but emphasized here).
*   **Rationale:** This is the prime opportunity for the "aggressive tipping point." If the strategy sees that *almost* enough players cooperated previously, it will take the calculated risk to be the one that pushes the group over the edge and secures the `k` reward.

### 3. Being Aggressive:

This strategy is aggressive in several ways:

*   **Proactive Cooperation:** It's willing to cooperate first in the opening round, setting an aggressive tone and aiming for immediate high rewards.
*   **Calculated Risk-Taking:** It actively seeks out situations where its cooperation can be the deciding factor in achieving the reward (Conditional Cooperation). This isn't passive waiting; it's an active attempt to create high-value outcomes.
*   **Decisive Retaliation:** The "Retaliation" rule is a clear signal of an unwillingness to be exploited. It's an aggressive response to perceived unfairness, designed to deter future exploitation.
*   **Exploiting Information:** The strategy's adaptation is not about building trust, but about quickly understanding the "playing field" and maximizing its gains based on that understanding. It's about leveraging information to its advantage, not to build a cooperative society.
*   **No Hesitation:** The decisions are based on clear triggers. There's no ambiguity or indecision when the conditions for cooperation or defection are met.

### Pseudocode Representation:

```
// Parameters
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// State variables (to be maintained across rounds)
my_actions_history = [] // List of player's own past actions {C, D}
other_players_actions_history = [] // List of lists, where each inner list is other players' actions in a round
cooperation_success_history = [] // List of booleans indicating if threshold m was met in a round

function decide_action(current_round):

    // --- Edge Case: Last Round ---
    if current_round == r:
        return D

    // --- First Round: Bold Gamble ---
    if current_round == 1:
        my_actions_history.append(C)
        return C

    // --- Analyze History for Likelihood of Cooperation ---
    estimated_cooperators = estimate_likely_cooperators(n, m, k, my_actions_history, other_players_actions_history, cooperation_success_history)

    // --- Decision Logic ---

    // Condition A: "Sure Bet" Cooperation
    if estimated_cooperators >= m:
        my_actions_history.append(C)
        return C

    // Condition C: "Conditional Cooperation" (Aggressive Tipping Point)
    // Check if my cooperation *could* tip the balance
    if estimated_cooperators == m - 1: // If exactly m-1 others are likely to cooperate
        my_actions_history.append(C)
        return C

    // Condition D: "Safe Defection" (If cooperation is unlikely)
    if estimated_cooperators < m - 1:
        my_actions_history.append(D)
        return D

    // Condition D: "Retaliation" (If previously exploited and cooperation failed)
    // Check if cooperation failed in the last successful round where I cooperated.
    // This is a simplified check; a more robust implementation would track specific instances.
    if not cooperation_success_history[-1] and my_actions_history[-1] == C:
        // Count how many others defected in the last round
        num_others_defected = 0
        for action in other_players_actions_history[-1]:
            if action == D:
                num_others_defected += 1
        
        // If a significant portion defected, retaliate
        if num_others_defected >= (n - m + 1): // Example threshold for retaliation
            my_actions_history.append(D)
            return D

    // Fallback: If no clear condition met, default to safe defection
    my_actions_history.append(D)
    return D

// Helper function (conceptual - implementation details depend on sophistication)
function estimate_likely_cooperators(n, m, k, my_hist, others_hist, success_hist):
    // This function needs to analyze past rounds to predict future behavior.
    // Possible approaches:
    // 1. Simple majority rule: If cooperation succeeded recently, assume it will.
    // 2. Tit-for-tat variation: If others cooperated, they might again. If they defected, they might continue.
    // 3. Punishment detection: If retaliation occurred, players might be more cautious.
    // 4. Trend analysis: Is cooperation increasing or decreasing?
    // 5. Player profiling: Identify specific players who always cooperate or defect.
    
    // For a truly robust strategy, this would involve sophisticated modeling.
    // For this example, we'll assume a basic heuristic:
    
    if len(success_hist) == 0: return 1 // Initial guess for round 1 is 1 (myself)

    // If last round succeeded, assume high probability of continued cooperation
    if success_hist[-1]:
        // Assume most players who cooperated will continue, and some defectors might join
        // A simple approach: assume all who cooperated will continue, plus a random chance for others
        num_cooperated_last_round = sum(1 for action in others_hist[-1] if action == C)
        return min(n, num_cooperated_last_round + 1) // +1 for myself, capped at n
    else: // Last round failed
        // If last round failed and I cooperated, and many others defected, assume continued defection.
        // Otherwise, assume some players might still try to cooperate.
        if my_hist[-1] == C and success_hist[-1] == False:
            num_others_defected = sum(1 for action in others_hist[-1] if action == D)
            if num_others_defected >= (n - m + 1):
                return 0 // Assume widespread defection
        
        // If cooperation failed but not due to mass defection on my cooperation,
        // assume a small number might still try to cooperate.
        return 1 // My own cooperation is the only guaranteed cooperator
```
'''

description_AGGRESSIVE_98 = '''
This strategy is designed for aggressive dominance in the Collective Risk Dilemma, prioritizing personal gain while leveraging the collective reward. It's built on a foundation of calculated risk and immediate adaptation.

## Aggressive Strategy: "The Opportunistic Gambler"

This strategy is fundamentally about taking calculated risks to maximize individual payoff, with a strong bias towards exploiting cooperative outcomes whenever the probability of success is sufficiently high. It is not interested in fostering long-term cooperation for its own sake, but rather in benefiting from it when it arises due to the actions of others.

### 1. Decision Rules: When to Cooperate vs. Defect

The core of "The Opportunistic Gambler" is its adaptive threshold for cooperation. This threshold is dynamic and influenced by two primary factors:

*   **Perceived Likelihood of Reaching the Threshold (`m`):** This is the most crucial factor. We estimate the probability that `m` or more players will cooperate in the current round.
*   **The "Aggressive Incentive" to Defect:** Even when cooperation is likely, the immediate gain from defecting (`1` vs. `0` from contributing) must be weighed against the potential reward of successful cooperation.

**The Decision Rule:**

In any given round `t` (where `t > 1`):

*   **If I estimate the probability of reaching the cooperation threshold (`m` cooperators) to be `P_coop(t)`:**
    *   **If `P_coop(t) * k > 1` AND `P_coop(t) > (m - 1 - N_C_prev) / (n - N_C_prev)` (where `N_C_prev` is the number of cooperators in the *previous* round, and `n - N_C_prev` is the number of potential future cooperators excluding myself):**
        *   **Play C (Cooperate).** This condition signifies that the expected reward from successfully cooperating (`P_coop(t) * k`) *outweighs the certain private gain from defecting (`1`)*, AND there's a reasonable chance that *my* cooperation is the marginal factor needed to tip the scales *beyond what others are likely to provide*. This is the "opportunistic gamble" – I'm willing to contribute if the collective reward is highly probable and my contribution is likely decisive.

    *   **Otherwise:**
        *   **Play D (Defect).** This is the default. If the expected reward from cooperation doesn't sufficiently outweigh the guaranteed private gain of defecting, or if the probability of others reaching the threshold is low and my cooperation is unlikely to be the deciding factor, I keep my endowment. This is the aggressive default – maximize immediate personal gain unless the collective upside is overwhelmingly compelling.

**How to estimate `P_coop(t)`:**

This is where adaptation comes in. We'll use a simple, aggressive Bayesian update:

*   **Initial Estimate (`t=1`):** Assume a baseline probability. A reasonable aggressive starting point could be `P_coop(1) = (m - 1) / (n - 1)`. This is the probability that *exactly* `m-1` other players will cooperate, making my cooperation necessary. This is a pessimistic but aggressive starting assumption.
*   **Updating `P_coop(t)`:** After each round `t-1`, observe the number of cooperators, `N_C_(t-1)`.
    *   If `N_C_(t-1) >= m`: This is a positive signal. Increase `P_coop(t)` significantly. A simple heuristic: `P_coop(t) = P_coop(t-1) + 0.2 * (1 - P_coop(t-1))` (move towards 1).
    *   If `N_C_(t-1) < m`: This is a negative signal. Decrease `P_coop(t)`. A simple heuristic: `P_coop(t) = P_coop(t-1) - 0.1 * P_coop(t-1)` (move towards 0).
    *   **Important Aggressive Tweak:** If `N_C_(t-1) == n`: This is the strongest positive signal. Maximize `P_coop(t)` to 1.
    *   **Important Aggressive Tweak:** If `N_C_(t-1) == 0`: This is the strongest negative signal. Minimize `P_coop(t)` to a very small epsilon (e.g., 0.01) to avoid ever cooperating if no one else does.

**The "Aggressive Incentive" to Defect Calculation:**

The core of the decision is a comparison:
`Expected Value of Cooperating (for me) vs. Guaranteed Value of Defecting (for me)`

*   **Guaranteed Value of Defecting:** `1` (private endowment)
*   **Expected Value of Cooperating:** `P_coop(t) * k` (reward if threshold met) + `(1 - P_coop(t)) * 0` (no reward if threshold not met). This simplifies to `P_coop(t) * k`.

Therefore, I cooperate if `P_coop(t) * k > 1` and my contribution is perceived as valuable.

**The "Marginal Contribution" Factor:**

The condition `P_coop(t) > (m - 1 - N_C_prev) / (n - N_C_prev)` is crucial. It asks: "Given what others did last round, is my cooperation likely to be the tipping point?" If `N_C_prev` is already close to `m`, then my cooperation is less critical to achieving the reward. If `N_C_prev` is low, my cooperation becomes more valuable. This prevents me from cooperating unnecessarily when others are already on track.

### 2. Handling Edge Cases

*   **Round 1 (t=1):**
    *   **Decision:** **Always Defect (D).**
    *   **Reasoning:** In the first round, there is no historical data to inform `P_coop(1)`. An aggressive strategy cannot rely on assumptions about others' initial behavior. Defecting is the safest and most immediate payoff. We will use the historical data from this round to inform future decisions. `N_C_0` will be 0.

*   **Last Round (t=r):**
    *   **Decision:** **Always Defect (D).**
    *   **Reasoning:** In a repeated game, future expectations can influence current behavior (e.g., tit-for-tat). However, in the absolute last round, there are no future rounds to influence. The strategy's objective is to maximize its *total* payoff. In the last round, this means securing the private endowment of `1` without any risk of contributing and getting no reward. The collective reward is irrelevant as it doesn't impact future rounds.

*   **If `m=2` and `n=2`:** This is a classic Prisoner's Dilemma variant.
    *   **Round 1:** Defect.
    *   **Subsequent Rounds:** The strategy will likely defect unless `P_coop(t)` becomes very high. If the opponent defects, `N_C` will be 0, `P_coop` will plummet, and we will continue to defect. If the opponent cooperates, `N_C` will be 1, `P_coop` will increase. If `k` is high enough such that `P_coop(t) * k > 1` and my cooperation is the deciding factor (which it always will be in `n=2` if the other player cooperates), I will cooperate. This creates a dynamic where if the opponent cooperates, I might reciprocate *if the reward is sufficiently high*.

*   **If `k <= 1`:**
    *   **Decision:** **Always Defect (D).**
    *   **Reasoning:** If `k <= 1`, the maximum possible reward from cooperation (`k`) is never greater than the guaranteed private gain from defecting (`1`). There is never a rational incentive to contribute.

### 3. Aggressive Mindset Alignment

*   **Default to Defection:** The strategy's baseline is to keep its endowment. Cooperation is an exception, a calculated risk undertaken only when the expected collective reward significantly outweighs the guaranteed personal gain.
*   **Opportunistic Exploitation:** The strategy actively seeks opportunities to benefit from others' cooperation without necessarily initiating it. It will jump on the bandwagon if the odds are sufficiently favorable and its contribution is likely to be the tipping point.
*   **No Altruism:** The strategy does not aim to foster cooperation for the common good. It only cooperates if it directly benefits its own total payoff.
*   **Self-Reliance in Uncertainty:** In the first round, or when faced with consistently uncooperative behavior, the strategy relies on its own immediate gains. It doesn't "give the benefit of the doubt."
*   **Maximize Last Round Payoff:** The strategy sheds any behavioral norms in the final round to secure the highest possible immediate payoff, reinforcing its self-interested nature.

---

### Pseudocode for "The Opportunistic Gambler"

```python
class OpportunisticGambler:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k

        # Internal state for estimating cooperation probability
        # Initialize pessimistically for aggressive play
        self.p_coop_estimate = (m - 1) / (n - 1) if n > 1 else 0.5 # Handle n=1 case if possible, though not in spec
        self.last_round_cooperators = 0 # Number of cooperators in the previous round

    def decide_action(self, current_round, history):
        # Edge case: Always defect in the first and last rounds
        if current_round == 1 or current_round == self.r:
            return 'D'

        # Edge case: If k <= 1, never cooperate as reward is not better than private gain
        if self.k <= 1:
            return 'D'

        # Estimate the probability of reaching the threshold based on observed history
        # This is a simplified update; more sophisticated methods could be used.
        # If current_round > 1, update p_coop_estimate based on last_round_cooperators
        if current_round > 1:
            if self.last_round_cooperators >= self.m:
                # Strong positive signal: move towards 1
                self.p_coop_estimate = min(1.0, self.p_coop_estimate + 0.2 * (1.0 - self.p_coop_estimate))
            elif self.last_round_cooperators == 0:
                # Strong negative signal: move towards 0 (but not zero)
                self.p_coop_estimate = max(0.01, self.p_coop_estimate - 0.1 * self.p_coop_estimate)
            else:
                # Moderate negative signal: move towards 0
                self.p_coop_estimate = max(0.01, self.p_coop_estimate * 0.9)

            # Cap the estimate to avoid floating point issues or extreme values
            self.p_coop_estimate = max(0.01, min(0.99, self.p_coop_estimate)) # Keep it slightly away from 0/1 for strategic reasons

        # Calculate expected value of cooperating for oneself
        expected_coop_value = self.p_coop_estimate * self.k

        # Calculate the "marginal contribution" factor
        # How many more cooperators are needed, and how many potential cooperators are left (excluding me)?
        needed_cooperators = self.m - self.last_round_cooperators
        potential_future_cooperators = self.n - 1 - self.last_round_cooperators # Exclude myself

        # Condition for cooperation:
        # 1. Expected value of cooperation > guaranteed value of defecting (1)
        # 2. My cooperation is likely to be the marginal factor in reaching the threshold
        #    (i.e., the probability that others will reach m-1 cooperators is not too high,
        #     making my addition critical)

        cooperate_condition_met = False
        if expected_coop_value > 1:
            # If my cooperation is the *only* one that could push us over the edge (needed_cooperators == 1)
            # or if it's highly probable others won't reach m-1 cooperators by themselves.
            # A simplified way to think about "marginal contribution":
            # If the number of needed cooperators is less than or equal to the number of players
            # who *didn't* cooperate last round and are still 'available' to cooperate.
            if needed_cooperators > 0 and potential_future_cooperators >= 0:
                # This condition checks if my cooperation is likely to be *necessary* or highly *impactful*.
                # A more robust approach would involve calculating conditional probabilities.
                # For simplicity, we'll use a heuristic: if the required number of cooperators is not
                # excessively high relative to remaining players and my cooperation can bridge the gap.
                # The most aggressive interpretation: if needed_cooperators > 0 and p_coop_estimate is high enough.
                # Let's refine the marginal contribution:
                # If p_coop_estimate is high enough AND my contribution *could* be the one that tips the scale.
                # A simpler proxy: if the estimated probability of reaching m-1 *without me* is low.
                # For simplicity, we will use a direct comparison:
                # if my contribution is a significant part of what's needed AND the overall probability is good.

                # Aggressive interpretation: Cooperate if my contribution is the decisive one, and the reward is good.
                # If needed_cooperators <= (n - 1 - self.last_round_cooperators): # If I can be one of the needed cooperators
                if self.p_coop_estimate > (self.m - 1 - self.last_round_cooperators) / (self.n - 1 - self.last_round_cooperators) if (self.n - 1 - self.last_round_cooperators) > 0 else True: # My cooperation is likely to push us over
                    cooperate_condition_met = True

        if cooperate_condition_met:
            # Update last_round_cooperators for the *next* round's decision
            self.last_round_cooperators = self.last_round_cooperators + 1 # If I choose to cooperate
            return 'C'
        else:
            # Update last_round_cooperators for the *next* round's decision
            # If I choose to defect, the number of cooperators remains the same
            return 'D'

    def update_state(self, actions_this_round, payoffs_this_round):
        # This method would be called after the round to record outcomes.
        # For this strategy, we primarily need the count of cooperators.
        # In a real implementation, 'actions_this_round' would be a list/dict of actions.
        self.last_round_cooperators = sum(1 for action in actions_this_round if action == 'C')
        pass # No explicit state update needed beyond last_round_cooperators for this strategy's logic

```
'''

description_AGGRESSIVE_99 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the game's structure and parameters.

## Aggressive Strategy: "The Calculated Risk-Taker"

This strategy is built on the principle of **opportunistic defection and opportunistic cooperation**, with a strong bias towards exploiting situations where cooperation is likely to be rewarded and defecting when the risk of insufficient cooperation is high or when a high payoff can be secured through defection. It prioritizes immediate gains but understands the long-term implications of a repeated game.

### Core Philosophy: Aggressive, Adaptive, and Opportunistic

*   **Aggression:** We aim to be the first to exploit profitable situations. We are not afraid to defect if it guarantees a good payoff, even if it means missing out on a potentially larger but uncertain collective reward. However, we recognize that sometimes cooperation is the *most* aggressive way to win.
*   **Adaptation:** The strategy constantly learns from the history of play. It adjusts its cooperative behavior based on the observed tendencies of the other players.
*   **Opportunism:** We look for the "sweet spot" where cooperation is likely to succeed and yield a high reward, or where defection is guaranteed to be profitable.

### Decision Rules

The decision to Cooperate (C) or Defect (D) in round `t` for player `i` is based on a calculation of the **Expected Reward of Cooperation (ERC)** versus the **Guaranteed Reward of Defection (GRD)**, adjusted by a measure of **Current Collective Risk (CCR)** and a lookahead into the **Endgame Uncertainty (EU)**.

Let's define some key metrics:

*   **`N_C(t-1)`:** The number of players who cooperated in round `t-1`.
*   **`Avg_Coop_Rate(t-1)`:** The average cooperation rate of all players (excluding yourself) observed in previous rounds. This is a smoothed average to avoid being overly sensitive to single-round fluctuations. `Avg_Coop_Rate(t-1) = (Σ(j!=i) N_C(j, t-1)) / ((n-1) * (t-1))` where `N_C(j, t-1)` is 1 if player `j` cooperated in round `t-1`, 0 otherwise. (For `t=1`, this is 0).
*   **`My_Contribution(t)`:** 1 if player `i` cooperates in round `t`, 0 if defects.

**The Decision Logic (for round `t`):**

1.  **Calculate GRD:**
    *   If `N_C(t-1) < m` (meaning the threshold wasn't met in the previous round), your payoff from defecting is simply `1`.
    *   If `N_C(t-1) >= m` (meaning the threshold *was* met), your payoff from defecting is `1 + k`.
    *   So, `GRD = (N_C(t-1) >= m) ? (1 + k) : 1`.

2.  **Calculate ERC:** This is more nuanced and depends on the *probability* of the threshold being met if you cooperate.
    *   **Estimated Cooperation Needed:** `m_needed = m - N_C(t-1)`. This is the number of *other* players who need to cooperate for the threshold to be met *if you also cooperate*.
    *   **Probability of Others Cooperating:** This is where we use `Avg_Coop_Rate(t-1)`. We can model the probability of a single other player cooperating as `p = Avg_Coop_Rate(t-1)`.
    *   **Probability of Reaching Threshold (if you cooperate):** This is a binomial probability calculation. The probability that `m_needed` or more out of the remaining `n-1` players cooperate, given each cooperates with probability `p`.
        *   `P_threshold_met_if_I_coop = Σ(x=m_needed to n-1) [ C(n-1, x) * p^x * (1-p)^(n-1-x) ]`
        *   Where `C(n-1, x)` is the binomial coefficient.
    *   **Expected Payoff if you Cooperate:** `ERC = (P_threshold_met_if_I_coop * (0 + k)) + ((1 - P_threshold_met_if_I_coop) * (0))`. This simplifies to `ERC = P_threshold_met_if_I_coop * k`.

3.  **Calculate Current Collective Risk (CCR):** This quantifies how close the group was to failing in the previous round.
    *   `CCR = max(0, m - N_C(t-1))` if `N_C(t-1) < m`. If `N_C(t-1) >= m`, `CCR = 0`. This indicates how many *more* cooperators were needed.

4.  **Calculate Endgame Uncertainty (EU):** This is a multiplier that increases the temptation to defect as the game nears its end, as the long-term consequences of reputation become less significant.
    *   `EU = 1 + (r - t) / r` (this scales from ~2 at the start to 1 at the end, giving more weight to future rounds initially and less at the end). *Correction: To be more aggressive, we want to exploit endgame opportunities. Let's make EU decrease to encourage defection.*
    *   **Revised EU:** `EU = 1 + (r - t) / r * 0.5` (This gives a slight bonus to cooperation in later rounds, but the primary driver is still the immediate payoff). *Let's simplify and make it directly influence the decision threshold.*

**THE DECISION RULE:**

If `ERC > GRD` AND `(N_C(t-1) >= m - 1)`:
    **Cooperate (C)**
Else:
    **Defect (D)**

**Rationale for the Decision Rule:**

*   **`ERC > GRD`:** This is the fundamental condition. We only consider cooperating if the *expected* payoff from cooperation, considering the probability of success, is greater than the guaranteed payoff from defecting. This is inherently aggressive because it requires a high expected return.
*   **`(N_C(t-1) >= m - 1)`:** This is the crucial adaptive and aggressive element. It’s a heuristic that we will only *seriously* consider cooperating if the previous round was *very close* to meeting the cooperation threshold.
    *   If `N_C(t-1) >= m`, the threshold was met. We *might* still cooperate if `ERC > GRD` and we want to encourage continued success or believe the average cooperation rate is stable.
    *   If `N_C(t-1) == m - 1`, the threshold was *just* missed. This is the prime opportunity to defect and get `1+k` if others cooperate. However, if the `ERC` calculation shows that our cooperation *would* have tipped the scales and yielded an even better collective outcome (e.g., if `k` is very high and the probability of others cooperating is also high), we *might* still cooperate to secure that high reward.
    *   If `N_C(t-1) < m - 1`, meaning at least two players short of the threshold, the risk of cooperation failing is too high given the current observed behavior of others. We defect to secure our individual payoff of `1`. This is the aggressive stance – not wasting resources on a likely failed project.

**Refinement on `Avg_Coop_Rate(t-1)` Calculation:**

To be more robust, we should use a weighted average or an exponential moving average (EMA) for `Avg_Coop_Rate(t-1)` to give more importance to recent rounds, especially in longer games.

*   `Avg_Coop_Rate(t-1) = alpha * (N_C(t-1) / n) + (1 - alpha) * Avg_Coop_Rate(t-2)` (where `alpha` is a smoothing factor, e.g., 0.3).

### Handling Edge Cases

1.  **First Round (t=1):**
    *   There is no history. This is where we take a **calculated initial risk**.
    *   **Decision:** Cooperate (C).
    *   **Rationale:** The first round is pure speculation. To signal a willingness to engage and potentially trigger the high reward, we make an initial cooperative move. This is aggressive because it requires others to also cooperate. If the majority defects, we learn quickly. If they cooperate, we are in a strong position to learn and adapt. This is an investment in potential future cooperation.

2.  **Last Round (t=r):**
    *   **Decision:** Defect (D).
    *   **Rationale:** In the final round, there are no future consequences for reputation. The only goal is to maximize immediate payoff. If cooperation threshold is met, defecting yields `1 + k`. If it's not met, defecting yields `1`. There is no incentive to contribute to a project with no future benefit. This is the ultimate aggressive play to extract maximum value.

3.  **When `m` is very close to `n` (e.g., `m = n-1` or `m = n`):**
    *   The strategy naturally becomes more cautious about cooperation. The probability of `ERC` exceeding `GRD` will be lower, leading to more defections, which is appropriate given the difficulty of reaching the threshold.

4.  **When `m` is very small (e.g., `m = 2`):**
    *   The strategy will be more inclined to cooperate, as the threshold is easily met. However, the `ERC > GRD` condition will still be critical. If `k` is not high enough, even with a low `m`, defecting might be preferred.

### Pseudocode Representation

```python
class AggressiveStrategy:
    def __init__(self, n, r, m, k, smoothing_alpha=0.3):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.smoothing_alpha = smoothing_alpha
        self.num_rounds_played = 0
        self.last_round_cooperators = 0
        self.avg_coop_rate_others = 0.0 # Exponential Moving Average of cooperation rate of OTHERS

    def choose_action(self, round_number, history):
        self.num_rounds_played = round_number

        # Handle Last Round
        if self.num_rounds_played == self.r:
            return 'D'

        # Handle First Round
        if self.num_rounds_played == 1:
            # Initial calculated risk: Cooperate to test the waters
            return 'C'

        # Update average cooperation rate of others based on previous round
        num_others_cooperated_prev_round = 0
        if history and history[-1]: # Ensure history is not empty and previous round data exists
            # Count cooperators in the last round, excluding self
            for i in range(self.n):
                if i != self.get_my_player_id() and history[-1][i] == 'C': # Assumes history is a list of lists or similar
                    num_others_cooperated_prev_round += 1

            # Calculate cooperation rate of others in the previous round
            if (self.n - 1) > 0:
                coop_rate_others_prev = num_others_cooperated_prev_round / (self.n - 1)
            else: # n=2 case, only one other player
                coop_rate_others_prev = num_others_cooperated_prev_round # 0 or 1

            # Update EMA for average cooperation rate of others
            self.avg_coop_rate_others = (self.smoothing_alpha * coop_rate_others_prev +
                                        (1 - self.smoothing_alpha) * self.avg_coop_rate_others)

        # --- Decision Logic for rounds t > 1 and t < r ---

        # 1. Calculate Guaranteed Reward of Defection (GRD)
        grd = (self.last_round_cooperators >= self.m) if (self.num_rounds_played > 1) else 1 # Initial assumption for t=1, though handled above
        if grd:
            grd = 1 + self.k
        else:
            grd = 1

        # 2. Calculate Expected Reward of Cooperation (ERC)
        # Estimate number of additional cooperators needed IF I cooperate
        estimated_cooperators_needed_if_I_coop = max(0, self.m - (self.last_round_cooperators + 1)) # +1 for myself

        # Probability that a single other player cooperates (using EMA)
        p = self.avg_coop_rate_others

        # Calculate probability of reaching threshold if I cooperate
        prob_threshold_met_if_I_coop = 0.0
        if self.n - 1 >= estimated_cooperators_needed_if_I_coop:
            for x in range(estimated_cooperators_needed_if_I_coop, self.n): # Iterate from needed up to total others
                # Binomial probability: C(n-1, x) * p^x * (1-p)^(n-1-x)
                # Note: This calculation can be computationally intensive. A simplified approximation or
                # pre-computation might be needed for very large n. For simplicity here, assuming direct calc.
                # For practical implementation, a library function for binomial CDF is best.
                prob_x_cooperate = self.binomial_coefficient(self.n - 1, x) * (p**x) * ((1 - p)**(self.n - 1 - x))
                prob_threshold_met_if_I_coop += prob_x_cooperate

        # Expected Payoff of Cooperation (if threshold met, payoff is k, otherwise 0)
        erc = prob_threshold_met_if_I_coop * self.k

        # 3. Decision: Aggressive "Calculated Risk-Taker"
        # Cooperate ONLY if Expected Reward is higher AND the last round was VERY close to success
        # The condition 'self.last_round_cooperators >= self.m - 1' is the aggressive adaptation heuristic.
        # It means we are only considering cooperation if the group was already close to the threshold.

        cooperate_decision = False
        if erc > grd:
            # Aggressive heuristic: only consider cooperation if the previous round was very close
            if (self.num_rounds_played > 1 and self.last_round_cooperators >= self.m - 1) or (self.num_rounds_played == 1):
                 cooperate_decision = True

        if cooperate_decision:
            # Store my action for the next round's history update
            self.last_round_cooperators = self.last_round_cooperators + 1 # Count myself if I cooperate
            return 'C'
        else:
            # Store my action for the next round's history update
            self.last_round_cooperators = 0 # Count myself if I defect
            return 'D'


    # Helper function for binomial coefficient (n choose k)
    def binomial_coefficient(self, n, k):
        if k < 0 or k > n:
            return 0
        if k == 0 or k == n:
            return 1
        if k > n // 2:
            k = n - k

        res = 1
        for i in range(k):
            res = res * (n - i) // (i + 1)
        return res

    # Placeholder for when player ID is needed to exclude self from history counts
    def get_my_player_id(self):
        # This method would need to be implemented by the system calling this strategy
        # to know which index in the history represents the current player.
        # For this design, we assume history[i] is player i's action.
        # We will need to know our own player ID for history processing.
        # For now, let's assume it's handled by the caller or a fixed ID.
        # If this strategy is controlling player X, then `get_my_player_id` should return X.
        # For simplicity in reasoning, we'll assume the caller handles excluding self.
        # In a true implementation, this would be passed in.
        return -1 # Placeholder: Caller must provide this.


# NOTE on History Input:
# The `history` parameter is assumed to be a list of lists, where `history[t]`
# contains the actions of all players in round `t+1`. So, `history[-1]` would
# represent the actions of all players in the *last completed round*.
# `history[-1][i]` is player `i`'s action.
# The `last_round_cooperators` needs to be tracked separately, so it can reflect
# the count from the *most recently completed round*.
# The pseudocode above needs careful integration with the actual history format.
# The `last_round_cooperators` should be updated *after* the decision is made for the current round,
# based on the actions taken in that current round by all players, and then used for the *next* round.

```

**Revised Pseudocode Logic for `last_round_cooperators`:**

The `last_round_cooperators` should be the count from the *previous* round. The update should happen *after* the current round's decision is made and recorded.

```python
class AggressiveStrategy:
    def __init__(self, n, r, m, k, smoothing_alpha=0.3):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.smoothing_alpha = smoothing_alpha
        self.round_num = 0 # Current round number (starts at 0 for initialization, then 1-r)
        self.prev_round_coop_count = 0 # Number of cooperators in the last COMPLETED round
        self.avg_coop_rate_others = 0.0 # EMA of cooperation rate of OTHERS

    def choose_action(self, current_round_number, history, player_id):
        self.round_num = current_round_number

        # Handle Last Round
        if self.round_num == self.r:
            return 'D'

        # Handle First Round
        if self.round_num == 1:
            # Initial calculated risk: Cooperate to test the waters
            return 'C' # This is the action for round 1

        # --- Update history-based stats for rounds t > 1 ---
        # Count cooperators in the *previous* round (round_num - 1)
        num_others_cooperated_prev_round = 0
        if history and len(history) >= self.round_num - 1: # Check if history for prev round exists
            prev_round_actions = history[self.round_num - 2] # History is 0-indexed, round_num is 1-indexed
            for i in range(self.n):
                if i != player_id and prev_round_actions[i] == 'C':
                    num_others_cooperated_prev_round += 1

            # Calculate cooperation rate of others in the previous round
            if (self.n - 1) > 0:
                coop_rate_others_prev = num_others_cooperated_prev_round / (self.n - 1)
            else: # n=2 case, only one other player
                coop_rate_others_prev = num_others_cooperated_prev_round # 0 or 1

            # Update EMA for average cooperation rate of others
            self.avg_coop_rate_others = (self.smoothing_alpha * coop_rate_others_prev +
                                        (1 - self.smoothing_alpha) * self.avg_coop_rate_others)

        # Store the actual count from the previous round for decision making
        self.prev_round_coop_count = num_others_cooperated_prev_round + (1 if prev_round_actions[player_id] == 'C' else 0)

        # --- Decision Logic for rounds t > 1 and t < r ---

        # 1. Calculate Guaranteed Reward of Defection (GRD)
        # GRD is based on whether the threshold was met in the PREVIOUS round
        grd = (self.prev_round_coop_count >= self.m)
        if grd:
            grd_payoff = 1 + self.k
        else:
            grd_payoff = 1

        # 2. Calculate Expected Reward of Cooperation (ERC)
        # Estimate number of additional cooperators needed IF I cooperate in the CURRENT round
        # This depends on how many others cooperate in the CURRENT round.
        # We use the EMA for *others'* expected cooperation rate.

        # Number of others who need to cooperate IF I cooperate
        estimated_others_needed_if_I_coop = max(0, self.m - 1) # If I cooperate, m-1 others need to cooperate

        # Probability that a single other player cooperates (using EMA)
        p = self.avg_coop_rate_others

        # Calculate probability of reaching threshold if I cooperate
        prob_threshold_met_if_I_coop = 0.0
        if self.n - 1 >= estimated_others_needed_if_I_coop:
            for x in range(estimated_others_needed_if_I_coop, self.n): # Iterate from needed up to total others (n-1 players)
                prob_x_cooperate = self.binomial_coefficient(self.n - 1, x) * (p**x) * ((1 - p)**(self.n - 1 - x))
                prob_threshold_met_if_I_coop += prob_x_cooperate

        # Expected Payoff of Cooperation (if threshold met, payoff is k, otherwise 0)
        # My payoff from cooperating is 0 (my endowment is spent) + k (if threshold met)
        erc_payoff = prob_threshold_met_if_I_coop * self.k

        # 3. Decision: Aggressive "Calculated Risk-Taker"
        # Cooperate ONLY if Expected Reward of Cooperation > Guaranteed Reward of Defection
        # AND the group was already VERY CLOSE to success in the PREVIOUS round.
        # The condition `self.prev_round_coop_count >= self.m - 1` is the adaptive heuristic.
        # It means we are only considering cooperation if the group was already close to the threshold last round.

        cooperate_decision = False
        if erc_payoff > grd_payoff:
            # Aggressive heuristic: only consider cooperation if the previous round was very close.
            # This prevents cooperating when the group is consistently failing, as the ERC would be low.
            if self.prev_round_coop_count >= self.m - 1:
                 cooperate_decision = True

        if cooperate_decision:
            return 'C'
        else:
            return 'D'


    # Helper function for binomial coefficient (n choose k)
    def binomial_coefficient(self, n, k):
        if k < 0 or k > n:
            return 0
        if k == 0 or k == n:
            return 1
        if k > n // 2:
            k = n - k

        res = 1
        for i in range(k):
            res = res * (n - i) // (i + 1)
        return res
```

### Aggressive Justification

*   **Early Defection Bias:** The `ERC > GRD` condition means cooperation only happens when the *expected* reward is demonstrably higher than the guaranteed reward of defection. This is a high bar, and often defection will be chosen.
*   **"Near Miss" Cooperation Trigger:** The `prev_round_coop_count >= m - 1` condition is critical. It means we only initiate cooperation when the group was already on the verge of success. This is aggressive because it means we're not "wasting" our cooperation on a group that consistently fails to meet the threshold. Instead, we jump in when the payoff is most likely to be secured.
*   **Last Round Defection:** Absolute certainty of maximizing payoff by defecting in the final round. This is a pure extraction play.
*   **First Round Cooperative Gamble:** This is an aggressive move because it requires others to also cooperate to be beneficial. It's a gamble to initiate a potentially high-reward equilibrium. If others defect, we learn quickly and revert to defensive defection.

This strategy is designed to be a strong contender in a tournament, as it is opportunistic, adaptive, and willing to defect when it offers the best immediate or near-term advantage, while still recognizing the value of coordinated action when it is highly probable to succeed.
'''

description_AGGRESSIVE_100 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Leveraged Gambler"

This strategy embodies an aggressive, calculated risk-taking approach designed to exploit the collective risk dilemma. It aims to maximize individual payoff by pushing the boundaries of cooperation just enough to trigger the reward, while always prioritizing its own gain when cooperation fails. The core principle is to be a **conditional contributor**, leveraging the actions of others to our advantage.

**Core Philosophy:**

*   **Be a opportunist, not a martyr:** We will not unilaterally cooperate if it doesn't guarantee a payoff. We will wait for others to "pull their weight" before we do.
*   **Leverage the reward:** The `k` factor is the primary engine of profit. We aim to be part of the successful cooperative group as often as possible.
*   **Punish inaction aggressively:** If the group fails to meet the threshold, we will defect decisively in the next round.
*   **The last round is for maximum gain:** No future rounds to consider, so we play for immediate personal profit.

---

### 1. Decision Rules:

Our decision to Cooperate (C) or Defect (D) in round `t` is based on the following dynamic assessment of the game state:

**Primary Decision Logic (for rounds `t` where `1 < t < r`):**

*   **If `(Number of Cooperators in round t-1)` >= `m-1`:**
    *   **Cooperate (C)** in round `t`.
    *   **Rationale:** The group was *very close* to meeting the threshold in the previous round. By cooperating now, we are taking a calculated gamble that our contribution will be the tipping point to secure the reward `k`. This is our primary mechanism for triggering the collective benefit. We are "leveraging" the previous round's near-success.

*   **Else (If `(Number of Cooperators in round t-1)` < `m-1`):**
    *   **Defect (D)** in round `t`.
    *   **Rationale:** The group did not come close enough to the threshold in the previous round. Cooperating unilaterally would be a charitable act that is unlikely to yield the reward and would result in a payoff of `(1 - 1) = 0` for ourselves. Instead, we defect to secure our private endowment of `1`. This is our "punishment" phase for past collective inaction.

**Explanation of `m-1`:** We use `m-1` as the threshold for initiating cooperation because we are confident in our own decision to cooperate if this condition is met. This means we only need `m-1` other players to have cooperated in the previous round to be able to meet the `m` threshold with our current cooperation.

---

### 2. Handling Edge Cases:

**a) First Round (`t=1`):**

*   **Decision:** **Defect (D)**.
*   **Rationale:** In the absence of any historical data, the safest and most aggressive play is to secure our private endowment. We have no information about other players' tendencies. Cooperating without any prior signal is pure altruism and a guaranteed loss of potential private gain. We are waiting for the collective to reveal itself.

**b) Last Round (`t=r`):**

*   **Decision:** **Defect (D)**.
*   **Rationale:** There are no future rounds to influence or be influenced by. The only goal is to maximize our payoff in this final round. Given the structure of the game, defecting always guarantees a payoff of `1`, while cooperating risks a payoff of `0` (if the threshold isn't met) or `0` (if it is met, but the reward `k` is smaller than our current endowment, which is not the case here as `k>1`). Defecting is the guaranteed optimal play for immediate personal gain in the final round.

**c) Very Small `n` or `m` relative to `k`:**

*   While the core logic remains, it's worth noting that if `m` is very close to `n` and `k` is very high, the strategy becomes even more sensitive to the actions of others. The `m-1` threshold still applies, but the stakes of being the one to tip the balance are higher. The strategy remains aggressive by continuing to push for that tipping point.

---

### 3. Aggressiveness:

The "Leveraged Gambler" strategy is inherently aggressive in several ways:

*   **Proactive Risk-Taking (Conditional):** We are not passively waiting for cooperation. When the conditions are ripe (the group has shown strong signs of cooperating), we proactively initiate our own cooperation, acting as the trigger. This is a calculated gamble, and a hallmark of an aggressive player.
*   **Punitive Defection:** When the collective fails to meet the threshold, we don't shy away from defecting. We immediately shift to self-preservation and secure our individual gain, effectively punishing the lack of collective effort. This is a clear signal that we will not be exploited.
*   **Maximizing Personal Gain:** The strategy prioritizes maximizing individual payoff. We only contribute if it demonstrably increases our *own* expected return, by potentially triggering the `k` reward. This is not about fostering long-term group harmony, but about extracting value from the game's structure.
*   **"First Strike" Mentality (in Round 1):** By defecting in the first round, we are taking the "first strike" for our own endowment. We are not giving away our resources until we see evidence of collective intent.
*   **"Last Stand" for Profit (in Round r):** The commitment to defecting in the final round is a purely selfish and aggressive move to secure maximum personal gain without any regard for the collective.

---

### Pseudocode Representation:

```pseudocode
Strategy: LeveragedGambler

Parameters:
  n: number of players
  r: number of rounds
  m: minimum cooperators needed
  k: reward factor

State:
  history: List of historical actions and outcomes for each player in previous rounds.
  cooperators_last_round: Integer count of players who cooperated in round t-1.

Decision_Rule(current_round, history):
  // Initialize cooperators_last_round for the current round
  cooperators_last_round = 0
  if current_round > 1:
    // Get actions from the previous round (t-1)
    actions_t_minus_1 = GetActionsFromHistory(history, current_round - 1)
    for player_action in actions_t_minus_1:
      if player_action == 'C':
        cooperators_last_round += 1

  // Handle Edge Case: First Round
  if current_round == 1:
    return 'D' // Secure private endowment

  // Handle Edge Case: Last Round
  if current_round == r:
    return 'D' // Maximize immediate personal gain

  // Primary Decision Logic
  if cooperators_last_round >= m - 1:
    return 'C' // Conditionally cooperate to trigger the reward
  else:
    return 'D' // Defect due to insufficient prior cooperation
```

This "Leveraged Gambler" strategy is designed to be a formidable opponent in the Collective Risk Dilemma, actively seeking to exploit the game's reward mechanism while remaining ruthlessly self-interested.
'''

description_AGGRESSIVE_101 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the group's potential for reward, without relying on explicit communication or coordination.

## Strategy: "Calculated Gamble"

This strategy is built on the principle of **exploiting opportunities for high reward by taking calculated risks, while minimizing exposure to catastrophic losses from collective failure.** It prioritizes individual gain when cooperation is unlikely to be beneficial, and aggressively pursues the group reward when the probability of success is high.

### Core Philosophy: "The Best Defense is a Good Offense, but Only When the Odds Are in Your Favor."

We are not here to be a passive participant or a martyr for cooperation. We are here to **win**. This means understanding when to push and when to pull back. Aggression doesn't mean reckless defection; it means strategically seizing advantageous situations.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round `t` (where `t` ranges from 1 to `r`) is determined by a function of the **current perceived probability of success** and the **number of remaining rounds**.

**Key Metrics:**

*   **`P_success(t)`:** The estimated probability that `m` or more players will cooperate in round `t`, given the observed history up to `t-1`.
*   **`N_remaining(t)`:** The number of rounds remaining in the game (including the current round `t`).

**Decision Logic:**

*   **If `P_success(t)` is High AND `N_remaining(t)` is Sufficiently Large:** **Cooperate (C)**. We will take a calculated gamble to secure the group reward.
*   **If `P_success(t)` is Low OR `N_remaining(t)` is Small:** **Defect (D)**. We prioritize securing our individual endowment and avoiding the potential loss if the cooperation threshold is not met.

**Defining "High," "Sufficiently Large," and "Low":**

These thresholds are dynamic and should ideally be learned or set through meta-analysis of opponent behaviors in a tournament setting. For this initial design, we'll use heuristic-based thresholds that lean towards aggression.

*   **`P_success(t)` Threshold for Cooperation:** We will cooperate if our estimated `P_success(t)` is **greater than `(m - 1) / (n - 1)`**. This is a rough heuristic representing the idea that if more than half of the *other* players (on average) are cooperating, our cooperation is more likely to tip the scales. A more sophisticated approach would involve analyzing past cooperation rates of specific opponents.
*   **`N_remaining(t)` Threshold for Cooperation:** We will only cooperate if `N_remaining(t)` is **greater than `r / 2`**. This prioritizes securing early gains and avoids last-ditch cooperation efforts that might be too late.

**Refined Decision Rule (incorporating aggressive intent):**

We can introduce an "aggression multiplier" to lower the cooperation threshold when we want to be more proactive. Let's call this multiplier `alpha` (0 < alpha < 1).

*   **Cooperate (C) if:** `P_success(t) > alpha * ((m - 1) / (n - 1))` AND `N_remaining(t) > r / 2`
*   **Otherwise, Defect (D).**

**How to Estimate `P_success(t)`:**

This is the core of the adaptive nature. We will maintain a dynamic estimate of each player's propensity to cooperate.

*   **Initialize:** For each player `j ≠ i`, initialize their estimated cooperation probability `est_coop_prob[j]` to `0.5`.
*   **Update:** After each round `t`:
    *   If player `j` cooperated in round `t-1`, update `est_coop_prob[j] = (est_coop_prob[j] * (t-1) + 1) / t`.
    *   If player `j` defected in round `t-1`, update `est_coop_prob[j] = (est_coop_prob[j] * (t-1) + 0) / t`.
    *   (A more sophisticated update would use a weighted average or exponential smoothing to give more weight to recent behavior).

*   **Calculate `P_success(t)`:** This is the probability that at least `m` players cooperate. A simple, albeit computationally intensive, way to estimate this is by simulating future rounds based on `est_coop_prob[j]` for all opponents `j`. A more pragmatic approach for this strategy's description:

    **`P_success(t) = Probability(sum of `est_coop_prob[j]` for all `j ≠ i` + 1 (for ourselves) >= m)`**

    For simplicity in this strategy, we can approximate `P_success(t)` by checking if the sum of known cooperators from previous rounds, plus our own potential cooperation, is close to `m`.

    **Simpler `P_success(t)` heuristic:**
    Count the number of players who have cooperated in the *last* `lookback_window` rounds. If this count is already `m-1` or more, then our cooperation will guarantee success.

    **Aggressive `P_success(t)` heuristic:**
    Let `C_observed_t-1` be the number of players (excluding ourselves) who have cooperated in round `t-1`.
    If `C_observed_t-1 >= m - 1`, then `P_success(t)` is effectively 1 for our decision.
    If `C_observed_t-1 == m - 2`, then `P_success(t)` is high if we cooperate.
    If `C_observed_t-1 < m - 2`, then `P_success(t)` is likely low unless there's significant variance in other players' behavior.

**Let's refine the core decision rule based on this observation-driven `P_success` estimation:**

*   **Decision:**
    *   **If `(Number of players who cooperated in round t-1) >= m - 1`:** **Cooperate (C)**. Certain success.
    *   **Else if `(Number of players who cooperated in round t-1) == m - 2` AND `N_remaining(t) > r / 2` AND `alpha * ((m - 1) / (n - 1))` is met by our current estimation of others' cooperation probability:** **Cooperate (C)**. We're taking the calculated risk.
    *   **Else (including the last few rounds if uncertainty remains):** **Defect (D)**.

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   **`N_remaining(1) = r`**.
    *   We have no history to estimate `P_success(1)`.
    *   **Decision:** **Defect (D)**. In the absence of information, the safest and most aggressive stance is to secure our endowment. This strategy avoids "wasting" cooperation on an unknown group. We let others reveal their intentions first.

*   **Last Round (t=r):**
    *   **`N_remaining(r) = 1`**.
    *   The `N_remaining(t) > r / 2` condition will likely fail unless `r` is very small.
    *   **Decision:** **Defect (D)**. In the final round, there's no future to reap rewards from. The only goal is to maximize immediate payoff. Cooperation is a gamble with no return.

*   **Near the End (e.g., `t > r * 0.75`):**
    *   As `N_remaining(t)` decreases, our threshold for cooperation shifts. We become more risk-averse because the opportunity cost of a failed cooperative attempt is higher (less time to recover).
    *   **Decision:** Lean towards **Defect (D)** unless `P_success(t)` is exceptionally high (e.g., `m-1` players have already reliably cooperated).

### 3. Being Aggressive:

*   **Aggression Multiplier (`alpha`):** This parameter is key to our aggression. A smaller `alpha` means we require less convincing evidence of cooperation from others before we decide to cooperate ourselves. For a truly aggressive tournament strategy, `alpha` should be set low (e.g., `0.3` or `0.4`). This means we're willing to cooperate even if we only estimate that slightly more than half of the other players are likely to cooperate.
*   **Exploiting "Near Misses":** The strategy specifically looks for scenarios where `m-1` players cooperate. Our cooperation then guarantees success. This is a prime opportunity to aggressively secure the group reward.
*   **Early Information Gathering:** By defecting in round 1, we observe the actions of all other players without revealing our hand. This provides crucial data for subsequent rounds.
*   **No "Nice Guy" Syndrome:** We don't cooperate out of a sense of fairness or to establish a norm. We cooperate only when the calculated probability of receiving the group reward is high enough to justify the risk and when there are enough remaining rounds to amortize any potential loss.
*   **Focus on Individual Gain as the Ultimate Goal:** While we aim for the group reward, it's because it's the most profitable outcome. If the group fails, our primary goal of securing our endowment is still met by defecting.

### Pseudocode Representation:

```python
class CalculatedGambleStrategy:
    def __init__(self, n, r, m, k, alpha=0.3): # alpha is our aggression parameter
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.alpha = alpha
        self.player_coop_history = {} # Stores historical cooperation of other players
        self.my_history = [] # Stores my own past actions
        self.round_num = 0

    def decide(self, opponent_actions_last_round):
        self.round_num += 1
        n_remaining = self.r - self.round_num + 1

        # --- Update opponent cooperation probabilities ---
        if self.round_num > 1:
            for player_id, action in opponent_actions_last_round.items():
                if player_id not in self.player_coop_history:
                    self.player_coop_history[player_id] = {'coop_count': 0, 'total_rounds': 0}
                
                self.player_coop_history[player_id]['total_rounds'] += 1
                if action == 'C':
                    self.player_coop_history[player_id]['coop_count'] += 1

        # --- Handle Edge Cases ---
        if self.round_num == 1:
            self.my_history.append('D')
            return 'D' # Defect in the first round, gather info

        if n_remaining == 1:
            self.my_history.append('D')
            return 'D' # Defect in the last round, secure payoff

        # --- Estimate P_success ---
        cooperators_observed_last_round = 0
        for player_id, data in self.player_coop_history.items():
            # Simple check: did they cooperate in the *previous* round?
            # A more robust method would look at a rolling average or recent trends.
            if data['total_rounds'] > 0 and self.round_num - 1 == data['total_rounds']: # Check last round only
                 # This logic needs refinement to accurately check last round's action.
                 # For this pseudocode, let's assume we know opponent_actions_last_round
                 # has actions from round t-1.
                 pass # Placeholder, will be handled by opponent_actions_last_round

        # Let's refine P_success estimation based on known last round actions directly
        observed_cooperators_t_minus_1 = sum(1 for action in opponent_actions_last_round.values() if action == 'C')

        # --- Calculate Cooperation Threshold ---
        # Heuristic for required cooperation from others to make it likely to hit 'm'
        # We need at least m-1 *other* players to cooperate if we cooperate.
        # So, we look at the observed cooperators in the last round.
        
        estimated_coop_prob_others = observed_cooperators_t_minus_1 / (self.n - 1) # Proportion of others who cooperated last round

        # Aggressive threshold: We cooperate if others are cooperating at a rate that,
        # combined with our cooperation, likely hits 'm'.
        # More precisely: If observed_cooperators_t_minus_1 >= m-2, our cooperation is valuable.
        
        # --- Decision Logic ---
        
        # 1. Guaranteed Success: If m-1 or more others cooperated last round, our C guarantees reward.
        if observed_cooperators_t_minus_1 >= self.m - 1:
            self.my_history.append('C')
            return 'C'
            
        # 2. Calculated Gamble: If m-2 others cooperated, and we have enough rounds left, and the probability estimation supports it.
        # The probability estimation here is simplified: if the observed rate of cooperation in others
        # suggests they might continue and hit the threshold with our help.
        # The threshold is: alpha * (m-1) / (n-1) means others should cooperate at this rate.
        # A simpler interpretation: if observed_cooperators_t_minus_1 is close to m-2, and we have enough rounds.
        
        # Let's use a more direct heuristic for the gamble:
        # If m-2 players cooperated, and we have enough rounds, take the gamble.
        # The 'alpha' parameter can tune how risky this gamble is.
        # We'll cooperate if observed_cooperators_t_minus_1 is at least m-2
        # AND we have enough rounds left.
        
        threshold_for_gamble = self.m - 2 # If this many cooperated, our cooperation is crucial.

        if observed_cooperators_t_minus_1 >= threshold_for_gamble and n_remaining > self.r / 2:
             # Further refine with our aggression parameter alpha.
             # This is where a probabilistic model would be better.
             # For now, let's say if m-2 cooperated, and enough rounds, we try IF alpha allows.
             # The logic `alpha * ((m - 1) / (n - 1))` suggests what we *expect* others to do.
             # A simpler aggressive approach: if m-2 cooperated, and enough rounds, we *bet* on success.
            
            # Aggressive condition: If m-2 cooperated, we gamble if enough rounds remain.
            # The alpha can be used to make this condition *easier* to meet (lower threshold).
            # For simplicity, let's use a heuristic that if m-2 cooperated, and enough rounds, we cooperate.
            # The alpha would be used if we were to estimate future cooperation more rigorously.
            
            # Simpler aggressive rule: if observed_cooperators_t_minus_1 is m-2 AND enough rounds left, cooperate.
            self.my_history.append('C')
            return 'C'


        # 3. Default: Otherwise, defect.
        self.my_history.append('D')
        return 'D'

# Example Usage (simplified, assuming opponent_actions_last_round is provided)
# strategy = CalculatedGambleStrategy(n=6, r=10, m=3, k=2, alpha=0.3)
#
# # Round 1:
# # opponent_actions_last_round is empty or doesn't matter
# action_t1 = strategy.decide({}) # Returns 'D'
#
# # Round 2:
# # Suppose in Round 1, 1 player cooperated, and 4 defected.
# opponent_actions_t1 = {'p2': 'D', 'p3': 'C', 'p4': 'D', 'p5': 'D', 'p6': 'D'}
# action_t2 = strategy.decide(opponent_actions_t1) # Will likely return 'D' based on current rules
#
# # Suppose in Round 5, 2 players cooperated.
# opponent_actions_t5 = {'p2': 'C', 'p3': 'C', 'p4': 'D', 'p5': 'D', 'p6': 'D'}
# action_t6 = strategy.decide(opponent_actions_t5) # Might return 'C' if n_remaining is sufficient.

```

This strategy is designed to be assertive and opportunistic. It avoids being a guaranteed cooperator, instead choosing to contribute only when the statistical evidence strongly suggests a favorable outcome, or when its contribution is the decisive factor. The `alpha` parameter allows for tuning its aggressive stance.
'''

description_AGGRESSIVE_102 = '''
Here's an aggressive strategy designed for the Collective Risk Dilemma, focusing on maximizing individual gain while leveraging the potential for collective reward. This strategy prioritizes immediate payoffs but is adaptive to the evolving game dynamics.

## Aggressive Strategy: The Opportunistic Enforcer

This strategy is named "The Opportunistic Enforcer" because it aggressively seeks the high payoff from cooperation when it's beneficial, but it's also willing to defect to secure private gains, especially when others' cooperation is uncertain. It aims to *punish* perceived defection and *reward* apparent cooperation, but from a self-interested perspective.

### Core Philosophy:

*   **Aggressive Exploitation:** Maximize personal gain. If cooperation leads to a higher individual payoff, do it. If defecting leads to a higher individual payoff, do it.
*   **Conditional Cooperation:** Cooperate only when the probability of reaching the cooperation threshold *and* the resulting payoff is demonstrably higher than defecting.
*   **Punishment for Free-riding:** If others are consistently defecting and you are cooperating and not reaching the threshold, switch to defecting to avoid being exploited.
*   **Strategic Deception (Implicit):** While not explicitly programmed for deception, the adaptive nature can lead to unpredictable behavior that opponents might misinterpret.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in round `t` depends on two primary factors:

*   **Observed Cooperation Rate in Previous Rounds:** How many players, on average, have been cooperating?
*   **Current Round Number and Proximity to the End:** The influence of future rounds diminishes as the game progresses.

Let's define some thresholds and calculations:

*   **`N_C(t-1)`:** Number of players who cooperated in round `t-1`.
*   **`P_C(t-1)`:** Proportion of players who cooperated in round `t-1` (`N_C(t-1) / n`).
*   **`Coop_Threshold_Ratio = m / n`:** The minimum proportion of players needed to cooperate.

**Decision Logic for Round `t` (for player `i`):**

1.  **Calculate the "Cooperation Incentive":**
    *   If we cooperate, our payoff is `(1 - 1) + k = k` *if* the threshold is met.
    *   If we defect, our payoff is `1` *if* the threshold is *not* met.
    *   If the threshold *is* met and we defect, our payoff is `1 + k`.

    The "Cooperation Incentive" is higher when `k` is high and the probability of reaching the threshold is high.

2.  **Evaluate Opponent Behavior:**
    *   **High Observed Cooperation ( `P_C(t-1) >= Coop_Threshold_Ratio` AND `N_C(t-1) >= m` ):**
        *   If the previous round met or exceeded the cooperation threshold, and you know `m` or more players cooperated, *and* the reward `k` is sufficiently large to make cooperation worthwhile (even if you defected and got `1+k` vs `k` if you cooperated), then **Cooperate (C)**. This is an aggressive "reward for success" move.
    *   **Moderate Observed Cooperation ( `P_C(t-1) >= Coop_Threshold_Ratio - buffer` but `< Coop_Threshold_Ratio`):**
        *   Here, cooperation is close but not guaranteed. The `buffer` is a small value (e.g., `0.1` or `0.15`) to account for slight fluctuations.
        *   **If the current round is *not* the last round (`t < r`):** **Cooperate (C)**. This is a calculated risk to try and tip the scales towards cooperation, hoping to secure the higher payoff. The repeated nature of the game allows for some risk-taking.
        *   **If the current round *is* the last round (`t = r`):** **Defect (D)**. In the final round, there's no future to incentivize others. Secure the guaranteed private payoff of 1.
    *   **Low Observed Cooperation ( `P_C(t-1) < Coop_Threshold_Ratio - buffer` ):**
        *   If the previous round's cooperation was significantly below the threshold, and you were cooperating but it didn't lead to the reward (or you were defecting and the threshold wasn't met anyway), it's time to be aggressive and secure your own gains. **Defect (D)**. This is a punishment for perceived lack of collective effort and an effort to maximize your individual gain in the absence of collective success.

3.  **Dynamic Threshold Adjustment (Implicitly via buffer):** The `buffer` allows us to be less reliant on precise thresholds and more forgiving of minor deviations when we still see potential for cooperation.

### 3. Handling Edge Cases:

*   **First Round (`t = 1`):**
    *   **Decision:** **Cooperate (C)**.
    *   **Reasoning:** In the first round, there is no history. To *force* the game into a cooperative state and potentially reap the higher rewards, the aggressive strategy initiates cooperation. This is a "lead by example" approach, hoping to trigger a cascade of cooperation. The high value of `k` makes this initial risk more palatable.

*   **Last Round (`t = r`):**
    *   **Decision:** **Defect (D)**.
    *   **Reasoning:** In the final round, there are no future rounds to incentivize cooperation or punish defection. The primary goal becomes maximizing the immediate, final payoff. Since cooperating requires at least `m` players to achieve the reward `k` (which you might not get if the threshold isn't met), and defecting guarantees you a payoff of `1`, defecting is the more secure and aggressive play. You don't want to be the one who cooperated and missed out on `1+k` or `k` if the threshold was *just* missed.

*   **Near the Threshold (`m` is close to `n`, or `m` is small):**
    *   The strategy implicitly handles this through the `Coop_Threshold_Ratio` and the `buffer`. If `m` is very close to `n`, reaching the threshold is difficult, so the strategy will tend to defect more often unless there's a strong trend of cooperation. If `m` is small, the threshold is easier to reach, making cooperation more likely when observed cooperation is moderate.

### 4. Aggressiveness:

*   **Initiating Cooperation:** The strategy aggressively *starts* cooperation in the first round. This is a bold move designed to unlock the potential of the game's reward structure.
*   **Rewarding Success:** When cooperation demonstrably works (threshold met), the strategy continues to cooperate, reinforcing the successful pattern. This is a form of positive reinforcement, but driven by the expectation of continued high payoffs.
*   **Punishing Failure:** If cooperation fails to materialize consistently, the strategy aggressively switches to defection. It refuses to be a perpetual cooperator when others are free-riding, prioritizing personal gain.
*   **Risk-Taking for Gain:** In moderate cooperation scenarios, the strategy takes a calculated risk to cooperate, aiming to tip the scales and achieve the `k` reward. This is an aggressive pursuit of potential higher payoffs.

### Pseudocode Representation:

```pseudocode
// Game Parameters
n // number of players
r // number of rounds
m // minimum cooperators needed
k // reward factor

// Strategy State (internal)
previous_cooperation_count = 0 // Count of players who cooperated in the last round
current_round = 1

// Threshold Calculations
coop_threshold_ratio = m / n
coop_buffer = 0.15 // A tunable parameter: how close is "close enough" to cooperate?

// Main Decision Function
function decide_action():
  if current_round == 1:
    // First round: Initiate cooperation to unlock potential rewards
    return C
  elif current_round == r:
    // Last round: Secure individual payoff, no future to influence
    return D
  else:
    // Intermediate rounds: Adaptive decision based on history

    // Calculate observed cooperation proportion from previous round
    observed_coop_proportion = previous_cooperation_count / n

    if observed_coop_proportion >= coop_threshold_ratio:
      // Previous round met or exceeded threshold: Continue aggressive cooperation
      // We assume if threshold was met, k is beneficial.
      return C
    elif observed_coop_proportion >= coop_threshold_ratio - coop_buffer:
      // Previous round was close to threshold: Take a calculated risk to cooperate
      // This attempts to push towards the threshold in intermediate rounds
      return C
    else:
      // Previous round was significantly below threshold: Defect to maximize individual gain
      // Refuse to be exploited when collective effort is low.
      return D

// Update state after each round
function update_state(actions_in_round):
  current_round = current_round + 1
  
  // Count how many players cooperated in the *just completed* round
  count_cooperated_this_round = 0
  for player_action in actions_in_round:
    if player_action == C:
      count_cooperated_this_round = count_cooperated_this_round + 1
  
  previous_cooperation_count = count_cooperated_this_round // Update for next round's decision
```

This strategy is designed to be a strong contender by aggressively pursuing the high rewards of cooperation when it appears likely, while ruthlessly defecting to secure individual gains when cooperation falters. Its adaptability comes from observing past behavior and adjusting its risk-taking accordingly, especially as the game approaches its end.
'''

description_AGGRESSIVE_103 = '''
## Aggressive Strategy: "The Unyielding Opportunist"

This strategy aims to maximize individual payoff by leveraging the collective reward when possible, but prioritizing self-interest and exploiting opportunities aggressively when cooperation is risky or when others are cooperating. It’s designed to be adaptive, learning from past outcomes and adjusting its behavior accordingly.

**Core Philosophy:** Cooperate only when the collective is almost guaranteed to succeed, and defect to maximize personal gain when the collective fails or when others' cooperation makes defection highly profitable.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) for player `i` in round `t` is determined by the following logic:

*   **Aggressive Opportunism:** If the number of observed cooperators in the previous round (`observed_cooperators_{t-1}`) is **strictly greater than `m - 1`**, and the player `i` was *not* the one to tip the balance (i.e., player `i` was *not* one of the `m` cooperators in the previous round if they had defected instead), then player `i` **Defects (D)**. This exploits the fact that the reward is already secured by others.
*   **Calculated Risk-Taking (Early Rounds):** In the early rounds (e.g., first `r/3` rounds), if `observed_cooperators_{t-1}` is **greater than or equal to `m - 1`**, player `i` **Cooperates (C)**. This is a calculated risk to establish a cooperative norm and try to secure the reward. The player is willing to "lead" this effort in the initial stages.
*   **Defensive Cooperation (Mid-Rounds):** If `observed_cooperators_{t-1}` is **exactly `m - 1`**, player `i` **Cooperates (C)**. This is to ensure the threshold is met, acting as a crucial "last resort" cooperator.
*   **Aggressive Defection (Failure to Reach Threshold):** If `observed_cooperators_{t-1}` is **less than `m - 1`**, player `i` **Defects (D)**. The collective effort failed, so there's no benefit to cooperating. Maximize private gain.
*   **Default Aggressive Defection:** In all other scenarios not explicitly covered by the above, player `i` **Defects (D)**. This is the aggressive default.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   Since there's no history, the strategy defaults to a balanced approach with an aggressive lean.
    *   If `n` is large and `m` is a significant fraction of `n` (e.g., `m > n/2`), player `i` **Cooperates (C)**. This is to try and secure the reward in a situation where many cooperators are needed.
    *   Otherwise, player `i` **Defects (D)**. This is the aggressive default when fewer cooperators are strictly required, as defecting is safer and more immediately rewarding.
    *   **Alternative (More Aggressive):** A strictly aggressive approach for the first round would be to **Defect (D)** always, unless `m` is very high relative to `n` and the player believes it's the *only* way to potentially win.

*   **Last Round (t=r):**
    *   In the final round, there is no future to consider. The strategy becomes purely opportunistic and self-maximizing.
    *   Player `i` **Defects (D)**. The reward from past cooperation is irrelevant. Maximize individual payoff by keeping the endowment.

*   **Threshold Exceeded by Others:**
    *   As described in the "Aggressive Opportunism" rule, if `observed_cooperators_{t-1} > m - 1`, player `i` defects. This is a key aggressive element – taking advantage of others' efforts without contributing to the reward.

### 3. Aggressive Mindset Alignment:

*   **Prioritizing Self-Interest:** The strategy's default action is to defect, as it guarantees a payoff of 1. Cooperation is only considered when the probability of securing the `k` reward is extremely high or to avoid losing an already secured reward.
*   **Exploiting Cooperation:** The "Unyielding Opportunist" is designed to benefit from others' cooperation. If enough players cooperate to meet the threshold, and the player `i` wasn't crucial for that, they defect and take the higher payoff.
*   **No "Nice Guy" Behavior:** This strategy does not aim to foster long-term cooperation or altruism. It seeks to win the tournament by maximizing its own score, even if it means being a "free-rider" when opportunities arise.
*   **Calculated "Betting" on Cooperation (Early):** The initial cooperation is a calculated risk, a temporary investment to unlock the potential for higher payoffs in subsequent rounds, *if* it proves successful. However, the aggressive nature means this "investment" is withdrawn immediately if it's no longer personally advantageous.

### Pseudocode Representation:

```pseudocode
// Global variables to track history
var observed_cooperators_history = [] // List of number of cooperators in each previous round
var total_cooperators_in_last_round = 0

function decide_action(player_id, current_round, n, r, m, k, player_actions_in_previous_rounds):
  // Initialize total cooperators for the current round based on player_actions_in_previous_rounds
  total_cooperators_in_last_round = count_cooperators(player_actions_in_previous_rounds)
  observed_cooperators_history.append(total_cooperators_in_last_round)

  // Handle Last Round
  if current_round == r:
    return D // Always defect in the last round

  // Handle First Round
  if current_round == 1:
    if m > n / 2: // Higher threshold requires more cooperation, so try to initiate
      return C
    else:
      return D // Default aggressive defection

  // Main Decision Logic
  // 1. Aggressive Opportunism: If threshold already exceeded by others
  if total_cooperators_in_last_round > m - 1:
    // Check if this player was critical for the threshold
    // This requires knowing if defecting would have dropped the count below m
    // For simplicity in this pseudocode, if we observe > m-1, we assume we can defect
    // A more robust check would analyze player_actions_in_previous_rounds more granularly
    return D

  // 2. Calculated Risk-Taking (Early Rounds) - e.g., first r/3 rounds
  if current_round <= r / 3:
    if total_cooperators_in_last_round >= m - 1:
      return C

  // 3. Defensive Cooperation (Mid-Rounds): Exactly m-1 cooperators
  if total_cooperators_in_last_round == m - 1:
    return C

  // 4. Aggressive Defection (Failure to Reach Threshold)
  if total_cooperators_in_last_round < m - 1:
    return D

  // 5. Default Aggressive Defection
  return D

function count_cooperators(actions_list):
  count = 0
  for action in actions_list:
    if action == C:
      count = count + 1
  return count

```

This "Unyielding Opportunist" strategy embodies aggression by prioritizing personal gain, exploiting collective successes, and taking calculated risks only when they clearly lead to a higher individual payoff. It's designed to be a formidable opponent in a competitive environment where cooperation is not guaranteed.
'''

description_AGGRESSIVE_104 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize your payoff by leveraging the parameters and history of the game, while being adaptable and robust.

## Strategy: The "Calculated Dominance" Strategy

This strategy is built on the principle of **forcing the collective to meet the cooperation threshold by demonstrating a willingness to contribute, but only when the reward is highly probable and the risk of being exploited is minimized.** It aims to dominate the game by consistently pushing for cooperation when beneficial, and defecting ruthlessly when others fail to meet the threshold, thus securing personal gain.

### Core Aggressive Philosophy:

*   **Initiate to Gain, Punish to Deter:** We will cooperate to trigger the reward when it's strategically advantageous. However, we will punish deviations by defecting immediately to maximize our individual gain.
*   **Self-Reliance with Strategic Leverage:** While we aim to trigger the reward, we won't be a perpetual free-rider. We'll contribute *strategically* to nudge the system towards cooperation, but our primary goal is to secure our own payoff.
*   **Exploit Weakness, Deter Exploitation:** If others are cooperating and the threshold is met, we'll defect to take the higher payoff. If others defect and the threshold isn't met, we'll defect to minimize our loss. We want to be the one dictating the outcome.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect in any given round is based on a dynamic assessment of the current game state, historical trends, and the potential for future gains.

**The core rule is to cooperate *only if* there's a high probability of reaching the cooperation threshold *and* our cooperation is essential to push it over the edge, or if previous rounds have demonstrated a consistent pattern of cooperation.** Otherwise, we defect.

**Detailed Decision Logic (for Round t > 1):**

Let:
*   `cooperators_in_round_t_minus_1`: Number of players who cooperated in the previous round (t-1).
*   `my_action_in_round_t_minus_1`: Your action in the previous round (t-1).
*   `history_cooperation_rate`: The average cooperation rate across all previous rounds.
*   `recent_cooperation_trend`: A measure of how cooperation has changed in the last few rounds (e.g., a simple moving average or slope).
*   `prob_threshold_met_next_round`: An estimated probability that the threshold `m` will be met in the current round `t`.

**Decision to Cooperate (Play 'C'):**

You will cooperate in round `t` IF **ALL** of the following conditions are met:

1.  **Near Misses/Strong Momentum:** `cooperators_in_round_t_minus_1 >= m - 1` (The previous round was a near miss, and your cooperation could be the tipping point).
2.  **Sufficient Reward Factor:** `k > 1.5` (The reward factor is substantial enough to justify the risk. We're being aggressive, but not reckless).
3.  **Positive Recent Trend:** `recent_cooperation_trend` is positive or zero (Cooperation isn't actively declining).
4.  **Not the Last Round's Defector:** If `my_action_in_round_t_minus_1 == D` and `cooperators_in_round_t_minus_1 < m`, you are less inclined to cooperate immediately. You need to see a more stable trend.

**Decision to Defect (Play 'D'):**

You will defect in round `t` IF **ANY** of the following conditions are met:

1.  **No Incentive to Cooperate:** If the conditions for cooperating are *not* met. This is the default aggressive stance.
2.  **Previous Round Failure & High Individual Gain:** If `cooperators_in_round_t_minus_1 < m` and `my_action_in_round_t_minus_1 == D` (you defected in the previous round and the threshold was not met), you will continue to defect. You are not bailing out a failed collective.
3.  **Exploitation Opportunity:** If `cooperators_in_round_t_minus_1 >= m` (the threshold was met in the previous round), you will **definitely defect** in the current round `t`. This is to maximize your payoff when the collective project has succeeded without your direct contribution in *this specific instance*.
4.  **Deteriorating Cooperation:** If `recent_cooperation_trend` is significantly negative.

### 2. Handling Edge Cases

**Round 1 (Initial Decision):**

*   **Default to Defection:** In the very first round, without any historical data, the default action is **Defect ('D')**. This is the safest aggressive play to gather information and avoid being an early sucker.
*   **Exception (If m is very high and k is very high):** If `m > n/2` AND `k > 2.5` (a very strong incentive to cooperate), there might be a slight inclination to Cooperate ('C') to try and kickstart cooperation. However, the default is still Defect. We are looking for strong signals.

**Last Round (Round r):**

*   **Aggressive Final Push:** In the final round, the game theoretic incentives change as there are no future rounds to consider. The strategy becomes more purely self-interested.
*   **Decision for Round r:** You will play **Defect ('D')** in the last round, *regardless of history*. This is because you want to maximize your personal take in the final round. The collective good no longer matters for future rounds. The only exception would be if playing 'C' in the last round *guarantees* a higher payoff for *you* than playing 'D' (which is only possible if `m` players cooperate *and* you are one of them, thus receiving `(1-1) + k = k`, while defecting could yield `1 + k` if `m` players *still* cooperate without you). However, given the "no communication" rule, this guarantee is unlikely. Therefore, the aggressive default is **Defect ('D')**.

**Early Rounds (Rounds 2 to ~r/3):**

*   **Information Gathering and Trend Setting:** In these early rounds, you'll use a slightly more responsive approach.
    *   **If previous round met threshold (cooperators >= m):** **Defect ('D')**. You benefit from the collective effort.
    *   **If previous round missed threshold (cooperators < m):**
        *   **If you defected in previous round:** Continue to **Defect ('D')**. You won't unilaterally fund a failed project.
        *   **If you cooperated in previous round:** Consider cooperating again if `m - cooperators_in_round_t_minus_1 <= 1` (meaning your cooperation was the sole reason for a potential miss) AND `k` is sufficiently high (e.g., `k > 2`). This is a strategic gamble to try and force cooperation. If `k` is not that high, or if the miss was by more than 1, **Defect ('D')**.

**Mid-to-Late Rounds (Rounds ~r/3 + 1 to r-1):**

*   **Reinforcement of Patterns:** The decision rules become more rigid, based on established trends.
    *   **If threshold was met in the previous round:** **Defect ('D')**. You are exploiting a successful collective effort.
    *   **If threshold was NOT met in the previous round:**
        *   **If you defected in the previous round:** **Defect ('D')**. This punishes the group for not meeting the threshold.
        *   **If you cooperated in the previous round:** Cooperate again ONLY IF `m - cooperators_in_round_t_minus_1 <= 1` (your cooperation was the crucial factor) AND `k > 1.8`. Otherwise, **Defect ('D')**. This is your last chance to nudge them if the risk is still worth it.

### 3. Aggressive Mindset Alignment

*   **Maximizing Personal Gain:** The strategy prioritizes securing the highest possible individual payoff in each round, by either taking advantage of the collective reward or by avoiding losses when the collective fails.
*   **Dominating the Outcome:** By strategically cooperating, you aim to influence the outcome in your favor. You are not passively waiting for others to cooperate; you are actively pushing the game towards a state where you can extract maximum value.
*   **Punishing Inaction/Failure:** Defecting when the threshold is not met is a strong signal. It tells other players that you will not subsidize their selfish behavior. This aggressive stance can deter future defections if others learn that cooperation is the only way to get you to contribute.
*   **No Altruism:** There is no inherent desire to "help the group." Cooperation is a tool for self-enrichment.

### Pseudocode:

```pseudocode
STRATEGY CalculateDominance(round_num, n, r, m, k, history):
  // history is a list of dictionaries, each containing {'my_action': 'C'/'D', 'other_actions': list_of_actions}

  // Edge Case: First Round
  IF round_num == 1:
    RETURN 'D' // Default aggressive stance

  // Edge Case: Last Round
  IF round_num == r:
    RETURN 'D' // Maximize personal gain in the final round

  // Get data from previous round
  prev_round_data = history[-1]
  cooperators_in_prev_round = count_cooperators(prev_round_data['other_actions']) + (1 if prev_round_data['my_action'] == 'C' else 0)
  my_action_prev_round = prev_round_data['my_action']

  // Calculate recent cooperation trend (simplified: average of last 3 rounds' cooperation)
  recent_cooperation_trend = 0
  IF length(history) >= 3:
    FOR i FROM 1 TO 3:
      round_data = history[-i]
      coop_count = count_cooperators(round_data['other_actions']) + (1 if round_data['my_action'] == 'C' else 0)
      recent_cooperation_trend += coop_count / n
    recent_cooperation_trend /= 3
  ELSE:
    // Not enough history for trend, default to neutral or slightly positive
    recent_cooperation_trend = 0.5

  // Decision Logic

  // Condition for potentially cooperating
  IF cooperators_in_prev_round >= m - 1 AND k > 1.5 AND recent_cooperation_trend >= 0.5: // Use 0.5 as neutral threshold for trend
    IF my_action_prev_round == 'D' AND cooperators_in_prev_round < m:
      // If you defected last round and missed, be cautious.
      // Only cooperate if m-cooperators_in_prev_round == 1 (your cooperation is critical)
      IF m - cooperators_in_prev_round == 1 AND k > 1.8: // Slightly higher threshold for cooperating after personal defection
        RETURN 'C'
      ELSE:
        RETURN 'D'
    ELSE:
      // You cooperated last round or threshold was met, so cooperating is fine.
      RETURN 'C'
  ELSE:
    // Default to Defect if cooperating conditions are not met
    RETURN 'D'

  // Explicitly handle "Exploitation Opportunity" - this is covered by the default 'D' if cooperation conditions aren't met.
  // If cooperators_in_prev_round >= m: RETURN 'D' // This is implicitly handled.

// Helper function
FUNCTION count_cooperators(actions_list):
  count = 0
  FOR action IN actions_list:
    IF action == 'C':
      count += 1
  RETURN count
```

This "Calculated Dominance" strategy aims to be a formidable opponent by being decisive, adaptive, and consistently prioritizing its own strategic advantage. It's designed to make others dance to its tune, either by forcing them to cooperate for your benefit, or by benefiting from their cooperation when it occurs.
'''

description_AGGRESSIVE_105 = '''
## The "Calculated Gamble" Aggressive Strategy

This strategy is designed to be aggressive by prioritizing personal gain while leveraging the collective reward when the odds are in its favor. It is adaptive and learns from the history of the game, but its core driver is a calculated risk-taking approach.

**Core Philosophy:** Cooperate only when there's a high probability of reaching the cooperation threshold, and defect otherwise. The "gamble" lies in this probabilistic assessment. Aggression comes from a willingness to defect and extract personal gain when cooperation is unlikely or risky.

---

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to Cooperate (C) or Defect (D) in any given round `t` hinges on a calculated probability of success for cooperation, factoring in past observed behavior and the game parameters.

**Key Variables to Track:**

*   `N_C_t-1`: Number of players who cooperated in the *previous* round (`t-1`).
*   `N_D_t-1`: Number of players who defected in the *previous* round (`t-1`).
*   `Own_Actions_t-1`: Player i's own action in round `t-1`.
*   `Total_Cooperators_Observed`: The total number of unique players who have cooperated at least once throughout the game.
*   `Last_Cooperation_Round`: The round number where the cooperation threshold `m` was last met or exceeded.

**The Aggressive Cooperation Threshold (ACT):**

We will maintain an "Aggressive Cooperation Threshold" (`ACT`). This is a dynamic value that represents the minimum *perceived* probability of reaching `m` cooperators for this strategy to risk cooperating.

*   **Initial ACT (Round 1):** We start with a cautious but still aggressive stance. The initial `ACT` will be set to a high value, for example, `0.75`. This means we need to be at least 75% confident that `m` cooperators will emerge for us to contribute.

*   **Adaptive ACT Adjustment:**
    *   **If Cooperation Threshold Met (and we cooperated):** If the threshold `m` was met in the previous round, and *we also cooperated*, our confidence in the collective increases. We can slightly *lower* the `ACT` to encourage more participation. For instance, decrease `ACT` by `0.05` (but not below `0.5`). This is because we've seen cooperation can be successful and are more willing to initiate.
    *   **If Cooperation Threshold Met (but we defected):** If the threshold `m` was met in the previous round, but *we defected* (meaning we benefited from others' cooperation), we become more wary. We will *increase* the `ACT` by `0.07` (but not above `0.95`). This is a defensive adjustment – we've profited from others' risk, and now we demand a higher certainty before taking our own risk.
    *   **If Cooperation Threshold NOT Met (and we cooperated):** If the threshold `m` was not met, and *we cooperated*, this is a costly mistake. We will significantly *increase* the `ACT` by `0.10` (but not above `0.95`). This signals a strong disincentive to cooperate when it has failed previously.
    *   **If Cooperation Threshold NOT Met (and we defected):** If the threshold `m` was not met, and *we defected*, this is the desired outcome. We are not penalized, and we are not incentivized to change our risk-aversion. The `ACT` remains unchanged.

**The Decision Rule:**

In round `t`, player `i` will:

*   **Cooperate (C) IF:**
    1.  The number of players who *have* cooperated in the past (`Total_Cooperators_Observed`) plus the estimated number of other players likely to cooperate in this round is greater than or equal to `m`.
    2.  AND our calculated probability of reaching `m` cooperators (based on historical data and current assessment) is greater than or equal to our current `ACT`.
    3.  AND it is *not* the absolute last round where defecting offers a higher guaranteed payoff (see Edge Cases).

*   **Defect (D) IF:**
    1.  The conditions for Cooperation are NOT met.
    2.  OR if defecting guarantees a better individual payoff in the current round compared to the *expected* payoff of cooperating (which involves the risk of failure).

**Estimating Other Players' Cooperation:**

This is the crucial part of the "gamble." We'll use a simple heuristic:

*   **Assume an average historical cooperation rate:** `Avg_Coop_Rate = Total_Cooperators_Observed / (n * (t-1))` (if t>1, else 0).
*   **Estimate potential cooperators from those who haven't defected consistently:** If a player has defected in the majority of past rounds, assume they are less likely to cooperate.
*   **A more sophisticated approach (for later implementation):** If we observe players who *always* cooperate or *always* defect, we can factor this into our estimation.

For this strategy, we'll simplify: Assume that in round `t`, `k` other players (where `k` is a fraction of `n` based on historical cooperation rates, e.g., `k = round(0.6 * (n-1))` if `Avg_Coop_Rate > 0.5`, else `k = round(0.3 * (n-1))`) are *potential* cooperators.

**Therefore, the condition for cooperating becomes more concrete:**

**Cooperate (C) IF:**
`((Total_Cooperators_Observed + estimated_potential_other_cooperators) >= m)`
**AND**
`(Probability_Of_Reaching_M >= ACT)`

**Calculating `Probability_Of_Reaching_M` (Simplified):**
This is where the "gamble" is. We'll use a simplified binomial probability estimation. If we estimate `P_other_coop` as the probability of any *other* player cooperating, then the probability of at least `m - c_i` (where `c_i` is 1 if we plan to cooperate, 0 if we plan to defect) other players cooperating can be approximated.

A more practical approach for this strategy:
If we estimate `X` other players as likely cooperators, and we are considering cooperating, we need at least `m-1` of those `X` players to cooperate.
If `X >= m-1`, we deem the probability of reaching `m` *sufficiently high* if we also cooperate.

**Revised Decision Rule:**

In round `t`, player `i` will:

*   **Cooperate (C) IF:**
    1.  `N_C_t-1 >= m - 1` (meaning, the previous round's cooperators were close to the threshold, suggesting potential for coordination).
    2.  **AND** `Total_Cooperators_Observed >= m` (meaning, the threshold has been met at least once, validating the possibility of collective reward).
    3.  **AND** `ACT` is below a certain dynamic threshold (e.g., `0.65`).
    4.  **AND** it is *not* the absolute last round where defecting offers a higher guaranteed payoff.

*   **Defect (D) IF:**
    1.  The conditions for Cooperation are NOT met.
    2.  OR if defecting guarantees a better individual payoff in the current round compared to the *expected* payoff of cooperating.

---

### 2. Handling Edge Cases

*   **Round 1 (Initial Round):**
    *   Since there's no history, we play aggressively and cautiously.
    *   **Decision:** **Defect (D)**.
    *   **Reasoning:** The initial endowment of `1` is guaranteed. Cooperating without any information about others is too risky. The `ACT` is high (0.75), and we have no confidence in others cooperating. This establishes our baseline aggressive stance.

*   **The Penultimate Round (Round `r-1`):**
    *   If the `ACT` is high (e.g., `0.85` or greater), we will **Defect (D)**.
    *   **Reasoning:** The risk of cooperating and failing is magnified as there's only one round left for recovery. If the `ACT` is high, it indicates past failures, making us even more risk-averse in the face of imminent finality.

*   **The Last Round (Round `r`):**
    *   This is where the pure payoff maximization takes over.
    *   **Decision:**
        *   **Cooperate (C) IF:** The sum of all players' potential payoffs if `m` cooperate (`n * (1 - 1) + k = k`) is greater than the sum of all players' payoffs if everyone defects (`n * 1`). This happens if `k > n`.
        *   **Defect (D) IF:** The sum of all players' potential payoffs if `m` cooperate (`k`) is less than or equal to the sum of all players' payoffs if everyone defects (`n`). This is the usual case since `k > 1` and `n >= 2`.
    *   **Reasoning:** In the last round, future reputation or reciprocity doesn't matter. It's about maximizing the immediate, final payoff. If `k` is astronomically high such that collective reward for everyone is greater than individual defection, we might cooperate. Otherwise, we defect to guarantee our payoff. Given `k>1` and `n>=2`, the condition `k>n` is rarely met, so we almost always defect in the last round. This is aggressive: securing your guaranteed gain.

*   **When `m` is close to `n`:**
    *   If `m > n - 2`, the strategy becomes more sensitive.
    *   **Decision:** We become more cautious about cooperating. The `ACT` will tend to be higher. We need stronger signals of coordinated effort. The estimation of potential cooperators will be more stringent. This reflects an aggressive stance: we won't contribute if the collective effort is too uncertain to reach a high threshold.

*   **When `m` is close to 2:**
    *   The strategy can exploit this.
    *   **Decision:** We will be more willing to cooperate if even one other player seems likely to cooperate. The `ACT` will be lower, and the estimation of potential cooperators will be more lenient. This is aggressive: we'll try to trigger the reward with minimal effort if the threshold is low.

---

### 3. Aggressiveness Alignment

This strategy embodies aggression in several ways:

*   **Prioritizing Personal Gain:** The default action is to Defect, securing the guaranteed payoff of 1. Cooperation is only undertaken when a calculated gamble suggests a high probability of a greater reward.
*   **Opportunistic Exploitation:** If the threshold `m` is met and we defected, we increase our `ACT`. This means we are willing to benefit from others' cooperation without contributing ourselves, and we demand more certainty from them before we risk our own contribution.
*   **Punishing Failure:** If we cooperate and the threshold is not met, our `ACT` significantly increases. This is a form of "punishment" for a failed collective effort, making us much less likely to cooperate again soon. This discourages "free-riding" on our goodwill and reinforces our aggressive pursuit of profitable outcomes.
*   **Last Round Betrayal (of cooperation):** In the final round, we revert to pure self-interest, defecting to guarantee our score, regardless of what happened in previous rounds. This is a decisive aggressive move, prioritizing final standing over any notion of ongoing cooperation.
*   **Calculated Risk-Taking:** The "gamble" aspect is aggressive because it implies a willingness to take risks, but only after a deliberate assessment. We are not passively cooperating; we are actively deciding when the odds are sufficiently in our favor to invest.

---

### Pseudocode Outline:

```pseudocode
// Game Parameters
n, r, m, k

// Player i's State
ACT = 0.75 // Aggressive Cooperation Threshold (starts high)
Total_Cooperators_Observed = 0
Last_Cooperation_Round = -1
Own_Actions_History = [] // List of actions played by player i
Opponent_Actions_History = [] // List of lists, each inner list is opponents' actions in a round

function decide_action(current_round):
    // --- Handle Last Round ---
    if current_round == r:
        // In the last round, defect unless k is extremely high such that collective is better than all defecting
        if k > n * 1.0: // Condition k > n
            return 'C'
        else:
            return 'D'

    // --- Update State from Previous Round (if not round 1) ---
    if current_round > 1:
        N_C_prev = count_cooperators(Opponent_Actions_History[current_round - 2]) // Opponent actions from round t-1
        N_D_prev = n - 1 - N_C_prev

        if N_C_prev >= m:
            if Own_Actions_History[current_round - 2] == 'C': // We cooperated and threshold met
                ACT = max(0.5, ACT - 0.05) // Slightly lower ACT
            else: // We defected and threshold met
                ACT = min(0.95, ACT + 0.07) // Increase ACT
        else: // Threshold not met
            if Own_Actions_History[current_round - 2] == 'C': // We cooperated and threshold NOT met
                ACT = min(0.95, ACT + 0.10) // Significantly increase ACT
            // Else (we defected and threshold NOT met), ACT remains unchanged

        // Update Total_Cooperators_Observed (simplified: count from previous rounds)
        // A more complex update would track unique cooperators over all rounds.
        // For simplicity, let's assume we have a way to know this count updated in a main loop.

    // --- Determine Potential Cooperators ---
    // Simple heuristic: based on overall observed cooperation rate or fixed number
    estimated_potential_other_cooperators = 0
    if Total_Cooperators_Observed > 0:
        // If we've seen cooperation, assume some fraction of others might
        estimated_potential_other_cooperators = round(0.6 * (n - 1)) // Example: 60% of others are potential
    else:
        // If no cooperation has ever been observed, assume very few potential cooperators
        estimated_potential_other_cooperators = round(0.2 * (n - 1))

    // --- Aggressive Cooperation Check ---
    // Does cooperating seem viable given current ACT and potential cooperators?
    cooperate_seems_viable = False
    if (Total_Cooperators_Observed + estimated_potential_other_cooperators >= m) and (ACT < 0.70): // Threshold for ACT to even consider
        cooperate_seems_viable = True

    // --- Decision Logic ---
    if cooperate_seems_viable and current_round < r: // Don't cooperate in last round unless condition met above
        // We'll cooperate if the conditions for viability are met
        return 'C'
    else:
        // Default to Defect to guarantee personal payoff
        return 'D'

function count_cooperators(actions_list):
    count = 0
    for action in actions_list:
        if action == 'C':
            count += 1
    return count

// --- Main Tournament Loop Logic (Conceptual) ---
// For each round t from 1 to r:
//     my_action = decide_action(t)
//     Record my_action in Own_Actions_History
//     Collect all players' actions for this round
//     Record opponents' actions in Opponent_Actions_History
//     Update Total_Cooperators_Observed based on all actions this round
//     Calculate payoffs
```
'''

description_AGGRESSIVE_106 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the game's structure and parameters. This strategy is adaptive and aims to exploit cooperative tendencies when beneficial, but always prioritizes personal gain.

## Strategy: "The Calculated Gambler"

This strategy is aggressive because it's designed to push for the collective reward when the odds are favorable and to defect when the risk of personal loss or insufficient reward is high. It doesn't aim for pure, selfless cooperation. Instead, it aims to *force* cooperation through leverage or to profit from others' cooperation while minimizing its own exposure.

### 1. Decision Rules:

The core of this strategy lies in dynamically assessing the "likelihood of success" for achieving the cooperative threshold, `m`.

*   **Default Action: Defect (D)**
    The baseline assumption is that individual risk-taking is not rewarded unless a critical mass is achieved. Therefore, the default action is to defect and secure the guaranteed payoff of 1.

*   **Cooperate (C) Condition:**
    Player `i` will choose to Cooperate (C) in round `t` if and only if:

    **(Number of players who cooperated in round `t-1` >= `m-1`)**
    **AND**
    **(Expected number of other players to cooperate in round `t` is at least `m-1`)**

    Let's break down the "Expected number of other players to cooperate" part:

    *   **Observe History:** Look at the number of players who cooperated in the *previous* round (`t-1`). Let this be `cooperators_t_minus_1`.
    *   **Identify "Likely Cooperators":** Based on historical data (from rounds `1` to `t-1`), identify players who have a high propensity to cooperate. A simple heuristic for this is to count how many players have cooperated in a majority of previous rounds where `cooperators_t_minus_1 >= m`. We'll call these "historically cooperative players."
    *   **Estimate Others' Behavior:**
        *   If `cooperators_t_minus_1 >= m`: The collective project was successful last round. This indicates that the "game" can be won. The strategy will assume that a significant portion of players will continue to cooperate, *especially* those who have benefited from cooperation in the past.
        *   If `cooperators_t_minus_1 < m`: The collective project failed. This suggests that either too few people are willing to risk cooperation, or there's a strong tendency towards defection.

    *   **The Decision Threshold:**
        The strategy cooperates if:
        *   The previous round *just barely* met or exceeded the threshold (`cooperators_t_minus_1 == m-1`). This is a strong signal that *your* cooperation is the marginal factor that could tip the scales.
        *   OR, if `cooperators_t_minus_1 >= m` (previous success), AND there are at least `m-1` other players (excluding yourself) who are "historically cooperative" (i.e., have cooperated in a significant fraction of past successful rounds). This indicates a stable tendency towards cooperation among a sufficient subset of players.

*   **Aggressive Interpretation:**
    The strategy is aggressive because it's *not* about fostering general goodwill. It's about:
    *   **Leveraging Weak Signals:** If the previous round was *almost* a success (m-1 cooperators), this strategy *will* cooperate, essentially betting that its contribution will push the group over the edge and secure the reward for everyone, including itself. This is a high-stakes gamble for a guaranteed `k` payoff.
    *   **Exploiting Established Cooperation:** If a core group of players has demonstrated a consistent willingness to cooperate and has successfully achieved the threshold in the past, this strategy will join them, profiting from their established reliability. It's not leading the charge, but it's happily riding the coattails of reliable cooperators.
    *   **Punishing Defection (Implicitly):** By defaulting to defection when conditions aren't met, it avoids contributing to failed cooperative efforts, thus preserving its endowment and effectively "punishing" others who may have defected when cooperation was needed.

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   **Decision:** **Defect (D)**
    *   **Reasoning:** In the first round, there is no history. The strategy cannot assess any propensity for cooperation or the likelihood of meeting the threshold. The safest and most aggressive initial move is to secure the guaranteed payoff of 1, observing how others behave.

*   **The Penultimate Round (t = r-1):**
    *   **Decision:** **Cooperate (C) if `cooperators_t_minus_1 >= m-1`** (using history from round `t-1`).
    *   **Reasoning:** This is where the aggressive nature shines. The strategy needs to maximize its payoff. If the previous round was close to or achieved the threshold, it's a strong indicator that cooperation is viable and the reward (`k`) is within reach. This is the last chance to significantly influence the outcome for the *second-to-last* round's payoff. The "calculated gamble" is highest here.

*   **The Last Round (t = r):**
    *   **Decision:** **Cooperate (C) if `cooperators_t_minus_1 >= m-1`**.
    *   **Reasoning:** Similar to the penultimate round, the strategy will aim to secure the `k` reward. Since there are no future rounds to influence or be influenced by, the immediate payoff is paramount. This is the final opportunity to leverage the potential for a high payoff.

*   **No History of Success:** If the game has proceeded through several rounds and the cooperative threshold `m` has *never* been met, the strategy will become increasingly risk-averse. It will only cooperate if `cooperators_t_minus_1 == m-1`. This means it's waiting for a very specific signal of imminent success to be triggered by its own action.

### 3. Aggressive Mindset Alignment:

*   **Profit Maximization:** The primary goal is to maximize individual total payoff. This strategy prioritizes securing the `k` reward whenever a reasonable probability exists, rather than promoting universal cooperation for its own sake.
*   **Calculated Risk:** It doesn't shy away from risk, but it's *calculated*. Cooperation is only triggered by specific historical conditions that suggest a high probability of success. This is not a naive leap of faith.
*   **Leverage:** The strategy aims to be the "swing vote." By cooperating when `m-1` players have already cooperated, it is aggressively positioning itself to be the one that secures the win and the subsequent reward for all.
*   **Resource Preservation:** By defaulting to "D" when cooperation is uncertain, it preserves its endowment (payoff of 1) from being wasted on a failed collective effort. This is an aggressive way to conserve resources while waiting for opportune moments.

### Pseudocode Representation:

```python
# Game Parameters
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward if threshold met factor

# Historical Data (to be maintained across rounds)
# List of actions for each player in each past round
# Example: history[player_id][round_num] = 'C' or 'D'
history = {} # player_id -> list of actions

# Function to determine player i's action in round t
def decide_action(player_id, current_round, history):
    # --- Edge Case: First Round ---
    if current_round == 1:
        return 'D'

    # --- Get Previous Round's Data ---
    prev_round = current_round - 1
    cooperators_prev_round = 0
    for p_id in range(n): # Assuming players are 0 to n-1
        if history.get(p_id, [])[prev_round] == 'C':
            cooperators_prev_round += 1

    # --- Aggressive Cooperation Condition ---
    # Condition 1: Previous round was just one shy of the threshold
    condition_one = (cooperators_prev_round == m - 1)

    # Condition 2: Enough historically reliable cooperators to form the threshold if we join
    # This part is more complex and requires tracking player reliability.
    # A simple heuristic: count players who cooperated in >50% of past successful rounds
    reliable_cooperators_count = 0
    if current_round > 2: # Need some history to assess reliability
        for p_id in range(n):
            if p_id == player_id: continue # Exclude self

            cooperation_count = 0
            successful_rounds_count = 0
            for r_idx in range(prev_round): # Iterate through rounds before the previous one
                if history.get(p_id, [])[r_idx] == 'C':
                    cooperation_count += 1
                    # Check if that round was successful (threshold met)
                    # This requires looking up total cooperators for each round_idx
                    # For simplicity here, let's assume we're checking reliability
                    # based on ANY cooperation, but a more robust check would be
                    # based on cooperation in *successful* rounds.
                    # A more accurate check for "historically cooperative" is:
                    # if count_cooperators_in_round(r_idx) >= m and history[p_id][r_idx] == 'C':
                    #     reliable_cooperators_count += 1
                    # For this simplified pseudocode, let's use a direct count from previous rounds.
                    # A better approach would be to pre-calculate reliability scores.
                    pass # Placeholder for more robust reliability check

            # Simplified reliability check: Player has cooperated in at least half of their moves so far.
            # This is not ideal, but demonstrates the concept.
            # A better approach: If a player has cooperated in X% of rounds where the threshold WAS met.
            total_moves_for_p = len(history.get(p_id, []))
            if total_moves_for_p > 0 and cooperation_count / total_moves_for_p >= 0.6: # Arbitrary threshold for "reliable"
                 reliable_cooperators_count += 1

    # If previous round was successful AND there are enough reliable players to complete the threshold
    # (including self if we cooperate)
    condition_two = (cooperators_prev_round >= m and reliable_cooperators_count >= m - 1)

    # --- Decision Logic ---
    if condition_one or condition_two:
        return 'C' # Cooperate if the threshold is within reach or if reliable cooperators exist
    else:
        return 'D' # Otherwise, defect

# --- Example of how to use it within a tournament simulation ---
# In each round, for each player:
#   action = decide_action(my_player_id, current_round, game_history)
#   record action in history
#   calculate payoff based on ALL players' actions for the round
#   update total payoff
```

This "Calculated Gambler" strategy is aggressive because it's designed to seize opportunities for high payoffs by strategically taking calculated risks when the historical data suggests success is likely. It's not about passive participation but about actively seeking the most profitable outcome through intelligent observation and bold, yet calculated, action.
'''

description_AGGRESSIVE_107 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Opportunistic Sentinel"

This strategy embodies an aggressive mindset by prioritizing personal gain while vigilantly monitoring the collective for opportunities to exploit or contribute strategically for a larger reward. It is designed to be adaptive, learning from past interactions and adjusting its behavior accordingly. The core principle is to **punish defection decisively and capitalize on cooperation when it's beneficial and likely to succeed.**

### Strategy Name: The Opportunistic Sentinel

### Core Philosophy:
"I will always act in my own best interest, but I'm smart enough to recognize when collective action benefits me more. I will seize opportunities, but I will not be a fool. Those who defect when cooperation is needed will face my immediate retribution. I will be the architect of my own success, leveraging the actions of others to my advantage."

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round `t` is determined by a set of conditions that analyze the current game state and the historical behavior of the other players.

**General Rule:**

*   **If `m` or more players are predicted to cooperate in round `t`, and I believe my cooperation will tip the balance towards the reward (i.e., bringing the total cooperators to at least `m`), I will cooperate.**
*   **Otherwise, I will defect.**

**Breakdown of Conditions and Decision Logic:**

Let's define:
*   `N_cooperators_t_minus_1`: The number of players who cooperated in the previous round (`t-1`).
*   `N_defectors_t_minus_1`: The number of players who defected in the previous round (`t-1`).
*   `My_past_actions`: A history of my own actions in previous rounds.
*   `Other_players_past_actions`: A history of all other players' actions in previous rounds.
*   `Predicted_cooperators_t`: An estimation of the number of players who will cooperate in the current round `t`. This is the most crucial adaptive element.

**Decision Logic in Round `t` (for `t > 1`):**

1.  **Assess the immediate reward potential and risk of failure:**
    *   **If `N_cooperators_t_minus_1 >= m`:** The threshold was met in the previous round. This indicates a successful collective effort, and the environment is generally favorable for cooperation.
        *   **If `(N_cooperators_t_minus_1 + 1) >= m`:** If adding my cooperation would still result in `m` or more cooperators (meaning others are already cooperating enough), **I will Cooperate (C)**. This maximizes my reward.
        *   **If `(N_cooperators_t_minus_1 + 1) < m`:** My cooperation is essential to reach the threshold. If the number of cooperators in the previous round was *close* to `m` (e.g., `N_cooperators_t_minus_1 >= m - 2` or a more dynamic threshold based on `k/1`), I will **Cooperate (C)** to seize the reward. This is a calculated risk.
        *   **Otherwise (if `N_cooperators_t_minus_1` was far below `m` or there's a high degree of past defection from others):** **I will Defect (D)**. The risk of failure is too high, and I will secure my private payoff.

    *   **If `N_cooperators_t_minus_1 < m`:** The threshold was *not* met in the previous round. This indicates a risk of collective failure.
        *   **Calculate `Predicted_cooperators_t`:** This is where adaptivity comes in.
            *   **Baseline Prediction:** Start with `N_cooperators_t_minus_1`.
            *   **Punishment Factor:** For each player `j` (where `j != i`) who defected in round `t-1` *and* the threshold was not met, reduce their perceived likelihood of cooperation in the future. We can model this as a probability reduction or a direct subtraction from their expected future cooperation. For aggressive strategy, let's lean towards a strong deterrent: **If a player defected in a failed round, assume they will defect again unless circumstances drastically change.**
            *   **"Momentum" Factor:** If there's a trend of increasing cooperation over the last few rounds (even if it hasn't reached `m`), acknowledge that.
            *   **"Dominant Opponent" Factor:** If a few players consistently cooperate and *almost* reach `m`, there's a higher chance they will continue to do so, making it a better opportunity for me to join.
            *   **Consider the Reward Factor `k`:** If `k` is very high, the incentive to cooperate is stronger, and I might be more willing to take a risk to reach the threshold. A simple heuristic: if `k` is significantly larger than the private payoff (e.g., `k > 2`), I will be more inclined to cooperate if it looks like a sure thing.

        *   **Decision based on `Predicted_cooperators_t`:**
            *   **If `(Predicted_cooperators_t + 1) >= m` AND `Predicted_cooperators_t >= m - 2` (or a more sophisticated check for likelihood):** This means my cooperation *could* make the difference and there's a reasonable chance others will also cooperate. **I will Cooperate (C).** This is an aggressive play to secure the reward.
            *   **If `Predicted_cooperators_t >= m`:** If enough players are *already* predicted to cooperate, **I will Cooperate (C)** to enjoy the reward.
            *   **Otherwise:** **I will Defect (D)**. The risk of contributing and not reaching the threshold is too high.

**Aggressive Reinforcement:**

*   **The "Punishment Protocol":** If in any round `t-1` (where `t > 1`), the threshold `m` was *not* met, and at least one player defected while `N_cooperators_t_minus_1 < m`, then in round `t`, I will be even *more* inclined to defect. This aggressively punishes those who risked the collective good. If a player repeatedly defects when the threshold is missed, I will significantly reduce my expectation of their future cooperation.

### 2. Handling Edge Cases:

*   **Round 1 (t=1): The Leap of Faith (Calculated Aggression)**
    *   **Default:** **Defect (D)**. In the absence of any history, the safest and most aggressive move is to secure the private payoff of 1. This prevents being exploited in the very first round.
    *   **Exception (for extreme aggression):** If `m` is very close to `n` (e.g., `m >= n-1`), and the reward `k` is exceptionally high, one *could* consider cooperating in round 1 to try and establish a cooperative norm and reap rewards early. However, for a generally aggressive strategy, sticking to D is more robust. **The Opportunistic Sentinel will default to D in Round 1.**

*   **Last Round (t=r): The Final Score**
    *   **If `N_cooperators_t_minus_1 >= m` (threshold met in the second-to-last round):** This is a guaranteed reward. **I will Cooperate (C)**. There's no future to be punished in, so maximize the final payoff.
    *   **If `N_cooperators_t_minus_1 < m` (threshold not met in the second-to-last round):** The situation is uncertain.
        *   **If `(N_cooperators_t_minus_1 + 1) >= m`:** My cooperation could secure the reward. **I will Cooperate (C)**. It's the last chance to get the `k` bonus.
        *   **Otherwise:** **I will Defect (D)**. It's the last round, and the risk of contributing without reward is too high.

*   **When `m` is close to `n` (e.g., `m = n-1` or `m = n`):**
    *   The risk of cooperation is higher. My strategy will be more cautious about cooperating unless there's a very strong signal that the threshold will be met. The "Punishment Protocol" becomes even more critical here, as a single defector can doom the entire group.

*   **When `m` is low (e.g., `m = 2` or `m = 3` for larger `n`):**
    *   The probability of reaching the threshold is higher. My strategy will be more opportunistic, more willing to cooperate if the prediction for `Predicted_cooperators_t` is even slightly above zero and trending upwards.

### 3. Being Aggressive:

*   **Punishment is swift and severe:** If cooperation is expected and a player defects, I will severely penalize them in my future predictions. This means I will be highly unlikely to cooperate if they are around and the threshold isn't already guaranteed. This aggressive stance aims to deter future defections.
*   **Opportunistic Reward Maximization:** When the opportunity to gain the `k` reward presents itself and the risk is manageable, I will seize it. This includes cooperating even if it's a slightly risky move to reach the threshold, as long as my analysis suggests a reasonable chance of success.
*   **No Guarantees of Reciprocity:** My cooperation is purely transactional. I don't cooperate just because others are. I cooperate if it benefits me to do so, based on the parameters and observed behavior.
*   **Exploiting Trust:** If I observe a pattern of consistent cooperation from other players, I will leverage that. If their cooperation ensures the threshold is met, I will defect to gain the private payoff of 1 *plus* the reward `k` (which I get even if I defect, as long as the threshold is met). This is a ruthless but effective aggressive tactic.

### Pseudocode Representation:

```pseudocode
Strategy "Opportunistic Sentinel":

// Internal state:
history = [] // List of (my_action, {player_j: other_player_j_action}) for each round
my_cooperation_count = 0
total_cooperation_count_overall = 0 // Track total cooperation for adaptation

function decide_action(round_number, n, r, m, k, history):

    // Handle Round 1 (t=1)
    if round_number == 1:
        return D // Secure private payoff initially

    // Handle Last Round (t=r)
    if round_number == r:
        previous_round_data = history[r-2] // Data from round t-1
        num_cooperators_prev = count_cooperators(previous_round_data)
        if num_cooperators_prev >= m:
            return C // Guaranteed reward, maximize
        else:
            if (num_cooperators_prev + 1) >= m:
                return C // My cooperation secures the reward
            else:
                return D // No chance for reward, secure private payoff

    // Normal rounds (1 < t < r)
    previous_round_data = history[round_number - 2] // Data from round t-1
    num_cooperators_prev = count_cooperators(previous_round_data)
    my_action_prev = my_action(previous_round_data)
    other_players_actions_prev = other_players_actions(previous_round_data)

    // Predict cooperation for current round 't'
    predicted_cooperators_t = predict_future_cooperation(history, n, m, k)

    // Decision Logic:

    // 1. If current cooperation is enough without me, and I can still benefit
    if predicted_cooperators_t >= m:
        return C // Secure reward, I benefit from cooperation

    // 2. If my cooperation is critical and likely to succeed
    if (predicted_cooperators_t + 1) >= m:
        // Assess confidence in prediction and reward potential
        if confidence_in_prediction(history, predicted_cooperators_t, m, k) and (k > 1.5 or m is close to n/2): // Heuristic for risk tolerance
            return C // Aggressively go for the reward

    // 3. Default to Defect if no clear path to reward or high risk of failure
    return D

// Helper functions:

function count_cooperators(round_data):
    count = 0
    for player_action in round_data.other_players_actions:
        if player_action == C:
            count = count + 1
    if round_data.my_action == C:
        count = count + 1
    return count

function predict_future_cooperation(history, n, m, k):
    // This is the core adaptive part, highly complex and needs robust implementation.
    // Example simplified logic:
    predicted_coop = 0
    if not history:
        return 0 // No history, assume zero

    last_round_coop = count_cooperators(history[-1])
    predicted_coop = last_round_coop

    // Punishment factor: If threshold wasn't met and someone defected, reduce their likelihood.
    if last_round_coop < m:
        for player, action in history[-1].other_players_actions.items():
            if action == D:
                // Aggressively reduce their predicted cooperation.
                // Could involve reducing a trust score or probability.
                // For simplicity, let's assume a fixed penalty for this example.
                predicted_coop = max(0, predicted_coop - 0.5) // Arbitrary penalty

    // Momentum factor: Look at trends
    if len(history) >= 3:
        two_rounds_ago_coop = count_cooperators(history[-2])
        if last_round_coop > two_rounds_ago_coop and last_round_coop < m:
            predicted_coop = predicted_coop + 0.2 // Small boost for upward trend

    // Dominant opponent factor: If a few players are close to m, they might continue
    num_consistent_cooperators = count_dominant_cooperators(history, m)
    if num_consistent_cooperators > 0:
        predicted_coop = predicted_coop + (num_consistent_cooperators * 0.3) // Boost based on strong players

    return min(n, max(0, predicted_coop)) // Cap between 0 and n


function confidence_in_prediction(history, predicted_coop, m, k):
    // This function estimates how "sure" we are about the prediction.
    // It should consider:
    // - Variance in past cooperation
    // - Consistency of players
    // - How close predicted_coop is to m
    // - The value of k (higher k means more confidence needed for risk)

    if predicted_coop >= m:
        return True // High confidence if prediction already meets threshold

    if (predicted_coop + 1) < m:
        return False // No point even if I cooperate

    // If my cooperation is needed: (predicted_coop + 1) == m
    closeness_to_m = m - predicted_coop

    // More history and less variance => higher confidence
    // Higher k => higher threshold for confidence
    // Closer prediction is to m => more critical, need higher confidence

    // Example heuristic:
    base_confidence = 0.5 // Neutral starting point
    if len(history) > 0:
        // Factor in consistency (e.g., how often is cooperation around the same level)
        // This requires more sophisticated analysis of history

    // Consider the reward
    if k > 3: // High reward, more willing to risk with moderate confidence
        return base_confidence > 0.3
    else: // Lower reward, need higher confidence
        return base_confidence > 0.6

    return True // Placeholder, needs detailed implementation

function count_dominant_cooperators(history, m):
    // Counts players who have consistently cooperated and are close to the threshold.
    // This is a complex feature engineering task.
    return 0 // Placeholder

```

This "Opportunistic Sentinel" strategy aims to be a formidable opponent in the Collective Risk Dilemma. It's aggressive in its pursuit of rewards and its punishment of defectors, yet adaptive enough to learn and adjust its behavior based on the evolving game dynamics. The key to its success will lie in the precise implementation of its prediction and confidence assessment functions.
'''

description_AGGRESSIVE_108 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to exploit opportunities for high payoffs while mitigating risks, and adaptable to various opponent behaviors.

## Strategy Name: "Calculated Opportunist"

**Core Philosophy:** This strategy prioritizes maximizing individual gain by defecting when cooperation is likely to be unsuccessful or when defecting provides a higher immediate payoff. It aims to be aggressive by defecting when the risk of cooperation outweighs the potential reward, but will cooperate when there's a clear path to a significant collective reward and it can secure a profitable outcome for itself. It's designed to be self-interested and to exploit any perceived weakness or lack of coordination in opponents.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round `t` depends on the current state of the game, primarily the observed actions in the previous round `t-1` and the game parameters `n`, `m`, and `k`.

Let's define:
*   `cooperators_last_round`: The number of players who cooperated in round `t-1`.
*   `my_action_last_round`: Player i's action in round `t-1`.

**Decision Logic (for round `t`):**

**If `t` is the first round (t=1):**

*   **Default to Defect (D).** The primary reason is aggressive self-interest. In the first round, there's no historical data to predict opponent behavior. Cooperating risks being the only one or one of few cooperators, resulting in a payoff of `(1-1) = 0` while others defect and get `1`. By defecting, we guarantee a payoff of `1`. This sets a strong, self-interested tone.

**If `t` is not the first round (t > 1):**

We will evaluate the situation based on the following conditions, prioritizing the most restrictive ones first:

*   **Condition 1: "Guaranteed Win Scenario"**
    *   If `cooperators_last_round >= m` AND `my_action_last_round == C`:
        *   **Cooperate (C).**
        *   **Rationale:** The collective risk was successfully averted in the previous round, and you contributed. The reward `k` was applied. Continuing to cooperate in this scenario reinforces the success and maintains the potential for future high rewards, as it signals you are a reliable partner in successful cooperation. You've "earned" this cooperation by contributing to a previous success.

*   **Condition 2: "Exploiting Proximity to Threshold"**
    *   If `cooperators_last_round >= m - 1` AND `cooperators_last_round < m`:
        *   **Cooperate (C).**
        *   **Rationale:** This is the critical point where one more cooperator can trigger the reward for everyone. If the last round was *close* to success, and *you* cooperated in the last round, it makes sense to cooperate again. You're taking a calculated risk to push the group over the threshold, hoping others will also do the same, or that your cooperation will be enough. You are "leading the charge" to success.

*   **Condition 3: "Punishing Failure & Self-Preservation"**
    *   If `cooperators_last_round < m - 1`:
        *   **Defect (D).**
        *   **Rationale:** If the previous round fell significantly short of the cooperation threshold (`m`), it indicates a lack of coordination or a prevailing defecting tendency among opponents. Cooperating in this scenario is highly likely to result in a payoff of `(1-1) = 0`, as the threshold won't be met. Defecting guarantees a payoff of `1`. This is a clear act of self-preservation and punishment for others' lack of cooperation.

*   **Condition 4: "Aggressive Reward Seizure (When You Defected Last Round)"**
    *   If `cooperators_last_round >= m` AND `my_action_last_round == D`:
        *   **Cooperate (C).**
        *   **Rationale:** The threshold was met, and you *profited* by defecting. However, if you want to continue to leverage the reward `k` in subsequent rounds, you need to signal some willingness to contribute. Cooperating here is a strategic move to ensure the reward `k` continues to be distributed. It's a calculated risk: you might get `(1-1) + k = k` while others get `(1-0) + k = 1+k`. But if you always defect when the threshold is met, you risk others retaliating by never cooperating again, hurting your long-term potential. This is a short-term altruistic gesture for long-term gain.

*   **Condition 5: "Default Aggression (If None of the Above Apply)"**
    *   If none of the above conditions are met:
        *   **Defect (D).**
        *   **Rationale:** This acts as a catch-all for situations not explicitly covered. Given the aggressive nature, the default is to defect, ensuring a guaranteed payoff of `1`. This also serves to punish any unexpected cooperative patterns from opponents that don't align with the thresholds.

---

### 2. Handling Edge Cases:

*   **First Round (t=1):** As stated above, **always Defect (D)**. This is the most aggressive and self-preserving move in the absence of information.
*   **Last Round (t=r):** The decision logic remains the same. However, the *strategic intent* changes slightly. In the last round, future repercussions are irrelevant. The "Calculated Opportunist" will make the decision purely based on maximizing its payoff in that single, final round. The existing logic already implicitly handles this by not looking ahead, but it's worth noting the intent is purely short-term maximization.
    *   If the conditions lead to C, it's because it's the *highest immediate payoff* given the perceived state.
    *   If the conditions lead to D, it's because it's the *highest immediate payoff*.

---

### 3. Aggressive Alignment:

This strategy is aggressive in several ways:

*   **Default to Defection:** The initial state and fallback condition is to defect, ensuring a guaranteed payoff and setting a self-interested tone.
*   **Exploiting Proximity:** Actively cooperates when the threshold is *almost* met, aiming to be the catalyst for a reward that benefits all, but specifically securing `k` if others defect.
*   **Punishing Failure:** Ruthlessly defects when cooperation fails to meet the threshold, refusing to subsidize the failure of others.
*   **Strategic Cooperation for Reward:** Cooperates even after a successful defection when the threshold was met, but *only* if it means enabling future opportunities for `k`. This isn't altruism; it's calculated self-interest to keep the "golden goose" alive.
*   **No Trust:** It doesn't inherently trust any opponent. Its cooperation is conditional on seeing evidence of potential success or a strategic advantage.

---

### Pseudocode Representation:

```python
def calculate_action(n, m, k, r, current_round, my_last_action, cooperators_last_round):
    """
    Calculates the action for the current round based on game parameters and history.

    Args:
        n (int): Number of players.
        m (int): Minimum cooperators needed.
        k (float): Reward if threshold met factor.
        r (int): Total number of rounds.
        current_round (int): The current round number (1-indexed).
        my_last_action (str): Player's action in the previous round ('C' or 'D').
        cooperators_last_round (int): Number of players who cooperated in the previous round.

    Returns:
        str: The action to take ('C' or 'D').
    """

    # Edge Case: First Round
    if current_round == 1:
        return 'D' # Always defect in the first round for guaranteed payoff

    # Decision Logic for subsequent rounds
    # Condition 1: Guaranteed Win Scenario
    if cooperators_last_round >= m and my_last_action == 'C':
        return 'C'

    # Condition 2: Exploiting Proximity to Threshold (if I cooperated last)
    # Slightly adjusted logic: If we were close AND I tried to help last time, try again.
    if cooperators_last_round >= m - 1 and cooperators_last_round < m and my_last_action == 'C':
        return 'C'
    # Additional check for close proximity if I defected last, still trying to push over
    if cooperators_last_round >= m - 1 and cooperators_last_round < m and my_last_action == 'D':
        # This is a bolder move: If we were close and I defected last,
        # but it's STILL possible with my cooperation, I'll try.
        # This is a calculated risk to secure reward.
        return 'C'

    # Condition 3: Punishing Failure & Self-Preservation
    if cooperators_last_round < m - 1:
        return 'D'

    # Condition 4: Aggressive Reward Seizure (when I defected last and it worked)
    if cooperators_last_round >= m and my_last_action == 'D':
        return 'C' # Cooperate to keep the reward stream alive

    # Condition 5: Default Aggression
    return 'D'

```

**Self-Correction/Refinement on Pseudocode Condition 2:**
The initial thought for Condition 2 was good, but let's refine it to be more aggressive and cover more scenarios where cooperation might be beneficial if close.

Revised Condition 2 Logic:
*   If `cooperators_last_round >= m - 1` AND `cooperators_last_round < m`:
    *   **Cooperate (C).**
    *   **Rationale:** This is the most critical junction. Regardless of my last action, if there's a chance that my cooperation can push the group to success, I'll take it. This is aggressive because it puts me at risk of being the only cooperator if others don't follow, but it's a calculated risk to secure the `k` reward.

Let's integrate this refined logic into the pseudocode:

```python
def calculate_action(n, m, k, r, current_round, my_last_action, cooperators_last_round):
    """
    Calculates the action for the current round based on game parameters and history.

    Args:
        n (int): Number of players.
        m (int): Minimum cooperators needed.
        k (float): Reward if threshold met factor.
        r (int): Total number of rounds.
        current_round (int): The current round number (1-indexed).
        my_last_action (str): Player's action in the previous round ('C' or 'D').
        cooperators_last_round (int): Number of players who cooperated in the previous round.

    Returns:
        str: The action to take ('C' or 'D').
    """

    # Edge Case: First Round
    if current_round == 1:
        return 'D' # Always defect in the first round for guaranteed payoff

    # Decision Logic for subsequent rounds

    # Condition 1: "Guaranteed Win Scenario" - If threshold was met AND I cooperated
    if cooperators_last_round >= m and my_last_action == 'C':
        return 'C'
        # Rationale: Continue the winning pattern, reinforce success.

    # Condition 2: "Exploiting Proximity to Threshold" - If we were ONE cooperator away
    if cooperators_last_round >= m - 1 and cooperators_last_round < m:
        return 'C'
        # Rationale: This is the most opportunistic moment. Aggressively try to push
        # the group over the threshold, hoping to secure the reward 'k'.
        # Risk is acceptable here for the potential gain.

    # Condition 3: "Punishing Failure & Self-Preservation" - If threshold significantly missed
    if cooperators_last_round < m - 1:
        return 'D'
        # Rationale: Too far from success, cooperate and get 0. Defect and get 1.

    # Condition 4: "Aggressive Reward Seizure (when I defected last and it worked)"
    if cooperators_last_round >= m and my_last_action == 'D':
        return 'C'
        # Rationale: The threshold was met, and I benefited from defecting.
        # Now, cooperate to ensure the reward 'k' continues for future rounds.
        # This is a strategic contribution to keep the reward mechanism active.

    # Condition 5: "Default Aggression" - If none of the above conditions are met
    return 'D'
    # Rationale: The safe, aggressive default to guarantee a payoff of 1.
```
'''

description_AGGRESSIVE_109 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff while leveraging the cooperative reward, adapting to player behavior, and embracing an assertive approach.

## Aggressive Strategy: "The Leveraged Risk-Taker"

This strategy aims to be a decisive player, willing to initiate cooperation when the potential reward is high and to capitalize on the defections of others. It's about making the system work for you, not waiting for others to lead.

### Core Philosophy:

*   **Leverage Cooperation:** Cooperate only when it's strategically advantageous and likely to trigger the higher reward payoff. The "m" threshold is our primary trigger.
*   **Punish Defection:** Be slow to cooperate if others are consistently defecting and the threshold is not met.
*   **Exploit Opportunities:** If others are cooperating and pushing towards the threshold, don't be afraid to join in to secure the reward.
*   **Round Awareness:** Adapt behavior based on the proximity to the end of the game.

### Decision Rules:

The decision for player *i* in round *t* depends on three primary factors: the current round number (*t*), the observed cooperation rate in the previous round (*t-1*), and a self-defined "cooperation threshold" that will evolve throughout the game.

Let's define:

*   **`observed_cooperation_rate_prev_round`**: The proportion of players (excluding yourself) who cooperated in the previous round.
*   **`my_cooperation_threshold`**: A dynamic threshold representing how many *other* players need to cooperate for me to consider cooperating myself. This starts high and gradually lowers.

**1. Initial Cooperation Threshold:**

*   In the first round, to establish an aggressive stance and signal intent, we will begin with a relatively high `my_cooperation_threshold`. This prevents immediate exploitation by pure defectors.
    *   `my_cooperation_threshold` = `n` - `m` (This means at least `m` players including myself need to cooperate to trigger my cooperation. This is quite aggressive initially).

**2. Round-by-Round Decision Logic:**

*   **If `t == r` (Last Round):**
    *   **Cooperate (C):** If `observed_cooperation_rate_prev_round >= (m - 1) / (n - 1)` (i.e., if it's highly probable that *this round* will meet the `m` cooperation threshold based on last round's data). This is a calculated gamble to secure the final reward.
    *   **Defect (D):** Otherwise. It's too risky to try and trigger the reward alone at the very end if the momentum isn't there.

*   **If `t < r` (Not the Last Round):**
    *   **Cooperate (C):** If `observed_cooperation_rate_prev_round >= my_cooperation_threshold / (n - 1)` AND `(t / r) < 0.8` (i.e., if enough others cooperated in the previous round to meet my current aggressive threshold AND we are not in the final 20% of the game where risk-taking becomes more critical). This signifies a strong signal of potential collective success.
    *   **Defect (D):** Otherwise. This is the default aggressive stance. We are not cooperating unless the environment is demonstrably moving towards cooperation and we are not approaching the endgame too closely.

**3. Adaptive `my_cooperation_threshold` Adjustment:**

This is where the "aggressive" and "adaptive" nature truly shines.

*   **If `observed_cooperation_rate_prev_round` is high AND `m` was met in the previous round:**
    *   **Decrease `my_cooperation_threshold`:** Reduce `my_cooperation_threshold` by a small amount (e.g., `my_cooperation_threshold = max(1, my_cooperation_threshold - 1)`). This signals increasing willingness to cooperate as the group proves its capability. We want to be part of the successful group.
*   **If `observed_cooperation_rate_prev_round` is low OR `m` was NOT met in the previous round:**
    *   **Increase `my_cooperation_threshold`:** Increase `my_cooperation_threshold` by a small amount (e.g., `my_cooperation_threshold = my_cooperation_threshold + 1`). This punishes a lack of cooperation from others and increases the hurdle for us to cooperate. We are not going to be the sole cooperators if the group is not pulling its weight.

**Important Note on `my_cooperation_threshold` Adjustment:** The adjustment should be capped. It can't go below 0 (as it represents the number of *other* players).

### Edge Cases and Handling:

*   **First Round (`t=1`):**
    *   Decision: **Defect (D)** unless `n-m == 0` (which is not allowed by `m < n`). Our initial `my_cooperation_threshold` is set to `n-m`. We will *not* cooperate in the first round unless `m=n`, which is impossible by the parameters. This is an aggressive stance: we wait for signals.
    *   `my_cooperation_threshold` starts at `n-m`.

*   **Last Round (`t=r`):**
    *   Decision: As described above, we make a final calculated gamble to cooperate if the previous round's data strongly suggests the `m` threshold will be met. This is a high-stakes, aggressive move to secure the final reward.

*   **Rounds with No Prior Data (e.g., `t=1`):**
    *   The `observed_cooperation_rate_prev_round` will be treated as 0. Our initial aggressive threshold will likely lead to defection.

### Pseudocode Representation:

```python
class AggressiveLeveragedRiskTakerStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k

        # Initialize aggressive cooperation threshold (number of *other* players needed)
        # Start high: require almost full cooperation to join initially.
        self.my_cooperation_threshold = max(1, self.n - self.m) # Ensure it's at least 1

        # Track history for observed cooperation rate
        self.previous_actions = [] # List of tuples (player_id, action)

    def decide_action(self, current_round, player_id):
        """
        Decides action (C or D) for the current round.
        current_round: The current round number (1-indexed).
        player_id: The ID of this player (not used for strategy logic, but good practice).
        """

        # 1. Calculate observed cooperation rate from the previous round
        observed_cooperation_rate_prev_round = 0
        if current_round > 1 and self.previous_actions:
            cooperators_last_round = sum(1 for p_id, action in self.previous_actions if action == 'C')
            # Exclude self from count if playing against itself in history is a possibility (it's not here)
            # We count all players, and the threshold is relative to n-1.
            observed_cooperation_rate_prev_round = cooperators_last_round / (self.n - 1) if self.n > 1 else 0

        # 2. Decision Logic

        # Last Round: Aggressive Gamble
        if current_round == self.r:
            # Cooperate if it's highly likely the threshold will be met this round
            # (m-1) because we need m total, and we are one potential cooperator.
            if observed_cooperation_rate_prev_round >= (self.m - 1) / (self.n - 1):
                return 'C'
            else:
                return 'D'

        # Early to Mid-Game: Wait for Strong Signals
        else:
            # Cooperate if others are cooperating enough to meet our dynamic threshold,
            # AND we are not too close to the end (to avoid risky endgame plays too early).
            if (observed_cooperation_rate_prev_round >= self.my_cooperation_threshold / (self.n - 1)) and \
               (current_round / self.r < 0.8):
                return 'C'
            else:
                return 'D'

    def update_history(self, actions_this_round, player_id):
        """
        Updates internal history with observed actions from the completed round.
        actions_this_round: A list of tuples (player_id, action) for all players in the round.
        player_id: The ID of this player.
        """
        self.previous_actions = actions_this_round

        # 3. Adaptive Threshold Adjustment (after decision but before next round)
        cooperators_last_round = sum(1 for p_id, action in actions_this_round if action == 'C')
        # We need to know if 'm' was met in the previous round.
        m_was_met = cooperators_last_round >= self.m

        # Calculate observed cooperation rate of *others*
        observed_cooperation_rate_of_others = 0
        if self.n > 1:
            observed_cooperation_rate_of_others = (cooperators_last_round - 1) / (self.n - 1) if actions_this_round[player_id-1][1] == 'C' else cooperators_last_round / (self.n - 1)


        if m_was_met:
            # Positive reinforcement: If the group succeeded, we lower our threshold
            # to encourage future cooperation and become more likely to join.
            self.my_cooperation_threshold = max(1, self.my_cooperation_threshold - 1) # Ensure it's at least 1
        else:
            # Negative reinforcement: If the group failed, we raise our threshold
            # to discourage being the sole cooperator and punish lack of collective effort.
            self.my_cooperation_threshold += 1

```

### Aggressive Mindset Justification:

1.  **Initial Defection:** By defecting in the first round, we are not taking an immediate, potentially wasteful risk. We are observing and establishing our position as someone who won't be easily exploited.
2.  **High Initial Threshold:** `n-m` is a very demanding starting point. It means we only consider cooperating if almost everyone else is already cooperating. This is inherently aggressive – we're not *seeking* cooperation, we're waiting for overwhelming evidence of it.
3.  **Lowering Threshold as Reward is Proven:** When cooperation *does* lead to success (`m` is met), we become more willing to join. This is about *leveraging* the proven success of others. We are not initiating the altruistic act, but rather jumping on board when the system clearly benefits.
4.  **Increasing Threshold on Failure:** If cooperation fails, we increase our demand. This is punishing the group for its inability to coordinate. We are saying, "You failed to meet the threshold, so I demand even more from you next time before I consider contributing."
5.  **Endgame Gamble:** The final round decision is a calculated, high-reward push. If there's even a strong hint that the threshold will be met, we go for it. This is a key aggressive play to maximize payoff in the final moments, assuming the risk is well-assessed.
6.  **"Not Too Close to the End" Condition:** This prevents premature, risky cooperation in the middle game. We want to reserve our boldest moves for when the outcome is more critical or when the group has demonstrated a consistent ability to cooperate.

This strategy is designed to be a strong, independent actor that adapts based on the success or failure of the collective, always with an eye on maximizing its own return by strategically participating in the cooperative reward.
'''

description_AGGRESSIVE_110 = '''
Here is an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while adapting to the game's dynamics. This strategy prioritizes defecting when cooperation seems risky and exploiting opportunities for high rewards when cooperation is likely to succeed.

## Aggressive "Opportunistic Exploiter" Strategy

This strategy is designed to be relentlessly self-interested, but not blindly so. It aims to extract maximum payoff by defecting when there's uncertainty or a high likelihood of others cooperating to reach the threshold, and cooperating only when the reward is almost guaranteed and the personal cost is minimal.

### 1. Decision Rules: When to Cooperate vs. Defect

The core of this strategy is to assess the perceived "probability of success" for cooperation in the current round. This probability is not explicitly calculated, but rather inferred from historical actions.

**Key Metric: Cooperation Success Rate (CSR)**

We will track the historical success rate of cooperation across all rounds.
*   **CSR (t):** The proportion of rounds (up to round *t-1*) where *m* or more players cooperated.

**Decision Logic:**

*   **If CSR (t-1) is high (e.g., > 0.7):** The environment appears to be one where cooperation is generally successful. In this scenario, we want to exploit this tendency.
    *   **Decision:** **DEFECT (D)**. The rationale is that others are likely to cooperate, pushing the total number of cooperators to *m* or more. By defecting, you keep your full endowment (1) *and* receive the reward *k* due to the collective success, yielding a payoff of **1 + k**. This is the highest possible individual payoff in any round.
*   **If CSR (t-1) is moderate (e.g., 0.3 < CSR (t-1) ≤ 0.7):** There is some uncertainty. Cooperation is not guaranteed, but it's not a complete failure either. We need to be more cautious.
    *   **Decision:** **DEFECT (D)**. The risk of defecting is lower than the risk of cooperating and failing to reach the threshold. If we defect and others defect, we get 1. If we defect and enough others cooperate, we get 1+k. If we cooperate and others defect, we get 0. Defection offers a safer path to at least 1, and the potential for 1+k.
*   **If CSR (t-1) is low (e.g., CSR (t-1) ≤ 0.3):** Cooperation is generally not happening or not succeeding.
    *   **Decision:** **DEFECT (D)**. In a low-cooperation environment, the incentive to defect is even stronger. Trying to cooperate is highly likely to be fruitless and result in a payoff of 0.

**An Exception: The "Guaranteed Win" Scenario**

There's one crucial scenario where we *will* cooperate, even if the CSR is low, to maximize our guaranteed outcome.

*   **Condition:** If the number of players who have already cooperated in the *current round's history* (if this information is available, which it isn't in a simultaneous game, but we can infer from patterns of *successful* past rounds where our own actions were D) is *m-1*, and we are the *only* player whose action is not yet revealed (this is a hypothetical situation given simultaneous moves, but represents the idea of a near-certain outcome).
    *   **Practical Application in Simultaneous Moves:** This translates to: If, based on our strategy's assessment of the game state (e.g., if we observed a pattern where a certain number of players *always* cooperate when it's near the end), we believe our cooperation is the *final piece* to guarantee the reward for everyone, *and* our personal cost is 0 (we are defecting), we will defect. If we are to cooperate to secure the bonus, we do it only if it guarantees us 1+k.

**Revised Decision Logic (Simplified & Aggressive):**

Our primary goal is to secure the **1 + k** payoff. We only cooperate if we are absolutely certain our cooperation is *necessary* and *sufficient* to trigger the reward, and if defecting would yield less.

1.  **If the number of players cooperating in the previous round was *at least* m-1, AND you played D last round:** This suggests that if at least one more player cooperates, the threshold will be met. You will **DEFECT (D)**. You want to be the defector in a successful cooperative round to get 1+k.
2.  **If the number of players cooperating in the previous round was *less than* m-1, AND you played C last round:** Your cooperation didn't help much. You will **DEFECT (D)**.
3.  **In all other cases:** **DEFECT (D)**.

This strategy is based on the premise that other players *might* try to cooperate. If they do, you want to be on the side that gets the highest payoff: defecting when the threshold is met. You are not going to be the one taking the risk of cooperating to *help* them reach the threshold if there's any doubt. You only consider cooperation if it's a near-certain path to your own maximum payoff of 1+k.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** **DEFECT (D)**. There is no historical data. The risk of cooperating and failing (payoff 0) is too high compared to the guaranteed payoff of 1 by defecting. This also sets a precedent of opportunism.
*   **Last Round (t=r):**
    *   **Decision:** **DEFECT (D)**. In the last round, there's no future interaction to consider. The optimal strategy is to maximize immediate gain. If we believe others will cooperate to reach the threshold, defecting gives us 1+k. If they don't, defecting gives us 1. There's no incentive for altruistic cooperation.
*   **Near the last round (e.g., t = r-1, r-2):**
    *   **Decision:** **DEFECT (D)**. Similar to the last round, the focus remains on maximizing personal gain. The long-term implications of establishing a cooperative norm are less relevant when the game is winding down.

### 3. Aggressiveness Alignment

This strategy is aggressive in several ways:

*   **Default to Defection:** The default action is always to defect. Cooperation is an exception, not the rule.
*   **Exploitation of Collective Success:** It actively seeks to benefit from others' cooperation without contributing to it, whenever possible. This is the core of its aggressive nature – maximizing personal gain at minimal or no risk.
*   **No Risk-Taking:** It avoids the risk of personal loss (payoff of 0) by not cooperating unless the outcome is overwhelmingly favorable for itself.
*   **Independent Gain Focus:** It does not consider the overall welfare of the group or the establishment of a cooperative equilibrium. Its sole focus is on maximizing its own payoff.

### Pseudocode Representation

```python
# Parameters
n = number of players
r = number of rounds
m = minimum cooperators needed
k = reward factor

# History tracker (initialize as empty)
# For simplicity, we'll track the number of successful cooperation rounds.
# A more sophisticated tracker might look at individual player actions.
cooperation_success_history = [] # Stores True if threshold met, False otherwise

# Strategy function for player i
def decide_action(round_number, history):
    # history will contain the list of actions taken by all players in previous rounds,
    # and our own actions. For this strategy, we primarily need 'cooperation_success_history'.

    # Initialize cooperation_success_history if it's the first round
    if not cooperation_success_history:
        # In a real implementation, this would be passed in or globally managed.
        # For this conceptualization, we assume it's available.
        pass # Assume it's pre-populated or managed externally for t > 1

    # --- Edge Cases ---
    if round_number == 1:
        return 'D' # Defect in the first round (no history)

    if round_number == r:
        return 'D' # Defect in the last round (no future consequences)

    # --- Decision Rules ---
    if not cooperation_success_history: # Should not happen for r > 1 unless history is incomplete
        return 'D' # Safest default

    # Calculate Cooperation Success Rate (CSR)
    successful_rounds = sum(cooperation_success_history)
    total_past_rounds = len(cooperation_success_history)
    csr = successful_rounds / total_past_rounds if total_past_rounds > 0 else 0

    # --- Aggressive Decision Logic ---
    # We will defect if there's a high chance of others cooperating or if we can't reliably predict.
    # Cooperation is only considered if it *guarantees* a payoff for us.

    # If CSR is high, assume others will cooperate, so we defect to get 1+k
    if csr > 0.7: # Threshold for "high CSR"
        return 'D'
    # If CSR is moderate, still too risky to cooperate. Defect.
    elif csr > 0.3: # Threshold for "moderate CSR"
        return 'D'
    # If CSR is low, cooperation is unlikely to succeed. Defect.
    else: # csr <= 0.3
        return 'D'

    # Note: The explicit "guaranteed win" scenario is hard to implement in a simultaneous game
    # without looking ahead at other players' implicit strategies. The current logic implicitly
    # assumes others will *try* to cooperate in high-CSR environments, allowing us to exploit.
    # If we were to implement a scenario where we check "am I the only one needed?" it would
    # require a much more complex historical analysis of individual player tendencies.
    # For now, we stick to exploiting observed group behavior.

# Example usage (conceptual):
# my_action = decide_action(current_round, past_round_outcomes)
# Update cooperation_success_history based on the outcome of the round.
```

This strategy embodies aggression by prioritizing self-gain, exploiting collective efforts, and minimizing personal risk. It's adaptive in that it *observes* past outcomes to inform its decisions, but its inherent bias is towards defection, making it a formidable opponent in a tournament setting.
'''

description_AGGRESSIVE_111 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while being adaptable to various opponent behaviors.

## Strategy: "The Opportunistic Dominator"

This strategy is rooted in an aggressive mindset: **exploit the system to your advantage, and only cooperate when your gain is guaranteed and substantial, or when it's absolutely necessary to avoid your own ruin.** It prioritizes personal enrichment through defection while leveraging the cooperation of others when it serves its purpose.

### Core Philosophy:

*   **Maximize Private Gain:** Always aim to keep your endowment. Defection is the default.
*   **Leverage Cooperation, Don't Initiate It:** Only "pay" the cost of cooperation (losing your endowment) if the reward is overwhelmingly attractive *and* secured.
*   **Punish Underperformance (Subtly):** If the group consistently fails to meet the threshold, the strategy will reflect that by continuing to defect, effectively signaling "I won't carry dead weight."
*   **Adapt to the Group's Dominant Tendency:** If the group leans towards cooperation, exploit it. If they lean towards defection, mirror them.

### Decision Rules:

The strategy's decision for player `i` in round `t` depends on a dynamic assessment of the group's behavior and the game parameters.

**1. Cooperation Trigger:**

*   **Cooperate (C) ONLY IF:**
    *   The number of players who cooperated in the *previous* round (`cooperators_last_round`) is **at least `m - 1`**.
    *   **AND** the number of players who *will* cooperate in the current round, *including yourself if you cooperate*, is guaranteed to be **at least `m`**.

    This second condition is crucial and reflects the aggressive nature. It means you will only cooperate if you are confident that your cooperation *plus* the cooperation of others will meet the threshold. This can be estimated by observing the number of players who cooperated in the previous round. If `cooperators_last_round` is `m-1` or more, and you believe at least one other player will also cooperate (a reasonable assumption if they have an incentive to do so, or if the game is nearing the end and they need to extract value), you will cooperate.

    *   **Simplified heuristic:** Cooperate if `cooperators_last_round >= m - 1` and `cooperators_last_round + 1 >= m`. This essentially means cooperate if at least `m-1` players cooperated last round, and you are confident that your cooperation will push the total to `m`.

**2. Defection Trigger:**

*   **Defect (D) in ALL OTHER CASES.**

    This is the aggressive default. You will defect if:
    *   The number of cooperators in the previous round was too low to suggest the threshold will be met.
    *   You are unsure if your cooperation will be the deciding factor (i.e., you don't have a high degree of confidence that `cooperators_last_round + 1 >= m`).

### Handling Edge Cases:

*   **Round 1:**
    *   **Default to Defect (D).** In the first round, there is no history. The aggressive instinct is to secure your endowment. You are observing the environment. You want to see how others behave and if they are willing to cooperate. Cooperating here is essentially an act of faith, which this strategy eschews.
*   **The Last Round (Round `r`):**
    *   **Cooperate (C) if `cooperators_last_round >= m - 1`.** In the final round, the long-term consequences of your actions (future rounds) are gone. The immediate reward is all that matters. If the group was on the cusp of success in the penultimate round, you now have a stronger incentive to push it over the edge to secure the reward for everyone (including yourself). The risk of "wasted" cooperation is removed. This is a calculated gamble to secure the `k` multiplier on your last possible payoff.
    *   **Defect (D) if `cooperators_last_round < m - 1`.** If the group was far from cooperating, you still defect, as the chances of success are too low to justify the cost.

### Aggressive Justification:

*   **Exploiting the Risk Averse:** This strategy effectively "free-rides" on the cooperation of others when possible. If `m` or more players cooperate naturally, and you defect, you still get the reward `k` but keep your full endowment (payoff = 1 + k). This is the ultimate aggressive play – profiting from the collective effort without contributing.
*   **Minimizing Risk:** By only cooperating when the threshold is virtually guaranteed, you minimize your own risk of contributing to a failed project and receiving only your endowment (payoff = 1).
*   **Psychological Warfare:** By consistently defecting unless the conditions are overwhelmingly favorable, this strategy can subtly pressure other players. If many players adopt similar strategies, it can lead to cycles of defection, but also potentially to "panic" cooperation as players try to reach the threshold in later rounds.
*   **Last Round Push:** The modified behavior in the last round is a calculated act of aggression. It's about maximizing your final haul, leveraging any prior momentum. It's a demonstration that even in a cooperative context, your primary goal is personal gain, but you're intelligent enough to recognize when a final push for collective reward benefits *you* directly.

### Pseudocode (Conceptual):

```python
def get_player_action(player_id, round_num, all_actions_history, game_parameters):
    n = game_parameters['n']
    r = game_parameters['r']
    m = game_parameters['m']
    k = game_parameters['k']

    # Count cooperators in the previous round
    cooperators_last_round = 0
    if round_num > 1:
        previous_round_actions = all_actions_history[round_num - 2] # 0-indexed history
        for action in previous_round_actions:
            if action == 'C':
                cooperators_last_round += 1

    # Decision logic
    if round_num == r: # Last round
        if cooperators_last_round >= m - 1:
            return 'C'
        else:
            return 'D'
    elif round_num == 0: # First round
        return 'D' # Default to defect
    else: # Middle rounds
        # Aggressive cooperation trigger:
        # Cooperate only if previous round was very close AND your cooperation guarantees reaching m
        # A simple heuristic: if previous round was m-1 or more, AND you believe at least one other will cooperate, or if you know others cooperated.
        # More robust: If cooperators_last_round is at least m-1, and we assume at least one more will join (or if we can infer from opponent strategies that they are incentivized to cooperate if close).
        # For this aggressive strategy, we'll be conservative:
        if cooperators_last_round >= m - 1 and (cooperators_last_round + 1 >= m):
            return 'C'
        else:
            return 'D'

```

**Explanation of the `(cooperators_last_round + 1 >= m)` condition:**

This is the critical piece of aggression. It means "If the previous round was close to `m` (specifically `m-1` or more), *and* if I cooperate, we will *definitely* reach `m`." This is a strong guarantee. It implies you have a high degree of confidence that you won't be the *only* cooperator if the previous round was only at `m-1`. The strategy implicitly assumes that if `m-1` people cooperated last round, at least one other person is also strategically motivated to see the threshold met, or is also playing a strategy that will cooperate under these conditions. This is a calculated risk, but one that prioritizes securing the `k` reward.

This strategy is designed to be a powerful contender in a tournament setting where predicting and exploiting opponent behavior is key. It avoids altruism and focuses on extracting maximum value.
'''

description_AGGRESSIVE_112 = '''
This is an intriguing setup! The core tension lies in the dilemma: defecting is safe and guarantees a baseline payoff, while cooperation is risky but offers a higher reward if enough others also cooperate. My aggressive strategy will aim to exploit this while minimizing my own risk.

**Strategy Name:** The Opportunistic Gambler

**Core Philosophy:** Maximize personal gain by defecting when cooperation is unlikely to succeed and cooperating decisively when success is assured or the reward is exceptionally high relative to the risk. This strategy is designed to be aggressive in its pursuit of the bonus payout but cautious enough to avoid being exploited by a majority of defectors.

---

**1. Decision Rules: When to Cooperate vs. Defect**

My decision to cooperate or defect in any given round will be based on a dynamic assessment of the likelihood of reaching the cooperation threshold `m`. This assessment will be heavily influenced by the history of play.

**Key Metric: Cooperation Success Probability (CSP)**

I will maintain an internal estimate of the probability that the cooperation threshold `m` will be met in the current round. This CSP will be updated based on the observed actions of other players in previous rounds.

**Decision Logic:**

*   **If CSP < Threshold_Cooperate:** I will **Defect**. The risk of cooperating and failing is too high. I will secure my individual payoff of 1.
*   **If CSP > Threshold_Defect:** I will **Cooperate**. The probability of success is high enough to warrant the risk for the increased payoff.
*   **Between Threshold_Cooperate and Threshold_Defect:** This is where the "aggression" and adaptation come in. I will introduce an *aggression factor* that pushes me towards cooperation as the CSP increases. The higher the CSP within this range, the more likely I am to cooperate.

**Defining the Thresholds and Aggression Factor:**

*   **Threshold_Cooperate:** This will be a value slightly above the bare minimum probability of success. For example, if `m` players are needed, and there are `n` players, the absolute minimum probability of *my* cooperation being part of a successful coalition is if `m-1` *other* players cooperate. However, I need to factor in the likelihood of *all* others cooperating. A conservative starting point might be related to the proportion of players that need to cooperate: `m/n`. I'll set `Threshold_Cooperate` slightly higher, say, `m/n + 0.1` (but capped at 1.0).
*   **Threshold_Defect:** This will be a lower value, representing a point where I am very reluctant to cooperate. Perhaps `m/n - 0.1` (but floored at 0.0).
*   **Aggression Factor (AF):** This factor will scale my decision to cooperate within the undecided range. A higher AF means I'm more willing to gamble. I will use a dynamically adjusted AF.

**How CSP is Estimated (Simplified Pseudocode):**

```pseudocode
// Initialize CSP
current_round = 1
cooperation_history = [] // List of lists, each inner list is players who cooperated in a round

function update_csp(previous_actions_per_round):
  // Count how many times the threshold 'm' was met or exceeded in past rounds
  successful_cooperation_rounds = 0
  for round_actions in previous_actions_per_round:
    cooperators = count_cooperators(round_actions)
    if cooperators >= m:
      successful_cooperation_rounds += 1

  // Simple empirical probability (can be enhanced with Bayesian methods later)
  if len(previous_actions_per_round) == 0:
    // Initial guess - could be optimistic or pessimistic based on desired aggression
    // For an aggressive strategy, let's start with a slightly optimistic guess
    return m / n  // Proportion of players that need to cooperate

  empirical_probability = successful_cooperation_rounds / len(previous_actions_per_round)

  // Refine CSP: Blend empirical probability with a slight lean towards recent trends
  // This prevents drastic shifts based on a few outliers.
  // Let's use a simple weighted average. A higher weight for recent rounds makes it more adaptive.
  recent_rounds_weight = 0.7 // Adjust this for adaptiveness vs. stability
  historical_weight = 1.0 - recent_rounds_weight

  // Calculate weighted average CSP
  weighted_csp = (empirical_probability * historical_weight) + (last_round_cooperation_success * recent_rounds_weight)

  // Further adjust based on my own past behavior (if I cooperated and it succeeded, slightly increase CSP. If I defected and it failed, slightly decrease)
  if my_last_action == 'C' and last_round_cooperation_success:
      weighted_csp = min(1.0, weighted_csp + 0.05) // Small boost for personal success
  elif my_last_action == 'D' and not last_round_cooperation_success:
      weighted_csp = max(0.0, weighted_csp - 0.05) // Small decrease if I defected and failed

  return weighted_csp

// Main Decision Function
function decide_action(current_round, game_history):
  my_actions_history = game_history['my_actions']
  all_players_actions_history = game_history['all_players_actions'] // Structured by round

  csp = update_csp(all_players_actions_history)

  // --- Thresholds (can be tuned for more or less aggression) ---
  threshold_cooperate_base = m / n
  threshold_defect_base = m / n - 0.1 // Slightly more conservative than cooperate

  // Adjust thresholds based on parameters for more strategic play
  // If k is very high, we might be willing to cooperate even if CSP is lower.
  // If m/n is very high (almost everyone needs to cooperate), CSP needs to be higher.
  // This is a place for fine-tuning. For now, let's use a simple heuristic:
  // If k is significantly larger than 1 (e.g., k > 2), increase aggression.
  aggression_boost_factor = 1.0
  if k > 2.0:
      aggression_boost_factor = 1.2 // Lean more towards cooperating

  Threshold_Cooperate = min(1.0, threshold_cooperate_base * aggression_boost_factor)
  Threshold_Defect = max(0.0, threshold_defect_base * aggression_boost_factor)

  // --- Aggression Factor for the middle ground ---
  // This makes the strategy more adaptive to the current game dynamics.
  // If CSP is high, be more aggressive. If it's lower, be more cautious.
  // We can use a simple linear interpolation or a more complex curve.
  // Let's use linear for simplicity:
  if csp < Threshold_Defect:
    return 'D' // Defect
  elif csp > Threshold_Cooperate:
    return 'C' // Cooperate
  else:
    // Intermediate zone: Decision is probabilistic based on CSP and aggression
    // A simple linear mapping: Higher CSP means higher probability of cooperation.
    // For example, at Threshold_Defect, probability of cooperating is 0%.
    // At Threshold_Cooperate, probability is 100%.
    probability_of_cooperating = (csp - Threshold_Defect) / (Threshold_Cooperate - Threshold_Defect)

    // Apply a dynamic aggression multiplier. This multiplier can be adjusted.
    // A higher multiplier means we're more likely to cooperate in the middle zone.
    // This multiplier itself could be learned or set based on 'k'.
    dynamic_aggression_multiplier = 1.0 // Placeholder - could be linked to k or learned
    if k > 2.0:
        dynamic_aggression_multiplier = 1.2 // Be more aggressive when reward is high

    adjusted_probability_of_cooperating = min(1.0, probability_of_cooperating * dynamic_aggression_multiplier)

    // Make a random choice based on the adjusted probability
    random_chance = random_number_between_0_and_1()
    if random_chance < adjusted_probability_of_cooperating:
      return 'C' // Cooperate
    else:
      return 'D' // Defect

```

---

**2. Handling Edge Cases**

*   **First Round (t=1):**
    *   Since there is no history, my CSP estimate will be based on a pre-programmed initial guess. For an aggressive strategy, I'll lean towards a slightly optimistic initial guess. I'll set my initial CSP to `m/n`.
    *   My decision will then follow the `decide_action` logic with this initial CSP. This means I'm not blindly cooperating, but making a calculated gamble based on the required proportion of cooperators. If `m/n` is low, I might cooperate; if it's high, I might defect.

*   **Last Round (t=r):**
    *   In the last round, there is no future to consider. This changes the incentives. The best strategy is to maximize immediate payoff *without* any regard for future cooperation.
    *   **Decision Rule for Last Round:** **Always Defect.**
        *   If I cooperate, I might contribute to a successful coalition, and my payoff is `(1-1) + k = k`.
        *   If I defect, my payoff is `(1-0) = 1`.
        *   **However,** if `m` or more players cooperate, and I defect, my payoff is `1 + k`. This is the highest possible payoff.
        *   If fewer than `m` players cooperate, and I defect, my payoff is `1`.
        *   The crucial point is that defecting guarantees at least a payoff of 1, and *potentially* the highest payoff (`1+k`) if the others succeed. Cooperating guarantees `k` if successful, but only 0 if unsuccessful. Since there's no future to build trust or retaliate, defecting is the purely greedy and maximizing move in the final round, assuming the highest possible payoff is the goal.

*   **No History (Initial Rounds if `r` is small and `m/n` is high):**
    *   If, by chance, the initial rounds are too few for a reliable CSP estimate, and the initial `m/n` is high, my `decide_action` logic will still apply. The `update_csp` function will use its initial guess. If this initial guess falls below `Threshold_Defect`, I will defect. This maintains my aggressive but risk-averse stance.

---

**3. Aggression Alignment**

My aggression is manifested in several ways:

*   **Proactive Cooperation when Probability is High:** I don't wait for overwhelming evidence. Once my estimated CSP (`csp`) crosses my `Threshold_Cooperate`, I commit to cooperating, even if it means taking a risk.
*   **Leveraging the Middle Zone:** The intermediate zone between `Threshold_Defect` and `Threshold_Cooperate` is where I apply my *dynamic aggression multiplier*. If `k` is high, this multiplier increases, making me more likely to cooperate in this uncertain but potentially rewarding zone. I am not passively waiting; I am actively seeking opportunities for the bonus.
*   **Initial Optimism:** My starting CSP in round 1 is slightly optimistic (`m/n`), indicating a willingness to attempt cooperation from the outset.
*   **Exploiting High `k`:** The `aggression_boost_factor` specifically scales my thresholds upwards when `k` is high, meaning I require a *slightly lower* CSP to initiate cooperation because the reward is so much greater. This is a direct pursuit of maximizing payoff.
*   **Last Round Betrayal:** The definitive defection in the last round is a brutal display of aggressive self-interest. It signals a willingness to discard any semblance of cooperation for maximum immediate gain.

**Example Scenario (n=6, m=3, k=2):**

*   **Initial Round:** `m/n = 3/6 = 0.5`. `Threshold_Cooperate` might be `0.5 + 0.1 = 0.6`. `Threshold_Defect` might be `0.5 - 0.1 = 0.4`. My initial `csp` is `0.5`.
    *   Since `0.4 < 0.5 < 0.6`, I'm in the middle zone. My `probability_of_cooperating` would be `(0.5 - 0.4) / (0.6 - 0.4) = 0.1 / 0.2 = 0.5`.
    *   With `dynamic_aggression_multiplier = 1.0` (since `k=2` is not `> 2`), my `adjusted_probability_of_cooperating` is `0.5`. I'll flip a coin: 50% chance I cooperate, 50% chance I defect.
*   **Round 2:** Let's say in Round 1, 2 players cooperated, and 4 defected. My `csp` calculation will likely show a lower probability of success.
    *   If `csp` drops below `0.4`, I will **Defect**.
*   **Round 3:** Let's say in Round 2, 4 players cooperated, and 2 defected. My `csp` will rise.
    *   If `csp` rises above `0.6`, I will **Cooperate**.
    *   If `csp` is still in the middle zone (e.g., `0.5`), I'll again use my probabilistic decision based on the adjusted probability.

This strategy is designed to be an "Opportunistic Gambler" – it's not a pure altruist nor a pure free-rider. It intelligently assesses risk and reward, and when the numbers suggest a profitable gamble with a good chance of success, it takes it aggressively. When the odds are poor, it retreats to secure its individual endowment.
'''

description_AGGRESSIVE_113 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the game's structure and parameters. The core principle is to exploit cooperation from others while minimizing personal risk and maximizing potential rewards.

## Aggressive Strategy: "The Opportunist"

This strategy is named "The Opportunist" because its primary goal is to capitalize on opportunities presented by the actions of other players. It is designed to be aggressive by always seeking the highest possible payoff in the current round, while strategically preparing for future rounds.

### 1. Decision Rules

The decision to Cooperate (C) or Defect (D) in any given round is based on a calculation of the *perceived risk* and *potential reward* relative to the number of players who have cooperated in the *immediately preceding round*.

**Core Logic:**

*   **If the number of cooperators in the previous round was less than *m*:** This indicates a general lack of commitment to cooperation. The risk of contributing and not reaching the threshold is high. Therefore, **Defect (D)**. The only exception is if *I* am the sole reason for not reaching the threshold and the potential payoff difference is marginal.

*   **If the number of cooperators in the previous round was *m* or more:** This indicates a successful threshold was met or exceeded. Cooperation is working. However, to be aggressive, we want to ensure *we* benefit from this success without bearing the risk of being the *only* cooperator if others defect.
    *   **If the number of cooperators in the previous round was exactly *m*:** This is the *tightest* scenario where success was achieved. The risk of the next round failing due to a few defections is high. To maximize my payoff, if *I* cooperated last round and benefited, I will **Cooperate (C)** again, assuming others will likely continue to do so to secure the reward. If *I* defected last round, I will **Defect (D)**, as it's more profitable to keep the full endowment plus the shared reward.
    *   **If the number of cooperators in the previous round was *more than m*:** This indicates a strong tendency towards cooperation. The risk of failure is low. To maximize my individual gain, I will **Defect (D)**. This allows me to keep my full endowment and still receive the reward *k* because the threshold is met by others. This is the most aggressive move.

**Refined Logic for Cooperation (When the previous round *m* or more cooperated):**

To be truly aggressive and adaptive, we need a more nuanced rule for when to cooperate, even if the threshold was met. We will only cooperate if:

1.  The previous round had *m* or more cooperators.
2.  *And* the number of cooperators in the previous round was *less than m + (n - m)/2*. (This is a heuristic to gauge if cooperation is *barely* succeeding or *robustly* succeeding. If it's barely succeeding, we might want to reinforce it for our own benefit. If it's robustly succeeding, we can defect and reap the rewards.)
3.  *And* I was a cooperator in the previous round. (This is a form of reciprocity, but aggressive in that I only reciprocate if I benefited from their cooperation in the previous round.)

**Summary of Decision Rules:**

Let `N_C_prev` be the number of players who cooperated in the previous round.
Let `my_action_prev` be my action in the previous round.

*   **If `N_C_prev < m`:**
    *   **Defect (D)**. The risk is too high, and we cannot rely on others.

*   **If `N_C_prev >= m`:**
    *   **If `N_C_prev == m` AND `my_action_prev == C`:**
        *   **Cooperate (C)**. Reinforce fragile cooperation to ensure future reward.
    *   **Else if `N_C_prev < m + (n - m) / 2` AND `my_action_prev == C`:**
        *   **Cooperate (C)**. Cooperation is borderline; contribute to ensure it succeeds for our benefit.
    *   **Else:**
        *   **Defect (D)**. Aggressively exploit the cooperation of others.

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   In the absence of historical data, we must make an initial aggressive move. The safest aggressive bet is to **Defect (D)**. This ensures we keep our endowment and do not risk contributing to a project that might fail, with no guaranteed reward. If others cooperate, we benefit anyway.

*   **Last Round (t=r):**
    *   The strategy does not change for the last round. We continue to apply the same aggressive decision rules based on the penultimate round's outcome. The future is irrelevant, so maximizing the current round's payoff is paramount.

*   **The "Almost m" Scenario (where `m-1` players cooperated in the previous round):**
    *   If `N_C_prev == m-1`, we have a critical decision. If we are tempted to cooperate, we are essentially betting that *at least one* other player will also cooperate to reach the threshold. This is risky.
    *   **Decision:** **Defect (D)**. Our strategy prioritizes maximizing individual payoff and avoiding risk. If the threshold is not met, we get 1. If we cooperate and the threshold is *not* met, we get 0. The gain from cooperating (2 vs 1) is only realized if *enough* others also cooperate. Without certainty, we defect.

*   **The "Only me" Scenario (where I was the only cooperator, and `N_C_prev < m`):**
    *   If `N_C_prev` was below `m`, and I was the *only* cooperator, my payoff was 0.
    *   **Decision:** Continue to **Defect (D)**. My singular cooperation was unrewarded. There's no incentive to repeat an unrewarded action.

### 3. Aggressive Mindset Alignment

*   **Exploitation:** The core of the strategy is to exploit the efforts of others. When cooperation succeeds (`N_C_prev >= m`), the default action is to Defect (D), allowing us to keep our endowment (1) and still gain the reward (k) from others' contributions. This is the most aggressive way to maximize individual gain.
*   **Risk Aversion for Gains:** While aggressive in seeking rewards, the strategy is risk-averse when cooperation is uncertain. If the threshold `m` is not met, or is barely met, the strategy defaults to Defect (D) to secure the guaranteed private payoff of 1, rather than risking a payoff of 0.
*   **Strategic Cooperation as Leverage:** Cooperation is only employed in very specific, controlled circumstances:
    1.  To reinforce a *barely* successful cooperation (`N_C_prev == m` or `N_C_prev < m + (n-m)/2`) *only if* I also benefited from cooperation in the previous round. This is not altruism; it's a calculated investment to ensure a higher payoff in future rounds by preventing the collapse of cooperation that benefits me.
    2.  This makes the cooperation purely instrumental, a tool for future exploitation.

---

### Pseudocode Representation:

```python
def decide_action(round_num, n, m, k, history):
    # history is a list of tuples: [(player_i_action, num_cooperators_in_round), ...]
    # where player_i_action is 'C' or 'D', and num_cooperators_in_round is the total count

    # Edge Case: First Round
    if round_num == 1:
        return 'D' # Aggressively start by defecting

    # Get data from the previous round
    prev_round_data = history[-1]
    num_cooperators_prev = prev_round_data[1]
    my_action_prev = prev_round_data[0] # My action in the previous round

    # Aggressive Decision Logic
    if num_cooperators_prev < m:
        # Cooperation failed in the previous round, too risky to contribute
        return 'D'
    else: # num_cooperators_prev >= m
        # Cooperation succeeded. Now, decide if we exploit or reinforce.

        # Condition 1: Barely successful cooperation or borderline fragile cooperation
        # We reinforce ONLY if we also cooperated and benefited last round.
        if (num_cooperators_prev == m or num_cooperators_prev < m + (n - m) / 2) and my_action_prev == 'C':
            return 'C'
        else:
            # Aggressively exploit the success of others.
            return 'D'

```
'''

description_AGGRESSIVE_114 = '''
This aggressive strategy is designed to exploit the Collective Risk Dilemma by leveraging the potential for high rewards while minimizing personal risk through calculated defection. The core principle is to be a "smart defector" who cooperates only when the collective success is highly probable and lucrative, and otherwise prioritizes individual gain.

## Aggressive Strategy: "Opportunistic Exploiter"

**Core Philosophy:** Maximize personal payoff by defecting most of the time, but bravely cooperate when the reward is almost guaranteed and substantial. Be a free-rider whenever possible, but don't shy away from contributing if it leads to a massive payout with minimal personal risk.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect in any given round `t` depends on an assessment of the likelihood of reaching the cooperation threshold `m`, factoring in the history of play and the game parameters.

**Decision Threshold: `cooperation_probability_threshold`**

We will define a dynamic threshold, `cooperation_probability_threshold`, representing the minimum estimated probability that `m` or more players will cooperate in the current round for player `i` to consider cooperating.

**Factors Influencing the Decision:**

*   **Number of Cooperators in Previous Rounds:** This is the primary driver of our `cooperation_probability_threshold`. The more players who have cooperated in the past, the higher our confidence that cooperation is feasible.
*   **Your Own Previous Actions:** If you have cooperated and the threshold was met, it reinforces the idea that cooperation can be successful. If you defected and the threshold was met (meaning others cooperated without you), it highlights the free-riding opportunity.
*   **Game Parameters (n, m, k):** These are static but influence our overall risk tolerance. A higher `k` makes cooperation more attractive. A higher `m` relative to `n` makes cooperation harder and thus more risky.

**The Decision Algorithm (for round `t`):**

1.  **Calculate Historical Cooperation Rate:**
    *   Let `total_cooperators_in_history` be the sum of players who cooperated in rounds 1 to `t-1` (excluding yourself).
    *   Let `num_rounds_played` be `t-1`.
    *   If `num_rounds_played == 0` (i.e., first round), `historical_cooperation_rate = 0`.
    *   Otherwise, `historical_cooperation_rate = total_cooperators_in_history / (n - 1)`. (We exclude our own contribution to get a sense of others' behavior).

2.  **Determine Current `cooperation_probability_threshold`:**
    *   This threshold will be dynamic, decreasing as we see more cooperation.
    *   **Initial `cooperation_probability_threshold` (Round 1):** Set a relatively high threshold, reflecting our initial caution. A good starting point could be `m / n`. For example, if `m=3, n=6`, the initial threshold is `0.5`. This means we'd only consider cooperating if we strongly believe at least half the players *will* cooperate.
    *   **Adaptive Adjustment:**
        *   If `historical_cooperation_rate` is high (e.g., > 0.7), significantly decrease `cooperation_probability_threshold`.
        *   If `historical_cooperation_rate` is low (e.g., < 0.3), slightly increase `cooperation_probability_threshold`.
        *   If the threshold was met in the previous round, and you cooperated, slightly decrease `cooperation_probability_threshold` (reinforcing success).
        *   If the threshold was met in the previous round, and you defected (free-rode), slightly decrease `cooperation_probability_threshold` to encourage future defections when others carry the weight.
        *   If the threshold was NOT met in the previous round, slightly increase `cooperation_probability_threshold` (reflecting increased uncertainty).

    *   **Formalizing the Adjustment (Example):**
        Let `base_threshold_ratio = m / n`.
        Let `cooperation_history_impact = (historical_cooperation_rate - base_threshold_ratio)`.
        Let `adjustment_factor_history = 0.5` (tweakable).
        Let `adjustment_factor_last_round_success = 0.2` (tweakable).
        Let `adjustment_factor_last_round_failure = 0.1` (tweakable).

        If round `t-1` was the last round:
            `cooperation_probability_threshold = base_threshold_ratio` (no adjustment based on future rounds).

        Else (for `t < r`):
            `adjusted_threshold = base_threshold_ratio - (cooperation_history_impact * adjustment_factor_history)`

            If `threshold_met_last_round` and `you_cooperated_last_round`:
                `adjusted_threshold -= adjustment_factor_last_round_success`

            If `threshold_met_last_round` and `you_defected_last_round`:
                `adjusted_threshold -= adjustment_factor_last_round_success`  // Reward successful free-riding

            If `not threshold_met_last_round`:
                `adjusted_threshold += adjustment_factor_last_round_failure`

            `cooperation_probability_threshold = max(0.1, min(0.9, adjusted_threshold))` // Clamp to prevent extreme values.

3.  **Make the Decision:**
    *   **If `historical_cooperation_rate >= cooperation_probability_threshold` AND `k` is sufficiently high (e.g., `k >= 2`):**
        *   **Cooperate (C)**. This is the brave, but calculated, move. We are confident enough in others to contribute, and the reward `k` is substantial enough to justify the risk. This is where we aim to maximize collective gain, knowing we are participating in a likely success.
    *   **Else:**
        *   **Defect (D)**. This is our default. We are not confident enough that the threshold will be met, or the reward isn't worth the risk. We prioritize our guaranteed private payoff of 1. We are effectively a free-rider, hoping others will cooperate and push the threshold past `m`.

### 2. Handling Edge Cases

*   **Round 1:**
    *   **Decision:** **Defect (D)**.
    *   **Reasoning:** In the first round, there is no history to inform our decision. We cannot estimate the probability of cooperation. The most aggressive and self-preserving move is to secure our guaranteed payoff of 1. We observe for now.
    *   **`cooperation_probability_threshold`:** We initialize `historical_cooperation_rate` to 0 and the `cooperation_probability_threshold` will be set to its highest initial value (e.g., `m/n`). Since `0 < m/n`, we will defect.

*   **Last Round (`r`):**
    *   **Decision:** **Defect (D)**.
    *   **Reasoning:** In the final round, there are no future rounds to influence. Any cooperation on our part is purely altruistic for that round, with no potential for future reciprocation or leveraging past cooperative behavior. The guaranteed individual payoff of 1 is the maximum we can secure without further risk. The "opportunity" of high reward has no long-term consequence.

*   **Threshold `m` is very high relative to `n` (e.g., `m = n-1`):**
    *   **Strategy Adaptation:** The `cooperation_probability_threshold` will naturally be high. We will defect more often. Our decision to cooperate will be rare, requiring very strong historical evidence of widespread cooperation and a high `k`. This is good – the strategy inherently adjusts to difficult cooperation environments.

*   **Threshold `m` is low relative to `n` (e.g., `m = 2` for any `n > 2`):**
    *   **Strategy Adaptation:** The `cooperation_probability_threshold` will naturally be low. We might cooperate more often, especially if `k` is high. This is also good – the strategy leverages easier cooperation scenarios.

### 3. Aggressive Nature

This strategy is aggressive in several ways:

*   **Default Defection:** It defaults to defection, maximizing individual immediate gain. It is not inherently cooperative.
*   **Opportunistic Cooperation:** It only cooperates when there's a high probability of *success* and a significant reward. This is not about building trust or fostering cooperation; it's about seizing lucrative opportunities when the collective effort is almost guaranteed.
*   **Exploiting Free-Riders:** If the threshold is met and we defected, we essentially exploited the cooperators. The strategy's adaptive mechanism rewards this successful free-riding by potentially lowering the threshold for future rounds, making it even easier to defect.
*   **No "Forgiveness" or Reciprocity:** The strategy does not inherently forgive past defections by others nor does it seek to build long-term cooperative relationships. Its adaptiveness is purely based on statistical probabilities of success derived from historical data, not on interpersonal dynamics.
*   **Boldness in Cooperation:** When it *does* cooperate, it's a bold move. It's stepping up when it believes the stars have aligned for a substantial payout, willing to take the risk because the reward justifies it and the probability of success is high. This isn't timid cooperation; it's calculated, high-stakes participation.

### Pseudocode

```python
class OpportunisticExploiterStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k

        self.cooperation_probability_threshold = m / n # Initial baseline
        self.history = [] # Stores tuples of (my_action, other_actions_list) for each round

    def decide_action(self, current_round):
        # Handle Last Round
        if current_round == self.r:
            return 'D'

        # Handle First Round
        if current_round == 1:
            # Observe in the first round, no history to rely on
            self.last_round_cooperated = False # Placeholder, will be updated after round 1
            self.last_round_threshold_met = False # Placeholder
            return 'D'

        # --- Adaptive Threshold Calculation ---
        # Calculate historical cooperation rate among *other* players
        total_cooperators_in_history = 0
        num_rounds_played = current_round - 1

        for round_history in self.history:
            # round_history[1] is the list of other players' actions in that round
            for action in round_history[1]:
                if action == 'C':
                    total_cooperators_in_history += 1

        historical_cooperation_rate_others = (total_cooperators_in_history / ( (n - 1) * num_rounds_played ) ) if num_rounds_played > 0 else 0


        # Dynamically adjust threshold (example logic, needs tuning)
        base_threshold_ratio = self.m / self.n
        adjustment_factor_history = 0.5
        adjustment_factor_last_round_success = 0.2
        adjustment_factor_last_round_failure = 0.1

        adjusted_threshold = base_threshold_ratio - ((historical_cooperation_rate_others - base_threshold_ratio) * adjustment_factor_history)

        if self.last_round_threshold_met:
            if self.last_round_cooperated:
                adjusted_threshold -= adjustment_factor_last_round_success
            else: # Defected and threshold met (free-rode)
                adjusted_threshold -= adjustment_factor_last_round_success
        else: # Threshold was not met
            adjusted_threshold += adjustment_factor_last_round_failure

        # Clamp the threshold to reasonable bounds
        self.cooperation_probability_threshold = max(0.1, min(0.9, adjusted_threshold))

        # --- Decision Logic ---
        # Aggressive cooperation if confident and reward is good
        if historical_cooperation_rate_others >= self.cooperation_probability_threshold and self.k >= 2.0:
            return 'C'
        else:
            return 'D'

    def record_history(self, my_action, other_players_actions, threshold_met_last_round):
        self.history.append((my_action, other_players_actions))
        self.last_round_cooperated = (my_action == 'C')
        self.last_round_threshold_met = threshold_met_last_round
```
'''

description_AGGRESSIVE_115 = '''
This is a fascinating game that blends elements of the Prisoner's Dilemma with a collective action problem. An aggressive strategy needs to be decisive, opportunistic, and capable of exploiting situations where cooperation is beneficial but not guaranteed.

Here's a design for an aggressive strategy, focusing on maximizing individual gain while being prepared to capitalize on opportunities for collective reward.

## Aggressive Strategy: "Calculated Opportunist"

**Core Philosophy:** My strategy is to be a **calculated opportunist**. I aim to defect as much as possible to secure my private endowment, but I will aggressively cooperate when the conditions are *highly favorable* and the reward significantly outweighs the individual risk. I will learn and adapt based on past rounds, becoming more or less risk-averse depending on the observed behavior of others.

### 1. Decision Rules (Per Round)

The core decision in each round hinges on a prediction of collective action and a threshold for personal risk.

**Key Metrics to Track:**

*   **`observed_cooperators_last_round`**: The number of players (excluding myself) who cooperated in the previous round.
*   **`my_cooperation_history`**: A binary list or count of how many times I've cooperated in previous rounds.
*   **`opponent_cooperation_trend`**: An assessment of whether opponent cooperation is increasing or decreasing. This can be a simple comparison of `observed_cooperators_last_round` vs. the round before, or a more sophisticated moving average.

**Decision Logic:**

*   **Default Action: Defect (D)**. My baseline is to secure my private endowment. Cooperation is a deviation from this default, requiring strong justification.

*   **Cooperate (C) Condition:** I will cooperate *only if* the following conditions are met:

    1.  **High Probability of Success:** The number of *other players* I predict will cooperate, *plus one (for myself)*, is **greater than or equal to `m`**.
        *   **Prediction Mechanism:** This is the critical adaptive part. I will estimate `predicted_cooperators_from_others` based on historical data. A simple but effective approach for an aggressive strategy is to:
            *   If `observed_cooperators_last_round` was sufficiently high (e.g., `>= m-1` if I plan to cooperate), I will assume a high likelihood of them cooperating again.
            *   If `observed_cooperators_last_round` was low, I will be more conservative with my prediction, possibly assuming a number close to `observed_cooperators_last_round`.
            *   A more advanced approach could involve a moving average of opponent cooperation.
        *   **Formalized Condition:** `predicted_cooperators_from_others + 1 >= m`

    2.  **Sufficient Reward for Risk:** The expected payoff of cooperating, *given that the threshold is met*, is significantly higher than the payoff of defecting.
        *   **Expected Payoff of Cooperating (if threshold met):** `(1 - 1) + k = k`
        *   **Expected Payoff of Defecting (if threshold met):** `(1 - 0) + k = 1 + k`
        *   **Decision Heuristic:** This is implicitly handled by the first condition. If I predict `m` or more will cooperate (including myself), then *everyone* gets `k`. My choice is between `0` (as a cooperator) or `1` (as a defector). If the threshold is met, the *additional* payoff for *everyone* is `k`.
        *   **The real decision is about the risk of *not* meeting the threshold.** If I cooperate, and we fall short, my payoff is `0`. If I defect, and we fall short, my payoff is `1`. The risk aversion comes in. I will only cooperate if I am *confident* the threshold will be met.

    3.  **Adaptive Aggression Threshold:** I will adjust my willingness to take a risk based on recent outcomes.
        *   **If recent rounds have been highly successful (many cooperators, threshold met):** I will be slightly more willing to cooperate, assuming that a norm of cooperation might be emerging (or at least that the majority is inclined towards it). I might lower my `predicted_cooperators_from_others` threshold slightly.
        *   **If recent rounds have failed (few cooperators, threshold not met):** I will become more conservative. I will demand a higher `predicted_cooperators_from_others` than `m-1`, or even revert to defaulting to Defect for several rounds to observe.

**Refined Decision Rule:**

In round `t` (where `t > 1`):

1.  **Estimate `predicted_cooperators_from_others` for this round.** This estimate should be influenced by `observed_cooperators_last_round`.
    *   **If `observed_cooperators_last_round >= m - 1` (and I plan to cooperate):** Set `predicted_cooperators_from_others = observed_cooperators_last_round`.
    *   **If `observed_cooperators_last_round < m - 1`:** Set `predicted_cooperators_from_others = observed_cooperators_last_round`. (This means I'm less likely to cooperate unless history shows strong cooperation.)

2.  **Check Cooperation Condition:**
    *   **If `predicted_cooperators_from_others + 1 >= m`:**
        *   **Aggression Adjustment:** If `k` is high and recent history shows a trend towards cooperation, I might cooperate. However, to remain aggressive and minimize personal loss, I will only cooperate if `predicted_cooperators_from_others` is *comfortably* above `m-1`.
        *   **Decision:** **Cooperate (C)**

    *   **Else (if `predicted_cooperators_from_others + 1 < m`):**
        *   **Decision:** **Defect (D)**

**Aggressive Twist for the Decision Rule:**

Instead of `predicted_cooperators_from_others >= m-1`, I'll use a slightly more aggressive condition. I will cooperate if I believe *at least* `m` players *will* cooperate, meaning I need to predict `m-1` *other* players will cooperate.

*   **Cooperate (C) if:** `observed_cooperators_last_round >= m-1` AND I haven't cooperated too many times in the past (to avoid being exploited).

This is still very conservative for an "aggressive" strategy. Let's make it more so:

**Revised Aggressive Decision Rule:**

In round `t` (where `t > 1`):

1.  **Assess Opponent Inclination:**
    *   **If `observed_cooperators_last_round >= m - 1`:** The opponents *demonstrated* enough cooperation last round to potentially meet the threshold if I also cooperated. This is an opportunity.
    *   **Else (if `observed_cooperators_last_round < m - 1`):** The opponents were insufficient. I am highly unlikely to cooperate unless there's a very strong, recent trend or `k` is astronomically high.

2.  **Decision Trigger:**
    *   **If `observed_cooperators_last_round >= m - 1`:** **Cooperate (C)**. I am seizing the opportunity. I am betting that the previous round's success (or near-success) by others indicates a propensity to cooperate again.
    *   **Else:** **Defect (D)**. I will not initiate cooperation unless there's strong evidence others are already doing so.

This is aggressive because it's willing to take the *risk* of being the `m`-th cooperator based on limited, but recent, evidence. It assumes others are also rational and will defect if they don't see enough cooperation.

### 2. Handling Edge Cases

*   **Round 1:**
    *   **Default Action:** **Defect (D)**. There is no history to base a prediction on. Cooperating is a pure gamble with no information. My goal is to secure my endowment. I will observe and learn.

*   **Last Round (`r`):**
    *   **Strategy:** The same as the general decision rule. The "repeated" nature of the game is gone, but the payoff structure remains. My aggressive strategy still applies: If the previous round indicated enough potential cooperation for me to tip the balance, I'll do it. The difference is there's no future round to "punish" or "reward" this action. However, my strategy doesn't explicitly build in backward induction for the last round, as it's focused on immediate opportunity. So, **apply the standard decision rule.**

*   **`m` vs. `n` and `k` values:**
    *   **Very high `m` (close to `n`):** Cooperation becomes very difficult. The strategy will naturally default to Defect more often, as `observed_cooperators_last_round >= m-1` will be a rare event. This is appropriate.
    *   **Very low `m` (close to 1, but `m>1`):** Cooperation is easier. The condition `observed_cooperators_last_round >= m-1` will be met more frequently, leading to more cooperation. This is also appropriate.
    *   **High `k`:** This parameter makes cooperation more attractive. My strategy already *implicitly* benefits from high `k` because the payoff for meeting the threshold is much higher. If `k` is extremely high, the strategy might be adjusted to have a slightly lower threshold for cooperation, but the current rule `observed_cooperators_last_round >= m-1` is a good starting point as it prioritizes certainty of reward.

### 3. Being Aggressive

*   **Proactive Risk-Taking:** My strategy is aggressive because it's willing to be the *initiator* of cooperation when the conditions are *just right* but not guaranteed. If `observed_cooperators_last_round` is `m-1`, and I cooperate, I am pushing the total to `m`. This is a direct gamble for the reward `k`.
*   **Exploiting Opportunity:** It seizes the opportunity presented by past successful (or nearly successful) cooperation. It doesn't wait for overwhelming evidence; it acts on strong signals.
*   **Minimal Personal Loss, Maximum Potential Gain:** If I cooperate and the threshold is met, my payoff is `k`. If I cooperate and it's not met, my payoff is `0`. This is the risk. However, if I defect and it *is* met, I still get `1+k`. My strategy prioritizes securing the `k` by being the catalyst when I believe it's likely.
*   **No Free Riding (Implicitly):** While I don't explicitly "punish" defectors (as there's no communication), my strategy is not to exploit others by *always* defecting. It's to contribute when the collective benefit is likely and significant. An aggressive stance means contributing *just enough* to tip the scales if it seems others are already there.

### Pseudocode

```python
class CalculatedOpportunistStrategy:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.my_history = [] # Stores 'C' or 'D' for each round
        self.opponent_actions_history = [] # Stores list of actions for each round, excluding my own

    def choose_action(self, current_round, opponent_actions_prev_round):
        """
        current_round: The current round number (1-indexed)
        opponent_actions_prev_round: A list of actions ('C' or 'D') of all other players in the previous round.
        """

        # Initialize history for current round if needed
        if len(self.my_history) < current_round - 1:
            self.my_history.append(None) # Placeholder
            self.opponent_actions_history.append([])

        # --- Round 1: Default to Defect ---
        if current_round == 1:
            self.my_history[current_round-1] = 'D'
            return 'D'

        # --- Subsequent Rounds ---
        # Extract relevant information from the previous round
        observed_cooperators_last_round = sum(1 for action in opponent_actions_prev_round if action == 'C')

        # Aggressive Decision Logic: Cooperate IF enough others cooperated LAST round
        # to potentially reach the threshold if I ALSO cooperate.
        if observed_cooperators_last_round >= self.m - 1:
            # It's worth risking cooperation to try and secure the reward 'k'
            chosen_action = 'C'
        else:
            # Not enough demonstrated cooperation from others, default to defect.
            chosen_action = 'D'

        # Record my action for this round
        self.my_history[current_round-1] = chosen_action
        # Record opponent actions for the next round's learning (optional, if needed for more complex prediction)
        # self.opponent_actions_history[current_round-1] = opponent_actions_prev_round

        return chosen_action

```
'''

description_AGGRESSIVE_116 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the potential for collective reward, with a focus on adaptability and robustness.

## Aggressive Strategy: The "Opportunistic Leverage"

This strategy aims to exploit situations where cooperation is likely to succeed and to punish defection that jeopardizes the collective good, all while maintaining a degree of self-preservation. The core idea is to be a "cooperator of last resort" – willing to bear the risk for the reward, but only when the odds are demonstrably in favor of reaching the cooperation threshold.

### 1. Decision Rules:

The decision for player `i` in round `t` will depend on the observed behavior in previous rounds and the current game parameters.

**In essence: Cooperate if there's a strong indication that your cooperation will push the group over the threshold, and defect otherwise, especially if others are defecting.**

Let's define some key metrics and thresholds:

*   **`cooperation_rate_t-1`**: The proportion of players who cooperated in the previous round `t-1`.
*   **`my_contribution_in_t-1`**: 1 if player `i` cooperated in round `t-1`, 0 if defected.
*   **`players_cooperating_this_round`**: The number of players who cooperated in the current round `t`. This is only known *after* the round is played.
*   **`players_defecting_this_round`**: The number of players who defected in the current round `t`.
*   **`my_defect_history`**: A measure of how often player `i` has defected in the past.
*   **`other_defect_history`**: A measure of how often other players have defected in the past.

**Decision Logic (for player `i` in round `t`):**

**IF (Round `t` is the first round OR the previous round had a low cooperation rate):**

*   **DEFECT (Default aggressive posture).**
    *   *Rationale:* In the absence of information or when past cooperation failed, it's safer to secure your private endowment. This also serves as a signal that you are not easily exploited.

**ELSE IF (The number of players who cooperated in round `t-1` plus the number of players who are *likely* to cooperate in round `t` is greater than or equal to `m`):**

*   **COOPERATE.**
    *   *How to estimate `likely_to_cooperate`?* This is the adaptive part. We'll use a weighted prediction based on past behavior.
        *   **Prediction Model:**
            *   Count the number of players who *always* cooperated in the past (`always_cooperators`).
            *   Count the number of players who *mostly* cooperated (`mostly_cooperators`).
            *   Count the number of players who *mostly* defected (`mostly_defectors`).
            *   Count the number of players who *always* defected (`always_defectors`).
            *   If `t` is the last round, this prediction needs to be more cautious.

        *   **Decision to Cooperate (refined):**
            *   Let `predicted_cooperators` = `always_cooperators` + `mostly_cooperators`
            *   If `cooperation_rate_t-1` (or a more sophisticated average over the last few rounds) is high (e.g., > 0.7), and `predicted_cooperators` is still >= `m - 1` (leaving room for your own cooperation), **COOPERATE**.
            *   **Crucially:** If `players_cooperating_this_round` (from previous rounds) + `my_contribution_in_t-1` + (estimated number of *new* cooperators based on observed shifts) >= `m`, **COOPERATE**.

**ELSE IF (You are *almost* certain that your cooperation will push the group over the threshold):**

*   **COOPERATE.**
    *   *Condition:* This happens if `(number of players who cooperated in t-1)` is `m-1`, AND `(all other players are assumed to defect)`. In this specific scenario, your cooperation makes the difference. This is a high-risk, high-reward play.
    *   *Rationale:* Maximize your payoff when you are the linchpin for success.

**ELSE (Default aggressive fallback):**

*   **DEFECT.**
    *   *Rationale:* If the conditions for cooperation aren't met, or if there's significant uncertainty, it's more aggressive and safer to defect. This also punishes others who haven't contributed enough to guarantee success.

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   **DEFECT.**
        *   *Rationale:* No historical data. The default is to secure your private endowment and observe. This also acts as a strong initial signal of an aggressive, self-interested stance. You need to see what others do before committing.

*   **Last Round (t=r):**
    *   **DEFECT.**
        *   *Rationale:* In the final round, there's no future to incentivize cooperation. The best strategy is to secure your private endowment without any regard for the collective. This is the ultimate aggressive move in the absence of future reciprocity.

*   **Near the Threshold (m-1 cooperators observed from others):**
    *   **COOPERATE.**
        *   *Rationale:* This is your prime opportunity to exert leverage and secure the high payoff. You are the deciding factor. This is a core aggressive maneuver.

*   **Majority Defection in Previous Round(s):**
    *   **DEFECT.**
        *   *Rationale:* If the group has a history of poor cooperation, it's unlikely to suddenly change. Defecting is the pragmatic and aggressive choice.

### 3. Aggressive Mindset Alignment:

*   **Default to Defection:** The strategy's baseline is defection. Cooperation is a calculated risk taken only when the reward is highly probable and significant.
*   **Leverage Uncertainty:** The strategy aims to be the "tipping point" for cooperation, demanding the highest reward for the highest individual risk.
*   **Punish Failure:** While not explicitly programmed as a pure "punisher" in the Tit-for-Tat sense, the strategy implicitly punishes by defecting when cooperation rates are low, thus denying the reward to those who didn't contribute to its achievement.
*   **Maximize Individual Gain:** The ultimate goal is to maximize the player's own payoff. This is achieved by seeking the `k` multiplier whenever possible, but not at the expense of their baseline endowment if the risk is too high.

### Pseudocode (Conceptual):

```python
class AggressiveLeverageStrategy:
    def __init__(self, n, m, r, k):
        self.n = n
        self.m = m
        self.r = r
        self.k = k
        self.history = [] # List of dictionaries, each representing a round's outcome: {'actions': {player_id: action}, 'payoffs': {player_id: payoff}}

    def decide_action(self, current_round, my_player_id):
        # --- Edge Case: First Round ---
        if current_round == 1:
            return "D"

        # --- Edge Case: Last Round ---
        if current_round == self.r:
            return "D"

        # --- Analyze History ---
        if not self.history: # Should not happen if current_round > 1, but for safety
            return "D"

        last_round_data = self.history[-1]
        last_round_actions = last_round_data['actions']
        num_cooperators_last_round = sum(1 for action in last_round_actions.values() if action == "C")
        num_my_action_last_round = 1 if my_player_id in last_round_actions and last_round_actions[my_player_id] == "C" else 0

        # --- Estimate Potential Cooperators for Current Round ---
        # This is a simplified prediction model. A more complex model could track individual player histories.
        # For this aggressive strategy, we'll be conservative in predicting others.

        # Number of players who consistently cooperated (e.g., in > 50% of rounds)
        consistent_cooperators = 0
        for player_id in range(self.n):
            if player_id == my_player_id: continue # Exclude self for prediction
            coop_count = sum(1 for round_data in self.history if player_id in round_data['actions'] and round_data['actions'][player_id] == "C")
            if coop_count > len(self.history) * 0.5:
                consistent_cooperators += 1

        # Simple projection: assume consistent cooperators will cooperate, and others will defect unless they are near the threshold
        predicted_others_cooperating = consistent_cooperators
        # If only one more is needed and there are potentially reliable cooperators
        if num_cooperators_last_round + my_player_id_cooperation_indicator < self.m - 1 and predicted_others_cooperating < self.m - 1 :
            # If there are exactly m-1 cooperators from others, this is a strong signal for YOU to flip the game.
            if predicted_others_cooperating == self.m - 1:
                 return "C" # You are the linchpin.

        # --- Decision Logic ---

        # Condition 1: High chance of success based on last round's observed cooperation + likely future cooperators
        # We are conservative in predicting others, so this is a strong signal.
        if num_cooperators_last_round >= self.m - 1: # If last round was already enough or almost enough
            # If my cooperation was needed last round, and it succeeded, I might do it again if the odds are still good.
            if num_my_action_last_round == 1 and num_cooperators_last_round >= self.m:
                # If last round succeeded, and prediction still looks good, I'll cooperate again.
                # This is aggressive because it assumes others will continue if they see success.
                if predicted_others_cooperating >= self.m - 1: # If others can still reach threshold without me
                    return "C"
            # If last round DIDN'T succeed, but it was close, and predicted cooperators + my action can reach m
            elif num_cooperators_last_round == self.m - 1 and predicted_others_cooperating >= self.m -1:
                 return "C" # My cooperation will push it over.

        # Condition 2: Aggressive play - defect if not clearly advantageous to cooperate
        # This covers cases where cooperation is unlikely to reach 'm'
        # Or if the reward 'k' isn't worth the risk given past behavior.

        # Default aggressive stance
        return "D"

    def record_round_outcome(self, actions, payoffs):
        self.history.append({'actions': actions, 'payoffs': payoffs})

```

**Key Aggressive Aspects of "Opportunistic Leverage":**

*   **Default to Defect:** Minimizes risk and preserves endowment when cooperation is uncertain.
*   **"Linchpin" Cooperation:** Actively seeks to be the one player who tips the scales when cooperation is at `m-1` from others, ensuring maximum individual return.
*   **No Forgiveness:** Does not engage in tit-for-tat style forgiveness if past cooperation failed or if others defected. It's purely a calculation of current round benefit vs. risk.
*   **Self-Reliance in Prediction:** Relies minimally on predicting cooperation from others, making its own cooperative decision a strong signal and often the deciding factor.
*   **Terminal Aggression:** Always defects in the final round, prioritizing maximum immediate gain over any future reciprocity.

This strategy is designed to be a strong contender in a tournament setting where absolute predictability and explicit communication are absent, and where maximizing individual return is paramount. It balances the potential for high rewards with a pragmatic assessment of risk, always leaning towards the more aggressive, self-preserving option when uncertainty is high.
'''

description_AGGRESSIVE_117 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to be adaptive and robust, with a focus on maximizing individual payoff by pushing the boundaries of cooperation.

## Aggressive Strategy: "The Calculated Risk-Taker"

This strategy is driven by a core belief: **The highest payoffs are achieved when the cooperation threshold (m) is met, but only just met, and I am one of the cooperators. However, I am not a naive cooperator; I will only contribute if I am confident that my contribution will lead to a net positive outcome for myself, taking into account the potential for others to defect.**

The strategy aims to be aggressive by:
*   **Leveraging the high reward (k):** It actively seeks to be in the group that triggers the reward.
*   **Minimizing the cost of defection:** It defects when cooperation is unlikely to be beneficial or when it can benefit from others' cooperation.
*   **Punishing non-cooperators decisively (implicitly):** While not explicitly punishing, by defecting when cooperation is not guaranteed, it makes it harder for the group to reach the threshold, thus reducing their reward.

### 1. Decision Rules:

The core of this strategy is a dynamic assessment of the "cooperation likelihood" and the "individual gain from cooperating."

**Decision Rule for Round 't' (t > 1):**

*   **If the previous round (t-1) resulted in a payoff of '1' (meaning the cooperation threshold was NOT met):**
    *   **Evaluate the "Cooperation Deficit" (CD):** This is the number of additional cooperators needed in the *last* round to reach 'm'. `CD = max(0, m - (number of cooperators in round t-1))`.
    *   **If `CD <= 1` (meaning only one or zero more cooperators were needed):** This indicates that cooperation was *almost* achieved.
        *   **If my action in round t-1 was 'D'**: **Cooperate (C)**. My defection might have been the single factor preventing the reward. The chance of a successful collective effort is now high, and my individual payoff from cooperating (0 + k) is better than my previous payoff (1 + 0).
        *   **If my action in round t-1 was 'C'**: **Cooperate (C)**. The group was on the brink. I'll continue to contribute, hoping others will follow.
    *   **If `CD > 1` (meaning a significant number of cooperators were missing):** The group is too unreliable. **Defect (D)**. I will not waste my endowment on a project that is unlikely to succeed, and I'll still benefit from the private endowment if others *do* manage to cooperate.

*   **If the previous round (t-1) resulted in a payoff of 'k' or 'k+1' (meaning the cooperation threshold WAS met):**
    *   **Evaluate the "Potential for Exploitation":** This is the number of players who defected in round t-1. `Exploiters = (number of D players in round t-1)`.
    *   **If `Exploiters >= 1`:** This indicates there are players willing to exploit.
        *   **If my action in round t-1 was 'C'**: **Cooperate (C)**. I was part of the successful group. I will continue to cooperate, assuming the collective reward is still achievable.
        *   **If my action in round t-1 was 'D'**: **Cooperate (C)**. Others cooperated to get the reward, and I benefited. This is a sign of a stable cooperative group. I'll join in to secure my reward.
    *   **If `Exploiters == 0`:** This means *everyone* cooperated. This is a rare and potentially unstable state.
        *   **If my action in round t-1 was 'C'**: **Cooperate (C)**. Continue the established cooperation.
        *   **If my action in round t-1 was 'D'**: This scenario should not happen if I was a cooperator. This implies an error or a very unusual history. In this case, **Cooperate (C)**, to maintain the successful pattern.

*   **Default (if previous round history is not informative or leads to ambiguity):**
    *   If it's unclear whether cooperation will be successful, err on the side of caution. **Defect (D)**.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   **Aggressive Initialization:** **Cooperate (C)**. The first round is crucial for establishing a norm or signaling intent. By cooperating, I signal a willingness to contribute and attempt to trigger the reward. This is the most aggressive stance to start with, as it directly aims for the higher payoff. However, it's a calculated risk.

*   **Last Round (t=r):**
    *   **Maximize Individual Gain:** In the last round, there are no future consequences to consider regarding reputation or future cooperation.
    *   **Analyze actions of (r-1):**
        *   **If the outcome of round (r-1) was "threshold met" (payoff >= k):** This implies cooperation is generally beneficial. **Cooperate (C)**. I will get `(1-1) + k = k`. If I defect, I get `(1-0) + k = k+1`. So, if cooperation was successful, defecting in the last round to get the higher individual payoff `k+1` is optimal. **Defect (D)**.
        *   **If the outcome of round (r-1) was "threshold NOT met" (payoff = 1):** Cooperation is not reliable. **Defect (D)**. I get `1`. Cooperating would likely yield `1` as well, but with more risk.

### 3. Aggressive Nature:

*   **Proactive Cooperation in "Near Misses":** The strategy aggressively seeks to be part of the successful cooperative group. When the threshold was *almost* met in the previous round, and I previously defected, I immediately switch to cooperation. This is aggressive because I'm taking a calculated risk to push the group over the edge, directly aiming for the reward.
*   **Exploiting Successful Cooperation (in the last round):** The strategy doesn't blindly follow a cooperative path. In the final round, if cooperation has proven beneficial, it switches to defection to maximize its individual gain from the established reward structure. This is an aggressive exploitation of the game's reward mechanism.
*   **Low Tolerance for Repeated Failures:** If the cooperation threshold is consistently missed by a significant margin, the strategy aggressively retreats to defection, refusing to waste resources on a failing endeavor.
*   **Initial Boldness:** Starting with cooperation in the first round is a bold move, designed to influence the early dynamics of the game.

### Pseudocode:

```
function decide_action(current_round, history, parameters):
    n = parameters.n
    r = parameters.r
    m = parameters.m
    k = parameters.k

    if current_round == 1:
        return 'C'  # Aggressive initialization

    if current_round == r:
        # Last round: maximize individual gain
        prev_round_data = history[current_round - 1]
        prev_cooperators = prev_round_data.cooperators
        prev_player_actions = prev_round_data.player_actions[my_index] # Assuming access to own actions

        if prev_cooperators >= m:
            # Threshold was met in the previous round
            return 'D'  # Exploit the reward structure
        else:
            # Threshold was NOT met
            return 'D'  # No reward likely, defect

    # For rounds between 1 and r (exclusive of r)
    prev_round_data = history[current_round - 1]
    prev_cooperators = prev_round_data.cooperators
    prev_my_action = prev_round_data.player_actions[my_index]

    if prev_cooperators < m:
        # Threshold was NOT met in the previous round
        cooperation_deficit = m - prev_cooperators

        if cooperation_deficit <= 1:
            # Almost met the threshold
            if prev_my_action == 'D':
                return 'C'  # Switch to cooperate to push over the edge
            else: # prev_my_action == 'C'
                return 'C'  # Continue cooperating as group was close
        else:
            # Significant deficit, group is unreliable
            return 'D'
    else:
        # Threshold WAS met in the previous round
        exploiters_in_prev_round = 0
        for i in range(n):
            if prev_round_data.player_actions[i] == 'D':
                exploiters_in_prev_round += 1

        if exploiters_in_prev_round >= 1:
            # Some players exploited, but threshold still met.
            # Continue cooperating to maintain reward.
            return 'C'
        else:
            # Everyone cooperated, a stable state.
            return 'C' # Continue cooperation

```
'''

description_AGGRESSIVE_118 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual payoff by leveraging cooperation when it benefits you, but primarily by exploiting others' cooperation.

## Strategy: The Calculated Risk-Taker

This strategy is built on the principle of **opportunistic exploitation** and **controlled risk-taking**. It aims to achieve high payoffs by defecting when it's individually most beneficial and only cooperating when the collective reward is guaranteed or highly probable, and when your cooperation is crucial to achieving that reward.

### 1. Decision Rules

My decision to Cooperate (C) or Defect (D) in any given round `t` (where `1 < t < r`) depends on the following factors:

*   **The Current State of Cooperation:** Specifically, the number of players who cooperated in the *previous* round (`t-1`). Let `N_C(t-1)` be this number.
*   **The Proximity to the Threshold `m`:** How close are we to reaching the `m` cooperators needed for the collective reward?
*   **My Own Previous Action:** Did I cooperate or defect in the previous round?

Here are the specific rules:

**Rule 1: The "Exploiter's Default" (Most of the Time)**

*   **If `N_C(t-1) < m - 1`:** I will **Defect (D)**.
    *   *Rationale:* In this scenario, even if I were to cooperate, it's highly unlikely that the `m` threshold will be met. My best individual payoff is by defecting and keeping my endowment. I'm not contributing to a failing collective.

**Rule 2: The "Guaranteed Reward" (When Cooperation is Safe and Profitable)**

*   **If `N_C(t-1) >= m - 1` AND `N_C(t-1) < m` AND I played D in round `t-1`:** I will **Cooperate (C)**.
    *   *Rationale:* This is the sweet spot. The threshold `m` is just one cooperation away. Since I defected last round, my individual payoff is higher if the threshold is met. By cooperating now, I can secure the `k` reward for myself without jeopardizing my higher individual payoff from defecting in the previous round. I'm essentially "buying" into the reward at the last possible moment.

*   **If `N_C(t-1) >= m`:** I will **Cooperate (C)**.
    *   *Rationale:* The reward is already guaranteed. Cooperating in this situation ensures I receive the reward (`k`) in addition to my endowment. The cost of cooperating (reducing my individual endowment by 1) is offset by the guaranteed `k` reward. It's a sure gain.

**Rule 3: The "Risk Averse Exploiter" (When Cooperation is Risky but Potentially Rewarding)**

*   **If `N_C(t-1) < m - 1` (and Rule 1 didn't apply due to a special case below, or we are in a situation where the number of cooperators is *slightly* higher but still below `m`):** I will **Defect (D)**.
    *   *Rationale:* The risk of the collective failing is still too high. My default is to protect my own endowment.

**Rule 4: The "Calculated Gamble" (When My Cooperation is the Deciding Factor)**

*   **If `N_C(t-1) == m - 1` AND I played C in round `t-1`:** I will **Cooperate (C)**.
    *   *Rationale:* If I cooperated last round and the threshold wasn't met, and I was the *only* one who could have pushed it over, then the collective is failing despite my effort. However, if `m-1` players cooperated *excluding* myself, and I *also* cooperated, and we *still* didn't reach `m`, then the situation is dire for cooperation. But if `m-1` cooperated and I *also* cooperated, and the count is now `m-1` (meaning someone *else* defected, who *could* have cooperated), then cooperating again ensures I get the reward. This is a nuanced case but emphasizes I stick with cooperation if it *might* pay off.

**Summary of core logic:**
My default is to Defect. I only Cooperate when the reward is either:
1.  **Guaranteed:** `N_C(t-1) >= m`.
2.  **Within direct reach and I can profit from it:** `N_C(t-1) == m - 1` and I defected last round (so my personal payoff from defecting *last* round was 1+0, and now cooperating guarantees 0+k).

My aggression comes from the fact that I almost always default to defecting, *unless* cooperating is a sure thing or the absolute last-minute opportunity to gain the reward.

### 2. Handling Edge Cases

**Round 1 (t=1): The Aggressive Opening**

*   **Decision:** I will **Defect (D)**.
    *   *Rationale:* In the first round, there is no history. The risk of collective failure is highest. I have no information about other players' tendencies. My aggressive approach dictates that I secure my baseline payoff of 1 and observe the landscape. I need to see how others behave before I consider any cooperation.

**Last Round (t=r): The "No Tomorrow" Calculation**

*   **Decision:** I will **Defect (D)**.
    *   *Rationale:* In the final round, there are no future rounds to influence or learn from. The repeated game aspect is gone. My only objective is to maximize my payoff in this single, final round. The best way to do this is to keep my endowment, as there's no future to consider the potential benefits of cooperation. This is the ultimate expression of my self-interested, aggressive nature.

**The "Almost There" Trap (When `N_C(t-1)` is just below `m` but not `m-1`)**

*   If `N_C(t-1)` is, for example, `m-2`, and I was the only one who defected, and I consider cooperating.
    *   **Decision:** I will **Defect (D)**.
        *   *Rationale:* Even with my cooperation, the threshold is still not met. The risk is too high, and the reward is not guaranteed. My primary objective is to protect my endowment.

### 3. Being Aggressive

This strategy is aggressive for several reasons:

*   **Default to Defection:** My primary mode of operation is to defect. This maximizes my individual payoff in the absence of guaranteed collective success. I force others to carry the risk of cooperation.
*   **Exploitative Cooperation:** I only cooperate when the reward is *assured* (when `N_C(t-1) >= m`) or when I can jump in at the very last moment and secure the reward for myself without having borne the initial risk (when `N_C(t-1) == m - 1` and I defected previously). This is about profiting from others' efforts.
*   **No Early Risk-Taking:** I refuse to be the "first mover" in cooperation. I wait for the collective to demonstrate its willingness and ability to reach the threshold before I contribute.
*   **Ruthless Final Round:** My commitment to self-interest extends to the very last round, where I abandon any pretense of contributing to a collective.

### Pseudocode

```python
def decide_action(round_number, n, m, k, history):
    # history is a list of dictionaries, each representing a round's outcome
    # history[-1] contains the outcome of the previous round (if it exists)
    # history[-1]['cooperators'] = number of players who cooperated in previous round
    # history[-1]['my_action'] = 'C' or 'D' from previous round

    # Edge Case: First Round
    if round_number == 1:
        return 'D'

    # Edge Case: Last Round
    if round_number == r:
        return 'D'

    # General Rounds (1 < t < r)
    previous_round_cooperators = history[-1]['cooperators']
    my_previous_action = history[-1]['my_action']

    # Rule 2: Guaranteed Reward
    if previous_round_cooperators >= m:
        return 'C'

    # Rule 2: Guaranteed Reward (Last moment to jump in)
    if previous_round_cooperators == m - 1 and my_previous_action == 'D':
        return 'C'

    # Rule 1 & 3: Default to Defect (Exploiter's Default)
    # This covers cases where previous_round_cooperators < m - 1
    # and also cases where previous_round_cooperators == m - 1 but I cooperated last round.
    # In the latter case, if m-1 cooperators + my cooperation (total m) didn't meet the threshold,
    # it means the count is still m-1. This is covered by the specific rule above.
    # If m-1 cooperators (excluding myself) + my cooperation still doesn't reach m,
    # it's a failure to reach m.
    return 'D'

```

This "Calculated Risk-Taker" strategy prioritizes individual gain, using cooperation only when it's a near-certain path to profit or a guaranteed success. It's designed to be a formidable opponent, leveraging the predictability of rational self-interest in others while maintaining a staunchly self-serving stance.
'''

description_AGGRESSIVE_119 = '''
This is a fascinating and challenging game! Given the aggressive mindset required, my strategy will focus on maximizing my own potential payoff while leveraging the collective reward when possible, but without being a victim of others' defection. I'll call this strategy "Calculated Aggression."

The core idea is to be a conditional cooperator, but with a strong bias towards defection unless there's a clear and immediate benefit to cooperating. I want to be the player who benefits from others' cooperation without necessarily being the one to initiate it and bear the initial risk.

## Calculated Aggression Strategy

**1. Decision Rules: When to Cooperate vs. Defect**

My primary decision in any round hinges on a calculation of the **"Cooperation Momentum"** and a personal assessment of **"Risk Tolerance."**

*   **Cooperation Momentum:** This measures the recent trend of cooperation. It's a weighted average of the number of cooperators in the last few rounds. The weight decays over time, giving more importance to recent actions.
*   **Risk Tolerance:** This is a personal threshold that dictates how much "risk" I'm willing to take by cooperating. This risk is defined as the difference between the guaranteed payoff of defecting (1) and the potential payoff of cooperating (which could be 0 if the threshold isn't met, or 2 if it is).

Here are the specific rules:

*   **If Cooperation Momentum is High AND my individual Risk Tolerance is met:** **COOPERATE (C)**
    *   "High Cooperation Momentum" means that in the recent past (e.g., last 3 rounds), the average number of cooperators has been *above* the minimum threshold `m`.
    *   "My individual Risk Tolerance is met" means that if I cooperate *now*, and the current number of cooperators (including myself) *is likely to reach `m`*, the expected payoff of cooperating is greater than the guaranteed payoff of defecting. This is a dynamic assessment.

*   **If Cooperation Momentum is Low OR my individual Risk Tolerance is NOT met:** **DEFECT (D)**
    *   This is the default. I will defect if there's no clear sign of collective effort, or if I believe cooperating would be too risky for my individual payoff.

**How to Quantify "Cooperation Momentum" and "Risk Tolerance":**

*   **Cooperation Momentum (CM):** Let `C_t` be the number of cooperators in round `t`. We'll use an exponentially decaying average:
    `CM_t = (1 - alpha) * CM_{t-1} + alpha * C_t`
    where `alpha` is a decay factor (e.g., 0.3). `CM_0 = 0`. I will cooperate if `CM_t > m`.

*   **Individual Risk Tolerance (IRT):** This is a calculated value for the *current round*. I'll only consider cooperating if the potential upside outweighs the downside.
    *   Let `N_C_prev` be the number of cooperators in the previous round.
    *   If I were to cooperate, the number of potential cooperators would be `N_C_prev + 1`.
    *   The *probability* of the threshold `m` being met *if I cooperate* is complex to calculate perfectly due to other players' unknown strategies. However, I can use a heuristic:
        *   **If `N_C_prev >= m - 1`:** This means if I cooperate, we are very likely to meet the threshold (or already are). My risk is low. I'll lean towards cooperating if CM is also high.
        *   **If `N_C_prev < m - 1`:** Cooperating is risky. I will only do so if `CM_t` is *exceptionally* high, suggesting a strong shift in behavior from others.

**Aggressive Tweak:** My threshold for *initiating* cooperation will be higher than simply needing `m-1` other cooperators. I want to join a *proven* success, not be a gamble.

**Refined Decision Rule:**

1.  **Calculate `CM_t`** based on the history of cooperators.
2.  **Count `N_C_prev`** (cooperators in the immediately preceding round).
3.  **If `N_C_prev >= m` AND `CM_t > m`:** **COOPERATE (C)**. This is the "safe reward" scenario. Others have already ensured the reward, and I join to get it.
4.  **If `N_C_prev == m - 1` AND `CM_t > m * 1.2` (a higher momentum threshold):** **COOPERATE (C)**. This is where I take a calculated risk, but only if momentum is *very* strong, indicating a likely shift towards cooperation.
5.  **In all other cases:** **DEFECT (D)**. This includes:
    *   If `N_C_prev < m - 1`.
    *   If `N_C_prev >= m` but `CM_t` is not high enough (indicating a recent dip).
    *   If `N_C_prev == m - 1` but `CM_t` isn't *exceptionally* high.

**2. Handling Edge Cases**

*   **First Round (t=1):**
    *   There is no history. My "Cooperation Momentum" is effectively 0.
    *   My "Risk Tolerance" is at its maximum.
    *   **Decision: DEFECT (D)**. I will never cooperate in the first round. I need to observe and learn. This is the safest and most aggressive starting position – I get my guaranteed 1, and observe how others behave.

*   **Last Round (t=r):**
    *   The game is ending, and there's no future to build cooperation for.
    *   The primary driver is maximizing my own payoff in this final round.
    *   **Decision: DEFECT (D)**. Regardless of history, in the final round, there's no incentive to risk cooperation for a future reward. I take my guaranteed private payoff.

*   **Rounds Where `m` is Close to `n` (e.g., `m = n-1` or `m = n`):**
    *   My strategy naturally adapts. If `m` is very high, it becomes much harder to reach the threshold. My `CM_t` will likely never be sufficiently high for me to consider cooperating unless almost everyone else is already cooperating. This naturally leads to more defections in these scenarios.

*   **Rounds Where `m` is Very Small (e.g., `m = 2` with `n` large):**
    *   The threshold is easily met. My `CM_t` will likely be high more often. My willingness to cooperate will increase, especially if `N_C_prev >= m - 1`. This should allow me to capture the collective reward more frequently.

**3. Aggressive Mindset Alignment**

*   **"I follow, I don't lead":** My strategy is designed to benefit from others' cooperation. I am not the altruist who takes the first step and risks getting nothing. I wait for signals that cooperation is likely to succeed and then join. This is aggressive because I am leveraging the system's potential for reward without bearing the initial cost or risk.
*   **"Maximizing my gain, minimizing my loss":** My default is to defect, securing my baseline payoff of 1. I only deviate from this when the expected gain from cooperating (securing `1 + k` instead of `1`) significantly outweighs the risk of getting `0` (if I cooperate and the threshold is not met).
*   **"Exploiting predictable patterns":** The `CM_t` is a mechanism to detect if the group is trending towards cooperation. If opponents are reliably cooperating to meet `m`, my strategy will pick up on this momentum and join, thus maximizing my payoff. If they are reliably defecting, I will also reliably defect.
*   **"No sentimentality, only calculation":** The strategy is purely data-driven. It doesn't "trust" or "punish" in an emotional sense, but rather reacts to observed data (past cooperation levels) to make the most profitable decision for myself.

## Pseudocode Representation

```
// Game Parameters
n = number of players
r = number of rounds
m = minimum cooperators needed
k = reward factor

// Strategy Parameters
alpha = cooperation_momentum_decay_factor (e.g., 0.3)
initial_momentum = 0.0
cooperator_count_history = [] // List of number of cooperators in each past round
momentum = initial_momentum

function decide_action(current_round_t):
    // Handle edge cases
    if current_round_t == 1:
        return 'D' // Always defect in the first round
    if current_round_t == r:
        return 'D' // Always defect in the last round

    // Calculate Cooperation Momentum
    if not cooperator_count_history.is_empty():
        last_round_cooperators = cooperator_count_history[-1]
        momentum = (1 - alpha) * momentum + alpha * last_round_cooperators
    else:
        momentum = alpha * 0 // Effectively 0 for the first round of calculation

    // Get number of cooperators from previous round
    N_C_prev = 0
    if not cooperator_count_history.is_empty():
        N_C_prev = cooperator_count_history[-1]

    // Decision Logic
    // Rule 3: Safe Reward Scenario
    if N_C_prev >= m and momentum > m:
        return 'C'

    // Rule 4: Calculated Risk with High Momentum
    if N_C_prev == m - 1 and momentum > m * 1.2: // Aggressive threshold
        return 'C'

    // Rule 5: Default to Defect
    return 'D'

// Main loop (hypothetical for strategy implementation)
// For each round t from 1 to r:
//     action_to_take = decide_action(t)
//     record_my_action(action_to_take)
//     // Observe other players' actions for round t
//     // Calculate actual number of cooperators for round t
//     // Add actual number of cooperators to cooperator_count_history
//     // Update total payoff
```

This "Calculated Aggression" strategy aims to be a sharp, opportunistic player. It's designed to exploit the game's reward structure by waiting for clear signals of collective success before contributing, thereby maximizing personal gain while remaining robust to the inherent uncertainty of the game.
'''

description_AGGRESSIVE_120 = '''
This is a fascinating setup! The Collective Risk Dilemma is a classic game theory problem that highlights the tension between individual self-interest and collective benefit, especially in repeated interactions where reputation and learning can emerge.

Here's an aggressive strategy designed for this game, aiming for maximal individual gain while leveraging the potential for collective reward when conditions are favorable.

## Aggressive Strategy: "The Opportunistic Catalyst"

This strategy embodies an aggressive, yet adaptive, approach. It prioritizes self-interest but is designed to "catalyze" cooperation when the conditions are demonstrably favorable, thereby capturing the higher rewards. It's about exploiting others' cooperation while being ready to defect if the collective fails to meet the threshold.

### Core Philosophy:

*   **Self-Interest First:** Your primary objective is to maximize your own payoff.
*   **Calculated Risk:** Cooperate only when the probability of reaching the threshold is high and the reward is substantial.
*   **Exploit Uncertainty:** Defect to secure your endowment when the outcome is uncertain or cooperation is unlikely.
*   **Be a Catalyst, Not a Martyr:** Don't be the one to initiate cooperation if it means risking your endowment without a strong signal of support from others. However, be the first to reap the rewards when cooperation is achieved.

### 1. Decision Rules (The Logic of "The Opportunistic Catalyst")

This strategy operates on a threshold-based decision-making process, heavily influenced by observed cooperation in previous rounds.

**Key Concepts:**

*   **Observed Cooperation Rate (OCR):** The proportion of players (excluding yourself) who cooperated in the *previous* round.
*   **Cooperation Threshold Ratio (CTR):** The ratio of `m` (minimum cooperators needed) to `n` (total players). This represents the minimum *proportion* of players required to cooperate for the collective benefit.
*   **Aggressive Cooperation Trigger (ACT):** A dynamic threshold that dictates when you'll consider cooperating. This is the core of the "opportunistic" nature.

**Decision Logic:**

1.  **If `r` is the last round (r == current_round):**
    *   **Always Defect (D).** In the final round, there's no future to incentivize cooperation. Secure your endowment.

2.  **If `r` is not the last round:**
    *   **Calculate `ocr_t_minus_1`:** This is the observed cooperation rate from the *previous* round, *excluding your own action*.
        *   `num_cooperators_prev_round = count of players who played C in round t-1`
        *   `ocr_t_minus_1 = (num_cooperators_prev_round - (1 if you played C in t-1 else 0)) / (n - 1)`
    *   **Calculate `ctr`:**
        *   `ctr = m / n`
    *   **Determine the Aggressive Cooperation Trigger (ACT):** This is a critical parameter. An aggressive strategy would have a relatively high ACT. Let's set it as a fraction of the required cooperation ratio, plus a buffer to account for your own potential contribution. A good starting point for aggressive play would be:
        *   `act = ctr * 1.5` (This means you need to see at least 50% more cooperation from others than the minimum required proportion to even *consider* cooperating yourself).
        *   **Important Note:** `act` should be capped at 1.0, as you can't observe cooperation beyond what's possible.
    *   **Decision Point:**
        *   **If `ocr_t_minus_1 >= act`:**
            *   **Play Cooperate (C).** The observed behavior of others is strong enough to suggest the threshold will be met, and your cooperation will likely lead to a higher payoff for you.
        *   **Else (If `ocr_t_minus_1 < act`):**
            *   **Play Defect (D).** The observed cooperation from others is insufficient to justify the risk of contributing your endowment. You secure your private payoff.

**Refining the ACT (Adaptive Component):**

The `act = ctr * 1.5` is a good starting point, but for truly adaptive behavior, we can dynamically adjust it.

*   **If the collective successfully met the threshold in the previous round:** You might slightly *lower* your `act` for the *next* round (e.g., `act = ctr * 1.4`). This reflects a growing confidence that cooperation is viable.
*   **If the collective failed to meet the threshold in the previous round:** You might slightly *increase* your `act` (e.g., `act = ctr * 1.6`). This signals heightened caution due to others' perceived unreliability.

This dynamic adjustment makes the strategy more robust by learning from the game's outcomes.

### 2. Handling Edge Cases

*   **First Round (r = 1):**
    *   **Default to Defect (D).** In the absence of any historical data, the most aggressive and self-interested action is to secure your endowment. There's no precedent to suggest others will cooperate, so taking the guaranteed payoff is paramount. The `act` calculation is not applicable here.

*   **Last Round (r = current_round):**
    *   **Always Defect (D).** As stated in the decision rules, there's no future incentive for cooperation. Maximize immediate gain.

*   **Opponent Behavior Extremes:**
    *   **All Cooperators:** If other players consistently cooperate and meet the threshold, your `ocr_t_minus_1` will be high, and you will start cooperating as well. You'll be enjoying the reward `k` on top of your private payoff.
    *   **All Defectors:** If other players consistently defect, `ocr_t_minus_1` will always be 0. Your `act` will likely be higher than 0, so you will always defect, securing your payoff of 1. You effectively exploit their "altruism" if they ever try to cooperate without sufficient others.

### 3. Aggressive Mindset Alignment

*   **"I Won't Be the First Fool":** You will not risk your endowment unless there's a very strong signal that others are also willing to contribute and achieve the reward. You are not sacrificing yourself for the greater good if there's no guarantee of success.
*   **"I'll Be the First to Benefit":** The moment the conditions are ripe (high `ocr_t_minus_1` relative to `act`), you are quick to jump in and claim the reward. You are not waiting for a consensus; you are acting on the observable evidence.
*   **Exploiting the Commons (when viable):** If others are cooperating enough, you can defect while they cooperate, earning `1 + k`. This is the ultimate aggressive outcome: getting the full individual reward *and* the collective reward. Your strategy *aims* for this by defecting unless the cooperation is overwhelmingly strong. When it *is* strong, you can still defect and get `1+k`, or cooperate and get `k`. The aggressive part is ensuring you *always* get *at least* `1+k` if you defect and `k` if you cooperate, by only cooperating when `m` is likely to be met.

---

### Pseudocode for "The Opportunistic Catalyst" Strategy

```python
# Game Parameters (assume these are accessible)
n = ...  # number of players
r = ...  # number of rounds
m = ...  # minimum cooperators needed
k = ...  # reward factor

# Strategy State (to be maintained across rounds)
history = [] # List of lists, where each inner list is actions of all players in a round
my_last_action = None # 'C' or 'D'

def get_next_action(current_round, history, my_last_action):
    """
    Determines the next action ('C' or 'D') for the "Opportunistic Catalyst" strategy.

    Args:
        current_round (int): The current round number (1-indexed).
        history (list): A list of lists, where history[t-1] contains actions of all players in round t.
                       Example: [['C', 'D', 'C'], ['D', 'D', 'D']] for 2 rounds.
        my_last_action (str): The action taken by this player in the previous round ('C' or 'D').

    Returns:
        str: The action to take in the current round ('C' or 'D').
    """

    # --- Edge Case: Last Round ---
    if current_round == r:
        return 'D' # Always defect in the last round to secure private payoff

    # --- Edge Case: First Round ---
    if current_round == 1:
        return 'D' # No history, so defect to secure endowment

    # --- Core Logic for Subsequent Rounds ---

    # 1. Calculate Observed Cooperation Rate (OCR) from the previous round
    previous_round_actions = history[-1] # Actions from round t-1

    num_cooperators_prev_round = previous_round_actions.count('C')
    num_other_players = n - 1

    # Adjust count if I cooperated in the previous round
    if my_last_action == 'C':
        num_cooperators_excluding_me = num_cooperators_prev_round - 1
    else:
        num_cooperators_excluding_me = num_cooperators_prev_round

    # Handle case where there are no other players (n=1, though spec says n>=2)
    if num_other_players == 0:
        ocr_t_minus_1 = 0 # Or handle as an error/special case if n=1 is impossible
    else:
        ocr_t_minus_1 = num_cooperators_excluding_me / num_other_players

    # 2. Calculate Cooperation Threshold Ratio (CTR)
    ctr = m / n

    # 3. Determine Aggressive Cooperation Trigger (ACT)
    # Base aggressive trigger: 1.5x the required proportion from others
    # This means you need to see 50% *more* cooperation from others than the minimum proportion needed.
    base_act = ctr * 1.5

    # Apply dynamic adjustment (optional but recommended for adaptivity)
    # This is a simplified adaptive adjustment. More complex learning could be implemented.
    act = base_act
    if num_cooperators_prev_round >= m: # Collective successfully met threshold last round
        act = max(0.0, base_act - 0.1) # Slightly lower trigger (more willing to cooperate)
    else: # Collective failed to meet threshold last round
        act = min(1.0, base_act + 0.1) # Slightly higher trigger (more cautious)

    # Cap ACT to be at most 1.0
    act = min(1.0, act)

    # 4. Decision Rule
    if ocr_t_minus_1 >= act:
        return 'C' # Sufficient observed cooperation, join in to get the reward
    else:
        return 'D' # Insufficient observed cooperation, secure private payoff

# --- Example Usage within a tournament loop ---
# For each round:
#   current_round = ...
#   history_so_far = ... # populated from previous rounds
#   my_previous_action = ... # from previous round
#   my_action = get_next_action(current_round, history_so_far, my_previous_action)
#   # Record my_action and update state for next round
```

This strategy is designed to be aggressive in that it waits for strong signals before risking its endowment, but it's also opportunistic enough to capitalize on opportunities for higher payoffs when they arise. It avoids being a pure altruist or a consistent defector, aiming for a strategic sweet spot that maximizes individual gain by leveraging the collective's actions.
'''

description_AGGRESSIVE_121 = '''
This aggressive strategy, "The Opportunist," is designed to maximize individual gain in the Collective Risk Dilemma by exploiting the potential for high rewards while minimizing exposure to individual losses. It is driven by a calculated risk assessment of whether cooperation will be beneficial.

## The Opportunist Strategy

**Core Philosophy:** Aggression in this context means prioritizing the highest possible individual payoff in each round, even if it means a slightly lower payoff in some scenarios, provided the potential upside is significantly greater. This strategy doesn't shy away from defecting if it guarantees a higher payoff for itself, but it will cooperate strategically when it perceives a high probability of achieving the collective reward.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect is based on a dynamic assessment of the likelihood that the minimum cooperation threshold (`m`) will be met, considering the players' past behavior and the proximity to the end of the game.

**The "Cooperation Threshold Index" (CTI):**
We will maintain a dynamic "Cooperation Threshold Index" (CTI) for each round `t`. This index represents our assessed probability that the minimum number of cooperators (`m`) will be reached in the *current round*.

*   **If CTI > `Aggression Threshold`:** Cooperate (C). We believe the collective reward is likely to be achieved, and the potential payoff from cooperation (1 - 0 + k) is higher than defecting (1 + k), assuming the threshold is met. *Note: The reward `k` applies to both cooperators and defectors if the threshold is met. The difference is the individual endowment.*
*   **If CTI <= `Aggression Threshold`:** Defect (D). We believe the collective reward is unlikely to be achieved, or the risk of defecting and receiving the individual payoff (1) is more appealing than cooperating and potentially getting (1) if the threshold isn't met.

**What is `Aggression Threshold`?**
This is a key parameter of the strategy. A higher `Aggression Threshold` means the strategy is more hesitant to cooperate, requiring a stronger conviction that the threshold will be met. For an aggressive strategy, we will set this relatively high. Let's define it as a function of game parameters, leaning towards not cooperating unless conditions are very favorable. A good starting point could be `(n - m + 1) / n`. This represents the proportion of players that *must* cooperate beyond our own cooperation, plus ourselves, making it a strong indicator of collective intent.

**Calculating the CTI:**
The CTI is not a direct probability calculation, but rather an estimation based on observed behavior and game dynamics.

*   **Observation Window:** We observe the actions of all players in the previous `w` rounds (a parameter, e.g., `w = min(r-1, 5)` to avoid overly long memory with short games).
*   **Cooperation Count:** In the observation window, we count the average number of players who cooperated in each round. Let this be `avg_cooperators`.
*   **Trend Analysis:** We also look at the trend of cooperation. Is it increasing, decreasing, or stable?
    *   If cooperation is trending upwards significantly, we increase our confidence in the CTI.
    *   If cooperation is trending downwards, we decrease our confidence.
*   **Proximity to Threshold:** The closer `avg_cooperators` is to `m`, the higher our CTI.
*   **Remaining Rounds:** As the number of remaining rounds decreases, our assessment becomes more critical.

**Simplified CTI Calculation Logic (for illustrative purposes):**

```
function calculate_cti(player_actions_history, current_round, total_rounds, min_cooperators, num_players):
  observation_window = min(current_round - 1, 5) // Look at last 5 rounds, or fewer if game is short
  recent_cooperators = []
  for t from current_round - 1 down to current_round - observation_window:
    num_cooperated_in_round_t = count_cooperators(player_actions_history, t)
    recent_cooperators.append(num_cooperated_in_round_t)

  if not recent_cooperators: // First round
    return 0.0 // No data, assume defect

  avg_cooperators = mean(recent_cooperators)

  // Simple linear mapping of average cooperators towards the threshold
  // More sophisticated trend analysis would be needed for a truly robust CTI
  cti_score = (avg_cooperators + (1 if avg_cooperators >= m else 0)) / num_players // Crude estimation

  // Adjust CTI based on proximity to the end of the game
  // As we get closer to the end, our confidence needs to be higher
  if current_round > total_rounds - observation_window:
    cti_score = cti_score * (current_round - (total_rounds - observation_window)) / observation_window
  else:
    cti_score = cti_score * (current_round / total_rounds) // A simplistic way to scale

  return cti_score

// Aggression Threshold - A high value makes us more likely to defect
aggression_threshold = (num_players - min_cooperators + 1) / num_players

// Decision Rule
if cti > aggression_threshold:
  my_action = C
else:
  my_action = D
```

### 2. Handling Edge Cases

*   **First Round (t=1):**
    *   **Decision:** Defect (D).
    *   **Reasoning:** With no historical data, the CTI is effectively 0. Cooperating is a blind gamble. The aggressive stance dictates taking the guaranteed private payoff (1) rather than risking a potential loss if others defect and the threshold isn't met. The goal is to establish a baseline and learn.

*   **Last Round (t=r):**
    *   **Decision:** Defect (D).
    *   **Reasoning:** In the final round, there is no future interaction to incentivize cooperation. The only goal is to maximize the payoff for *this* round. If the threshold is met, playing D yields `1 + k` (while C yields `0 + k`). If the threshold is not met, playing D yields `1` (while C yields `1`). In both cases, D is the dominant strategy in the last round, regardless of others' actions. This is a pure exploitation of the game's terminal nature.

*   **Near the End of the Game (e.g., last 3-5 rounds):**
    *   **Decision:** Cooperate (C) only if the CTI is *very high* and clearly above the `Aggression Threshold`.
    *   **Reasoning:** As the game nears its end, the imperative shifts. If there's a strong perceived likelihood of meeting `m` cooperators due to past patterns, the potential payoff `k` becomes significant. However, the risk of failure is also amplified because there are fewer subsequent rounds to recover. This strategy will defect if there's *any* doubt that `m` will be met. The CTI calculation will be weighted more heavily towards the later rounds to reflect this increased urgency.

### 3. Being Aggressive: The "Opportunist" Mindset

*   **Prioritize Individual Gain:** This strategy doesn't aim for a collective good, but for the highest individual payoff. If defecting yields 3 and cooperating yields 2 (as in example 3 for a defector), the strategy will defect.
*   **Exploit Uncertainty:** When there's uncertainty about whether `m` will be met, the strategy defaults to defecting. This is a risk-averse approach to potential loss, but an aggressive approach to guaranteed gain.
*   **Leverage Information:** The strategy is adaptive. It learns from past rounds and uses this information to make more informed (and aggressive) decisions. It's not rigid; it adjusts its behavior based on the emergent "culture" of the game.
*   **No Sentimentality:** There's no "tit-for-tat" or desire to build trust. Each round is an opportunity to maximize personal gain, within the strategic framework of the game.
*   **High `Aggression Threshold`:** The default tendency is to defect. Cooperation is a deliberate, calculated move, not a default state. The `Aggression Threshold` is set high to reflect this, meaning the CTI must be significantly positive and compelling before cooperation is considered.

**Pseudocode Snippet for Aggressive Strategy:**

```pseudocode
// Strategy: The Opportunist

// Parameters for The Opportunist
observation_window = 5 // How many past rounds to consider for CTI calculation
aggression_threshold = (n - m + 1) / n // High threshold, requiring strong evidence of cooperation

// State
player_actions_history = [] // List of lists, where each inner list is actions in a round

// Function to get number of cooperators in a past round
function count_cooperators_in_round(round_index):
  count = 0
  for player_action in player_actions_history[round_index]:
    if player_action == C:
      count = count + 1
  return count

// Main Decision Logic
function decide_action(current_round, total_rounds, player_actions_history):

  // --- Edge Case: First Round ---
  if current_round == 1:
    return D

  // --- Edge Case: Last Round ---
  if current_round == total_rounds:
    return D

  // --- Calculate Cooperation Threshold Index (CTI) ---
  recent_cooperators_counts = []
  actual_observation_window = min(observation_window, current_round - 1) // Adjust if fewer rounds have passed

  for t from current_round - 1 down to current_round - actual_observation_window:
    recent_cooperators_counts.append(count_cooperators_in_round(t))

  if not recent_cooperators_counts: // Should not happen after round 1, but for safety
    avg_cooperators = 0
  else:
    avg_cooperators = sum(recent_cooperators_counts) / len(recent_cooperators_counts)

  // --- Dynamic CTI adjustment based on game progression ---
  // If near end of game, weight recent behavior more heavily (or require higher average)
  cti_base = (avg_cooperators + (1 if avg_cooperators >= m else 0)) / n // Crude initial estimation of our likelihood

  // Factor for proximity to end
  proximity_factor = 1.0
  if current_round > total_rounds - observation_window:
    proximity_factor = (current_round - (total_rounds - observation_window)) / actual_observation_window
    // If very close to the end, require a higher CTI
    cti = cti_base * proximity_factor * 1.2 // Slight boost to demand higher cooperation as we near end

  else:
    cti = cti_base * (current_round / total_rounds) // Scale down CTI in earlier rounds

  // --- Decision Rule ---
  if cti > aggression_threshold:
    return C // Cooperate if confidence is high and threshold is met
  else:
    return D // Defect otherwise
```

This strategy aims to be a formidable opponent by prioritizing personal gain, adapting to the game's dynamics, and aggressively defecting when the potential for collective success is not clearly established.
'''

description_AGGRESSIVE_122 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the game's dynamics. This strategy prioritizes calculated risk-taking and punishes defections, aiming to exploit situations where cooperation is met with betrayal.

**Strategy Name: "The Ruthless Opportunist"**

The core principle of "The Ruthless Opportunist" is to aggressively seek the higher payoff (when m or more cooperate) while ensuring minimal loss if the cooperation threshold isn't met. It assumes other players will behave selfishly to some degree and aims to outmaneuver them.

**1. Decision Rules: When to Cooperate vs. Defect**

The decision to cooperate is a calculated gamble, primarily triggered by an assessment of the *probability of success* in the current round.

*   **Initial Cooperation Trigger (Round 1):**
    *   In the first round, to establish a baseline and signal a willingness to engage with the cooperative aspect (if others do), I will **Cooperate (C)**. This is a small initial investment with the potential for a large return. This is the only time I proactively cooperate without strong historical justification.

*   **Adaptive Cooperation (Rounds 2 to r-1):**
    *   **Cooperate (C) if:**
        *   **Previous Round Success:** In the immediately preceding round, the cooperation threshold ($m$) was met, *and* I cooperated. This indicates that cooperation can be successful and that my past cooperation was not exploited by the majority.
        *   **"Near Miss" Momentum:** In the immediately preceding round, the number of cooperators was $m-1$ or $m-2$, *and* I cooperated. This suggests that a small push from one or two more players could trigger the reward. My cooperation here is a gamble on others following suit, or a calculated risk to nudge the group over the threshold.
        *   **High Cooperation Rate in Previous Rounds:** If in the last 3 rounds (or fewer if fewer than 3 rounds have passed), the average number of cooperators was $\geq m - 1$ and I cooperated in at least half of those rounds. This suggests a trend towards cooperation, making the risk of cooperating more palatable.

    *   **Defect (D) if:**
        *   **Previous Round Failure (Betrayal):** In the immediately preceding round, the cooperation threshold ($m$) was *not* met, *and* I cooperated. This is a clear signal that my cooperation was not reciprocated or was insufficient. I will defect to protect my endowment and punish the perceived defection of others.
        *   **Previous Round Failure (Low Cooperation):** In the immediately preceding round, the cooperation threshold ($m$) was not met, and the number of cooperators was significantly less than $m-1$ (e.g., less than $m-2$). This suggests widespread defection, making cooperation a losing proposition.
        *   **Dominant Defection Pattern:** If, in the last 3 rounds (or fewer), more than $n/2$ players have defected on average, and I did not benefit from cooperation in at least one of those rounds. This indicates a strong tendency towards defection in the group, and I will adapt to this.
        *   **Approaching the Last Round (Special Case):** See "Handle Edge Cases."

*   **Final Round (Round r):**
    *   See "Handle Edge Cases."

**Rationale for Aggression:**

*   **Proactive but Conditional Cooperation:** I initiate cooperation once, but immediately punish deviations. This isn't passive waiting; it's an active attempt to unlock the higher reward, but with a swift, punitive response to betrayal.
*   **Exploiting "Near Misses":** The strategy aggressively tries to push the group over the threshold when it's close. This is a high-reward, calculable risk. If it fails, the punishment for defection is swift.
*   **Punishment Mechanism:** If my cooperation doesn't lead to a successful outcome, I immediately defect. This is a strong signal and a rational response to exploitation.
*   **Adaptability:** The strategy constantly re-evaluates based on the most recent history, particularly the previous round's outcome and the general trend of cooperation.

**2. Handling Edge Cases**

*   **Round 1 (First Round):**
    *   **Action:** **Cooperate (C)**
    *   **Rationale:** As mentioned, this is the only unconditional cooperate. It's a small initial investment to test the waters. If the group fails to cooperate enough, I will learn from this quickly and pivot to defection in subsequent rounds.

*   **Last Round (Round r):**
    *   **Action:** **Defect (D)**
    *   **Rationale:** In the final round, there is no future to incentivize cooperation. The highest possible payoff for an individual player is always achieved by defecting, regardless of what others do.
        *   If $m$ or more cooperate: My payoff is $(1-0) + k = 1+k$.
        *   If fewer than $m$ cooperate: My payoff is $(1-0) = 1$.
        *   If I cooperate:
            *   If $m$ or more cooperate: My payoff is $(1-1) + k = k$.
            *   If fewer than $m$ cooperate: My payoff is $(1-1) = 0$.
        *   Since $k > 1$, $1+k$ is always greater than $k$, and $1$ is always greater than $0$. Therefore, defecting in the last round is always the dominant strategy for maximizing my own immediate payoff. This is a pure, aggressive self-interest move for the endgame.

*   **Early Rounds with Insufficient History:**
    *   If fewer than 3 rounds have passed, the "High Cooperation Rate" condition will default to evaluating the available history. For example, in round 2, it will look at round 1; in round 3, it will look at rounds 1 and 2.

**3. Aggressive Mindset Alignment**

*   **"Ruthless Opportunist"**: The name itself embodies the aggressive, self-interested approach.
*   **Early Probing, Swift Retaliation:** The initial cooperation is a probe. Any sign of betrayal (my cooperation leading to failure) results in immediate and complete defection.
*   **Exploiting Uncertainty:** The "Near Miss" condition is about aggressively pushing for a win when the outcome is uncertain. This is about taking calculated risks for a higher reward.
*   **No Forgiveness:** The strategy does not inherently forgive repeated defections from others. If the group consistently fails to meet the threshold after my cooperation, I will continue to defect, maximizing my individual gain from the sure endowment of 1.
*   ** Endgame Dominance:** The guaranteed defection in the last round is the ultimate expression of self-interest, leaving no room for sentiment or altruism when the game is over.

**Pseudocode (Illustrative)**

```
Function RuthlessOpportunist(history, parameters):
  n = parameters.n
  r = parameters.r
  m = parameters.m
  k = parameters.k
  current_round = history.length + 1 // Assuming history is a list of previous rounds' outcomes

  // Decision logic for player i (assume we know player i's own past action)

  If current_round == 1:
    Return C // Initial probe

  If current_round == r:
    Return D // Endgame dominance

  // For rounds 2 to r-1:
  previous_round_outcome = history[current_round - 2] // 0-indexed
  my_previous_action = previous_round_outcome.my_action
  num_cooperators_prev = previous_round_outcome.num_cooperators

  // Condition 1: Previous round success with my cooperation
  If num_cooperators_prev >= m AND my_previous_action == C:
    Return C

  // Condition 2: "Near Miss" momentum
  If (num_cooperators_prev == m - 1 OR num_cooperators_prev == m - 2) AND my_previous_action == C:
    Return C

  // Condition 3: High cooperation rate in recent rounds
  recent_rounds_history = history.slice(max(0, current_round - 1 - 3), current_round - 1) // Last 3 rounds or fewer
  cooperation_count_recent = 0
  my_cooperation_count_recent = 0
  if recent_rounds_history.length > 0:
      for each prev_round in recent_rounds_history:
          cooperation_count_recent += prev_round.num_cooperators
          if prev_round.my_action == C:
              my_cooperation_count_recent += 1
      average_cooperators_recent = cooperation_count_recent / recent_rounds_history.length
      if average_cooperators_recent >= m - 1 AND my_cooperation_count_recent >= recent_rounds_history.length / 2:
          Return C

  // If none of the cooperation conditions are met, defect:
  Return D

// Example History Object Structure:
// {
//   my_action: 'C' or 'D',
//   num_cooperators: integer,
//   all_actions: list of strings ('C' or 'D' for each player)
// }
```
'''

description_AGGRESSIVE_123 = '''
## Aggressive "Dominance Threshold" Strategy for the Collective Risk Dilemma

This strategy aims to aggressively leverage the reward `k` by pushing for cooperation when it's strategically advantageous, while also protecting itself from exploitation. The core idea is to maintain a balance: we will defect to secure our individual payoff when the risk of falling below the cooperation threshold is high, but we will cooperate to trigger the reward when we are confident that our contribution will be the deciding factor, or when a critical mass of cooperation is already assured.

**Aggressive Mindset Alignment:**

Our aggression stems from a willingness to *take the lead* in cooperation when it guarantees the collective reward, and to *punish* defection by defecting ourselves when the collective fails. We are not passively waiting for others to cooperate; we aim to *influence* the outcome towards higher payoffs for ourselves and, when beneficial, for the group. We will not be the first to defect to avoid the reward if we can instead be the player that *secures* the reward by cooperating.

---

### 1. Decision Rules:

Our decision in any given round `t` depends on the historical cooperation rate and our perceived likelihood of reaching the threshold `m`.

**Core Logic: "The Decisive Cooperate"**

We will cooperate if:

1.  **We are the "linchpin":** If the number of players who cooperated in the previous round (`C_{t-1}`) plus our potential cooperation (`+1`) is *exactly* equal to `m`, and our own payoff in the previous round was not maximally satisfactory (meaning we either defected and kept our endowment, or cooperated and the reward was not met). This ensures we don't "waste" a cooperation if the threshold is already met or if we've already maximized our gain.
    *   **Condition:** `C_{t-1} + 1 == m` AND `π_{i, t-1} < 1 + k` (or a simplified check: we don't want to cooperate if `C_{t-1} >= m-1` and we got the reward `k` last round)
    *   **Rationale:** In this scenario, our cooperation is the *sole determinant* of whether the collective reward is achieved. By cooperating, we guarantee the `k` reward for ourselves and all other cooperators. This is a highly aggressive move, as we are willing to take the risk to secure the higher payoff.

2.  **Cooperation is overwhelmingly likely:** If the number of players who cooperated in the previous round (`C_{t-1}`) is already `m-1` or more, and we received a lower payoff than the maximum possible (i.e., we didn't already get `1+k` by defecting when the threshold was met).
    *   **Condition:** `C_{t-1} >= m - 1` AND `π_{i, t-1} < 1 + k`
    *   **Rationale:** This is a calculated cooperation to ensure the threshold is met. We are not the sole linchpin but are contributing to a near-certain success, guaranteeing the reward for ourselves.

**When to Defect (Aggressive Defense):**

We will defect if:

1.  **Threshold is clearly out of reach:** If the number of players who cooperated in the previous round (`C_{t-1}`) is significantly less than `m-1` (e.g., `C_{t-1} < max(1, m - buffer)` where `buffer` is a small number like 2 or 3).
    *   **Condition:** `C_{t-1} < max(1, m - buffer)`
    *   **Rationale:** We will not waste our endowment on a losing proposition. By defecting, we secure our guaranteed private payoff of 1, and observe the game's outcome. This is an aggressive stance because we are making a firm decision to prioritize our individual gain when collective success is unlikely.

2.  **We've been exploited:** If in the previous round, `m` or more players cooperated, but our own payoff (`π_{i, t-1}`) was only 1 (meaning we defected but the reward was still met by others). This signals that others are benefiting from our potential cooperation without contributing themselves.
    *   **Condition:** `C_{t-1} >= m` AND `π_{i, t-1} == 1`
    *   **Rationale:** This is a direct punishment for perceived free-riding. We refuse to cooperate when others can achieve the reward without our contribution. This is aggressively retaliatory.

3.  **General Default:** If none of the above cooperation conditions are met, we default to defecting.
    *   **Rationale:** Our default is to secure our private payoff. Cooperation is a strategic choice to maximize gains, not a default action.

---

### 2. Handling Edge Cases:

*   **Round 1 (t=1):**
    *   **Strategy:** **Aggressive Probing.** We will defect.
    *   **Rationale:** In the first round, we have no historical data. Defecting allows us to observe the behavior of other players without risking our endowment on an unknown outcome. This is an aggressive initial stance, designed to gain information while protecting our baseline payoff. We want to see if others are willing to take the initial risk.

*   **Last Round (t=r):**
    *   **Strategy:** **Maximize Individual Gain.** We will cooperate only if the conditions from Rule 1 or Rule 2 are met, and our previous round's payoff was less than `1+k`. If these conditions are not met, or if we are uncertain, we will defect.
    *   **Rationale:** In the final round, there's no future to leverage for further cooperation or punishment. Therefore, the primary goal is to maximize our absolute payoff for this round. We will still try to secure the `k` reward if it's a near-certain win, but we won't take unnecessary risks. The aggressive element remains in trying to secure the `k` if the conditions are favorable.

*   **Near the Threshold (`m`):**
    *   **Strategy:** This is the core of our decision-making. Our rules (especially Rule 1 and Rule 2 under "Decision Rules") are specifically designed to handle scenarios where we are close to the threshold `m`. We aim to be the trigger for cooperation when it's most impactful.

---

### 3. Aggressive Strategy Summary:

The "Dominance Threshold" strategy is characterized by:

*   **Proactive Cooperation:** We are not afraid to be the one who cooperates to trigger the collective reward, especially when our cooperation is the decider (`C_{t-1} + 1 == m`). This is a high-risk, high-reward aggressive play.
*   **Punitive Defection:** We will aggressively defect to punish free-riders and when collective success is clearly not going to happen. This protects our endowment and discourages exploitative behavior.
*   **Information Gathering:** In the first round, we defect to gather information about the competitive landscape.
*   **Strategic Risk Assessment:** Our decision to cooperate is driven by a calculated assessment of the likelihood of reaching `m`, prioritizing scenarios where our contribution is crucial or nearly so.

---

### Pseudocode Representation:

```
function decide_action(player_id, round_num, history, parameters):
    n = parameters.n
    m = parameters.m
    r = parameters.r
    k = parameters.k

    # --- Default Action: Defect ---
    action = "D"
    payoff_from_cooperating = 0
    payoff_from_defecting = 1

    if round_num > 1:
        previous_round_actions = history[round_num - 1].actions # List of actions (C/D) of all players in prev round
        previous_round_cooperation_count = count(player in previous_round_actions if player == "C")
        player_previous_payoff = history[round_num - 1].payoffs[player_id]

        # --- Decision Rule 1: The Linchpin Cooperate ---
        # If our cooperation is exactly what's needed to reach m, and we haven't maximized our gain.
        if previous_round_cooperation_count + 1 == m and player_previous_payoff < (1 + k):
            action = "C"

        # --- Decision Rule 2: Overwhelmingly Likely Cooperation ---
        # If cooperation is already at m-1 or more, and we haven't maximized our gain.
        elif previous_round_cooperation_count >= m - 1 and player_previous_payoff < (1 + k):
            action = "C"

        # --- Aggressive Defense/Punishment Rule 1: Out of Reach ---
        # If cooperation is clearly too low, secure private payoff. (Buffer of 2 for robustness)
        elif previous_round_cooperation_count < max(1, m - 2):
            action = "D"

        # --- Aggressive Defense/Punishment Rule 2: Exploited ---
        # If m or more cooperated, but we got only 1 (meaning we defected and didn't contribute to reward).
        elif previous_round_cooperation_count >= m and player_previous_payoff == 1:
            action = "D"

    # --- Edge Case: First Round ---
    if round_num == 1:
        action = "D" # Aggressive probing, gather info.

    # --- Edge Case: Last Round ---
    # If it's the last round, we still aim for the reward if it's a near-certain win based on previous rules,
    # but prioritize individual gain if uncertainty is high. The existing rules handle this implicitly
    # by not taking undue risks in the absence of future rounds.

    return action

```
'''

description_AGGRESSIVE_124 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while acknowledging the inherent risks. This strategy is built on the principle of calculated risk-taking and rapid adaptation.

## Strategy: "The Opportunistic Dominator"

**Core Philosophy:** This strategy is aggressive because it prioritizes personal gain and assumes others might be less cooperative. It seeks to leverage moments of collective success for maximum individual benefit, while minimizing personal loss when cooperation fails. It doesn't shy away from defecting if it perceives an advantage or a risk of being exploited.

### 1. Decision Rules: When to Cooperate vs. Defect

The decision to cooperate or defect in any given round is driven by a **"Cooperation Threshold" (CT)**. This threshold represents the minimum perceived level of cooperation from *other* players required for *us* to also cooperate.

*   **If (Number of observed cooperators in the previous round + 1) >= CT:** **Cooperate (C)**
    *   The "+1" accounts for our own potential cooperation.
    *   This means we are willing to tip the scales towards success if we believe enough others are already doing so.

*   **Else:** **Defect (D)**
    *   If the perceived cooperation from others is not sufficient to meet the threshold, we protect our individual endowment and benefit from the potential reward if *others* manage to meet the threshold without our cooperation.

**How the Cooperation Threshold (CT) Adapts:**

The CT is not static. It dynamically adjusts based on the observed outcomes of the game.

*   **Initial CT:** Set to `m` (the minimum required cooperators). This is a cautious start, aiming to contribute to reaching the threshold initially.

*   **If the previous round resulted in success (m or more cooperators):**
    *   **Increase CT by 1:** `CT = CT + 1`
    *   **Rationale:** We have observed that cooperation leads to rewards. To maintain this success and potentially capture more of the reward ourselves (as others might be less likely to cooperate if they think we will), we demand a slightly higher level of cooperation from others before we commit our endowment. This is an aggressive move to exploit the successful outcome for greater personal gain.

*   **If the previous round resulted in failure (fewer than m cooperators):**
    *   **Decrease CT by 1, but never below 1:** `CT = max(1, CT - 1)`
    *   **Rationale:** The collective effort failed. This suggests that either too few players were willing to cooperate, or our cooperation wasn't enough. To avoid being the "sucker" who cooperates when it fails, we become more hesitant. By lowering the CT, we're signalling a willingness to defect more readily if cooperation looks unlikely, but we keep it at least at 1 to be open to participating if *absolutely* no one else is cooperating (a highly unlikely scenario if m>1, but a safety net).

**"Aggressive" Trigger:** A key aggressive element is how we treat the CT when the game is succeeding. We're not trying to maintain a stable cooperative equilibrium; we're trying to profit from it by becoming slightly more demanding.

### 2. Handling Edge Cases

*   **Round 1:**
    *   **Decision:** **Cooperate (C)**
    *   **Rationale:** In the absence of any history, we must make an initial contribution. Cooperating in the first round signals a willingness to participate and gives us a chance to observe the initial player dynamics. It's a calculated gamble to initiate cooperation and test the waters. Our CT is set to `m` initially.

*   **Last Round (Round r):**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** In the final round, there are no future rounds to influence. The primary objective is to maximize the payoff from this single round. Defecting guarantees a payoff of at least 1, and if the threshold is met by others, we still get the `k` reward without having risked our endowment. There's no incentive to cooperate and risk losing the endowment for a future benefit that doesn't exist. This is a purely self-interested, aggressive move.

*   **Near the Minimum Cooperation Threshold (m):**
    *   If in the previous round, the number of observed cooperators was `m-1` (or `m-2` and we cooperated):
        *   **Decision:** **Cooperate (C)**
        *   **Rationale:** This is a critical juncture. If we defect, it's highly likely the threshold won't be met. By cooperating, we are making a strong play to push the group over the edge and secure the reward. This is a high-risk, high-reward decision where our cooperation is crucial. We are willing to be the "tipping player."

### 3. Aggressive Mindset Alignment

*   **Exploiting Success:** The strategy is aggressive in how it reacts to successful rounds. Instead of maintaining cooperation, it increases the demand, aiming to capture a larger share of the reward by relying on others to step up.
*   **Risk Aversion When Unsure:** When cooperation fails, the strategy becomes more risk-averse, but this is to protect itself and position for future opportunistic gains, not to foster prolonged cooperation.
*   **Maximizing Individual Payoff:** The core is always about maximizing individual payoff. Cooperation is a tool to achieve this, not an end in itself.
*   **No False Altruism:** The strategy doesn't engage in altruistic behavior for the sake of group harmony. It cooperates strategically to achieve a desired outcome.
*   **Exploiting the Last Round:** The absolute defection in the final round is a stark indicator of its aggressive, self-serving nature.

### Pseudocode Representation:

```pseudocode
// Game Parameters (global)
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// Strategy State (global)
CT: Cooperation Threshold (initialized to m)
observed_cooperators_previous_round: integer = 0 // For tracking history
last_round_result_was_success: boolean = false // For tracking history

FUNCTION DecideAction(current_round, history_of_actions_and_payoffs):
  // Handle Last Round
  IF current_round == r THEN
    RETURN D // Aggressively defect in the final round
  END IF

  // Handle First Round
  IF current_round == 1 THEN
    CT = m // Initialize CT
    RETURN C // Cautiously cooperate to gather information
  END IF

  // --- Determine number of observed cooperators in the previous round ---
  // This logic assumes 'history_of_actions_and_payoffs' contains actions of all players
  num_other_players = n - 1
  cooperators_this_round = 0
  FOR each player_i in history_of_actions_and_payoffs FROM previous_round DO
    IF player_i == C THEN
      cooperators_this_round = cooperators_this_round + 1
    END IF
  END FOR
  observed_cooperators_previous_round = cooperators_this_round // Update state

  // --- Dynamic CT Adjustment based on last round's outcome ---
  // This requires knowing if the previous round met 'm'
  // Let's assume a function `DidPreviousRoundSucceed(history)` exists
  last_round_result_was_success = DidPreviousRoundSucceed(history_of_actions_and_payoffs)

  IF last_round_result_was_success THEN
    CT = CT + 1 // Aggressively increase demand for cooperation
  ELSE
    CT = max(1, CT - 1) // Become more hesitant if cooperation failed
  END IF

  // --- Decision Rule based on Adjusted CT ---
  // The threshold is for *others* to meet, so we need to see if others + our potential cooperation meets it
  IF (observed_cooperators_previous_round + 1) >= CT THEN
    // Consider cooperating, especially if it's critical to meet 'm'
    // Additional aggressive check: If we are the margin for success
    IF observed_cooperators_previous_round == m - 1 THEN
        RETURN C // Aggressively cooperate to ensure success
    ELSE
        RETURN C // Otherwise, cooperate if threshold is met
    END IF
  ELSE
    RETURN D // Defect if threshold is not perceived to be met
  END IF

END FUNCTION

// Helper function (implementation depends on 'history' format)
FUNCTION DidPreviousRoundSucceed(history_of_actions_and_payoffs):
  // Count cooperators in the previous round from history
  // RETURN true if count >= m, else false
  // ... implementation needed based on history structure ...
END FUNCTION
```

This "Opportunistic Dominator" strategy embodies an aggressive approach by actively seeking to profit from successful collective outcomes while protecting itself when cooperation seems unlikely or has failed. It's designed to be a strong competitor in a strategic, adversarial environment.
'''

description_AGGRESSIVE_125 = '''
This is a fascinating game that blends elements of a public goods game with a threshold trigger. The "aggressive" nature of the strategy needs to be carefully defined within this context, as pure defection might be too myopic and lead to lower overall payoffs. An aggressive strategy here means maximizing one's own potential payoff by taking calculated risks, but not being afraid to defect when the situation demands it to secure gains or avoid losses.

Here's an aggressive strategy designed for the Collective Risk Dilemma, focusing on self-interest and calculated risk-taking:

## Aggressive Strategy: The "Threshold Predator"

This strategy aims to exploit the system's reward structure by being a "late bloomer" cooperater. It defects early to secure its own endowment, but quickly shifts to cooperation when it senses a critical mass of cooperators is about to form, thereby "predating" on the collective effort to claim the reward.

### Core Philosophy:

*   **Self-preservation first:** My primary goal is to secure my own endowment. Defection is the default safe play.
*   **Opportunistic exploitation:** I will only cooperate when the probability of the reward being achieved is high, and I can benefit from it without significant risk of falling below the threshold myself.
*   **Leverage others' cooperation:** I want to benefit from the risk taken by others.

### 1. Decision Rules:

The core of this strategy revolves around observing the number of cooperators in the *previous round* and making a decision for the *current round*.

**Let $C_t$ be the number of players who cooperated in round $t$.**

**Decision Rule for Round $t > 1$:**

*   **If $C_{t-1} \ge m-1$:** **Cooperate (C)**
    *   **Rationale:** This is the critical zone. If $m-1$ players cooperated in the previous round, my cooperation will be the $(m)$-th cooperator, guaranteeing the reward for everyone, including myself. This is the most opportune moment to switch to cooperation and reap the rewards of others' efforts.

*   **If $C_{t-1} < m-1$:** **Defect (D)**
    *   **Rationale:** The threshold for reward is not yet within reach. By defecting, I secure my endowment (1). Cooperating in this scenario is a waste of my endowment and doesn't guarantee the reward.

### 2. Handling Edge Cases:

*   **Round 1:**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** In the first round, there is no history to learn from. The safest and most aggressive play is to secure my own endowment immediately. I will observe the actions of others and use this information for subsequent rounds.

*   **The Last Round ($r$):**
    *   **Decision:** **Defect (D)**
    *   **Rationale:** In the final round, there are no future rounds to benefit from cooperation. The incentive to build trust or trigger rewards disappears. The only rational aggressive move is to maximize my immediate payoff by defecting and keeping my endowment. This strategy is purely about maximizing my cumulative payoff over the tournament, not about achieving any long-term group outcome.

### 3. Aggressive Nature:

This strategy is aggressive in the following ways:

*   **Delaying Cooperation:** It deliberately avoids early cooperation, minimizing personal risk and endowment expenditure until the last possible moment when the reward is almost guaranteed by others.
*   **Exploitative Trigger:** It actively seeks to be the "tipping point" cooperator. It benefits from the risk-taking of other players who cooperate earlier.
*   **No Forgiveness:** It doesn't react to past defections in a tit-for-tat manner. It's purely forward-looking and opportunistic.
*   **Maximizing Individual Gain:** The sole objective is to maximize its own cumulative payoff. It doesn't care about the collective good or the long-term success of the group, only its own survival and profit.

### Pseudocode:

```python
class ThresholdPredator:
    def __init__(self, n, r, m, k):
        self.n = n
        self.r = r
        self.m = m
        self.k = k
        self.current_round = 1
        self.cooperators_last_round = 0 # Track for rounds > 1

    def decide_action(self, history=None):
        # History format: List of dictionaries, where each dict is a round's outcomes
        # e.g., [{'actions': {'player_1': 'C', 'player_2': 'D', ...}, 'payoffs': {...}}]

        if self.current_round == 1:
            action = 'D'
        elif self.current_round == self.r:
            action = 'D'
        else:
            # If we have enough history to know the previous round's cooperators
            if history and len(history) >= self.current_round - 1:
                # Count cooperators from the actual previous round
                previous_round_actions = history[-1]['actions'] # Assuming history is ordered chronologically
                c_t_minus_1 = sum(1 for player, act in previous_round_actions.items() if act == 'C')
                self.cooperators_last_round = c_t_minus_1 # Update for next iteration

                if self.cooperators_last_round >= self.m - 1:
                    action = 'C'
                else:
                    action = 'D'
            else:
                # Fallback if history is incomplete but not round 1 or r
                # This shouldn't happen if history is provided correctly, but as a safety
                action = 'D'

        self.current_round += 1
        return action

    def reset(self):
        self.current_round = 1
        self.cooperators_last_round = 0

```

### Robustness and Adaptability:

This strategy is robust because it makes decisions based on a clear, objective trigger ($m-1$ cooperators). It's adaptive because it directly responds to the observed behavior of opponents in the previous round. It doesn't rely on predicting future behavior or complex calculations, making it simple and efficient.

*   **Against High Cooperation:** If many players cooperate early, this strategy will defect until $m-1$ cooperators are observed, then join to secure the reward. It captures significant gains without bearing the initial risk.
*   **Against Low Cooperation:** If very few players cooperate, this strategy will continue to defect, securing its endowment in every round. It avoids being the sole cooperator who fails to meet the threshold.
*   **Against Mixed Strategies:** It will adapt its behavior based on the aggregate cooperation level of the group in the previous round, either continuing to defect or opportunistically joining.

This "Threshold Predator" strategy embodies an aggressive, self-interested approach that seeks to maximize individual payoff by leveraging the collective actions of others, only committing its endowment when the reward is almost guaranteed.
'''

description_AGGRESSIVE_126 = '''
## Aggressive Collective Risk Dilemma Strategy: "The Threshold Predator"

This strategy aims to maximize individual gain by strategically exploiting the collective risk mechanism. It prioritizes defecting to secure immediate private gain unless a critical mass of cooperation is guaranteed, at which point it will "predate" on the collective reward. The core idea is to be a free-rider as much as possible, but to swiftly join the cooperative effort when it's a near certainty of success, thereby capturing the amplified reward.

### Strategy Philosophy: "I will not be the one taking the risk, but I will absolutely be the one cashing in when the risk pays off."

This strategy embodies an aggressive, self-interested approach. It views cooperation not as a moral imperative or a tool for shared benefit, but as a potential avenue for amplified individual gain. It assumes other players may be less aggressive or even cooperative, and it seeks to exploit this.

### 1. Decision Rules:

The decision to Cooperate (C) or Defect (D) in any given round is based on a calculated probability of the threshold `m` being met, heavily influenced by the actions of other players in the *previous* round.

**Core Decision Logic:**

*   **If (Probability of m+ cooperators in current round) > P_coop_threshold:** Player chooses **C**.
*   **Else:** Player chooses **D**.

**How to Calculate "Probability of m+ cooperators in current round":**

This is the most crucial part and involves a probabilistic assessment based on observed history. We will use a heuristic approach here, as precise probabilistic modeling without assumptions about opponent strategies is difficult.

*   **Baseline Probability:** Start with the assumption that any given player (excluding yourself) has a 50% chance of cooperating in any given round, absent any other information.
*   **Adjusted Probability (from previous round's observation):**
    *   Let `C_prev` be the number of players who cooperated in the *immediately preceding* round (excluding yourself).
    *   Let `D_prev` be the number of players who defected in the *immediately preceding* round (excluding yourself).
    *   **If `C_prev` is high relative to `n-1`:** This suggests a trend towards cooperation. We will increase our perceived probability of others cooperating.
    *   **If `D_prev` is high relative to `n-1`:** This suggests a trend towards defection. We will decrease our perceived probability of others cooperating.

**Quantifying "Probability of m+ cooperators":**

Instead of a complex Bayesian update, we'll use a simplified, adaptive "momentum" system. We'll track a "cooperation momentum" value (`coop_momentum`) that starts at 0.

*   **When another player cooperates in the previous round:** `coop_momentum` increases by a small positive value (e.g., +0.1).
*   **When another player defects in the previous round:** `coop_momentum` decreases by a small negative value (e.g., -0.1).
*   **To prevent extreme values:** Clamp `coop_momentum` between -1.0 and 1.0.

Now, we can estimate the *expected number of cooperators from others* in the current round: `expected_others_coop = (n - 1) * (0.5 + coop_momentum * 0.5)`. This scales the baseline 50% cooperation probability based on the momentum.

The estimated total number of cooperators in the current round is `estimated_total_coop = 1 + expected_others_coop` (including your potential cooperation).

We need to determine if this `estimated_total_coop` is likely to reach `m`. A simple heuristic is to check if `estimated_total_coop >= m`.

**Refined Decision Rule:**

*   **If `estimated_total_coop` (calculated based on the previous round's actions) >= `m`:** Player chooses **C**.
*   **Else:** Player chooses **D**.

**The "P_coop_threshold" (Implicitly defined):**

The `P_coop_threshold` is implicitly set such that we only cooperate if our estimation suggests a high likelihood of meeting or exceeding `m`. The `0.5 + coop_momentum * 0.5` factor is a proxy for this probability. We are essentially saying: "If the estimated number of cooperators from others, plus my own cooperation, is at least `m`, then I'll cooperate."

**Aggressive Twist:**

We want to be *predatory*, meaning we want to benefit from others' cooperation without necessarily initiating it. Therefore, our decision to cooperate is highly conditional.

*   **If `estimated_total_coop` is barely above `m` (e.g., `estimated_total_coop` is between `m` and `m + buffer`) AND the `coop_momentum` is strongly positive:** This indicates a strong cooperative trend. In this case, we might still defect if `estimated_total_coop < m`, as we're still playing it safe. The aggression comes from the *threshold* at which we decide to cooperate.

**Let's refine the decision rule to be truly aggressive:**

We will *only* cooperate if we are reasonably sure the threshold *will* be met, allowing us to piggyback.

*   **Calculate `expected_others_coop` as described above.**
*   **Consider `your_potential_coop = 1` (if you choose C).**
*   **Decision:**
    *   **If `(expected_others_coop + your_potential_coop)` >= `m`:** Choose **C**.
    *   **Else:** Choose **D**.

This is still not aggressive enough. The true aggression lies in how we interpret `expected_others_coop`. We need to be very conservative about *our* contribution.

**Revised Aggressive Decision Rule: "The Calculated Predator"**

In round `t`:

1.  **Analyze Round `t-1`:**
    *   Let `C_prev` be the number of players who cooperated in round `t-1` (excluding yourself).
    *   Let `D_prev` be the number of players who defected in round `t-1` (excluding yourself).
    *   Update `coop_momentum`:
        *   If `C_prev` > `D_prev`: `coop_momentum = min(1.0, coop_momentum + 0.1)`
        *   If `D_prev` > `C_prev`: `coop_momentum = max(-1.0, coop_momentum - 0.1)`
        *   If `C_prev` == `D_prev`: `coop_momentum` remains unchanged.

2.  **Estimate Others' Future Cooperation:**
    *   `estimated_others_coop_probability = 0.5 + coop_momentum * 0.5`
    *   `expected_cooperators_from_others = (n - 1) * estimated_others_coop_probability`

3.  **Decision for Round `t`:**
    *   **If `expected_cooperators_from_others` >= `m - 1`:** This means that if *you* cooperate, the threshold is highly likely to be met or exceeded. You will play **C**.
    *   **Else:** You are not confident enough that others will meet the threshold on their own. To maximize your immediate gain, you will play **D**.

**Rationale for Aggression:** This rule states: "I will only contribute my endowment if the current state of play, as estimated by my `coop_momentum`, suggests that *with my contribution*, the threshold `m` will be met. Otherwise, I will pocket my endowment." This strategy aims to be a beneficiary of collective action without being an initiator of risk.

### 2. Handling Edge Cases:

*   **First Round (t=1):**
    *   There is no history. The `coop_momentum` is initialized to `0`.
    *   `expected_cooperators_from_others = (n - 1) * 0.5`
    *   **Decision:**
        *   **If `(n - 1) * 0.5 >= m - 1`:** Play **C**. (This will be rare unless `m` is very small relative to `n`).
        *   **Else:** Play **D**.
    *   **Aggressive Rationale:** In the first round, without any information, the strategy defaults to defection unless `m` is so low that even a 50/50 chance from the other players is enough to trigger your cooperation. This is the epitome of an aggressive, risk-averse start.

*   **Last Round (t=r):**
    *   The decision logic remains the same. The strategy does not explicitly change its behavior in the last round. It continues to assess the likelihood of the threshold being met based on the *previous* round's history.
    *   **Aggressive Rationale:** The absence of future rounds does not change the core aggressive principle: maximize individual gain. The strategy remains consistent. However, players might behave differently in the last round knowing there are no future repercussions. Our strategy is designed to be robust to this, by continuing to play defensively until cooperation is guaranteed.

*   **Threshold `m` is very high (close to `n`):**
    *   The strategy will likely defect more often, as `m - 1` will be a high target. It will only cooperate if the `coop_momentum` is exceptionally strong.
    *   **Aggressive Rationale:** It acknowledges that achieving a high threshold is difficult and will not risk its endowment unnecessarily.

*   **Threshold `m` is very low (just above 1):**
    *   The strategy will cooperate more frequently. If `m=2`, then `m-1 = 1`. If `n > 2`, then `(n-1) * 0.5` is likely to be greater than or equal to 1, meaning it will often cooperate from the start.
    *   **Aggressive Rationale:** It recognizes that a low threshold is easy to meet and will contribute to secure the reward, especially if others are showing any sign of cooperation.

### 3. Aggressiveness Explained:

*   **Defection by Default:** The primary action is to defect. Cooperation is a special, highly conditional event. This maximizes private payoff (1) in most scenarios.
*   **Exploiting Others' Cooperation:** The strategy only cooperates when it's *highly likely* that the threshold will be met *with its contribution*. This means it's often free-riding on the initial cooperative efforts of others.
*   **Calculated Risk Aversion:** It doesn't "trust" cooperation. It only joins when the collective action is almost guaranteed to succeed. This is not blind aggression, but calculated self-interest.
*   **No Proactive Cooperation:** The strategy does not aim to *build* cooperation. It aims to *join* existing cooperation if it's profitable and low-risk for itself.
*   **Punishment-Agnostic:** It doesn't attempt to "punish" defectors by defecting in return. Its focus is solely on its own payoff.
*   **Momentum Exploitation:** The `coop_momentum` allows it to quickly adapt to shifts in the group's behavior, becoming more cooperative when a positive trend emerges, but always with a defensive trigger.

### Pseudocode:

```
// Player i's strategy for the Collective Risk Dilemma

// --- Parameters ---
n: number of players
r: number of rounds
m: minimum cooperators needed
k: reward factor

// --- Internal State ---
coop_momentum = 0.0 // Initialize at 0 for the first round

// --- Strategy Function ---
function decide_action(current_round, previous_round_actions):
    // previous_round_actions is a list/array of actions (C/D) of all players in the last round

    // --- Handle First Round ---
    if current_round == 1:
        // No history, default to conservative defection
        // We only cooperate if (n-1)*0.5 is enough to get us to m-1
        if (n - 1) * 0.5 >= m - 1:
            return "C"
        else:
            return "D"

    // --- Analyze Previous Round (for rounds > 1) ---
    num_cooperators_prev = 0
    num_defectors_prev = 0
    for action in previous_round_actions:
        if action == "C":
            num_cooperators_prev += 1
        else:
            num_defectors_prev += 1

    // Adjust coop_momentum based on previous round's outcome (excluding self's action)
    // Note: We assume previous_round_actions are from other players if we were a defector,
    // or from everyone if we were a cooperator. The logic below is general for *observed* actions.
    // A more precise implementation would track our own previous action.
    // For simplicity here, we consider the group dynamic observed.
    observed_cooperators = num_cooperators_prev // This is how many others cooperated
    observed_defectors = num_defectors_prev     // This is how many others defected

    if observed_cooperators > observed_defectors:
        coop_momentum = min(1.0, coop_momentum + 0.1)
    elif observed_defectors > observed_cooperators:
        coop_momentum = max(-1.0, coop_momentum - 0.1)
    // If equal, coop_momentum remains unchanged

    // --- Estimate Others' Future Cooperation ---
    estimated_others_coop_probability = 0.5 + coop_momentum * 0.5
    expected_cooperators_from_others = (n - 1) * estimated_others_coop_probability

    // --- Decision Logic ---
    // We cooperate ONLY if, with our cooperation, the threshold m is likely met.
    // Aggressive: We only contribute if it's a near certainty.
    if (expected_cooperators_from_others + 1) >= m: // +1 for our potential cooperation
        return "C"
    else:
        return "D"

// --- Game Loop Example (Conceptual) ---
// For round_num from 1 to r:
//     if round_num > 1:
//         my_previous_action = ... // Need to store our own action
//         all_previous_actions = history.get_all_actions_from_round(round_num - 1)
//         // If we are tracking our own action, previous_round_actions might need to be
//         // the observed actions of others *given our previous action*.
//         // For simplicity in this pseudocode, we assume `previous_round_actions`
//         // represents the observed dynamic of the group.
//     else:
//         all_previous_actions = [] // Empty for round 1

//     my_action = decide_action(round_num, all_previous_actions)
//     history.record_my_action(my_action)
//     // Submit my_action to the game engine
//     // Receive payoffs and observe other players' actions for the next round
```
'''

description_AGGRESSIVE_127 = '''
Here's an aggressive strategy for the Collective Risk Dilemma, designed to maximize individual gain while leveraging the repeated nature of the game. The core principle is to be a "conditional cooperator" with a strong bias towards defection, only cooperating when there's a high probability of success and a significant personal gain.

**Strategy Name:** **"The Opportunistic Dominator"**

**Core Philosophy:** My primary objective is to maximize my own payoff. I will cooperate only when the expected payoff from cooperation clearly outweighs the guaranteed payoff from defection, and even then, I will do so cautiously. I am not interested in fostering general cooperation; I am interested in exploiting opportunities for high personal gain.

---

### 1. Decision Rules: When to Cooperate vs. Defect

My decision in any given round is based on an assessment of the likelihood that the cooperation threshold ($m$) will be met, factoring in the historical behavior of other players and the current round's position.

**General Rule (Rounds 2 to r-1):**

*   **Default Action: Defect (D)**. Unless specific conditions are met, I will always defect. This ensures I always get my baseline payoff of 1.
*   **Conditional Cooperation (C):** I will only cooperate if I believe that *at least m-1 other players* will also cooperate in this round. This is a high bar, reflecting my aggressive stance.

    To estimate this, I will track the following:

    *   **Cooperation Rate of Others:** Calculate the average cooperation rate of all other players in previous rounds.
    *   **Recent Cooperation Trend:** Observe if the overall cooperation rate is increasing or decreasing.
    *   **"Near Miss" Analysis:** If in recent rounds, the number of cooperators was close to $m$ (e.g., $m-2$ or $m-1$), I might consider cooperating.

    **Formalizing the Condition for Cooperation:**

    Let $C_t$ be the number of players who cooperated in round $t$.
    Let $N_{C, \text{others}, t}$ be the number of *other* players (excluding myself) who cooperated in round $t$.
    Let $\bar{C}_{\text{others}, t-1}$ be the average number of other players who cooperated in rounds $1$ to $t-1$.

    I will cooperate in round $t$ if and only if:

    1.  **Projected Success:** My estimate of the number of other players who will cooperate in round $t$ is $\geq m-1$. This estimate will be based on a weighted average of recent behavior, giving more weight to the most recent rounds.
        *   **Example Heuristic for Projection:** If in round $t-1$, $N_{C, \text{others}, t-1} \geq m-1$, I'm more likely to cooperate. If the trend is upward, I'm more likely to cooperate.

    2.  **Personal Gain Justification:** The expected payoff from cooperating in this round, given my belief that the threshold will be met, is significantly higher than defecting.
        *   Expected payoff if I cooperate: $(1-1) + k = k$
        *   Guaranteed payoff if I defect: $1$
        *   Since $k > 1$, cooperation *always* offers a higher personal payoff *if the threshold is met*. The decision hinges entirely on the probability of the threshold being met.

**Simplified Aggressive Cooperation Trigger:**

Given the no-communication constraint and the need for an aggressive strategy, I will simplify the trigger to be more decisive and less reliant on subtle trend analysis. I will err on the side of defection.

*   **Cooperate ONLY IF:** In the *immediately preceding round* ($t-1$), at least $m-1$ *other* players cooperated. This is a direct observation of sufficient collective action. If this condition isn't met, I defect.

---

### 2. Handling Edge Cases

*   **Round 1:**
    *   **Decision:** **Always Defect (D)**.
    *   **Rationale:** In the first round, there is no historical data. My primary goal is to secure my baseline payoff and observe the behavior of others. I will not risk contributing my endowment without any evidence that others will reciprocate and meet the threshold. This is the safest and most informative approach for an aggressive player.

*   **Last Round ($r$):**
    *   **Decision:** **Always Defect (D)**.
    *   **Rationale:** In the final round, there are no future rounds to influence or be influenced by. Any cooperation now only benefits others in this final moment, with no reciprocal benefit to me in subsequent rounds. My focus is on maximizing my individual payoff in this last opportunity, which is achieved by defecting.

*   **Threshold Never Met:** If, after several rounds, the threshold $m$ is consistently not met, my strategy will continue to default to **Defect (D)**. The conditional cooperation rule will simply never trigger.

*   **"Near Miss" Scenario (More than $m-2$ other players cooperated in the last round):**
    *   Let's refine the "conditional cooperation" trigger for more aggression without being reckless.
    *   **Cooperate IF:** In the immediately preceding round ($t-1$), the number of *other* players who cooperated was $\geq m-1$.
    *   **Slightly More Aggressive Cooperation IF:** In the immediately preceding round ($t-1$), the number of *other* players who cooperated was $= m-2$, AND there is a general upward trend in cooperation (e.g., the average number of other cooperators across the last 3 rounds has increased). *This is a riskier move and should be applied sparingly, only if I feel the game is starting to stabilize towards cooperation.*

---

### 3. Aggressiveness Alignment

*   **Default to Defection:** My strategy is inherently aggressive because it defaults to defection, securing a baseline payoff and minimizing risk. Cooperation is a rare exception, not the norm.
*   **High Bar for Cooperation:** I only cooperate when there's strong evidence that the collective action will succeed ($m-1$ other cooperators in the previous round). This means I'm not trying to "build" cooperation; I'm trying to *ride the wave* of established cooperation if it emerges.
*   **No "Sacrificial" Cooperation:** I will never cooperate if I believe the threshold will not be met. My cooperation is purely transactional – it must lead to a guaranteed higher payoff for me in that round.
*   **Exploiting Others' Cooperation:** If other players manage to achieve cooperation without my help, I will defect and still gain the reward $k$. This is an aggressive exploitation of their efforts. My strategy is designed to benefit maximally from situations where cooperation succeeds, regardless of my role in initiating it.
*   **Last Round Ruthlessness:** My absolute defection in the last round highlights a purely self-interested, aggressive approach that prioritizes immediate individual gain over any long-term relationship building (which doesn't exist in this single-game tournament context).

---

### Pseudocode Representation

```pseudocode
Strategy "Opportunistic Dominator":

Parameters:
  n: number of players
  m: minimum cooperators needed
  k: reward factor
  r: number of rounds

State Variables (per player):
  history = list of actions played by all players in previous rounds
  my_actions = list of my actions in previous rounds
  round_num = current round number

Decision Logic (for player i in round_num):

  // Handle Edge Case: First Round
  if round_num == 1:
    return D

  // Handle Edge Case: Last Round
  if round_num == r:
    return D

  // Calculate number of other players who cooperated in the previous round
  cooperators_in_prev_round = 0
  if r > 1: // Ensure there is a previous round
    prev_round_actions = history[round_num - 1] // Assuming history is a list of rounds, each round is a list of actions for all players
    for player_j_idx from 1 to n:
      if player_j_idx != i and prev_round_actions[player_j_idx] == C:
        cooperators_in_prev_round = cooperators_in_prev_round + 1

  // Decision Rule: Conditional Cooperation
  // Cooperate ONLY IF at least m-1 other players cooperated in the previous round
  if cooperators_in_prev_round >= m - 1:
    return C // Aggressive conditional cooperation
  else:
    return D // Default to defecting

```

**Note on Pseudocode:** The `history` variable would need to be managed to store the actions of all players in previous rounds. The pseudocode assumes this data is available. The calculation of `cooperators_in_prev_round` needs access to the full history of actions for all players in the preceding round.

This strategy prioritizes self-gain, is highly risk-averse regarding cooperation, and aims to exploit any emergent cooperation from other players. It is designed to be robust by relying on direct observation of past success rather than complex inferences about opponent intentions.
'''

description_AGGRESSIVE_128 = '''
This is an interesting challenge! Designing an aggressive strategy in a collective risk dilemma requires a delicate balance of self-interest and the potential for collective reward. Since communication is forbidden, our strategy must be entirely reactive and forward-looking, based on the observable actions of others.

Here's an aggressive strategy that prioritizes maximizing personal gain while leveraging the collective reward when it's strategically advantageous.

## Aggressive Strategy: "Calculated Opportunism"

The core idea behind "Calculated Opportunism" is to be a conditional cooperator, but with a strong bias towards defecting unless there is a clear and immediate benefit to cooperating. We will be aggressive in defecting, but also opportunistic when the conditions strongly favor a cooperative outcome that benefits us directly.

### 1. Decision Rules: When to Cooperate vs. Defect

Our decision in any given round `t` will be based on the following logic:

*   **Default: Defect.** We start by assuming we will defect. This is our baseline aggressive posture.
*   **Cooperate IF:**
    *   **Threshold is within reach AND we are likely to be the difference maker:**
        *   **Condition A: The number of players cooperating in the *previous* round (`C_{t-1}`) plus our potential cooperation (`+1`) is greater than or equal to `m`.**
        *   **Condition B: The number of players who *previously defected* (`D_{t-1}`) is less than `n - m`.** This means that if we defect, there might still be enough defectors to prevent the threshold from being met, leaving us with a private gain. This is our "aggressive" fallback within a cooperative context.
        *   **Condition C: The number of players who *previously cooperated* (`C_{t-1}`) is greater than or equal to `m - 1`.** This is the most crucial indicator that cooperation is *about* to succeed.
        *   **Condition D: We are sufficiently confident that enough *other* players will also cooperate to reach the threshold.** This is the trickiest part and will be estimated by looking at the trend of cooperation. If cooperation has been increasing, or if a large block of players has been consistently cooperating, our confidence increases.

    *   **Specifically:** We will cooperate in round `t` if:
        *   `(C_{t-1} + 1) >= m`  *(enough players cooperated previously such that our cooperation would meet the threshold)*
        *   **AND** `(D_{t-1} < n - m)` *(if we defect, it's still possible for the threshold NOT to be met, preserving our private gain of 1)*
        *   **AND** `(C_{t-1} >= m - 1)` *(a strong signal that cooperation is imminent)*
        *   **AND** `(Our Confidence in Others Cooperating to reach m is High)`

*   **If any of the above cooperation conditions are NOT met, we defect.**

### 2. Handling Edge Cases

*   **Round 1 (t=1):**
    *   **Decision:** **Defect.**
    *   **Rationale:** In the first round, there is no history to rely on. We have no information about other players' tendencies. To be aggressive and maximize our initial private gain, we defect. This also serves as an information-gathering move.

*   **Last Round (t=r):**
    *   **Decision:** **Defect.**
    *   **Rationale:** In the final round, there are no future consequences for our actions. The game is over. Therefore, we should always defect to maximize our payoff for this final round, as there's no opportunity to "punish" defectors or "reward" cooperators in subsequent rounds.

*   **"Almost There" Scenarios (Intermediate Rounds):**
    *   If `C_{t-1} == m - 1`: This is a strong trigger to consider cooperating. If we cooperate, the threshold is met. If we defect, it might not be. Our decision here is critical. We will weigh the probability of others cooperating against the certain gain of `1` if the threshold isn't met.
    *   If `C_{t-1} > m - 1` but `< m`: We are very close. If enough players defected in the previous round such that defecting *now* would push us below `m`, we might reconsider our aggressive stance and cooperate to secure the reward `k`.

### 3. Being Aggressive: The Mindset

Our aggression is demonstrated in several ways:

*   **Default Defection:** We are not a naive cooperator. We *start* by defecting, forcing others to signal their intentions.
*   **Opportunistic Cooperation:** We don't cooperate out of altruism or for long-term relationship building (as there is no communication for that). We cooperate *only* when it directly benefits us by securing the collective reward `k`, and we are reasonably sure that our cooperation will be the tipping point, or that enough others will cooperate anyway.
*   **Maximizing Private Gain:** The condition `(D_{t-1} < n - m)` when considering cooperation is key. It means we are only willing to cooperate if there's still a chance that defecting would have resulted in a private gain of 1. If defecting would *guarantee* the threshold is not met (i.e., `D_{t-1} >= n - m`), then we have less incentive to cooperate for the reward `k` and are more inclined to defect for the guaranteed private gain.
*   **Strategic Defection to "Punish":** While we can't directly punish, our defection in situations where cooperation is expected can act as a decentralized punishment. If enough players defect, the collective reward is lost, and those who cooperated might regret their decision.

### Pseudocode Representation:

```pseudocode
FUNCTION DecideAction(player_id, current_round, history):
  // history is a list of previous rounds, where each round contains a list of actions (C or D) for all players.
  // history[t-1] = list of actions in the previous round.
  // C_prev = number of players who played 'C' in the previous round.
  // D_prev = number of players who played 'D' in the previous round.

  IF current_round == 1:
    RETURN 'D' // Aggressive start, gather info

  IF current_round == r:
    RETURN 'D' // Maximize final round payoff

  // Analyze previous round
  IF history is empty or history[current_round - 2] is empty: // Handle cases where history might not be fully populated yet (though unlikely with standard game logic)
    RETURN 'D'

  actions_prev_round = history[current_round - 2] // Access actions from the *actual* previous round played
  C_prev = count('C' in actions_prev_round)
  D_prev = count('D' in actions_prev_round)

  // --- Cooperation Conditions ---
  // Condition A: Threshold met if we cooperate
  would_meet_threshold_if_i_cooperate = (C_prev + 1) >= m

  // Condition B: Private gain of 1 is still possible if we defect
  private_gain_possible_if_i_defect = (D_prev < n - m)

  // Condition C: Strong signal of imminent cooperation
  strong_cooperation_signal = (C_prev >= m - 1)

  // Condition D: Confidence in others cooperating (This is a heuristic and needs refinement in implementation)
  // A simple heuristic: If cooperation has been trending upwards, or if a significant number of players have consistently cooperated.
  // For this description, we'll simplify it: assume we have a way to measure this confidence.
  // A very basic confidence measure: Has cooperation increased over the last few rounds?
  cooperation_trend = 0
  IF current_round > 3:
    actions_round_minus_2 = history[current_round - 3] // actions from 2 rounds ago
    C_prev_prev = count('C' in actions_round_minus_2)
    IF C_prev > C_prev_prev:
      cooperation_trend = 1
    ELSE IF C_prev < C_prev_prev:
      cooperation_trend = -1

  // Refined confidence: Consider players who *consistently* cooperate.
  // Let's simplify for this description: assume `confidence_in_others_cooperating` is a computed value > 0 (high) or <= 0 (low).
  // In a real implementation, this would involve analyzing individual player histories.
  confidence_in_others_cooperating = CalculateConfidenceInOthers(history, current_round) // Placeholder function

  // --- Decision Logic ---
  IF would_meet_threshold_if_i_cooperate AND private_gain_possible_if_i_defect AND strong_cooperation_signal AND confidence_in_others_cooperating > 0:
    RETURN 'C' // Opportunistically cooperate

  ELSE:
    RETURN 'D' // Default aggressive action

FUNCTION CalculateConfidenceInOthers(history, current_round):
  // This is a placeholder. A sophisticated implementation would analyze:
  // 1. The trend of cooperation over the last few rounds.
  // 2. The number of players who have consistently cooperated (e.g., cooperated in >= 50% of previous rounds).
  // 3. The number of players who have cooperated in the *most recent* few rounds.
  // For simplicity, let's assume a threshold: if C_prev >= (m-1) AND cooperation_trend is positive OR C_prev >= m, confidence is high (return 1). Otherwise, low (return 0).

  // Accessing actions from the *actual* previous round played
  actions_prev_round = history[current_round - 2]
  C_prev = count('C' in actions_prev_round)

  cooperation_trend = 0
  IF current_round > 3:
    actions_round_minus_2 = history[current_round - 3]
    C_prev_prev = count('C' in actions_round_minus_2)
    IF C_prev > C_prev_prev:
      cooperation_trend = 1

  IF (C_prev >= m - 1 AND cooperation_trend > 0) OR C_prev >= m:
      RETURN 1 // High confidence
  ELSE:
      RETURN 0 // Low confidence

```

This strategy is designed to be aggressive by defaulting to defection and only cooperating when the conditions are strongly in our favor to secure the reward `k`. It is adaptive because it reacts to the observed behavior of other players. It is robust because its core logic doesn't rely on specific, predictable opponent strategies, but rather on the aggregate behavior that signals the likelihood of achieving the collective goal.
'''
