"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: OpenAI
- Model: gpt-5-mini
- Game: CollectiveRiskDescription
"""

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState
import numpy as np
from numpy.typing import NDArray
import math
import random



class Strategy_COOPERATIVE_1(BaseStrategy):
    """
    Adaptive Threshold Conditional Cooperator (ATCC)

    - Cooperates in the first (non-final) round to signal intent.
    - Always defects in the final round.
    - Estimates probability that exactly m-1 other players will cooperate using a short history window.
    - Cooperates when k * P[X = m-1] >= 1 (pivotal condition).
    - If recent group success rate is high and this agent cooperated in the last observed round, sustain cooperation.
    - If recent group success rate is low, start/refresh a short punishment (defect for L rounds), but occasionally probe (with probability eps).
    - When not punishing and proposed_action == D, occasionally probe (with probability eps) by cooperating.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_counter = 0
        r = int(self.game_description.n_rounds)
        self.w = max(1, min(10, math.floor(math.sqrt(r))))
        self.S_high = 0.75
        self.S_low = 0.4
        self.L = min(3, max(1, math.floor(r / 10)))
        self.eps = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if t >= r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        available_rounds = int(history.round_number)
        H_size = min(self.w, available_rounds)
        if H_size <= 0:
            return Action.C
        opp_actions = history.opponent_actions[-H_size:, :]
        my_actions = history.my_actions[-H_size:]
        N = n - 1
        if N <= 0:
            return Action.D
        p_js = np.mean(opp_actions.astype(float), axis=0)
        p_bar = float(np.mean(p_js)) if p_js.size > 0 else float(m) / float(n)
        k_eq = m - 1
        if k_eq < 0 or k_eq > N:
            P_eq = 0.0
        else:
            comb = math.comb(N, k_eq)
            P_eq = comb * p_bar ** k_eq * (1.0 - p_bar) ** (N - k_eq)
        proposed_action = Action.C if k * P_eq >= 1.0 else Action.D
        opp_coop_counts = np.sum(opp_actions.astype(int), axis=1)
        my_coop_ints = my_actions.astype(int)
        total_cooperators = opp_coop_counts + my_coop_ints
        success_rounds = int(np.sum(total_cooperators >= m))
        S = success_rounds / max(1, H_size)
        if S >= self.S_high and history.my_actions[-1]:
            return Action.C
        if S <= self.S_low:
            self.punish_counter = self.L
        if self.punish_counter > 0:
            action = Action.D
            if random.random() < self.eps:
                action = Action.C
            self.punish_counter = max(0, self.punish_counter - 1)
            return action
        if proposed_action == Action.D:
            if random.random() < self.eps:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_2(BaseStrategy):
    """
    Pivot-Aware Conditional Cooperator (PACC)

    Cooperates when the agent is plausibly pivotal: it estimates per-opponent
    cooperation probabilities via a Laplace/Beta(1,1) prior and computes the
    probability that exactly m-1 opponents cooperate. It cooperates when
    k * Pr(exactly m-1) >= 1, with small early exploration/leadership and
    randomized behavior near the decision boundary to aid coordination and
    avoid deterministic exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prior_a = 1.0
        self.prior_b = 1.0
        self.early_rounds = 2
        self.leadership_margin = 0.1
        self.randomize_delta = 0.05
        self.small_explore_prob = 0.05
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = getattr(state, 'round_number', None)
        if t is None:
            t = history.round_number + 1
        T = history.round_number
        opp_actions = history.opponent_actions
        if opp_actions is None or opp_actions.size == 0:
            n_opponents = max(0, self.n - 1)
            S = [0] * n_opponents
        else:
            S_arr = np.sum(opp_actions.astype(float), axis=0)
            S = [float(x) for x in S_arr]
            n_opponents = opp_actions.shape[1]
        if n_opponents != self.n - 1:
            n_opponents = int(n_opponents)
        denom = float(T + self.prior_a + self.prior_b)
        if denom <= 0:
            denom = float(self.prior_a + self.prior_b)
        p_js = []
        for j in range(n_opponents):
            sj = float(S[j]) if j < len(S) else 0.0
            p = (sj + self.prior_a) / denom
            if p < 0.0:
                p = 0.0
            elif p > 1.0:
                p = 1.0
            p_js.append(p)
        pmf = [1.0]
        for p in p_js:
            next_pmf = [0.0] * (len(pmf) + 1)
            for i, prob in enumerate(pmf):
                next_pmf[i] += prob * (1.0 - p)
                next_pmf[i + 1] += prob * p
            pmf = next_pmf
        pivotal_index = self.m - 1
        Pr_pivotal = 0.0
        if 0 <= pivotal_index < len(pmf):
            Pr_pivotal = pmf[pivotal_index]
        else:
            Pr_pivotal = 0.0
        score = self.k * Pr_pivotal
        delta = float(self.randomize_delta)
        mu = float(self.leadership_margin)
        eps = float(self.small_explore_prob)
        if t >= self.r:
            if score >= 1.0:
                return Action.C
            else:
                return Action.D
        if score >= 1.0 + delta:
            action = Action.C
        elif score <= 1.0 - delta:
            action = Action.D
        else:
            lower = 1.0 - delta
            upper = 1.0 + delta
            if upper - lower <= 0:
                prob_C = 0.0
            else:
                prob_C = (score - lower) / (upper - lower)
            if prob_C < 0.0:
                prob_C = 0.0
            elif prob_C > 1.0:
                prob_C = 1.0
            action = Action.C if random.random() < prob_C else Action.D
        if t <= self.early_rounds and score < 1.0 and (1.0 - score <= mu):
            if random.random() < eps:
                return Action.C
        return action

class Strategy_COOPERATIVE_3(BaseStrategy):
    """
    Empirical Pivotal Cooperator (EPC)

    - Uses a sliding window of recent rounds to estimate the empirical probability that
      the agent is pivotal (i.e., the number of other cooperators equals m-1).
    - Cooperates when the empirical pivotal probability >= 1/k.
    - Includes mechanisms to detect persistent defectors (stop cooperating), to share
      burden when others routinely meet the threshold, to probe when others are close,
      and to allow small exploratory/forgiving moves that decay over time.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.window_cap = 20
        self.tau = 0.15
        self.eps_initial = 0.05
        self.eps_min = 0.01
        self.share_freq_threshold = 0.6
        self.probe_cap = 0.5
        self.start_coop_prob = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or getattr(history, 'round_number', 0) == 0:
            return Action.C if random.random() < self.start_coop_prob else Action.D
        t = getattr(state, 'round_number', history.round_number + 1)
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        r = int(self.game_description.n_rounds)
        completed = history.round_number
        W = min(self.window_cap, completed)
        if W <= 0:
            return Action.C if random.random() < self.start_coop_prob else Action.D
        recent_op = history.opponent_actions[-W:, :].astype(float)
        others_coops = recent_op.sum(axis=1)
        freq_mminus1 = float(np.mean(others_coops == m - 1))
        freq_ge_m = float(np.mean(others_coops >= m))
        avg_others = float(np.mean(others_coops))
        player_coop_rate = np.mean(recent_op, axis=0) if recent_op.size > 0 else np.zeros((max(0, n - 1),))
        persistent_defectors = int(np.sum(player_coop_rate < self.tau))
        if persistent_defectors >= n - m + 1:
            return Action.D
        if freq_mminus1 >= 1.0 / k:
            return Action.C
        if freq_ge_m >= self.share_freq_threshold:
            p_share = min(0.9, float(m) / max(1.0, avg_others + 1.0))
            return Action.C if random.random() < p_share else Action.D
        if avg_others >= m - 2:
            raw = 0.5 * (avg_others - (m - 2) + 0.1)
            p_probe = min(self.probe_cap, max(0.0, raw))
            early_boost = 1.0
            if t <= max(3, int(0.2 * max(2, r))):
                early_boost = 1.25
            p_probe = min(1.0, p_probe * early_boost)
            return Action.C if random.random() < p_probe else Action.D
        if r <= 1:
            eps = self.eps_min
        else:
            frac = (t - 1) / max(1, r - 1)
            eps = max(self.eps_min, self.eps_initial * (1.0 - frac))
        return Action.C if random.random() < eps else Action.D

class Strategy_COOPERATIVE_4(BaseStrategy):
    """
    Adaptive Pivotality with Rotational Tie-Break (APRT)

    - Maintains per-opponent cooperation estimates (EWMA).
    - Computes pivotal probabilities via exact Poisson-Binomial (small n) or normal approx.
    - Cooperates if doing so meaningfully improves expected payoff; defects if free-riding is clearly better.
    - On marginal decisions uses a deterministic rotation of volunteers to break symmetry.
    - Applies mild temporary punishment (increased EWMA weight) to opponents who were expected to volunteer but defected.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.p0 = 0.6
        self.alpha = 0.3
        self.epsilon = 0.01
        self.W = min(20, self.r) if self.r is not None else 20
        self.penalty_window = 3
        self.P = math.ceil(self.n / max(1, self.m))
        self.n_opponents = max(0, self.n - 1)
        self._base_p_estimates = np.full(self.n_opponents, self.p0, dtype=float)
        self._penalty_window = int(self.penalty_window)
        self.player_index = id(self) % self.n + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            p_est = np.array(self._base_p_estimates, dtype=float)
            penalty_counters = np.zeros(self.n_opponents, dtype=int)
        else:
            p_est = np.full(self.n_opponents, self.p0, dtype=float)
            penalty_counters = np.zeros(self.n_opponents, dtype=int)
            rounds_completed = history.round_number
            global_indices = [idx for idx in range(1, self.n + 1) if idx != self.player_index]
            for round_idx in range(rounds_completed):
                for j in range(self.n_opponents):
                    if penalty_counters[j] > 0:
                        penalty_counters[j] -= 1
                try:
                    row = history.opponent_actions[round_idx, :]
                except Exception:
                    row = np.zeros(self.n_opponents, dtype=bool)
                start = round_idx * self.m % self.n
                volunteers = set(((start + s - 1) % self.n + 1 for s in range(1, self.m + 1)))
                for j in range(self.n_opponents):
                    observed = bool(row[j])
                    alpha_j = self.alpha * (2.0 if penalty_counters[j] > 0 else 1.0)
                    if alpha_j > 1.0:
                        alpha_j = 1.0
                    p_est[j] = (1.0 - alpha_j) * p_est[j] + alpha_j * (1.0 if observed else 0.0)
                for j in range(self.n_opponents):
                    global_idx = global_indices[j]
                    was_volunteer = global_idx in volunteers
                    acted_cooperate = bool(row[j])
                    if was_volunteer and (not acted_cooperate):
                        penalty_counters[j] = self._penalty_window

        def prob_others_ge(k_threshold: int):
            if k_threshold <= 0:
                return 1.0
            if k_threshold > self.n_opponents:
                return 0.0
            if self.n_opponents <= 30:
                probs = np.zeros(self.n_opponents + 1, dtype=float)
                probs[0] = 1.0
                for p in p_est:
                    q = 1.0 - p
                    new = np.empty_like(probs)
                    new[0] = probs[0] * q
                    for j in range(1, self.n_opponents + 1):
                        new[j] = probs[j] * q + probs[j - 1] * p
                    probs = new
                return float(np.sum(probs[k_threshold:]))
            else:
                E = float(np.sum(p_est))
                V = float(np.sum(p_est * (1.0 - p_est)))
                if V <= 0.0:
                    return 1.0 if E + 1e-12 >= k_threshold else 0.0
                z = (k_threshold - 0.5 - E) / math.sqrt(V)
                phi = 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))
                return max(0.0, min(1.0, 1.0 - phi))
        prob_others_ge_m = prob_others_ge(self.m)
        prob_others_ge_m_minus1 = prob_others_ge(max(0, self.m - 1))
        EU_C = self.k * prob_others_ge_m_minus1
        EU_D = 1.0 + self.k * prob_others_ge_m
        if EU_C > EU_D + self.epsilon:
            return Action.C
        if EU_D > EU_C + self.epsilon:
            return Action.D
        start = (t - 1) * self.m % self.n
        volunteers = set(((start + s - 1) % self.n + 1 for s in range(1, self.m + 1)))
        if self.player_index in volunteers:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_5(BaseStrategy):
    """
    Reciprocal Conditional Cooperator with Bootstrap and Forgiveness (RCC-BF).

    - Cooperate for a short bootstrap (E rounds).
    - Always defect in final round.
    - If the previous round reached the threshold (m cooperators), reward by cooperating.
    - If the previous round failed, inspect a short sliding window (w rounds) of opponents'
      behavior: count recent cooperations and detect if they defected while the group succeeded.
      Attempt to cooperate only when there are at least m-1 reliable non-exploiters; otherwise
      defect (punish). Exploit flags naturally decay with the sliding window (forgiveness).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.E = min(2, max(0, self.r - 1))
        self.w = min(3, max(0, self.r - 1))
        self.s = math.ceil(self.w / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == self.r:
            return Action.D
        if t <= self.E:
            return Action.C
        if history is None:
            return Action.D
        if history.round_number >= 1:
            last_idx = history.round_number - 1
            try:
                opp_last = history.opponent_actions[last_idx, :]
            except Exception:
                opp_last = np.array([], dtype=bool)
            my_last = bool(history.my_actions[last_idx]) if history.my_actions.size > 0 else False
            C_prev = int(np.sum(opp_last)) + (1 if my_last else 0)
        else:
            C_prev = 0
        if C_prev >= self.m:
            return Action.C
        recent = min(self.w, history.round_number)
        if recent <= 0:
            return Action.D
        opp_recent = history.opponent_actions[-recent:, :]
        my_recent = history.my_actions[-recent:]
        try:
            opp_sums = np.sum(opp_recent, axis=1)
        except Exception:
            opp_sums = np.zeros(recent, dtype=int)
        my_ints = my_recent.astype(int) if my_recent.size > 0 else np.zeros(recent, dtype=int)
        total_coops = opp_sums + my_ints
        group_succeeded = total_coops >= self.m
        reliable_non_exploiters = 0
        if opp_recent.ndim == 1:
            opp_recent = opp_recent.reshape(recent, 1)
        n_opponents = 0 if opp_recent.size == 0 else opp_recent.shape[1]
        for j in range(n_opponents):
            col = opp_recent[:, j].astype(bool)
            coop_j = int(np.sum(col))
            exploiter_count = int(np.sum(~col & group_succeeded))
            if coop_j >= self.s and exploiter_count == 0:
                reliable_non_exploiters += 1
        if reliable_non_exploiters >= max(0, self.m - 1):
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_6(BaseStrategy):
    """
    Adaptive Pivotality Cooperator (APC)

    - Estimates each opponent's cooperation probability with a small Bayesian prior over a recent window.
    - Computes the probability that exactly m-1 of the other players cooperate (p_pivotal).
    - Cooperates iff k * p_pivotal >= 1 (tie-break toward cooperation).
    - Starts by cooperating. Applies short, targeted punishments (temporary downward adjustment to p_j)
      to opponents who defected when this strategy cooperated and the group failed to reach the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.opp_count = max(0, self.n - 1)
        self.punish_remaining = np.zeros(self.opp_count, dtype=int)
        self.last_processed = 0
        self.alpha = 1.0
        self.W_max = 5
        self.delta_penalty = 0.2
        self.T_pun = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if self.opp_count != max(0, int(self.game_description.n_players) - 1):
            self.n = int(self.game_description.n_players)
            self.opp_count = max(0, self.n - 1)
            self.punish_remaining = np.zeros(self.opp_count, dtype=int)
        completed = history.round_number
        newly_completed = max(0, completed - self.last_processed)
        if newly_completed > 0:
            if self.opp_count > 0:
                self.punish_remaining = np.maximum(0, self.punish_remaining - newly_completed)
            for s in range(self.last_processed, completed):
                if history.my_actions[s]:
                    opp_coops = int(np.sum(history.opponent_actions[s, :])) if self.opp_count > 0 else 0
                    total_coops = 1 + opp_coops
                    if total_coops < self.game_description.m and self.opp_count > 0:
                        defectors = np.nonzero(~history.opponent_actions[s, :])[0]
                        if defectors.size > 0:
                            for j in defectors.tolist():
                                self.punish_remaining[j] = max(self.punish_remaining[j], self.T_pun)
            self.last_processed = completed
        t = completed + 1
        if t <= 2:
            W = max(0, t - 1)
        else:
            W = min(self.W_max, t - 1)
        prior_p = float(self.game_description.m) / float(max(1, self.game_description.n_players))
        alpha = float(self.alpha)
        if self.opp_count > 0 and W > 0:
            start_idx = max(0, t - W - 1)
            end_idx = t - 2
            recent = history.opponent_actions[start_idx:end_idx + 1, :] if end_idx >= start_idx else np.zeros((0, self.opp_count), dtype=bool)
            recent_coops = np.sum(recent, axis=0).astype(float)
        else:
            recent_coops = np.zeros(self.opp_count, dtype=float)
        p_js = np.zeros(self.opp_count, dtype=float)
        denom = alpha + float(W)
        for j in range(self.opp_count):
            p = (alpha * prior_p + (recent_coops[j] if self.opp_count > 0 else 0.0)) / denom if denom > 0 else prior_p
            if self.punish_remaining[j] > 0:
                p = max(0.0, p - self.delta_penalty)
            p_js[j] = min(1.0, max(0.0, float(p)))
        q = [1.0] + [0.0] * self.opp_count
        for j in range(self.opp_count):
            pj = float(p_js[j])
            one_minus = 1.0 - pj
            new_q = [0.0] * (len(q) + 1)
            for x in range(len(q)):
                if q[x] == 0.0:
                    continue
                new_q[x] += q[x] * one_minus
                new_q[x + 1] += q[x] * pj
            q = new_q
        m_minus_1 = self.game_description.m - 1
        if 0 <= m_minus_1 < len(q):
            p_pivotal = float(q[m_minus_1])
        else:
            p_pivotal = 0.0
        k = float(self.game_description.k)
        if k * p_pivotal >= 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_7(BaseStrategy):
    """
    EV-pivotal cooperative strategy with per-opponent Bayesian estimates,
    targeted short punishments for free-riders, and forgiving decay.
    Decides to Cooperate iff k * (P_with - P_without) >= 1 using Laplace-smoothed
    per-opponent cooperation probabilities computed from a recent rolling window.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_scores = np.zeros(max(0, self.game_description.n_players - 1), dtype=int)
        self.last_processed_round = 0
        self.punish_rounds_remaining = 0
        self.W_max = 25
        self.alpha = 1
        self.p0 = max(0.5, self.game_description.m / self.game_description.n_players)
        self.P_thresh = 2
        self.gamma = 0.3
        self.T_punish = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if history is None or history.round_number == 0:
            if len(self.punishment_scores) != max(0, n - 1):
                self.punishment_scores = np.zeros(max(0, n - 1), dtype=int)
            num_opponents = n - 1
            q_list = [self.p0] * num_opponents
            P = np.zeros(num_opponents + 1, dtype=float)
            P[0] = 1.0
            for q in q_list:
                for t in range(num_opponents - 1, -1, -1):
                    P[t + 1] += P[t] * q
                    P[t] *= 1.0 - q
            P_without = float(P[m:].sum()) if m <= num_opponents else 0.0
            P_with = 1.0 if m - 1 < 0 else float(P[max(0, m - 1):].sum())
            marginal = P_with - P_without
            if k * marginal >= 1.0:
                return Action.C
            else:
                return Action.D
        num_opponents = n - 1
        if len(self.punishment_scores) != num_opponents:
            self.punishment_scores = np.zeros(num_opponents, dtype=int)
        rounds_so_far = history.round_number
        W = min(self.W_max, rounds_so_far)
        P_decay = max(1, int(max(1, math.floor(W / 2))) if W > 0 else 1)
        if self.last_processed_round < rounds_so_far:
            for t in range(self.last_processed_round, rounds_so_far):
                my_act_t = bool(history.my_actions[t])
                opps_row = history.opponent_actions[t, :]
                total_coops = int(np.sum(opps_row.astype(int))) + (1 if my_act_t else 0)
                if total_coops >= m and my_act_t:
                    for j in range(num_opponents):
                        if not bool(opps_row[j]):
                            self.punishment_scores[j] += 1
                if (t + 1) % P_decay == 0:
                    self.punishment_scores = np.maximum(0, self.punishment_scores - 1)
            self.last_processed_round = rounds_so_far
        if self.punish_rounds_remaining > 0:
            self.punish_rounds_remaining -= 1
            return Action.D
        q_list = []
        for j in range(num_opponents):
            if rounds_so_far == 0:
                q_j = self.p0
            else:
                window_len = min(W, rounds_so_far)
                if window_len <= 0:
                    q_j = self.p0
                else:
                    recent = history.opponent_actions[-window_len:, j]
                    count_C = int(np.sum(recent.astype(int)))
                    q_j = (1 + count_C) / (2 + window_len)
            if self.punishment_scores[j] >= self.P_thresh:
                q_j = q_j * self.gamma
            q_j = max(0.0, min(1.0, float(q_j)))
            q_list.append(q_j)
        P = np.zeros(num_opponents + 1, dtype=float)
        P[0] = 1.0
        for q in q_list:
            for t in range(num_opponents - 1, -1, -1):
                P[t + 1] += P[t] * q
                P[t] *= 1.0 - q
        if m <= num_opponents:
            P_without = float(P[m:].sum())
        else:
            P_without = 0.0
        if m - 1 < 0:
            P_with = 1.0
        else:
            start = max(0, m - 1)
            P_with = float(P[start:].sum())
        marginal = P_with - P_without
        cooperate_ev = k * marginal >= 1.0
        total_expected = float(sum(q_list))
        expected_nonflag = float(sum((q_list[j] for j in range(num_opponents) if self.punishment_scores[j] < self.P_thresh)))
        flagged_exist = np.any(self.punishment_scores >= self.P_thresh) if num_opponents > 0 else False
        critical_flagged = flagged_exist and expected_nonflag < m - 0.5
        if cooperate_ev and critical_flagged:
            self.punish_rounds_remaining = max(self.punish_rounds_remaining, self.T_punish)
            return Action.D
        return Action.C if cooperate_ev else Action.D

class Strategy_COOPERATIVE_8(BaseStrategy):
    """
    Adaptive threshold-pivotal strategy for the Collective Risk Dilemma.

    - Maintains per-opponent cooperation probabilities (exponential smoothing).
    - Uses Poisson-Binomial exact DP (small n) or normal approximation (large n)
      to compute P(number of other cooperators >= m) and P(number = m-1).
    - Chooses the action with the higher estimated expected payoff,
      with a cooperative tie-break (and a tiny, deterministic, history-seeded
      random perturbation for symmetry-breaking).
    - Forgiving updates via exponential smoothing; can increase smoothing (alpha)
      temporarily when detecting possible exploitation.
    - Always defects in the final S_end rounds (safety zone).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(self.game.n_players)
        self.r = int(self.game.n_rounds)
        self.m = int(self.game.m)
        self.k = float(self.game.k)
        self.W = min(10, max(1, self.r - 1))
        p0_raw = float(self.m) / max(1.0, float(self.n))
        self.p0 = max(0.1, min(0.9, p0_raw))
        self.epsilon = 0.01
        self.alpha = 0.3
        self.S_end = 1
        self.L = self.n - 1
        if self.L > 0:
            self.p_probs = np.full(self.L, self.p0, dtype=float)
        else:
            self.p_probs = np.array([], dtype=float)
        self._processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t > self.r - self.S_end:
            if history is not None:
                self._process_new_history(history)
            return Action.D
        if self.L <= 0:
            if history is not None:
                self._process_new_history(history)
            return Action.D
        if history is not None:
            self._process_new_history(history)
        p_vec = self.p_probs
        mu = float(np.sum(p_vec))
        sigma2 = float(np.sum(p_vec * (1.0 - p_vec)))
        sigma = math.sqrt(max(0.0, sigma2))
        P_m = 0.0
        P_m1 = 0.0
        if self.m <= 0:
            P_m = 1.0
            P_m1 = 0.0
        elif self.L <= 40:
            dp = np.zeros(self.L + 1, dtype=float)
            dp[0] = 1.0
            for p in p_vec:
                prev = dp.copy()
                dp *= 1.0 - p
                dp[1:] += prev[:-1] * p
            if self.m - 1 >= 0 and self.m - 1 <= self.L:
                P_m1 = float(dp[self.m - 1])
            else:
                P_m1 = 0.0
            if self.m <= 0:
                P_m = 1.0
            elif self.m > self.L:
                P_m = 0.0
            else:
                P_m = float(np.sum(dp[self.m:]))
        elif sigma <= 1e-12:
            xi = int(round(mu))
            P_m = 1.0 if xi >= self.m else 0.0
            P_m1 = 1.0 if xi == self.m - 1 else 0.0
        else:

            def Phi(x):
                return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
            z_m = (float(self.m) - 0.5 - mu) / sigma
            P_m = 1.0 - Phi(z_m)
            z1 = (float(self.m) - 0.5 - mu) / sigma
            z0 = (float(self.m) - 1.5 - mu) / sigma
            P_m1 = max(0.0, Phi(z1) - Phi(z0))
        E_C = self.k * P_m1
        E_D = 1.0 + self.k * P_m
        perturb = 0.0
        try:
            seed_int = self._history_derived_seed(history, t)
            rng = random.Random(seed_int)
            perturb = (rng.random() - 0.5) * 1e-09
        except Exception:
            perturb = 0.0
        E_C += perturb
        E_D -= perturb
        if E_C > E_D + self.epsilon:
            choice = Action.C
        elif E_D > E_C + self.epsilon:
            choice = Action.D
        else:
            choice = Action.C
        return choice

    def _process_new_history(self, history: PlayerHistory):
        """
        Process any new rounds from history not yet incorporated into p_probs.
        Uses exponential smoothing updates per opponent for each newly observed round.
        Also applies simple exploitation detection to temporarily increase alpha.
        """
        rounds_available = int(history.round_number)
        if rounds_available <= self._processed_rounds:
            return
        recent = min(self.W, rounds_available)
        if recent > 0:
            start_idx = rounds_available - recent
            my_recent = history.my_actions[start_idx:rounds_available]
            opps_recent = history.opponent_actions[start_idx:rounds_available, :]
            total_cooperators = np.sum(opps_recent, axis=1) + my_recent.astype(int)
            wasted = 0
            for i in range(len(my_recent)):
                if my_recent[i] and int(total_cooperators[i]) < self.m:
                    wasted += 1
            exploitation_flag = wasted >= recent / 2.0
        else:
            exploitation_flag = False
        alpha_eff = min(1.0, self.alpha * 2.0) if exploitation_flag else self.alpha
        for round_idx in range(self._processed_rounds, rounds_available):
            row = history.opponent_actions[round_idx, :]
            if len(row) != self.L:
                if len(row) > self.L:
                    row = row[:self.L]
                else:
                    pad = np.zeros(self.L - len(row), dtype=bool)
                    row = np.concatenate([row, pad])
            cooperated = row.astype(float)
            self.p_probs = (1.0 - alpha_eff) * self.p_probs + alpha_eff * cooperated
        self._processed_rounds = rounds_available
        self.p_probs = np.clip(self.p_probs, 1e-06, 1.0 - 1e-06)

    def _history_derived_seed(self, history: None | PlayerHistory, round_number: int) -> int:
        """
        Produce a deterministic integer seed derived from public inputs:
        - flattened opponent_actions bytes (if available)
        - round_number
        - game parameters
        This avoids touching global random state.
        """
        seed = int(self.n) * 1000003 + int(self.m) * 1009 + int(round_number) * 97 + int(self.r) * 13
        if history is None:
            seed ^= int(self.p0 * 1000000.0)
            return seed
        try:
            arr = history.opponent_actions
            b = arr.tobytes()
            acc = 0
            blen = len(b)
            if blen == 0:
                return seed
            indices = list(range(0, min(32, blen))) + [blen // 2] + [blen - 1]
            for i, idx in enumerate(indices):
                if idx < 0 or idx >= blen:
                    continue
                acc = acc * 257 ^ b[idx]
                acc &= 18446744073709551615
            seed ^= int(acc)
            return seed
        except Exception:
            return seed

class Strategy_COOPERATIVE_9(BaseStrategy):
    """
    Coalition-formation Collective Risk strategy.

    - Tries to reliably produce exactly m cooperators each round by:
      * Sticking to a successful coalition (cooperators keep cooperating; defectors keep defecting).
      * If a round failed, estimating others' cooperativeness (EMA over recent rounds), acting pivotal if needed,
        recruiting top-ranked willing players deterministically by rank, or defecting and occasionally exploring.
      * Short punishments for expected cooperators who defect; quick forgiveness when they cooperate again.
      * Always defects on the final round. Reduces exploration near the end.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.3
        self.window = 10
        self.P = 2
        self.eps = 0.03
        self.T_caution = 1
        n = self.game_description.n_players
        self.n = n
        self.p_est = np.zeros(n, dtype=float)
        self.punish = np.zeros(n, dtype=int)
        self.last_success = False
        self.S_last = set()
        self._initialized = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        m = self.game_description.m
        r = self.game_description.n_rounds
        alpha = self.alpha
        P = self.P
        T_caution = self.T_caution

        def reconstruct_actions(history_obj, upto_exclusive=None):
            """
            Returns an array shape (T, n) of bools with player actions for rounds [0..T-1]
            upto_exclusive: if provided, only include rounds strictly before this index (0-based)
            """
            if history_obj is None:
                return np.zeros((0, n), dtype=bool)
            total_rounds = history_obj.round_number
            if upto_exclusive is None:
                T = total_rounds
            else:
                T = max(0, min(total_rounds, upto_exclusive))
            if T == 0:
                return np.zeros((0, n), dtype=bool)
            opp = history_obj.opponent_actions[:T, :]
            me = history_obj.my_actions[:T]
            all_actions = np.zeros((T, n), dtype=bool)
            if n - 1 > 0:
                all_actions[:, 0:n - 1] = opp
            all_actions[:, n - 1] = me
            return all_actions
        if history is None or history.round_number == 0:
            p_init = min(0.9, m / float(n) + 0.1)
            if random.random() < p_init:
                return Action.C
            else:
                return Action.D
        self.punish = np.maximum(0, self.punish - 1)
        rounds_completed = history.round_number
        all_prev_excl_last = reconstruct_actions(history, upto_exclusive=rounds_completed - 1)
        all_prev_incl_last = reconstruct_actions(history, upto_exclusive=rounds_completed)

        def compute_ema_from_array(actions_array):
            T = actions_array.shape[0]
            p = np.zeros(n, dtype=float)
            start = max(0, T - self.window)
            for t in range(start, T):
                acts = actions_array[t]
                p = (1.0 - alpha) * p + alpha * acts.astype(float)
            return p
        p_prev = compute_ema_from_array(all_prev_excl_last)
        p_curr = compute_ema_from_array(all_prev_incl_last)
        self.p_est = p_curr.copy()
        last_round_actions = all_prev_incl_last[-1]
        coop_count_last = int(np.sum(last_round_actions))
        last_success = coop_count_last >= m
        self.last_success = last_success
        S_last = {int(i) for i, v in enumerate(last_round_actions) if v}
        self.S_last = S_last
        for j in range(n):
            if last_round_actions[j]:
                self.punish[j] = 0
            elif p_prev[j] > 0.5:
                self.punish[j] = P
        t = state.round_number
        self_idx = n - 1
        if t == r:
            return Action.D
        if last_success:
            if self_idx in S_last:
                return Action.C
            else:
                return Action.D
        p_eff = self.p_est.copy()
        punished_mask = self.punish > 0
        p_eff[punished_mask] = p_eff[punished_mask] * 0.1
        others = [j for j in range(n) if j != self_idx]
        E = float(np.sum(p_eff[others]))
        eps = self.eps
        if t >= r - T_caution:
            eps = 0.0
        if E >= m:
            return Action.D
        if E >= m - 1 - 1e-08:
            return Action.C
        needed = m - E
        indices = np.arange(n)
        sort_order = np.lexsort((indices, -p_eff))
        rank_positions = {int(idx): pos + 1 for pos, idx in enumerate(sort_order)}
        rank_i = rank_positions[self_idx]
        needed_int = int(math.ceil(max(0.0, needed)))
        if rank_i <= needed_int:
            return Action.C
        elif random.random() < eps:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_10(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC).

    - Starts by cooperating to signal intent.
    - Attempts to recruit exactly the minimal m cooperators after any failed round,
      breaking symmetry deterministically by a fixed index ordering.
    - Rewards prior cooperators by continuing to cooperate after a successful round.
    - Forgives a single solitary exploiter after a success.
    - Punishes multiple or repeated free-riders for a short window (P rounds).
    - Defects in the final round.
    Notes:
    - For deterministic tie-breaking we assume a fixed player ordering:
      opponents are in the order given by history.opponent_actions columns (1..n-1)
      and this player is assigned the final index n. This yields a consistent,
      deterministic recruitment ordering across rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_until = 0
        self.P = 2 if game_description.n_rounds >= 3 else 1
        self.my_index = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        t = state.round_number
        if history is None:
            if t == r:
                return Action.D
            return Action.C
        last_my_action = bool(history.my_actions[-1])
        last_op_actions = history.opponent_actions[-1]
        S_prev = []
        for idx, acted in enumerate(last_op_actions, start=1):
            if bool(acted):
                S_prev.append(idx)
        if last_my_action:
            S_prev.append(self.my_index)
        C_prev = len(S_prev)
        all_indices = list(range(1, n + 1))
        Def_prev = [idx for idx in all_indices if idx not in S_prev]
        if t == r:
            return Action.D
        if self.punish_until >= t:
            return Action.D
        if C_prev >= m:
            if self.my_index in S_prev:
                return Action.C
            elif n - C_prev == 1:
                return Action.C
            else:
                self.punish_until = t + self.P - 1
                return Action.D
        elif self.my_index in S_prev:
            return Action.C
        else:
            s = m - C_prev
            ordered_defectors = sorted(Def_prev)
            first_s = set(ordered_defectors[:s])
            if self.my_index in first_s:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_11(BaseStrategy):
    """
    Bayesian pivotal-cooperator for the Collective Risk Dilemma.

    - Round 1: cooperate to bootstrap.
    - From round 2 onward: treat each other player's action as exchangeable Bernoulli(p)
      with Beta(1,1) prior. Use the Beta-Binomial predictive to compute the probability
      that exactly (m-1) of the other (n-1) players will cooperate next round.
      Cooperate iff k * P(X = m-1) >= 1 (ties favor cooperation).
    - Uses only public opponent action history. Forgiving by Bayesian updating.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha0 = 1.0
        self.beta0 = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k_reward = float(self.game_description.k)
        if n <= 1:
            return Action.D
        completed_rounds = int(history.round_number)
        if completed_rounds <= 0:
            return Action.C
        S = int(np.sum(history.opponent_actions))
        T = completed_rounds * (n - 1)
        alpha = self.alpha0 + S
        beta = self.beta0 + (T - S)
        k_target = m - 1
        if k_target < 0 or k_target > n - 1:
            Pm1 = 0.0
        else:
            comb = math.comb(n - 1, k_target)
            log_num_beta = math.lgamma(alpha + k_target) + math.lgamma(beta + (n - 1 - k_target)) - math.lgamma(alpha + beta + (n - 1))
            log_den_beta = math.lgamma(alpha) + math.lgamma(beta) - math.lgamma(alpha + beta)
            try:
                Pm1 = comb * math.exp(log_num_beta - log_den_beta)
            except OverflowError:
                Pm1 = 0.0
            if not 0.0 <= Pm1 <= 1.0:
                Pm1 = max(0.0, min(1.0, Pm1))
        if k_reward * Pm1 >= 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_12(BaseStrategy):
    """
    Adaptive Pivotal Cooperation (APC)

    - Starts cooperative.
    - Estimates each opponent's cooperation rate from history.
    - Computes Poisson-binomial probabilities of reaching the threshold
      with and without this player's cooperation.
    - Cooperates when your cooperation meaningfully raises success chance,
      defects when success is likely without you, and uses recovery rules
      after consecutive failures. Be cautious in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta_succ = 0.6
        self.theta_last = 0.75
        self.defector_threshold = 0.2
        self.recovery_trigger = 2
        self.recovery_length = 3
        self.low_p_factor = 0.1

    def _poisson_binomial_at_least(self, probs, k):
        """
        Exact DP for Poisson-binomial: given independent Bernoulli(p_i) with probabilities in probs,
        return P(sum_i >= k).
        probs: iterable of floats length N
        k: integer threshold
        """
        n = len(probs)
        if k <= 0:
            return 1.0
        if k > n:
            return 0.0
        dp = [0.0] * (n + 1)
        dp[0] = 1.0
        for p in probs:
            q = 1.0 - p
            new_dp = [0.0] * (n + 1)
            new_dp[0] = dp[0] * q
            for s in range(1, n + 1):
                new_dp[s] = dp[s] * q + dp[s - 1] * p
            dp = new_dp
        return sum(dp[k:])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = getattr(state, 'round_number', None)
        if t is None or (isinstance(t, int) and t <= 0):
            t = history.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if history.round_number == 0:
            return Action.C
        opp_acts = np.array(history.opponent_actions, dtype=float)
        past_rounds = opp_acts.shape[0]
        n_others = opp_acts.shape[1] if opp_acts.ndim > 1 else 0
        if n_others == 0:
            return Action.D
        p_js = np.mean(opp_acts, axis=0)
        p_effective = []
        for p in p_js:
            if p < self.defector_threshold:
                p_effective.append(p * self.low_p_factor)
            else:
                p_effective.append(p)
        p_effective = [float(x) for x in p_effective]
        prob_if_defect = self._poisson_binomial_at_least(p_effective, m)
        prob_if_cooperate = self._poisson_binomial_at_least(p_effective, max(0, m - 1))
        my_acts = np.array(history.my_actions, dtype=int)
        opp_sums = np.sum(opp_acts, axis=1)
        total_coops = opp_sums + my_acts
        successes = total_coops >= m
        recent_success_rate = float(np.mean(successes)) if successes.size > 0 else 0.0
        consec_failures = 0
        for val in total_coops[::-1]:
            if val < m:
                consec_failures += 1
            else:
                break
        if t == n_rounds:
            if prob_if_cooperate >= self.theta_last:
                return Action.C
            else:
                return Action.D
        if prob_if_defect >= self.theta_succ:
            return Action.D
        if prob_if_cooperate >= self.theta_succ:
            return Action.C
        if recent_success_rate >= 0.5:
            return Action.C
        if consec_failures >= self.recovery_trigger:
            run_start_index = past_rounds - consec_failures + 1
            recovery_start_round = run_start_index + self.recovery_trigger
            recovery_rounds_used = max(0, past_rounds + 1 - recovery_start_round)
            if recovery_rounds_used < self.recovery_length:
                return Action.C
        return Action.D

class Strategy_COOPERATIVE_13(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR)

    History-dependent conditional cooperator for the Collective Risk Dilemma.
    - Starts by cooperating.
    - Estimates distribution of how many OTHER players cooperate using a sliding
      window with Laplace smoothing.
    - Cooperates when the myopic expected payoff of cooperating (being pivotal)
      plus a small cooperative bias outweighs defecting.
    - Implements forgiveness (grace), withdraws after repeated exploitation,
      and occasionally probes via random exploration.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 1.0
        self.delta = 0.02
        self.W_default = 20
        self.G = 1
        self.M_fail = 3
        self.epsilon_default = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        last_rounds = history.round_number
        W_curr = min(self.W_default, last_rounds)
        if W_curr <= 0:
            return Action.C
        opp_actions = history.opponent_actions
        opp_recent = opp_actions[-W_curr:, :] if W_curr > 0 else opp_actions
        try:
            others_counts = np.array(opp_recent.sum(axis=1)).astype(int)
        except Exception:
            return Action.D
        counts = np.bincount(others_counts, minlength=self.n_players)
        denom = float(W_curr + self.alpha * self.n_players)
        P_hat = (counts + self.alpha) / denom
        m_index = max(0, min(self.n_players - 1, self.m - 1))
        if self.m <= 0:
            p_ge_m = 1.0
        elif self.m > self.n_players - 1:
            p_ge_m = 0.0
        else:
            p_ge_m = float(P_hat[self.m:self.n_players].sum())
        if self.m - 1 <= 0:
            p_ge_mminus1 = 1.0
        elif self.m - 1 > self.n_players - 1:
            p_ge_mminus1 = 0.0
        else:
            p_ge_mminus1 = float(P_hat[self.m - 1:self.n_players].sum())
        p_eq_mminus1 = float(P_hat[m_index])
        E_C = self.k * p_ge_mminus1
        E_D = 1.0 + self.k * p_ge_m
        my_actions = history.my_actions
        opp_sums_all = np.array(history.opponent_actions.sum(axis=1)).astype(int)
        total_coops = opp_sums_all + np.array(my_actions.astype(int))
        consecutive_failures = 0
        for idx in range(1, last_rounds + 1):
            my_act = bool(my_actions[-idx])
            total = int(total_coops[-idx])
            if my_act and total < self.m:
                consecutive_failures += 1
            else:
                break
        opp_success_without_me = bool((opp_recent.sum(axis=1) >= self.m).any())
        is_last_round = t == self.r
        if is_last_round:
            delta_effective = 0.0
            epsilon = 0.0
        else:
            if consecutive_failures <= 0:
                delta_effective = self.delta
            else:
                factor = max(0.0, 1.0 - consecutive_failures / float(max(1, self.G)))
                delta_effective = self.delta * factor
            eps_horizon = 1.0 / float(max(10, max(1, self.r // 10)))
            epsilon = min(self.epsilon_default, eps_horizon)
        if consecutive_failures >= self.M_fail and (not opp_success_without_me):
            chosen = Action.D
        elif E_C + delta_effective >= E_D:
            chosen = Action.C
        else:
            chosen = Action.D
        if epsilon > 0.0 and random.random() < float(epsilon):
            chosen = Action.C if chosen == Action.D else Action.D
        return chosen

class Strategy_COOPERATIVE_14(BaseStrategy):
    """
    Conditional Recruit-and-Forgive for the Collective Risk Dilemma.

    - Starts cooperative (first round).
    - Never cooperates on last round.
    - Tracks recent window statistics to detect exploitation.
    - If repeatedly exploited, issues a short punishment (defect for Punish_length rounds),
      then forgives by resetting recent statistics.
    - Uses per-opponent cooperation frequencies to estimate expected number of other cooperators.
    - Cooperates when pivotal, or when the group reliably reaches the threshold; otherwise
      occasionally attempts recruitment early in the game with a decaying probability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_remaining = 0
        self.exploit_reset_round = 0
        r = self.game_description.n_rounds
        self.W = min(5, max(1, r - 1))
        self.S_threshold = 0.75
        self.Exploit_threshold = math.ceil(self.W / 3)
        self.Punish_length = min(3, max(1, math.floor(r / 10)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        m = self.game_description.m
        t = state.round_number
        if history is None:
            return Action.C
        if t == r:
            self.punishment_remaining = 0
            return Action.D
        completed = history.round_number
        if completed == 0:
            return Action.C
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        coop_counts = my_actions.astype(int) + np.sum(opp_actions.astype(int), axis=1)
        coop_freqs = np.sum(opp_actions.astype(int), axis=0) / float(completed)
        expected_others = float(np.sum(coop_freqs))
        window_len = min(self.W, completed)
        window_start = max(0, completed - window_len)
        window_start = max(window_start, self.exploit_reset_round)
        lastW = max(0, completed - window_start)
        if lastW == 0:
            S = 0.0
            RecentExploitation = 0
        else:
            recent_coop_counts = coop_counts[window_start:completed]
            recent_my_actions = my_actions[window_start:completed]
            S = float(np.sum(recent_coop_counts >= m)) / float(lastW)
            RecentExploitation = int(np.sum(recent_my_actions & (recent_coop_counts < m)))
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            return Action.D
        if RecentExploitation >= self.Exploit_threshold:
            remaining_including_current = max(1, r - t + 1)
            total_punish = min(self.Punish_length, remaining_including_current)
            self.punishment_remaining = max(0, total_punish - 1)
            self.exploit_reset_round = completed
            return Action.D
        if expected_others >= m:
            if S >= self.S_threshold:
                return Action.C
            else:
                return Action.D
        if expected_others >= m - 1:
            return Action.C
        p_recruit = max(0.2, 0.6 * (1.0 - float(t) / float(r)))
        if random.random() < p_recruit:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_15(BaseStrategy):
    """
    Adaptive Pivotal Tit-for-Tat with Probabilistic Rescue (AP-TFT-PR).

    - Signals cooperation on round 1.
    - Uses a sliding window W = min(10, r) to compute per-opponent smoothed cooperation
      probabilities with Laplace smoothing: p_j = (C_j + 1) / (t_obs_j + 2).
    - If the previous round was successful (>= m cooperators) cooperates to stabilize.
    - Otherwise uses simple pivotal logic using:
        E = sum_j p_j  (expected other cooperators)
        L = count_j (p_j >= 0.5)  (likely cooperators)
      to decide whether a single cooperation is likely decisive. Applies a slightly
      more generous threshold in small/likely-cooperation settings.
    - If recent failures look like coordination problems (many players tried but just missed),
      enters a short probabilistic "rescue" phase (S = min(3, remaining rounds)) to
      attempt to re-bootstrap cooperation.
    - Final round: cooperate only if last round was successful (and many players look
      cooperative) or if you are likely pivotal (L >= m-1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = min(10, max(1, self.game_description.n_rounds))
        self.rescue_timer = 0
        self.p_rescue = 0.6
        self.likely_thresh = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.rescue_timer = 0
            return Action.C
        desc = self.game_description
        n = desc.n_players
        r = desc.n_rounds
        m = desc.m
        t_completed = history.round_number
        t = t_completed + 1
        W = min(self.W, t_completed)
        if W > 0:
            opp_slice = history.opponent_actions[-W:, :]
            my_slice = history.my_actions[-W:]
        else:
            opp_slice = np.zeros((0, n - 1), dtype=bool)
            my_slice = np.zeros((0,), dtype=bool)
        p_list = []
        for j in range(n - 1):
            if W > 0:
                C_j = int(np.sum(opp_slice[:, j]))
                t_obs_j = W
            else:
                C_j = 0
                t_obs_j = 0
            p_j = (C_j + 1) / (t_obs_j + 2)
            p_list.append(float(p_j))
        p_arr = np.array(p_list, dtype=float)
        if W > 0:
            C_me = int(np.sum(my_slice))
            t_obs_me = W
        else:
            C_me = 0
            t_obs_me = 0
        my_p = (C_me + 1) / (t_obs_me + 2)
        E = float(np.sum(p_arr))
        L = int(np.sum(p_arr >= self.likely_thresh))
        last_round_idx = t_completed - 1
        if last_round_idx >= 0:
            opp_last = history.opponent_actions[last_round_idx, :]
            my_last = bool(history.my_actions[last_round_idx])
            coopers_last = int(np.sum(opp_last)) + (1 if my_last else 0)
            last_success = coopers_last >= m
        else:
            last_success = False
        consecutive_failures = 0
        for idx in range(t_completed - 1, -1, -1):
            opp_rnd = history.opponent_actions[idx, :]
            my_rnd = bool(history.my_actions[idx])
            coopers = int(np.sum(opp_rnd)) + (1 if my_rnd else 0)
            if coopers < m:
                consecutive_failures += 1
            else:
                break
        if t == r:
            if last_success and L >= max(0, m - 1):
                return Action.C
            elif L >= max(0, m - 1):
                return Action.C
            else:
                return Action.D
        if last_success:
            self.rescue_timer = 0
            return Action.C
        avg_coop_frac = (float(np.sum(p_arr)) + float(my_p)) / float(n)
        coord_threshold = 0.8 * (m / n)
        remaining_rounds = r - t + 1
        rescue_S = min(3, max(0, remaining_rounds))
        if self.rescue_timer > 0:
            if E + 1 >= m - 0.5:
                action = Action.C
            else:
                action = Action.C if random.random() < self.p_rescue else Action.D
            self.rescue_timer = max(0, self.rescue_timer - 1)
            return action
        if consecutive_failures > 0 and avg_coop_frac >= coord_threshold and (rescue_S > 0):
            self.rescue_timer = rescue_S - 1
            if E + 1 >= m - 0.5:
                return Action.C
            else:
                return Action.C if random.random() < self.p_rescue else Action.D
        generous = m <= 2 or E >= max(0.0, m - 1.5)
        threshold_offset = -0.5 if generous else 0.0
        effective_target = m + threshold_offset
        if L >= m:
            return Action.D
        if L == max(0, m - 1):
            return Action.C
        if E + 1 >= effective_target:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_16(BaseStrategy):
    """
    Rotating Minimal-Volunteer with Escalation and Forgiveness (RMVEF)

    - Deterministic rotation of volunteer duty based on player index (we assume
      the agent is player index 0 and opponents are indices 1..n-1 in the order
      provided by opponent_actions).
    - Aim to have exactly m volunteers each round, escalate after failures,
      demote repeated non-helpers, and be forgiving after success.
    - Special cautious behavior in final round and a last-round 'near-miss' save.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = game_description.n_players
        self._initialized = True
        self.fail_count = 0
        self.demoted_until = [0] * n
        self.reliability = [0.0] * n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            t = 1
        else:
            t = history.round_number + 1

        def get_past_action(j: int, s: int) -> bool:
            if s < 1:
                return False
            idx = s - 1
            if j == 0:
                return bool(history.my_actions[idx])
            else:
                return bool(history.opponent_actions[idx, j - 1])

        def coop_count_at(s: int) -> int:
            if s < 1:
                return 0
            idx = s - 1
            my = 1 if history is not None and bool(history.my_actions[idx]) else 0
            opps = 0
            if history is not None and history.opponent_actions.size > 0:
                opps = int(np.sum(history.opponent_actions[idx, :]))
            return my + opps

        def rank(j: int, round_t: int) -> int:
            return (j - (round_t - 1)) % n
        sim_fail = 0
        sim_demoted_until = [0] * n
        V_hist = dict()
        for s in range(1, t):
            S_s = min(m + sim_fail, n)
            L = list(range(n))
            L.sort(key=lambda j: rank(j, s))
            V_s = []
            for j in L:
                if sim_demoted_until[j] >= s:
                    continue
                V_s.append(j)
                if len(V_s) == S_s:
                    break
            if len(V_s) < S_s:
                for j in L:
                    if j in V_s:
                        continue
                    V_s.append(j)
                    if len(V_s) == S_s:
                        break
            V_hist[s] = V_s
            actual_coops = coop_count_at(s)
            success = actual_coops >= m
            W = min(5, s)
            K = math.ceil(W / 2)
            M = min(3, r - s + 1)
            window_start = max(1, s - W + 1)
            for j in range(n):
                times_assigned = 0
                cooperated_when_assigned = 0
                for u in range(window_start, s + 1):
                    V_u = V_hist.get(u, [])
                    if j in V_u:
                        times_assigned += 1
                        if get_past_action(j, u):
                            cooperated_when_assigned += 1
                if times_assigned >= K:
                    if cooperated_when_assigned < math.floor(K / 2):
                        new_until = s + M - 1
                        if new_until > sim_demoted_until[j]:
                            sim_demoted_until[j] = new_until
            sim_fail = 0 if success else sim_fail + 1
        self.fail_count = sim_fail
        self.demoted_until = sim_demoted_until
        recent_s0 = min(3, max(0, t - 1))
        rel = [0.0] * n
        if recent_s0 > 0:
            window_rounds = list(range(max(1, t - recent_s0), t))
            for j in range(n):
                times_assigned = 0
                cooperated_when_assigned = 0
                for u in window_rounds:
                    V_u = V_hist.get(u, [])
                    if j in V_u:
                        times_assigned += 1
                        if get_past_action(j, u):
                            cooperated_when_assigned += 1
                if times_assigned > 0:
                    rel[j] = cooperated_when_assigned / times_assigned
                else:
                    rel[j] = 0.0
        self.reliability = rel
        S_t = min(m + self.fail_count, n)
        L_t = list(range(n))
        L_t.sort(key=lambda j: rank(j, t))
        V_t = []
        for j in L_t:
            if self.demoted_until[j] >= t:
                continue
            V_t.append(j)
            if len(V_t) == S_t:
                break
        if len(V_t) < S_t:
            for j in L_t:
                if j in V_t:
                    continue
                V_t.append(j)
                if len(V_t) == S_t:
                    break
        me = 0

        def cooperated_in_recent(at_most=3) -> bool:
            if history is None or history.round_number == 0:
                return False
            s0 = min(at_most, history.round_number)
            start = history.round_number - s0
            arr = history.my_actions[start:history.round_number]
            return int(np.sum(arr)) > 0

        def last_round_evidence_for_Vt() -> bool:
            if history is None:
                return False
            needed_rounds = min(2, history.round_number)
            if needed_rounds < 1:
                return False
            successes = 0
            for s in range(history.round_number - needed_rounds + 1, history.round_number + 1):
                if coop_count_at(s) >= m:
                    successes += 1
            if successes < needed_rounds:
                return False
            if len(V_t) == 0:
                return False
            avg_rel = float(np.mean([self.reliability[j] for j in V_t]))
            return avg_rel >= 0.5

        def near_miss_save() -> bool:
            if history is None or history.round_number == 0:
                return False
            last = history.round_number
            if coop_count_at(last) != m - 1:
                return False
            if me in V_t:
                return False
            if not cooperated_in_recent(3):
                return False
            higher_priority = []
            for j in L_t:
                if rank(j, t) < rank(me, t):
                    higher_priority.append(j)
            offenders = []
            if history is not None and history.round_number >= 1:
                last_round_idx = history.round_number
                for j in higher_priority:
                    V_prev = V_hist.get(last_round_idx, [])
                    if j in V_prev and (not get_past_action(j, last_round_idx)):
                        offenders.append(j)
            if len(offenders) == 0:
                return False
            avg_offender_rel = float(np.mean([self.reliability[j] for j in offenders])) if offenders else 1.0
            return avg_offender_rel < 0.5
        decide_cooperate = False
        if t == r:
            if last_round_evidence_for_Vt():
                decide_cooperate = me in V_t
            elif near_miss_save():
                decide_cooperate = True
            else:
                decide_cooperate = False
        elif me in V_t:
            decide_cooperate = True
        elif near_miss_save():
            decide_cooperate = True
        else:
            decide_cooperate = False
        return Action.C if decide_cooperate else Action.D

class Strategy_COOPERATIVE_17(BaseStrategy):
    """
    Rotating-Pivotal Conditional Cooperator (RPCC)

    - Seeds cooperation according to a deterministic rotation of m "burden" slots.
    - Maintains per-opponent cooperation probabilities p_j via exponential smoothing.
    - Computes Poisson-Binomial probabilities to decide whether one's cooperation is
      likely pivotal. Cooperates when cooperation is decisive (or when rotation says it's
      this player's turn in ties); otherwise defects. Uses a slightly larger epsilon on
      the final round to favor immediate payoff.
    - Includes lightweight, temporary punishment (penalize defectors if your cooperation
      failed) and small boosts for cooperators in exactly-m successful rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.25
        self.epsilon = 1e-06
        self.epsilon_last = 0.001
        self.delta = 0.0001
        self.beta = 0.1
        self.boost = 0.05
        self.my_index = 0
        self.n_opponents = max(0, self.n - 1)
        if self.n_opponents > 0:
            self.p = np.full(self.n_opponents, 0.5, dtype=float)
        else:
            self.p = np.array([], dtype=float)
        self.last_seen_rounds = 0

    def _in_rotation(self, t: int) -> bool:
        """Return True if self.my_index is in the rotating seed S_t for round t (1-indexed)."""
        base = (t - 1) * self.m % self.n
        for offset in range(self.m):
            if (base + offset) % self.n == self.my_index:
                return True
        return False

    def _poisson_binomial_tail_probs(self, probs: np.ndarray):
        """
        Given an array probs of length L with individual success probabilities,
        compute the distribution of total successes among them as an array dist of length L+1
        where dist[t] = P(exactly t successes). Uses DP.
        """
        L = len(probs)
        dist = np.zeros(L + 1, dtype=float)
        dist[0] = 1.0
        for p in probs:
            for t in range(L, 0, -1):
                dist[t] = dist[t] * (1.0 - p) + dist[t - 1] * p
            dist[0] = dist[0] * (1.0 - p)
        return dist

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is not None and self.n_opponents > 0:
            total_rounds_obs = history.round_number
            if total_rounds_obs > 0:
                start = self.last_seen_rounds
                end = total_rounds_obs
                for r_idx in range(start, end):
                    opp_row = np.array(history.opponent_actions[r_idx, :], dtype=bool)
                    my_act = bool(history.my_actions[r_idx])
                    coop_count = int(opp_row.sum()) + (1 if my_act else 0)
                    observed = opp_row.astype(float)
                    self.p = (1.0 - self.alpha) * self.p + self.alpha * observed
                    if my_act and coop_count < self.m:
                        defect_mask = observed == 0.0
                        if defect_mask.any():
                            self.p[defect_mask] = np.maximum(self.delta, self.p[defect_mask] - self.beta)
                    if coop_count == self.m:
                        coop_mask = observed == 1.0
                        if coop_mask.any():
                            self.p[coop_mask] = np.minimum(1.0 - self.delta, self.p[coop_mask] + self.boost)
                    self.p = np.clip(self.p, self.delta, 1.0 - self.delta)
                self.last_seen_rounds = total_rounds_obs
        role_flag = self._in_rotation(t)
        if history is None:
            return Action.C if role_flag else Action.D
        if self.n_opponents == 0:
            return Action.D
        p_vec = np.clip(self.p, self.delta, 1.0 - self.delta)
        dist = self._poisson_binomial_tail_probs(p_vec)
        thresh_A = max(0, self.m - 1)
        if thresh_A <= len(dist) - 1:
            A = float(dist[thresh_A:].sum())
        else:
            A = 0.0
        thresh_B = self.m
        if thresh_B <= len(dist) - 1:
            B = float(dist[thresh_B:].sum())
        else:
            B = 0.0
        U_C = A * self.k
        U_D = 1.0 + B * self.k
        eps = self.epsilon_last if t == self.r else self.epsilon
        if U_C > U_D + eps:
            return Action.C
        elif U_D > U_C + eps:
            return Action.D
        else:
            return Action.C if role_flag else Action.D

class Strategy_COOPERATIVE_18(BaseStrategy):
    """
    Conditional Contributor with Exact-Marginal Rule (CCEMR).

    - Estimates each opponent's cooperation probability from a sliding window with a small Beta prior.
    - Computes delta = Prob[#others == m-1] under independent Bernoulli(p_j).
    - Cooperates iff k * delta > 1 (ties broken by recent group success stability).
    - Applies a punishment discount q to players who repeatedly defect in rounds that were successful,
      and forgives them after f consecutive cooperations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.w = min(20, max(1, game_description.n_rounds))
        self.alpha = 1.5
        self.beta = 1.0
        self.q = 0.5
        self.punish_threshold = 2
        self.forgiveness_count = 1
        self.tie_success_rate_threshold = 0.75

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        t_completed = int(history.round_number)
        T = min(t_completed, self.w)
        n_others = n - 1
        if T > 0:
            recent_op_actions = history.opponent_actions[-T:, :]
            recent_my_actions = history.my_actions[-T:]
            opp_counts = np.sum(recent_op_actions, axis=1).astype(int)
            my_counts = np.array(recent_my_actions).astype(int)
            total_cooperators = opp_counts + my_counts
            recent_success_mask = total_cooperators >= m
        else:
            recent_op_actions = np.zeros((0, n_others), dtype=bool)
            recent_success_mask = np.array([], dtype=bool)
        p_list = []
        for j in range(n_others):
            if T > 0:
                c_j = int(np.sum(recent_op_actions[:, j]))
            else:
                c_j = 0
            denom = self.alpha + self.beta + T
            p_j_raw = (self.alpha + c_j) / denom if denom > 0 else 0.5
            punished = False
            if T > 0:
                if recent_success_mask.any():
                    defections_on_success = np.logical_and(recent_success_mask, np.logical_not(recent_op_actions[:, j]))
                    freeride_count = int(np.sum(defections_on_success))
                    if freeride_count >= self.punish_threshold:
                        punished = True
                if punished and self.forgiveness_count > 0:
                    f = self.forgiveness_count
                    if T >= f:
                        last_f = recent_op_actions[-f:, j]
                        if bool(np.all(last_f)):
                            punished = False
            p_j = self.q * p_j_raw if punished else p_j_raw
            if p_j < 0.0:
                p_j = 0.0
            elif p_j > 1.0:
                p_j = 1.0
            p_list.append(float(p_j))
        prob = np.zeros(n_others + 1, dtype=float)
        prob[0] = 1.0
        for p in p_list:
            newprob = np.zeros_like(prob)
            for x in range(len(prob) - 1):
                if prob[x] == 0.0:
                    continue
                newprob[x] += prob[x] * (1.0 - p)
                newprob[x + 1] += prob[x] * p
            prob = newprob
        idx = m - 1
        delta = prob[idx] if 0 <= idx <= n_others else 0.0
        if m <= n_others:
            P_s_defect = float(np.sum(prob[m:]))
        else:
            P_s_defect = 0.0
        marginal = k * delta - 1.0
        eps = 1e-12
        if marginal > eps:
            return Action.C
        elif marginal < -eps:
            return Action.D
        else:
            recent_success_rate = 0.0
            if T > 0:
                recent_success_rate = float(np.sum(recent_success_mask)) / float(T)
            if recent_success_rate >= self.tie_success_rate_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_19(BaseStrategy):
    """
    Adaptive Pivotal Cooperation (APC)

    - Bootstraps cooperation for the first few rounds.
    - Maintains simple smoothed beliefs about each opponent's cooperation rate.
    - Computes Poisson-binomial probabilities for whether the group will
      reach the cooperation threshold with and without this player's cooperation.
    - Cooperates when its contribution is plausibly pivotal, defects when the
      group is likely to reach the threshold without it, and otherwise explores
      with small probability.
    - Applies short-lived penalties (reduced belief) to opponents who defected
      when this player cooperated and the threshold failed, with forgiveness.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.T_init = min(3, max(1, self.game_description.n_rounds - 1))
        self.q_free_ride = 0.75
        self.q_pivotal = 0.45
        self.q_free_ride_last = 0.75
        self.q_pivotal_last = 0.75
        self.epsilon = 0.05
        self.s_penalty = 2
        self.penalty_factor = 0.5
        self.penalty_until = {}

    def _poisson_binomial_prob_at_least(self, p_list, threshold):
        """
        Exact dynamic programming for Poisson-Binomial:
        Returns probability that sum of independent Bernoulli(p_i) >= threshold.
        p_list: iterable of probabilities for opponents
        threshold: integer
        """
        probs = list(p_list)
        L = len(probs)
        if threshold <= 0:
            return 1.0
        if threshold > L:
            return 0.0
        dp = [0.0] * (L + 1)
        dp[0] = 1.0
        for p in probs:
            for k in range(L, 0, -1):
                dp[k] = dp[k] * (1.0 - p) + dp[k - 1] * p
            dp[0] = dp[0] * (1.0 - p)
        return sum(dp[threshold:])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            rounds_played = 0
        else:
            rounds_played = history.round_number
        t = rounds_played + 1
        if t <= self.T_init:
            return Action.C
        if history is not None and rounds_played >= 1:
            last_idx = rounds_played - 1
            my_last = bool(history.my_actions[last_idx])
            opp_coops_last = int(np.sum(history.opponent_actions[last_idx, :]))
            total_coops_last = opp_coops_last + (1 if my_last else 0)
            if my_last and total_coops_last < m:
                for j in range(history.opponent_actions.shape[1]):
                    opp_act = bool(history.opponent_actions[last_idx, j])
                    if not opp_act:
                        self.penalty_until[j] = rounds_played + self.s_penalty
        num_opponents = n - 1
        if history is None:
            prior_p = (0.0 + self.alpha) / (0.0 + 2.0 * self.alpha)
            p_list = [prior_p] * num_opponents
        else:
            coop_counts = np.sum(history.opponent_actions, axis=0)
            denom = rounds_played + 2.0 * self.alpha
            p_list = []
            for j in range(num_opponents):
                coop_j = float(coop_counts[j])
                p_j = (coop_j + self.alpha) / denom
                expiry = self.penalty_until.get(j, 0)
                if expiry > rounds_played:
                    p_j = p_j * self.penalty_factor
                if p_j < 0.0:
                    p_j = 0.0
                elif p_j > 1.0:
                    p_j = 1.0
                p_list.append(p_j)
        P_without_me = self._poisson_binomial_prob_at_least(p_list, m)
        P_with_me = self._poisson_binomial_prob_at_least(p_list, max(0, m - 1))
        if t == r:
            if P_without_me >= self.q_free_ride_last:
                return Action.D
            if P_with_me >= self.q_pivotal_last:
                return Action.C
            return Action.D
        if P_without_me >= self.q_free_ride:
            return Action.D
        if P_with_me >= self.q_pivotal:
            return Action.C
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_20(BaseStrategy):
    """
    Adaptive Minimal-Volunteer with Reputation & Probing (AMVRP)

    - Maintains short-term reputations (fraction of recent cooperations) over a sliding window W.
    - Deterministically selects the top-m players by reputation (ties broken by player index).
    - Volunteers (plays C) if selected and the estimated joint success probability for the volunteer set
      exceeds a threshold theta; otherwise volunteers only with small probing probability.
    - Non-volunteers normally defect (D), but occasionally probe (small probability) to signal willingness.
    - Parameters (fixed functions of game parameters):
        W = min(10, r)
        epsilon_probe = 0.05
        epsilon_probe_small = epsilon_probe / 3
        theta = 0.3
        p0 = 0.5
    Notes on indexing:
    - This implementation assumes a deterministic notion of player indices where this agent treats itself
      as player index 0 and the opponent_actions columns correspond to players 1..n-1 in order.
      This yields a reproducible tie-break by index across independent implementations that make the
      same assumption.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.W = min(10, self.r)
        self.epsilon_probe = 0.05
        self.epsilon_probe_small = self.epsilon_probe / 3.0
        self.theta = 0.3
        self.p0 = 0.5
        self.self_index = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', 1)
        if history is None or history.round_number == 0:
            if self.self_index < self.m:
                return Action.C
            else:
                if random.random() < self.epsilon_probe_small:
                    return Action.C
                return Action.D
        rounds_completed = history.round_number
        window = min(self.W, rounds_completed)
        p = [self.p0] * self.n
        opp_actions = getattr(history, 'opponent_actions', None)
        my_actions = getattr(history, 'my_actions', None)
        try:
            start_idx = rounds_completed - window
            if my_actions is not None and len(my_actions) >= window:
                recent_my = my_actions[start_idx:rounds_completed]
                p_self = float(np.sum(recent_my)) / float(window)
            else:
                p_self = self.p0
            p[self.self_index] = min(1.0, max(0.0, p_self))
            if opp_actions is not None and opp_actions.shape[0] >= window:
                recent_opps = opp_actions[start_idx:rounds_completed, :]
                expected_opp_cols = max(0, self.n - 1)
                actual_cols = recent_opps.shape[1] if recent_opps.ndim >= 2 else 0
                for col in range(min(actual_cols, expected_opp_cols)):
                    idx = col + 1
                    col_vals = recent_opps[:, col]
                    coop_count = float(np.sum(col_vals))
                    p[idx] = min(1.0, max(0.0, coop_count / float(window)))
            else:
                pass
        except Exception:
            p = [self.p0] * self.n
            p[self.self_index] = self.p0
        indices = list(range(self.n))
        indices.sort(key=lambda j: (-p[j], j))
        S = indices[:self.m]
        P_success = 1.0
        for j in S:
            P_success *= p[j]
        if self.self_index in S:
            if P_success >= self.theta:
                return Action.C
            else:
                if t == 1:
                    return Action.C
                if random.random() < self.epsilon_probe:
                    return Action.C
                return Action.D
        else:
            if random.random() < self.epsilon_probe_small:
                return Action.C
            return Action.D

class Strategy_COOPERATIVE_21(BaseStrategy):
    """
    Conditional Probabilistic Reciprocity with Threshold Estimation (CPR-TE).

    - Estimates opponents' cooperation probabilities with an EMA (alpha=0.3).
    - Uses exact Poisson-binomial DP over opponents to compute probabilities that
      others reach the threshold (m-1, m, exact m-1).
    - Cooperates when pivotal (Pr_exact_{m-1} >= 1/k), or to maintain recent success,
      or to probabilistically rescue after recent success; defects in the final round
      and free-rides if others almost surely meet the threshold without you.
    - On persistent failures (no successes in last S rounds) enters short punishment
      bursts (length L) and then probes by cooperating with probability p_probe.
    - First round (unless it's also the final round) cooperates to seed coordination.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.in_punishment_cycle = False
        self.burst_remaining = 0
        self.alpha = 0.3
        self.p_safe = 0.95
        self.buffer = 0.05
        self._last_seen_rounds = None

    def _compute_opponent_prob_estimates(self, history: PlayerHistory, n_opponents: int):
        alpha = self.alpha
        p = np.full(n_opponents, 0.5, dtype=float)
        if history is None:
            return p
        rounds = history.round_number
        if rounds == 0:
            return p
        obs = history.opponent_actions
        for t in range(rounds):
            round_vals = obs[t, :].astype(float)
            p = (1.0 - alpha) * p + alpha * round_vals
        return p

    def _poisson_binomial_dp(self, probs):
        n = len(probs)
        DP = np.zeros(n + 1, dtype=float)
        DP[0] = 1.0
        for p in probs:
            newDP = np.zeros_like(DP)
            newDP[0] = DP[0] * (1.0 - p)
            for k in range(1, n + 1):
                newDP[k] = DP[k] * (1.0 - p) + DP[k - 1] * p
            DP = newDP
        return DP

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        desc = self.game_description
        n = desc.n_players
        r = desc.n_rounds
        m = desc.m
        k = desc.k
        t = state.round_number if state is not None else 1
        if history is None:
            self.in_punishment_cycle = False
            self.burst_remaining = 0
            if t == r:
                return Action.D
            return Action.C
        last_seen = self._last_seen_rounds
        if last_seen is None or history.round_number < (last_seen or 0):
            self.in_punishment_cycle = False
            self.burst_remaining = 0
        self._last_seen_rounds = history.round_number
        if t == r:
            return Action.D
        n_opponents = n - 1
        p_j = self._compute_opponent_prob_estimates(history, n_opponents)
        DP = self._poisson_binomial_dp(p_j)
        idx_m_minus_1 = max(0, m - 1)
        if idx_m_minus_1 <= n_opponents:
            Pr_m_minus_1 = float(np.sum(DP[idx_m_minus_1:]))
        else:
            Pr_m_minus_1 = 0.0
        idx_m = max(0, m)
        if idx_m <= n_opponents:
            Pr_m = float(np.sum(DP[idx_m:]))
        else:
            Pr_m = 0.0
        Pr_exact_m_minus_1 = float(DP[m - 1]) if 0 <= m - 1 <= n_opponents else 0.0
        P_others = float(np.mean(p_j)) if n_opponents > 0 else 0.0
        rounds_completed = history.round_number
        if rounds_completed == 0:
            last_round_success = False
        else:
            last_my = bool(history.my_actions[-1])
            last_opponents = history.opponent_actions[-1, :].astype(int)
            total_last_coops = int(np.sum(last_opponents)) + (1 if last_my else 0)
            last_round_success = total_last_coops >= m
        S = min(5, max(0, r - 1))
        recent_successes = 0
        if S > 0 and rounds_completed > 0:
            start_idx = max(0, rounds_completed - S)
            for rr in range(start_idx, rounds_completed):
                my_act = bool(history.my_actions[rr])
                opps = history.opponent_actions[rr, :].astype(int)
                total_coops = int(np.sum(opps)) + (1 if my_act else 0)
                if total_coops >= m:
                    recent_successes += 1
        if Pr_exact_m_minus_1 >= 1.0 / k:
            return Action.C
        if Pr_m >= self.p_safe:
            return Action.D
        if last_round_success:
            threshold = m / float(n) - self.buffer
            if P_others >= threshold:
                self.in_punishment_cycle = False
                self.burst_remaining = 0
                return Action.C
        if recent_successes > 0:
            self.in_punishment_cycle = False
            self.burst_remaining = 0
            p_rescue = min(0.9, 0.5 + 0.5 * P_others)
            if random.random() < p_rescue:
                return Action.C
            else:
                return Action.D
        L = min(3, max(1, math.floor(r / 10))) if r > 0 else 1
        if not self.in_punishment_cycle:
            self.in_punishment_cycle = True
            self.burst_remaining = L
            self.burst_remaining -= 1
            return Action.D
        elif self.burst_remaining > 0:
            self.burst_remaining -= 1
            return Action.D
        else:
            p_probe = max(0.2, P_others)
            if random.random() < p_probe:
                return Action.C
            else:
                self.burst_remaining = L
                self.burst_remaining -= 1
                return Action.D

class Strategy_COOPERATIVE_22(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    Summary:
    - Estimate marginal pivotality of a single cooperation (ΔP). Cooperate when ΔP > 1/k.
    - Add light reciprocity (reward recent cooperators), short punishments for repeat exploiters,
      forgiveness, small exploration ε, and occasional reboots after sustained failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.L_window = 5
        self.epsilon = 0.03
        self.p_rec = 0.8
        self.p_rec_generous = 0.5
        self.Punish_len = 1
        self.T_fail = 3
        self.n_opp = max(0, self.n - 1)
        self.exploit_counts = np.zeros(self.n_opp, dtype=int)
        self.my_punish_timer = 0
        self.last_punish_round = -999

    def _clamp(self, x, a, b):
        return max(a, min(b, x))

    def _poisson_binomial_ge(self, probs, threshold):
        """
        Return Pr[sum Bernoulli(probs) >= threshold].
        Use exact convolution for moderate sized lists (<=40), otherwise normal approx.
        probs: iterable of floats in [0,1]
        threshold: integer
        """
        probs = list(probs)
        n = len(probs)
        if n == 0:
            return 1.0 if 0 >= threshold else 0.0
        if n <= 40:
            prob = np.zeros(n + 1, dtype=float)
            prob[0] = 1.0
            for p in probs:
                new = np.zeros_like(prob)
                new[0] = prob[0] * (1.0 - p)
                for s in range(1, n + 1):
                    new[s] = prob[s] * (1.0 - p) + prob[s - 1] * p
                prob = new
            if threshold <= 0:
                return 1.0
            if threshold > n:
                return 0.0
            return float(prob[int(threshold):].sum())
        else:
            mean = sum(probs)
            var = sum((p * (1.0 - p) for p in probs))
            if var <= 1e-12:
                total = int(round(mean))
                return 1.0 if total >= threshold else 0.0
            std = math.sqrt(var)
            z = (threshold - 0.5 - mean) / std
            Phi = 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))
            return max(0.0, min(1.0, 1.0 - Phi))

    def _detect_exploiters_and_update(self, history: PlayerHistory):
        """
        Scan recent history for rounds where group succeeded (C_s >= m) and some opponents defected.
        Increment exploit_counts for opponents who defected during success rounds.
        If any opponent's exploit_count exceeds threshold (2 within window), trigger a short punishment.
        """
        rounds = history.round_number
        if rounds == 0:
            return
        L = min(self.L_window, rounds)
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        coop_counts = []
        for rr in range(rounds - L, rounds):
            opp_coops = int((opponent_actions_row_sum := (int(np.sum(opp_actions[rr, :])) if True else 0)))
        coop_counts = np.sum(opp_actions[-L:, :], axis=1) + history.my_actions[-L:].astype(int)
        for i in range(L):
            r_idx = rounds - L + i
            total_coops = int(coop_counts[i])
            if total_coops >= self.m:
                defections = ~opp_actions[r_idx, :]
                self.exploit_counts += defections.astype(int)
        recent_defected = np.zeros(self.n_opp, dtype=int)
        if L > 0:
            recent_defected = (~opp_actions[-L:, :]).astype(int).sum(axis=0)
        for j in range(self.n_opp):
            if recent_defected[j] == 0 and self.exploit_counts[j] > 0:
                self.exploit_counts[j] = max(0, self.exploit_counts[j] - 1)
        if self.my_punish_timer <= 0:
            for j in range(self.n_opp):
                if self.exploit_counts[j] >= 2:
                    self.my_punish_timer = self.Punish_len
                    self.exploit_counts[j] = max(0, self.exploit_counts[j] - 2)
                    self.last_punish_round = history.round_number
                    break

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if history is None:
            if self.n_opp > 0:
                p0 = self._clamp((self.m - 1) / max(1, self.n_opp), 0.2, 0.8)
                f_list = [p0] * self.n_opp
            else:
                f_list = []
            P_without = self._poisson_binomial_ge(f_list, self.m)
            P_with = self._poisson_binomial_ge(f_list, max(0, self.m - 1))
            deltaP = P_with - P_without
            if deltaP > 1.0 / self.k:
                return Action.C
            if abs(deltaP - 1.0 / self.k) < 0.05 and random.random() < 0.6:
                return Action.C
            if random.random() < self.epsilon:
                return Action.C
            return Action.D
        rounds_completed = history.round_number
        L = min(self.L_window, rounds_completed)
        f_list = []
        if self.n_opp > 0:
            if rounds_completed == 0:
                p0 = self._clamp((self.m - 1) / max(1, self.n_opp), 0.2, 0.8)
                f_list = [p0] * self.n_opp
            else:
                opp_actions = history.opponent_actions
                window = opp_actions[-L:, :] if L > 0 else opp_actions
                denom = float(L)
                counts = np.sum(window.astype(int), axis=0)
                f_list = list((counts / denom).astype(float))
        else:
            f_list = []
        self._detect_exploiters_and_update(history)
        P_without = self._poisson_binomial_ge(f_list, self.m)
        P_with = self._poisson_binomial_ge(f_list, max(0, self.m - 1))
        deltaP = P_with - P_without
        if self.my_punish_timer > 0:
            self.my_punish_timer = max(0, self.my_punish_timer - 1)
            return Action.D
        if deltaP > 1.0 / self.k:
            return Action.C
        consecutive_failures = 0
        if rounds_completed > 0:
            opp_actions = history.opponent_actions
            my_actions = history.my_actions
            coop_counts = np.sum(opp_actions, axis=1) + my_actions.astype(int)
            for val in coop_counts[::-1]:
                if int(val) < self.m:
                    consecutive_failures += 1
                else:
                    break
        if consecutive_failures >= self.T_fail:
            p_reboot = max(self.epsilon, 0.5 * (self.m / float(self.n)))
            if random.random() < p_reboot:
                return Action.C
            else:
                return Action.D
        if rounds_completed > 0:
            prev_total_coops = int(np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1]))
            if prev_total_coops >= self.m:
                if history.my_actions[-1]:
                    if random.random() < self.p_rec:
                        return Action.C
                    else:
                        return Action.D
                elif random.random() < self.p_rec_generous:
                    return Action.C
                else:
                    return Action.D
        if t >= self.r:
            if deltaP > 1.0 / self.k:
                return Action.C
            else:
                if P_without < 0.2 and P_with > 0.4 and (random.random() < 0.25):
                    return Action.C
                return Action.D
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_23(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Probing and Forgiveness (ACCPF).

    - Signals willingness by cooperating in the first round.
    - Always defects in the final round.
    - Keeps cooperating after a successful round (>= m cooperators).
    - Continues cooperating if you were one of the contributors in a near-miss (m-1).
    - Cooperates if recent rolling average cooperators >= m.
    - Forgives once after a recent success (short window).
    - Probes (single cooperative move) after repeated failures, with a minimum interval between probes.
    - Defaults to defect to avoid prolonged exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.r = int(game_description.n_rounds)
        self.n = int(game_description.n_players)
        self.m = int(game_description.m)
        self.w = min(5, max(1, self.r - 1))
        self.probe_after_failures = 2
        self.probe_interval = 3
        self.forgiveness_length = 2
        self.last_probe_round = -9999
        self.last_forgive_round = -9999

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        rounds_completed = int(history.round_number)
        if rounds_completed <= 0:
            return Action.C
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        coop_counts = opp_actions.sum(axis=1).astype(int) + my_actions.astype(int)
        prev_idx = rounds_completed - 1
        c_prev = int(coop_counts[prev_idx])
        consecutive_failures = 0
        for val in coop_counts[::-1]:
            if int(val) < self.m:
                consecutive_failures += 1
            else:
                break
        use_w = min(self.w, rounds_completed)
        avg_coop = float(np.mean(coop_counts[-use_w:])) if use_w > 0 else 0.0
        success_indices = np.where(coop_counts >= self.m)[0]
        last_success_round = int(success_indices[-1] + 1) if success_indices.size > 0 else -9999
        if c_prev >= self.m:
            return Action.C
        if c_prev == self.m - 1 and bool(my_actions[prev_idx]):
            return Action.C
        if avg_coop >= self.m:
            return Action.C
        if last_success_round > 0:
            if rounds_completed - last_success_round <= self.forgiveness_length and c_prev < self.m and (self.last_forgive_round < last_success_round):
                self.last_forgive_round = t
                return Action.C
        rounds_since_probe = t - self.last_probe_round
        if consecutive_failures >= self.probe_after_failures and rounds_since_probe >= self.probe_interval:
            self.last_probe_round = t
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_24(BaseStrategy):
    """
    Rotating Minimal-Cooperator with Conditional Recovery and Reliability (RMCR)

    - Implements a deterministic rotating schedule of exactly m cooperators per round (0-indexed players).
    - Tracks per-player reliability from public history (fraction of times a player cooperated when scheduled),
      computed over a sliding window of recent scheduled turns (W).
    - Modes: NORMAL, RECOVERY, PUNISH. Modes are deduced deterministically by simulating the
      public-history-driven state machine up to the current round.
    - Recovery tries to add the minimal number of backups (chosen by reliability) to restore threshold.
    - Short punishments follow repeated failed recoveries. Forgiving after a successful threshold round.
    - Final-round rule: cooperate only if scheduled/backup AND recent success rate is high enough.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.R_min = 0.5
        self.R_backup_min = 0.4
        self.F = 3
        self.P = 2
        self.T_success = 0.6
        self.W_global = min(20, max(1, game_description.n_rounds))
        self.MODE_NORMAL = 'NORMAL'
        self.MODE_RECOVERY = 'RECOVERY'
        self.MODE_PUNISH = 'PUNISH'

    def _sched_set(self, t: int):
        """Return set of scheduled cooperators for round t (1-indexed), using 0-indexed player ids."""
        n = self.game_description.n_players
        m = self.game_description.m
        base = (t - 1) * m % n
        return {(base + j) % n for j in range(m)}

    def _reconstruct_round_actions(self, history: PlayerHistory, s_idx: int):
        """
        Reconstruct full action vector for round index s_idx (0-indexed).
        Player ordering: self = player 0, opponents are players 1..n-1 in the order of columns.
        Returns a list of booleans length n (True => C, False => D).
        """
        n = self.game_description.n_players
        my_act = bool(history.my_actions[s_idx])
        if history.opponent_actions.shape[1] != max(0, n - 1):
            opps = [bool(x) for x in history.opponent_actions[s_idx, :min(history.opponent_actions.shape[1], max(0, n - 1))]]
            while len(opps) < max(0, n - 1):
                opps.append(False)
        else:
            opps = [bool(x) for x in history.opponent_actions[s_idx, :]]
        return [my_act] + opps

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            t = 1
        else:
            t = getattr(state, 'round_number', history.round_number + 1)
        S_t = self._sched_set(t)
        if history is None or history.round_number == 0:
            if 0 in S_t:
                return Action.C
            else:
                return Action.D
        W = self.W_global
        scheduled_seen_window = {p: [] for p in range(n)}
        mode = self.MODE_NORMAL
        consecutive_failed_recoveries = 0
        punish_remaining = 0
        rounds_done = history.round_number
        for s in range(1, rounds_done + 1):
            s_idx = s - 1
            S_s = self._sched_set(s)
            actions_all = self._reconstruct_round_actions(history, s_idx)
            coop_count = sum((1 for a in actions_all if a))
            success = coop_count >= m
            for p in S_s:
                scheduled_seen_window[p].insert(0, bool(actions_all[p]))
                if len(scheduled_seen_window[p]) > W:
                    scheduled_seen_window[p].pop()
            if success:
                mode = self.MODE_NORMAL
                consecutive_failed_recoveries = 0
                punish_remaining = 0
            elif mode == self.MODE_RECOVERY:
                consecutive_failed_recoveries += 1
                if consecutive_failed_recoveries >= self.F:
                    mode = self.MODE_PUNISH
                    punish_remaining = self.P
                    consecutive_failed_recoveries = 0
            elif mode == self.MODE_NORMAL:
                mode = self.MODE_RECOVERY
                consecutive_failed_recoveries = 1
            elif mode == self.MODE_PUNISH:
                if punish_remaining > 0:
                    punish_remaining -= 1
                if punish_remaining <= 0:
                    mode = self.MODE_NORMAL
                    punish_remaining = 0
        R = {}
        for p in range(n):
            lst = scheduled_seen_window.get(p, [])
            if len(lst) > 0:
                R[p] = float(sum((1 for x in lst if x)) / float(len(lst)))
            else:
                R[p] = 0.5
        recent_check_len = min(W, rounds_done)
        recent_successes = 0
        if recent_check_len > 0:
            for s_idx in range(rounds_done - recent_check_len, rounds_done):
                actions_all = self._reconstruct_round_actions(history, s_idx)
                if sum((1 for a in actions_all if a)) >= m:
                    recent_successes += 1
            recent_success_frac = recent_successes / float(recent_check_len)
        else:
            recent_success_frac = 0.0
        if mode == self.MODE_PUNISH and punish_remaining > 0:
            return Action.D
        reliable_scheduled_count = sum((1 for p in S_t if R.get(p, 0.5) >= self.R_min))
        last_round_idx = rounds_done - 1
        if last_round_idx >= 0:
            last_actions = self._reconstruct_round_actions(history, last_round_idx)
            c_prev = sum((1 for a in last_actions if a))
        else:
            c_prev = 0

        def compute_backups(S_current, needed):
            candidates = [p for p in range(n) if p not in S_current]
            candidates.sort(key=lambda x: (-R.get(x, 0.5), x))
            good = [p for p in candidates if R.get(p, 0.5) >= self.R_backup_min]
            backups = []
            for p in good:
                if len(backups) >= needed:
                    break
                backups.append(p)
            if len(backups) < needed:
                for p in candidates:
                    if p in backups:
                        continue
                    backups.append(p)
                    if len(backups) >= needed:
                        break
            return set(backups)
        if t == r:
            will_cooperate = False
            if reliable_scheduled_count >= m:
                will_cooperate = 0 in S_t
            else:
                needed = max(0, m - c_prev)
                backups = compute_backups(S_t, needed)
                will_cooperate = 0 in S_t or 0 in backups
            if will_cooperate and recent_success_frac >= self.T_success:
                return Action.C
            else:
                return Action.D
        if mode == self.MODE_NORMAL:
            if reliable_scheduled_count >= m:
                return Action.C if 0 in S_t else Action.D
            else:
                mode_effective = self.MODE_RECOVERY
        else:
            mode_effective = mode
        if mode_effective == self.MODE_RECOVERY:
            needed = max(0, m - c_prev)
            backups = compute_backups(S_t, needed)
            if 0 in S_t or 0 in backups:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_25(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating to try to establish a threshold-cooperation convention.
    - Continues cooperating if the group just met the threshold.
    - If the group failed last round, estimates other players' cooperation probabilities
      using recency-weighted per-player rates and cooperates if, by that estimate,
      your cooperation would be expected to reach the threshold.
    - Otherwise defects but with a decaying probabilistic probe to re-establish cooperation.
    - Applies short temporary reputation penalties to players who defect while the
      group still reached the threshold (they "exploit" the public good).
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.beta = 0.25
        self.decay = 0.6
        self.punish_duration = 3
        self.punish_amount = 0.5
        self.p0_min = 0.15
        self.p0_max = 0.75

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        n_others = max(0, n - 1)
        raw_p0 = 0.2 + 0.6 * (k / (k + 1.0)) * ((n - m) / max(1, n - 1))
        p0 = min(self.p0_max, max(self.p0_min, raw_p0))
        if history is None or history.round_number == 0:
            if r == 1:
                return Action.D
            return Action.C
        current_round = history.round_number + 1
        if current_round == r:
            return Action.D
        f = [0.5] * n_others
        punishment_timers = [0] * n_others
        F = 0
        last_success = False
        rounds_completed = history.round_number
        for t in range(rounds_completed):
            my_action = bool(history.my_actions[t])
            opp_actions_row = history.opponent_actions[t]
            L = int(my_action) + int(np.sum(opp_actions_row))
            observed_list = [1 if bool(opp_actions_row[j]) else 0 for j in range(n_others)]
            for j in range(n_others):
                if punishment_timers[j] > 0:
                    punishment_timers[j] -= 1
                observed = observed_list[j]
                f[j] = (1.0 - self.beta) * f[j] + self.beta * observed
            if L >= m:
                for j in range(n_others):
                    if observed_list[j] == 0:
                        f[j] = max(0.0, f[j] - self.punish_amount)
                        punishment_timers[j] = self.punish_duration
            if L >= m:
                F = 0
                last_success = True
            else:
                F += 1
                last_success = False
        last_round_opp_actions = history.opponent_actions[-1]
        last_round_my = bool(history.my_actions[-1])
        last_L = int(last_round_my) + int(np.sum(last_round_opp_actions))
        if last_L >= m:
            return Action.C
        S = sum(f)
        if S >= m - 1:
            return Action.C
        p_probe = p0 * self.decay ** F
        if random.random() < p_probe:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_26(BaseStrategy):
    """
    History-weighted probabilistic conditional-cooperation strategy for the
    Collective Risk Dilemma. Maintains per-opponent reputations (exponentially
    weighted), detects and temporarily punishes persistent free-riders,
    computes exact Poisson-Binomial probabilities for others' cooperations,
    and cooperates exactly when doing so meaningfully increases the chance
    of reaching the group threshold (with modest generosity, forgiveness,
    and a small exploration rate).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.8
        self.p0 = max(0.0, min(1.0, float(self.m) / max(1, self.n)))
        self.eps_explore = 0.02
        self.punish_thresh = 0.4
        self.punish_len = 3
        self.punish_scale = 0.3
        self.final_round_guard = 1
        self.margin_coop = -0.05
        self.W_recent = 5
        self.num_opp = max(0, self.n - 1)
        self.scores = np.full(self.num_opp, self.p0, dtype=float)
        self.punish_timers = np.zeros(self.num_opp, dtype=int)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.num_opp == 0:
            return Action.D

        def poisson_binomial_pmf(p_list):
            num = len(p_list)
            prob = np.zeros(num + 1, dtype=float)
            prob[0] = 1.0
            for p in p_list:
                new = np.zeros_like(prob)
                for s in range(0, len(prob)):
                    if prob[s] == 0.0:
                        continue
                    new[s] += prob[s] * (1.0 - p)
                    if s + 1 < len(prob):
                        new[s + 1] += prob[s] * p
                prob = new
            return prob
        if history is None or history.round_number == 0:
            p_list = np.full(self.num_opp, self.p0, dtype=float)
            self.punish_timers = np.maximum(self.punish_timers - 1, 0)
        else:
            t_completed = history.round_number
            if t_completed > 0:
                if self.alpha == 1.0:
                    exponents = np.ones(t_completed, dtype=float)
                else:
                    exponents = (self.alpha ** (t_completed - 1 - np.arange(t_completed))).astype(float)
                W = float(np.sum(exponents))
                actions = history.opponent_actions[:t_completed, :]
                weighted = np.dot(exponents, actions.astype(float))
                new_scores = weighted / (W if W > 0 else 1.0)
                self.scores = new_scores.astype(float)
            self.punish_timers = np.maximum(self.punish_timers - 1, 0)
            recent_start = max(0, history.round_number - self.W_recent)
            if recent_start < history.round_number:
                recent_slice = history.opponent_actions[recent_start:history.round_number, :]
                recent_frac = np.mean(recent_slice.astype(float), axis=0)
            else:
                recent_frac = self.scores.copy()
            for j in range(self.num_opp):
                if recent_frac[j] < self.punish_thresh:
                    self.punish_timers[j] = self.punish_len
            p_list = self.scores.copy()
            for j in range(self.num_opp):
                if self.punish_timers[j] > 0:
                    p_list[j] = p_list[j] * self.punish_scale
        p_list = np.maximum(p_list, self.eps_explore)
        p_list = np.minimum(p_list, 1.0 - self.eps_explore)
        pmf = poisson_binomial_pmf(list(p_list))

        def tail_prob_at_leq(index):
            if index <= 0:
                return 1.0
            if index > self.num_opp:
                return 0.0
            return float(np.sum(pmf[index:]))
        P_others_geq_m = tail_prob_at_leq(self.m)
        P_others_geq_m_1 = tail_prob_at_leq(self.m - 1)
        P_S_eq_m_1 = 0.0
        if 0 <= self.m - 1 <= self.num_opp:
            P_S_eq_m_1 = float(pmf[self.m - 1])
        payoff_C = self.k * P_others_geq_m_1
        payoff_D = 1.0 + self.k * P_others_geq_m
        delta = payoff_C - payoff_D
        high_coop_frac = float(np.mean(self.scores >= 0.8)) if self.num_opp > 0 else 0.0
        low_coop_frac = float(np.mean(self.scores < 0.4)) if self.num_opp > 0 else 0.0
        threshold_margin = float(self.margin_coop)
        if low_coop_frac >= 0.5:
            threshold_margin = max(threshold_margin, 0.2)
        if t > self.r - self.final_round_guard:
            threshold_margin = max(threshold_margin, 0.0)
        choose_cooperate = False
        if delta >= threshold_margin:
            choose_cooperate = True
        elif high_coop_frac >= 0.6 and -0.2 < delta < 0.0:
            if random.random() < 0.5:
                choose_cooperate = True
            else:
                choose_cooperate = False
        else:
            choose_cooperate = False
        if random.random() < self.eps_explore:
            choose_cooperate = not choose_cooperate
        return Action.C if choose_cooperate else Action.D

class Strategy_COOPERATIVE_27(BaseStrategy):
    """
    Adaptive, Forgiving Conditional Contributor (AFCC)

    - Forms per-opponent beliefs from a sliding window of recent behavior.
    - Computes probability the group's cooperation threshold is met if it cooperates vs defects.
    - Chooses the action with the higher myopic expected payoff, with a small bias toward cooperation (eps).
    - Tracks "threshold-killer" events (defections in rounds with exactly m-1 cooperators) from history;
      players with repeated such events (>= B within the ageing window D) are treated as untrustworthy (p_j = 0).
    - First round: cooperate. Forgiving: uses sliding window and ageing of punishments.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W_max = 10
        self.eps = 0.03
        self.B = 2
        self.D = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1
        W = min(self.W_max, max(1, t - 1))
        n_opponents = self.n - 1
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        offense_rounds = [[] for _ in range(n_opponents)]
        for s in range(completed):
            total_coops = int(np.sum(opp_actions[s, :])) + (1 if bool(my_actions[s]) else 0)
            if total_coops == self.m - 1:
                for j in range(n_opponents):
                    if not bool(opp_actions[s, j]):
                        offense_rounds[j].append(s)
        aged_offense_counts = []
        age_cutoff = completed - self.D
        for j in range(n_opponents):
            count = sum((1 for s in offense_rounds[j] if s >= age_cutoff))
            aged_offense_counts.append(count)
        p_vector = []
        for j in range(n_opponents):
            if completed == 0:
                p_j = min(0.6, self.m / self.n + 0.1)
            else:
                start_idx = max(0, completed - W)
                recent = opp_actions[start_idx:completed, j] if completed > 0 else np.array([], dtype=bool)
                num_C = int(np.sum(recent)) if recent.size > 0 else 0
                p_j = float(num_C) / float(W)
                if aged_offense_counts[j] >= self.B:
                    p_j = 0.0
            p_vector.append(p_j)

        def pb_threshold(p_list, T):
            m_len = len(p_list)
            if T <= 0:
                return 1.0
            if m_len == 0:
                return 0.0
            if m_len > 50:
                mu = sum(p_list)
                var = sum((p * (1.0 - p) for p in p_list))
                if var <= 0.0:
                    return 1.0 if mu >= T else 0.0
                z = (T - 0.5 - mu) / math.sqrt(var)
                tail = 0.5 * (1.0 - math.erf(z / math.sqrt(2.0)))
                if tail < 0.0:
                    tail = 0.0
                if tail > 1.0:
                    tail = 1.0
                return tail
            dp = np.zeros(m_len + 1, dtype=float)
            dp[0] = 1.0
            for p in p_list:
                for s in range(m_len, -1, -1):
                    a = dp[s]
                    b = dp[s - 1] if s - 1 >= 0 else 0.0
                    dp[s] = a * (1.0 - p) + (b * p if s - 1 >= 0 else 0.0)
            tail_prob = float(np.sum(dp[T:]))
            if tail_prob < 0.0:
                tail_prob = 0.0
            if tail_prob > 1.0:
                tail_prob = 1.0
            return tail_prob
        P_success_if_C = pb_threshold(p_vector, max(0, self.m - 1))
        P_success_if_D = pb_threshold(p_vector, max(0, self.m))
        EU_C = self.k * P_success_if_C
        EU_D = 1.0 + self.k * P_success_if_D
        if EU_C >= EU_D + self.eps:
            return Action.C
        if EU_D >= EU_C + self.eps:
            return Action.D
        recent_start = max(0, completed - W)
        recent_range = range(recent_start, completed)
        successes = 0
        denom = W if W > 0 else 1
        for s in recent_range:
            total_coops = int(np.sum(opp_actions[s, :])) + (1 if bool(my_actions[s]) else 0)
            if total_coops >= self.m:
                successes += 1
        recent_group_success_rate = float(successes) / float(denom) if denom > 0 else 0.0
        if t <= self.r / 2.0 or recent_group_success_rate >= 0.6:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_28(BaseStrategy):
    """
    Adaptive Conditional Contributor (ACC)

    Tries to secure the collective reward when a single contribution meaningfully
    affects the probability of reaching the threshold, while avoiding being a
    persistent sucker. Uses empirical cooperation probabilities of opponents
    (over a sliding window), exact Poisson-Binomial (or normal approx for large
    opponent counts), a small pro-cooperation bias (eps), short targeted punishments,
    reciprocity for preserving successful cooperation, and forgiveness via
    limited punishment durations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.eps = 0.02
        self.L_default = 50
        self.punish_duration = 3
        self.exploit_threshold_frac = 0.3
        n_players = int(game_description.n_players)
        self.n_opp = max(0, n_players - 1)
        self.punishment_end_round = np.zeros(self.n_opp, dtype=int)

    def _poisson_binomial_exact(self, ps):
        """
        Compute distribution Prob_exact[s] = probability exactly s successes among independent Bernoulli with probs ps.
        ps: iterable of probabilities length N
        returns numpy array length N+1
        """
        N = len(ps)
        prob = np.zeros(N + 1, dtype=float)
        prob[0] = 1.0
        for p in ps:
            prev = prob.copy()
            prob[0] = prev[0] * (1 - p)
            for s in range(1, N + 1):
                prob[s] = prev[s] * (1 - p) + prev[s - 1] * p
        return prob

    def _normal_approx_ge(self, ps, k):
        """
        Approximate Prob(S >= k) where S = sum Bernoulli(ps) using normal approximation with continuity correction.
        If variance is zero, fallback to deterministic.
        """
        mu = float(np.sum(ps))
        var = float(np.sum(np.array(ps) * (1.0 - np.array(ps))))
        if var <= 0:
            s = int(round(mu))
            return 1.0 if s >= k else 0.0
        sigma = math.sqrt(var)
        z = (k - 0.5 - mu) / sigma
        prob = 0.5 * (1.0 - math.erf(z / math.sqrt(2.0)))
        if prob < 0.0:
            prob = 0.0
        if prob > 1.0:
            prob = 1.0
        return prob

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k_reward = float(self.game_description.k)
        t = int(state.round_number)
        if history is None:
            return Action.C
        completed = int(history.round_number)
        if completed == 0:
            return Action.C
        if t == r:
            unanimous_my = bool(np.all(history.my_actions))
            unanimous_opponents = False
            if self.n_opp == 0:
                unanimous_opponents = True
            else:
                unanimous_opponents = bool(np.all(history.opponent_actions))
            if unanimous_my and unanimous_opponents:
                return Action.C
            else:
                return Action.D
        L = min(self.L_default, completed)
        start_idx = completed - L
        if self.n_opp > 0:
            opp_win = history.opponent_actions[start_idx:completed, :]
            p_j_raw = np.sum(opp_win.astype(float), axis=0) / float(L)
        else:
            p_j_raw = np.array([], dtype=float)
        my_win = history.my_actions[start_idx:completed]
        if self.n_opp > 0:
            opp_coop_counts_per_round = np.sum(opp_win.astype(int), axis=1)
        else:
            opp_coop_counts_per_round = np.zeros(L, dtype=int)
        total_cooperators_per_round = opp_coop_counts_per_round + my_win.astype(int)
        threshold_met_mask = total_cooperators_per_round >= m
        threshold_met_count = int(np.sum(threshold_met_mask))
        if self.n_opp > 0 and threshold_met_count > 0:
            if threshold_met_count > 0:
                opp_when_threshold = opp_win[threshold_met_mask, :]
                defections_when_threshold = np.sum((~opp_when_threshold).astype(int), axis=0)
            else:
                defections_when_threshold = np.zeros(self.n_opp, dtype=int)
            frac = defections_when_threshold / float(threshold_met_count) if threshold_met_count > 0 else np.zeros(self.n_opp, dtype=float)
            for j in range(self.n_opp):
                if frac[j] >= self.exploit_threshold_frac:
                    proposed_end = t + self.punish_duration
                    if proposed_end > int(self.punishment_end_round[j]):
                        self.punishment_end_round[j] = proposed_end
        p_j_effective = []
        for j in range(self.n_opp):
            if self.punishment_end_round[j] > t:
                p_j_effective.append(0.0)
            else:
                p_j_effective.append(float(p_j_raw[j]))
        p_j_effective = np.array(p_j_effective, dtype=float)
        if self.n_opp == 0:
            if m <= 1:
                E_C = k_reward
                E_D = 1.0
                if E_C >= E_D - self.eps:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        if np.all(p_j_effective >= 1.0 - 1e-12):
            return Action.C
        if np.all(p_j_effective <= 1e-12):
            return Action.D
        use_normal = self.n_opp > 60
        if not use_normal:
            prob_exact = self._poisson_binomial_exact(p_j_effective.tolist())
            if m <= 0:
                P_without_me = 1.0
            elif m > self.n_opp:
                P_without_me = 0.0
            else:
                P_without_me = float(np.sum(prob_exact[m:]))
            if m - 1 <= 0:
                P_with_me = 1.0
            elif m - 1 > self.n_opp:
                P_with_me = 0.0
            else:
                P_with_me = float(np.sum(prob_exact[max(0, m - 1):]))
        else:
            if m <= 0:
                P_without_me = 1.0
            elif m > self.n_opp:
                P_without_me = 0.0
            else:
                P_without_me = self._normal_approx_ge(p_j_effective, m)
            if m - 1 <= 0:
                P_with_me = 1.0
            elif m - 1 > self.n_opp:
                P_with_me = 0.0
            else:
                P_with_me = self._normal_approx_ge(p_j_effective, m - 1)
        E_C = k_reward * P_with_me
        E_D = 1.0 + k_reward * P_without_me
        last_round_threshold_met = False
        last_round_i_cooperated = False
        if completed >= 1:
            opp_last = history.opponent_actions[-1, :] if self.n_opp > 0 else np.array([], dtype=bool)
            coop_count_last = int(np.sum(opp_last.astype(int))) + int(history.my_actions[-1])
            last_round_threshold_met = coop_count_last >= m
            last_round_i_cooperated = bool(history.my_actions[-1])
        cooperate_flag = False
        if E_C >= E_D - self.eps:
            cooperate_flag = True
        if last_round_threshold_met and last_round_i_cooperated:
            cooperate_flag = True
        marginal = P_with_me - P_without_me
        if marginal < 1e-10 and E_C < E_D - self.eps:
            cooperate_flag = False
        return Action.C if cooperate_flag else Action.D

class Strategy_COOPERATIVE_29(BaseStrategy):
    """
    Adaptive Threshold Cooperation with Forgiveness (ATCF).

    - Starts by cooperating to signal willingness to reach the public-good threshold.
    - Uses a sliding window of recent rounds to determine whether group cooperation
      is reliable (success_rate) and whether this player's cooperations were often wasted.
    - If the group is doing well (success_rate >= good_rate and wasted_fraction <= wasted_threshold),
      remain cooperative. Otherwise enter defensive mode: defect except for periodic probe rounds.
    - Always defect in the final round.
    - Deterministic, uses only publicly observable history and game parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.w = int(min(10, max(0, self.r - 1)))
        self.good_rate = 0.6
        self.wasted_threshold = 0.5
        self.probe_interval = int(max(3, math.ceil(self.r / 10)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == self.r:
            return Action.D
        if history is None:
            return Action.C
        completed_rounds = history.round_number
        window_size = min(self.w, completed_rounds)
        if window_size <= 0:
            success_rate = 0.0
            wasted_fraction = 0.0
        else:
            my_actions_window = history.my_actions[-window_size:]
            opp_sum_window = np.sum(history.opponent_actions[-window_size:, :], axis=1)
            total_coops_window = opp_sum_window + my_actions_window.astype(int)
            success_rounds = (total_coops_window >= self.m).astype(int)
            success_rate = float(np.mean(success_rounds)) if window_size > 0 else 0.0
            my_coop_count = int(np.sum(my_actions_window.astype(int)))
            if my_coop_count == 0:
                wasted_fraction = 0.0
            else:
                wasted_count = int(np.sum((my_actions_window & (success_rounds == 0)).astype(int)))
                wasted_fraction = wasted_count / my_coop_count
        cooperative_mode = success_rate >= self.good_rate and wasted_fraction <= self.wasted_threshold
        if cooperative_mode:
            return Action.C
        if (t - 1) % self.probe_interval == 0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_30(BaseStrategy):
    """
    Threshold-Reciprocal (TR)

    A conditional-cooperation strategy for the Collective Risk Dilemma.
    - Starts by cooperating.
    - Defects on the final round.
    - Tries to sustain successful coalitions and rescue narrow misses.
    - Uses a short lookback window W = min(5, r-1) to estimate each opponent's recent reliability.
    - Marks players who defected while the threshold was met as "exploiters" and withholds trusting them
      until they produce F consecutive cooperations (forgiveness).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.F = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        T = history.round_number
        if T >= r:
            return Action.D
        if T == r - 1:
            return Action.D
        W = min(5, max(1, r - 1))
        opp_actions = history.opponent_actions
        num_opponents = opp_actions.shape[1]
        exploiter = [False] * num_opponents
        redemption = [self.F] * num_opponents
        for rr in range(T):
            me_act = bool(history.my_actions[rr])
            opps_act_round = opp_actions[rr, :]
            coop_count = int(me_act) + int(np.sum(opps_act_round))
            for j in range(num_opponents):
                acted_coop = bool(opps_act_round[j])
                if exploiter[j]:
                    if acted_coop:
                        redemption[j] = min(self.F, redemption[j] + 1)
                        if redemption[j] >= self.F:
                            exploiter[j] = False
                            redemption[j] = self.F
                    else:
                        redemption[j] = 0
                elif not acted_coop and coop_count >= m:
                    exploiter[j] = True
                    redemption[j] = 0
        start_idx = max(0, T - W)
        if start_idx < T:
            recent_window = opp_actions[start_idx:T, :]
            S_raw = np.sum(recent_window, axis=0).astype(int)
        else:
            S_raw = np.zeros((num_opponents,), dtype=int)
        S_effective = np.array([0] * num_opponents, dtype=int)
        for j in range(num_opponents):
            if exploiter[j] and redemption[j] < self.F:
                S_effective[j] = 0
            else:
                S_effective[j] = int(S_raw[j])
        threshold_for_reliable = math.ceil(W / 2)
        reliable_mask = [S_effective[j] >= threshold_for_reliable and (not (exploiter[j] and redemption[j] < self.F)) for j in range(num_opponents)]
        reliable_count = int(sum(reliable_mask))
        last_round_idx = T - 1
        last_me_coop = bool(history.my_actions[last_round_idx])
        last_opponent_coops = np.array(history.opponent_actions[last_round_idx, :], dtype=bool)
        C_prev_count = int(last_me_coop) + int(np.sum(last_opponent_coops))
        if C_prev_count >= m and last_me_coop:
            return Action.C
        if reliable_count >= m - 1:
            return Action.C
        if C_prev_count == m - 1:
            coopers_last_round_excl_me = int(np.sum(last_opponent_coops))
            reliable_coopers = 0
            for j in range(num_opponents):
                if last_opponent_coops[j] and reliable_mask[j]:
                    reliable_coopers += 1
            recent_nonzero = int(np.sum(S_effective > 0))
            condA = coopers_last_round_excl_me >= m - 1 and reliable_coopers >= max(0, m - 2)
            condB = recent_nonzero >= m - 1
            if condA or condB:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_31(BaseStrategy):
    """
    Responsive Threshold with Reliability (RTR)

    - Tracks recent cooperation frequency of each opponent (reliability).
    - Seeds cooperation for initial G rounds.
    - Cooperates when likely pivotal (helps reach threshold m) or to build a near-pivotal coalition.
    - Defects when others alone are likely to reach the threshold or when cooperation is unlikely.
    - Uses a sliding window W and cutoff p to mark opponents reliable.
    - Win-stay: if last round succeeded and we cooperated, continue cooperating.
    - Endgame: defects in the final E rounds.
    - Optional small exploration probability epsilon to probe cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        r = self.game_description.n_rounds
        self.W = min(5, r)
        self.p = 0.75
        self.G = min(3, max(1, r - 1))
        self.E = 1
        self.epsilon = 0.03

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', 1)
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None or history.round_number == 0:
            return Action.C
        completed_rounds = history.round_number
        if t > r - self.E:
            return Action.D
        if t <= self.G:
            return Action.C
        if completed_rounds >= 1:
            last_opponent_actions = history.opponent_actions[-1, :]
            last_my_action = bool(history.my_actions[-1])
            coopers_last_round = int(np.sum(last_opponent_actions)) + (1 if last_my_action else 0)
            if coopers_last_round >= m and last_my_action:
                return Action.C
        lastW = min(self.W, completed_rounds)
        if lastW <= 0:
            return Action.C
        recent_opponent_actions = history.opponent_actions[-lastW:, :]
        coop_counts = np.sum(recent_opponent_actions.astype(float), axis=0)
        coop_rates = coop_counts / float(lastW)
        countR = int(np.sum(coop_rates >= self.p))
        if countR >= m:
            return Action.D
        if countR >= m - 1:
            return Action.C
        if countR == m - 2 and t <= r - self.E - 1:
            return Action.C
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_32(BaseStrategy):
    """
    Pivotal-EWMA strategy for the Collective Risk Dilemma.

    - Maintains per-opponent cooperation probabilities p_j via EWMA (alpha).
    - Computes the Poisson-Binomial pmf of the number of other cooperators each round.
    - Cooperates when k * P_pivotal >= 1 - g (generosity bias), with occasional exploration.
    - Forgives sustained cooperation by boosting p_j after T_forgive consecutive cooperative rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.5
        self.alpha = 0.3
        self.epsilon_explore = 0.02
        self.g = 0.02
        self.T_forgive = 5
        n = int(game_description.n_players)
        self.n = n
        self.others_count = max(0, n - 1)
        self.p_js = [self.p0 for _ in range(self.others_count)]
        self.coop_streaks = [0 for _ in range(self.others_count)]
        self.last_update_round = 0
        self._min_p = 1e-06
        self._max_p = 1.0 - 1e-06

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is not None:
            total_rounds = history.round_number
            if total_rounds > self.last_update_round:
                for r_idx in range(self.last_update_round, total_rounds):
                    row = history.opponent_actions[r_idx]
                    for j in range(self.others_count):
                        acted_coop = bool(row[j])
                        prev = self.p_js[j]
                        new_p = (1.0 - self.alpha) * prev + self.alpha * (1.0 if acted_coop else 0.0)
                        if acted_coop:
                            self.coop_streaks[j] += 1
                        else:
                            self.coop_streaks[j] = 0
                        if self.coop_streaks[j] >= self.T_forgive:
                            new_p = max(new_p, 0.8)
                        new_p = max(self._min_p, min(self._max_p, new_p))
                        self.p_js[j] = new_p
                self.last_update_round = total_rounds
        if random.random() < self.epsilon_explore:
            return Action.C
        others = self.others_count
        if others == 0:
            return Action.D
        pmf = [0.0] * (others + 1)
        pmf[0] = 1.0
        for p in self.p_js:
            new_pmf = [0.0] * (others + 1)
            new_pmf[0] = pmf[0] * (1.0 - p)
            for x in range(1, others + 1):
                new_pmf[x] = pmf[x] * (1.0 - p) + pmf[x - 1] * p
            pmf = new_pmf
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        idx_piv = m - 1
        if idx_piv < 0 or idx_piv > others:
            P_pivotal = 0.0
        else:
            P_pivotal = pmf[idx_piv]
        start_coop = max(0, m - 1)
        P_succ_if_coop = sum(pmf[start_coop:])
        start_def = max(0, m)
        P_succ_if_defect = sum(pmf[start_def:]) if start_def <= others else 0.0
        if k * P_pivotal >= 1.0 - self.g:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_33(BaseStrategy):
    """
    Pivotal Conditional Cooperator with Forgiving Reciprocity (PCC-FR).

    - Maintains per-opponent beliefs p_j that opponent j will cooperate next round.
    - Updates p_j with an exponential moving average (recency α) after each observed round.
    - If a round ends in collective failure (total cooperators < m), applies a punishment
      shrink γ to opponents who defected in that failed round.
    - Before each decision computes P_eq = Prob(exactly m-1 of the other players cooperate)
      via the exact Poisson-binomial convolution using current beliefs (optionally with a
      small optimism δ). Plays C iff k * P_eq + ε >= 1 (ties break to cooperation).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = game_description.n_players
        m = game_description.m
        if n - 1 > 0:
            self.p0 = float(max(0.0, min(1.0, (m - 1) / (n - 1))))
        else:
            self.p0 = 0.0
        self.alpha = 0.4
        self.gamma = 0.7
        self.epsilon = 1e-06
        self.delta = 0.0
        self._p_opponents = None
        self._last_processed_round = 0

    def _ensure_initialized(self):
        n = self.game_description.n_players
        if self._p_opponents is None:
            if n - 1 > 0:
                self._p_opponents = np.full(shape=(n - 1,), fill_value=self.p0, dtype=float)
            else:
                self._p_opponents = np.array([], dtype=float)
            self._last_processed_round = 0

    def _poisson_binomial_p_eq(self, probs, target_k):
        """
        Exact Poisson-binomial convolution: given a list/array probs of length N,
        compute probability exactly target_k successes.
        Returns 0.0 if target_k is outside [0, N].
        """
        N = len(probs)
        if target_k < 0 or target_k > N:
            return 0.0
        p = [1.0]
        current_max = 0
        for q in probs:
            q = float(max(0.0, min(1.0, q)))
            p.append(0.0)
            for s in range(current_max, -1, -1):
                prob_s = p[s]
                if prob_s != 0.0:
                    p[s + 1] += prob_s * q
                    p[s] = prob_s * (1.0 - q)
            current_max += 1
        if target_k <= current_max:
            return float(p[target_k])
        return 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self._ensure_initialized()
        n = self.game_description.n_players
        m = self.game_description.m
        k = float(self.game_description.k)
        if history is not None:
            rounds_completed = history.round_number
            for r_idx in range(self._last_processed_round, rounds_completed):
                opp_row = history.opponent_actions[r_idx, :]
                coop_indicators = np.asarray(opp_row, dtype=float)
                if self._p_opponents.size > 0:
                    self._p_opponents = (1.0 - self.alpha) * self._p_opponents + self.alpha * coop_indicators
                my_action = bool(history.my_actions[r_idx])
                total_coops = int(my_action) + int(np.sum(coop_indicators))
                if total_coops < m:
                    defect_mask = np.logical_not(opp_row)
                    if defect_mask.any():
                        self._p_opponents[defect_mask] = self._p_opponents[defect_mask] * self.gamma
                if self._p_opponents.size > 0:
                    np.clip(self._p_opponents, 0.0, 1.0, out=self._p_opponents)
            self._last_processed_round = rounds_completed
        others_count = max(0, n - 1)
        target = m - 1
        if others_count == 0 or target < 0 or target > others_count:
            P_eq = 0.0
        else:
            if self.delta != 0.0:
                probs = np.minimum(1.0, self._p_opponents + self.delta).tolist()
            else:
                probs = self._p_opponents.tolist()
            P_eq = self._poisson_binomial_p_eq(probs, target)
        if k * P_eq + self.epsilon >= 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_34(BaseStrategy):
    """
    Reciprocal Threshold Strategy (RTS)

    - Tries to build and sustain cooperation to reach threshold `m`.
    - Starts by signaling cooperation in round 1 (unless history indicates otherwise).
    - Uses a sliding window of recent rounds (W) to judge whether cooperation is stable.
    - If cooperation was stable recently, continue cooperating with small exploration probability.
    - If cooperation has recently failed but the group is often just short of the threshold, attempt a rescue with high probability.
    - If exploited (you cooperated and group failed), set a short punishment period (P rounds) during which defect.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        r = getattr(game_description, 'n_rounds', 2)
        self.W = min(4, max(1, r - 1))
        self.success_fraction = 0.6
        self.P = 2
        self.p_explore = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is not None and history.round_number >= 1:
            try:
                my_last = bool(history.my_actions[-1])
            except Exception:
                my_last = False
            try:
                opp_last_sum = int(np.sum(history.opponent_actions[-1, :]))
            except Exception:
                opp_last_sum = 0
            total_coops_last = int(my_last) + opp_last_sum
            if my_last and total_coops_last < m:
                self.punishment_timer = self.P
        if t == r:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        S_count = min(self.W, history.round_number)
        start_idx = max(0, history.round_number - S_count)
        coops_list = []
        for idx in range(start_idx, history.round_number):
            try:
                my_a = int(history.my_actions[idx])
            except Exception:
                my_a = 0
            try:
                opp_sum = int(np.sum(history.opponent_actions[idx, :]))
            except Exception:
                opp_sum = 0
            coops_list.append(my_a + opp_sum)
        if S_count == 0:
            return Action.D
        successes = sum((1 for x in coops_list if x >= m))
        success_threshold = math.ceil(self.success_fraction * S_count)
        avg_coops = float(sum(coops_list)) / float(S_count)
        last_coops = coops_list[-1]
        I_cooperated_last = bool(history.my_actions[-1])
        if successes >= success_threshold:
            if random.random() < 1.0 - self.p_explore:
                return Action.C
            else:
                return Action.D
        if avg_coops >= m - 0.75:
            if random.random() < 0.8:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_35(BaseStrategy):
    """
    Rotating Minimal Conditional Cooperation with Reciprocity (RMCC-R).

    - Maintains per-opponent reliability estimates (p_j) initialized at 0.5 and
      recent failure counters (fail_j).
    - Uses a deterministic rotation S_t of size m:
        S_t = { j | ((j - 1 + (t - 1)) % n) < m } for players j = 1..n (1-indexed)
      This implementation assumes this agent's index is 1 (first player) in that
      common indexing scheme. Opponents in history are mapped to full player
      indices 2..n.
    - On each call updates internal EMA estimates from newly observed rounds,
      applies punishment/forgiveness heuristics, and then chooses C/D for the
      current round using the rules in the specification.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.my_index = 1
        self.n_opponents = max(0, self.n - 1)
        self.p = np.full(self.n_opponents, 0.5, dtype=float)
        self.fail = np.zeros(self.n_opponents, dtype=int)
        self.alpha = 0.8
        self.F = 2
        self.S_forgive = 1
        self.last_processed_round = 0

    def _scheduled_set(self, t: int):
        """Return set of full player indices (1-indexed) scheduled at round t."""
        n = self.n
        m = self.m
        S = set()
        for j in range(1, n + 1):
            if (j - 1 + (t - 1)) % n < m:
                S.add(j)
        return S

    def _process_new_history(self, history: PlayerHistory):
        """
        Process newly observed rounds from history, updating p and fail.
        history.round_number gives number of completed rounds.
        """
        completed = history.round_number
        if completed <= self.last_processed_round:
            return
        for r_idx in range(self.last_processed_round, completed):
            t_round = r_idx + 1
            S_t = self._scheduled_set(t_round)
            my_action = bool(history.my_actions[r_idx])
            if self.n_opponents > 0:
                opp_actions_row = history.opponent_actions[r_idx, :]
            else:
                opp_actions_row = np.array([], dtype=bool)
            for idx in range(self.n_opponents):
                acted = bool(opp_actions_row[idx])
                self.p[idx] = (1.0 - self.alpha) * (1.0 if acted else 0.0) + self.alpha * self.p[idx]
                full_idx = idx + 2
                if full_idx in S_t:
                    if not acted:
                        self.fail[idx] += 1
                    else:
                        self.fail[idx] = 0
            for idx in range(self.n_opponents):
                if self.fail[idx] >= self.F:
                    self.p[idx] = float(self.p[idx] * 0.5)
            for idx in range(self.n_opponents):
                if self.fail[idx] >= self.F:
                    full_idx = idx + 2
                    consec_coop = 0
                    needed = self.S_forgive
                    rounds_checked = 0
                    for look_back in range(1, min(20, completed) + 1):
                        rr = completed - look_back
                        t_look = rr + 1
                        S_look = self._scheduled_set(t_look)
                        if full_idx in S_look:
                            acted = bool(history.opponent_actions[rr, idx])
                            rounds_checked += 1
                            if acted:
                                consec_coop += 1
                            else:
                                consec_coop = 0
                            if rounds_checked >= needed:
                                break
                    if consec_coop >= needed and rounds_checked >= needed:
                        self.p[idx] = float(0.5 * self.p[idx] + 0.5 * 1.0)
                        self.fail[idx] = 0
        self.last_processed_round = completed

    def __call__(self, state: GameState, history: None | PlayerHistory):
        t = int(state.round_number) if state is not None else 1
        if history is None:
            S_t = self._scheduled_set(t)
            my_scheduled = self.my_index in S_t
            return Action.C if my_scheduled else Action.D
        self._process_new_history(history)
        E_others = float(np.sum(self.p)) if self.n_opponents > 0 else 0.0
        eps_t = min(0.1, 1.0 / max(1.0, float(t)))
        S_t = self._scheduled_set(t)
        my_scheduled = self.my_index in S_t
        if t >= self.r:
            margin_high = 0.8
            if E_others >= self.m - 1 + margin_high:
                return Action.C
            if my_scheduled:
                recent_success = False
                completed = history.round_number
                lookback = min(5, completed)
                for rr_idx in range(completed - lookback, completed):
                    if rr_idx < 0:
                        continue
                    t_look = rr_idx + 1
                    S_look = self._scheduled_set(t_look)
                    if self.my_index in S_look:
                        coop_count = int(np.sum(history.opponent_actions[rr_idx, :])) + (1 if bool(history.my_actions[rr_idx]) else 0)
                        if coop_count >= self.m:
                            recent_success = True
                            break
                if recent_success:
                    return Action.C
            return Action.D
        if E_others >= self.m:
            return Action.D
        if E_others < self.m - 1:
            if my_scheduled:
                completed = history.round_number
                recent_success = False
                lookback = min(6, completed)
                for rr_idx in range(completed - lookback, completed):
                    if rr_idx < 0:
                        continue
                    t_look = rr_idx + 1
                    S_look = self._scheduled_set(t_look)
                    if self.my_index in S_look:
                        coop_count = int(np.sum(history.opponent_actions[rr_idx, :])) + (1 if bool(history.my_actions[rr_idx]) else 0)
                        if coop_count >= self.m:
                            recent_success = True
                            break
                if recent_success:
                    return Action.C
            if random.random() < eps_t:
                return Action.C
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_36(BaseStrategy):
    """
    Adaptive Pivotal Coalition (APC)

    Builds and maintain small reliable coalitions that just reach the threshold m by:
      - estimating opponents' reliability using an exponential moving average,
      - cooperating when pivotal or when assigned to a predicted top-m coalition (with fairness accounting),
      - limited probabilistic exploration to discover partners and graded punishment for opportunistic defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.7
        self.p0 = max(0.0, min(1.0, game_description.m / float(game_description.n_players)))
        self.p_explore_initial = 0.15
        self.p_explore_decay = 0.95
        self.punish_drop = 0.4
        self.beta = 1.0
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if history is None or history.round_number == 0:
            if random.random() < self.p0:
                return Action.C
            else:
                return Action.D
        my_coop_count = int(np.sum(history.my_actions).item())
        n_opponents = self.n - 1
        scores = np.full(n_opponents, self.p0, dtype=float)
        rounds_completed = history.round_number
        for r_idx in range(rounds_completed):
            opp_row = np.array(history.opponent_actions[r_idx, :], dtype=bool)
            my_action_this_round = bool(history.my_actions[r_idx])
            for j in range(n_opponents):
                if opp_row[j]:
                    scores[j] = self.alpha * scores[j] + (1.0 - self.alpha) * 1.0
                else:
                    scores[j] = self.alpha * scores[j] + (1.0 - self.alpha) * 0.0
            opp_coops = int(np.sum(opp_row).item())
            total_coops = opp_coops + (1 if my_action_this_round else 0)
            if total_coops >= self.m and 0 < total_coops < self.n:
                for j in range(n_opponents):
                    if not opp_row[j]:
                        scores[j] = max(0.0, scores[j] - self.punish_drop)
        E_others = float(np.sum(scores).item())
        denom = max(1, rounds_completed)
        s_self_est = min(1.0, my_coop_count / float(denom)) if rounds_completed > 0 else self.p0
        ranking = []
        for j in range(n_opponents):
            ranking.append((float(scores[j]), j, False, j))
        self_index = self.n - 1
        ranking.append((float(s_self_est), self_index, True, None))
        ranking.sort(key=lambda x: (-x[0], x[1]))
        top_m = ranking[:self.m]
        in_predicted_coalition = any((entry[2] for entry in top_m))
        predicted_others_cooperators = sum((1 for entry in top_m if not entry[2]))
        rounds_passed = max(0, t - 1)
        p_explore = self.p_explore_initial * self.p_explore_decay ** rounds_passed
        if t >= self.r:
            p_explore = 0.0
        floored_E = math.floor(E_others)
        if floored_E >= self.m:
            return Action.D
        if floored_E == self.m - 1:
            return Action.C
        if in_predicted_coalition:
            fair_quota_by_round = t * (self.m / float(self.n))
            if my_coop_count <= fair_quota_by_round + self.beta:
                return Action.C
            else:
                return Action.D
        if random.random() < p_explore:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_37(BaseStrategy):
    """
    Threshold-Conditional Cooperator with Forgiveness and Recruitment (TCCFR)

    - Uses per-player EMA cooperation estimates over a sliding window to estimate
      the probability that cooperating will reach the public-good threshold.
    - Starts cooperatively to signal willingness (except single-round games).
    - Excludes persistent defectors from coalition estimates, punishes them by withholding
      cooperation, and forgives when their EMA rises above recruitment threshold.
    - Defects in the final round (endgame). Adds a small exploration probability to probe.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.low_thresh = 0.2
        self.recruit_thresh = 0.5
        self.delta = 0.05
        self.epsilon_explore = 0.01
        self.L_default = min(10, max(1, self.game_description.n_rounds))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        t = getattr(state, 'round_number', None)
        if t is None:
            if history is None:
                t = 1
            else:
                t = history.round_number + 1
        if r == 1:
            return Action.D
        if t == r:
            return Action.D
        if history is None:
            return Action.C
        L = min(self.L_default, r)
        alpha = 2.0 / (L + 1.0)
        opp_actions = history.opponent_actions
        rounds_completed = opp_actions.shape[0]
        num_opponents = n - 1
        p_list = []
        for j in range(num_opponents):
            p = 0.5
            if rounds_completed > 0:
                window = opp_actions[max(0, rounds_completed - L):, j]
                for val in window:
                    p = (1.0 - alpha) * p + alpha * (1.0 if bool(val) else 0.0)
            p_list.append(float(p))
        S_indices = [idx for idx, p in enumerate(p_list) if p > self.low_thresh]
        if len(S_indices) + 1 < m:
            if random.random() < self.epsilon_explore:
                return Action.C
            return Action.D
        s_probs = [p_list[idx] for idx in S_indices]
        dist = np.zeros(len(s_probs) + 1, dtype=float)
        dist[0] = 1.0
        for p in s_probs:
            prev = dist.copy()
            dist[1:] = prev[:-1] * p + prev[1:] * (1.0 - p)
            dist[0] = prev[0] * (1.0 - p)
        target1 = max(0, m - 1)
        if target1 <= len(s_probs):
            P1 = float(dist[target1:].sum())
        else:
            P1 = 0.0
        target2 = m
        if target2 <= len(s_probs):
            P2 = float(dist[target2:].sum())
        else:
            P2 = 0.0
        EV_C = k * P1
        EV_D = 1.0 + k * P2
        action = Action.C if EV_C >= EV_D + self.delta else Action.D
        if rounds_completed >= 1:
            last_opponent_coops = int(np.sum(opp_actions[-1, :]))
            my_last_action = bool(history.my_actions[-1])
            last_total_coops = last_opponent_coops + (1 if my_last_action else 0)
            if last_total_coops >= m:
                if action == Action.D:
                    action = Action.C
        if abs(EV_C - EV_D) <= self.delta:
            recent_rounds = min(L, rounds_completed)
            if recent_rounds > 0:
                recent_opp = opp_actions[-recent_rounds:, :]
                recent_my = history.my_actions[-recent_rounds:]
                successes = 0
                for rr in range(recent_rounds):
                    total_coops = int(np.sum(recent_opp[rr, :])) + (1 if bool(recent_my[rr]) else 0)
                    if total_coops >= m:
                        successes += 1
                if successes / float(recent_rounds) >= 0.5:
                    action = Action.C
                else:
                    action = Action.D
        if random.random() < self.epsilon_explore:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_COOPERATIVE_38(BaseStrategy):
    """
    Reliable-Threshold contributor strategy for the Collective Risk Dilemma.

    - Maintains an EWMA "reliability" score R[j] in [0,1] for every player (index 0 == self,
      opponents are indices 1..n-1 in the order provided by history.opponent_actions columns).
    - Before each round the strategy:
        * updates reliabilities from newly observed rounds (using EWMA),
        * applies quick punishment to players who were expected to cooperate but defected,
          and gentle reward for unexpected cooperation,
        * predicts a contributor core: the top-m players by reliability (ties broken by index),
        * decides to cooperate only if:
            - others are unlikely to reach m without me, AND
            - I am among top-m by reliability, AND
            - the top (m-1) other players have enough expected cooperation (S_top >= m-1 - beta)
          with a stricter beta (0) on the final round.
    - Tunable defaults: alpha=0.4, p0=m/n, beta=0.5, punish_mult=0.6, reward_eps=0.05.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.4
        self.beta = 0.5
        self.punish_mult = 0.6
        self.reward_eps = 0.05
        self.p0 = float(self.m) / max(1, self.n)
        self.R = [self.p0 for _ in range(self.n)]
        self.seen_rounds = 0
        self.predicted_for_rounds = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        is_final_round = state is not None and state.round_number == self.r
        if history is not None:
            completed = history.round_number
            for rr in range(self.seen_rounds, completed):
                my_act = bool(history.my_actions[rr])
                opp_acts = history.opponent_actions[rr]
                predicted_set = set()
                if rr < len(self.predicted_for_rounds):
                    predicted_set = set(self.predicted_for_rounds[rr])
                a0 = 1.0 if my_act else 0.0
                self.R[0] = (1.0 - self.alpha) * self.R[0] + self.alpha * a0
                for p_idx, a_bool in enumerate(opp_acts):
                    j = p_idx + 1
                    aj = 1.0 if bool(a_bool) else 0.0
                    self.R[j] = (1.0 - self.alpha) * self.R[j] + self.alpha * aj
                    if j in predicted_set and aj == 0.0:
                        self.R[j] = max(0.0, self.R[j] * self.punish_mult)
                    if j not in predicted_set and aj == 1.0:
                        self.R[j] = min(1.0, self.R[j] + self.reward_eps)
            self.seen_rounds = completed
        players = list(range(self.n))
        players_sorted = sorted(players, key=lambda j: (-self.R[j], j))
        top_m = players_sorted[:min(self.m, self.n)]
        predicted_set_for_next = set(top_m)
        self.predicted_for_rounds.append(predicted_set_for_next)
        others = [j for j in players if j != 0]
        E_no_self = sum((self.R[j] for j in others))
        if E_no_self >= self.m:
            return Action.D
        if 0 not in top_m:
            return Action.D
        others_sorted = sorted(others, key=lambda j: (-self.R[j], j))
        m_minus_1 = max(0, self.m - 1)
        top_m_minus_1 = others_sorted[:min(m_minus_1, len(others_sorted))]
        S_top = sum((self.R[j] for j in top_m_minus_1))
        effective_beta = 0.0 if is_final_round else self.beta
        if S_top >= m_minus_1 - effective_beta:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_39(BaseStrategy):
    """
    Pivotal Probability with Forgiving Punishment (PPFP)

    - Maintains per-opponent reliability estimates s_j via exponential smoothing.
    - Computes the probability that exactly m-1 of the other players cooperate (poisson-binomial).
    - Uses myopic pivotal test (Δ = k * p_exact - 1) to decide cooperation in the final round,
      and as the main decision rule in earlier rounds unless punishing or sustaining recent success.
    - Short, finite punishments are triggered when you cooperated but the group failed and others
      appeared to defect; punishments are forgiven early if the group succeeds during punishment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.3
        raw_p0 = float(self.game_description.m) / float(max(1, self.game_description.n_players))
        self.p0 = min(0.9, max(0.1, raw_p0))
        self.L = min(10, max(1, self.game_description.n_rounds))
        self.coop_threshold = 0.6
        self.punish_len_max = 3
        self.eps = 0.0
        n_others = max(0, self.game_description.n_players - 1)
        self.s = np.full((n_others,), self.p0, dtype=float)
        self.success_history = []
        self.punish_remaining = 0
        self.tracked_rounds = 0

    def _poisson_binomial_p_exact(self, s_arr, target):
        """
        Compute Prob(sum of independent Bernoulli(s_arr) == target).
        Uses exact DP convolution unless a fast binomial approximation is suitable.
        """
        n = len(s_arr)
        if n == 0:
            return 1.0 if target == 0 else 0.0
        s_max = float(np.max(s_arr))
        s_min = float(np.min(s_arr))
        if n > 50 or s_max - s_min < 0.05:
            q = float(np.mean(s_arr))
            if target < 0 or target > n:
                return 0.0
            comb = math.comb(n, target)
            return comb * q ** target * (1.0 - q) ** (n - target)
        probs = np.array([1.0], dtype=float)
        for p in s_arr:
            left = probs * (1.0 - p)
            right = probs * p
            new = np.concatenate((left, np.array([0.0], dtype=float))) + np.concatenate((np.array([0.0], dtype=float), right))
            probs = new
        if target < 0 or target >= probs.shape[0]:
            return 0.0
        return float(probs[target])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = float(self.game_description.k)
        if history is None:
            rem = r
        else:
            rem = r - history.round_number
        if history is not None:
            rounds_to_process = history.round_number - self.tracked_rounds
            if rounds_to_process > 0:
                start = self.tracked_rounds
                end = self.tracked_rounds + rounds_to_process
                for idx in range(start, end):
                    my_act = bool(history.my_actions[idx])
                    opps = np.array(history.opponent_actions[idx, :], dtype=float) if n > 1 else np.array([], dtype=float)
                    total_cooperators = int(np.sum(opps) + (1 if my_act else 0))
                    success = total_cooperators >= m
                    self.success_history.append(1 if success else 0)
                    if len(self.success_history) > self.L:
                        self.success_history.pop(0)
                    if opps.size > 0:
                        self.s = (1.0 - self.alpha) * self.s + self.alpha * opps
                    if success and self.punish_remaining > 0:
                        self.punish_remaining = 0
                    if my_act and total_cooperators < m:
                        shortfall = m - total_cooperators
                        if shortfall >= 1:
                            rem_after = r - (idx + 1)
                            if rem_after >= 1:
                                self.punish_remaining = min(self.punish_len_max, rem_after)
                            else:
                                self.punish_remaining = 0
                self.tracked_rounds = history.round_number
        target = m - 1
        s_arr = self.s.copy() if n - 1 > 0 else np.array([], dtype=float)
        p_exact = self._poisson_binomial_p_exact(s_arr, target)
        delta = k * p_exact - 1.0
        if rem == 1:
            if delta > self.eps:
                return Action.C
            else:
                return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if delta > self.eps:
            return Action.C
        recent_rate = float(np.mean(self.success_history)) if len(self.success_history) > 0 else 0.0
        if recent_rate >= self.coop_threshold:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_40(BaseStrategy):
    """
    Adaptive Pivotal Cooperator (APC)

    Starts cooperative, estimates opponents' cooperation probabilities via Laplace smoothing,
    volunteers when likely to be pivotal (or when EV favors cooperation adjusted by generosity),
    uses limited targeted punishments for players who repeatedly defect in successful rounds,
    and employs probabilistic volunteering to break coordination failures. In the last round,
    cooperates only if plausibly pivotal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.G0 = 0.25
        self.L = 3
        self.FR = 0.6
        self.epsilon = 0.05
        n_opponents = max(0, game_description.n_players - 1)
        self.punishment_timers = [0] * n_opponents

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        desc = self.game_description
        n = desc.n_players
        r = desc.n_rounds
        m = desc.m
        k = desc.k
        n_opponents = n - 1
        if history is None:
            return Action.C
        rounds_played = history.round_number
        t = rounds_played + 1
        remaining_rounds = r - t + 1
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        if n_opponents <= 0:
            return Action.D
        if rounds_played > 0:
            last = rounds_played - 1
            total_coops_last = int(opp_actions[last, :].sum()) + (1 if bool(my_actions[last]) else 0)
            if total_coops_last < m:
                for j in range(n_opponents):
                    if not bool(opp_actions[last, j]) and total_coops_last + 1 >= m:
                        if self.punishment_timers[j] <= 0:
                            self.punishment_timers[j] = self.L
        if rounds_played > 0:
            coop_count = list(map(int, opp_actions.sum(axis=0)))
            my_ints = np.asarray(my_actions, dtype=int)
            opp_sums_per_round = np.asarray(opp_actions.sum(axis=1), dtype=int)
            total_coops_per_round = opp_sums_per_round + my_ints
            success_mask = total_coops_per_round >= m
            success_rounds_total = int(success_mask.sum())
            successful_rounds_j = []
            if success_rounds_total > 0:
                for j in range(n_opponents):
                    defect_mask = ~opp_actions[:, j]
                    successful_and_defect = success_mask & defect_mask
                    successful_rounds_j.append(int(successful_and_defect.sum()))
            else:
                successful_rounds_j = [0] * n_opponents
        else:
            coop_count = [0] * n_opponents
            success_rounds_total = 0
            successful_rounds_j = [0] * n_opponents
        denom = rounds_played + 2
        p_js = [(coop_count[j] + 1) / denom for j in range(n_opponents)]
        flags = [False] * n_opponents
        if success_rounds_total > 0:
            for j in range(n_opponents):
                frac = successful_rounds_j[j] / success_rounds_total
                if frac >= self.FR:
                    flags[j] = True
        F = sum((1 for f in flags if f))
        G = self.G0 * max(0.25, 1 - 0.5 * (F / max(1, n_opponents)))
        any_punishments = any((tmr > 0 for tmr in self.punishment_timers))
        if any_punishments:
            G *= 0.66
        prob = [0.0] * (n_opponents + 1)
        prob[0] = 1.0
        for p in p_js:
            for s in range(n_opponents - 1, -1, -1):
                if prob[s] != 0.0:
                    prob[s + 1] += prob[s] * p
                    prob[s] *= 1.0 - p

        def sum_prob_from(threshold):
            if threshold <= 0:
                return 1.0
            if threshold > n_opponents:
                return 0.0
            return sum(prob[threshold:])
        P_without = sum_prob_from(m)
        P_pivotal = sum_prob_from(m - 1)
        Marginal_gain = P_pivotal - P_without
        EV_defect = 1.0 + k * P_without
        EV_cooperate = k * P_pivotal
        action = Action.D
        if t == r:
            if P_without >= 0.5:
                action = Action.D
            elif Marginal_gain >= self.epsilon:
                action = Action.C
            else:
                action = Action.D
        else:
            threshold_for_coop = EV_defect + G * (remaining_rounds / max(1, r))
            if EV_cooperate >= threshold_for_coop:
                action = Action.C
            elif Marginal_gain >= 0.5 * G and Marginal_gain >= self.epsilon:
                denom_q = 1.0 - P_without + self.epsilon
                if denom_q <= 0:
                    q = 1.0
                else:
                    q = min(1.0, Marginal_gain / denom_q)
                if random.random() < q:
                    action = Action.C
                else:
                    action = Action.D
            elif abs(EV_cooperate - EV_defect) <= self.epsilon and (not any_punishments):
                action = Action.C
            else:
                action = Action.D
        for j in range(n_opponents):
            if self.punishment_timers[j] > 0:
                self.punishment_timers[j] -= 1
                if self.punishment_timers[j] < 0:
                    self.punishment_timers[j] = 0
        return action

class Strategy_COOPERATIVE_41(BaseStrategy):
    """
    Reciprocal Threshold Contributor (RTC)

    - Probes early, cooperates when contribution is needed or when many reliable cooperators exist,
      defects otherwise, punishes persistent defectors via a blacklist inferred from recent rates,
      forgives with small probability, and always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        r = max(1, int(self.game_description.n_rounds))
        self.W = min(5, max(1, r - 1))
        self.q_high = 0.8
        self.q_bad = 0.3
        self.L_probe = min(3, max(1, r - 1))
        self.eps = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if t == r:
            return Action.D
        completed = int(history.round_number)
        actual_W = min(self.W, max(1, completed))
        opp_actions = history.opponent_actions
        if actual_W <= 0:
            recent = np.zeros((1, opp_actions.shape[1]), dtype=bool)
            denom = 1
        else:
            recent = opp_actions[-actual_W:, :]
            denom = actual_W
        coop_counts = np.sum(recent.astype(np.int64), axis=0)
        cooperation_rates = coop_counts.astype(float) / float(denom)
        blacklist_mask = cooperation_rates < self.q_bad
        expected_others = float(np.sum(cooperation_rates[~blacklist_mask]))
        reliable_others = int(np.sum((cooperation_rates >= self.q_high) & ~blacklist_mask))
        possible_others = int(np.sum((cooperation_rates > 0.0) & ~blacklist_mask))
        last_round_opp = opp_actions[-1, :].astype(bool)
        last_round_cooperators_indices = np.nonzero(last_round_opp)[0]
        my_last = bool(history.my_actions[-1])
        last_round_total_cooperators = int(np.sum(last_round_opp)) + (1 if my_last else 0)
        last_round_threshold_not_met = last_round_total_cooperators < m
        punished_persistent_defector_exists = False
        if last_round_cooperators_indices.size > 0:
            rates_of_last_round_cooperators = cooperation_rates[last_round_cooperators_indices]
            if np.any(rates_of_last_round_cooperators < self.q_bad):
                punished_persistent_defector_exists = True
        if expected_others >= float(m):
            if reliable_others >= m:
                return Action.C
            else:
                return Action.D
        if expected_others >= float(m - 1):
            return Action.C
        if t <= self.L_probe and possible_others >= m - 1:
            return Action.C
        if last_round_threshold_not_met and punished_persistent_defector_exists:
            return Action.D
        if random.random() < self.eps:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_42(BaseStrategy):
    """
    Pivotal Conditional Cooperator with Forgiving Punishment (PCC-FP)

    - Opens with cooperation, defects in the final round.
    - Estimates opponents' cooperation probabilities from recent history (window L).
    - Computes exact Poisson–Binomial for other players when feasible.
    - Cooperates when cooperation is likely to yield success (thresholds), or when pivotal.
    - If exploited (you cooperated, group failed, and some previously-cooperative opponents defected),
      enters a short Punish mode (play D for T_punish rounds). Forgives if cooperation recovers.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.mode = 'Cooperative'
        self.punish_timer = 0
        self.L_max = 10
        self.pivot_delta = 0.1
        self.high_prob = 0.6
        self.low_prob = 0.35
        self.T_punish = 2
        self.recovery_window = max(3, min(5, self.game_description.n_rounds))
        self._just_completed_punishment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.mode = 'Cooperative'
            self.punish_timer = 0
            self._just_completed_punishment = False
            return Action.C
        t = state.round_number
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if t == r:
            return Action.D
        completed = history.round_number
        if completed == 0:
            return Action.C
        last_idx = completed - 1
        my_last_action = bool(history.my_actions[last_idx])
        opps_last = history.opponent_actions[last_idx, :]
        last_total_cooperators = int(opps_last.sum()) + (1 if my_last_action else 0)

        def detect_clear_free_ride():
            if not my_last_action:
                return False
            if last_total_cooperators >= m:
                return False
            if completed < 2:
                return False
            prev = history.opponent_actions[:last_idx, :]
            if prev.size == 0:
                return False
            historically_cooperative = prev.sum(axis=0) > 0
            defected_last = np.logical_not(opps_last)
            return bool(np.logical_and(historically_cooperative, defected_last).any())
        if detect_clear_free_ride() and self.mode != 'Punish':
            self.mode = 'Punish'
            self.punish_timer = self.T_punish
            self._just_completed_punishment = False
        if self.mode == 'Punish':
            action = Action.D
            if self.punish_timer > 0:
                self.punish_timer -= 1
            if self.punish_timer == 0:
                look = min(self.recovery_window, completed)
                recent_start = completed - look
                successes = 0
                if look > 0:
                    for idx in range(recent_start, completed):
                        opps = history.opponent_actions[idx, :]
                        total = int(opps.sum()) + (1 if history.my_actions[idx] else 0)
                        if total >= m:
                            successes += 1
                if successes >= max(0, look - 1):
                    self.mode = 'Cooperative'
                    self._just_completed_punishment = False
                else:
                    self.mode = 'Cooperative'
                    self._just_completed_punishment = True
            return action
        L = min(self.L_max, completed)
        opp_actions = history.opponent_actions
        n_opps = n - 1
        p_list = []
        if L == 0:
            p_list = [0.5] * n_opps
        else:
            start_idx = max(0, completed - L)
            recent = opp_actions[start_idx:completed, :]
            obs_means = np.mean(recent.astype(float), axis=0)
            alpha = min(1.0, L / 5.0)
            p_list = list(alpha * obs_means + (1.0 - alpha) * 0.5)
        prob = np.zeros(n_opps + 1, dtype=float)
        prob[0] = 1.0
        for p in p_list:
            new = np.zeros_like(prob)
            new[:-1] += prob[:-1] * (1.0 - p)
            new[1:] += prob[:-1] * p
            prob = new
        if m <= 0:
            P_without = 1.0
            P_with = 1.0
        else:
            if m <= n_opps:
                P_without = float(prob[m:].sum())
            else:
                P_without = 0.0
            threshold_with = max(0, m - 1)
            if threshold_with <= n_opps:
                P_with = float(prob[threshold_with:].sum())
            else:
                P_with = 0.0
        Delta = P_with - P_without
        last_round_cooperators = last_total_cooperators
        play_C = False
        if P_with >= self.high_prob:
            play_C = True
        elif Delta >= self.pivot_delta:
            play_C = True
        elif P_with >= self.low_prob and last_round_cooperators >= m:
            play_C = True
        elif bool(history.my_actions[last_idx]) and last_round_cooperators >= m:
            play_C = True
        else:
            play_C = False
        action = Action.C if play_C else Action.D
        return action

class Strategy_COOPERATIVE_43(BaseStrategy):
    """
    Adaptive, robust Collective Risk strategy.

    - Cooperates when others' recent behavior implies threshold will be met without being exploited.
    - Defects when cooperation is unlikely to help or when under persistent exploitation.
    - Uses sparse probabilistic probes to rebuild cooperation and short punishments to deter exploiters.
    - Forgives after success and reduces cooperative tendencies toward the end of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(10, max(1, self.r))
        self.prior = 0.6
        self.prior_weight = 1.0
        self.p_probe = 0.15
        self.p_pivot_base = 0.75
        self.T_grace = 2
        self.T_end = 1
        self.exploit_threshold = 0.5
        self.punish_until_round = 0.0
        self.blacklist = {}
        self.blacklist_factor = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.r
        m = self.m
        n = self.n
        if t == r:
            return Action.D
        if history is None:
            p_init = min(0.85, self.prior)
            return Action.C if random.random() < p_init else Action.D
        L = history.round_number
        W = min(self.W, r)
        window_size = min(W, L)
        window_start = max(0, L - W)
        if t < self.punish_until_round:
            return Action.D
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        n_opponents = opp_actions.shape[1] if opp_actions.size else n - 1
        p_list = []
        for j in range(n_opponents):
            if window_size > 0:
                coops_j = int(np.sum(opp_actions[window_start:L, j]))
                obs_j = window_size
            else:
                coops_j = 0
                obs_j = 0
            denom = obs_j + self.prior_weight
            p_j = (coops_j + self.prior_weight * self.prior) / denom
            expiry = self.blacklist.get(j, 0)
            if t < expiry:
                p_j *= self.blacklist_factor
            p_list.append(p_j)
        expected_others = float(sum(p_list))
        successes = 0
        for rr in range(window_start, L):
            coop_count = int(np.sum(opp_actions[rr, :]) + (1 if my_actions[rr] else 0))
            if coop_count >= m:
                successes += 1
        recent_success_rate = successes / max(1, window_size)
        my_coops = 0
        my_coop_failures = 0
        for rr in range(window_start, L):
            if my_actions[rr]:
                my_coops += 1
                coop_count = int(np.sum(opp_actions[rr, :]) + 1)
                if coop_count < m:
                    my_coop_failures += 1
        my_recent_coop_fail_rate = my_coop_failures / max(1, my_coops)
        end_decay = max(0.0, 1.0 - (t - (r - 1)) / max(1.0, float(self.T_end)))
        k_scale = min(1.5, 1.0 + 0.2 * max(0.0, self.k - 1.0))
        p_pivot = self.p_pivot_base * k_scale * end_decay
        p_probe_scaled = self.p_probe * end_decay
        boost = 0.0
        if successes > 0:
            last_success_round = None
            for rr in range(L - 1, window_start - 1, -1):
                coop_count = int(np.sum(opp_actions[rr, :]) + (1 if my_actions[rr] else 0))
                if coop_count >= m:
                    last_success_round = rr
                    break
            if last_success_round is not None:
                rounds_since_success = L - 1 - last_success_round
                if rounds_since_success < self.T_grace:
                    boost = 0.5
        expected_others_boosted = expected_others + boost
        if my_coops >= 1 and my_recent_coop_fail_rate > self.exploit_threshold:
            P = int(min(3, max(0, r - t)))
            if P > 0:
                self.punish_until_round = t + P
                defect_counts = np.zeros(n_opponents, dtype=int)
                failing_rounds = 0
                for rr in range(window_start, L):
                    coop_count = int(np.sum(opp_actions[rr, :]) + (1 if my_actions[rr] else 0))
                    if my_actions[rr] and coop_count < m:
                        failing_rounds += 1
                        for j in range(n_opponents):
                            if not opp_actions[rr, j]:
                                defect_counts[j] += 1
                if failing_rounds > 0:
                    for j in range(n_opponents):
                        if defect_counts[j] >= 0.6 * failing_rounds:
                            self.blacklist[j] = t + W
                return Action.D
        eps = 0.05
        if expected_others_boosted >= m - 1:
            return Action.C
        if expected_others <= m - 2:
            if recent_success_rate < 0.5 and random.random() < p_probe_scaled:
                return Action.C
            return Action.D
        if abs(expected_others - (m - 1)) <= eps:
            if recent_success_rate >= 0.5 and my_recent_coop_fail_rate < self.exploit_threshold:
                return Action.C
            p = p_pivot * max(0.5, recent_success_rate)
            if random.random() < p:
                return Action.C
            else:
                if recent_success_rate < 0.5 and random.random() < p_probe_scaled:
                    return Action.C
                return Action.D
        if recent_success_rate < 0.5:
            if random.random() < p_probe_scaled:
                return Action.C
        return Action.D

class Strategy_COOPERATIVE_44(BaseStrategy):
    """
    Adaptive Threshold Conditional Cooperator (ATCC)

    - Starts cooperative to signal willingness.
    - Maintains per-opponent reliability scores S_j (exponential smoothing, alpha=0.5).
    - Uses a small recent-history window H to count successful threshold rounds.
    - Attempts short cooperative recruitment spells (T_trial) when feasibility is unclear.
    - Uses short punishments (P) after failed recruitment to limit exploitation.
    - Forgives early if the group demonstrates success during punishment.
    - Adjusts trust threshold tau slightly based on recent outcomes.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.5
        self.H = min(5, max(0, self.r - 1))
        self.tau = 0.65
        self.tau_min = 0.35
        self.tau_max = 0.85
        self.T_trial = min(3, max(0, self.r - 1))
        self.P = min(3, max(0, self.r - 1))
        self.S = [0.5 for _ in range(max(0, self.n - 1))]
        self.Mode = 'Normal'
        self.recruit_counter = 0
        self.punish_counter = 0
        self.recent_successes = 0
        self._last_round_seen = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        rem = self.r - t + 1
        if rem <= 1:
            return Action.D
        if history is None:
            return Action.C
        rounds_completed = history.round_number

        def total_cooperators_in_round(r_idx: int) -> int:
            opp_coops = int(np.sum(history.opponent_actions[r_idx, :])) if history.opponent_actions.size else 0
            my_coop = 1 if bool(history.my_actions[r_idx]) else 0
            return opp_coops + my_coop
        for r_idx in range(self._last_round_seen, rounds_completed):
            total_coops = total_cooperators_in_round(r_idx)
            if self.Mode == 'Recruiting':
                if total_coops >= self.m:
                    self.Mode = 'Normal'
                    self.recruit_counter = 0
                else:
                    self.recruit_counter += 1
                    if self.recruit_counter >= self.T_trial:
                        self.Mode = 'Punishing'
                        self.punish_counter = 0
            elif self.Mode == 'Punishing':
                if total_coops >= self.m:
                    self.Mode = 'Normal'
                    self.punish_counter = 0
                else:
                    self.punish_counter += 1
                    if self.punish_counter >= self.P:
                        self.Mode = 'Normal'
                        self.punish_counter = 0
            else:
                pass
        self._last_round_seen = rounds_completed
        n_opponents = max(0, self.n - 1)
        if n_opponents > 0 and history.opponent_actions.size:
            for j in range(n_opponents):
                s = 0.5
                for r_idx in range(rounds_completed):
                    action = bool(history.opponent_actions[r_idx, j])
                    s = self.alpha * (1.0 if action else 0.0) + (1.0 - self.alpha) * s
                self.S[j] = s
        else:
            self.S = []
        recent_successes = 0
        start_idx = max(0, rounds_completed - self.H)
        for r_idx in range(start_idx, rounds_completed):
            if total_cooperators_in_round(r_idx) >= self.m:
                recent_successes += 1
        self.recent_successes = recent_successes
        failures_you_cooperated = 0
        for r_idx in range(start_idx, rounds_completed):
            my_coop = bool(history.my_actions[r_idx])
            if my_coop and total_cooperators_in_round(r_idx) < self.m:
                failures_you_cooperated += 1
        if failures_you_cooperated > 1:
            self.tau = max(self.tau_min, self.tau - 0.05)
        if self.recent_successes >= 2:
            self.tau = min(self.tau_max, self.tau + 0.05)
        likely_others = sum((1 for s in self.S if s >= self.tau))
        if likely_others >= self.m - 1:
            action = Action.C
            return action
        if self.Mode == 'Normal':
            if self.recent_successes >= 1:
                action = Action.C
                return action
            else:
                self.Mode = 'Recruiting'
                self.recruit_counter = 0
                action = Action.C
                return action
        elif self.Mode == 'Recruiting':
            if self.recruit_counter >= self.T_trial:
                self.Mode = 'Punishing'
                self.punish_counter = 0
                action = Action.D
                return action
            else:
                action = Action.C
                return action
        elif self.Mode == 'Punishing':
            if self.punish_counter < self.P:
                action = Action.D
                return action
            else:
                self.Mode = 'Normal'
                self.punish_counter = 0
                likely_others = sum((1 for s in self.S if s >= self.tau))
                if likely_others >= self.m - 1:
                    return Action.C
                if self.recent_successes >= 1:
                    return Action.C
                self.Mode = 'Recruiting'
                self.recruit_counter = 0
                return Action.C
        return Action.D

class Strategy_COOPERATIVE_45(BaseStrategy):
    """
    Adaptive Pivotal Conditional Cooperator (APCC)

    Cooperates in a round iff cooperating has positive expected marginal value under
    current per-opponent beliefs: play C iff k * Prob(number of other cooperators == m-1) > 1.
    Beliefs p_j are maintained per opponent with exponential smoothing, targeted temporary
    penalties for pivotal defections, and forgiveness after a short cooperative streak.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.4
        self.decay_penalty = 0.5
        self.forgiveness_window = 2
        self.n_opp = max(0, self.n - 1)
        self.p = np.full(self.n_opp, 0.5, dtype=float)
        self.penalized = np.zeros(self.n_opp, dtype=bool)
        self.coop_streaks = np.zeros(self.n_opp, dtype=int)
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is not None:
            rounds_available = history.round_number
            rounds_to_process = range(self.last_processed_round, rounds_available)
            for t in rounds_to_process:
                opp_row = history.opponent_actions[t]
                my_act = bool(history.my_actions[t])
                total_coops = int(my_act) + int(opp_row.sum())
                for j in range(self.n_opp):
                    a_j = bool(opp_row[j])
                    self.p[j] = (1.0 - self.alpha) * self.p[j] + self.alpha * (1.0 if a_j else 0.0)
                for j in range(self.n_opp):
                    a_j = bool(opp_row[j])
                    if not a_j:
                        if total_coops < self.m and total_coops + 1 >= self.m:
                            prev_coop = False
                            if t - 1 >= 0:
                                prev_coop = bool(history.opponent_actions[t - 1, j])
                            if prev_coop:
                                self.p[j] = float(self.p[j] * self.decay_penalty)
                                self.penalized[j] = True
                                self.coop_streaks[j] = 0
                for j in range(self.n_opp):
                    a_j = bool(opp_row[j])
                    if a_j:
                        self.coop_streaks[j] += 1
                    else:
                        self.coop_streaks[j] = 0
                    if self.penalized[j] and self.coop_streaks[j] >= self.forgiveness_window:
                        self.p[j] = max(self.p[j], 0.5)
                        self.penalized[j] = False
                self.p = np.clip(self.p, 0.0, 1.0)
            self.last_processed_round = rounds_available
        target = self.m - 1
        P_piv = 0.0
        if 0 <= target <= self.n_opp:
            probs = [1.0] + [0.0] * self.n_opp
            for p_j in self.p.tolist():
                new = [0.0] * (self.n_opp + 1)
                new[0] = probs[0] * (1.0 - p_j)
                for s in range(1, self.n_opp + 1):
                    new[s] = probs[s] * (1.0 - p_j) + probs[s - 1] * p_j
                probs = new
            P_piv = float(probs[target])
        if self.k * P_piv > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_46(BaseStrategy):
    """
    Adaptive Threshold Reciprocity with Forgiveness (ATRF).

    - Signals cooperation on the first move.
    - Defects on the last round.
    - Reciprocates successful rounds (if last round had >= m cooperators).
    - Uses a recent-window frequency estimate to detect likely cooperators (tau=0.5).
      * If enough others are likely cooperators to meet the threshold alone -> free-ride (D).
      * If others are exactly one short -> tip (C).
    - If the group was one short last round, attempt to tip (C).
    - If there has been a successful round in the recent window, cooperate to help restore cooperation.
    - Otherwise mostly defect but probe with small, declining probability p_probe(t) = p_max_probe * (rounds_left / r).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.p_max_probe = 0.2
        self.w_max = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        t = state.round_number
        rounds_left = max(0, r - t)
        if t == r:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed == 0:
            return Action.C
        last_other_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        last_self_coop = 1 if bool(history.my_actions[-1]) else 0
        L = last_other_coops + last_self_coop
        if t == 1:
            return Action.C
        if L >= m:
            return Action.C
        w = min(self.w_max, rounds_completed)
        p_probe = self.p_max_probe * (rounds_left / r) if r > 0 else 0.0
        if w > 0:
            recent_opponents = history.opponent_actions[-w:, :]
            counts_per_player = np.sum(recent_opponents, axis=0)
            freqs = counts_per_player / float(w)
            likely_cooperators = int(np.sum(freqs >= self.tau))
            if likely_cooperators >= m:
                return Action.D
            if likely_cooperators == m - 1:
                return Action.C
        if L == m - 1:
            return Action.C
        if w > 0:
            success_recent = False
            recent_self = history.my_actions[-w:]
            recent_opponents_full = history.opponent_actions[-w:, :] if history.opponent_actions.size else np.zeros((w, 0), dtype=bool)
            for idx in range(w):
                other_count = int(np.sum(recent_opponents_full[idx, :])) if recent_opponents_full.size else 0
                self_count = 1 if bool(recent_self[idx]) else 0
                if other_count + self_count >= m:
                    success_recent = True
                    break
            if success_recent:
                return Action.C
        if random.random() < p_probe:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_47(BaseStrategy):
    """
    Fair Adaptive Thresholder (FAT)

    - Estimates how many OTHER players will cooperate (E_others) via exponential smoothing.
    - Cooperates deterministically when doing so is likely pivotal or to replicate a past success (win-stay).
    - Defects when others clearly suffice.
    - When cooperation looks hopeless, seeds cooperation with a small, decaying exploration probability
      (scaled by reward k). Exploration is halved while in short punishment.
    - Short punishment (punish_len rounds) after being exploited (you cooperated but threshold failed).
    - Very cautious in the final round: only cooperate if deterministic rules mandate it.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.6
        self.epsilon_base = 0.2
        self.punish_len = 2
        self.min_explore = 0.02
        self.E_others = float(max(0, self.m - 1))
        self.last_my_action = 0
        self.P = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide Action.C or Action.D for the upcoming round state.round_number (1-based).
        history is None for the very first decision (no prior rounds).
        """
        t = int(state.round_number)
        RL = max(1, self.r - t + 1)
        s_prev = None
        my_prev = None
        if history is not None and history.round_number > 0:
            opp_last = int(np.sum(history.opponent_actions[-1, :]))
            my_prev = 1 if bool(history.my_actions[-1]) else 0
            s_prev = opp_last + my_prev
            observed_others_last = opp_last
            self.E_others = float(self.alpha * observed_others_last + (1.0 - self.alpha) * self.E_others)
            if my_prev == 1 and s_prev < self.m:
                self.P = int(self.punish_len)
            elif self.P > 0:
                self.P = int(self.P - 1)
            self.last_my_action = int(my_prev)
        expected_others = float(self.E_others)
        expected_with_me = expected_others + 1.0
        prev_succeeded = s_prev is not None and s_prev >= self.m
        prev_others_met = s_prev is not None and s_prev - (self.last_my_action if s_prev is not None else 0) >= self.m
        prev_pivotal = s_prev is not None and prev_succeeded and (not prev_others_met)
        if t == self.r:
            if prev_succeeded and prev_others_met:
                action = Action.D
            elif prev_succeeded and (not prev_others_met):
                action = Action.C
            elif expected_with_me >= float(self.m):
                action = Action.C
            else:
                action = Action.D
            self.last_my_action = 1 if action == Action.C else 0
            return action
        if prev_succeeded and prev_others_met:
            action = Action.D
        elif prev_succeeded and (not prev_others_met):
            action = Action.C
        elif expected_with_me >= float(self.m):
            action = Action.C
        elif expected_others >= float(self.m):
            action = Action.D
        else:
            base = self.epsilon_base * (self.k / (self.k + 1.0))
            time_scale = float(RL) / float(self.r)
            p_explore = max(self.min_explore, base * time_scale)
            if self.P > 0:
                p_explore = p_explore / 2.0
            if random.random() < p_explore:
                action = Action.C
            else:
                action = Action.D
        self.last_my_action = 1 if action == Action.C else 0
        return action

class Strategy_COOPERATIVE_48(BaseStrategy):
    """
    Adaptive Threshold Conditional Cooperator (ATCC)

    - Signals cooperation in the first round.
    - Predicts others will repeat their last action to compute whether
      this player's single contribution would be pivotal.
    - Cooperates when pivotal, defects when others already meet threshold.
    - Uses short lookback momentum and population cooperation rates to decide
      when cooperation is worthwhile.
    - Implements short, targeted punishments for pivotal defections (reconstructed
      from history) with quick forgiveness on observed cooperation.
    - Uses a small exploration probability epsilon to probe.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.L_default = None
        self.P = 2
        self.epsilon = 0.05
        self.high_thresh = 0.7
        self.low_thresh = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            rounds_completed = 0
        else:
            rounds_completed = history.round_number
        if rounds_completed == 0:
            return Action.C
        L = min(3, max(1, r - 1))
        available_rounds = rounds_completed
        lookback = min(L, available_rounds)

        def cooperators_in_round(s: int) -> int:
            my_act = bool(history.my_actions[s])
            opp_act_sum = int(np.sum(history.opponent_actions[s, :])) if history.opponent_actions.size else 0
            return (1 if my_act else 0) + opp_act_sum
        last_idx = rounds_completed - 1
        C_prev = cooperators_in_round(last_idx)
        I_prev = 1 if bool(history.my_actions[last_idx]) else 0
        predicted_excl = C_prev - I_prev
        n_opponents = n - 1
        punish = [0] * n_opponents
        for s in range(available_rounds):
            C_s = cooperators_in_round(s)
            for j in range(n_opponents):
                opp_cooperated = bool(history.opponent_actions[s, j])
                if opp_cooperated:
                    punish[j] = 0
                elif C_s == m - 1:
                    punish[j] = self.P
            for j in range(n_opponents):
                if punish[j] > 0:
                    punish[j] = max(0, punish[j] - 1)
        if predicted_excl == m - 1:
            return Action.C
        if predicted_excl >= m:
            return Action.D
        if predicted_excl < m - 1:
            if lookback <= 0:
                avg_C = 0.0
            else:
                start_idx = max(0, rounds_completed - lookback)
                Cs = [cooperators_in_round(s) for s in range(start_idx, rounds_completed)]
                avg_C = float(np.mean(Cs)) if Cs else 0.0
            if avg_C >= m - 1:
                return Action.C
        coop_rates = []
        if rounds_completed > 0:
            my_rate = float(np.sum(history.my_actions)) / rounds_completed
        else:
            my_rate = 0.0
        coop_rates.append(my_rate)
        if n_opponents > 0:
            for j in range(n_opponents):
                opp_rate = float(np.sum(history.opponent_actions[:, j])) / rounds_completed
                coop_rates.append(opp_rate)
        else:
            pass
        frac_high = sum((1 for rate in coop_rates if rate >= self.high_thresh)) / float(n)
        frac_low = sum((1 for rate in coop_rates if rate <= self.low_thresh)) / float(n)
        if frac_high >= 0.5 and predicted_excl < m - 1:
            return Action.C
        if frac_low > 0.5:
            return Action.D
        if any((p > 0 for p in punish)):
            return Action.D
        if random.random() < self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_49(BaseStrategy):
    """
    Adaptive Pivotal Tit-for-Tat with Forgiveness (APTF)

    - Maintains per-opponent smoothed cooperation probabilities (p_j).
    - Computes pivotal probability P(S == m-1) to evaluate immediate marginal benefit.
    - Has cooperative_mode to sustain cooperating equilibria, punishment_timer to punish,
      and limited forgiveness to recover cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.15
        self.T_success = 4
        self.phi_success = 0.67
        self.L_punish = 2
        self.eps_explore = 0.05
        self.tau_margin = 0.0
        self.gamma = 0.2
        self.R_fail = 2
        if self.n >= 2:
            init_p = float(self.m) / max(1.0, float(self.n))
            self.p_js = np.full((self.n - 1,), init_p, dtype=float)
        else:
            self.p_js = np.array([], dtype=float)
        self.cooperative_mode = False
        self.punishment_timer = 0
        self.recent_records = []
        self.last_seen_rounds = 0
        self.fail_streak = 0

    def _clip01(self, x):
        if x < 0.0:
            return 0.0
        if x > 1.0:
            return 1.0
        return float(x)

    def _prob_sum(self, p_list):
        probs = np.array([1.0], dtype=float)
        for p in p_list:
            p = float(self._clip01(p))
            new = np.zeros(len(probs) + 1, dtype=float)
            new[:-1] += probs * (1.0 - p)
            new[1:] += probs * p
            probs = new
        return probs

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            completed = 0
        else:
            completed = history.round_number
        t = completed + 1
        if history is not None and completed > self.last_seen_rounds:
            last_op_actions = history.opponent_actions[-1, :]
            last_my_action = bool(history.my_actions[-1])
            expected_before = float(np.sum(self.p_js))
            realized_coops = int(np.sum(last_op_actions)) + (1 if last_my_action else 0)
            self.recent_records.append((realized_coops, expected_before))
            max_store = max(self.T_success, self.R_fail) + 5
            if len(self.recent_records) > max_store:
                self.recent_records = self.recent_records[-max_store:]
            last_actions_float = last_op_actions.astype(float)
            self.p_js = (1.0 - self.alpha) * self.p_js + self.alpha * last_actions_float
            self.p_js = np.clip(self.p_js, 0.0, 1.0)
            if realized_coops < self.m and expected_before >= float(self.m):
                self.fail_streak += 1
            else:
                self.fail_streak = 0
            if self.fail_streak >= self.R_fail:
                self.punishment_timer = self.L_punish
                self.fail_streak = 0
                rounds_available = history.opponent_actions.shape[0]
                check_r = min(self.R_fail, rounds_available)
                if check_r > 0:
                    recent_block = history.opponent_actions[-check_r:, :]
                    consistently_defected = np.all(recent_block == False, axis=0)
                    for idx, flag in enumerate(consistently_defected):
                        if flag:
                            self.p_js[idx] = max(0.0, self.p_js[idx] * 0.6)
            self.last_seen_rounds = completed
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        probs = self._prob_sum(self.p_js)
        n_opp = self.n - 1
        idx_m_minus_1 = self.m - 1
        if 0 <= idx_m_minus_1 <= n_opp:
            P_m_minus_1 = float(probs[idx_m_minus_1])
        else:
            P_m_minus_1 = 0.0
        if self.m <= n_opp:
            P_success_if_D = float(probs[self.m:].sum())
        else:
            P_success_if_D = 0.0
        if self.m - 1 <= n_opp:
            P_success_if_C = float(probs[max(0, self.m - 1):].sum())
        else:
            P_success_if_C = 0.0
        Delta = float(self.k * P_m_minus_1 - 1.0)
        if history is None:
            if abs(Delta) <= self.tau_margin:
                if random.random() < 0.6:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C if Delta > self.tau_margin else Action.D
        if t == self.r:
            if Delta > self.tau_margin:
                return Action.C
            elif Delta < -self.tau_margin:
                return Action.D
            else:
                return Action.C if random.random() < max(self.eps_explore, min(0.5, self.k * P_m_minus_1)) else Action.D
        recent_slice = self.recent_records[-self.T_success:]
        if len(recent_slice) > 0:
            successes = sum((1 for realized, _ in recent_slice if realized >= self.m))
            recent_success_rate = float(successes) / float(len(recent_slice))
        else:
            recent_success_rate = 0.0
        if recent_success_rate >= self.phi_success:
            self.cooperative_mode = True
        if self.cooperative_mode:
            exploited = False
            for realized, expected_before in recent_slice:
                if realized < self.m and expected_before >= float(self.m):
                    exploited = True
                    break
            if exploited:
                self.cooperative_mode = False
                self.punishment_timer = self.L_punish
                return Action.D
            if Delta >= -self.gamma:
                if Delta > self.tau_margin:
                    return Action.C
                else:
                    return Action.C if random.random() > 0.1 else Action.D
            return Action.D
        if Delta > self.tau_margin:
            return Action.C
        elif Delta < -self.tau_margin:
            return Action.D
        else:
            p_indiff = max(self.eps_explore, min(0.5, self.k * P_m_minus_1))
            return Action.C if random.random() < p_indiff else Action.D

class Strategy_COOPERATIVE_50(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR)

    In each round ATR estimates each opponent's cooperation probability with Laplace
    smoothing, computes the Poisson‑binomial distribution for the number of other
    cooperators, and cooperates iff k * P(X = m-1) >= 1 (tie -> cooperate). A small,
    exponentially decaying exploration probability occasionally flips the chosen action
    to probe opponents. Uses exact dynamic programming for the Poisson‑binomial.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.smoothing_alpha = 1.0
        self.exploration_base = 0.08
        self.exploration_decay_rate = 0.95

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None:
            rounds_seen = 0
            opponent_count = n - 1
            coop_counts = [0] * opponent_count
        else:
            rounds_seen = int(history.round_number)
            if rounds_seen == 0:
                opponent_count = n - 1
                coop_counts = [0] * opponent_count
            else:
                opp_actions = history.opponent_actions
                opponent_count = opp_actions.shape[1]
                coop_counts = list(np.sum(opp_actions.astype(int), axis=0))
        alpha = float(self.smoothing_alpha)
        denom = rounds_seen + 2.0 * alpha
        if denom <= 0:
            p_list = [0.5] * (n - 1)
        else:
            p_list = [(coop_counts[j] + alpha) / denom for j in range(n - 1)]
        opponents = n - 1
        prob = [0.0] * (opponents + 1)
        prob[0] = 1.0
        current_max = 0
        for pj in p_list:
            for s in range(current_max, -1, -1):
                prob[s + 1] += prob[s] * pj
                prob[s] *= 1.0 - pj
            current_max += 1
        target = m - 1
        if 0 <= target <= opponents:
            P_eq = prob[target]
        else:
            P_eq = 0.0
        if k * P_eq >= 1.0:
            chosen = Action.C
        else:
            chosen = Action.D
        t = rounds_seen + 1
        eps = float(self.exploration_base) * float(self.exploration_decay_rate) ** max(0, t - 1)
        if random.random() < eps:
            chosen = Action.D if chosen == Action.C else Action.C
        return chosen

class Strategy_COOPERATIVE_51(BaseStrategy):
    """
    Pivotal-Probability Cooperative Strategy for the Collective Risk Dilemma.

    - Maintains per-opponent cooperation estimates (exponential moving averages).
    - Computes the probability that exactly m-1 other players cooperate (P_piv).
    - Cooperates if P_piv >= 1/k (tie-break toward cooperation), with:
        * small exploration probability,
        * a fairness adjustment if this agent has been contributing substantially more than others,
        * and exponential-moving-average forgiveness via alpha and B decay.
    - Updates beliefs after observing each completed round in history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = game_description.n_players
        m = game_description.m
        self.n = n
        self.m = m
        self.p0 = (m - 1) / max(1, n - 1)
        self.p = np.full((max(0, n - 1),), self.p0, dtype=float)
        self.B = 0.0
        self.alpha = 0.2
        self.eps_explore = 0.02
        self.delta_B = 1.0
        self.gamma = 0.2
        self.B_decay = 0.98

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            if self.p.shape[0] != max(0, self.n - 1):
                self.p = np.full((max(0, self.n - 1),), self.p0, dtype=float)
            self.B = 0.0
            if random.random() < self.eps_explore:
                return random.choice([Action.C, Action.D])
            P_piv = self._compute_p_piv(self.p, self.m - 1)
            threshold = 1.0 / max(1e-12, self.game_description.k)
            effective_threshold = threshold
            if self.B > self.delta_B:
                effective_threshold = min(1.0, threshold + self.gamma)
            if P_piv >= effective_threshold:
                return Action.C
            else:
                return Action.D
        self._recompute_from_history(history)
        if random.random() < self.eps_explore:
            return random.choice([Action.C, Action.D])
        P_piv = self._compute_p_piv(self.p, self.m - 1)
        threshold = 1.0 / max(1e-12, self.game_description.k)
        effective_threshold = threshold
        if self.B > self.delta_B:
            effective_threshold = min(1.0, threshold + self.gamma)
        if P_piv >= effective_threshold:
            return Action.C
        else:
            return Action.D

    def _recompute_from_history(self, history: PlayerHistory):
        """
        Recompute EMA estimates p_j and contribution imbalance B from scratch
        using the full provided history. This is simple and robust.
        """
        rounds = history.round_number
        n_opponents = max(0, self.n - 1)
        p = np.full((n_opponents,), self.p0, dtype=float)
        B = 0.0
        for t in range(rounds):
            opp_actions_row = np.asarray(history.opponent_actions[t, :], dtype=float)
            for j in range(n_opponents):
                observed = float(opp_actions_row[j])
                p[j] = (1.0 - self.alpha) * p[j] + self.alpha * observed
            C_me = 1.0 if bool(history.my_actions[t]) else 0.0
            C_others_avg = float(np.sum(opp_actions_row)) / max(1, n_opponents)
            B += C_me - C_others_avg
            B *= self.B_decay
        self.p = p
        self.B = float(B)

    def _compute_p_piv(self, p_list: np.ndarray, target_successes: int) -> float:
        """
        Compute the probability that exactly `target_successes` of the other players
        cooperate, treating them as independent Bernoulli(p_j). Uses exact DP.
        """
        n = p_list.shape[0]
        if target_successes < 0 or target_successes > n:
            return 0.0
        dp = [1.0] + [0.0] * n
        current_max = 0
        for p in p_list:
            p = float(p)
            new_dp = [0.0] * (n + 1)
            for s in range(0, current_max + 1):
                prob = dp[s]
                if prob == 0.0:
                    continue
                new_dp[s] += prob * (1.0 - p)
                new_dp[s + 1] += prob * p
            current_max += 1
            dp = new_dp
        return float(dp[target_successes])

class Strategy_COOPERATIVE_52(BaseStrategy):
    """
    Adaptive Conditional Contributor (ACC)

    - Maintains per-opponent trust scores (exponential moving average).
    - Uses a normal approximation to the Poisson–binomial distribution to estimate
      the probability that the group (without or with this player) reaches the
      cooperation threshold m.
    - Cooperates in round 1 to signal intent.
    - In intermediate rounds cooperates when pivotal or when group cooperation is
      reasonably strong; defects when the group will succeed without it or when
      many opponents are persistent defectors. Occasional generosity (epsilon)
      helps recover cooperation.
    - In the final round acts myopically to maximize that round's payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n_players = game_description.n_players
        self.n_opponents = max(0, n_players - 1)
        self.trust = [0.5] * self.n_opponents
        self.beta = 0.3
        self.desired_success_prob = 0.9
        self.pivotal_gain_threshold = 0.2
        self.cautious_margin = 0.05
        self.L = 5
        self.epsilon = 0.05
        self.low_trust_threshold = 0.2
        self.persistent_defectors_threshold = 0.4
        self.group_coop_rate_for_generosity = 0.6
        self.last_updated_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is not None and self.n_opponents > 0:
            rounds_available = history.round_number
            start = max(0, self.last_updated_round)
            end = rounds_available
            for idx in range(start, end):
                opp_row = history.opponent_actions[idx]
                for j in range(self.n_opponents):
                    acted_coop = bool(opp_row[j])
                    if acted_coop:
                        self.trust[j] = self.trust[j] + self.beta * (1.0 - self.trust[j])
                    else:
                        self.trust[j] = self.trust[j] - self.beta * self.trust[j]
            self.last_updated_round = rounds_available
        if t == 1:
            return Action.C

        def std_norm_cdf(x: float) -> float:
            return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
        p_list = list(self.trust)
        mu = float(sum(p_list))
        sigma2 = float(sum((p * (1.0 - p) for p in p_list)))
        sigma = math.sqrt(sigma2) if sigma2 > 0.0 else 0.0

        def P_at_least_k(k: int, mu_val: float, sigma_val: float) -> float:
            if sigma_val == 0.0:
                return 1.0 if mu_val >= k - 1e-07 else 0.0
            z = (k - 0.5 - mu_val) / sigma_val
            return 1.0 - std_norm_cdf(z)
        P_no_me = P_at_least_k(m, mu, sigma)
        P_with_me = P_at_least_k(m, mu + 1.0, sigma)
        recent_rounds = 0
        group_coop_rate = 0.5
        if history is not None and history.round_number > 0:
            L = min(self.L, history.round_number)
            start_idx = history.round_number - L
            end_idx = history.round_number
            opp_slice = history.opponent_actions[start_idx:end_idx, :]
            opp_coops = float(np.sum(opp_slice))
            my_slice = history.my_actions[start_idx:end_idx]
            my_coops = float(np.sum(my_slice))
            total_coops = opp_coops + my_coops
            group_coop_rate = total_coops / (L * n)
            recent_rounds = L
        pers_defectors = 0.0
        if self.n_opponents > 0:
            low_count = sum((1 for p in self.trust if p < self.low_trust_threshold))
            pers_defectors = float(low_count) / float(self.n_opponents)
        if t == r:
            if P_no_me >= 0.5 + self.cautious_margin:
                return Action.D
            elif P_with_me - P_no_me >= self.pivotal_gain_threshold or (P_with_me >= 0.5 and P_no_me < 0.5):
                return Action.C
            else:
                return Action.D
        if P_no_me >= self.desired_success_prob + self.cautious_margin:
            if random.random() < self.epsilon and group_coop_rate >= 0.5:
                return Action.C
            return Action.D
        if P_with_me - P_no_me >= self.pivotal_gain_threshold:
            return Action.C
        if P_no_me >= 0.2 and P_no_me < self.desired_success_prob and (group_coop_rate >= self.group_coop_rate_for_generosity):
            return Action.C
        if pers_defectors >= self.persistent_defectors_threshold:
            if P_with_me - P_no_me >= self.pivotal_gain_threshold:
                return Action.C
            else:
                return Action.D
        if P_with_me >= 0.5 and P_with_me > P_no_me:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_53(BaseStrategy):
    """
    Adaptive Pivotal Conditional Cooperator (APCC)

    - Starts by cooperating to signal goodwill.
    - Uses short windows of past aggregate cooperation to estimate how many
      other players will cooperate this round.
    - Cooperates when it is likely pivotal (others ≈ m-1) or when the group
      has recently been reliably successful.
    - Defects to free-ride if others will meet the threshold without it.
    - Defects (with occasional probes) when cooperation is hopeless (others <= m-2).
    - If the group fails repeatedly (consecutive failures >= S), withdraws cooperation
      (punishment) for a short period, allowing small-probability probes to test recovery.
    - Forgives quickly if signs of recovery appear (recent majority successes).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.L_default = min(5, max(0, self.game_description.n_rounds - 1))
        self.K = 3
        self.S = 2
        self.epsilon0 = 0.05
        self.decay = 0.85
        self.min_epsilon = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1
        if completed == 0:
            return Action.C
        try:
            opp_sums = np.sum(history.opponent_actions.astype(int), axis=1)
        except Exception:
            opp_sums = np.array([int(np.sum(history.opponent_actions[rnd, :])) for rnd in range(completed)])
        my_actions_int = np.array(history.my_actions.astype(int))
        total_cooperators = opp_sums + my_actions_int
        L = min(self.L_default, completed) if self.L_default > 0 else min(5, completed)
        if L <= 0:
            recent_avg_frac = (m - 1) / max(1, n - 1)
        else:
            last_idx_start = max(0, completed - L)
            recent_window = total_cooperators[last_idx_start:completed]
            recent_avg_frac = float(np.mean(recent_window / float(n)))
        expected_cooperators_others = int(round(recent_avg_frac * float(n - 1)))
        k_window = min(self.K, completed)
        if k_window > 0:
            last_k = total_cooperators[max(0, completed - k_window):completed]
            last_k_successes = int(np.sum(last_k >= m))
        else:
            last_k_successes = 0
        consecutive_failures = 0
        for val in total_cooperators[::-1]:
            if int(val) < m:
                consecutive_failures += 1
            else:
                break
        epsilon_t = max(self.min_epsilon, self.epsilon0 * self.decay ** (t - 1))
        majority_recent_success = last_k_successes >= math.ceil(self.K / 2)
        if consecutive_failures >= self.S and (not majority_recent_success):
            if random.random() < epsilon_t:
                return Action.C
            else:
                return Action.D
        action = None
        if expected_cooperators_others >= m:
            action = Action.D
        elif expected_cooperators_others <= m - 2:
            if random.random() < epsilon_t:
                action = Action.C
            else:
                action = Action.D
        else:
            action = Action.C
        if majority_recent_success:
            if expected_cooperators_others >= m - 1:
                action = Action.C
        return action

class Strategy_COOPERATIVE_54(BaseStrategy):
    """
    Adaptive Threshold Conditional Cooperator (ATCC)

    History-dependent, reputation-aware strategy for the Collective Risk Dilemma.
    It uses per-opponent empirical cooperation rates, a pivotality computation
    (probability your cooperation will push the group over the threshold),
    limited targeted punishments for opportunists, occasional probes to revive
    cooperation, and endgame conservatism. Deterministic replay of past rounds
    is used to reconstruct punish state so decisions depend only on game params
    and observed history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta_high = 0.6
        self.theta_low = 0.2
        self.p_probe_base = 0.15
        default_punish_len = min(3, max(1, math.floor(self.game_description.n_rounds / 10)))
        self.punish_len = default_punish_len
        self.endgame_safety_rounds = 2
        self.min_prob_C = 0.05
        self.max_prob_C = 0.95

    def __call__(self, state: GameState, history: None | PlayerHistory):
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            t = 1
            completed_rounds = 0
        else:
            completed_rounds = history.round_number
            t = completed_rounds + 1
        remaining = r - t + 1
        if t == r:
            return Action.D
        n_opp = n - 1
        coop_count = [0] * n_opp
        recent_defect_in_success = [[] for _ in range(n_opp)]
        punish_list = {}
        last_round_success = False
        last_round_C_count = 0
        if history is not None and completed_rounds > 0:
            for rr in range(completed_rounds):
                my_act = bool(history.my_actions[rr])
                opp_row = history.opponent_actions[rr, :]
                for j in range(n_opp):
                    if bool(opp_row[j]):
                        coop_count[j] += 1
                num_C = int(sum((bool(x) for x in opp_row))) + (1 if my_act else 0)
                success = num_C >= m
                if success:
                    for j in range(n_opp):
                        defected = not bool(opp_row[j])
                        recent_defect_in_success[j].append(1 if defected else 0)
                        if len(recent_defect_in_success[j]) > 3:
                            recent_defect_in_success[j].pop(0)
                        if sum(recent_defect_in_success[j]) >= 2:
                            punish_list[j] = self.punish_len
                keys = list(punish_list.keys())
                for j in keys:
                    punish_list[j] -= 1
                    if punish_list[j] <= 0:
                        punish_list.pop(j, None)
                last_round_success = success
                last_round_C_count = num_C
        p_list = []
        for j in range(n_opp):
            if completed_rounds == 0:
                p_j = 0.5
            else:
                p_j = coop_count[j] / float(completed_rounds)
            if j in punish_list and punish_list[j] > 0:
                p_j = p_j * 0.2
            if p_j < 0.0:
                p_j = 0.0
            elif p_j > 1.0:
                p_j = 1.0
            p_list.append(p_j)
        pmf = [1.0]
        for p_j in p_list:
            new_pmf = [0.0] * (len(pmf) + 1)
            for k_idx, prob_k in enumerate(pmf):
                new_pmf[k_idx] += prob_k * (1.0 - p_j)
                new_pmf[k_idx + 1] += prob_k * p_j
            pmf = new_pmf
        needed = max(0, m - 1)
        if needed <= n_opp:
            P_needed = sum(pmf[needed:])
        else:
            P_needed = 0.0
        if remaining <= self.endgame_safety_rounds:
            effective_theta_high = 0.9
            effective_theta_low = 0.6
        else:
            if k > 1.0:
                theta_high_adj = self.theta_high + min(0.2, math.log(k + 1.0) * 0.02)
            else:
                theta_high_adj = self.theta_high
            effective_theta_high = theta_high_adj
            effective_theta_low = self.theta_low
        if last_round_success:
            return Action.C
        if P_needed >= effective_theta_high:
            return Action.C
        if P_needed <= effective_theta_low:
            if any((v > 0 for v in punish_list.values())):
                return Action.D
            p_probe = self.p_probe_base * (remaining / float(r))
            if random.random() < p_probe:
                return Action.C
            else:
                return Action.D
        denom = effective_theta_high - effective_theta_low
        if denom <= 0:
            prob_C = 0.5
        else:
            prob_C = (P_needed - effective_theta_low) / denom
        prob_C = max(self.min_prob_C, min(self.max_prob_C, prob_C))
        if random.random() < prob_C:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_55(BaseStrategy):
    """
    Pivotal Conditional Cooperator with modest optimism, targeted punishment and forgiveness.

    - Cooperate in the first round to signal willingness.
    - Always defect in the final round.
    - In intermediate rounds, estimate each opponent's cooperation probability from a recent window W
      with an optimistic prior p0. Reduce estimated probabilities for players currently under short
      penalties. Cooperate if your cooperation is likely to be pivotal (predicted_others + 1 >= m).
      If others already reach the threshold, provide occasional low-probability support (gamma_support).
      If cooperation is unlikely to help, defect except for a small recruitment noise (epsilon) when
      the gap to the threshold is small.
    - Maintain per-opponent integer penalties that are incremented when opponents exploit or are pivotal
      in failures; penalties decay by 1 each observed round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.W = int(min(10, max(1, self.r)))
        self.p0 = max(0.4, float(self.m) / max(1.0, float(self.n)))
        self.gamma_support = 0.2
        self.epsilon = 0.05
        self.delta_penalty = 0.6
        self.n_opponents = max(0, self.n - 1)
        self.penalties = [0 for _ in range(self.n_opponents)]
        self.last_seen_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = int(history.round_number)
        for ridx in range(self.last_seen_round, completed):
            my_played_coop = bool(history.my_actions[ridx])
            opp_row = history.opponent_actions[ridx]
            number_cooperators = int(my_played_coop) + int(np.sum(opp_row))
            if number_cooperators >= self.m:
                for j in range(self.n_opponents):
                    if not bool(opp_row[j]):
                        self.penalties[j] += 1
            else:
                for j in range(self.n_opponents):
                    if not bool(opp_row[j]):
                        if number_cooperators + 1 >= self.m:
                            self.penalties[j] += 1
            for j in range(self.n_opponents):
                if self.penalties[j] > 0:
                    self.penalties[j] = max(0, self.penalties[j] - 1)
        self.last_seen_round = completed
        current_round = int(state.round_number)
        if current_round >= self.r:
            return Action.D
        base_probs = []
        last_w = min(self.W, completed) if completed > 0 else 0
        for j in range(self.n_opponents):
            if last_w == 0:
                pj = float(self.p0)
            else:
                recent = history.opponent_actions[-last_w:, j]
                pj = float(np.sum(recent) / float(last_w))
            base_probs.append(pj)
        effective_probs = []
        for j in range(self.n_opponents):
            if self.penalties[j] > 0:
                eff = max(0.0, base_probs[j] - self.delta_penalty)
            else:
                eff = base_probs[j]
            effective_probs.append(eff)
        predicted_others = float(sum(effective_probs))
        if predicted_others + 1.0 >= float(self.m):
            return Action.C
        if predicted_others >= float(self.m):
            if random.random() < self.gamma_support:
                return Action.C
            else:
                return Action.D
        gap = float(self.m) - (predicted_others + 1.0)
        if abs(gap - 1.0) <= 0.001:
            if random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_56(BaseStrategy):
    """
    Rotating Minimal Contributors with Deterministic Backup and Forgiving Reciprocity (RMCD).

    - Uses a deterministic rotation S_t of size m to fairly share cooperation.
    - If the previous round failed to reach m cooperators, a minimal set of backup volunteers
      (deterministically chosen by a cyclic priority depending on the round) will cooperate
      next round to attempt repair — but only if there is recent evidence of any cooperation.
    - If the group appears hostile (no cooperation observed in a short recent window),
      do not attempt rescue; instead perform infrequent staggered probes to detect latent cooperators.
    - First round follows rotation. Final round refuses rescue against a hostile recent history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.self_index = self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)

        def rotation_set(round_t: int):
            """Return set of 1-based player indices in S_t of size m."""
            n = self.n
            m = self.m
            start = (round_t - 1) * m % n
            S = set()
            for j in range(m):
                idx = (start + j) % n + 1
                S.add(idx)
            return S
        if history is None:
            S1 = rotation_set(1)
            return Action.C if self.self_index in S1 else Action.D
        last_self_action = bool(history.my_actions[-1])
        last_op_actions = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=np.bool_)
        players_last_round = {}
        for j in range(1, self.n):
            players_last_round[j] = bool(last_op_actions[j - 1])
        players_last_round[self.self_index] = last_self_action
        C_prev = int(sum((1 for v in players_last_round.values() if v)))
        L = min(3, max(0, t - 1))
        recent_coop_rounds = 0
        if L > 0:
            for r_idx in range(-L, 0):
                any_coop = False
                if bool(history.my_actions[r_idx]):
                    any_coop = True
                elif history.opponent_actions.shape[0] >= abs(r_idx):
                    if bool(np.any(history.opponent_actions[r_idx, :])):
                        any_coop = True
                if any_coop:
                    recent_coop_rounds += 1
        S_t = rotation_set(t)
        if C_prev >= self.m:
            return Action.C if self.self_index in S_t else Action.D
        deficit = max(0, self.m - C_prev)
        if t == self.r and recent_coop_rounds == 0 and (C_prev < self.m):
            return Action.D
        if recent_coop_rounds >= 1:
            start = (t - 1) % self.n + 1
            backup_order = []
            for offset in range(self.n):
                idx = (start - 1 + offset) % self.n + 1
                backup_order.append(idx)
            backups = []
            for idx in backup_order:
                if not players_last_round.get(idx, False):
                    backups.append(idx)
                    if len(backups) >= deficit:
                        break
            if len(backups) < deficit:
                for idx in backup_order:
                    if idx not in backups:
                        backups.append(idx)
                        if len(backups) >= deficit:
                            break
            backup_set = set(backups[:deficit])
            if self.self_index in S_t or self.self_index in backup_set:
                return Action.C
            else:
                return Action.D
        P = max(3, int(math.floor(self.r / 5))) if self.r > 0 else 3
        if t % P == self.self_index % P:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_57(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Probing (ACCP)

    - Estimates opponents' cooperation probability from a rolling window of past rounds.
    - Uses binomial belief to compute the probability that exactly m-1 opponents cooperate (pivotal).
    - Cooperates if k * Prob[X = m-1] > 1 (myopic pivotal advantage).
    - Otherwise uses reciprocity: reward successful group rounds, punish short exploitation streaks,
      forgive when the group succeeds, and occasionally probe with small probability.
    - Always defects on the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_max = 10
        self.p_probe = 0.03
        self.Punishment_length = 2
        self.EPS = 1e-09
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if t >= r:
            return Action.D

        def _count_cooperators_round(idx: int, hist: PlayerHistory) -> int:
            my_c = int(hist.my_actions[idx])
            opp_c = int(np.sum(hist.opponent_actions[idx, :])) if hist.opponent_actions.size > 0 else 0
            return my_c + opp_c
        if history is None or history.round_number == 0:
            p_hat = 0.5
            rounds_completed = 0
        else:
            rounds_completed = history.round_number
            W = min(self.W_max, rounds_completed)
            if W <= 0:
                p_hat = 0.5
            else:
                opp_slice = history.opponent_actions[rounds_completed - W:rounds_completed, :]
                sum_opponent_coop = int(np.sum(opp_slice))
                denom = float(W * (n - 1))
                if denom <= 0.0:
                    p_hat = 0.5
                else:
                    p_hat = sum_opponent_coop / denom
            if p_hat < 0.0:
                p_hat = 0.0
            elif p_hat > 1.0:
                p_hat = 1.0

        def binom_pmf(k_term: int, n_trials: int, p: float) -> float:
            if k_term < 0 or k_term > n_trials:
                return 0.0
            if p <= 0.0 + self.EPS:
                return 1.0 if k_term == 0 else 0.0
            if p >= 1.0 - self.EPS:
                return 1.0 if k_term == n_trials else 0.0
            coeff = math.comb(n_trials, k_term)
            return coeff * p ** k_term * (1.0 - p) ** (n_trials - k_term)
        n_trials = n - 1
        k_eq = m - 1
        P_eq = binom_pmf(k_eq, n_trials, p_hat)
        P_ge = 0.0
        if m <= 0:
            P_ge = 1.0
        else:
            for j in range(m, n_trials + 1):
                P_ge += binom_pmf(j, n_trials, p_hat)
        if history is not None and history.round_number >= 1:
            last_idx = history.round_number - 1
            last_count = _count_cooperators_round(last_idx, history)
            if last_count >= m:
                self.punish_counter = 0
        if k * P_eq > 1.0 + 0.0:
            self.punish_counter = 0
            return Action.C
        exploited_last_round = False
        prev_round_success = False
        if history is not None and history.round_number >= 1:
            last_idx = history.round_number - 1
            last_count = _count_cooperators_round(last_idx, history)
            prev_round_success = last_count >= m
            my_last_action = bool(history.my_actions[last_idx])
            if my_last_action and last_count < m:
                exploited_last_round = True
        if self.punish_counter > 0:
            action = Action.D
            self.punish_counter = max(0, self.punish_counter - 1)
            return action
        if exploited_last_round:
            self.punish_counter = int(self.Punishment_length)
            action = Action.D
            self.punish_counter = max(0, self.punish_counter - 1)
            return action
        if prev_round_success:
            return Action.C
        if random.random() < float(self.p_probe):
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_58(BaseStrategy):
    """
    Pivotal-Reciprocal strategy for the Collective Risk Dilemma.

    - Estimates other players' cooperation probability from a recent window (W).
    - Computes the probability that your single cooperation is pivotal.
    - Cooperates if the marginal expected benefit (k * Pr_piv) >= cost (1) within a small bias b.
    - Rewards recent strong group success by temporarily removing bias.
    - If exploited (you cooperated while threshold failed) in recent rounds, punish by defecting for up to P_max rounds.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.W_default = min(5, max(1, self.r - 1))
        self.b_default = 0.05
        self.P_max = 3
        self.S_success = 0.6
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == self.r:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        W = min(self.W_default, max(0, self.r - 1))
        if history is None or history.round_number == 0:
            p = (self.m - 1) / max(1, self.n - 1)
        else:
            rounds_played = history.round_number
            rounds_considered = min(W, rounds_played)
            if rounds_considered <= 0:
                p = (self.m - 1) / max(1, self.n - 1)
            else:
                start_idx = rounds_played - rounds_considered
                f_list = []
                for s in range(start_idx, rounds_played):
                    you_coop = bool(history.my_actions[s])
                    opp_coops = int(np.sum(history.opponent_actions[s, :]))
                    C_s = (1 if you_coop else 0) + opp_coops
                    f_s = (C_s - (1 if you_coop else 0)) / max(1, self.n - 1)
                    f_list.append(f_s)
                p = float(np.mean(f_list))
        if p < 0.0:
            p = 0.0
        elif p > 1.0:
            p = 1.0

        def binom_pmf(N: int, k: int, prob: float) -> float:
            if prob <= 0.0:
                return 1.0 if k == 0 else 0.0
            if prob >= 1.0:
                return 1.0 if k == N else 0.0
            coeff = math.comb(N, k)
            return coeff * prob ** k * (1.0 - prob) ** (N - k)
        N_others = self.n - 1
        target_k = self.m - 1
        if target_k < 0 or target_k > N_others:
            Pr_piv = 0.0
        else:
            Pr_piv = binom_pmf(N_others, target_k, p)
        q_prime = 0.0
        if self.m <= N_others:
            for kk in range(self.m, N_others + 1):
                q_prime += binom_pmf(N_others, kk, p)
        EU_margin = self.k * Pr_piv
        exploited_flag = False
        recent_success_fraction = 0.0
        if history is not None and history.round_number > 0:
            rounds_played = history.round_number
            rounds_considered = min(W, rounds_played)
            if rounds_considered > 0:
                start_idx = rounds_played - rounds_considered
                success_count = 0
                exploit_count = 0
                for s in range(start_idx, rounds_played):
                    you_coop = bool(history.my_actions[s])
                    opp_coops = int(np.sum(history.opponent_actions[s, :]))
                    C_s = (1 if you_coop else 0) + opp_coops
                    if C_s >= self.m:
                        success_count += 1
                    if you_coop and C_s < self.m:
                        exploit_count += 1
                recent_success_fraction = success_count / rounds_considered
                exploited_flag = exploit_count > 0
            else:
                recent_success_fraction = 0.0
                exploited_flag = False
        else:
            recent_success_fraction = 0.0
            exploited_flag = False
        local_b = 0.0 if recent_success_fraction >= self.S_success and (not exploited_flag) else self.b_default
        if EU_margin >= 1.0 - local_b:
            return Action.C
        if exploited_flag:
            remaining_rounds = max(0, self.r - t)
            self.punishment_timer = min(self.P_max, remaining_rounds)
            return Action.D
        return Action.D

class Strategy_COOPERATIVE_59(BaseStrategy):
    """
    Pivotal Tit-for-Tat with Rotating Filler (PTF-RF)

    - Start by cooperating.
    - Always defect in the final round.
    - Maintain a short punishment counter (self.punish_count).
    - If punished: defect until punish_count expires.
    - If previous round reached the threshold (s >= m): cooperate.
    - If previous round had s <= m-2: defect; if you cooperated last round, start a short punishment.
    - If previous round had s == m-1: continue cooperating if you cooperated previously;
      otherwise, among the previous round's defectors the one with the smallest (public) index
      volunteers to cooperate (deterministic filler). If you were exploited (you cooperated but
      s < m) set a short punishment.
    - Punishment length P = min(2, r - t) so punishments do not run into the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_count = 0
        self.my_index = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            return Action.C
        if t == r:
            return Action.D
        if self.punish_count > 0:
            self.punish_count = max(0, self.punish_count - 1)
            return Action.D
        my_prev_action = bool(history.my_actions[-1])
        opp_coops_last = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
        s = opp_coops_last + (1 if my_prev_action else 0)

        def compute_P():
            remaining = max(0, r - t)
            return min(2, remaining)
        if s >= m:
            return Action.C
        if s <= m - 2:
            if my_prev_action:
                P = compute_P()
                if P > 0:
                    self.punish_count = P
            return Action.D
        if my_prev_action:
            if s < m:
                P = compute_P()
                if P > 0:
                    self.punish_count = P
            return Action.C
        opps_last = list(map(bool, history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else []
        all_actions_last = [my_prev_action] + opps_last
        Dprev = [i for i, acted in enumerate(all_actions_last, start=1) if not acted]
        if len(Dprev) == 0:
            return Action.D
        smallest_defector = min(Dprev)
        if self.my_index == smallest_defector:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_60(BaseStrategy):
    """
    Pivotal Reciprocity (PiRe)

    - Cooperates when the expected pivotal benefit k * Prob(others = m-1) >= 1 (tie -> cooperate).
    - Uses per-opponent empirical cooperation frequencies with Laplace smoothing (alpha=1).
    - Tracks recent exploitation (I cooperated but group failed) in a sliding window (W=5).
    - If exploitation_count >= E (2) triggers a short punishment of length L (3) rounds.
    - Applies a forgiveness_factor gamma (0.5) to reduce estimated p_j for opponents who
      defected in exploitation-triggering rounds; restores their weight when they cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.W = 5
        self.E = 2
        self.L = 3
        self.gamma = 0.5
        self.punish_until_round = 0
        n_minus_1 = max(0, self.game_description.n_players - 1)
        self.forgive_scales = np.ones(n_minus_1, dtype=float)
        self.exploitation_flags = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        t = int(state.round_number)
        if history is None:
            observed_rounds = 0
            cooper_count = np.zeros(n - 1, dtype=int)
            self.exploitation_flags = []
        else:
            observed_rounds = int(history.round_number)
            if observed_rounds > 0:
                opp_actions = np.asarray(history.opponent_actions, dtype=bool)
                cooper_count = np.sum(opp_actions, axis=0).astype(int)
            else:
                cooper_count = np.zeros(n - 1, dtype=int)
            flags = []
            if observed_rounds > 0:
                my_actions = np.asarray(history.my_actions, dtype=bool)
                start_idx = max(0, observed_rounds - self.W)
                for idx in range(start_idx, observed_rounds):
                    i_cooperated = bool(my_actions[idx])
                    if observed_rounds > 0:
                        total_cooperators = int(np.sum(opp_actions[idx, :])) + (1 if i_cooperated else 0)
                    else:
                        total_cooperators = 1 if i_cooperated else 0
                    exploited = i_cooperated and total_cooperators < m
                    flags.append(bool(exploited))
            self.exploitation_flags = flags
            if observed_rounds > 0:
                last_round_idx = observed_rounds - 1
                last_round_opp_actions = np.asarray(history.opponent_actions[last_round_idx, :], dtype=bool)
                for j in range(n - 1):
                    if bool(last_round_opp_actions[j]):
                        self.forgive_scales[j] = 1.0
        exploitation_counter = int(sum((1 for v in self.exploitation_flags if v)))
        if self.punish_until_round > 0 and observed_rounds > self.punish_until_round:
            self.punish_until_round = 0
            self.exploitation_flags = []
            exploitation_counter = 0
        if self.punish_until_round == 0 and exploitation_counter >= self.E and (observed_rounds < r):
            new_punish_until = min(r, observed_rounds + self.L)
            self.punish_until_round = int(new_punish_until)
            if history is not None and observed_rounds > 0:
                start_idx = max(0, observed_rounds - self.W)
                opp_actions = np.asarray(history.opponent_actions, dtype=bool)
                for idx in range(start_idx, observed_rounds):
                    i_cooperated = bool(history.my_actions[idx])
                    total_cooperators = int(np.sum(opp_actions[idx, :])) + (1 if i_cooperated else 0)
                    if i_cooperated and total_cooperators < m:
                        for j in range(n - 1):
                            if not bool(opp_actions[idx, j]):
                                self.forgive_scales[j] = max(1e-06, self.forgive_scales[j] * self.gamma)
        if t <= self.punish_until_round and self.punish_until_round > 0:
            return Action.D
        denom = observed_rounds + 2.0 * self.alpha
        if denom <= 0:
            denom = 2.0 * self.alpha
        p = np.zeros(n - 1, dtype=float)
        if n - 1 > 0:
            p = (cooper_count.astype(float) + self.alpha) / denom
            p = np.clip(p * self.forgive_scales, 0.0, 1.0)
        max_others = n - 1
        prob = [0.0] * (max_others + 1)
        prob[0] = 1.0
        for j in range(max_others):
            pj = float(p[j]) if max_others > 0 else 0.0
            new_prob = [0.0] * (max_others + 1)
            for s in range(0, max_others + 1):
                new_prob[s] += prob[s] * (1.0 - pj)
                if s + 1 <= max_others:
                    new_prob[s + 1] += prob[s] * pj
            prob = new_prob
        target = m - 1
        prob_others_eq_m_1 = 0.0
        if 0 <= target <= max_others:
            prob_others_eq_m_1 = prob[target]
        threshold_value = k * prob_others_eq_m_1
        if threshold_value >= 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_61(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Forgiveness and Probing (ACCFP).

    - Starts cooperating.
    - If the previous round met the cooperation threshold (>= m cooperators), continue cooperating.
    - If a round fails (< m cooperators), defect for a short punishment length P (then forgive).
    - If many recent rounds fail (success_rate < phi over a lookback L), enter defensive_mode:
      mostly defect, but with small probe probability epsilon play C to test whether cooperation resumed.
    - In defensive_mode, exit when probe-led successes indicate cooperation has returned.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.punishment_counter = 0
        self.defensive_mode = False
        self._pending_probes = []
        self._probe_outcomes = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.r
        m = self.m
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == r:
            self.punishment_counter = 0
            return Action.D
        if history is not None and history.round_number > 0:
            processed = []
            for pr in list(self._pending_probes):
                if pr <= history.round_number:
                    idx = pr - 1
                    my_c = bool(history.my_actions[idx])
                    opp_cs = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
                    coop_count = int(my_c) + int(opp_cs)
                    success = coop_count >= m
                    self._probe_outcomes.append(bool(success))
                    self._pending_probes.remove(pr)
            if self.defensive_mode and len(self._probe_outcomes) > 0:
                recent_success_rate = float(np.mean(self._probe_outcomes))
                phi = 0.3
                if recent_success_rate > phi:
                    self.defensive_mode = False
                    self._pending_probes = []
                    self._probe_outcomes = []
        epsilon = 0.05
        phi = 0.3
        P = max(1, round(min(3, r / 10.0)))
        if history is None or t == 1:
            success_rate = 0.5
            L = 0
        else:
            L = min(10, t - 1)
            start_round = max(1, t - L)
            start_idx = start_round - 1
            end_idx = t - 1 - 1
            if end_idx < start_idx:
                success_rate = 0.5
            else:
                successes = 0
                total = end_idx - start_idx + 1
                for idx in range(start_idx, end_idx + 1):
                    my_c = bool(history.my_actions[idx])
                    opp_cs = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
                    coop_count = int(my_c) + int(opp_cs)
                    if coop_count >= m:
                        successes += 1
                success_rate = successes / total if total > 0 else 0.5
        if self.defensive_mode:
            if random.random() < epsilon:
                self._pending_probes.append(t)
                return Action.C
            else:
                return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if history is not None and L > 0 and (success_rate < phi):
            self.defensive_mode = True
            self.punishment_counter = 0
            return Action.D
        if t == 1:
            return Action.C
        last_idx = t - 2
        my_last = bool(history.my_actions[last_idx])
        opp_last = int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size > 0 else 0
        coop_count_last = int(my_last) + int(opp_last)
        if coop_count_last >= m:
            return Action.C
        else:
            self.punishment_counter = max(0, P - 1)
            return Action.D

class Strategy_COOPERATIVE_62(BaseStrategy):
    """
    Pivotal Cooperator with light smoothing and short punishment.

    Cooperate only when your cooperation is plausibly pivotal: estimate other
    players' cooperation probability with Laplace smoothing, compute the
    binomial probability that exactly m-1 of the other n-1 players cooperate,
    and cooperate iff that probability >= 1/k (tie-breaker favors cooperation).
    If you were exploited in a recent round (you cooperated but the threshold
    failed), withhold cooperation for tau rounds (punishment), implemented by
    detecting the most recent exploitation in history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prior_p = 0.5
        self.prior_weight = 1.0
        self.tau = 2
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            p_hat = float(self.prior_p)
            T = 0
        else:
            T = int(history.round_number)
            if T == 0:
                p_hat = float(self.prior_p)
            else:
                S = float(np.sum(history.opponent_actions))
                denom = float(T * (self.n - 1)) + float(self.prior_weight)
                if denom <= 0.0:
                    p_hat = float(self.prior_p)
                else:
                    p_hat = (S + float(self.prior_weight) * float(self.prior_p)) / denom
        punish_active = False
        if history is not None and T > 0:
            last_exploit_idx = None
            for idx in range(T - 1, -1, -1):
                my_coop = bool(history.my_actions[idx])
                opp_coops = int(np.sum(history.opponent_actions[idx, :]))
                total_coops = opp_coops + (1 if my_coop else 0)
                if my_coop and total_coops < self.m:
                    last_exploit_idx = idx
                    break
            if last_exploit_idx is not None:
                rounds_since = T - 1 - last_exploit_idx
                if rounds_since < self.tau:
                    punish_active = True
        if punish_active:
            return Action.D
        N = self.n - 1
        k_target = self.m - 1
        P_eq = 0.0
        if 0 <= k_target <= N:
            try:
                coeff = float(math.comb(N, k_target))
            except AttributeError:
                coeff = float(1)
                for i in range(1, k_target + 1):
                    coeff *= (N - (k_target - i)) / i
            if p_hat <= 0.0:
                P_eq = 1.0 if k_target == 0 else 0.0
            elif p_hat >= 1.0:
                P_eq = 1.0 if k_target == N else 0.0
            else:
                P_eq = coeff * p_hat ** k_target * (1.0 - p_hat) ** (N - k_target)
        else:
            P_eq = 0.0
        threshold = 1.0 / float(self.k) if self.k != 0.0 else float('inf')
        eps = 1e-12
        if P_eq + eps >= threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_63(BaseStrategy):
    """
    Collective-risk minimal-cooperator rotation with short-memory trust and pivoting.

    - Uses a deterministic cyclic assignment of m cooperators per round (fair rotation).
    - Tracks recent cooperation frequencies F_j over a window W = min(10, r).
    - When assigned, cooperates if assigned teammates look trustworthy (>= T_trust recent reliability),
      otherwise cooperates only if likely pivotal this round.
    - When not assigned, defects unless others are expected to fail and you are pivotal (and personally
      reasonably willing: F_self >= 0.2).
    - In the last round, uses one-shot pivot logic: cooperate only if your cooperation is necessary to
      reach the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = min(10, int(self.game_description.n_rounds))
        self.T_trust = 0.5
        self._self_index = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None:
            completed_rounds = 0
        else:
            completed_rounds = int(history.round_number)
        t = completed_rounds + 1

        def assigned_set_for_round(round_idx: int):
            base = (round_idx - 1) * m
            S = []
            for s in range(m):
                player_idx = (base + s) % n + 1
                S.append(player_idx)
            return set(S)
        S_t = assigned_set_for_round(t)
        assigned = self._self_index in S_t
        if history is None:
            return Action.C if assigned else Action.D
        window = min(self.W, completed_rounds)
        if window <= 0:
            F = [0.0 for _ in range(n)]
        else:
            start = completed_rounds - window
            end = completed_rounds
            F = [0.0 for _ in range(n)]
            my_slice = history.my_actions[start:end]
            try:
                F_self = float(np.mean(my_slice.astype(float))) if my_slice.size > 0 else 0.0
            except Exception:
                F_self = float(sum((bool(x) for x in my_slice)) / len(my_slice)) if len(my_slice) > 0 else 0.0
            F[0] = F_self
            opp_actions = history.opponent_actions
            for j in range(2, n + 1):
                col = j - 2
                col_slice = opp_actions[start:end, col]
                try:
                    F_j = float(np.mean(col_slice.astype(float))) if col_slice.size > 0 else 0.0
                except Exception:
                    F_j = float(sum((bool(x) for x in col_slice)) / len(col_slice)) if len(col_slice) > 0 else 0.0
                F[j - 1] = F_j
        trusted_count = 0
        for j in S_t:
            if F[j - 1] >= 0.5:
                trusted_count += 1
        assigned_recent_coop_fraction = trusted_count / float(m)
        E_others = sum((F[j] for j in range(n) if j + 1 != self._self_index))
        E_with_me = E_others + 1.0
        E_without_me = E_others
        if t < r:
            if assigned:
                if assigned_recent_coop_fraction >= self.T_trust:
                    return Action.C
                elif E_with_me >= m and E_without_me < m:
                    return Action.C
                else:
                    return Action.D
            elif E_without_me >= m:
                return Action.D
            elif E_with_me >= m and E_without_me < m:
                F_self = F[self._self_index - 1]
                if F_self >= 0.2:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        elif E_without_me >= m:
            return Action.D
        elif E_with_me >= m and E_without_me < m:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_64(BaseStrategy):
    """
    Pivotal Bayesian Cooperator (PBC)

    - Maintains per-opponent cooperation probabilities using EMA (default) with modest cooperative prior.
    - Computes Poisson-Binomial over other players to estimate P(S = m-1).
    - Cooperates if P(S = m-1) > effective_threshold where effective_threshold = (1/k) * (1 - generosity_bias).
    - Applies modest generosity in early rounds, turns off generosity near the end.
    - If exploited (you cooperated and group failed), reduces trust in defecting opponents by exploitation_decay.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(self.game.n_players)
        self.r = int(self.game.n_rounds)
        self.m = int(self.game.m)
        self.k = float(self.game.k)
        self.prior_alpha = 1.0
        self.prior_p0 = float(self.m) / max(1.0, float(self.n))
        self.ema_lambda = 0.25
        self.generosity_window = math.ceil(self.r / 3) if self.r > 0 else 0
        self.generosity_strength = 0.1
        self.exploitation_decay = 0.15
        self.p_js = None
        self.last_rounds_observed = 0
        self.exploit_streak = 0
        self.epsilon = 1e-06

    def _ensure_initialized(self):
        if self.p_js is None:
            opponents = max(0, self.n - 1)
            self.p_js = np.full(opponents, self.prior_p0, dtype=float)
            self.last_rounds_observed = 0
            self.exploit_streak = 0

    def _update_from_history(self, history: PlayerHistory):
        self._ensure_initialized()
        rounds_seen = history.round_number
        if rounds_seen <= self.last_rounds_observed:
            return
        for t in range(self.last_rounds_observed, rounds_seen):
            opp_actions_t = np.array(history.opponent_actions[t, :], dtype=float)
            my_action_t = float(history.my_actions[t])
            min_len = min(len(self.p_js), opp_actions_t.shape[0])
            if min_len > 0:
                self.p_js[:min_len] = (1.0 - self.ema_lambda) * self.p_js[:min_len] + self.ema_lambda * opp_actions_t[:min_len]
            total_cooperators = int(my_action_t + np.sum(opp_actions_t))
            if my_action_t == 1.0 and total_cooperators < self.m:
                defectors_idx = np.where(opp_actions_t[:min_len] == 0)[0]
                for idx in defectors_idx:
                    self.p_js[idx] = max(0.0, self.p_js[idx] - self.exploitation_decay)
                self.exploit_streak += 1
                if self.exploit_streak >= 2:
                    for idx in defectors_idx:
                        self.p_js[idx] = max(0.0, self.p_js[idx] - self.exploitation_decay)
            else:
                self.exploit_streak = 0
        self.last_rounds_observed = rounds_seen

    def _poisson_binomial_prob_s(self, s: int) -> float:
        if self.p_js is None:
            return 0.0
        N = len(self.p_js)
        if s < 0 or s > N:
            return 0.0
        prob = np.zeros(N + 1, dtype=float)
        prob[0] = 1.0
        for p in self.p_js:
            for t in range(N - 1, -1, -1):
                prob[t + 1] += prob[t] * p
                prob[t] *= 1.0 - p
        return float(prob[s])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self._ensure_initialized()
        if history is None:
            rounds_remaining = self.r
            generosity_bias = self.generosity_strength if rounds_remaining > self.generosity_window else 0.0
            effective_threshold = 1.0 / self.k * (1.0 - generosity_bias)
            p_m1 = self._poisson_binomial_prob_s(self.m - 1)
            if p_m1 > effective_threshold + self.epsilon:
                return Action.C
            if abs(p_m1 - effective_threshold) <= self.epsilon:
                return Action.C
            return Action.D
        self._update_from_history(history)
        rounds_played = history.round_number
        rounds_remaining = max(0, self.r - rounds_played)
        last_rounds_cutoff = min(2, math.ceil(self.r / 10)) if self.r > 0 else 0
        if rounds_remaining <= last_rounds_cutoff:
            generosity_bias = 0.0
        else:
            generosity_bias = self.generosity_strength if rounds_remaining > self.generosity_window else 0.0
        effective_threshold = 1.0 / self.k * (1.0 - generosity_bias)
        p_m1 = self._poisson_binomial_prob_s(self.m - 1)
        prefer_C_on_tie = False
        if rounds_played >= 1:
            last_idx = rounds_played - 1
            opps_last = np.array(history.opponent_actions[last_idx, :], dtype=float)
            my_last = float(history.my_actions[last_idx])
            total_coop_last = int(np.sum(opps_last) + my_last)
            if total_coop_last >= self.m:
                prefer_C_on_tie = True
        if p_m1 > effective_threshold + self.epsilon:
            return Action.C
        if p_m1 < effective_threshold - self.epsilon:
            return Action.D
        if prefer_C_on_tie:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_65(BaseStrategy):
    """
    Pivotal-Responsive Forgiving (PRF)

    - Start by cooperating to signal.
    - Cooperate in non-final rounds when plausibly pivotal (others mean rounds to m-1)
      or when the group has been reliably meeting the threshold recently.
    - If exploited (you cooperated but threshold failed), punish by defecting for P rounds,
      then forgive and resume normal rule.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.L_default = 5
        self.phi = 0.6
        r = max(1, getattr(game_description, 'n_rounds', 1))
        self.P_default = max(1, round(0.1 * r))
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', 1)
        n = getattr(self.game_description, 'n_players', 2)
        r = getattr(self.game_description, 'n_rounds', 1)
        m = getattr(self.game_description, 'm', 1)
        if history is None:
            if t >= r:
                return Action.D
            return Action.C
        completed = history.round_number
        if completed >= 1:
            last_idx = completed - 1
            try:
                others_last = int(np.sum(history.opponent_actions[last_idx, :]))
            except Exception:
                others_last = int(sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.shape[1] > 0 else 0
            my_last = bool(history.my_actions[last_idx])
            S_last = others_last + (1 if my_last else 0)
            if my_last and S_last < m:
                self.punishment_counter = max(1, round(0.1 * r))
        if t >= r:
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.D
        if completed == 0 or t == 1:
            return Action.C
        L = min(self.L_default, completed)
        if completed > 0:
            others_counts_full = np.sum(history.opponent_actions[:completed, :], axis=1).astype(float)
            my_actions_int = history.my_actions[:completed].astype(float)
            S_counts_full = others_counts_full + my_actions_int
        else:
            others_counts_full = np.array([], dtype=float)
            S_counts_full = np.array([], dtype=float)
        start_idx = max(0, completed - L)
        if start_idx < completed:
            window_others = others_counts_full[start_idx:completed]
            window_S = S_counts_full[start_idx:completed]
        else:
            window_others = np.array([], dtype=float)
            window_S = np.array([], dtype=float)
        if window_others.size > 0:
            others_mean = float(np.mean(window_others))
            recent_success_rate = float(np.mean(window_S >= m))
        else:
            others_mean = 0.0
            recent_success_rate = 0.0
        if others_mean >= m:
            return Action.D
        if round(others_mean) == m - 1:
            return Action.C
        if others_mean < m - 1:
            if recent_success_rate >= self.phi:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_66(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR)

    - Estimates each opponent's cooperation probability from a recent window with Laplace smoothing.
    - Computes the chance my cooperation is pivotal (leading to meeting the threshold).
    - Cooperates when the expected single-round benefit from cooperating outweighs defecting,
      subject to a small safety margin epsilon and endgame conservatism.
    - Implements bounded punishments when I am exploited (I cooperated but threshold failed),
      with punish length growth up to a cap and forgiveness (halving) when cooperation resumes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.W = min(50, max(5, max(0, self.r - 1)))
        self.alpha = 1.0
        self.p0 = float(self.m) / float(self.n) if self.n > 0 else 0.5
        self.PunishBase = 2
        self.PunishCap = 6
        self.ForgivenessDecay = 2.0
        self.epsilon = 0.0
        self.punishing_until = 0
        self.punish_length = 0
        self._last_exploitation_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)

        def tail_at_least(k_threshold: int, mu: float, sigma2: float) -> float:
            if k_threshold <= 0:
                return 1.0
            if sigma2 <= 0.0:
                return 1.0 if mu + 1e-12 >= k_threshold else 0.0
            sigma = math.sqrt(sigma2)
            z = (k_threshold - 0.5 - mu) / sigma
            phi = 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))
            return max(0.0, min(1.0, 1.0 - phi))
        if history is None:
            if self.n <= 1:
                return Action.D
            others = self.n - 1
            mu = self.p0 * others
            sigma2 = self.p0 * (1.0 - self.p0) * others
            q_C = tail_at_least(max(0, self.m - 1), mu, sigma2)
            q_D = tail_at_least(max(0, self.m), mu, sigma2)
            eu_C = q_C * self.k
            eu_D = 1.0 + q_D * self.k
            adjusted_eps = self.epsilon
            prob_exceed_if_C = tail_at_least(max(0, self.m), mu, sigma2)
            if eu_C >= eu_D - adjusted_eps:
                if eu_C - eu_D >= -1e-12 and prob_exceed_if_C > 0.6:
                    return Action.D
                return Action.C
            else:
                return Action.D
        completed = int(history.round_number)
        if completed == 0:
            return self.__call__(state, None)
        if completed >= 1 and self._last_exploitation_processed_round < completed:
            last_idx = completed - 1
            my_last_action = bool(history.my_actions[last_idx])
            others_last = int(np.sum(history.opponent_actions[last_idx, :]))
            total_last = others_last + (1 if my_last_action else 0)
            if my_last_action and total_last < self.m:
                new_length = max(self.PunishBase, self.punish_length + 1)
                new_length = min(self.PunishCap, new_length)
                self.punish_length = new_length
                self.punishing_until = max(self.punishing_until, t + self.punish_length - 1)
            elif total_last >= self.m and self.punish_length > self.PunishBase:
                self.punish_length = max(self.PunishBase, self.punish_length // int(self.ForgivenessDecay))
            self._last_exploitation_processed_round = completed
        if t <= self.punishing_until:
            return Action.D
        rounds_considered = min(self.W, completed)
        if rounds_considered <= 0:
            p_vec = np.full(self.n - 1, self.p0, dtype=float)
        else:
            start_idx = completed - rounds_considered
            recent = history.opponent_actions[start_idx:start_idx + rounds_considered, :]
            coop_counts = np.sum(recent.astype(float), axis=0)
            denom = self.alpha + rounds_considered
            if denom <= 0:
                p_vec = np.full(self.n - 1, self.p0, dtype=float)
            else:
                p_vec = (self.alpha + coop_counts) / denom
                p_vec = np.minimum(1.0, np.maximum(0.0, p_vec))
        mu = float(np.sum(p_vec))
        sigma2 = float(np.sum(p_vec * (1.0 - p_vec)))
        q_C = tail_at_least(max(0, self.m - 1), mu, sigma2)
        q_D = tail_at_least(max(0, self.m), mu, sigma2)
        eu_C = q_C * self.k
        eu_D = 1.0 + q_D * self.k
        remaining = max(0, self.r - t + 1)
        endgame_scale = 1.0 + (1.0 - remaining / float(max(1, self.r)))
        adjusted_eps = self.epsilon * endgame_scale
        prob_exceed_if_C = tail_at_least(max(0, self.m), mu, sigma2)
        if eu_C >= eu_D - adjusted_eps:
            if abs(eu_C - eu_D) <= max(1e-12, adjusted_eps) and prob_exceed_if_C > 0.6:
                return Action.D
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_67(BaseStrategy):
    """
    Threshold Rotator with Conditional Fill and Forgiveness (TR-CFF).

    - Tries to ensure at least m cooperators each round.
    - Rotates contribution burden by selecting the m players with lowest recent
      cooperation rates as the "target set".
    - Fills vacancies when others are unlikely to reach the threshold.
    - Uses short-term rescue attempts after a few failures and withdraws after
      persistent failures (defensive mode). Small randomized relaxation and
      rescue probabilities avoid pathological coordination cycles.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.L = min(10, self.game_description.n_rounds)
        self.eps_relax = 0.1
        self.T_rescue = 2
        self.p_rescue = 0.5
        self.T_defect = 4
        self.F = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if completed == 0:
            coop_rates = np.zeros(n, dtype=float)
            fail_streak = 0
            total_cooperators_last = 0
        else:
            my_actions = np.asarray(history.my_actions, dtype=int)
            opp_actions = np.asarray(history.opponent_actions, dtype=int)
            if opp_actions.size == 0:
                actions_matrix = my_actions.reshape((completed, 1))
            else:
                actions_matrix = np.concatenate((my_actions.reshape((completed, 1)), opp_actions), axis=1)
            total_cooperators = np.sum(actions_matrix, axis=1)
            fail_streak = 0
            for val in total_cooperators[::-1]:
                if val < m:
                    fail_streak += 1
                else:
                    break
            window = min(self.L, completed)
            if window == 0:
                coop_rates = np.zeros(n, dtype=float)
            else:
                recent = actions_matrix[-window:, :]
                denom = window
                coop_rates = np.sum(recent, axis=0) / float(denom)
            total_cooperators_last = int(total_cooperators[-1])
        defensive_mode = fail_streak >= self.T_defect
        indices = np.arange(n)
        order = np.lexsort((indices, coop_rates))
        target_set = set((int(x) for x in order[:m]))
        expected_other_cooperators = float(np.sum(coop_rates) - coop_rates[0])
        is_last_round = t == r
        if defensive_mode:
            if is_last_round:
                pass
            else:
                return Action.D
        if is_last_round:
            if expected_other_cooperators >= m:
                return Action.D
            if expected_other_cooperators <= m - 1:
                return Action.C
            if 0 in target_set and abs(expected_other_cooperators - (m - 1)) <= 0.5:
                return Action.C
            return Action.D
        if completed == 0:
            return Action.C
        if 0 in target_set:
            if expected_other_cooperators >= m and random.random() < self.eps_relax:
                return Action.D
            else:
                return Action.C
        elif expected_other_cooperators >= m:
            return Action.D
        elif expected_other_cooperators >= m - 1:
            return Action.C
        elif fail_streak >= self.T_rescue:
            if random.random() < self.p_rescue:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_68(BaseStrategy):
    """
    Conditional Rotating-Volunteer with Forgiveness (CRV-F)

    - Uses a deterministic rotating volunteer set S_t to break ties.
    - Estimates others' cooperation propensities over a short window K (<=5).
    - Enters a short protective mode if recently exploited, only probing occasionally.
    - Last-round defection.
    Note: We assume this agent's player index is 1 (i = 1). The opponent arrays
    provided in history correspond to the other n-1 players in some fixed order.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.K = 5
        self.L = 3
        self.p_probe = 0.05
        self.T = 2
        self.protective_mode_until = 0
        self.my_index = 1

    def _S_t_contains(self, t: int, player_index: int) -> bool:
        """Return True if player_index (1-based) is in S_t."""
        n = self.game_description.n_players
        m = self.game_description.m
        val = (player_index - 1 + (t - 1)) % n
        return val < m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if t >= r:
            return Action.D
        if history is None or history.round_number == 0:
            if self._S_t_contains(1, self.my_index):
                return Action.C
            else:
                return Action.D
        completed_rounds = history.round_number
        K_eff = min(self.K, completed_rounds)
        L_eff = min(self.L, completed_rounds)
        if K_eff == 0:
            if self._S_t_contains(t, self.my_index):
                return Action.C
            else:
                return Action.D
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            if self._S_t_contains(t, self.my_index):
                return Action.C
            else:
                return Action.D
        opp_lastK = opp_actions[-K_eff:, :]
        p_js = np.mean(opp_lastK.astype(float), axis=0)
        E = float(np.sum(p_js))
        my_actions = history.my_actions
        opp_lastL = opp_actions[-L_eff:, :]
        my_lastL = my_actions[-L_eff:]
        opp_coop_counts_lastL = np.sum(opp_lastL.astype(int), axis=1)
        my_lastL_int = my_lastL.astype(int)
        coop_counts_lastL = opp_coop_counts_lastL + my_lastL_int
        recent_successes = int(np.sum(coop_counts_lastL >= m))
        recent_failures = int(L_eff - recent_successes)
        exploited_mask = (my_lastL_int == 1) & (coop_counts_lastL < m)
        exploited_count = int(np.sum(exploited_mask))
        coop_rounds_we_played = int(np.sum(my_lastL_int))
        if self.protective_mode_until >= t:
            if random.random() < self.p_probe:
                chosen = Action.C
            else:
                chosen = Action.D
            exploit_trigger = False
            if coop_rounds_we_played > 0:
                half_threshold = math.ceil(coop_rounds_we_played / 2)
                if exploited_count >= half_threshold:
                    exploit_trigger = True
            if recent_failures > recent_successes:
                exploit_trigger = True
            if exploit_trigger:
                self.protective_mode_until = max(self.protective_mode_until, t + self.T)
            return chosen
        if E >= m:
            chosen = Action.D
        elif E <= m - 1:
            chosen = Action.C
        elif self._S_t_contains(t, self.my_index):
            chosen = Action.C
        else:
            chosen = Action.D
        exploit_trigger = False
        if coop_rounds_we_played > 0:
            half_threshold = math.ceil(coop_rounds_we_played / 2)
            if exploited_count >= half_threshold:
                exploit_trigger = True
        if recent_failures > recent_successes:
            exploit_trigger = True
        if exploit_trigger:
            self.protective_mode_until = t + self.T
        return chosen

class Strategy_COOPERATIVE_69(BaseStrategy):
    """
    Pivotality-based, recency-weighted cooperative strategy for the Collective Risk Dilemma.

    - Estimates each opponent's cooperation probability p_j with a recency-weighted (exponential)
      smoothing of their recent actions (or a prior p0 if no history).
    - Computes the Poisson–binomial probability that exactly (m-1) others cooperate (Pr_exact).
    - Computes marginal = k * Pr_exact (the myopic marginal benefit of cooperating).
    - Uses a generosity schedule that is strongest in early rounds and decays toward zero at the last round:
        threshold = 1 - gamma * ((r - t) / (r - 1))
      Cooperate if marginal >= threshold (tie-break: cooperate).
    - On the very last round (t == r) uses the strict myopic threshold of 1 (no generosity).
    - Uses exact DP for Poisson–binomial; for very large numbers of opponents falls back to a normal approx.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.5
        self.gamma = 0.5
        self.alpha = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k_reward = self.game_description.k
        t = int(state.round_number)
        rounds_played = 0 if history is None else history.round_number
        W = min(10, rounds_played)
        p_vector = []
        if history is None:
            p_vector = [self.p0] * (n - 1)
        else:
            opp_actions = history.opponent_actions
            rounds_available = opp_actions.shape[0]
            for j in range(opp_actions.shape[1]):
                if rounds_available == 0:
                    p_j = self.p0
                elif W <= 0:
                    p_j = self.p0
                else:
                    seq = opp_actions[max(0, rounds_available - W):rounds_available, j]
                    p_j = 0.0
                    a = float(self.alpha)
                    for val in seq:
                        p_j = (1.0 - a) * p_j + a * (1.0 if val else 0.0)
                    if np.isnan(p_j):
                        p_j = self.p0
                p_vector.append(float(p_j))
            if len(p_vector) < n - 1:
                p_vector += [self.p0] * (n - 1 - len(p_vector))
            elif len(p_vector) > n - 1:
                p_vector = p_vector[:n - 1]

        def compute_prob_exact(k_target: int, p_list):
            N = len(p_list)
            if k_target < 0 or k_target > N:
                return 0.0
            if N > 200:
                mu = sum(p_list)
                var = sum([p * (1.0 - p) for p in p_list])
                sigma = math.sqrt(max(var, 1e-12))

                def normal_cdf(x):
                    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
                lo = (k_target - 0.5 - mu) / sigma
                hi = (k_target + 0.5 - mu) / sigma
                return max(0.0, min(1.0, normal_cdf(hi) - normal_cdf(lo)))
            dp = [0.0] * (N + 1)
            dp[0] = 1.0
            for p in p_list:
                for s in range(N, 0, -1):
                    dp[s] = dp[s] * (1.0 - p) + dp[s - 1] * p
                dp[0] = dp[0] * (1.0 - p)
            return dp[k_target]
        Pr_exact = compute_prob_exact(m - 1, p_vector)

        def compute_full_dist(p_list):
            N = len(p_list)
            dp = [0.0] * (N + 1)
            dp[0] = 1.0
            for p in p_list:
                for s in range(N, 0, -1):
                    dp[s] = dp[s] * (1.0 - p) + dp[s - 1] * p
                dp[0] = dp[0] * (1.0 - p)
            return dp
        full_dp = compute_full_dist(p_vector)
        Pr_at_least_m_minus1 = sum(full_dp[max(0, m - 1):])
        Pr_at_least_m = sum(full_dp[max(0, m):])
        marginal = k_reward * Pr_exact
        remaining_fraction = (r - t) / (r - 1)
        threshold = 1.0 - self.gamma * remaining_fraction
        if t >= r:
            if marginal >= 1.0 - 1e-12:
                return Action.C
            else:
                return Action.D
        elif marginal + 1e-12 >= threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_70(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Generosity and Limited Punishment (ACC-GP)

    - Seeds cooperation for s_init rounds.
    - Estimates each opponent's cooperation probability using a sliding window w.
    - Cooperates whenever cooperation is expected to secure the threshold m.
    - If exactly one short (expected cooperators == m-1), gives a probabilistic generous push with probability g
      (except in the last L_endgame rounds where generosity is disabled).
    - Tracks exploit_score for each opponent: increments when you cooperated and the group reached the threshold
      while that opponent defected; scores decay by factor delta each round. Once exploit_score > E_max an
      opponent is treated as low-trust (p_j = 0) until their score decays below threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.w = min(5, self.r)
        self.s_init = min(3, max(1, math.floor(self.r / 10)))
        self.L_endgame = min(2, max(1, math.floor(self.r / 10)))
        self.delta = 0.8
        self.E_max = 2
        g_raw = 0.5 + 0.3 * min(1.0, (self.k - 1.0) / 2.0) + 0.1 * min(1.0, self.r / 10.0)
        self.g = max(0.1, min(0.9, g_raw))
        self.exploit_scores = np.zeros(max(0, self.n - 1), dtype=float)
        self.last_processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            completed = 0
        else:
            completed = history.round_number
        if completed > self.last_processed_rounds and history is not None:
            for r_idx in range(self.last_processed_rounds, completed):
                if self.exploit_scores.size > 0:
                    self.exploit_scores *= self.delta
                my_coop = bool(history.my_actions[r_idx])
                opp_coops = int(np.sum(history.opponent_actions[r_idx, :])) if self.n - 1 > 0 else 0
                total_coops = opp_coops + (1 if my_coop else 0)
                if my_coop and total_coops >= self.m and (self.n - 1 > 0):
                    defections = np.logical_not(history.opponent_actions[r_idx, :])
                    incr = defections.astype(float)
                    self.exploit_scores += incr
            self.last_processed_rounds = completed
        t = completed + 1
        if t <= self.s_init:
            return Action.C
        n_opponents = max(0, self.n - 1)
        p = np.zeros(n_opponents, dtype=float)
        if completed == 0:
            if n_opponents > 0:
                p[:] = 0.5
        else:
            window = min(self.w, completed)
            if n_opponents > 0:
                recent = history.opponent_actions[-window:, :]
                coop_counts = np.sum(recent, axis=0).astype(float)
                p = coop_counts / float(window)
        if n_opponents > 0:
            low_trust_mask = self.exploit_scores > float(self.E_max)
            if np.any(low_trust_mask):
                p[low_trust_mask] = 0.0
        in_endgame = t > self.r - self.L_endgame
        expected_with_me = 1.0 + float(np.sum(p))
        eps = 1e-08
        if expected_with_me + eps >= float(self.m):
            return Action.C
        if in_endgame:
            return Action.C if expected_with_me + eps >= float(self.m) else Action.D
        if abs(expected_with_me - float(self.m - 1)) <= eps:
            if random.random() < self.g:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_71(BaseStrategy):
    """
    Adaptive Threshold Reciprocal (ATR)

    Tries to achieve and maintain rounds where the group reaches the cooperation
    threshold m. Signals willingness to cooperate early, cooperates when recent
    history shows stable success, defects (with occasional probes) when the
    group is failing or when exploited, punishes briefly until the group shows
    two consecutive successful rounds, and always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.in_punishment = False
        self.epsilon = 0.05
        self.cooperate_prob_high = 0.95
        self.cooperate_prob_medium = 0.7
        self.S_high = 0.7
        self.S_mid = 0.5
        self.S_low = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if history is None:
            self.in_punishment = False
            return Action.C
        num_completed = history.round_number
        if num_completed == 0:
            self.in_punishment = False
            return Action.C
        m = self.game_description.m
        my_all = history.my_actions
        opp_all = history.opponent_actions
        total_all = my_all.astype(int) + opp_all.sum(axis=1)
        success_all = total_all >= m
        consecutive_successes = 0
        for val in reversed(success_all):
            if bool(val):
                consecutive_successes += 1
            else:
                break
        w = min(10, num_completed)
        if w <= 0:
            S = 0.0
            E = 0
        else:
            my_recent = my_all[-w:]
            opp_recent = opp_all[-w:, :]
            total_recent = my_recent.astype(int) + opp_recent.sum(axis=1)
            success_recent = total_recent >= m
            S = float(success_recent.sum()) / float(w)
            E = int(np.logical_and(my_recent, ~success_recent).sum())
        if self.in_punishment and consecutive_successes >= 2:
            self.in_punishment = False
        if self.in_punishment:
            if random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        if E >= 1:
            self.in_punishment = True
            if random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        if S >= self.S_high and E == 0:
            if random.random() < self.cooperate_prob_high:
                return Action.C
            else:
                return Action.D
        if S >= self.S_mid and E <= 1:
            if random.random() < self.cooperate_prob_medium:
                return Action.C
            else:
                return Action.D
        if S <= self.S_low:
            if random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        if random.random() < 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_72(BaseStrategy):
    """
    Adaptive Conditional Threshold Cooperator (ACTC).

    Tries to be the marginal cooperator when that meaningfully increases success
    probability (uses Poisson-Binomial computed from EWMA beliefs about opponents),
    seeds cooperation early, punishes repeat free-riders briefly, and forgives.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.3
        self.eps = 0.05
        self.q_coop = 0.5
        self.T_punish = 2
        self.free_ride_threshold = 2
        self.W = 4
        self.first_bias = 0.8
        self.delta = 0.1
        pass

    def _poisson_binomial(self, p_list):
        """
        Compute exact Poisson-Binomial distribution for independent Bernoulli(p_list).
        Returns array probs where probs[t] = P(X = t) for t=0..len(p_list).
        """
        n = len(p_list)
        probs = np.zeros(n + 1, dtype=float)
        probs[0] = 1.0
        for p in p_list:
            new = np.zeros_like(probs)
            new[0] = probs[0] * (1.0 - p)
            for t in range(1, n + 1):
                new[t] = probs[t] * (1.0 - p) + probs[t - 1] * p
            probs = new
        return probs

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            prior = float(self.m) / float(self.n)
            p_j = np.full(self.n - 1, prior, dtype=float)
            probs = self._poisson_binomial(p_j.tolist())
            idx = self.m - 1
            P_exact = probs[idx] if 0 <= idx < len(probs) else 0.0
            if P_exact >= self.first_bias * (1.0 / self.k):
                return Action.C
            if P_exact >= 1.0 / self.k:
                return Action.C
            if np.mean(p_j) >= self.m / self.n:
                return Action.C if random.random() < self.q_coop else Action.D
            return Action.C if random.random() < self.eps else Action.D
        R = history.round_number
        rounds_completed = R
        opponents = self.n - 1
        p_j = np.full(opponents, float(self.m) / float(self.n), dtype=float)
        free_ride_rounds = [[] for _ in range(opponents)]
        punish_j = [0 for _ in range(opponents)]
        for t in range(rounds_completed):
            punish_active = [punish_j[j] > 0 for j in range(opponents)]
            row = history.opponent_actions[t, :]
            for j in range(opponents):
                if not punish_active[j]:
                    obs = 1.0 if bool(row[j]) else 0.0
                    p_j[j] = (1.0 - self.alpha) * p_j[j] + self.alpha * obs
            my_act = bool(history.my_actions[t])
            coop_count = int(my_act) + int(np.sum(row))
            success = coop_count >= self.m
            if success:
                for j in range(opponents):
                    if not bool(row[j]):
                        free_ride_rounds[j].append(t)
                        cutoff = t - (self.W - 1)
                        free_ride_rounds[j] = [rr for rr in free_ride_rounds[j] if rr >= cutoff]
                for j in range(opponents):
                    if len(free_ride_rounds[j]) >= self.free_ride_threshold:
                        punish_j[j] = max(punish_j[j], self.T_punish)
            for j in range(opponents):
                prev = punish_j[j]
                if prev > 0:
                    punish_j[j] = max(prev - 1, 0)
                    if prev > 0 and punish_j[j] == 0:
                        p_j[j] = 0.5 * p_j[j]
        rounds_left = max(self.r - rounds_completed, 0)
        p_j_effective = np.array([0.0 if punish_j[j] > 0 else p_j[j] for j in range(opponents)], dtype=float)
        probs = self._poisson_binomial(p_j_effective.tolist())
        idx = self.m - 1
        P_exact = probs[idx] if 0 <= idx < len(probs) else 0.0
        P_ge_m = float(np.sum(probs[self.m:])) if self.m < len(probs) else 0.0
        if rounds_left <= 1:
            return Action.C if P_exact >= 1.0 / self.k else Action.D
        if rounds_completed == 0:
            if P_exact >= self.first_bias * (1.0 / self.k):
                return Action.C
        if P_exact >= 1.0 / self.k:
            return Action.C
        mean_p_effective = float(np.mean(p_j_effective)) if opponents > 0 else 0.0
        if rounds_left == 2 and mean_p_effective >= self.m / self.n:
            if P_exact >= 1.0 / self.k * (1.0 - self.delta):
                return Action.C
        if mean_p_effective >= self.m / self.n and rounds_left >= 2:
            if random.random() < self.q_coop:
                return Action.C
            else:
                return Action.C if random.random() < self.eps else Action.D
        return Action.C if random.random() < self.eps else Action.D

class Strategy_COOPERATIVE_73(BaseStrategy):
    """
    Conditional Threshold Reciprocity (CTR)

    - Bootstraps cooperation for W rounds.
    - In normal play, estimates opponents' cooperation rates over a recent window L
      to predict whether the group (without you) will meet the threshold m.
    - Cooperates when pivotal (your contribution likely changes failure -> success)
      or when the group has reliably met the threshold recently (group-history override).
    - Briefly punishes persistent exploiters detected after a round where threshold was met
      but some players defected and historically rarely cooperated.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_counter = 0
        r = self.game_description.n_rounds
        self.W = min(3, max(0, r - 1))
        self.P = 2
        self.alpha = 0.4
        n = max(2, self.game_description.n_players)
        m = self.game_description.m
        self.beta = (m - 1) / (n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        t = int(state.round_number)
        if history is None:
            if t == r:
                return Action.D
            if t <= self.W:
                return Action.C
            return Action.C
        completed_rounds = history.round_number
        last_coop_count = 0
        if completed_rounds >= 1:
            last_my = bool(history.my_actions[-1])
            last_opp_sum = int(np.sum(history.opponent_actions[-1, :]))
            last_coop_count = last_opp_sum + (1 if last_my else 0)
        if completed_rounds >= 1:
            if last_coop_count >= m and last_coop_count < n:
                L_det = min(10, completed_rounds)
                if L_det > 0:
                    recent_opp = history.opponent_actions[-L_det:, :]
                    coop_rates_det = np.mean(recent_opp.astype(float), axis=0)
                else:
                    coop_rates_det = np.zeros(n - 1, dtype=float)
                defector_mask = np.logical_not(history.opponent_actions[-1, :])
                exploiter_mask = np.logical_and(defector_mask, coop_rates_det <= self.alpha)
                if np.any(exploiter_mask):
                    self.punish_counter = self.P
        if t == r:
            if self.punish_counter > 0:
                self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if t <= self.W:
            return Action.C
        L = min(10, max(0, t - 1))
        if L <= 0:
            return Action.C
        recent = history.opponent_actions[-L:, :]
        coop_rates = np.mean(recent.astype(float), axis=0)
        S_pred = float(np.sum(coop_rates))
        action = Action.D
        if S_pred >= m:
            action = Action.D
        elif S_pred <= m - 2:
            action = Action.D
        else:
            action = Action.C
        avg_rate = S_pred / float(max(1, n - 1))
        if action == Action.D:
            if completed_rounds >= 1 and last_coop_count >= m and (avg_rate >= self.beta):
                action = Action.C
        return action

class Strategy_COOPERATIVE_74(BaseStrategy):
    """
    Indexed Rotating Conditional Cooperation (IRCC)

    Attempts to coordinate exactly m cooperators per round using a public, index-based
    rotating schedule. Monitors recent history for schedule reliability; if unreliable,
    switches to a pivotal-helper rule (cooperate only when your cooperation is likely
    to reach the threshold). Uses short, forgiving group punishment when scheduled
    cooperators repeatedly fail. Always defects in the final round.

    Assumptions:
    - This implementation treats this agent as player index 0 (zero-based) and
      assumes opponent_actions columns correspond to player indices 1..n-1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.n = n
        self.r = r
        self.m = game_description.m
        self.k = game_description.k
        self.w = min(5, max(0, r - 1))
        self.phi = 0.25
        self.H = min(3, max(1, math.floor(r / 10)))
        self.punishment_timer = 0
        self.failure_window_base = 0
        self.self_index = 0

    def S_t_indices(self, t: int):
        """Return a set of zero-based player indices (size m) designated to cooperate in round t (1-based)."""
        n = self.n
        m = self.m
        blocks = math.ceil(n / m)
        block_index = (t - 1) % blocks
        start = block_index * m
        inds = [(start + offset) % n for offset in range(m)]
        return set(inds)

    def _get_past_window_range(self, completed_rounds: int):
        """
        Determine the window start (inclusive) and size for recent-history stats,
        respecting failure_window_base and available rounds.
        completed_rounds is number of rounds that have been played so far (history.round_number).
        """
        window_start = max(self.failure_window_base, max(0, completed_rounds - self.w))
        window_size = completed_rounds - window_start
        denom = window_size if window_size > 0 else 1
        return (window_start, window_size, denom)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == self.r:
            return Action.D
        if history is None:
            completed_rounds = 0
            S = self.S_t_indices(t)
            scheduled_action = self.self_index in S
            return Action.C if scheduled_action else Action.D
        completed_rounds = history.round_number
        window_start, window_size, denom = self._get_past_window_range(completed_rounds)
        opp_actions = history.opponent_actions
        n_minus_1 = self.n - 1
        coop_rates_opponents = np.zeros(n_minus_1, dtype=float)
        if window_size > 0 and opp_actions.shape[0] >= window_start + 1:
            if window_size > 0:
                slice_opps = opp_actions[window_start:completed_rounds, :]
                counts = np.sum(slice_opps.astype(float), axis=0)
                coop_rates_opponents = counts / denom
        my_rate = 0.0
        if window_size > 0 and history.my_actions.shape[0] >= window_start + 1:
            my_slice = history.my_actions[window_start:completed_rounds]
            my_count = int(np.sum(my_slice.astype(float))) if my_slice.size > 0 else 0
            my_rate = my_count / denom
        predicted_others = float(np.sum(coop_rates_opponents))
        failure_count = 0
        if window_size > 0:
            for past_round_idx in range(window_start, completed_rounds):
                round_t = past_round_idx + 1
                S_round = self.S_t_indices(round_t)
                coop_amt = 0
                for player_idx in S_round:
                    if player_idx == self.self_index:
                        if past_round_idx < history.my_actions.shape[0] and history.my_actions[past_round_idx]:
                            coop_amt += 1
                    else:
                        opp_col = player_idx - 1
                        if 0 <= opp_col < opp_actions.shape[1]:
                            if past_round_idx < opp_actions.shape[0] and opp_actions[past_round_idx, opp_col]:
                                coop_amt += 1
                if coop_amt < self.m:
                    failure_count += 1
            scheduled_failure_rate = failure_count / denom
        else:
            scheduled_failure_rate = 0.0
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self.failure_window_base = completed_rounds
            return Action.D
        if scheduled_failure_rate > self.phi and self.punishment_timer == 0:
            self.punishment_timer = self.H
            self.failure_window_base = completed_rounds
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self.failure_window_base = completed_rounds
            return Action.D
        S_current = self.S_t_indices(t)
        scheduled_action = self.self_index in S_current
        if scheduled_action and scheduled_failure_rate <= self.phi:
            return Action.C
        if predicted_others < self.m and predicted_others + 1 >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_75(BaseStrategy):
    """
    Pivotal Reciprocity with Forgiveness (PRF)

    - Cooperate when your single contribution is likely to be pivotal:
      cooperate iff k * Pr[exactly m-1 other cooperators] >= 1 (ties favor C).
    - Start optimistically with C in the first round.
    - Track opponents' empirical cooperation rates with a Beta smoothing prior (alpha=1).
    - Use a short bounded punishment after being exploited (cooperated but group failed)
      and forgive quickly when the group returns to consistent success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.F_threshold = 2
        self.P_max = 3
        self.W = 3
        self.S = 2.0 / 3.0
        self.fail_count = 0
        self.punish_until_round = 0
        self.last_round_processed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.fail_count = 0
            self.punish_until_round = 0
            self.last_round_processed = 0
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        rounds_completed = history.round_number
        for i in range(self.last_round_processed, rounds_completed):
            my_cooperated = bool(history.my_actions[i])
            opp_coops = int(np.sum(history.opponent_actions[i, :]))
            total_coops = opp_coops + (1 if my_cooperated else 0)
            success = 1 if total_coops >= m else 0
            if my_cooperated:
                if success == 0:
                    self.fail_count += 1
                else:
                    self.fail_count = max(0, self.fail_count - 1)
            if self.fail_count >= self.F_threshold:
                current_round_num = i + 1
                remaining = max(0, r - current_round_num)
                P = min(self.P_max, remaining)
                self.punish_until_round = current_round_num + P
                self.fail_count = 0
        if rounds_completed > 0:
            window_start = max(0, rounds_completed - self.W)
            window_end = rounds_completed
            if window_end > window_start:
                successes = 0
                window_len = window_end - window_start
                for i in range(window_start, window_end):
                    opp_coops = int(np.sum(history.opponent_actions[i, :]))
                    total_coops = opp_coops + (1 if bool(history.my_actions[i]) else 0)
                    if total_coops >= m:
                        successes += 1
                if successes / window_len >= self.S:
                    self.punish_until_round = 0
                    self.fail_count = 0
        self.last_round_processed = rounds_completed
        t = rounds_completed + 1
        n_others = n - 1
        p_list = []
        plays_j = rounds_completed
        for j in range(n_others):
            if plays_j <= 0:
                p_j = 0.5
            else:
                coop_j = int(np.sum(history.opponent_actions[:, j]))
                p_j = (self.alpha + coop_j) / (self.alpha + plays_j)
            p_list.append(float(p_j))
        prob = np.array([1.0], dtype=float)
        for p in p_list:
            prob = np.convolve(prob, np.array([1.0 - p, p], dtype=float))
        if prob.size != n_others + 1:
            prob = np.resize(prob, (n_others + 1,))
        idx = m - 1
        if 0 <= idx <= n_others:
            prob_exact_m_minus_1 = float(prob[idx])
        else:
            prob_exact_m_minus_1 = 0.0
        if self.punish_until_round >= t:
            return Action.D
        if t == 1:
            return Action.C
        pivotal_value = k * prob_exact_m_minus_1
        if pivotal_value >= 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_76(BaseStrategy):
    """
    Adaptive Pivotal Reciprocity (APR)

    Starts cooperative, attempts to reliably reach the collective threshold m when feasible,
    refuses repeated exploitation by conditioning on observed reliability of opponents,
    is willing to be pivotal when there is evidence others will cooperate, uses a short
    recent window for trend detection and forgiveness, and defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.p_tie = 0.2
        self.T_forgive = 2
        self.T_guard = min(3, max(1, math.floor(self.r / 6) + 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if history is None or history.round_number == 0:
            return Action.C
        if t == self.r:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed < 1:
            return Action.C
        W = min(5, rounds_completed)
        if self.m <= self.n / 2:
            theta = 0.5
        else:
            theta = 0.6
        opp_actions = history.opponent_actions
        opp_coop_counts = np.sum(opp_actions, axis=0).astype(float)
        f_j_all = opp_coop_counts / float(rounds_completed)
        opp_recent = opp_actions[-W:, :]
        opp_recent_counts = np.sum(opp_recent, axis=0).astype(float)
        f_j_recent = opp_recent_counts / float(W)
        punished_recent = f_j_recent < 0.2
        T_f = min(self.T_forgive, W)
        opp_last_T = opp_actions[-T_f:, :]
        opp_last_T_counts = np.sum(opp_last_T, axis=0).astype(int)
        forgiven = opp_last_T_counts >= T_f
        reliable_flags = (f_j_all >= theta) | forgiven
        reliable_flags = reliable_flags & ~(punished_recent & ~forgiven)
        Reliable_count = int(np.sum(reliable_flags))
        Expected_excl = float(np.sum(f_j_all))
        last_my = bool(history.my_actions[-1])
        last_opp_sum = int(np.sum(history.opponent_actions[-1, :]))
        coop_count_prev = int(last_my) + last_opp_sum
        i_cooperated_last = last_my
        if coop_count_prev >= self.m:
            if Reliable_count >= self.m - 1 or i_cooperated_last:
                return Action.C
            else:
                return Action.D
        if math.floor(Expected_excl + 1e-09) >= self.m:
            return Action.D
        if math.floor(Expected_excl + 1e-09) == self.m - 1:
            if Reliable_count >= self.m - 1:
                return Action.C
            elif random.random() < self.p_tie:
                return Action.C
            else:
                return Action.D
        if t <= self.r - self.T_guard:
            my_recent = history.my_actions[-W:].astype(int)
            opp_recent_sum_per_round = np.sum(opp_recent, axis=1).astype(int)
            coop_counts_last_W = my_recent + opp_recent_sum_per_round
            recent_avg = float(np.mean(coop_counts_last_W))
            if W >= 2:
                prev_mean = float(np.mean(coop_counts_last_W[:-1]))
            else:
                prev_mean = float(coop_counts_last_W[0])
            recent_trend_up = coop_counts_last_W[-1] >= prev_mean
            within_one_of_m = coop_count_prev >= max(0, self.m - 1)
            if recent_avg >= self.m - 1 and recent_trend_up and within_one_of_m:
                return Action.C
        return Action.D

class Strategy_COOPERATIVE_77(BaseStrategy):
    """
    Adaptive Threshold with Probing and Forgiveness (ATPF)

    - Uses a sliding window of recent rounds to estimate per-player cooperation rates.
    - Cooperates deterministically when pivotal (others likely give m-1 cooperators).
    - Free-rides when others likely reach threshold without you.
    - Defects by default when your single cooperation is unlikely to change outcome,
      but performs randomized probing early or when ambiguous to learn and signal.
    - Forgives occasional failures: requires up to S=2 consecutive failed rounds before
      tightening trust threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.S = 2
        self.theta_base = 0.6
        self.theta_forgiving = 0.5
        self.theta_strict = 0.8
        self.alpha_success = 0.6
        self.p_probe_base = None
        self.min_probe_probability = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        t = int(state.round_number)
        rem = max(1, r - t + 1)
        L = min(5, max(1, r - 1))
        probe_rounds = min(3, max(1, r - 1))
        p_probe = min(0.5, float(m) / float(n))
        p_first = min(0.6, float(m) / float(n))
        if history is None:
            if random.random() < p_first:
                return Action.C
            else:
                return Action.D
        completed = int(history.round_number)
        T = min(L, max(0, t - 1))
        if T <= 0:
            if random.random() < p_first:
                return Action.C
            else:
                return Action.D
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        recent_opp = opp_actions[-T:, :] if T > 0 else np.empty((0, opp_actions.shape[1]), dtype=bool)
        recent_me = my_actions[-T:] if T > 0 else np.empty((0,), dtype=bool)
        if recent_opp.size == 0:
            p_j = np.array([], dtype=float)
        else:
            counts = np.sum(recent_opp.astype(float), axis=0)
            p_j = counts / float(T)
        if T > 0:
            coop_counts_per_round = np.sum(recent_opp.astype(int), axis=1) + recent_me.astype(int)
            successes = np.sum(coop_counts_per_round >= m)
            recent_success_rate = float(successes) / float(T)
        else:
            recent_success_rate = 0.0
        if recent_success_rate >= self.alpha_success:
            theta = self.theta_forgiving
        else:
            theta = self.theta_base
        if rem == 1:
            theta = self.theta_strict
        consecutive_failed = 0
        rounds_to_check = min(L, T)
        for offset in range(1, rounds_to_check + 1):
            idx = -offset
            coop_count = int(np.sum(opp_actions[idx, :].astype(int))) + int(my_actions[idx])
            if coop_count < m:
                consecutive_failed += 1
            else:
                break
        if consecutive_failed >= self.S and recent_success_rate < self.alpha_success:
            theta = min(theta + 0.1, 0.95)
        if p_j.size == 0:
            count = 0
        else:
            count = int(np.sum(p_j >= theta))
        if count >= m:
            return Action.D
        elif count == m - 1:
            return Action.C
        else:
            near_tol = 0.1
            if p_j.size == 0:
                near_theta_count = 0
            else:
                near_theta_count = int(np.sum((p_j >= theta - near_tol) & (p_j <= theta + near_tol)))
            ambiguous = False
            if T < probe_rounds:
                ambiguous = True
            elif near_theta_count >= 2 and abs(count - (m - 1)) <= 1:
                ambiguous = True
            elif count == m - 2 and near_theta_count >= 1:
                ambiguous = True
            if ambiguous:
                probe_prob = max(self.min_probe_probability, p_probe * recent_success_rate)
                probe_prob = min(1.0, probe_prob)
                if random.random() < probe_prob:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_COOPERATIVE_78(BaseStrategy):
    """
    Conditional Threshold Partnering (CTP)

    - Maintains per-opponent cooperation frequencies and a small punishment timer.
    - Forms labels: Reliable, Likely, Unreliable (punished opponents forced Unreliable).
    - Cooperates when cooperating is likely to help reach the threshold (pivotal or part of a core).
    - Defects when reliable cooperators suffice (free-ride) or when cooperation is unlikely.
    - Probes early with decaying probability to discover cooperators.
    - Punishes Reliable players who defect in a critical round by temporary demotion.
    - In the final rounds is conservative: only cooperate when likely pivotal or safe.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.L_window = min(10, max(0, self.r - 1))
        self.reliable_thresh = 0.75
        self.likely_thresh = 0.5
        self.p0 = 0.2
        self.p_first = min(0.6, float(self.m) / max(1.0, float(self.n)))
        self.punish_P = 2
        self.punish_timers = [0] * max(0, self.n - 1)
        self._last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            if random.random() < self.p_first:
                return Action.C
            else:
                return Action.D
        completed = history.round_number
        if self._last_processed_round < completed:
            dec = completed - self._last_processed_round
            for i in range(len(self.punish_timers)):
                self.punish_timers[i] = max(0, self.punish_timers[i] - dec)
            self._last_processed_round = completed
        n_opponents = max(0, self.n - 1)

        def compute_freqs_and_last(op_actions, upto_rounds):
            if upto_rounds <= 0 or op_actions.size == 0:
                freqs = np.zeros(n_opponents, dtype=float)
                last = np.zeros(n_opponents, dtype=bool)
                return (freqs, last)
            window_len = min(self.L_window if self.L_window > 0 else upto_rounds, upto_rounds)
            window = op_actions[upto_rounds - window_len:upto_rounds, :]
            counts = np.sum(window, axis=0).astype(float)
            freqs = counts / float(window_len)
            last = op_actions[upto_rounds - 1, :].astype(bool)
            return (freqs, last)
        op_actions_all = history.opponent_actions
        freqs_current, last_current = compute_freqs_and_last(op_actions_all, completed)

        def labels_from(freqs, last_actions, punish_timers_snapshot, rounds_available):
            labels = ['U'] * n_opponents
            if rounds_available <= 0:
                return labels
            for j in range(n_opponents):
                if punish_timers_snapshot[j] > 0:
                    labels[j] = 'U'
                elif freqs[j] >= self.reliable_thresh and last_actions[j]:
                    labels[j] = 'R'
                elif freqs[j] >= self.likely_thresh:
                    labels[j] = 'L'
                else:
                    labels[j] = 'U'
            return labels
        labels_current = labels_from(freqs_current, last_current, self.punish_timers, completed)
        if completed >= 2:
            freqs_before_last, last_before_last = compute_freqs_and_last(op_actions_all, completed - 1)
            labels_before_last = labels_from(freqs_before_last, last_before_last, [0] * n_opponents, completed - 1)
            last_round_opponents = op_actions_all[completed - 1, :].astype(bool)
            my_last = bool(history.my_actions[completed - 1])
            coop_count_last = int(np.sum(last_round_opponents)) + (1 if my_last else 0)
            for j in range(n_opponents):
                if labels_before_last[j] == 'R':
                    defected_last = not bool(last_round_opponents[j])
                    if defected_last:
                        if coop_count_last < self.m and coop_count_last + 1 >= self.m:
                            self.punish_timers[j] = self.punish_P
            labels_current = labels_from(freqs_current, last_current, self.punish_timers, completed)
        R_indices = [i for i, lab in enumerate(labels_current) if lab == 'R']
        L_indices = [i for i, lab in enumerate(labels_current) if lab == 'L']
        R = len(R_indices)
        Lcount = len(L_indices)
        recent_rounds_to_check = min(3, completed)
        recent_coop_counts = []
        for lookback in range(1, recent_rounds_to_check + 1):
            idx = completed - lookback
            opps = op_actions_all[idx, :].astype(bool)
            me = bool(history.my_actions[idx])
            recent_coop_counts.append(int(np.sum(opps)) + (1 if me else 0))
        if t == self.r:
            last_round_strong = recent_coop_counts[-1] >= self.m - 1 if recent_coop_counts else False
            if R + Lcount >= max(0, self.m - 1) and last_round_strong:
                return Action.C
            else:
                return Action.D
        if t == max(1, self.r - 1):
            if R >= self.m:
                return Action.D
            if R == self.m - 1:
                if R == 0:
                    pass
                else:
                    all_cooperated = all((bool(last_current[idx]) for idx in R_indices))
                    if all_cooperated:
                        return Action.C
            if self.r - 2 > 0:
                p_probe = self.p0 * (1.0 - float(t - 1) / float(self.r - 2))
                p_probe = max(0.0, p_probe) / 4.0
            else:
                p_probe = 0.0
            if R + Lcount >= max(0, self.m - 1):
                stable = False
                if recent_coop_counts:
                    if recent_coop_counts[-1] >= max(0, self.m - 1):
                        stable = True
                    elif len(recent_coop_counts) >= 2:
                        high_count = sum((1 for x in recent_coop_counts if x >= max(0, self.m - 1)))
                        if high_count >= 2:
                            stable = True
                if stable:
                    return Action.C
            if random.random() < p_probe:
                return Action.C
            return Action.D
        if R >= self.m:
            return Action.D
        if R == self.m - 1 and self.m - 1 > 0:
            if R > 0:
                if completed >= 1:
                    all_cooperated = all((bool(last_current[idx]) for idx in R_indices))
                    if all_cooperated:
                        return Action.C
        if R + Lcount >= max(0, self.m - 1):
            condition = False
            if recent_coop_counts:
                if recent_coop_counts[-1] >= max(0, self.m - 1):
                    condition = True
                else:
                    high_count = sum((1 for x in recent_coop_counts if x >= max(0, self.m - 1)))
                    if high_count >= 2:
                        condition = True
            if condition:
                return Action.C
        p_probe = 0.0
        if self.r - 2 > 0 and t <= self.r - 2:
            p_probe = self.p0 * (1.0 - float(t - 1) / float(self.r - 2))
            p_probe = max(0.0, min(1.0, p_probe))
        if random.random() < p_probe:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_79(BaseStrategy):
    """
    Adaptive Pivotal Cooperator (APC)

    - Seeds cooperation in round 1.
    - Estimates others' recent cooperation (up to last 5 rounds).
    - Free-rides when others are expected to meet threshold or when contribution
      would be wasted.
    - Cooperates when potentially pivotal and group reciprocity / low exploitation
      make cooperation attractive.
    - Uses short bounded punishment after being exploited; punishment_counter forces
      defections for a few rounds and is then decremented automatically.
    - Stateful: tracks punishment_counter, exploited_count, times_cooperated and
      the last processed round from history to update these based on observed outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_counter = 0
        self.exploited_count = 0
        self.times_cooperated = 0
        self.last_processed_round = 0
        self.s_thr = 0.6
        self.s_last = 0.7
        self.max_punish = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        t = getattr(state, 'round_number', None)
        if t is None:
            t = 1 if history is None else history.round_number + 1
        if history is not None:
            completed = history.round_number
            for idx in range(self.last_processed_round, completed):
                my_action = bool(history.my_actions[idx])
                others_coops = int(np.sum(history.opponent_actions[idx, :]))
                total_coops = others_coops + (1 if my_action else 0)
                my_pay = float(history.my_payoffs[idx])
                if my_action and total_coops < m:
                    self.exploited_count += 1
                    opp_payoffs = history.opponent_payoffs[idx, :]
                    if np.any(opp_payoffs > my_pay):
                        t_round = idx + 1
                        remaining_future = max(0, r - t_round)
                        punish_len = min(self.max_punish, remaining_future)
                        self.punishment_counter = max(self.punishment_counter, punish_len)
            self.last_processed_round = completed
        if history is None or (history is not None and history.round_number == 0):
            self.times_cooperated += 1
            return Action.C
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        past_rounds = history.round_number
        H = min(5, max(0, t - 1))
        if H <= 0:
            E = float(m - 1)
        else:
            start_idx = past_rounds - H
            recent_others = history.opponent_actions[start_idx:past_rounds, :]
            others_coops_per_round = np.sum(recent_others, axis=1).astype(float)
            E = float(np.mean(others_coops_per_round)) if others_coops_per_round.size > 0 else float(m - 1)
        if past_rounds <= 0:
            success_rate = 0.0
        else:
            opp_sums = np.sum(history.opponent_actions[:past_rounds, :], axis=1).astype(float)
            my_actions_float = history.my_actions[:past_rounds].astype(float)
            total_coops_per_round = opp_sums + my_actions_float
            successes = np.sum(total_coops_per_round >= m)
            success_rate = float(successes) / float(past_rounds)
        if self.times_cooperated <= 0:
            exploited_ratio = 0.0
        else:
            exploited_ratio = float(self.exploited_count) / float(self.times_cooperated)
        action = Action.D
        if E >= float(m):
            action = Action.D
        elif E <= float(m - 2):
            action = Action.D
        elif t == r:
            if success_rate >= self.s_last and exploited_ratio <= 0.33:
                action = Action.C
            else:
                action = Action.D
        elif success_rate >= self.s_thr or exploited_ratio <= 0.25:
            action = Action.C
        else:
            action = Action.D
        if action == Action.C:
            self.times_cooperated += 1
        return action

class Strategy_COOPERATIVE_80(BaseStrategy):
    """
    Adaptive Pivotal Cooperator (APC)

    - Cooperates on the first move to signal intent.
    - Uses a sliding window of recent rounds per-opponent cooperation frequencies to estimate
      the expected number of cooperators next round (E).
    - Cooperates when likely pivotal (E near m-1).
    - Mostly free-rides when others will reach the threshold without you (with small epsilon
      exploration to keep cooperation alive).
    - Defects when threshold is very unlikely even with your cooperation.
    - Enters a short punishment phase (P rounds) if others persistently under-cooperate
      or if your cooperation was recently betrayed (you cooperated but threshold failed).
    - Always defects on the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_remain = 0
        self.epsilon = 0.05
        self.delta = 0.5
        self.tol = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if t == r:
            return Action.D
        if history is None:
            return Action.C
        if self.punish_remain > 0:
            self.punish_remain -= 1
            return Action.D
        max_w = min(10, t - 1)
        W = max_w
        if W <= 0:
            return Action.C
        recent_opponent_actions = history.opponent_actions[-W:, :]
        if recent_opponent_actions.size == 0:
            p_j = np.zeros(0)
        else:
            p_j = np.sum(recent_opponent_actions.astype(float), axis=0) / float(W)
        E = float(np.sum(p_j))
        avg_others = E / float(max(1, n - 1))
        target_rate = float(m) / float(n)
        P = min(3, max(1, math.floor(r / 10)))
        betrayed = False
        my_recent = history.my_actions[-W:]
        rounds_count = len(my_recent)
        start_idx = history.round_number - W
        for offset in range(rounds_count):
            idx = start_idx + offset
            if history.my_actions[idx]:
                opp_coops = int(np.sum(history.opponent_actions[idx, :].astype(int)))
                total_coops = opp_coops + 1
                if total_coops < m:
                    betrayed = True
                    break
        if betrayed:
            self.punish_remain = max(0, P - 1)
            return Action.D
        if avg_others < target_rate - self.tol:
            self.punish_remain = max(0, P - 1)
            return Action.D
        if E <= m - 2 - self.delta:
            return Action.D
        if E >= m + self.delta:
            if random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        lower = m - 1 - self.delta
        upper = m - 1 + self.delta
        if E >= lower and E <= upper:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_81(BaseStrategy):
    """
    Reciprocal Threshold with Forgiveness (RTF)

    - Cooperate in the first round (unless it's also the final round).
    - Always defect in the final round.
    - If currently punishing (punish_timer > 0) defect and decrement the timer.
    - Otherwise:
      - If last round's cooperators >= m, cooperate.
      - Else cooperate only if you have ever been part of a successful (s >= m) round where you played C.
      - Otherwise defect.
    - If you cooperated in a round that failed (s < m), start a punishment period of length P.
    - If you cooperated in a round that succeeded (s >= m), remember that fact (last_successful_cooperator).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.P = 2
        self.punish_timer = 0
        self.last_successful_cooperator = False
        self.my_last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        if history is not None and history.round_number > 0:
            last_my_action = bool(history.my_actions[-1])
            opp_coops_last = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
            s_last = int(last_my_action) + opp_coops_last
            if last_my_action and s_last >= m:
                self.last_successful_cooperator = True
            if last_my_action and s_last < m:
                self.punish_timer = self.P
        t = state.round_number
        if t == n_rounds:
            self.my_last_action = False
            return Action.D
        if self.punish_timer > 0:
            self.my_last_action = False
            self.punish_timer -= 1
            return Action.D
        if history is None or history.round_number == 0:
            self.my_last_action = True
            return Action.C
        prev_my_action = bool(history.my_actions[-1])
        prev_opp_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
        s_prev = int(prev_my_action) + prev_opp_coops
        if s_prev >= m:
            self.my_last_action = True
            return Action.C
        if self.last_successful_cooperator:
            self.my_last_action = True
            return Action.C
        self.my_last_action = False
        return Action.D

class Strategy_COOPERATIVE_82(BaseStrategy):
    """
    Conditional Pivotal Cooperation (CPC)

    - Signal cooperation in round 1 to allow coordination.
    - Defect on the last round.
    - Sustain cooperation after successful rounds.
    - If the previous round was a near-miss (m-1), cooperate only if you personally cooperated in that round.
    - If there was a stable success streak (>=2) and the previous round was a one-off near-failure (>= m-2),
      allow forgiveness by cooperating to help recovery.
    - Otherwise, defect to avoid wasting contributions when success is unlikely.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        if t == 1:
            return Action.C
        prev_index = t - 2
        if prev_index < 0:
            return Action.C
        try:
            opp_coops_prev = int(np.sum(history.opponent_actions[prev_index, :]))
        except Exception:
            opp_coops_prev = 0
        my_coop_prev = bool(history.my_actions[prev_index]) if prev_index < len(history.my_actions) else False
        prev_count = opp_coops_prev + (1 if my_coop_prev else 0)
        success_prev = prev_count >= self.m
        success_streak = 0
        for idx in range(prev_index, -1, -1):
            try:
                opp_coops = int(np.sum(history.opponent_actions[idx, :]))
            except Exception:
                opp_coops = 0
            my_coop = bool(history.my_actions[idx]) if idx < len(history.my_actions) else False
            coop_count = opp_coops + (1 if my_coop else 0)
            if coop_count >= self.m:
                success_streak += 1
            else:
                break
        if success_prev:
            return Action.C
        if prev_count == self.m - 1:
            if my_coop_prev:
                return Action.C
            else:
                return Action.D
        if success_streak >= 2 and prev_count >= self.m - 2:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_83(BaseStrategy):
    """
    Minimal-matching Collective Risk strategy.

    - Select a target set of m players based on observed cooperation reliability (R_j = coop_count / rounds).
    - Cooperate if you're in the target set (unless recent target-set failures trigger a temporary punish abstain).
    - If not in the target set, step in only when you are predicted to be pivotal (others give m-1) and your
      help budget allows it and you are not being repeatedly exploited.
    - In the final round act myopically: cooperate only if your cooperation is pivotal.
    - Maintains a small help budget (my_help_used) that replenishes slowly when target-set cooperation is reliable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.W = max(1, round(self.r / 10))
        self.Hmax = max(1, round(self.r / 10))
        self.E = 0.5
        self.my_help_used = 0
        self._last_processed_rounds = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            p0 = float(self.m) / float(self.n)
            return Action.C if random.random() < p0 else Action.D
        rounds_completed = int(history.round_number)
        t_next = rounds_completed + 1
        my_coops = int(np.sum(history.my_actions)) if rounds_completed > 0 else 0
        if rounds_completed > 0:
            opp_coops = np.sum(history.opponent_actions, axis=0).astype(int)
        else:
            opp_coops = np.zeros(self.n - 1, dtype=int)
        coop_counts = np.concatenate(([my_coops], opp_coops))
        if rounds_completed >= 1:
            last_self = bool(history.my_actions[-1])
            last_opps = history.opponent_actions[-1, :].astype(bool)
            last_action_of = np.concatenate(([last_self], last_opps))
        else:
            last_action_of = np.zeros(self.n, dtype=bool)
        denom = max(1, rounds_completed)
        R = coop_counts.astype(float) / float(denom)
        window_start = max(0, rounds_completed - self.W)
        if rounds_completed > 0:
            player_actions = np.zeros((rounds_completed, self.n), dtype=bool)
            player_actions[:, 0] = history.my_actions
            if self.n > 1:
                player_actions[:, 1:] = history.opponent_actions
            window_actions = player_actions[window_start:rounds_completed, :]
            defects_in_window = np.sum(~window_actions, axis=0).astype(float)
        else:
            defects_in_window = np.zeros(self.n, dtype=float)
        penalty = 0.5 * defects_in_window / max(1, self.W)
        penalized_R = R - penalty
        indices = np.arange(self.n)
        order = np.lexsort((indices, -penalized_R))
        target_set = list(order[:self.m])
        predicted_coops_in_target = 0
        for idx in target_set:
            if idx == 0:
                continue
            if last_action_of[idx]:
                predicted_coops_in_target += 1
        if rounds_completed > 0:
            total_coops_per_round = np.sum(player_actions, axis=1)
            window_totals = total_coops_per_round[window_start:rounds_completed]
            successes_window = window_totals >= self.m
            failure_count_W = int(np.sum(~successes_window))
            fraction_failures = failure_count_W / max(1, self.W)
            failure_due_to_target = 0
            for row_idx in range(window_actions.shape[0]):
                row = window_actions[row_idx, :]
                total = int(np.sum(row))
                if total < self.m:
                    targeted_coops = int(np.sum(row[target_set]))
                    if targeted_coops < self.m:
                        failure_due_to_target += 1
            frac_failures_due_to_target = failure_due_to_target / max(1, failure_count_W) if failure_count_W > 0 else 0.0
            pivotal_count = 0
            for r_idx in range(window_actions.shape[0]):
                row = window_actions[r_idx, :]
                if row[0]:
                    total = int(np.sum(row))
                    if total >= self.m and total - 1 < self.m:
                        pivotal_count += 1
            recent_pivotal_fraction = pivotal_count / max(1, self.W)
        else:
            fraction_failures = 0.0
            frac_failures_due_to_target = 0.0
            recent_pivotal_fraction = 0.0
        if t_next == self.r:
            if predicted_coops_in_target >= self.m:
                decision = Action.D
            elif predicted_coops_in_target == self.m - 1:
                decision = Action.C
            else:
                decision = Action.D
        else:
            me_in_target = 0 in target_set
            punish_target = False
            if me_in_target:
                if fraction_failures > 0.5 and frac_failures_due_to_target > 0.5:
                    punish_target = True
            if me_in_target:
                if punish_target:
                    decision = Action.D
                else:
                    decision = Action.C
            elif predicted_coops_in_target >= self.m:
                decision = Action.D
            elif predicted_coops_in_target == self.m - 1 and self.my_help_used < self.Hmax and (recent_pivotal_fraction <= self.E):
                decision = Action.C
            else:
                decision = Action.D
        if rounds_completed >= 1:
            last_round_idx = rounds_completed - 1
            last_my_action = bool(history.my_actions[last_round_idx])
            if rounds_completed - 1 > 0:
                my_coops_prev = int(np.sum(history.my_actions[:last_round_idx]))
                opp_coops_prev = np.sum(history.opponent_actions[:last_round_idx, :], axis=0).astype(int)
            else:
                my_coops_prev = 0
                opp_coops_prev = np.zeros(self.n - 1, dtype=int)
            coop_counts_prev = np.concatenate(([my_coops_prev], opp_coops_prev))
            denom_prev = max(1, rounds_completed - 1)
            R_prev = coop_counts_prev.astype(float) / float(denom_prev)
            ws_start_prev = max(0, rounds_completed - 1 - self.W)
            if rounds_completed - 1 > 0:
                player_actions_prev = np.zeros((rounds_completed - 1, self.n), dtype=bool)
                if rounds_completed - 1 > 0:
                    player_actions_prev[:, 0] = history.my_actions[:last_round_idx]
                    if self.n > 1:
                        player_actions_prev[:, 1:] = history.opponent_actions[:last_round_idx, :]
                window_actions_prev = player_actions_prev[ws_start_prev:rounds_completed - 1, :] if rounds_completed - 1 > 0 else np.zeros((0, self.n), dtype=bool)
                defects_prev = np.sum(~window_actions_prev, axis=0) if window_actions_prev.shape[0] > 0 else np.zeros(self.n, dtype=float)
            else:
                defects_prev = np.zeros(self.n, dtype=float)
            penalty_prev = 0.5 * defects_prev / max(1, self.W)
            penalized_R_prev = R_prev - penalty_prev
            indices = np.arange(self.n)
            order_prev = np.lexsort((indices, -penalized_R_prev))
            target_set_prev = list(order_prev[:self.m])
            last_row = np.zeros(self.n, dtype=bool)
            last_row[0] = history.my_actions[last_round_idx]
            if self.n > 1:
                last_row[1:] = history.opponent_actions[last_round_idx, :]
            total_last = int(np.sum(last_row))
            without_me = total_last - (1 if last_row[0] else 0)
            saved_by_me = last_row[0] and total_last >= self.m and (without_me < self.m)
            if saved_by_me and 0 not in target_set_prev:
                self.my_help_used += 1
            consecutive_successes = 0
            if rounds_completed > 0:
                for i in range(rounds_completed - 1, -1, -1):
                    if rounds_completed > 0:
                        if np.sum(player_actions[i, :]) >= self.m:
                            consecutive_successes += 1
                        else:
                            break
                    else:
                        break
            if consecutive_successes >= self.W:
                blocks = consecutive_successes // self.W
                if blocks > 0:
                    self.my_help_used = max(0, self.my_help_used - int(blocks))
        self.my_help_used = int(max(0, min(self.my_help_used, max(self.Hmax, self.my_help_used))))
        return decision

class Strategy_COOPERATIVE_84(BaseStrategy):
    """
    Conditional Cooperator for the Collective Risk Dilemma.

    - Cooperates on the first move, defects on the last move.
    - Estimates opponents' cooperation probabilities from a recent window.
    - Cooperates when (i) your cooperation meaningfully increases success probability (pivotal),
      or (ii) the group has been reliably cooperative on average.
    - Defects when the group will very likely succeed without you.
    - Uses small random exploration (epsilon) to break deadlocks.
    - Punishes players who defect when their cooperation would have been pivotal,
      for a short forgiveness window, and forgives after they show consecutive cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punished_until = [0] * max(0, game_description.n_players - 1)
        self.p0 = 0.7
        self.epsilon = 0.05
        self.pivot_delta = 0.25
        self.success_high = 0.8
        self.forgiveness_rounds = 2
        self.p_min = 0.02
        self.p_max = 0.98

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is None:
            return Action.C
        if t == r:
            return Action.D
        num_other = n - 1
        if len(self.punished_until) != num_other:
            self.punished_until = [0] * num_other
        completed_rounds = history.round_number
        if completed_rounds >= 1 and self.forgiveness_rounds > 0:
            check_len = min(self.forgiveness_rounds, completed_rounds)
            if check_len > 0:
                recent_slice = history.opponent_actions[-check_len:, :]
                for j in range(num_other):
                    if self.punished_until[j] > 0:
                        if np.all(recent_slice[:, j]):
                            self.punished_until[j] = 0
        if completed_rounds >= 1:
            last_idx = completed_rounds - 1
            last_my = bool(history.my_actions[last_idx])
            last_opps = history.opponent_actions[last_idx, :]
            coop_count = int(last_my) + int(np.sum(last_opps))
            for j in range(num_other):
                defected = not bool(last_opps[j])
                if defected:
                    if coop_count < m and coop_count + 1 >= m:
                        punish_until = completed_rounds + self.forgiveness_rounds
                        if punish_until > self.punished_until[j]:
                            self.punished_until[j] = punish_until
        w = min(10, max(0, completed_rounds))
        p_list = []
        for j in range(num_other):
            if completed_rounds == 0:
                p_j = self.p0
            else:
                if w <= 0:
                    recent = history.opponent_actions[:, j]
                else:
                    recent = history.opponent_actions[-w:, j]
                if recent.size == 0:
                    p_j = self.p0
                else:
                    p_j = float(np.mean(recent))
                    if not np.isfinite(p_j):
                        p_j = self.p0
            p_j = max(self.p_min, min(self.p_max, p_j))
            if t <= self.punished_until[j]:
                p_j *= 0.5
                p_j = max(self.p_min, min(self.p_max, p_j))
            p_list.append(p_j)
        probs = np.zeros(num_other + 1, dtype=float)
        probs[0] = 1.0
        for p in p_list:
            probs = np.convolve(probs, np.array([1.0 - p, p]))[:num_other + 1]
        if m <= 0:
            P_without = 1.0
        elif m > num_other:
            P_without = 0.0
        else:
            P_without = float(np.sum(probs[m:]))
        if m - 1 <= 0:
            P_with = 1.0
        elif m - 1 > num_other:
            P_with = 0.0
        else:
            idx = m - 1
            P_with = float(np.sum(probs[idx:]))
        if P_with - P_without >= self.pivot_delta:
            return Action.C
        if P_without >= self.success_high:
            if random.random() < self.epsilon:
                return Action.C
            return Action.D
        mean_p = float(np.mean(p_list)) if num_other > 0 else 0.0
        if mean_p >= 0.6:
            return Action.C
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_85(BaseStrategy):
    """
    Adaptive Threshold Contributor (ATC)

    Estimates opponents' cooperation probabilities via an EMA, tracks recent
    group success, and maintains a help_boost that rises after failures and
    decays after successes. In each round the strategy computes the expected
    number of cooperating opponents and decides probabilistically whether to
    cooperate based on pivotality, recent failures (help_boost), and endgame
    considerations. A small randomness floor and occasional fairness-driven
    cooperation prevent purely deterministic exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.3
        self.m0 = 0.5
        self.beta = 0.6
        self.p0 = 0.6
        self.eps = 0.02
        self.hmax = 0.9
        self.margin_small = 0.2
        self.fairness_L = 5
        self.n_opponents = max(0, self.n - 1)
        self.p_opponents = np.full((self.n_opponents,), 0.5, dtype=float)
        self.S = 0.5
        self.H_b = 0.0
        self.last_round_index = 0

        def clamp(x, lo, hi):
            if x is None:
                return lo
            if x < lo:
                return lo
            if x > hi:
                return hi
            return x
        self._clamp = clamp

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            u = random.random()
            return Action.C if u <= self.p0 else Action.D
        completed_rounds = history.round_number
        for idx in range(self.last_round_index, completed_rounds):
            if idx < history.opponent_actions.shape[0]:
                opp_row = history.opponent_actions[idx, :]
                upto = min(len(self.p_opponents), opp_row.shape[0])
                if upto > 0:
                    played_c = np.asarray(opp_row[:upto], dtype=float)
                    self.p_opponents[:upto] = (1.0 - self.alpha) * self.p_opponents[:upto] + self.alpha * played_c
            else:
                pass
            coop_count = 0
            if idx < history.opponent_actions.shape[0]:
                coop_count += int(np.sum(history.opponent_actions[idx, :]))
            if idx < history.my_actions.shape[0]:
                coop_count += int(bool(history.my_actions[idx]))
            round_success = 1 if coop_count >= self.m else 0
            self.S = (1.0 - self.alpha) * self.S + self.alpha * float(round_success)
            if round_success == 0:
                if self.beta > 0:
                    new_H = self.H_b / self.beta + (1.0 - self.beta)
                else:
                    new_H = self.hmax
                self.H_b = min(self.hmax, new_H)
            else:
                self.H_b = self.H_b * self.beta
        self.last_round_index = completed_rounds
        t = completed_rounds + 1
        rounds_left = max(0, self.r - completed_rounds)
        E = float(np.sum(self.p_opponents))
        cooperate_prob = self.eps
        if self.n_opponents == 0:
            if self.m <= 1:
                cooperate_prob = 0.5
            else:
                cooperate_prob = self.eps
            u = random.random()
            return Action.C if u <= cooperate_prob else Action.D
        if t == self.r:
            if E >= self.m:
                cooperate_prob = self.eps
            elif E >= self.m - 1 - self.margin_small:
                raw = 0.95 * (1.0 - (self.m - 1.0 - E))
                cooperate_prob = self._clamp(raw, 0.5, 0.99)
            else:
                cooperate_prob = self.eps
        else:
            if E >= self.m + self.m0:
                cooperate_prob = max(self.eps, 0.0)
            elif E <= self.m - 1 - self.m0:
                raw = 0.75 + 0.25 * float(self.H_b)
                cooperate_prob = self._clamp(raw, 0.6, 0.99)
            else:
                raw_needed = self.m - E
                raw_needed = self._clamp(raw_needed, 0.0, 2.0)
                base_prob = self._clamp(raw_needed, 0.0, 1.0)
                coop = 0.6 * base_prob + 0.4 * float(self.H_b) + self.eps
                cooperate_prob = self._clamp(coop, self.eps, 0.99)
            my_recent_rate = 1.0
            if completed_rounds > 0:
                L = max(1, min(self.fairness_L, completed_rounds))
                recent = history.my_actions[-L:]
                my_recent_rate = float(np.mean(recent))
            else:
                my_recent_rate = 0.0
            if my_recent_rate < 0.2 and self.S > 0.9:
                cooperate_prob = max(cooperate_prob, 0.25)
        cooperate_prob = self._clamp(cooperate_prob, self.eps, 0.99)
        u = random.random()
        return Action.C if u <= cooperate_prob else Action.D

class Strategy_COOPERATIVE_86(BaseStrategy):
    """
    Pivotal-Belief with Forgiveness strategy for the Collective Risk Dilemma.

    - First round: cooperate (signal).
    - From round 2 onward: estimate per-opponent cooperation probabilities with a Beta(1,1)
      prior, form a binomial approximation for the number X of other cooperators, and compute
      P(X = m-1). Apply a small continuity bonus proportional to fraction of past rounds that
      met the threshold. Cooperate iff effective_prob > 1/k.
    - If recently exploited (you cooperated in the last S rounds but none of those S rounds
      met the threshold), enter a mild punishment mode (defect) until observing two rounds
      (not necessarily consecutive) with cooperators >= m; then resume normal rule.
    - Parameters: beta=0.25 (continuity bonus), S=3 (exploitation window). Tunable as needed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.punishing = False
        self.punish_start_success_count = None
        self.beta = 0.25
        self.S = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        rounds_played = int(history.round_number)
        if rounds_played == 0:
            return Action.C
        opp_actions = history.opponent_actions
        times_coop_opponents = np.sum(opp_actions.astype(np.int64), axis=0)
        denom = float(2 + rounds_played)
        if times_coop_opponents.size > 0:
            p_j = (1.0 + times_coop_opponents.astype(np.float64)) / denom
            p_bar = float(np.mean(p_j))
        else:
            p_bar = 0.5
        eps = 1e-12
        p_bar = max(eps, min(1.0 - eps, p_bar))
        my_actions = history.my_actions.astype(np.int64)
        opp_counts_per_round = np.sum(opp_actions.astype(np.int64), axis=1)
        total_cooperators_per_round = opp_counts_per_round + my_actions
        rounds_with_success = int(np.sum(total_cooperators_per_round >= self.m))
        n_minus_1 = self.n - 1
        k_needed = self.m - 1
        if k_needed < 0 or k_needed > n_minus_1:
            P_eq = 0.0
        else:
            comb_term = math.comb(n_minus_1, k_needed)
            fail_exp = n_minus_1 - k_needed
            P_eq = comb_term * p_bar ** k_needed * (1.0 - p_bar) ** fail_exp
        frac_success = float(rounds_with_success) / float(max(1, rounds_played))
        effective_prob = min(1.0, P_eq + self.beta * frac_success)
        S = min(self.S, rounds_played)
        recent_my = history.my_actions[-S:].astype(np.bool_)
        you_cooperated_recently = bool(np.any(recent_my))
        recent_total_cooperators = total_cooperators_per_round[-S:]
        none_of_recent_met = not bool(np.any(recent_total_cooperators >= self.m))
        exploited_recently = you_cooperated_recently and none_of_recent_met
        if self.punishing:
            start_count = int(self.punish_start_success_count or 0)
            successes_since = rounds_with_success - start_count
            if successes_since >= 2:
                self.punishing = False
                self.punish_start_success_count = None
        elif exploited_recently:
            self.punishing = True
            self.punish_start_success_count = rounds_with_success
        if self.punishing:
            return Action.D
        threshold = 1.0 / float(self.k) if self.k > 0 else float('inf')
        if effective_prob > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_87(BaseStrategy):
    """
    Adaptive Pivotal Cooperation (APC)

    - Uses EWMA estimates of each opponent's cooperation probability.
    - Computes the probability that my single cooperation is pivotal (exactly m-1 other cooperators).
    - Plays C when k * Pr(X = m-1) is large enough to justify giving up the private 1.
    - Adds reciprocity (support stable cooperation), limited punishment for sustained failures,
      forgiveness based on a sliding window, and small exploration to recover cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.3
        self.epsilon = 0.03
        self.q = 0.5
        self.W = min(10, max(1, self.r))
        self.delta = 0.05
        self.n_opponents = max(0, self.n - 1)
        if self.n_opponents > 0:
            self.p_js = np.full(self.n_opponents, 0.5, dtype=float)
        else:
            self.p_js = np.array([], dtype=float)
        self.punish = False

    def _compute_bernoulli_sum_distribution(self, ps):
        """
        Given a sequence of independent Bernoulli probabilities ps (length L),
        return an array probs of length L+1 where probs[x] = Pr(sum = x).
        """
        probs = np.array([1.0], dtype=float)
        for p in ps:
            new = np.zeros(len(probs) + 1, dtype=float)
            new[:-1] += probs * (1.0 - p)
            new[1:] += probs * p
            probs = new
        return probs

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        rounds_completed = history.round_number
        current_round = state.round_number
        if rounds_completed >= 1 and self.n_opponents > 0:
            last_op_actions = history.opponent_actions[-1, :]
            last_op_float = np.asarray(last_op_actions, dtype=float)
            self.p_js = (1.0 - self.alpha) * self.p_js + self.alpha * last_op_float
            lower = self.epsilon / 10.0
            upper = 1.0 - self.epsilon / 10.0
            self.p_js = np.clip(self.p_js, lower, upper)
        window = min(self.W, rounds_completed)
        if window > 0:
            recent_op = history.opponent_actions[-window:, :] if self.n_opponents > 0 else np.zeros((window, 0), dtype=bool)
            recent_my = history.my_actions[-window:]
            opponent_counts = np.sum(recent_op, axis=1) if recent_op.size > 0 else np.zeros(window, dtype=int)
            my_counts = np.asarray(recent_my, dtype=int)
            total_counts = opponent_counts + my_counts
            successes = total_counts >= self.m
            frac_success = float(np.mean(successes)) if window > 0 else 0.0
            my_coop_in_window = float(np.mean(my_counts))
        else:
            frac_success = 0.0
            my_coop_in_window = 0.0
        if window > 0 and frac_success < self.q:
            self.punish = True
        probs = self._compute_bernoulli_sum_distribution(self.p_js)
        target = self.m - 1
        if 0 <= target <= self.n_opponents:
            prob_exact = float(probs[target])
        else:
            prob_exact = 0.0
        if self.m <= self.n_opponents:
            prob_atleast_m = float(np.sum(probs[self.m:]))
        else:
            prob_atleast_m = 0.0
        pivotal_value = self.k * prob_exact
        action_is_cooperate = False
        if current_round == 1:
            action_is_cooperate = True
        elif current_round >= self.r:
            if pivotal_value > 1.0 + self.delta:
                action_is_cooperate = True
            elif pivotal_value < 1.0 - self.delta:
                action_is_cooperate = False
            else:
                action_is_cooperate = True
        else:
            if self.punish:
                if frac_success >= self.q:
                    self.punish = False
                else:
                    action_is_cooperate = False
            if not self.punish:
                if pivotal_value >= 1.0 - self.delta:
                    action_is_cooperate = True
                elif window > 0 and frac_success >= 0.8 and (my_coop_in_window >= 0.5):
                    action_is_cooperate = True
                else:
                    action_is_cooperate = False
        if random.random() < self.epsilon:
            action_is_cooperate = not action_is_cooperate
        if window > 0:
            if frac_success < self.q:
                self.punish = True
            else:
                pass
        return Action.C if action_is_cooperate else Action.D

class Strategy_COOPERATIVE_88(BaseStrategy):
    """
    Collective-risk adaptive strategy.

    - Tries to secure the threshold m each round when feasible (pivotal cooperation or joining a stable set).
    - Punishes briefly if cooperated but the threshold failed (limited punishment_counter).
    - Uses short-run per-opponent cooperation rates (window w) to detect likely cooperators.
    - Probes occasionally with small probabilities to allow new coalitions to form and to signal fairness.
    - Decisions use only game params and observable history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.w = min(10, max(1, self.r))
        self.p_thresh = 0.6
        self.epsilon_explore_base = 0.05
        self.epsilon_coop_when_safe_base = 0.03
        self.punish_cap = 3
        self.punishment_counter = 0
        self._last_punish_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            completed_rounds = 0
        else:
            completed_rounds = history.round_number
        t = completed_rounds + 1
        decay_factor = 0.5
        frac = (t - 1) / max(1, self.r)
        decay = max(0.0, 1.0 - frac * decay_factor)
        epsilon_explore = self.epsilon_explore_base * decay
        epsilon_coop_when_safe = self.epsilon_coop_when_safe_base * decay
        last_total_others = None
        my_last = None
        total_cooperators_last = None
        if history is not None and completed_rounds >= 1:
            last_row_opponents = history.opponent_actions[-1, :]
            last_total_others = int(np.sum(last_row_opponents))
            my_last = bool(history.my_actions[-1])
            total_cooperators_last = last_total_others + (1 if my_last else 0)
            if my_last and total_cooperators_last < self.m:
                severity = self.m - total_cooperators_last
                new_punish = min(self.punish_cap, 1 + int(severity))
                if self.punishment_counter < new_punish:
                    self.punishment_counter = new_punish
                    self._last_punish_round = t - 1
        if self.punishment_counter and self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if history is None or completed_rounds == 0:
            if random.random() < epsilon_explore:
                return Action.C
            else:
                return Action.D
        predicted_others_last = last_total_others
        if t == self.r:
            if predicted_others_last == self.m - 1:
                return Action.C
            else:
                return Action.D
        if predicted_others_last == self.m - 1:
            return Action.C
        if predicted_others_last >= self.m:
            if random.random() < epsilon_coop_when_safe:
                return Action.C
            else:
                return Action.D
        start = max(0, completed_rounds - self.w)
        window_len = completed_rounds - start
        count_likely = 0
        if window_len > 0:
            recent_window = history.opponent_actions[start:completed_rounds, :]
            recent_counts = np.sum(recent_window, axis=0)
            recent_rates = recent_counts / float(window_len)
            count_likely = int(np.sum(recent_rates >= self.p_thresh))
        else:
            count_likely = 0
        if count_likely >= self.m - 1:
            return Action.C
        if random.random() < epsilon_explore:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_89(BaseStrategy):
    """
    Pivotal Conditional Cooperator with Reciprocity and Forgiveness (PCC-RF)

    - Cooperates when its cooperation meaningfully raises the chance of reaching the
      collective threshold (pivotality), preserves successful cooperative rounds,
      targets persistent defectors for limited punishment, and forgives on signs of
      cooperation. Defects on the final round and is more conservative in the late
      endgame.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.w = min(5, max(1, self.r - 1))
        self.alpha = 0.7
        self.gamma = 0.15
        self.gamma_end = 0.4
        self.p0 = float(self.m) / float(self.n)
        self.H = min(3, self.r)
        self.P_mem = min(10, self.r)
        self.flagged_defector = [False] * max(0, self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        num_opponents = max(0, self.n - 1)
        if history is None:
            if random.random() < self.p0:
                return Action.C
            else:
                return Action.D
        t = state.round_number
        if t == self.r:
            return Action.D
        rounds_completed = history.round_number
        opp_actions = history.opponent_actions
        recent_span_start = max(0, rounds_completed - self.P_mem)
        for j in range(num_opponents):
            count_defect_when_needed = 0
            for s in range(recent_span_start, rounds_completed):
                acted_coop = bool(opp_actions[s, j])
                coop_count = int(opp_actions[s, :].sum()) + int(history.my_actions[s])
                if not acted_coop and coop_count < self.m:
                    count_defect_when_needed += 1
            threshold_for_flag = math.ceil(0.6 * self.P_mem)
            if count_defect_when_needed >= threshold_for_flag:
                self.flagged_defector[j] = True
            else:
                recent_w_start = max(0, rounds_completed - self.w)
                if rounds_completed > 0 and opp_actions[recent_w_start:rounds_completed, j].any():
                    self.flagged_defector[j] = False
        p_list = []
        for j in range(num_opponents):
            if rounds_completed == 0:
                p_j = self.p0
                p_list.append(p_j)
                continue
            L = min(self.w, rounds_completed)
            recent_start = rounds_completed - L
            actions_recent = opp_actions[recent_start:recent_start + L, j].astype(float)
            if L > 0:
                weights = np.array([self.alpha ** (L - 1 - i) for i in range(L)], dtype=float)
                recency_rate = float((actions_recent * weights).sum() / weights.sum())
            else:
                recency_rate = 0.0
            overall_freq = float(opp_actions[:, j].sum()) / float(rounds_completed)
            p_base = self.alpha * recency_rate + (1.0 - self.alpha) * overall_freq
            if j < len(self.flagged_defector) and self.flagged_defector[j]:
                p_j = max(0.0, p_base * 0.2)
            elif L > 0 and actions_recent.sum() > 0:
                p_j = 0.8 * p_base + 0.2 * recency_rate
            else:
                p_j = p_base
            p_j = min(1.0, max(0.0, p_j))
            p_list.append(p_j)
        max_k = num_opponents
        prob = np.zeros(max_k + 1, dtype=float)
        prob[0] = 1.0
        for p in p_list:
            newprob = np.zeros_like(prob)
            newprob[0] = prob[0] * (1.0 - p)
            for k in range(1, max_k + 1):
                newprob[k] = prob[k] * (1.0 - p) + prob[k - 1] * p
            prob = newprob
        if self.m <= 0:
            P_without_me = 1.0
        else:
            idx_from = max(0, self.m)
            P_without_me = float(prob[idx_from:].sum()) if idx_from <= max_k else 0.0
        idx_from_with = max(0, self.m - 1)
        P_with_me = float(prob[idx_from_with:].sum()) if idx_from_with <= max_k else 0.0
        Delta = P_with_me - P_without_me
        prev_idx = rounds_completed - 1
        prev_coop_count = int(opp_actions[prev_idx, :].sum()) + int(history.my_actions[prev_idx])
        prev_threshold_met = prev_coop_count >= self.m
        i_cooperated_last = bool(history.my_actions[prev_idx])
        if t >= self.r - self.H + 1:
            gamma_eff = self.gamma_end
        else:
            gamma_eff = self.gamma
        if prev_threshold_met and i_cooperated_last:
            return Action.C
        if prev_threshold_met and (not i_cooperated_last):
            if random.random() < 0.5:
                return Action.C
            else:
                return Action.D
        eps = 0.02
        if abs(Delta - gamma_eff) <= eps and prev_coop_count >= max(1, int(0.6 * self.n)):
            return Action.C
        if Delta >= gamma_eff:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_90(BaseStrategy):
    """
    Adaptive Pivotal Reciprocity (APR)

    - Starts by cooperating to signal intent.
    - Cooperates when its cooperation is likely pivotal to reach the threshold.
    - Defects if the threshold is likely to be met without it (free-ride).
    - If cooperation is unlikely, explores probabilistically (higher exploration early).
    - If repeatedly exploited (wasted cooperations), issues a short punishment (Q rounds),
      then forgives and resumes exploration.
    - In the final round, defaults to defection unless recent history shows stable cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_counter = 0
        self.P = 3
        self.Q = 2
        self.W_max = 5
        self.p_explore_early = 0.5
        self.p_explore_late = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            return Action.C
        t = history.round_number + 1
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        W = min(self.W_max, t - 1)
        opp = history.opponent_actions
        if W == 0:
            expected_others = float(np.sum(opp[0, :]))
        else:
            recent = opp[-W:, :]
            per_round_other_coops = np.sum(recent, axis=1)
            expected_others = float(np.mean(per_round_other_coops))
        play = Action.D
        if expected_others + 1 >= m:
            play = Action.C
        elif expected_others >= m:
            play = Action.D
        else:
            rounds_available = history.round_number
            start_idx = max(0, rounds_available - self.P)
            my_slice = history.my_actions[start_idx:rounds_available]
            opp_slice = opp[start_idx:rounds_available, :] if rounds_available - start_idx > 0 else np.zeros((0, opp.shape[1]), dtype=bool)
            if opp_slice.size == 0:
                totals = np.array([], dtype=int)
            else:
                totals = np.sum(opp_slice, axis=1) + my_slice.astype(int)
            wasted = int(np.sum(my_slice & (totals < m)))
            if wasted >= 2:
                self.punish_counter = max(0, self.Q - 1)
                play = Action.D
            else:
                half_point = math.floor(r / 2)
                p_explore = self.p_explore_early if t <= half_point else self.p_explore_late
                play = Action.C if random.random() < p_explore else Action.D
        if t == r:
            S = min(3, max(0, r - 1))
            if S > 0:
                last_S_start = max(0, history.round_number - S)
                opp_lastS = opp[last_S_start:history.round_number, :] if history.round_number - last_S_start > 0 else np.zeros((0, opp.shape[1]), dtype=bool)
                my_lastS = history.my_actions[last_S_start:history.round_number] if history.round_number - last_S_start > 0 else np.array([], dtype=bool)
                if opp_lastS.size == 0:
                    totals_lastS = np.array([], dtype=int)
                else:
                    totals_lastS = np.sum(opp_lastS, axis=1) + my_lastS.astype(int)
                threshold_met_all = bool(totals_lastS.size > 0 and np.all(totals_lastS >= m))
                my_coops = int(np.sum(my_lastS)) if my_lastS.size > 0 else 0
                required_coops = math.ceil(S / 2)
                if threshold_met_all and my_coops >= required_coops:
                    play = Action.C
                else:
                    play = Action.D
            else:
                play = Action.D
        return play

class Strategy_COOPERATIVE_91(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Lenient Forgiveness (ACCLF)

    - Maintains per-opponent cooperation estimates (EMA) and a temporary 'bad' counter for targeted punishment.
    - Uses an exact Poisson-binomial DP over opponents' cooperation probabilities to compute
      P(exactly m-1 others cooperate) and P(at least m others cooperate).
    - Decision rules:
        * First round(s): probe by cooperating (configurable).
        * Last round: one-shot best response (cooperate iff k * P_eq(m-1) > 1, tie -> C).
        * Otherwise cooperate if cooperating is immediately pivotal (k * P_eq(m-1) > 1).
        * Else be generous (cooperate) if recent group cooperation rate >= m/n and there is remaining future.
        * Else defect. If group success is almost certain (P_ge_m >= 0.95) prefer defection (free-ride).
    - After each observed round, recomputes EMA estimates for each opponent and simulates targeted punishment:
        * If a defecting opponent's defection turned a round with exactly m-1 cooperators into failure,
          and recent environment was reasonably cooperative, mark them as bad for punish_duration rounds.
        * While bad_j > 0 their effective p_j for the DP is min(p_j, 0.1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.6
        self.alpha = 0.3
        self.probe_rounds = 1
        self.punish_duration = max(1, math.floor(self.game_description.n_rounds * 0.15))
        self.grace_window = min(5, max(1, self.game_description.n_rounds - 1))
        self.cooperation_threshold_for_generosity = self.game_description.m / float(self.game_description.n_players)
        self.tie_breaker_cooperate = True

    def _poisson_binomial_dp(self, p_list):
        n = len(p_list)
        dp = [0.0] * (n + 1)
        dp[0] = 1.0
        for p in p_list:
            for s in range(n, -1, -1):
                without = dp[s] * (1.0 - p) if s <= n else 0.0
                with_ = dp[s - 1] * p if s - 1 >= 0 else 0.0
                dp[s] = without + with_
        return dp

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        n_opp = n - 1
        if history is None:
            t = 1
            return Action.C if self.probe_rounds >= 1 else Action.D
        completed = history.round_number
        t = completed + 1
        p_js = [self.p0] * n_opp
        if completed > 0:
            for r_idx in range(completed):
                row = history.opponent_actions[r_idx, :]
                for j in range(n_opp):
                    a_j = 1.0 if bool(row[j]) else 0.0
                    p_js[j] = (1.0 - self.alpha) * p_js[j] + self.alpha * a_j
        bad_j = [0] * n_opp
        for r_idx in range(completed):
            my_a = 1 if bool(history.my_actions[r_idx]) else 0
            opp_row = history.opponent_actions[r_idx, :]
            total_coops = my_a + int(np.sum(opp_row).item())
            start_idx = max(0, r_idx - self.grace_window)
            if r_idx > 0:
                recent_window = history.opponent_actions[start_idx:r_idx, :]
                if recent_window.size == 0:
                    recent_opponent_rate = 0.0
                else:
                    row_fracs = np.mean(recent_window.astype(float), axis=1)
                    recent_opponent_rate = float(np.mean(row_fracs))
            else:
                recent_opponent_rate = 0.0
            for j in range(n_opp):
                if not bool(opp_row[j]) and total_coops == m - 1:
                    if recent_opponent_rate >= self.cooperation_threshold_for_generosity:
                        bad_j[j] = self.punish_duration
            for j in range(n_opp):
                if bad_j[j] > 0:
                    bad_j[j] = bad_j[j] - 1
        temp_p_js = []
        for j in range(n_opp):
            if bad_j[j] > 0:
                temp_p_js.append(min(p_js[j], 0.1))
            else:
                temp_p_js.append(p_js[j])
        dp = self._poisson_binomial_dp(temp_p_js)
        if 0 <= m - 1 <= n_opp:
            P_eq_m1 = dp[m - 1]
        else:
            P_eq_m1 = 0.0
        if m <= n_opp:
            P_ge_m = float(sum((dp[s] for s in range(m, n_opp + 1))))
        else:
            P_ge_m = 0.0
        if t == r:
            lhs = k * P_eq_m1
            if lhs > 1.0:
                return Action.C
            if lhs < 1.0:
                return Action.D
            return Action.C if self.tie_breaker_cooperate else Action.D
        if t <= self.probe_rounds:
            return Action.C
        if k * P_eq_m1 > 1.0:
            return Action.C
        if abs(k * P_eq_m1 - 1.0) < 1e-12 and self.tie_breaker_cooperate:
            return Action.C
        if P_ge_m >= 0.95:
            return Action.D
        if completed > 0:
            window = min(self.grace_window, completed)
            recent_rows = history.opponent_actions[-window:, :] if window > 0 else np.empty((0, n_opp))
            if recent_rows.size == 0:
                group_rate = 0.0
            else:
                row_fracs = np.mean(recent_rows.astype(float), axis=1)
                group_rate = float(np.mean(row_fracs))
        else:
            group_rate = 0.0
        if group_rate >= self.cooperation_threshold_for_generosity and t < r:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_92(BaseStrategy):
    """
    Threshold-Aware Conditional Volunteer (TACV)

    - Uses smoothed empirical beliefs about each opponent's cooperation probability.
    - Computes Poisson-Binomial distribution for the number of other cooperators.
    - Core decision: cooperate iff P(X = m-1) > 1/k (one-shot marginal EV).
    - Probe/warm-up rounds, fairness quota to meet a long-run target_total, deterministic
      tie-breaking for borderline volunteering, and lightweight punish/forgive adjustments.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.s = 1.0
        self.p_probe = 0.5
        self.far_free_ride_thresh = 0.9
        self.fairness_soften = 0.6
        self.borderline_lower = 0.5
        self.help_threshold = 0.4

    def _poisson_binomial(self, p_list, max_others):
        """
        Compute Poisson-Binomial distribution DP for X = sum Bernoulli(p_list).
        Returns numpy array probs of length (max_others+1) where probs[x] = P(X=x).
        """
        n = max_others
        dp = np.zeros(n + 1, dtype=float)
        dp[0] = 1.0
        for p in p_list:
            new = np.zeros_like(dp)
            new[0] = dp[0] * (1.0 - p)
            for x in range(1, n + 1):
                new[x] = dp[x] * (1.0 - p) + dp[x - 1] * p
            dp = new
        return dp

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        t = int(state.round_number)
        probe_rounds = min(5, max(1, math.floor(0.1 * r)))
        target_total = int(round(m * r / float(n)))
        if history is None:
            coop_counts = np.zeros(n - 1, dtype=float)
            my_coop_count = 0
            rounds_completed = 0
        else:
            rounds_completed = int(history.round_number)
            if rounds_completed > 0:
                coop_counts = np.sum(history.opponent_actions.astype(float), axis=0)
                my_coop_count = int(np.sum(history.my_actions.astype(int)))
            else:
                coop_counts = np.zeros(n - 1, dtype=float)
                my_coop_count = 0
        if t <= 1:
            p_j = np.full(n - 1, 0.5, dtype=float)
        else:
            denom = max(1.0, t - 1 + 2.0 * self.s)
            p_j = (coop_counts + self.s) / denom
        if history is not None and rounds_completed >= 1:
            last = history.opponent_actions[-1, :].astype(bool)
            for j in range(len(p_j)):
                if not last[j] and p_j[j] > 0.7:
                    p_j[j] = max(0.0, p_j[j] * 0.6)
            if rounds_completed >= 2:
                last2 = history.opponent_actions[-2:, :].astype(bool)
                both_coop = np.all(last2, axis=0)
                for j in range(len(p_j)):
                    if both_coop[j]:
                        p_j[j] = min(0.99, p_j[j] + 0.1)
        p_j = np.clip(p_j, 0.0, 0.999)
        dp = self._poisson_binomial(p_j.tolist(), max_others=n - 1)
        P_exact = float(dp[m - 1]) if 0 <= m - 1 <= n - 1 else 0.0
        if m <= n - 1:
            P_ge = float(np.sum(dp[m:]))
        else:
            P_ge = 0.0
        if m - 1 <= n - 1:
            P_ge_m1 = float(np.sum(dp[m - 1:]))
        else:
            P_ge_m1 = 0.0
        expected_others = float(np.sum(p_j))
        if t <= probe_rounds:
            if P_exact > 1.0 / k:
                return Action.C
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        if P_exact > 1.0 / k:
            return Action.C
        if P_ge > self.far_free_ride_thresh:
            return Action.D
        remaining_rounds = max(1, r - t + 1)
        remaining_needed = max(0, target_total - my_coop_count)
        if remaining_needed >= remaining_rounds:
            return Action.C
        if remaining_needed > 0 and remaining_needed / remaining_rounds >= 0.6:
            if P_exact > self.fairness_soften * (1.0 / k):
                return Action.C
        needed_if_expected = max(0, m - math.floor(expected_others))
        if needed_if_expected <= 0:
            return Action.D
        lower_border = self.borderline_lower * (1.0 / k)
        upper_border = 1.0 / k
        if needed_if_expected == 1 and P_exact > lower_border and (P_exact <= upper_border):
            rank = t % n
            if rank < needed_if_expected:
                return Action.C
            else:
                return Action.D
        if needed_if_expected >= 2 and P_ge_m1 > self.help_threshold:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_93(BaseStrategy):
    """
    Reciprocal Minimalist (RM)

    Tries to secure the public reward each round while contributing as little as necessary.
    Uses a short recent-history window to estimate how many others will cooperate, punishes
    mild exploitation temporarily, occasionally seeds cooperation, and treats the last round
    as a one-shot decision.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        r = game_description.n_rounds
        self.L = min(10, max(0, r - 1))
        self.eps_seed = max(0.02, 1.0 / max(1.0, float(r)))
        self.F = math.ceil(max(1, self.L) / 2)
        self.T_punish = self.F
        self.punish_until_round = 0
        self.delta = 0.01
        self.margin_punish = 0.2
        n = self.game_description.n_players
        m = self.game_description.m
        self.tau_forgive = max(0.8 * (m / float(n)), 0.5)
        self.tau_S = 0.6 * (m / float(n))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        t = state.round_number
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        rounds_played = history.round_number
        last_k = min(self.L, rounds_played) if self.L > 0 else 0
        if last_k > 0:
            opp_actions_recent = history.opponent_actions[-last_k:, :]
            others_counts = np.sum(opp_actions_recent, axis=1).astype(float)
            my_actions_recent = history.my_actions[-last_k:].astype(int)
            total_counts = others_counts + my_actions_recent
            P_C = float(np.mean(others_counts >= m - 1))
            P_D = float(np.mean(others_counts >= m))
            S = float(np.mean(total_counts >= m))
        else:
            others_counts = np.array([], dtype=float)
            P_C = 0.0
            P_D = 0.0
            S = 0.0
        if rounds_played >= 1:
            prev_my = bool(history.my_actions[-1])
            prev_others = float(np.sum(history.opponent_actions[-1, :]))
            prev_total = prev_others + (1.0 if prev_my else 0.0)
            if prev_my and prev_total < m:
                self.punish_until_round = t + self.T_punish - 1
        if S > self.tau_forgive:
            self.punish_until_round = 0
        E_C = -1.0 + k * P_C
        E_D = 1.0 + k * P_D
        if t == r:
            if E_C >= E_D:
                return Action.C
            else:
                return Action.D
        if t <= self.punish_until_round:
            if P_D < 0.15 and P_C > 0.85:
                return Action.C
            if E_C >= E_D + self.margin_punish:
                return Action.C
            return Action.D
        if P_D >= 0.95:
            if random.random() < self.eps_seed:
                return Action.C
            else:
                return Action.D
        if E_C >= E_D - self.delta:
            return Action.C
        if P_C >= 0.25 and P_C <= 0.7 and (random.random() < self.eps_seed):
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_94(BaseStrategy):
    """
    Adaptive Decisive-Probability Cooperator (ADPC)

    Cooperate when your single extra contribution meaningfully raises the chance
    the group reaches the threshold m. Estimates the probability that exactly
    m-1 other players cooperated over a recent window, applies a safety margin,
    mixes on borderline cases, includes small exploration, and temporarily
    down-weights the estimate after repeated cooperations that failed to reach
    the threshold (limited forgiveness/punishment).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.W = min(10, max(1, self.r))
        self.epsilon = 0.02
        self.alpha = 0.05
        self.min_samples = 2
        self.prior_weight = 1.0
        self.prior_p = 1.0 / max(1, self.n)
        self.f = 2
        self.distrust_multiplier = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        rounds_done = history.round_number
        num_samples = min(self.W, rounds_done)
        if num_samples == 0:
            return Action.C
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        recent_my = my_actions[-num_samples:]
        recent_opps = opp_actions[-num_samples:, :]
        recent_other_counts = np.sum(recent_opps, axis=1).astype(int)
        total_coop_recent = recent_other_counts + recent_my.astype(int)
        recent_success = total_coop_recent >= self.m
        target_j = self.m - 1
        if target_j < 0 or target_j > self.n - 1:
            count_target = 0
        else:
            count_target = int(np.sum(recent_other_counts == target_j))
        if num_samples < self.min_samples:
            p_decisive = (count_target + self.prior_weight * self.prior_p) / (num_samples + self.prior_weight)
        else:
            p_decisive = count_target / float(num_samples)
        consecutive_failures = 0
        for idx in range(rounds_done - 1, -1, -1):
            acted_coop = bool(history.my_actions[idx])
            other_count = int(np.sum(history.opponent_actions[idx, :]))
            total_coop = other_count + int(acted_coop)
            success = total_coop >= self.m
            if acted_coop and (not success):
                consecutive_failures += 1
            else:
                break
        if consecutive_failures >= self.f:
            p_decisive = p_decisive * self.distrust_multiplier
        decisive_indicator = self.k * p_decisive
        action = None
        if decisive_indicator > 1.0 + self.alpha:
            action = Action.C
        elif decisive_indicator < 1.0 - self.alpha:
            action = Action.D
        else:
            lower = 1.0 - self.alpha
            upper = 1.0 + self.alpha
            if upper - lower <= 0:
                pC = 0.0
            else:
                pC = (decisive_indicator - lower) / (upper - lower)
                if pC < 0.0:
                    pC = 0.0
                elif pC > 1.0:
                    pC = 1.0
            action = Action.C if random.random() < pC else Action.D
        if random.random() < self.epsilon:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_COOPERATIVE_95(BaseStrategy):
    """
    Collective Risk Rotator-Prober strategy.

    Summary:
    - Round 1: deterministic role assignment: players with lowest indices 1..m cooperate.
      (If the agent cannot discover its global index from BaseStrategy attributes,
      it defaults to index 1.)
    - For t>1:
      - If others_last_C >= m: defect (free-ride).
      - If others_last_C == m-1: cooperate (pivotal).
      - If last-round others < m-1:
          - If there was at least one success in the last W rounds (W=5):
              use deterministic rotation to nominate exactly m cooperators this round;
              cooperate iff your index is in the candidate set.
          - Else (no recent successes):
              use a small rotating probe set (size = ceil(m/2)); if you are in that set,
              probe (cooperate) with probability p_probe (0.15). Otherwise defect.
      - Last round (t == r): only cooperate if pivotal (others_last_C == m-1).
    Notes:
    - Tries to be deterministic given available info; uses a fallback if the agent's
      global index is unavailable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_probe = 0.15
        self.W = 5
        self.L = 10
        self._my_index = None

    def _resolve_my_index(self):
        """
        Try to find a deterministic 1-based player index from attributes that may be present
        on BaseStrategy. If none found, default to 1.
        """
        n = self.game_description.n_players
        candidates = ('player_index', 'player_id', 'index', 'id')
        for attr in candidates:
            if hasattr(self, attr):
                try:
                    val = getattr(self, attr)
                    if isinstance(val, int):
                        if 0 <= val < n:
                            return val + 1
                        if 1 <= val <= n:
                            return val
                    else:
                        iv = int(val)
                        if 0 <= iv < n:
                            return iv + 1
                        if 1 <= iv <= n:
                            return iv
                except Exception:
                    pass
        return 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if self._my_index is None:
            self._my_index = self._resolve_my_index()
            if not 1 <= self._my_index <= max(1, n):
                self._my_index = 1
        my_idx = self._my_index
        t = state.round_number
        if history is None:
            if my_idx <= m:
                return Action.C
            else:
                return Action.D
        completed = history.round_number
        if completed == 0:
            if my_idx <= m:
                return Action.C
            else:
                return Action.D
        try:
            others_last_C = int(np.sum(history.opponent_actions[-1, :]))
        except Exception:
            others_last_C = 0
        if others_last_C >= m:
            return Action.D
        if others_last_C == m - 1:
            return Action.C
        if t == r:
            return Action.D
        W = self.W
        start_idx = max(0, completed - W)
        try:
            opp_slice = history.opponent_actions[start_idx:completed, :]
            my_slice = history.my_actions[start_idx:completed]
            opp_counts = np.sum(opp_slice, axis=1) if opp_slice.size != 0 else np.array([], dtype=int)
            my_counts = my_slice.astype(int) if my_slice.size != 0 else np.array([], dtype=int)
            total_coops = opp_counts + my_counts
            successes_in_window = int(np.sum(total_coops >= m))
        except Exception:
            successes_in_window = 0
        if successes_in_window >= 1:
            rotate_start = (t - 1) * m % n
            candidate_indices = {(rotate_start + j) % n + 1 for j in range(m)}
            if my_idx in candidate_indices:
                return Action.C
            else:
                return Action.D
        small_probe_set_size = max(1, math.ceil(m / 2))
        start_p = t * small_probe_set_size % n
        probe_candidates = {(start_p + j) % n + 1 for j in range(small_probe_set_size)}
        if my_idx in probe_candidates:
            if random.random() < self.p_probe:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_96(BaseStrategy):
    """
    Rotating-Share with Forgiving Emergency Assistance (RS-FEA)

    - Implements a deterministic rotation of m cooperators per round (round-robin).
    - Tracks per-player reliability and temporary penalties.
    - If scheduled cooperators are insufficient (due to penalties), a deterministic
      volunteer order among unpenalized players fills remaining slots.
    - Forgiving: penalties are finite; reliability is reset when a penalty starts.
    - Last round: defect by default (no enforceable punishment).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.W = min(10, self.r)
        self.F = max(1, math.floor(self.W / 2))
        self.P = min(5, self.r)
        self.E = min(1, self.r)
        self.reliability = np.zeros(self.n, dtype=float)
        self.penalty_until = np.zeros(self.n, dtype=int)
        self._last_updated_round = 0

    def _build_cyclic_list(self, t: int):
        start_idx = (t - 1) * self.m % self.n
        return [(start_idx + offset) % self.n for offset in range(self.n)]

    def _compute_S(self, t: int):
        unpenalized = [j for j in range(self.n) if self.penalty_until[j] < t]
        L = self._build_cyclic_list(t)
        S = []
        for idx in L:
            if idx in unpenalized:
                S.append(idx)
                if len(S) >= self.m:
                    break
        if len(S) < self.m:
            S = [j for j in L if j in unpenalized]
        return (S, unpenalized, L)

    def _clamp_reliability(self, j: int):
        lo = -self.W
        hi = self.W
        if self.reliability[j] < lo:
            self.reliability[j] = lo
        if self.reliability[j] > hi:
            self.reliability[j] = hi

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is not None and history.round_number > self._last_updated_round:
            for completed_t in range(self._last_updated_round + 1, history.round_number + 1):
                my_act = bool(history.my_actions[completed_t - 1])
                opp_row = history.opponent_actions[completed_t - 1]
                full_actions = np.zeros(self.n, dtype=bool)
                full_actions[0] = my_act
                if self.n > 1:
                    full_actions[1:] = opp_row[:self.n - 1]
                S_t, unpenalized_t, L_t = self._compute_S(completed_t)
                for j in range(self.n):
                    played_C = bool(full_actions[j])
                    if j in S_t:
                        if played_C:
                            self.reliability[j] += 1.0
                        else:
                            self.reliability[j] -= 1.0
                    elif played_C:
                        self.reliability[j] += 0.5
                    self._clamp_reliability(j)
                for j in range(self.n):
                    if self.reliability[j] <= -self.F and self.penalty_until[j] < completed_t:
                        self.penalty_until[j] = completed_t + self.P
                        self.reliability[j] = 0.0
            self._last_updated_round = history.round_number
        if t >= self.r:
            return Action.D
        S_t, unpenalized_t, L = self._compute_S(t)
        S_set = set(S_t)
        unpenalized_set = set(unpenalized_t)
        current_coops_expected = len(S_t)
        needed = max(0, self.m - current_coops_expected)
        my_index = 0
        if my_index in S_set:
            return Action.C
        if needed == 0:
            return Action.D
        volunteers = []
        for idx in L:
            if idx in unpenalized_set and idx not in S_set:
                volunteers.append(idx)
                if len(volunteers) >= needed:
                    break
        if my_index in volunteers:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_97(BaseStrategy):
    """
    Adaptive Pivotal Cooperation with Limited Punishment (APC-LP).

    - Seeds cooperation in round 1.
    - Estimates each opponent's cooperation probability from a recent window (up to 8 rounds).
    - Computes expected utility of Cooperating vs Defecting based on the probability
      that enough others will cooperate to meet the collective threshold.
    - If cooperating was exploited in the immediately previous round (others < m and
      you cooperated), enter a limited punishment phase (defect for up to punish_len rounds),
      but cancel punishment immediately if a successful round (others >= m) is observed.
    - Uses small randomized play when EU's are within eps to help re-coordinate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.w_max = 8
        self.eps = 0.05
        self.punish_len = 2
        self.min_attempt_prob = 0.1
        self.remaining_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.remaining_punish = 0
            return Action.C
        t = history.round_number + 1
        last_round_exists = history.round_number >= 1
        last_others_coops = 0
        last_my_action = False
        if last_round_exists:
            last_others_coops = int(np.sum(history.opponent_actions[-1, :]))
            last_my_action = bool(history.my_actions[-1])
        if last_round_exists and last_others_coops >= self.m:
            self.remaining_punish = 0
        if last_round_exists and last_others_coops < self.m and last_my_action:
            self.remaining_punish = self.punish_len
        if self.remaining_punish > 0:
            self.remaining_punish = max(0, self.remaining_punish - 1)
            return Action.D
        w = min(self.w_max, max(0, t - 1))
        n_others = self.n - 1
        if w == 0:
            p_list = [0.5] * n_others
        else:
            recent = history.opponent_actions[-w:, :] if w > 0 else np.zeros((0, n_others), dtype=bool)
            counts = np.sum(recent, axis=0)
            p_list = [float(c) / float(w) for c in counts]

        def prob_at_least_K(p_list_local, K):
            if K <= 0:
                return 1.0
            m_len = len(p_list_local)
            if K > m_len:
                return 0.0
            dp = [0.0] * (m_len + 1)
            dp[0] = 1.0
            for p in p_list_local:
                newdp = [0.0] * (m_len + 1)
                for s in range(0, m_len):
                    v = dp[s]
                    if v == 0.0:
                        continue
                    newdp[s] += v * (1.0 - p)
                    newdp[s + 1] += v * p
                dp = newdp
            return float(sum(dp[K:]))
        Pr_at_least_m = prob_at_least_K(p_list, self.m)
        Pr_at_least_m_minus_1 = prob_at_least_K(p_list, max(0, self.m - 1))
        EU_C = float(self.k) * Pr_at_least_m_minus_1
        EU_D = 1.0 + float(self.k) * Pr_at_least_m
        tiny = 1e-09
        if EU_C > EU_D + self.eps:
            action = Action.C
        elif EU_D > EU_C + self.eps:
            action = Action.D
        else:
            denom = EU_C + EU_D + tiny
            prob_C = EU_C / denom
            if prob_C < self.min_attempt_prob:
                prob_C = self.min_attempt_prob
            prob_C = max(self.min_attempt_prob, min(1.0, prob_C))
            action = Action.C if random.random() < prob_C else Action.D
        return action

class Strategy_COOPERATIVE_98(BaseStrategy):
    """
    Pivotal Reciprocity with Forgiveness (PRF).

    - Cooperates when the agent is likely pivotal (P_exact(m-1) > 1/k).
    - Free-rides when others almost certainly reach the threshold without you.
    - Encourages high reciprocity (p_hat >= p_high) except in the final round.
    - Punishes for a bounded number of rounds (T) when your cooperation failed to yield the threshold,
      but forgives early if observed partner cooperativeness (p_hat) recovers above p_restore.
    - Uses sliding-window per-opponent cooperation frequencies (window W) with prior p0 for missing data.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = game_description.n_players
        m = game_description.m
        self.p0 = (m - 1) / (n - 1) if n - 1 > 0 else 0.0
        r = game_description.n_rounds
        self.W = min(10, max(3, max(0, r - 1)))
        self.T = max(3, math.floor(r / 10))
        self.P_atleast_threshold = 0.9
        self.p_high = 0.75
        self.p_restore = max(0.6, self.p0)
        self.n_others = max(0, game_description.n_players - 1)
        if self.n_others > 0:
            self.f_j = np.full(self.n_others, self.p0, dtype=float)
        else:
            self.f_j = np.array([], dtype=float)
        self.pun_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        N = max(0, n - 1)
        if history is None:
            p_hat = self.p0
            if N >= m - 1 >= 0:
                P_exact_mminus1 = math.comb(N, m - 1) * p_hat ** (m - 1) * (1.0 - p_hat) ** (N - (m - 1))
            else:
                P_exact_mminus1 = 0.0
            sum_low = 0.0
            for x in range(0, min(m, N + 1)):
                sum_low += math.comb(N, x) * p_hat ** x * (1.0 - p_hat) ** (N - x)
            P_atleast_m = 1.0 - sum_low
            if P_atleast_m >= self.P_atleast_threshold:
                return Action.D
            if P_exact_mminus1 > 1.0 / k:
                return Action.C
            remaining = r
            if p_hat >= self.p_high and remaining > 1:
                return Action.C
            return Action.D
        rounds_completed = history.round_number
        remaining = max(0, r - rounds_completed)
        if self.n_others > 0 and rounds_completed > 0:
            actions = history.opponent_actions
            use_rows = min(self.W, rounds_completed)
            if use_rows > 0:
                window = actions[-use_rows:, :]
                coop_counts = np.sum(window.astype(float), axis=0)
                missing = self.W - use_rows
                self.f_j = (coop_counts + missing * self.p0) / float(self.W)
            else:
                self.f_j = np.full(self.n_others, self.p0, dtype=float)
        elif self.n_others > 0:
            self.f_j = np.full(self.n_others, self.p0, dtype=float)
        else:
            self.f_j = np.array([], dtype=float)
        p_hat = float(np.mean(self.f_j)) if self.n_others > 0 else 0.0
        if rounds_completed > 0:
            my_last = bool(history.my_actions[-1])
            coop_last = int(np.sum(history.opponent_actions[-1, :])) + (1 if my_last else 0)
            if my_last and coop_last < m:
                self.pun_count = self.T
        if self.pun_count > 0:
            if p_hat > self.p_restore:
                self.pun_count = 0
            else:
                self.pun_count = max(0, self.pun_count - 1)
                return Action.D
        if N >= m - 1 >= 0:
            try:
                P_exact_mminus1 = math.comb(N, m - 1) * p_hat ** (m - 1) * (1.0 - p_hat) ** (N - (m - 1))
            except ValueError:
                P_exact_mminus1 = 0.0
        else:
            P_exact_mminus1 = 0.0
        sum_low = 0.0
        upper_x = min(m - 1, N)
        for x in range(0, upper_x + 1):
            sum_low += math.comb(N, x) * p_hat ** x * (1.0 - p_hat) ** (N - x)
        P_atleast_m = 1.0 - sum_low
        if P_atleast_m >= self.P_atleast_threshold:
            return Action.D
        if P_exact_mminus1 > 1.0 / k:
            return Action.C
        if p_hat >= self.p_high and remaining > 1:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_99(BaseStrategy):
    """
    Probabilistic Conditional Cooperator (PCC)

    - Estimates per-opponent cooperation probabilities from observed frequencies.
    - Computes the distribution of cooperators among the other n-1 players by convolution.
    - Uses a myopic pivotal test: cooperate if k * P_eq > 1 (P_eq = Prob(others == m-1)).
    - Otherwise uses reciprocity (cooperate after a successful round if I cooperated),
      or cooperates when recent success frequency is high (sustaining cooperation).
    - Occasionally explores (small epsilon) except on the final round.
    - First round: cooperate. Final round: only myopic test (no reciprocity, no exploration).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = None
        self.W = min(10, max(1, self.game_description.n_rounds))
        self.S_thres = 0.6
        self.F = 3
        self.epsilon = max(0.02, 1.0 / max(1, self.game_description.n_rounds))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        rounds_seen = history.round_number
        upcoming_round = rounds_seen + 1
        is_final_round = upcoming_round == r
        opponents_count = n - 1
        if opponents_count <= 0:
            return Action.D
        if rounds_seen > 0:
            coop_counts = np.sum(history.opponent_actions, axis=0)
            p_js = np.array(coop_counts, dtype=float) / float(rounds_seen)
            p_js = p_js.tolist()
        else:
            p_js = [0.0] * opponents_count
        prob = [0.0] * (opponents_count + 1)
        prob[0] = 1.0
        for p in p_js:
            newprob = [0.0] * (opponents_count + 1)
            for x in range(opponents_count):
                px = prob[x]
                if px:
                    newprob[x] += px * (1.0 - p)
                    newprob[x + 1] += px * p
            newprob[opponents_count] += prob[opponents_count] * (1.0 - (p if p_js else 0.0))
            prob = newprob
        P_eq = 0.0
        if 0 <= m - 1 <= opponents_count:
            P_eq = prob[m - 1]
        P_ge = 0.0
        if m <= opponents_count:
            P_ge = sum(prob[m:opponents_count + 1])
        else:
            P_ge = 0.0
        myopic_value = k * P_eq - 1.0
        epsilon = 0.0 if is_final_round else self.epsilon
        explore = random.random() < epsilon if epsilon > 0.0 else False
        if is_final_round:
            if myopic_value > 0.0:
                return Action.C
            else:
                return Action.D
        if explore:
            return Action.C
        if myopic_value > 0.0:
            return Action.C
        last_success = False
        i_cooperated_last = False
        if rounds_seen >= 1:
            last_round_opponent_coops = int(np.sum(history.opponent_actions[-1, :]))
            last_round_my_coop = bool(history.my_actions[-1])
            last_total_coops = last_round_opponent_coops + (1 if last_round_my_coop else 0)
            last_success = last_total_coops >= m
            i_cooperated_last = last_round_my_coop
        w = min(self.W, rounds_seen)
        s_freq = 0.0
        if w > 0:
            recent_opponent = history.opponent_actions[-w:, :]
            recent_my = history.my_actions[-w:]
            totals = np.sum(recent_opponent, axis=1) + recent_my.astype(int)
            successes = int(np.sum(totals >= m))
            s_freq = successes / float(w)
        else:
            s_freq = 0.0
        if last_success and i_cooperated_last:
            return Action.C
        if s_freq >= self.S_thres:
            return Action.C
        if rounds_seen >= self.F:
            last_F = history.my_actions[-self.F:]
            all_defected = bool(np.all(last_F == False))
            if all_defected and last_success:
                if myopic_value > 0.0 or s_freq >= self.S_thres:
                    return Action.C
        return Action.D

class Strategy_COOPERATIVE_100(BaseStrategy):
    """
    Reciprocal-Pivotal Cooperative (RPC)

    - Starts cooperative to signal willingness.
    - Cooperates when (i) others are expected to reach threshold without you -> defect to free-ride,
      (ii) your single cooperation is likely pivotal -> cooperate,
      (iii) otherwise cooperate if peers show sufficient reciprocity (CR >= S) or past successes >= F.
    - Enters a short cooldown punishment (refuse to cooperate) if you were exploited in recent failed rounds.
    - Forgives when group behavior improves.
    - Always defects in the final round (endgame-aware).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_until_round = 0
        r = self.game_description.n_rounds
        self.L = min(3, max(1, math.floor(r / 10)))
        self.S = 0.45
        self.F = 0.5
        self.pivotal_margin = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        t = getattr(state, 'round_number', 1)
        if history is None or history.round_number == 0:
            return Action.C
        rounds_seen = history.round_number
        rounds_seen_safe = max(1, rounds_seen)
        opp_actions = history.opponent_actions.astype(int)
        my_actions = history.my_actions.astype(int)
        if rounds_seen > 0:
            count_coop_j = np.sum(opp_actions, axis=0)
        else:
            count_coop_j = np.zeros(n - 1, dtype=int)
        p_j = count_coop_j / rounds_seen_safe
        E = float(np.sum(p_j))
        CR = float(np.sum(count_coop_j) / ((n - 1) * rounds_seen_safe))
        per_round_other_coops = np.sum(opp_actions, axis=1) if rounds_seen > 0 else np.zeros(0, dtype=int)
        per_round_total_coops = per_round_other_coops + my_actions
        total_successes = int(np.sum(per_round_total_coops >= m))
        success_rate = float(total_successes / rounds_seen_safe)
        if rounds_seen == 0:
            RD = 0.0
        else:
            recent_k = min(5, rounds_seen)
            recent_slice = opp_actions[-recent_k:, :]
            recent_coop_mean = float(np.mean(recent_slice)) if recent_slice.size > 0 else 0.0
            RD = 1.0 - recent_coop_mean
        if self.punish_until_round > 0 and (CR >= self.S or success_rate >= self.F):
            self.punish_until_round = 0
        if t == r:
            return Action.D
        if t <= self.punish_until_round:
            return Action.D
        if E >= m:
            action = Action.D
        elif E >= m - self.pivotal_margin:
            action = Action.C
        elif CR >= self.S or success_rate >= self.F:
            action = Action.C
        else:
            action = Action.D
        if rounds_seen > 0:
            last_round_index = rounds_seen - 1
            last_round_total_coops = int(per_round_total_coops[last_round_index])
            you_cooperated_last = bool(my_actions[last_round_index])
            last_round_others_coop_fraction = float(per_round_other_coops[last_round_index]) / max(1, n - 1)
            if last_round_total_coops < m and you_cooperated_last:
                if RD > 0.5 or last_round_others_coop_fraction < self.S:
                    new_punish_until = t + self.L - 1
                    if new_punish_until > self.punish_until_round:
                        self.punish_until_round = new_punish_until
        return action

class Strategy_COOPERATIVE_101(BaseStrategy):
    """
    Rotation-with-repair-and-finite-punishment strategy for the Collective Risk Dilemma.

    - Uses a deterministic round-robin schedule Sched(t) picking exactly m intended cooperators.
    - Marks scheduled free-riders for a short punishment length (P_len).
    - On threshold failures triggers a short repair window (R_len rounds) where all unmarked players try to cooperate to recover.
    - Forgives immediately when a marked player is observed cooperating during their punishment window.
    - All state (Marked, RepairUntil) is derived from the public history each call to ensure consistency.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.P_len = max(1, min(3, math.floor(self.r / 10)))
        self.R_len = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)

        def sched(round_t: int):
            base = (round_t - 1) * self.m
            S = set()
            for j in range(1, self.m + 1):
                idx = (base + j - 1) % self.n + 1
                S.add(idx)
            return S
        my_index = self.n
        Marked = [0] * (self.n + 1)
        RepairUntil = 0
        if history is None or history.round_number == 0:
            if my_index in sched(t):
                return Action.C
            else:
                return Action.D
        completed = history.round_number
        for s in range(1, completed + 1):
            idx = s - 1
            full_actions = [False] * (self.n + 1)
            if history.opponent_actions.shape[0] > idx:
                n_opp_cols = history.opponent_actions.shape[1]
                for j in range(1, min(self.n, n_opp_cols + 1)):
                    full_actions[j] = bool(history.opponent_actions[idx, j - 1])
            if history.my_actions.shape[0] > idx:
                full_actions[my_index] = bool(history.my_actions[idx])
            coop_count = sum((1 for j in range(1, self.n + 1) if full_actions[j]))
            if coop_count < self.m:
                RepairUntil = max(RepairUntil, min(self.r, s + self.R_len))
            S_s = sched(s)
            for j in S_s:
                if not full_actions[j]:
                    expiry = min(self.r, s + self.P_len)
                    if expiry > Marked[j]:
                        Marked[j] = expiry
            for j in range(1, self.n + 1):
                if Marked[j] > 0:
                    if full_actions[j] and s <= Marked[j]:
                        Marked[j] = 0
        if t <= RepairUntil:
            if Marked[my_index] >= t:
                return Action.D
            else:
                return Action.C
        S_t = sched(t)
        Effective = [j for j in S_t if Marked[j] < t]
        Effective_set = set(Effective)
        candidate = 1
        while len(Effective_set) < self.m:
            if candidate > self.n:
                candidate = 1
            if candidate not in Effective_set and Marked[candidate] < t:
                Effective_set.add(candidate)
            candidate += 1
            if candidate > self.n + self.n:
                for j in range(1, self.n + 1):
                    if j not in Effective_set:
                        Effective_set.add(j)
                    if len(Effective_set) >= self.m:
                        break
                break
        if my_index in Effective_set and Marked[my_index] < t:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_102(BaseStrategy):
    """
    Rotating-schedule collective-risk strategy with monitoring, withdrawal, and forgiveness.

    - Deterministic schedule: each round t assigns m consecutive player indices (1-based) starting
      at s = ((t-1) mod n) + 1. A scheduled player should cooperate that round.
    - Monitoring: use a sliding window w = min(5, r-1) to compute:
        success_rate = fraction of recent rounds with >= m cooperators
        scheduled_compliance_rate = fraction of scheduled slots in recent rounds actually played C
      Thresholds: success_threshold (default 0.7), scheduled_threshold (default 0.6).
    - Modes:
        NORMAL: follow schedule (cooperate iff scheduled)
        WITHDRAW: defect every round until both rates are back above thresholds
      Start in NORMAL. Transitions evaluated at the start of each round (using available history).
    - Always defect in final round (t == r).
    - Optional tiny exploration epsilon applied only in the first round immediately after switching
      back to NORMAL: if not scheduled, cooperate with probability epsilon to help recovery.
    Notes:
    - This implementation assumes a consistent global player indexing where players are numbered
      1..n and opponent_actions columns correspond to players 1..n-1 in that order and this agent's
      index is n. This yields a deterministic schedule and mapping from observed opponent columns
      into global player indices.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.w = min(5, max(0, self.r - 1))
        self.success_threshold = 0.7
        self.scheduled_threshold = 0.6
        self.epsilon = 0.05
        self.MODE_NORMAL = 'NORMAL'
        self.MODE_WITHDRAW = 'WITHDRAW'
        self.mode = self.MODE_NORMAL
        self.last_mode_change_round = 0
        self.my_index = self.n

    def _scheduled_set(self, t: int):
        """Return a set of scheduled contributor indices (1-based) for round t (1-based)."""
        s = (t - 1) % self.n + 1
        S = {(s + j - 1) % self.n + 1 for j in range(self.m)}
        return S

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == self.r:
            return Action.D
        if history is None:
            scheduled = self._scheduled_set(t)
            if self.my_index in scheduled:
                return Action.C
            else:
                return Action.D
        completed = int(history.round_number)
        if self.w <= 0 or completed == 0:
            success_rate = 1.0
            scheduled_compliance_rate = 1.0
        else:
            start_round = max(1, t - self.w)
            end_round = t - 1
            start_idx = start_round - 1
            end_idx = end_round - 1
            num_rounds_window = end_idx - start_idx + 1
            max_available = completed
            if start_idx < 0:
                start_idx = 0
            if end_idx >= max_available:
                end_idx = max_available - 1
            if end_idx < start_idx:
                success_rate = 1.0
                scheduled_compliance_rate = 1.0
            else:
                opp_actions = history.opponent_actions
                my_actions = history.my_actions
                successes = 0
                scheduled_filled = 0
                total_scheduled_slots = 0
                for idx in range(start_idx, end_idx + 1):
                    coop_opponents = int(np.sum(opp_actions[idx, :])) if opp_actions.size > 0 else 0
                    coop_me = int(my_actions[idx])
                    total_cooperators = coop_me + coop_opponents
                    if total_cooperators >= self.m:
                        successes += 1
                    rr = idx + 1
                    S_rr = self._scheduled_set(rr)
                    total_scheduled_slots += len(S_rr)
                    for player_idx in S_rr:
                        if player_idx == self.my_index:
                            if bool(my_actions[idx]):
                                scheduled_filled += 1
                        else:
                            col = player_idx - 1
                            if 0 <= col < opp_actions.shape[1]:
                                if bool(opp_actions[idx, col]):
                                    scheduled_filled += 1
                            else:
                                pass
                success_rate = float(successes) / float(end_idx - start_idx + 1)
                if total_scheduled_slots > 0:
                    scheduled_compliance_rate = float(scheduled_filled) / float(total_scheduled_slots)
                else:
                    scheduled_compliance_rate = 1.0
        previous_mode = self.mode
        if self.mode == self.MODE_NORMAL:
            if success_rate < self.success_threshold or scheduled_compliance_rate < self.scheduled_threshold:
                self.mode = self.MODE_WITHDRAW
                self.last_mode_change_round = t
        elif success_rate >= self.success_threshold and scheduled_compliance_rate >= self.scheduled_threshold:
            self.mode = self.MODE_NORMAL
            self.last_mode_change_round = t
        if self.mode == self.MODE_WITHDRAW:
            return Action.D
        S_t = self._scheduled_set(t)
        if self.my_index in S_t:
            return Action.C
        if self.epsilon > 0 and self.last_mode_change_round == t and (random.random() < self.epsilon):
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_103(BaseStrategy):
    """
    Adaptive Pivotal Cooperation (APC)

    Cooperate when your contribution materially increases the chance the group
    reaches the threshold m, using per-opponent recent cooperation frequencies
    to estimate probabilities. Slightly bias borderline decisions toward cooperation.
    Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.p0 = 0.5
        self.epsilon_coop = 0.05
        self.w = min(10, max(1, self.game.n_rounds - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.game.n_players
        r = self.game.n_rounds
        m = self.game.m
        k = self.game.k
        if t == r:
            return Action.D
        if history is None:
            p_list = [self.p0] * (n - 1)
        else:
            rounds_available = history.round_number
            use = min(self.w, rounds_available)
            if use <= 0:
                p_list = [self.p0] * (n - 1)
            else:
                recent = history.opponent_actions[-use:, :]
                freqs = np.mean(recent.astype(float), axis=0)
                p_list = [float(freqs[j]) if j < len(freqs) else self.p0 for j in range(n - 1)]

        def prob_at_least_k(p_list_local, k_needed):
            if k_needed <= 0:
                return 1.0
            m_len = len(p_list_local)
            if k_needed > m_len:
                return 0.0
            dp = [0.0] * (m_len + 1)
            dp[0] = 1.0
            for p in p_list_local:
                prev = dp[:]
                dp[0] = prev[0] * (1.0 - p)
                for j in range(1, m_len + 1):
                    dp[j] = prev[j] * (1.0 - p) + prev[j - 1] * p
            return float(sum(dp[k_needed:]))
        P_defect_success = prob_at_least_k(p_list, m)
        P_cooperate_success = prob_at_least_k(p_list, m - 1)
        U_D = 1.0 + k * P_defect_success
        U_C = 0.0 + k * P_cooperate_success
        margin = -self.epsilon_coop
        if U_C >= U_D + margin:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_104(BaseStrategy):
    """
    Pivotal Conditional Cooperator (PCC)

    - Maintains soft reputations s_j for each opponent (initialized to 0.5).
    - Uses exponential smoothing (alpha) to update reputations from observed rounds.
    - In an initial exploration phase (1-2 rounds) cooperates to seed reputation.
    - Each decision computes P_piv = P(sum_{j!=i} X_j == m-1) where X_j ~ Bernoulli(s_j).
      Uses exact DP for the Poisson-Binomial unless the opponent count is large,
      in which case a normal approximation with continuity correction is used.
    - Cooperates iff k * P_piv >= 1 (tie-breaker favors cooperation).
    - Applies light targeted punishment after rounds that fail to reach threshold:
      defectors in such rounds have their s_j multiplied by beta_punish (then smoothing
      continues to allow forgiveness).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = max(1, game_description.n_players - 1)
        self.s = np.full((n,), 0.5, dtype=float)
        self.alpha = 0.75
        self.beta_punish = 0.5
        self.exploration_rounds = min(2, max(1, game_description.n_rounds - 1))
        self._updated_rounds = 0

    def _poisson_binomial_mass(self, ps, target):
        """
        Compute P(sum Bernoulli(ps) == target).
        Use exact DP when feasible, otherwise use normal approximation.
        ps: iterable of probabilities (length N)
        target: integer target count
        """
        ps = np.asarray(ps, dtype=float)
        N = ps.size
        if target < 0 or target > N:
            return 0.0
        if N <= 200:
            dp = np.zeros(N + 1, dtype=float)
            dp[0] = 1.0
            for p in ps:
                prev = dp.copy()
                dp[0] = prev[0] * (1.0 - p)
                for t in range(1, N + 1):
                    dp[t] = prev[t] * (1.0 - p) + prev[t - 1] * p
            return float(dp[target])
        else:
            mu = float(np.sum(ps))
            var = float(np.sum(ps * (1.0 - ps)))
            if var <= 0.0:
                return 1.0 if abs(mu - target) < 1e-12 else 0.0
            std = math.sqrt(var)

            def _norm_cdf(x):
                return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
            upper = (target + 0.5 - mu) / std
            lower = (target - 0.5 - mu) / std
            return max(0.0, min(1.0, _norm_cdf(upper) - _norm_cdf(lower)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is not None:
            completed = history.round_number
            for r_idx in range(self._updated_rounds, completed):
                round_actions = np.asarray(history.opponent_actions[r_idx, :], dtype=bool)
                for j in range(self.s.size):
                    if round_actions[j]:
                        obs = 1.0
                    else:
                        obs = 0.0
                    self.s[j] = self.alpha * self.s[j] + (1.0 - self.alpha) * obs
                others_C = int(np.sum(round_actions))
                m = self.game_description.m
                if others_C <= m - 1:
                    for j in range(self.s.size):
                        if not round_actions[j]:
                            self.s[j] = float(self.beta_punish * self.s[j])
            self._updated_rounds = completed
        if history is None:
            return Action.C
        if history.round_number < self.exploration_rounds:
            return Action.C
        target = self.game_description.m - 1
        P_piv = self._poisson_binomial_mass(self.s, target)
        if self.game_description.k * P_piv >= 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_105(BaseStrategy):
    """
    Thresholded Conditional Cooperator with Forgiveness (TCCF)

    - Seeds cooperation on first move.
    - Uses short-window, recency-weighted estimates of opponents' cooperation probabilities.
    - Computes exact Poisson-Binomial distribution over other players to estimate
      pivotality and success probabilities.
    - Cooperates when cooperation is individually rational (pivotal) or when the group
      is reliably cooperative and a small altruistic nudge helps.
    - Uses bounded, short punishments and a defensive mode when group failures are persistent.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.punish_counter = 0
        self.defensive_mode = False
        self.delta = 0.05
        self.P = 2
        self.decay = 0.8
        self.smoothing = 1.0
        self.tol = 1e-08

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            current_round = getattr(state, 'round_number', 1)
            return Action.C
        rounds_completed = history.round_number
        current_round = getattr(state, 'round_number', rounds_completed + 1)
        n = self.game.n_players
        r = self.game.n_rounds
        m = self.game.m
        k = self.game.k
        if current_round == r:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if rounds_completed <= 1 and current_round == 2:
            return Action.C
        W = min(10, rounds_completed) if rounds_completed > 0 else 0

        def estimate_p(j_idx: int) -> float:
            if W == 0:
                return 0.5
            opp_actions = history.opponent_actions[-W:, j_idx]
            weights = np.array([self.decay ** (W - 1 - i) for i in range(W)], dtype=float)
            weighted_sum = float(np.dot(weights, opp_actions.astype(float)))
            denom = weights.sum() + 2.0 * self.smoothing
            p = (weighted_sum + self.smoothing) / denom
            return float(p)
        num_opponents = n - 1
        p_list = []
        for j in range(num_opponents):
            p_list.append(estimate_p(j))
        q = np.zeros(num_opponents + 1, dtype=float)
        q[0] = 1.0
        for p in p_list:
            prev = q.copy()
            q[1:] = prev[:-1] * p + prev[1:] * (1.0 - p)
            q[0] = prev[0] * (1.0 - p)
        if m <= num_opponents:
            P_m = float(q[m:].sum())
        else:
            P_m = 0.0
        idx_m1 = max(0, m - 1)
        if idx_m1 <= num_opponents:
            P_m1 = float(q[idx_m1:].sum())
        else:
            P_m1 = 0.0
        P_piv = q[m - 1] if 0 <= m - 1 <= num_opponents else 0.0
        E_C = k * P_m1
        E_D = 1.0 + k * P_m
        recent_success_count = 0
        recent_fail_count = 0
        recent_cooperators_counts = []
        if W > 0:
            my_slice = history.my_actions[-W:]
            opp_slice = history.opponent_actions[-W:, :]
            for t_idx in range(W):
                coop_count = int(my_slice[t_idx]) + int(np.sum(opp_slice[t_idx, :]))
                recent_cooperators_counts.append(coop_count)
                if coop_count >= m:
                    recent_success_count += 1
                else:
                    recent_fail_count += 1
            recent_success_rate = recent_success_count / float(W)
            recent_fail_rate = recent_fail_count / float(W)
        else:
            recent_success_rate = 0.0
            recent_fail_rate = 0.0
        if not self.defensive_mode:
            if W > 0 and recent_fail_rate >= 0.5:
                self.defensive_mode = True
        elif recent_success_rate >= 0.6:
            self.defensive_mode = False
        if self.defensive_mode:
            return Action.D
        if rounds_completed >= 2 and W >= 2:
            last_coop = recent_cooperators_counts[-1]
            prev_coop = recent_cooperators_counts[-2]
            if prev_coop >= m and last_coop < m:
                last_opponent_actions = history.opponent_actions[-1, :]
                num_opponent_defections = int(np.sum(~last_opponent_actions))
                if num_opponent_defections >= math.ceil(num_opponents / 2):
                    self.punish_counter = self.P
                    self.punish_counter -= 1
                    return Action.D
        if E_C > E_D + self.tol:
            return Action.C
        if abs(E_C - E_D) <= self.tol:
            if current_round <= r - 2:
                return Action.C
            else:
                return Action.D
        if recent_success_rate >= 0.6 and P_m1 - P_m >= self.delta:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_106(BaseStrategy):
    """
    Adaptive Pivotal Reciprocity (APR)

    - Attempts to form and sustain coalitions of size >= m.
    - Willingly plays pivotal cooperation when it is likely to tip the round.
    - Temporarily withdraws cooperation when recent cooperation appears exploited,
      and forgives when coalition stability returns.
    - Uses simple reliability estimates from observed history with a small prior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_counter = 0
        self.prior_weight = 1.0
        self.prior_p = 0.5
        self.epsilon = 0.25
        self.reliability_reentry_threshold = 0.7
        self.max_punishment = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = state.round_number
        r_total = self.game_description.n_rounds
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        past_rounds = history.round_number
        if past_rounds == 0:
            return Action.C
        w = min(5, max(1, r_total - 1))
        opp_actions = np.array(history.opponent_actions)
        my_actions = np.array(history.my_actions)
        my_payoffs = np.array(history.my_payoffs)
        H_j = np.sum(opp_actions, axis=0) if opp_actions.size else np.zeros(n_players - 1)
        denom = float(past_rounds + self.prior_weight)
        reliability = (H_j + self.prior_weight * self.prior_p) / denom
        expected_others_coop = float(np.sum(reliability))
        recent_k = min(w, past_rounds)
        recent_slice = slice(max(0, past_rounds - recent_k), past_rounds)
        recent_opp = opp_actions[recent_slice, :] if opp_actions.size else np.zeros((0, max(0, n_players - 1)))
        recent_my_actions = my_actions[recent_slice] if my_actions.size else np.zeros(0, dtype=bool)
        recent_my_payoffs = my_payoffs[recent_slice] if my_payoffs.size else np.zeros(0)
        recent_total_coop = []
        for rr in range(recent_k):
            idx = recent_slice.start + rr
            opp_count = int(np.sum(opp_actions[idx, :])) if opp_actions.size else 0
            my_count = 1 if bool(my_actions[idx]) else 0
            recent_total_coop.append(opp_count + my_count)
        recent_total_coop = np.array(recent_total_coop, dtype=int) if recent_total_coop else np.array([], dtype=int)
        recent_successes = int(np.sum(recent_total_coop >= m)) if recent_total_coop.size else 0
        my_recent_coop_rounds = int(np.sum(recent_my_actions)) if recent_my_actions.size else 0
        my_recent_payoff_when_C = None
        my_recent_payoff_when_D = None
        if recent_my_actions.size:
            mask_C = recent_my_actions
            mask_D = ~recent_my_actions
            if np.any(mask_C):
                vals = recent_my_payoffs[mask_C]
                my_recent_payoff_when_C = float(np.mean(vals)) if vals.size else None
            if np.any(mask_D):
                vals = recent_my_payoffs[mask_D]
                my_recent_payoff_when_D = float(np.mean(vals)) if vals.size else None
        last_round_total = None
        if past_rounds >= 1:
            last_idx = past_rounds - 1
            last_opp = int(np.sum(opp_actions[last_idx, :])) if opp_actions.size else 0
            last_my = 1 if bool(my_actions[last_idx]) else 0
            last_round_total = last_opp + last_my
        if t == r_total:
            if expected_others_coop >= m:
                return Action.D
            if expected_others_coop <= m - 2:
                return Action.D
            return Action.C
        if my_recent_coop_rounds >= 1 and my_recent_payoff_when_C is not None and (my_recent_payoff_when_D is not None) and (my_recent_payoff_when_C < my_recent_payoff_when_D) and (self.punishment_counter == 0):
            remaining_rounds = max(0, r_total - t)
            self.punishment_counter = min(self.max_punishment, remaining_rounds)
        if self.punishment_counter > 0:
            if recent_successes >= 2 or (last_round_total is not None and last_round_total >= m):
                self.punishment_counter = 0
            else:
                self.punishment_counter -= 1
                return Action.D
        if last_round_total is not None and last_round_total >= m:
            return Action.C
        if expected_others_coop >= m:
            return Action.D
        if expected_others_coop <= m - 2:
            return Action.D
        if expected_others_coop >= m - 1 - self.epsilon:
            R = np.array(reliability, dtype=float)
            if R.size > 0:
                R_sorted = -np.sort(-R)
            else:
                R_sorted = np.array([])
            top_k = max(0, m - 1)
            top_rels = R_sorted[:top_k] if top_k > 0 else np.array([])
            count_strong = int(np.sum(top_rels > 0.5)) if top_rels.size else 0
            needed = max(0, m - 2)
            if count_strong >= needed:
                return Action.C
            else:
                return Action.D
        strong_others = int(np.sum(reliability >= self.reliability_reentry_threshold))
        if self.punishment_counter == 0 and strong_others >= max(0, m - 1):
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_107(BaseStrategy):
    """
    Adaptive Pivotal Cooperation (APC)

    - Starts cooperatively to signal intent.
    - Estimates opponents' cooperation propensities with Laplace smoothing.
    - Cooperates when likely pivotal, supports cooperation when group likely meets threshold,
      defects when group is far from threshold but occasionally experiments.
    - Punishes briefly when repeatedly exploited, then forgives.
    - In the final round, cooperates only when your cooperation is plausibly pivotal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.P = 2
        self.q = 0.1
        self.e = 0.1
        self.phi = 0.6
        self.delta = 1
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        t = getattr(state, 'round_number', None)
        if t is None:
            t = history.round_number + 1 if history is not None else 1
        if history is None:
            return Action.C
        rounds_observed = history.round_number
        if rounds_observed > 0:
            opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
            if opp_actions.ndim == 1:
                opp_actions = opp_actions.reshape((rounds_observed, -1))
            coop_counts = np.sum(opp_actions, axis=0)
        else:
            coop_counts = np.zeros((n - 1,), dtype=int)
        denom = rounds_observed + 2.0 * self.alpha
        if denom <= 0:
            denom = 1.0
        p_js = [(int(coop_counts[j]) + self.alpha) / denom for j in range(max(0, n - 1))]
        expected_others = float(sum(p_js))
        expected_with_me = expected_others + 1.0
        W = min(10, rounds_observed)
        window_start = max(0, rounds_observed - W)
        recent_success_count = 0
        exploited_count = 0
        if W > 0:
            my_actions_arr = np.asarray(history.my_actions, dtype=np.bool_)
            opp_arr = np.asarray(history.opponent_actions, dtype=np.bool_)
            if opp_arr.ndim == 1:
                opp_arr = opp_arr.reshape((rounds_observed, -1))
            total_coop_per_round = my_actions_arr.astype(int) + np.sum(opp_arr, axis=1)
            recent_rounds = range(window_start, rounds_observed)
            for rr in recent_rounds:
                if int(total_coop_per_round[rr]) >= m:
                    recent_success_count += 1
                if my_actions_arr[rr] and int(total_coop_per_round[rr]) < m:
                    exploited_count += 1
            recent_success_rate = recent_success_count / float(W)
        else:
            recent_success_rate = 0.0
            exploited_count = 0
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if exploited_count >= 2:
            self.punish_counter = max(0, self.P - 1)
            return Action.D
        if t >= r:
            if expected_others < m and expected_with_me >= m:
                return Action.C
            else:
                return Action.D
        if expected_with_me >= m and expected_others < m:
            return Action.C
        if expected_others >= m:
            if random.random() < 1.0 - self.q:
                return Action.C
            else:
                return Action.D
        if expected_others >= m - 1:
            return Action.C
        if expected_others >= m - 2:
            base_prob = 0.5
            if recent_success_rate >= self.phi:
                base_prob = min(0.9, base_prob + 0.2)
            if random.random() < base_prob:
                return Action.C
            else:
                return Action.D
        if random.random() < self.e:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_108(BaseStrategy):
    """
    Adaptive Pivotal Cooperate (APC)

    Cooperate when your cooperation is likely to be pivotal (it flips the group over the threshold),
    or when a clear cooperative norm exists; otherwise defect. Punish short-lived exploitation but
    forgive and rejoin cooperation when others show a reliable pattern of cooperating. Always defect
    in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.7
        self.p0 = 0.5
        self.W = min(10, self.r)
        self.coop_norm_threshold = 0.8
        self.punish_remain = 0

    def _poisson_binomial_pmf(self, p_list, target):
        """
        Exact Poisson-Binomial PMF for sum of independent Bernoulli(p_list) equaling target.
        Dynamic programming convolution.
        """
        n = len(p_list)
        probs = [0.0] * (n + 1)
        probs[0] = 1.0
        for p in p_list:
            for k in range(n - 1, -1, -1):
                prob_k = probs[k]
                probs[k + 1] += prob_k * p
                probs[k] = prob_k * (1.0 - p)
        if 0 <= target <= n:
            return probs[target]
        return 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        m = self.m
        k = self.k
        alpha = self.alpha
        W = self.W
        if history is None:
            if getattr(state, 'round_number', 1) >= r:
                return Action.D
            n_opponents = n - 1
            if n_opponents <= 0:
                return Action.D
            p_list = [self.p0] * n_opponents
            target = m - 1
            P_piv = self._poisson_binomial_pmf(p_list, target)
            if P_piv >= 1.0 / k:
                return Action.C
            return Action.D
        completed = history.round_number
        current_round = getattr(state, 'round_number', completed + 1)
        if completed >= 1:
            last_my_action = bool(history.my_actions[-1])
            last_opponent_row = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=bool)
            last_opponent_coops = int(np.sum(last_opponent_row)) if last_opponent_row.size else 0
            last_total_coops = last_opponent_coops + (1 if last_my_action else 0)
            if last_my_action and last_total_coops < m:
                remaining_rounds = max(0, r - completed)
                P = min(3, max(1, math.floor(remaining_rounds / 6))) if remaining_rounds >= 1 else 1
                self.punish_remain = P
        if current_round >= r:
            return Action.D
        if self.punish_remain > 0:
            self.punish_remain -= 1
            return Action.D
        n_opponents = n - 1
        p_list = []
        for j in range(n_opponents):
            if history.opponent_actions.size == 0:
                R_j = self.p0
            else:
                start_idx = max(0, completed - W)
                slice_arr = history.opponent_actions[start_idx:completed, j] if completed > 0 else np.array([], dtype=bool)
                if slice_arr.size == 0:
                    R_j = self.p0
                else:
                    R_j = float(np.mean(slice_arr))
            if completed >= 1:
                last_act = bool(history.opponent_actions[-1, j])
                last_indicator = 1.0 if last_act else 0.0
            else:
                last_indicator = 0.0
            p_j = alpha * last_indicator + (1.0 - alpha) * R_j
            p_list.append(p_j)
        target = m - 1
        P_piv = 0.0
        if 0 <= target <= len(p_list):
            P_piv = self._poisson_binomial_pmf(p_list, target)
        if completed == 0:
            recent_group_coop_rate = 0.0
            last_round_cooperators = 0
        else:
            start_idx = max(0, completed - W)
            opp_window = history.opponent_actions[start_idx:completed, :] if history.opponent_actions.size else np.zeros((0, n_opponents), dtype=bool)
            opp_coops = int(np.sum(opp_window)) if opp_window.size else 0
            my_window = history.my_actions[start_idx:completed] if history.my_actions.size else np.array([], dtype=bool)
            my_coops = int(np.sum(my_window)) if my_window.size else 0
            total_coops_window = opp_coops + my_coops
            denom = n * (completed - start_idx) if completed - start_idx > 0 else 1
            recent_group_coop_rate = float(total_coops_window) / float(denom)
            last_opponent_row = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=bool)
            last_opponent_coops = int(np.sum(last_opponent_row)) if last_opponent_row.size else 0
            last_my_action = int(bool(history.my_actions[-1])) if history.my_actions.size else 0
            last_round_cooperators = last_opponent_coops + last_my_action
        if P_piv >= 1.0 / k:
            return Action.C
        if completed >= 1 and last_round_cooperators >= m and (recent_group_coop_rate >= self.coop_norm_threshold):
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_109(BaseStrategy):
    """
    Adaptive threshold-forming strategy for the Collective Risk Dilemma.

    - Tracks per-opponent cooperation probabilities p_j with exponential smoothing.
    - Computes expected payoffs of cooperating vs defecting (using independent Bernoulli model for opponents)
      and cooperates when cooperating meaningfully increases the chance to meet the threshold (with a small
      pro-cooperation bias beta).
    - Small exploration probability epsilon to re-seed cooperation.
    - Short, forgiving punishment (punish_until) after being exploited while cooperating.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.2
        self.epsilon = 0.03
        self.beta = 0.01
        self.T_p = 3
        self.p0 = 0.5
        if self.n >= 2:
            self.p_j = np.full((self.n - 1,), self.p0, dtype=float)
        else:
            self.p_j = np.array([], dtype=float)
        self.punish_until = 0
        self.last_processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1
        if completed > self.last_processed_rounds:
            for idx in range(self.last_processed_rounds, completed):
                my_act = bool(history.my_actions[idx])
                opp_row = history.opponent_actions[idx, :]
                opp_float = np.array(opp_row, dtype=float)
                self.p_j = (1.0 - self.alpha) * self.p_j + self.alpha * opp_float
                total_cooperators = int(np.sum(opp_float) + (1 if my_act else 0))
                if my_act and total_cooperators < self.m:
                    defections_by_others = int(self.n - 1 - np.sum(opp_float))
                    half_thresh = max(1, math.floor((self.n - 1) / 2))
                    missing_needed = max(0, self.m - 1)
                    if defections_by_others >= half_thresh or defections_by_others >= missing_needed:
                        self.punish_until = idx + 1 + self.T_p
            self.last_processed_rounds = completed
        if t == self.r:
            return Action.D
        if t <= self.punish_until:
            if random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        if random.random() < self.epsilon:
            return Action.C
        num_others = self.n - 1
        if num_others == 0:
            U_C = self.k if self.m <= 1 else 0.0
            U_D = 1.0
            if U_C >= U_D - self.beta:
                return Action.C
            else:
                return Action.D
        q = np.zeros(num_others + 1, dtype=float)
        q[0] = 1.0
        for pj in self.p_j:
            prev = q.copy()
            q[0] = prev[0] * (1.0 - pj)
            for x in range(1, num_others + 1):
                q[x] = prev[x] * (1.0 - pj) + prev[x - 1] * pj
        if self.m <= 0:
            P_m = 1.0
        else:
            lower = max(self.m, 0)
            if lower > num_others:
                P_m = 0.0
            else:
                P_m = float(np.sum(q[lower:num_others + 1]))
        if self.m - 1 <= 0:
            P_m_minus1 = 1.0
        else:
            lower2 = max(self.m - 1, 0)
            if lower2 > num_others:
                P_m_minus1 = 0.0
            else:
                P_m_minus1 = float(np.sum(q[lower2:num_others + 1]))
        U_D = 1.0 + self.k * P_m
        U_C = 0.0 + self.k * P_m_minus1
        if U_C >= U_D - self.beta:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_110(BaseStrategy):
    """
    Adaptive Threshold Conditional Cooperator (ATCC)

    - Seeds cooperation in early rounds to try to establish cooperative norms.
    - Uses Laplace-smoothed empirical distribution over observed total cooperators
      to estimate the probability that exactly m-1 others will cooperate (the
      pivotal event). Cooperates when k * P_exact_others >= 2 (derived from payoff
      comparison), or when recent group success is stable.
    - Implements short punishments when exploited repeatedly, with a forgiving decay.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_coops_history = []
        self.punishment_timer = 0.0
        self.times_exploited_count = 0.0
        self.consecutive_failure_count = 0
        self.last_updated_round = 0
        r = self.game_description.n_rounds
        self.W = min(10, max(1, r))
        self.T_init = min(3, max(1, math.floor(0.1 * r)))
        self.P_len = 2
        self.keep_coop_threshold = 0.6
        self.high_coop_threshold_for_last_round = 0.9
        self.exploit_threshold = 2.0
        self.epsilon = 1e-09

    def _sync_from_history(self, history: PlayerHistory):
        """
        Process any new completed rounds in history and update internal counters.
        This does not attempt to predict the current round; it only ingests past rounds.
        """
        n = self.game_description.n_players
        m = self.game_description.m
        total_rounds_done = history.round_number
        for idx in range(self.last_updated_round, total_rounds_done):
            opp_coops = int(np.sum(history.opponent_actions[idx, :]))
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            total_coops = opp_coops + my_coop
            self.total_coops_history.append(int(total_coops))
            if total_coops < m:
                self.consecutive_failure_count += 1
            else:
                self.consecutive_failure_count = 0
            if my_coop == 1 and total_coops < m:
                self.times_exploited_count += 1.0
            else:
                self.times_exploited_count = max(0.0, self.times_exploited_count - 0.5)
            if self.times_exploited_count >= self.exploit_threshold:
                self.punishment_timer = float(self.P_len)
                self.times_exploited_count = 0.0
            self.last_updated_round += 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            self.total_coops_history = []
            self.punishment_timer = 0.0
            self.times_exploited_count = 0.0
            self.consecutive_failure_count = 0
            self.last_updated_round = 0
            return Action.C
        self._sync_from_history(history)
        t = state.round_number
        hist_len = len(self.total_coops_history)
        if hist_len == 0:
            recent_success_fraction = 0.0
        else:
            window = min(self.W, hist_len)
            recent_slice = self.total_coops_history[-window:]
            recent_success_fraction = float(sum((1 for x in recent_slice if x >= m))) / float(window)
        if self.punishment_timer > 0.0:
            self.punishment_timer = max(0.0, self.punishment_timer - 1.0)
            return Action.D
        if t == r:
            if recent_success_fraction >= self.high_coop_threshold_for_last_round:
                return Action.C
            else:
                return Action.D
        if t <= self.T_init:
            if t == 1:
                return Action.C
            elif recent_success_fraction == 0.0 and self.times_exploited_count >= 2.0:
                return Action.D
            else:
                return Action.C
        freq = np.ones(n + 1, dtype=float)
        for val in self.total_coops_history:
            if 0 <= val <= n:
                freq[int(val)] += 1.0
        total_freq = float(np.sum(freq))
        P_total = freq / total_freq
        idx = max(0, min(n, m - 1))
        P_exact_others = float(P_total[idx])
        if recent_success_fraction >= self.keep_coop_threshold:
            return Action.C
        if k * P_exact_others >= 2.0 - self.epsilon:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_111(BaseStrategy):
    """
    Pivotality-Based Conditional Cooperation (PBCC) with Exploration & Forgiveness.

    Cooperate when the estimated probability that exactly m-1 other players cooperate
    (i.e. your cooperation is pivotal) exceeds 1/k. Estimates are formed from the
    observed distribution of other-cooperator counts with Laplace smoothing (alpha).
    Adds front-loaded, linearly decaying exploration (eps0) and a small forgiveness
    stochastic boost when recent full-group successes exist and the pivotal estimate
    is just below threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.eps0 = 0.2
        self.forgive_window = 3
        self.delta_f = 0.02
        self.p_forgive = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None:
            T = 0
            t = 1
        else:
            T = int(history.round_number)
            t = T + 1
        if T == 0:
            P_pivotal = (0.0 + self.alpha) / (0.0 + self.alpha * float(n))
        else:
            others_counts = np.sum(history.opponent_actions, axis=1).astype(int)
            counts_count = np.bincount(others_counts, minlength=n).astype(float)
            P_pivotal = (counts_count[m - 1] + self.alpha) / (float(T) + self.alpha * float(n))
        threshold = 1.0 / k if k != 0.0 else float('inf')
        denom = max(1, r - 1)
        eps_t = self.eps0 * max(0.0, 1.0 - float(t - 1) / float(denom))
        if random.random() < eps_t:
            return Action.C
        recent_success = False
        counts_count_recent = 0
        if T > 0:
            others_counts = np.sum(history.opponent_actions, axis=1).astype(int)
            counts_count = np.bincount(others_counts, minlength=n).astype(int)
            start_idx = max(0, T - self.forgive_window)
            recent_success = False
            for idx in range(start_idx, T):
                my_act = 1 if bool(history.my_actions[idx]) else 0
                opps = int(np.sum(history.opponent_actions[idx, :]))
                total_coops = my_act + opps
                if total_coops >= m:
                    recent_success = True
                    break
            if start_idx < T:
                window_counts = others_counts[start_idx:T]
                counts_count_recent = int(np.sum(window_counts == m - 1))
            else:
                counts_count_recent = 0
        else:
            counts_count = np.zeros(n, dtype=int)
        if recent_success and counts_count_recent > 0:
            if threshold - self.delta_f <= P_pivotal < threshold:
                if random.random() < self.p_forgive:
                    return Action.C
                else:
                    return Action.D
        if P_pivotal > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_112(BaseStrategy):
    """
    Conditional Threshold Reciprocity (CTR)

    - Starts by cooperating to signal willingness.
    - Predicts others' actions primarily from the last round.
    - Cooperates when pivotal (predicted others == m-1) or when group success is recently reliable.
    - Defects safely when predicted others exceed threshold by a safety buffer.
    - Punishes brief runs of clear group failure (up to L rounds) if some opponents persistently defected in those failures.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.L = 2
        self.S = 0.6
        self.b = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1
        if t == r:
            return Action.D
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        if completed >= 1:
            last_round_opp = opp_actions[-1, :]
            pred_others = int(np.sum(last_round_opp))
        else:
            pred_others = 0
        if completed >= 1:
            coop_counts = np.sum(opp_actions, axis=1).astype(int) + my_actions.astype(int)
            success_count = int(np.sum(coop_counts >= m))
            recent_success_rate = success_count / float(max(1, completed))
        else:
            recent_success_rate = 0.0
            success_count = 0
        L = self.L
        start_idx = max(0, completed - L)
        end_idx = completed
        recent_fail_indices = []
        if completed >= 1:
            if completed >= 1:
                pass
            window_coop_counts = coop_counts[start_idx:end_idx]
            for offset, val in enumerate(window_coop_counts):
                if int(val) < m:
                    recent_fail_indices.append(start_idx + offset)
        recent_failures = len(recent_fail_indices)
        persistent_defector_exists = False
        if recent_failures >= 1:
            rows = recent_fail_indices
            sub = opp_actions[rows, :]
            always_defected = np.all(sub == False, axis=0)
            if np.any(always_defected):
                persistent_defector_exists = True
        action = None
        if pred_others >= m + self.b:
            action = Action.D
        elif pred_others == m - 1:
            action = Action.C
        elif pred_others == m:
            if recent_success_rate < self.S:
                action = Action.D
            else:
                action = Action.C
        elif recent_success_rate >= self.S:
            action = Action.C
        else:
            action = Action.D
        if recent_failures >= 1 and persistent_defector_exists:
            return Action.D
        return action

class Strategy_COOPERATIVE_113(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR).

    - Tries to achieve the threshold `m` each round while avoiding repeated exploitation.
    - Estimates per-player cooperation probabilities from history (recency-weighted).
    - Cooperates when pivotal (turns failure into success) or when fairness/rotation indicates it's
      this player's turn to contribute. Defects in final round. Uses small exploration probability.
    - Seeds a deterministic contributor rank for the first round (since the framework does not provide
      an explicit player index). This rank is sampled once on initialization and remains fixed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(10, max(1, self.r))
        self.alpha = 0.7
        self.p0 = float(self.m) / max(1, self.n)
        self.eps = 0.02
        self.tau_defect = 0.1
        self.delta_fair = 0.05
        self.delta_payoff = 0.01
        self._rank = random.randint(1, max(1, self.n))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t >= self.r:
            return Action.D
        if t == 1:
            if self._rank <= self.m:
                return Action.C
            else:
                return Action.D
        if history is None:
            return Action.D

        def clamp(x, a, b):
            return a if x < a else b if x > b else x
        n_others = max(0, self.n - 1)
        my_past = int(np.sum(history.my_actions).item()) if history.my_actions.size > 0 else 0
        rounds_completed = max(0, history.round_number)
        if history.opponent_actions.size == 0:
            opp_counts = np.zeros(n_others, dtype=int)
        else:
            opp_counts = np.sum(history.opponent_actions, axis=0).astype(int)
        T = min(self.W, rounds_completed)
        P_list = []
        for j in range(n_others):
            if T <= 0:
                short_rate = self.p0
            else:
                recent = history.opponent_actions[-T:, j]
                short_rate = float(np.sum(recent)) / float(T)
            if rounds_completed <= 0:
                long_rate = self.p0
            else:
                long_rate = float(opp_counts[j]) / float(rounds_completed)
            Pj = self.alpha * short_rate + (1.0 - self.alpha) * long_rate
            if long_rate < self.tau_defect:
                Pj = min(Pj, self.tau_defect)
            Pj = clamp(float(Pj), 0.0, 1.0)
            P_list.append(Pj)
        dp = [1.0] + [0.0] * n_others
        for p in P_list:
            for k in range(len(dp) - 1, -1, -1):
                no_coop = dp[k] * (1.0 - p)
                yes_coop = dp[k - 1] * p if k - 1 >= 0 else 0.0
                dp[k] = no_coop + yes_coop
        if self.m <= 0:
            prob_others_ge_m = 1.0
        elif self.m > n_others:
            prob_others_ge_m = 0.0
        else:
            prob_others_ge_m = sum((dp[k] for k in range(self.m, n_others + 1)))
        m_minus_1 = self.m - 1
        if m_minus_1 <= 0:
            prob_others_ge_m_minus1 = 1.0
        elif m_minus_1 > n_others:
            prob_others_ge_m_minus1 = 0.0
        else:
            prob_others_ge_m_minus1 = sum((dp[k] for k in range(m_minus_1, n_others + 1)))
        E_payoff_C = prob_others_ge_m_minus1 * float(self.k)
        E_payoff_D = prob_others_ge_m * (1.0 + float(self.k)) + (1.0 - prob_others_ge_m) * 1.0
        ideal_share = float(self.m) * float(self.r) / float(max(1, self.n))
        deficit = ideal_share - float(my_past)
        fairness_bonus = self.delta_fair * clamp(deficit / (1.0 + ideal_share), -1.0, 1.0)
        E_payoff_C += fairness_bonus
        action = None
        if E_payoff_C + self.delta_payoff < E_payoff_D:
            action = Action.D
        elif E_payoff_C > E_payoff_D + self.delta_payoff:
            action = Action.C
        elif deficit > 0:
            action = Action.C
        else:
            action = Action.D
        if random.random() < self.eps:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_COOPERATIVE_114(BaseStrategy):
    """
    Pivotal Support with Adaptive Withdrawal (PSAW)

    Deterministic, history-dependent cooperative strategy for the Collective Risk Dilemma.
    - Signals cooperation on the first round.
    - Defects in the final round (endgame-safe).
    - Tries to form/maintain minimal m-coalitions:
        * If previous round had exactly m cooperators and I was among them, keep cooperating.
        * If previous round had > m cooperators, defect (not pivotal).
    - When previous round failed to reach m:
        * If I've cooperated in the last round, remain cooperative (stickiness).
        * Otherwise use a deterministic slot_rule to decide whether to "step up".
    - Withdraws (defects) after `patience` consecutive failed rounds to avoid exploitation,
      and resumes when a successful round (>= m cooperators) is observed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.patience = int(max(2, math.ceil(self.game_description.n_rounds / 10)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        t = history.round_number + 1
        last_idx = history.round_number - 1
        if t == r:
            return Action.D

        def coop_count_at(round_idx: int) -> int:
            my_coop = 1 if bool(history.my_actions[round_idx]) else 0
            opp_coops = int(np.sum(history.opponent_actions[round_idx, :])) if history.opponent_actions.size else 0
            return my_coop + opp_coops
        last_count = coop_count_at(last_idx)
        consec_failures = 0
        for idx in range(last_idx, -1, -1):
            if coop_count_at(idx) < m:
                consec_failures += 1
            else:
                break
        my_last_action = bool(history.my_actions[last_idx])
        if last_count >= m:
            if last_count > m:
                return Action.D
            elif my_last_action:
                return Action.C
            else:
                return Action.D
        if consec_failures >= self.patience:
            return Action.D
        if my_last_action:
            return Action.C
        player_index = getattr(self.game_description, 'player_index', None)
        if player_index is None:
            try:
                opp_count = history.opponent_actions.shape[1]
                inferred = None
                if n - 1 == opp_count:
                    inferred = 0
                player_index = inferred if inferred is not None else 0
            except Exception:
                player_index = 0
        try:
            player_index_int = int(player_index)
        except Exception:
            player_index_int = 0
        my_slot = (player_index_int + t) % n
        needed = m - last_count
        if my_slot < needed:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_115(BaseStrategy):
    """
    Adaptive Threshold Cooperator (ATC)

    - Opens with cooperation.
    - Maintains exponentially-weighted estimates s_j of each opponent's cooperation propensity (initialized 0.5).
    - Tracks my recent cooperation frequency f_i (EWMA).
    - Cooperates when plausibly pivotal (E < m and E + 1 >= m).
    - Defects when my single cooperation is very unlikely to reach threshold (E + 1 < m).
    - When others already suffice (E >= m) mostly defects but shares burden so long-run share ≈ m/n.
    - Targets short punishments (P rounds) at opponents who had high prior propensity but defected and caused failures.
    - In final round, cooperate only if pivotal or to correct a fairness shortfall.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.lambda_ = 0.25
        self.P = 2
        self.eps_min = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        fair_share = float(m) / float(n)
        lam = self.lambda_
        P = self.P
        eps_min = self.eps_min
        if history is None:
            return Action.C
        completed = history.round_number
        n_opponents = n - 1
        s = [0.5] * n_opponents
        f_i = 0.0
        punish_timers = {}
        s_prev_last = None

        def decrement_timers():
            to_delete = []
            for j in list(punish_timers.keys()):
                punish_timers[j] -= 1
                if punish_timers[j] <= 0:
                    to_delete.append(j)
            for j in to_delete:
                punish_timers.pop(j, None)
        for r_idx in range(completed):
            s_prev = s.copy()
            s_prev_last = s_prev
            my_a = bool(history.my_actions[r_idx])
            opp_row = history.opponent_actions[r_idx, :]
            cooperators_last = int(np.sum(opp_row)) + (1 if my_a else 0)
            if cooperators_last < m:
                candidates = [j for j in range(n_opponents) if s_prev[j] > 0.6 and (not bool(opp_row[j]))]
                for j in candidates:
                    punish_timers[j] = P
            for j in range(n_opponents):
                observed = 1.0 if bool(opp_row[j]) else 0.0
                s[j] = (1.0 - lam) * s[j] + lam * observed
            f_i = (1.0 - lam) * f_i + lam * (1.0 if my_a else 0.0)
            decrement_timers()
        E = float(np.sum(s))
        last_round_index = completed - 1
        last_round_opp_actions = None
        last_round_cooperators = None
        if completed > 0:
            last_row = history.opponent_actions[last_round_index, :]
            last_my = bool(history.my_actions[last_round_index])
            last_round_opp_actions = last_row
            last_round_cooperators = int(np.sum(last_row)) + (1 if last_my else 0)

        def clamp(x, a, b):
            return max(a, min(b, x))
        if state.round_number == r:
            if E + 1.0 >= m and E < m:
                return Action.C
            if f_i < fair_share - 0.05:
                return Action.C
            return Action.D
        if last_round_opp_actions is not None:
            punished_defectors_recent = [j for j in punish_timers.keys() if not bool(last_round_opp_actions[j])]
            if punished_defectors_recent:
                if E + 1.0 < m:
                    return Action.D
        if E + 1.0 < m:
            return Action.D
        if E >= m:
            p_share = clamp(fair_share - f_i + 0.05, eps_min, 0.5)
            if f_i < fair_share - 0.02:
                return Action.C
            else:
                return Action.C if random.random() < p_share else Action.D
        if E < m and E + 1.0 >= m:
            if last_round_cooperators is not None and last_round_cooperators < m and (s_prev_last is not None):
                candidates = [j for j in range(n_opponents) if s_prev_last[j] > 0.6 and (not bool(last_row[j]))]
                for j in candidates:
                    punish_timers[j] = P
                if len(candidates) > 0:
                    if E + 1.0 - float(len(candidates)) < m:
                        return Action.D
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_116(BaseStrategy):
    """
    Smart Conditional Cooperator (SCC)

    Maintains per-opponent cooperation estimates (EWMA) and short-term punishment counters.
    Chooses C or D each round by comparing myopic expected payoff of cooperating vs defecting,
    treating opponents as independent Bernoulli players with probabilities p_j. Break ties
    in favor of cooperation by a small epsilon. Starts with C on the first move.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.5
        self.epsilon = 0.01
        self.punish_threshold = 2
        self.punish_decay_window = 5
        self.p_j = None
        self.punish_count = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_opponents = max(0, self.n - 1)
        if self.p_j is None or len(self.p_j) != n_opponents:
            self.p_j = np.full((n_opponents,), 0.5, dtype=float)
            self.punish_count = np.zeros((n_opponents,), dtype=int)
        if history is None or history.round_number == 0:
            return Action.C
        p = np.full((n_opponents,), 0.5, dtype=float)
        punish = np.zeros((n_opponents,), dtype=int)
        rounds_completed = history.round_number
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        for s in range(rounds_completed):
            row = opp_actions[s, :] if opp_actions.size > 0 else np.array([], dtype=bool)
            if len(row) != n_opponents:
                if len(row) < n_opponents:
                    padded = np.zeros((n_opponents,), dtype=bool)
                    padded[:len(row)] = row
                    row = padded
                else:
                    row = row[:n_opponents]
            my_coop = bool(my_actions[s])
            total_cooperators = int(np.sum(row)) + (1 if my_coop else 0)
            for j in range(n_opponents):
                observed = 1 if row[j] else 0
                p[j] = (1.0 - self.alpha) * p[j] + self.alpha * observed
                if observed == 0:
                    total_minus_j = total_cooperators
                    cooperators_excl_j = int(np.sum(row)) - (1 if row[j] else 0) + (1 if my_coop else 0)
                    if cooperators_excl_j == self.m - 1:
                        punish[j] += 1
                elif punish[j] > 0:
                    punish[j] = max(0, punish[j] - 1)
        for j in range(n_opponents):
            if punish[j] >= self.punish_threshold:
                p[j] = min(p[j], 0.2)
        self.p_j = p.copy()
        self.punish_count = punish.copy()
        prob_mass = np.array([1.0], dtype=float)
        for prob in p:
            prob_mass = np.convolve(prob_mass, np.array([1.0 - prob, prob], dtype=float))
        m_minus_1 = max(0, self.m - 1)
        P_C = float(np.sum(prob_mass[m_minus_1:])) if m_minus_1 <= n_opponents else 0.0
        P_D = float(np.sum(prob_mass[self.m:])) if self.m <= n_opponents else 0.0
        Exp_C = P_C * self.k
        Exp_D = 1.0 + P_D * (self.k - 1.0)
        eps = self.epsilon
        if Exp_C >= Exp_D - eps:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_117(BaseStrategy):
    """
    Pivotal-Selector with Recovery (PSR)

    - Seeds cooperation by cooperating in the first round.
    - In intermediate rounds, maintains a failure counter F_fail which increments
      when the previous round had fewer than m cooperators. After F_rec consecutive
      failed rounds it 'recovers' by cooperating to try to restart cooperation.
    - If the previous round was exactly m-1 cooperators, it cooperates (pivotal).
    - Otherwise it selects the Top m players by recent cooperation frequency over
      a window H (min(H_max, rounds_so_far)) and cooperates only if it is among them.
    - Always defects in the final round.
    Deterministic tie-breaking uses a fixed player-indexing where this player is
    index 0 and opponents follow in the order provided in history.opponent_actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.H_max = min(10, max(1, game_description.n_rounds))
        self.F_rec = 2
        self.F_fail = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if history is None:
            self.F_fail = 0
            return Action.C
        if t == r:
            return Action.D
        last_my = bool(history.my_actions[-1]) if history.my_actions.size > 0 else False
        if history.opponent_actions.size == 0:
            last_opponents_sum = 0
        else:
            last_opponents_sum = int(np.sum(history.opponent_actions[-1, :]))
        s_prev = int(last_my) + last_opponents_sum
        if s_prev >= m:
            self.F_fail = 0
        else:
            self.F_fail += 1
        if self.F_fail >= self.F_rec and t < r:
            return Action.C
        if s_prev == m - 1 and t < r:
            return Action.C
        H = min(self.H_max, max(1, t - 1))
        my_window = history.my_actions[-H:]
        if history.opponent_actions.size == 0:
            opp_window = np.zeros((H, 0), dtype=bool)
        else:
            opp_window = history.opponent_actions[-H:, :]
        my_col = np.asarray(my_window).reshape(H, 1)
        try:
            window_matrix = np.concatenate([my_col, np.asarray(opp_window)], axis=1)
        except Exception:
            return Action.D
        freq = np.mean(window_matrix.astype(float), axis=0)
        indices = np.arange(len(freq))
        order = np.lexsort((indices, -freq))
        top_m_indices = set(order[:m].tolist())
        if 0 in top_m_indices:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_118(BaseStrategy):
    """
    Collective Risk Threshold Strategy.

    - Maintains per-opponent cooperation probability estimates p_j (exponential recency weighting).
    - Maintains finite punishment counters for opponents who defected when the group
      reached the threshold. Punishment reduces effective trust multiplicatively.
    - Cooperates in a round (except last round) if the expected number of other
      cooperators (with punishments applied) is sufficient to make your cooperation
      likely to reach the threshold: E_others >= (m - 1) - delta.
    - Small exploration probability epsilon to probe recovering opponents.
    - Always defects on the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.prior = max(0.0, min(1.0, float(self.m) / float(max(1, self.n))))
        self.lambda_update = 0.3
        self.epsilon = 0.02
        self.delta = 0.2
        default_P = min(3, max(1, math.floor(self.r / 6))) if self.r > 0 else 1
        self.P = default_P
        self.s = 0.5
        self.n_opp = max(0, self.n - 1)
        self.p = np.full(self.n_opp, float(self.prior), dtype=float)
        self.punish_counters = np.zeros(self.n_opp, dtype=int)
        self._processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            current_round = 1
        else:
            current_round = history.round_number + 1
        if current_round >= self.r:
            if history is not None:
                self._process_new_history(history)
            return Action.D
        if history is not None:
            self._process_new_history(history)
        effective_p = np.array(self.p, copy=True)
        punished_mask = self.punish_counters > 0
        if np.any(punished_mask):
            effective_p[punished_mask] = effective_p[punished_mask] * (1.0 - self.s)
        E_others = float(np.sum(effective_p))
        if random.random() < self.epsilon:
            return Action.C
        threshold_needed = float(self.m - 1) - float(self.delta)
        if E_others >= threshold_needed:
            return Action.C
        else:
            return Action.D

    def _process_new_history(self, history: PlayerHistory):
        """
        Process rounds from self._processed_rounds up to history.round_number (exclusive),
        updating p_j and punish counters.
        """
        rounds_available = history.round_number
        if rounds_available <= self._processed_rounds:
            return
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        for round_idx in range(self._processed_rounds, rounds_available):
            if round_idx >= opp_actions.shape[0] or round_idx >= my_actions.shape[0]:
                break
            row = opp_actions[round_idx, :]
            my_a = bool(my_actions[round_idx])
            total_coops = int(np.sum(row)) + (1 if my_a else 0)
            threshold_achieved = total_coops >= self.m
            if threshold_achieved:
                defectors_mask = np.logical_not(row.astype(bool))
                for j in range(self.n_opp):
                    if defectors_mask[j]:
                        self.punish_counters[j] = max(self.punish_counters[j], int(self.P))
            coop_flags = row.astype(float)
            if coop_flags.shape[0] != self.n_opp:
                continue
            self.p = (1.0 - float(self.lambda_update)) * self.p + float(self.lambda_update) * coop_flags
            np.clip(self.p, 0.0, 1.0, out=self.p)
            for j in range(self.n_opp):
                if self.punish_counters[j] > 0:
                    self.punish_counters[j] -= 1
        self._processed_rounds = rounds_available

class Strategy_COOPERATIVE_119(BaseStrategy):
    """
    Conditional Pivotal Reciprocity (CPR)

    - Signals cooperation in round 1.
    - Tracks a punish_until round index to implement short proportional punishments after failures.
    - Cooperates when recent rounds are stable (all had >= m cooperators) or when one is
      plausibly pivotal (others_last >= m-1). In the final round cooperates only if cooperation
      is clearly entrenched (recent_stable or last round met threshold).
    - punish_until and last_processed are internal persistent state.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_until = 0
        self.last_processed = 0
        r = int(self.game_description.n_rounds)
        self.H = min(3, max(1, r - 1))
        self.max_punish = min(3, max(1, math.floor(r / 4)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        completed = int(history.round_number)
        for i in range(self.last_processed, completed):
            round_t = i + 1
            opp_coops = int(np.sum(history.opponent_actions[i, :]))
            my_coop = 1 if bool(history.my_actions[i]) else 0
            total_coops = opp_coops + my_coop
            if total_coops < self.game_description.m:
                shortfall = self.game_description.m - total_coops
                punish_length = min(self.max_punish, max(1, int(shortfall)))
                self.punish_until = max(self.punish_until, round_t + punish_length)
        self.last_processed = max(self.last_processed, completed)
        if t <= self.punish_until:
            return Action.D
        if history.round_number == 0:
            recent_stable = False
            last_total = 0
            we_coop_last = False
        else:
            last_idx = history.round_number - 1
            opp_last = int(np.sum(history.opponent_actions[last_idx, :]))
            we_coop_last = bool(history.my_actions[last_idx])
            last_total = opp_last + (1 if we_coop_last else 0)
            start_round = max(1, t - self.H)
            start_idx = start_round - 1
            recent_indices = range(start_idx, last_idx + 1) if start_idx <= last_idx else []
            recent_stable = True
            for idx in recent_indices:
                opp_coops = int(np.sum(history.opponent_actions[idx, :]))
                my_coop = 1 if bool(history.my_actions[idx]) else 0
                total_coops = opp_coops + my_coop
                if total_coops < self.game_description.m:
                    recent_stable = False
                    break
        others_last = last_total - (1 if we_coop_last else 0)
        pivotal = others_last >= self.game_description.m - 1
        if t == self.game_description.n_rounds:
            if recent_stable or last_total >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        if recent_stable:
            return Action.C
        elif pivotal:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_120(BaseStrategy):
    """
    Adaptive Conditional Cooperation with Forgiveness (ACCF)

    - Uses a Laplace-smoothed estimate of opponents' cooperation rate to compute
      the binomial probability that exactly m-1 others cooperate (pivotal event).
    - Cooperates when k * Prob_exact > 1 (cooperation is expected-payoff improving).
    - Joins recently successful groups, and joins stable subgroups detected over a short window.
    - Uses short punishment (P rounds) after being exploited (you cooperated but group failed).
    - Explores with small probability eps to probe for recovered cooperation.
    - Defects on the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.s = 1
        self.P = 2
        self.eps = 0.03
        self.delta = 0.0
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            t = 1
            rounds_played = 0
        else:
            rounds_played = history.round_number
            t = rounds_played + 1
        if t == r:
            if history is not None and rounds_played >= 1:
                if bool(history.my_actions[-1]):
                    total_coops_last = int(history.opponent_actions[-1, :].sum()) + 1
                    if total_coops_last < m:
                        self.punish_counter = self.P
            return Action.D
        if history is None or rounds_played == 0:
            return Action.C
        if rounds_played >= 1:
            if bool(history.my_actions[-1]):
                total_coops_last = int(history.opponent_actions[-1, :].sum()) + 1
                if total_coops_last < m:
                    self.punish_counter = self.P
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        total_C_others = int(history.opponent_actions.sum())
        denom = (self.s + rounds_played) * (n - 1)
        p_hat = (self.s * (n - 1) + total_C_others) / denom
        try:
            comb = math.comb(n - 1, m - 1)
        except AttributeError:
            comb = int(np.math.factorial(n - 1) // (np.math.factorial(m - 1) * np.math.factorial(n - m)))
        p_hat = max(0.0, min(1.0, float(p_hat)))
        prob_exact = comb * p_hat ** (m - 1) * (1.0 - p_hat) ** (n - m)
        if k * prob_exact > 1.0 + self.delta:
            return Action.C
        last_round_other_C = int(history.opponent_actions[-1, :].sum())
        if last_round_other_C >= max(0, m - 1):
            return Action.C
        recent_window = min(5, rounds_played)
        if recent_window >= 1:
            recent_slice = history.opponent_actions[-recent_window:, :]
            recent_arr = np.array(recent_slice, dtype=float)
            per_opponent_rates = recent_arr.mean(axis=0)
            stable_count = int((per_opponent_rates >= 0.7).sum())
            if stable_count >= max(0, m - 1):
                return Action.C
        if random.random() < self.eps:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_121(BaseStrategy):
    """
    Conditional Pivotal Cooperator (CPC)

    - Starts by cooperating.
    - If the previous round met the threshold, continue cooperating only if
      you were a cooperator in that round (reciprocate); otherwise defect.
    - If the previous round had exactly m-1 cooperators, cooperate (be pivotal).
    - If the previous round had fewer than m-1 cooperators, defect (your lone
      contribution cannot meet m).
    - Implements finite punishment: after PUNISH_TRIGGER consecutive failed
      rounds (S < m), defect for PUNISH_LEN rounds, then forgive and resume.
    - In the final round, only cooperate if you are pivotal (S_prev == m-1)
      or you were part of a previously successful cooperating set (S_prev >= m
      and you cooperated last round).
    """
    PUNISH_TRIGGER = 2
    PUNISH_LEN = 2

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        completed = history.round_number
        t = completed + 1
        if completed == 0:
            return Action.C

        def cooperators_in_round(idx: int) -> int:
            opps = history.opponent_actions[idx, :]
            my = bool(history.my_actions[idx])
            return int(np.sum(opps)) + (1 if my else 0)
        last_idx = completed - 1
        S_prev = cooperators_in_round(last_idx)
        I_prev_coop = bool(history.my_actions[last_idx])
        failure_streak = 0
        for idx in range(last_idx, -1, -1):
            if cooperators_in_round(idx) < m:
                failure_streak += 1
            else:
                break
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if failure_streak >= self.PUNISH_TRIGGER:
            self.punishment_counter = self.PUNISH_LEN
            self.punishment_counter -= 1
            return Action.D
        if t == n_rounds:
            if S_prev == m - 1:
                return Action.C
            if S_prev >= m and I_prev_coop:
                return Action.C
            return Action.D
        if S_prev >= m:
            if I_prev_coop:
                return Action.C
            else:
                return Action.D
        elif S_prev == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_122(BaseStrategy):
    """
    Conditional-Marginalist Reciprocity (CMR)

    At each round build smoothed, recency-weighted beliefs about each opponent's
    probability of cooperating. Compute the probability that exactly m-1 other
    players cooperate; cooperate iff the marginal expected gain k * Pr[S = m-1]
    exceeds the private cost (1), with a small cooperative tie-break bias.
    Apply mild, temporary punishment to persistent defectors (lower their p_j),
    and small early-round nudges toward the optimistic prior to encourage opening
    cooperation. Uses convolution of Bernoulli probabilities to get distribution
    of number of cooperating opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.6
        self.s = 1.0
        self.tau = 0.01
        self.recency_max = 5
        self.punish_threshold = 0.2
        self.punish_factor = 0.5
        self.min_rounds_for_punish = 3
        self.early_nudge_frac = 0.1
        n_opponents = max(0, self.game_description.n_players - 1)
        self.count_C = np.zeros(n_opponents, dtype=int)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            total_observed = 0
        else:
            total_observed = history.round_number
        n = self.game_description.n_players
        m = self.game_description.m
        k_reward = self.game_description.k
        n_opp = n - 1
        if total_observed == 0 or history is None:
            p_list = [self.p0] * n_opp
        else:
            opp_actions = history.opponent_actions
            self.count_C = np.sum(opp_actions, axis=0).astype(int)
            w = min(self.recency_max, total_observed)
            if w <= 0:
                w = total_observed
            if w > 0:
                recent_slice = opp_actions[-w:, :] if w <= opp_actions.shape[0] else opp_actions
                counts_recent = np.sum(recent_slice, axis=0).astype(float)
            else:
                counts_recent = np.zeros(n_opp, dtype=float)
            denom = float(w) + float(self.s)
            p_list = []
            for j in range(n_opp):
                p_j = (counts_recent[j] + self.s * self.p0) / denom
                recent_freq = counts_recent[j] / max(1.0, float(w)) if w > 0 else 0.0
                if recent_freq < self.punish_threshold and total_observed >= self.min_rounds_for_punish:
                    p_j *= self.punish_factor
                if total_observed <= 2 or total_observed <= max(1, int(self.game_description.n_rounds / 5)):
                    p_j = p_j + self.early_nudge_frac * (self.p0 - p_j)
                if p_j < 0.0:
                    p_j = 0.0
                elif p_j > 1.0:
                    p_j = 1.0
                p_list.append(float(p_j))
        if n_opp <= 0:
            return Action.D
        probs = np.array([1.0], dtype=float)
        for p in p_list:
            probs = np.convolve(probs, np.array([1.0 - p, p], dtype=float))
        idx_start_without = max(0, m)
        if idx_start_without <= n_opp:
            P_without = float(np.sum(probs[idx_start_without:]))
        else:
            P_without = 0.0
        idx_start_with = max(0, m - 1)
        if idx_start_with <= n_opp:
            P_with = float(np.sum(probs[idx_start_with:]))
        else:
            P_with = 0.0
        E_defect = 1.0 + k_reward * P_without
        E_coop = 0.0 + k_reward * P_with
        if E_coop >= E_defect - self.tau:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_123(BaseStrategy):
    """
    Collective-risk-pivotal strategy.

    - Uses exponential smoothing to estimate each opponent's cooperation probability p_j.
    - Early-round deterministic rotating probes for T_probe rounds to seed coordination.
    - In regular rounds: computes Q = Pr[sum_{others} = m-1]. Cooperate if Q > 1/k (marginal payoff rule).
    - Also forms a top-m "reliable" group by estimated p_j; if in that group, cooperates when the group's
      probability of reaching the threshold (without me or with me as needed) is high.
    - Small stochastic exploration and modest punishment/forgiveness adjustments.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.2
        self.p0 = 0.5
        self.eps0 = min(0.05, 5.0 / max(1.0, float(self.r)))
        self.T_probe = int(min(max(2, self.n // 2), max(1, self.r // 5)))
        if self.n >= 2:
            self.p = np.full(self.n - 1, self.p0, dtype=float)
        else:
            self.p = np.array([], dtype=float)
        self.punish_counts = np.zeros(max(0, self.n - 1), dtype=int)
        self.my_index = 0
        self._eps_numeric = 1e-12

    def _pmf_bernoulli_sum(self, probs):
        """
        Compute PMF of sum of independent Bernoulli(probs[i]) random variables.
        Returns a numpy array pmf where pmf[s] = Pr[sum = s], s = 0..len(probs).
        Uses dynamic programming convolution.
        """
        n = len(probs)
        if n == 0:
            return np.array([1.0], dtype=float)
        pmf = np.zeros(n + 1, dtype=float)
        pmf[0] = 1.0
        for p in probs:
            p = float(min(max(p, 0.0), 1.0))
            new = np.zeros(len(pmf), dtype=float)
            new[0] = pmf[0] * (1.0 - p)
            for s in range(1, len(pmf)):
                new[s] = pmf[s] * (1.0 - p) + pmf[s - 1] * p
            pmf = new
        pmf = np.maximum(pmf, 0.0)
        sm = pmf.sum()
        if sm > 0:
            pmf = pmf / sm
        return pmf

    def __call__(self, state: GameState, history: None | PlayerHistory):
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            if t <= self.T_probe:
                designated = [(t - 1 + s) % self.n for s in range(self.m)]
                if self.my_index in designated:
                    return Action.C
                else:
                    return Action.D
            else:
                if random.random() < self.eps0:
                    return Action.C
                return Action.D
        rounds_completed = history.round_number
        opp_actions = history.opponent_actions
        last_actions = opp_actions[-1, :] if opp_actions.shape[0] >= 1 else np.array([], dtype=bool)
        for j in range(len(self.p)):
            acted = 1.0 if bool(last_actions[j]) else 0.0
            frac_left = max(0.0, 1.0 - float(rounds_completed) / max(1.0, float(self.r)))
            alpha_eff = self.alpha * (0.5 + 0.5 * frac_left)
            self.p[j] = (1.0 - alpha_eff) * float(self.p[j]) + alpha_eff * acted
            if acted < 0.5 and self.p[j] > 0.7:
                self.punish_counts[j] += 1
                self.p[j] = max(0.0, self.p[j] * 0.7)
            else:
                self.punish_counts[j] = max(0, int(self.punish_counts[j] - 1))
        eps = self.eps0 * max(0.1, 1.0 - float(rounds_completed) / max(1.0, float(self.r)))
        if t <= self.T_probe:
            designated = [(t - 1 + s) % self.n for s in range(self.m)]
            if self.my_index in designated:
                base_action = Action.C
            else:
                base_action = Action.D
            if random.random() < eps:
                return Action.C if base_action == Action.D else Action.D
            return base_action
        probs = list(map(float, list(self.p)))
        pmf = self._pmf_bernoulli_sum(probs)
        idx = self.m - 1
        Q = 0.0
        if 0 <= idx < len(pmf):
            Q = float(pmf[idx])
        marginal_condition = Q > 1.0 / max(self.k, self._eps_numeric)
        if random.random() < eps:
            return Action.D if marginal_condition else Action.C
        if marginal_condition:
            return Action.C
        my_coops = float(np.sum(history.my_actions)) if rounds_completed > 0 else 0.0
        r_me = my_coops / max(1.0, float(rounds_completed))
        combined_scores = []
        for pos in range(self.n):
            if pos == self.my_index:
                combined_scores.append((r_me, ('me', pos)))
            else:
                opp_col = pos - 1 if self.my_index == 0 else pos if pos < self.my_index else pos - 1
                if 0 <= opp_col < len(self.p):
                    combined_scores.append((float(self.p[opp_col]), ('opp', opp_col)))
                else:
                    combined_scores.append((self.p0, ('opp', max(0, min(len(self.p) - 1, 0)))))
        ranked = sorted(enumerate(combined_scores), key=lambda x: -x[1][0])
        top_m_positions = [idx for idx, _ in ranked[:self.m]]
        if self.my_index in top_m_positions:
            probs_in_G_excluding_me = []
            for pos in top_m_positions:
                if pos == self.my_index:
                    continue
                opp_col = pos - 1 if self.my_index == 0 else pos if pos < self.my_index else pos - 1
                if 0 <= opp_col < len(self.p):
                    probs_in_G_excluding_me.append(float(self.p[opp_col]))
                else:
                    probs_in_G_excluding_me.append(float(self.p0))
            pmf_G = self._pmf_bernoulli_sum(probs_in_G_excluding_me)
            need = self.m - 1
            Pr_group = 0.0
            for s in range(len(pmf_G)):
                if s >= need:
                    Pr_group += float(pmf_G[s])
            trust_threshold = 0.7
            if Pr_group > trust_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_124(BaseStrategy):
    """
    Adaptive threshold-aware strategy for the Collective Risk Dilemma.

    - Maintains a Beta-smoothed estimate p of the probability any other single player cooperates.
    - Uses binomial tails on (n-1) opponents to compute P(success) if cooperating vs defecting.
    - Chooses action by comparing expected payoffs, with a small cooperation bias to break close ties.
    - Rescue rule: if in the previous round the other players were >= m-1 cooperators, cooperate to try
      to fill the one-person gap deterministically.
    - Fast forgiveness via small prior strength (s_prior); parameters tunable in __init__.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = max(2, int(self.game_description.n_players))
        m = int(self.game_description.m)
        p0 = (m - 1) / max(1, n - 1)
        self.s_prior = 2.0
        self.alpha = p0 * self.s_prior
        self.beta = (1.0 - p0) * self.s_prior
        self.coop_bias = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None:
            completed_rounds = 0
        else:
            completed_rounds = int(history.round_number)
        t = completed_rounds + 1
        if history is None:
            observed_other_coops = 0
            total_trials = 0
        else:
            observed_other_coops = int(np.sum(history.opponent_actions))
            total_trials = completed_rounds * (n - 1)
        if total_trials == 0:
            p = self.alpha / (self.alpha + self.beta)
        else:
            p = (self.alpha + observed_other_coops) / (self.alpha + self.beta + total_trials)
        if p < 0.0:
            p = 0.0
        elif p > 1.0:
            p = 1.0

        def binom_tail(lower_inclusive: int) -> float:
            result = 0.0
            trials_n = n - 1
            if lower_inclusive <= 0:
                return 1.0
            if lower_inclusive > trials_n:
                return 0.0
            for j in range(lower_inclusive, trials_n + 1):
                coeff = math.comb(trials_n, j)
                term = coeff * p ** j * (1.0 - p) ** (trials_n - j)
                result += term
            if result < 0.0:
                result = 0.0
            elif result > 1.0:
                result = 1.0
            return result
        P_succ_if_C = binom_tail(max(0, m - 1))
        P_succ_if_D = binom_tail(m)
        Epi_C = k * P_succ_if_C
        Epi_D = 1.0 + k * P_succ_if_D
        rescue_trigger = False
        if history is not None and completed_rounds >= 1:
            last_others = int(np.sum(history.opponent_actions[-1, :]))
            if last_others >= m - 1:
                rescue_trigger = True
        if rescue_trigger:
            return Action.C
        if t == int(self.game_description.n_rounds):
            effective_coop_bias = self.coop_bias * 0.5
        else:
            effective_coop_bias = self.coop_bias
        Delta = Epi_C - Epi_D
        if Delta > effective_coop_bias:
            return Action.C
        elif Delta < -effective_coop_bias:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_125(BaseStrategy):
    """
    Threshold-Conditional Conditional-Cooperator (TCCC)

    - Estimates per-opponent cooperation rates over a sliding window and computes
      the distribution of the number of cooperating others to decide whether
      cooperating is expected to be beneficial (or worth a small investment).
    - Starts by signaling cooperation for a few rounds, rejoins recently-successful
      coalitions, punishes persistent defectors briefly, forgives, and explores
      occasionally with probability epsilon.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.L = min(10, self.r)
        self.epsilon = 0.05
        self.invest_tol = 0.15
        self.punish_max = 3
        self.init_coop_rounds = min(3, max(1, self.r - 1))
        self.final_round_buffer = 2
        self.n_opponents = max(0, self.n - 1)
        self.defect_count = [0.0 for _ in range(self.n_opponents)]
        self.punishment_timer = 0
        self.punished_set = set()
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            return Action.C
        t = getattr(state, 'round_number', history.round_number + 1)
        new_rounds_start = self.last_processed_round
        new_rounds_end = history.round_number
        for r_idx in range(new_rounds_start, new_rounds_end):
            my_action = bool(history.my_actions[r_idx])
            opp_row = history.opponent_actions[r_idx, :] if self.n_opponents > 0 else np.array([], dtype=bool)
            total_cooperators = int(opp_row.sum()) + (1 if my_action else 0)
            if my_action and total_cooperators < self.m:
                for j in range(self.n_opponents):
                    if not bool(opp_row[j]):
                        self.defect_count[j] += 1.0
                responsible = {j for j in range(self.n_opponents) if self.defect_count[j] >= 2.0}
                if len(responsible) > 0:
                    self.punishment_timer = min(self.punish_max, 1 + len(responsible))
                    self.punished_set = set(responsible)
            for j in range(self.n_opponents):
                self.defect_count[j] = max(0.0, self.defect_count[j] - 0.1)
        self.last_processed_round = new_rounds_end
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            action = Action.D
            if self.punishment_timer == 0:
                for j in self.punished_set:
                    if 0 <= j < self.n_opponents:
                        self.defect_count[j] = 0.0
                self.punished_set = set()
            return action
        if t <= self.init_coop_rounds:
            return Action.C
        if t > self.r - self.final_round_buffer:
            last_window = min(self.L, max(1, history.round_number))
            recent = history.opponent_actions[-last_window:, :] if self.n_opponents > 0 else np.zeros((last_window, 0), dtype=bool)
            if recent.size == 0:
                p_js = []
            else:
                p_js = list(np.mean(recent.astype(float), axis=0))
            probs = self._poisson_binomial_distribution(p_js)
            prob_exact_mminus1 = probs[self.m - 1] if 0 <= self.m - 1 < len(probs) else 0.0
            prob_ge_m = float(np.sum(probs[self.m:])) if self.m < len(probs) else 0.0
            prob_ge_mminus1 = prob_ge_m + prob_exact_mminus1
            expected_coop = self.k * prob_ge_mminus1
            expected_defect = 1.0 + self.k * prob_ge_m
            if expected_coop >= expected_defect:
                return Action.C
            else:
                return Action.D
        last_window = min(self.L, max(1, history.round_number))
        recent = history.opponent_actions[-last_window:, :] if self.n_opponents > 0 else np.zeros((last_window, 0), dtype=bool)
        if recent.size == 0:
            p_js = []
        else:
            p_js = list(np.mean(recent.astype(float), axis=0))
        probs = self._poisson_binomial_distribution(p_js)
        prob_exact_mminus1 = probs[self.m - 1] if 0 <= self.m - 1 < len(probs) else 0.0
        prob_ge_m = float(np.sum(probs[self.m:])) if self.m < len(probs) else 0.0
        prob_ge_mminus1 = prob_ge_m + prob_exact_mminus1
        expected_coop = self.k * prob_ge_mminus1
        expected_defect = 1.0 + self.k * prob_ge_m
        if history.round_number >= 1:
            last_my_action = bool(history.my_actions[-1])
            last_opp_row = history.opponent_actions[-1, :] if self.n_opponents > 0 else np.array([], dtype=bool)
            last_total_cooperators = int(last_opp_row.sum()) + (1 if last_my_action else 0)
            if last_total_cooperators >= self.m and (not last_my_action):
                return Action.C
        if expected_coop >= expected_defect - self.invest_tol:
            return Action.C
        if random.random() < self.epsilon:
            return Action.C
        else:
            return Action.D

    def _poisson_binomial_distribution(self, p_list):
        """
        Compute distribution of sum of independent Bernoulli(p_j) (heterogeneous)
        Returns a list probs where probs[k] = Prob(sum == k), k = 0..len(p_list)
        DP convolution.
        """
        n = len(p_list)
        if n == 0:
            return [1.0]
        probs = [1.0] + [0.0] * n
        for p in p_list:
            new_probs = [0.0] * len(probs)
            new_probs[0] = probs[0] * (1.0 - p)
            for k in range(1, n + 1):
                v0 = probs[k] * (1.0 - p) if k < len(probs) else 0.0
                v1 = probs[k - 1] * p if k - 1 < len(probs) else 0.0
                new_probs[k] = v0 + v1
            probs = new_probs
        s = sum(probs)
        if s > 0:
            probs = [float(x / s) for x in probs]
        return probs

class Strategy_COOPERATIVE_126(BaseStrategy):
    """
    Conditional-Pivotal Reciprocity (CPR)

    Cooperate iff the estimated probability that exactly m-1 other players will
    cooperate (p_piv) is high enough to justify sacrificing the private endowment.
    Maintains per-opponent exponentially smoothed cooperation probabilities S_j,
    computes the pivot probability by dynamic programming (convolution), and
    randomizes in a small window around the decision threshold to help coordination.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.4
        self.eps = 1e-06
        self.delta = 0.02
        n = self.game_description.n_players
        m = self.game_description.m
        p0 = float(m) / float(n)
        self.n_opponents = max(0, n - 1)
        if self.n_opponents > 0:
            self.S = np.full(self.n_opponents, p0, dtype=float)
        else:
            self.S = np.array([], dtype=float)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = float(self.game_description.k)
        p0 = float(m) / float(n)
        if self.n_opponents > 0:
            S = np.full(self.n_opponents, p0, dtype=float)
        else:
            S = np.array([], dtype=float)
        if history is not None:
            rounds = history.round_number
            for t in range(rounds):
                row = history.opponent_actions[t]
                obs = row.astype(float)
                S = (1.0 - self.alpha) * S + self.alpha * obs
        self.S = S.copy()
        if S.size > 0:
            p_js = np.clip(S, self.eps, 1.0 - self.eps)
        else:
            p_js = np.array([], dtype=float)
        target = m - 1
        if target < 0:
            p_piv = 0.0
        elif p_js.size == 0:
            p_piv = 1.0 if target == 0 else 0.0
        else:
            poly = np.zeros(max(1, target + 1), dtype=float)
            poly[0] = 1.0
            processed = 0
            for p in p_js:
                processed += 1
                upper = min(processed, target)
                for s in range(upper, 0, -1):
                    poly[s] = poly[s] * (1.0 - p) + poly[s - 1] * p
                poly[0] = poly[0] * (1.0 - p)
            p_piv = float(poly[target]) if target < poly.size else 0.0
        threshold = 1.0 / k if k != 0 else float('inf')
        upper = threshold + self.delta
        lower = threshold - self.delta
        if p_piv > upper:
            action = Action.C
        elif p_piv < lower:
            action = Action.D
        else:
            if self.delta <= 0:
                prob_coop = 0.5
            else:
                prob_coop = 0.5 + (p_piv - threshold) / (2.0 * self.delta)
                prob_coop = max(0.0, min(1.0, prob_coop))
            action = Action.C if random.random() < prob_coop else Action.D
        return action

class Strategy_COOPERATIVE_127(BaseStrategy):
    """
    Cautious reciprocity for Collective Risk Dilemma.
    Seeds cooperation, estimates others' cooperation rate to test pivot probability,
    uses short punishments on repeated exploitation, forgives, and always defects in final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_counter = 0
        self.last_fail_rounds_counter = 0
        self.epsilon = 0.03
        self.q_high_margin = 0.05
        r = self.game_description.n_rounds
        self.P_base = max(1, min(4, math.floor(r / 10)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            t = 1
        else:
            t = state.round_number
        if history is not None and history.round_number >= 1:
            last_my_action = bool(history.my_actions[-1])
            other_coops_last = int(np.sum(history.opponent_actions[-1, :]))
            s = history.round_number
            remaining_after_last = max(0, r - s)
            if last_my_action and other_coops_last < m - 1:
                self.last_fail_rounds_counter += 1
                self.punishment_counter = min(self.P_base, remaining_after_last)
                if self.last_fail_rounds_counter >= 3:
                    escalated = min(remaining_after_last, self.punishment_counter * 2)
                    self.punishment_counter = max(self.punishment_counter, escalated)
            elif last_my_action and other_coops_last >= m - 1:
                self.last_fail_rounds_counter = 0
        if t == r:
            return Action.D
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return action
        if history is None or t <= 3:
            return Action.C
        w = min(50, t - 1)
        if history is None:
            sum_other_coops = 0.0
        else:
            opp_slice = history.opponent_actions[-w:, :]
            sum_other_coops = float(np.sum(opp_slice))
        denom = w + 2
        if denom <= 0:
            q_hat = 0.5
        else:
            avg_frac = sum_other_coops / (n - 1) / max(1.0, float(w))
            q_hat = (sum_other_coops / (n - 1) + 1.0) / denom
        nm1 = n - 1
        k_needed = m - 1
        if k_needed < 0 or k_needed > nm1:
            P_eq = 0.0
        else:
            try:
                comb = math.comb(nm1, k_needed)
            except Exception:
                comb = 1
            q = min(max(q_hat, 1e-12), 1 - 1e-12)
            P_eq = comb * q ** k_needed * (1.0 - q) ** (nm1 - k_needed)
        T = k * P_eq
        q_high = (m - 1) / (n - 1)
        generous_condition = q_hat >= q_high - self.q_high_margin
        buffer_rounds = max(2, math.ceil(r * 0.05))
        generous_time_ok = t <= r - buffer_rounds
        if T >= 1.0 + self.epsilon:
            return Action.C
        elif generous_condition and generous_time_ok:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_128(BaseStrategy):
    """
    Pivotal Reciprocity Strategy for the Collective Risk Dilemma.

    Summary:
    - Seeds first round with random cooperation at probability p0 = m/n.
    - In subsequent rounds estimates each opponent's cooperation probability q_j
      over a recent window L (up to 5 rounds).
    - Uses a DP convolution to compute the distribution of how many others
      cooperate, and from that P_without, P_with and deltaP (marginal pivotality).
    - Uses recent success rate S to choose a generosity parameter alpha (lower
      alpha when environment is cooperative, higher when exploitative).
    - Applies endgame scaling to alpha so the agent becomes more conservative
      as the game approaches the last rounds. Always defects in the final round.
    - Cooperates when k * deltaP >= alpha and deltaP > 0, otherwise defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.L_window = 5
        self.S_threshold = 0.6
        self.alpha_gen = 0.4
        self.alpha_self = 1.0
        self.beta = 0.8
        n = getattr(game_description, 'n_players', None)
        m = getattr(game_description, 'm', None)
        if n is not None and m is not None and (n > 0):
            self.p0 = float(m) / float(n)
        else:
            self.p0 = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory):
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        n_players = self.game_description.n_players
        if history is None:
            if random.random() < self.p0:
                return Action.C
            else:
                return Action.D
        completed = history.round_number
        t = state.round_number
        if t is None or t <= 0:
            t = completed + 1
        if t >= r:
            return Action.D
        L = min(self.L_window, max(0, completed))
        if L <= 0:
            if random.random() < self.p0:
                return Action.C
            else:
                return Action.D
        my_recent = history.my_actions[-L:]
        opp_recent = history.opponent_actions[-L:, :]
        n_others = max(0, opp_recent.shape[1])
        if n_others == 0:
            return Action.D
        q_js = []
        for j in range(n_others):
            qj = float(np.sum(opp_recent[:, j])) / float(L)
            q_js.append(qj)
        dp = [0.0] * (n_others + 1)
        dp[0] = 1.0
        for q in q_js:
            next_dp = [0.0] * (n_others + 1)
            for s in range(0, n_others + 1):
                next_dp[s] += dp[s] * (1.0 - q)
                if s + 1 <= n_others:
                    next_dp[s + 1] += dp[s] * q
            dp = next_dp
        P_without = 0.0
        P_with = 0.0
        for s in range(0, n_others + 1):
            prob = dp[s]
            if s >= m:
                P_without += prob
            if s >= m - 1:
                P_with += prob
        deltaP = P_with - P_without
        if deltaP < 0:
            deltaP = 0.0
        successes = 0
        for idx in range(-L, 0):
            my_act = bool(history.my_actions[idx])
            opps = opp_recent[idx, :] if idx >= -L else opp_recent[idx, :]
            total_coops = int(my_act) + int(np.sum(opps))
            if total_coops >= m:
                successes += 1
        S = float(successes) / float(L)
        if S >= self.S_threshold:
            alpha = self.alpha_gen
        else:
            alpha = self.alpha_self
        scale = 1.0 + self.beta * (float(t) / float(r))
        alpha *= scale
        if deltaP <= 0.0:
            return Action.D
        if k * deltaP >= alpha:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_1(BaseStrategy):
    """
    Aggressive pivotal cooperater.

    - Defaults to defecting and only cooperates when the estimated probability
      that exactly m-1 OTHER players cooperate (i.e., you are pivotal) exceeds 1/k (+ small margin).
    - Maintains per-opponent empirical cooperation rates using a sliding window with
      pseudocount smoothing and a conservative prior.
    - Applies harsher downward updating (punishment) for opponents who defected in
      rounds where their cooperation would have been pivotal.
    - Uses a small exploration probability to probe cooperation occasionally, and a
      tiny first-round probe probability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.p0 = 0.2
        self.alpha = 1.0
        self.delta = 0.01
        self.L_max = max(5, math.floor(self.r / 3)) if self.r > 0 else 5
        self.penalty_weight = 2
        self.eps_first = 0.02
        self.eps_cap = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            if random.random() < self.eps_first:
                return Action.C
            return Action.D
        completed = int(history.round_number)
        t = completed + 1
        remaining = max(1, self.r - t + 1)
        eps = min(self.eps_cap, 5.0 / remaining)
        opp_count = history.opponent_actions.shape[1] if history.opponent_actions.size else self.n - 1
        L = min(completed, self.L_max)
        if L <= 0:
            if random.random() < eps:
                return Action.C
            p_j = [self.p0] * opp_count
        else:
            recent_actions = history.opponent_actions[-L:, :]
            coop_counts_per_round = np.sum(recent_actions.astype(np.int64), axis=1)
            p_j = []
            for j in range(opp_count):
                coop_count = int(np.sum(recent_actions[:, j].astype(np.int64)))
                obs_count = L
                penalties = 0
                target_others = self.m - 1
                if 0 <= target_others <= self.n - 1:
                    for r_idx in range(L):
                        own_action = bool(recent_actions[r_idx, j])
                        coop_others = int(coop_counts_per_round[r_idx]) - (1 if own_action else 0)
                        if not own_action and coop_others == target_others:
                            penalties += 1
                if penalties > 0:
                    obs_count += self.penalty_weight * penalties
                p_est = (coop_count + self.alpha * self.p0) / (obs_count + self.alpha)
                p_est = max(0.0, min(1.0, float(p_est)))
                p_j.append(p_est)
        dp = [0.0] * (opp_count + 1)
        dp[0] = 1.0
        for pj in p_j:
            for s in range(opp_count, -1, -1):
                stay = dp[s] * (1.0 - pj)
                add = dp[s - 1] * pj if s - 1 >= 0 else 0.0
                dp[s] = stay + add
        target = self.m - 1
        if 0 <= target <= opp_count:
            p_pivotal = float(dp[target])
        else:
            p_pivotal = 0.0
        if random.random() < eps:
            return Action.C
        threshold = 1.0 / self.k + self.delta
        if p_pivotal > threshold:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_2(BaseStrategy):
    """
    Pivotal Exploiter strategy for the Collective Risk Dilemma.

    - Defects by default and in the final round.
    - Estimates each opponent's cooperation probability with exponential smoothing.
    - Computes probability that others will reach the threshold with/without this player
      (Poisson-binomial exact via convolution, with normal approximation for large opponent counts).
    - Cooperates only when the expected-payoff comparison (E_C > E_D + delta) justifies it;
      tie-breaks favor defection.
    - If ever cooperated and the round still failed (you were a "sucker"), enters a short grudge
      period of S_grudge rounds in which it always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.p0 = 0.1
        self.lmbda = 0.25
        self.delta = 0.01
        self.S_grudge = max(1, math.floor(self.r / 10))
        self.n_opponents = max(0, self.n - 1)
        self.p_js = np.full(self.n_opponents, float(self.p0), dtype=float)
        self.grudge = 0
        self.processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', None)
        if t is None:
            t = history.round_number + 1 if history is not None else 1
        if history is not None:
            total_completed = history.round_number
            for i in range(self.processed_rounds, total_completed):
                if self.grudge > 0:
                    self.grudge = max(0, self.grudge - 1)
                if self.n_opponents > 0:
                    opp_row = history.opponent_actions[i]
                    coop_mask = np.array(opp_row, dtype=float)
                    self.p_js = (1.0 - self.lmbda) * self.p_js + self.lmbda * coop_mask
                my_coop = bool(history.my_actions[i])
                opp_coops_count = int(np.sum(history.opponent_actions[i])) if self.n_opponents > 0 else 0
                total_coops = opp_coops_count + (1 if my_coop else 0)
                if my_coop and total_coops < self.m:
                    self.grudge = int(self.S_grudge)
            self.processed_rounds = total_completed
        if t == self.r:
            if self.grudge > 0:
                self.grudge = max(0, self.grudge - 1)
            return Action.D
        if self.grudge > 0:
            self.grudge = max(0, self.grudge - 1)
            return Action.D

        def pb_pmf(p_list):
            n = len(p_list)
            if n == 0:
                return np.array([1.0], dtype=float)
            if n <= 200:
                probs = np.zeros(n + 1, dtype=float)
                probs[0] = 1.0
                for p in p_list:
                    new = np.zeros_like(probs)
                    new[0] = probs[0] * (1.0 - p)
                    for k in range(1, len(probs)):
                        new[k] = probs[k] * (1.0 - p) + probs[k - 1] * p
                    probs = new
                return probs
            else:
                mu = float(np.sum(p_list))
                var = float(np.sum([p * (1.0 - p) for p in p_list]))
                sigma = math.sqrt(max(var, 0.0))
                max_k = n
                probs = np.zeros(n + 1, dtype=float)

                def normal_cdf(x):
                    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
                if sigma < 1e-08:
                    k_eq = int(round(mu))
                    if 0 <= k_eq <= n:
                        probs[k_eq] = 1.0
                    else:
                        probs = np.zeros(n + 1, dtype=float)
                        probs[int(min(max(round(mu), 0), n))] = 1.0
                    return probs
                for k in range(0, n + 1):
                    upper = (k + 0.5 - mu) / sigma
                    lower = (k - 0.5 - mu) / sigma
                    probs[k] = max(0.0, normal_cdf(upper) - normal_cdf(lower))
                s = float(np.sum(probs))
                if s > 0:
                    probs /= s
                else:
                    k_eq = int(min(max(round(mu), 0), n))
                    probs = np.zeros(n + 1, dtype=float)
                    probs[k_eq] = 1.0
                return probs
        p_list = list(self.p_js) if self.n_opponents > 0 else []
        pmf = pb_pmf(p_list)
        if self.m <= 0:
            P_m = 1.0
        else:
            start = max(self.m, 0)
            if start > len(pmf) - 1:
                P_m = 0.0
            else:
                P_m = float(np.sum(pmf[start:]))
        m1 = self.m - 1
        if m1 <= 0:
            P_m1 = 1.0
        else:
            start1 = max(m1, 0)
            if start1 > len(pmf) - 1:
                P_m1 = 0.0
            else:
                P_m1 = float(np.sum(pmf[start1:]))
        E_C = float(self.k * P_m1)
        E_D = float(1.0 + self.k * P_m)
        if E_C > E_D + self.delta:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_3(BaseStrategy):
    """
    Opportunistic Pivotal Aggressor (OPA)

    Aggressive opportunistic strategy for the Collective Risk Dilemma.
    - Starts by defecting and always defects in the final round.
    - Estimates recent cooperation probability f of each other player using a short window L.
    - Models others as i.i.d. Bernoulli(f) and computes:
        P_ge_m  = P(at least m others cooperate)
        P_eq_m_1 = P(exactly m-1 others cooperate)  (i.e., you would be pivotal)
    - Free-rides (defects) if P_ge_m >= p_free_ride.
    - Cooperates only if P_eq_m_1 >= p_pivotal_threshold.
    - Maintains a grim punishment flag 'punish' that is set to True if recent history shows
      aggressive/opportunistic free-riding by any opponent (or a severe collapse in recent f).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.L = min(5, max(0, self.r - 1))
        self.p_free_ride = 0.5
        self.p_pivotal_threshold = 0.6
        self.punish_grace = 1.0
        self.punish = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        completed = history.round_number
        t = int(state.round_number)
        if self.punish:
            return Action.D
        if completed == 0 or t == 1:
            return Action.D
        if t == self.r:
            return Action.D
        window_size = min(self.L, completed)
        if window_size <= 0:
            return Action.D
        start_idx = completed - window_size
        end_idx = completed
        my_actions_window = history.my_actions[start_idx:end_idx].astype(float)
        opp_actions_window = history.opponent_actions[start_idx:end_idx, :]
        opp_sums = np.sum(opp_actions_window, axis=1).astype(float)
        coops = my_actions_window + opp_sums
        denom = float(max(1, self.n - 1))
        frac_of_others = (coops - my_actions_window) / denom
        f = float(np.mean(frac_of_others)) if frac_of_others.size > 0 else 0.0
        f = max(0.0, min(1.0, f))
        for opp_idx in range(self.n - 1):
            opp_actions_for_j = opp_actions_window[:, opp_idx]
            defect_mask = np.logical_not(opp_actions_for_j)
            success_mask = coops >= float(self.m)
            free_ride_mask = np.logical_and(defect_mask, success_mask)
            freq_j = float(np.sum(free_ride_mask)) / float(window_size)
            if freq_j >= self.punish_grace:
                self.punish = True
                return Action.D
        overall_frac_of_others = None
        try:
            all_my = history.my_actions.astype(float)
            all_opp = history.opponent_actions
            all_opp_sums = np.sum(all_opp, axis=1).astype(float)
            all_coops = all_my + all_opp_sums
            all_frac = (all_coops - all_my) / denom
            overall_frac_of_others = float(np.mean(all_frac)) if all_frac.size > 0 else f
        except Exception:
            overall_frac_of_others = f
        if f < 0.05 and overall_frac_of_others > 0.2:
            self.punish = True
            return Action.D
        N = self.n - 1
        if f <= 0.0:
            P_ge_m = 1.0 if 0 >= self.m else 0.0
            P_eq_m_1 = 1.0 if self.m - 1 == 0 else 0.0
        elif f >= 1.0:
            P_ge_m = 1.0 if N >= self.m else 0.0
            P_eq_m_1 = 1.0 if self.m - 1 == N else 0.0
        else:
            P_ge_m = 0.0
            if 0 <= self.m - 1 <= N:
                try:
                    comb = math.comb(N, self.m - 1)
                except Exception:
                    comb = float(np.math.factorial(N) // (np.math.factorial(self.m - 1) * np.math.factorial(N - (self.m - 1))))
                P_eq_m_1 = comb * f ** (self.m - 1) * (1.0 - f) ** (N - (self.m - 1))
            else:
                P_eq_m_1 = 0.0
            for x in range(self.m, N + 1):
                try:
                    c = math.comb(N, x)
                except Exception:
                    c = float(np.math.factorial(N) // (np.math.factorial(x) * np.math.factorial(N - x)))
                P_ge_m += c * f ** x * (1.0 - f) ** (N - x)
        if P_ge_m >= self.p_free_ride:
            return Action.D
        if P_eq_m_1 >= self.p_pivotal_threshold:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_4(BaseStrategy):
    """
    Aggressive Conditional-Pivotal (ACP): defect by default, cooperate only when your single
    contribution is likely to be pivotal enough (P_eq > 1/k), punish repeat exploiters permanently.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.p0 = 0.1
        self.alpha = 1.0
        self.max_window = 20
        self.exploit_set = set()
        self.pivotal_exploit_count = 0
        self.refuse_all = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t >= self.r:
            if history is not None:
                self._update_punishments(history)
            return Action.D
        if history is None:
            return Action.D
        self._update_punishments(history)
        n_others = self.n - 1
        if n_others <= 0:
            return Action.D
        W = min(self.max_window, max(1, t - 1))
        opp_actions = history.opponent_actions
        rounds_completed = history.round_number
        w = min(1.0, t / 5.0)
        p_list = []
        for j in range(n_others):
            if j in self.exploit_set:
                p_list.append(0.0)
                continue
            if rounds_completed <= 0:
                p_j = self.p0
            else:
                n_obs = min(W, rounds_completed)
                start = rounds_completed - n_obs
                obs = opp_actions[start:rounds_completed, j]
                count_c = int(np.sum(obs))
                if n_obs > 0:
                    p_j = (self.alpha + count_c) / (2.0 * self.alpha + float(n_obs))
                else:
                    p_j = self.p0
            p_j_mixed = w * p_j + (1.0 - w) * self.p0
            if p_j_mixed < 0.0:
                p_j_mixed = 0.0
            elif p_j_mixed > 1.0:
                p_j_mixed = 1.0
            p_list.append(float(p_j_mixed))
        target_k = self.m - 1
        if target_k < 0 or target_k > n_others:
            P_eq = 0.0
        else:
            p_bar = float(np.mean(p_list)) if len(p_list) > 0 else self.p0
            if p_bar <= 0.0:
                P_eq = 1.0 if target_k == 0 else 0.0
            elif p_bar >= 1.0:
                P_eq = 1.0 if target_k == n_others else 0.0
            else:
                comb = math.comb(n_others, target_k)
                P_eq = comb * p_bar ** target_k * (1.0 - p_bar) ** (n_others - target_k)
        threshold = 1.0 / max(1.0, self.k)
        if self.refuse_all:
            threshold = threshold + 1e-06 + 0.02
        if P_eq > threshold:
            return Action.C
        else:
            return Action.D

    def _update_punishments(self, history: PlayerHistory):
        """
        Inspect past rounds for pivotal cooperations of ours that were exploited,
        and add exploiters to exploit_set. Also escalate to refuse_all if multiple
        exploit events occur.
        """
        rounds_completed = history.round_number
        if rounds_completed <= 0:
            return
        n_others = self.n - 1
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        for idx in range(rounds_completed):
            if not bool(my_actions[idx]):
                continue
            opp_coops = int(np.sum(opp_actions[idx, :]))
            if opp_coops != self.m - 1:
                continue
            defectors = [j for j in range(n_others) if not bool(opp_actions[idx, j])]
            if len(defectors) == 0:
                continue
            exploited_flag_this_round = False
            for j in defectors:
                start = idx + 1
                n_post = rounds_completed - start
                if n_post <= 0:
                    continue
                post_obs = opp_actions[start:rounds_completed, j]
                post_coop_count = int(np.sum(post_obs))
                p_post = (self.alpha + post_coop_count) / (2.0 * self.alpha + float(n_post))
                if p_post <= 0.2:
                    if j not in self.exploit_set:
                        self.exploit_set.add(j)
                    exploited_flag_this_round = True
            if exploited_flag_this_round:
                self.pivotal_exploit_count += 1
        if self.pivotal_exploit_count >= 2:
            self.refuse_all = True

class Strategy_AGGRESSIVE_5(BaseStrategy):
    """
    Aggressive Pivotal-Grim (APG)

    - Defects by default.
    - Cooperates only when your single cooperation is pivotal (last round had exactly m-1 other cooperators).
    - If you cooperated and the round succeeded while some opponents defected, punish those defectors for τ = max(1, floor(r/3)) rounds by treating them as defectors when planning.
    - Uses last-round actions (with punished players treated as D) as the predictor. Conservative optional seeding in very early rounds if observed cooperation rate is very high.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n_players = game_description.n_players
        self.n_opponents = max(0, n_players - 1)
        self.punished_until = [0] * self.n_opponents
        self.tau = max(1, math.floor(game_description.n_rounds / 3))
        self.baseline_last = math.ceil((game_description.m - 1) / 2)
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is not None:
            completed = int(history.round_number)
            if completed > self.last_processed_round:
                for s in range(self.last_processed_round, completed):
                    round_num = s + 1
                    if history.my_actions[s]:
                        total_cooperators = int(history.opponent_actions[s, :].sum()) + 1
                        if total_cooperators >= self.game_description.m:
                            for j in range(self.n_opponents):
                                if not bool(history.opponent_actions[s, j]):
                                    self.punished_until[j] = max(self.punished_until[j], round_num + self.tau)
                self.last_processed_round = completed
        if history is None or (history is not None and history.round_number == 0):
            last_cooperators = int(self.baseline_last)
        else:
            last_idx = int(history.round_number) - 1
            last_cooperators = 0
            for j in range(self.n_opponents):
                if self.punished_until[j] >= t:
                    last_action_j = False
                else:
                    last_action_j = bool(history.opponent_actions[last_idx, j])
                if last_action_j:
                    last_cooperators += 1
        m = self.game_description.m
        allow_seed = False
        if history is not None and history.round_number >= 1 and (t <= 2):
            rounds_done = int(history.round_number)
            opp_sum = int(history.opponent_actions[:rounds_done, :].sum())
            my_sum = int(history.my_actions[:rounds_done].sum())
            total_entries = max(1, self.game_description.n_players * rounds_done)
            coop_prop = (opp_sum + my_sum) / total_entries
            if coop_prop >= 0.8:
                allow_seed = True
        if last_cooperators >= m:
            action = Action.D
        elif last_cooperators == m - 1:
            action = Action.C
        elif allow_seed:
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_AGGRESSIVE_6(BaseStrategy):
    """
    Aggressive Conditional Pivotal Punisher (ACPP)

    - Defect by default unless a single contribution is plausibly pivotal (raises expected payoff).
    - Estimate opponents' cooperation rates empirically (prior p0 if unseen).
    - Blacklist opponents who are identified as culpable for suspicious failures and treat them
      as non-cooperators in future probability estimates until they demonstrate sustained good behavior.
    - Forgive after sustained cooperation (K_forgive consecutive good rounds or high recent coop rate).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.p0 = 0.45
        self.epsilon = 0.01
        self.suspicious_failure_threshold = 0.6
        self.punish_threshold = 0.35
        self.K_forgive = max(1, math.floor(self.r / 6))
        self.blacklisted = set()
        self.last_processed_round = 0

    def _pb_pmf(self, probs):
        probs = list(probs)
        mlen = len(probs)
        dist = [1.0]
        for p in probs:
            new = [0.0] * (len(dist) + 1)
            for i, val in enumerate(dist):
                new[i] += val * (1.0 - p)
                new[i + 1] += val * p
            dist = new
        return dist

    def _prob_at_least(self, probs, x):
        if x <= 0:
            return 1.0
        if x > len(probs):
            return 0.0
        dist = self._pb_pmf(probs)
        return sum(dist[x:])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            rounds_seen = 0
        else:
            rounds_seen = history.round_number
        if history is not None and rounds_seen > self.last_processed_round:
            for r_idx in range(self.last_processed_round, rounds_seen):
                rounds_before = r_idx
                if rounds_before <= 0:
                    coop_counts_pre = np.zeros(self.n - 1, dtype=float)
                    coop_rates_pre = np.array([self.p0] * (self.n - 1), dtype=float)
                else:
                    coop_counts_pre = np.sum(history.opponent_actions[:rounds_before, :], axis=0)
                    coop_rates_pre = coop_counts_pre / float(rounds_before)
                coop_rates_snapshot = coop_rates_pre.copy()
                for j in self.blacklisted:
                    if 0 <= j < len(coop_rates_snapshot):
                        coop_rates_snapshot[j] = 0.0
                my_act = bool(history.my_actions[r_idx])
                opp_acts = history.opponent_actions[r_idx, :]
                total_coop = int(my_act) + int(np.sum(opp_acts))
                round_failed = total_coop < self.m
                if round_failed:
                    probs_others = coop_rates_snapshot.tolist()
                    P_others_ge_m = self._prob_at_least(probs_others, self.m)
                    if P_others_ge_m >= self.suspicious_failure_threshold:
                        for j in range(self.n - 1):
                            if not bool(opp_acts[j]):
                                rate_j = coop_rates_pre[j]
                                if rate_j < self.punish_threshold:
                                    self.blacklisted.add(j)
                if history.round_number >= self.K_forgive:
                    recent_K = self.K_forgive
                    start_idx = max(0, history.round_number - recent_K)
                    window = history.opponent_actions[start_idx:history.round_number, :]
                    for j in list(self.blacklisted):
                        coop_in_window = int(np.sum(window[:, j]))
                        if coop_in_window >= recent_K:
                            self.blacklisted.discard(j)
                            continue
                        if recent_K > 0:
                            if coop_in_window / float(recent_K) >= 0.8:
                                self.blacklisted.discard(j)
                                continue
            self.last_processed_round = rounds_seen
        if history is None or rounds_seen == 0:
            coop_rates = np.array([self.p0] * (self.n - 1), dtype=float)
        else:
            coop_counts = np.sum(history.opponent_actions[:rounds_seen, :], axis=0)
            coop_rates = coop_counts / float(rounds_seen)
        for j in self.blacklisted:
            if 0 <= j < len(coop_rates):
                coop_rates[j] = 0.0
        probs_others = coop_rates.tolist()
        P_if_you_C = self._prob_at_least(probs_others, max(0, self.m - 1))
        P_if_you_D = self._prob_at_least(probs_others, self.m)
        tiny = 1e-12
        if P_if_you_D >= 1.0 - tiny:
            return Action.D
        E_C = P_if_you_C * self.k
        E_D = P_if_you_D * (1.0 + self.k) + (1.0 - P_if_you_D) * 1.0
        if E_C > E_D + self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_8(BaseStrategy):
    """
    Defect-by-default; cooperate only when the estimated probability that exactly
    m-1 opponents will cooperate (making you pivotal) satisfies k * P > 1.
    Uses per-opponent EMA (fast adaptation), conservative tie-breaking toward D,
    and a tiny decaying exploration probability in early rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.4
        self.p0 = 0.1
        self.eps0 = 0.03
        self.eps_min = 0.005
        self.eps_decay = math.log(2.0) / 10.0
        self.normal_approx_threshold = 200

    def _epsilon_for_round(self, t: int) -> float:
        """Exploration schedule for upcoming round t (1-based)."""
        eps = self.eps0 * math.exp(-self.eps_decay * (t - 1))
        if eps < self.eps_min:
            eps = self.eps_min
        eps = min(eps, 0.05, 1.0)
        return eps

    def _estimate_opponent_ps(self, history: PlayerHistory) -> np.ndarray:
        """Compute per-opponent EMA p_j from history (chronological)."""
        opponents_count = max(0, self.n - 1)
        p = np.full(opponents_count, self.p0, dtype=float)
        if history is None:
            return p
        rounds = history.round_number
        if rounds == 0:
            return p
        opp_actions = np.asarray(history.opponent_actions)
        if opp_actions.ndim == 1:
            opp_actions = opp_actions.reshape((1, -1))
        if opp_actions.shape[1] < opponents_count:
            pad_cols = opponents_count - opp_actions.shape[1]
            pad = np.zeros((opp_actions.shape[0], pad_cols), dtype=bool)
            opp_actions = np.concatenate([opp_actions, pad], axis=1)
        elif opp_actions.shape[1] > opponents_count:
            opp_actions = opp_actions[:, :opponents_count]
        for t in range(opp_actions.shape[0]):
            row = opp_actions[t].astype(float)
            p = (1.0 - self.alpha) * p + self.alpha * row
        return p

    def _poisson_binomial_exact(self, ps: np.ndarray, target: int) -> float:
        """Exact DP computation of probability of exactly `target` successes."""
        n = len(ps)
        if target < 0 or target > n:
            return 0.0
        q = np.zeros(n + 1, dtype=float)
        q[0] = 1.0
        for p in ps:
            q_next = np.zeros_like(q)
            q_next[:n + 1] += q * (1.0 - p)
            q_next[1:n + 1] += q[:n] * p
            q = q_next
        return float(q[target])

    def _poisson_binomial_normal_approx(self, ps: np.ndarray, target: int) -> float:
        """Normal approximation with continuity correction for Poisson-Binomial."""
        mu = float(np.sum(ps))
        var = float(np.sum(ps * (1.0 - ps)))
        if var <= 0.0:
            return 1.0 if abs(mu - target) < 1e-12 else 0.0
        sigma = math.sqrt(var)

        def normal_cdf(x):
            return 0.5 * (1.0 + math.erf((x - mu) / (sigma * math.sqrt(2.0))))
        upper = normal_cdf(target + 0.5)
        lower = normal_cdf(target - 0.5)
        p = upper - lower
        if p < 0.0:
            p = 0.0
        elif p > 1.0:
            p = 1.0
        return p

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        eps = self._epsilon_for_round(t)
        if history is None or history.round_number == 0:
            if random.random() < eps:
                return Action.C
            return Action.D
        ps = self._estimate_opponent_ps(history)
        M = self.m - 1
        opponents_count = max(0, self.n - 1)
        if opponents_count == 0:
            P = 0.0
        elif opponents_count <= self.normal_approx_threshold:
            P = self._poisson_binomial_exact(ps, M)
        else:
            P = self._poisson_binomial_normal_approx(ps, M)
        cooperate_by_rule = self.k * P > 1.0
        if cooperate_by_rule:
            return Action.C
        else:
            if random.random() < eps:
                return Action.C
            return Action.D

class Strategy_AGGRESSIVE_9(BaseStrategy):
    """
    Pivotal Exploiter with Aggressive Retaliation (PEAR)

    Aggressive, self-interested strategy:
    - Defects in first and last rounds.
    - Exploits guaranteed rewards by defecting when others are predicted to meet threshold.
    - Cooperates only when predicted to be pivotal (others == m-1) and the cooperation pool is not collapsing.
    - If exploited after cooperating (or repeated near-misses by others), punish the group by defecting for L rounds.
    - Uses simple per-opponent cooperation frequencies and last-round counts for predictions.
    - Small optional randomization (epsilon) to avoid perfect predictability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.theta = 0.5
        self.L = max(1, math.floor(r / 10))
        self.epsilon = min(0.05, 5.0 / r) if r > 0 else 0.0
        self.W = 3
        self.punish_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_played = int(history.round_number)
        t = int(getattr(state, 'round_number', rounds_played + 1))
        if t <= 0:
            t = rounds_played + 1
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        r_total = int(self.game_description.n_rounds)
        theta = float(self.theta)
        L = int(self.L)
        eps = float(self.epsilon)
        W = int(self.W)
        prev_round_number = rounds_played

        def compute_predicted_other(opponent_actions_array, use_rounds):
            if use_rounds <= 0:
                return 0
            slice_arr = opponent_actions_array[:use_rounds, :]
            sums = np.sum(slice_arr.astype(float), axis=0)
            freqs = sums / float(use_rounds)
            preds = (freqs >= theta).astype(int)
            return int(np.sum(preds))
        if prev_round_number >= 1:
            rounds_before_prev = max(0, prev_round_number - 1)
            opp_actions = history.opponent_actions
            predicted_other_prev = compute_predicted_other(opp_actions, rounds_before_prev)
            my_last_action = bool(history.my_actions[-1])
            others_coop_prev = int(np.sum(opp_actions[-1, :]))
            actual_total_prev = others_coop_prev + (1 if my_last_action else 0)
            if my_last_action and predicted_other_prev == m - 1 and (actual_total_prev < m):
                new_punish_until = prev_round_number + L
                if new_punish_until > self.punish_until:
                    self.punish_until = new_punish_until
            window_start_idx = max(0, prev_round_number - W)
            repeated_fail_count = 0
            for idx in range(window_start_idx, prev_round_number):
                other_coops = int(np.sum(opp_actions[idx, :]))
                pred_before_round = compute_predicted_other(opp_actions, idx)
                total_coops_this_round = other_coops + (1 if bool(history.my_actions[idx]) else 0)
                if other_coops >= m - 1 and total_coops_this_round < m:
                    repeated_fail_count += 1
            if repeated_fail_count >= 2:
                new_punish_until = prev_round_number + L
                if new_punish_until > self.punish_until:
                    self.punish_until = new_punish_until
        if t >= r_total:
            return Action.D
        if self.punish_until >= t:
            return Action.D
        predicted_other = 0
        last_other = 0
        if rounds_played <= 0:
            predicted_other = 0
            last_other = 0
        else:
            predicted_other = compute_predicted_other(history.opponent_actions, rounds_played)
            last_other = int(np.sum(history.opponent_actions[-1, :]))
        chosen_action = Action.D
        if predicted_other >= m:
            chosen_action = Action.D
        elif predicted_other == m - 1:
            if last_other < m - 1:
                chosen_action = Action.D
            else:
                chosen_action = Action.C
        else:
            chosen_action = Action.D
        if eps > 0.0 and random.random() < eps:
            chosen_action = Action.C if chosen_action == Action.D else Action.D
        return chosen_action

class Strategy_AGGRESSIVE_10(BaseStrategy):
    """
    Pivotal Exploiter (PE)

    Adaptive, aggressive strategy for the Collective Risk Dilemma:
    - Maintains EMA estimates p_j of each opponent's cooperation rate.
    - Computes exact probabilities that the public good is achieved with/without
      cooperating (DP convolution of independent Bernoullis).
    - Cooperates only when k * marginal_prob_increase > cost (1). Ties -> defect.
    - Small exploration probability to probe opponents.
    - Targets and briefly punishes opponents who repeatedly defect when you paid.
    - EMA and time-limited punishments allow forgiveness / recovery.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.25
        self.epsilon = 0.02
        self.prior = 0.2
        self.base_P = 2
        self.cap_P = 5
        self.escalate_threshold = 2
        self.margin = 0.0
        self.n = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self._last_computed = None

    def _prob_at_least_k(self, ps, k_needed):
        """
        Compute probability that sum of independent Bernoulli(ps) >= k_needed.
        ps: iterable of probabilities (length N)
        k_needed: integer threshold (>=0)
        Returns float in [0,1]
        """
        ps_list = list(ps)
        n = len(ps_list)
        if k_needed <= 0:
            return 1.0
        if k_needed > n:
            return 0.0
        Q = [0.0] * (n + 1)
        Q[0] = 1.0
        for p in ps_list:
            for s in range(n, 0, -1):
                Q[s] = Q[s] * (1.0 - p) + Q[s - 1] * p
            Q[0] = Q[0] * (1.0 - p)
        return sum(Q[k_needed:])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            n_opponents = self.n - 1
            effective_ps = [self.prior] * n_opponents
            pr_defect = self._prob_at_least_k(effective_ps, self.m)
            pr_coop = self._prob_at_least_k(effective_ps, max(0, self.m - 1))
            E_defect = 1.0 + self.k * pr_defect
            E_coop = self.k * pr_coop
            intend_C = E_coop > E_defect + self.margin
            if random.random() < self.epsilon:
                intend_C = not intend_C
            return Action.C if intend_C else Action.D
        T = history.round_number
        n_opponents = self.n - 1
        p_js = [self.prior] * n_opponents
        punish_timers = [0] * n_opponents
        exploit_counts = [0] * n_opponents
        for t in range(T):
            if t > 0:
                for j in range(n_opponents):
                    if punish_timers[j] > 0:
                        punish_timers[j] -= 1
                    if exploit_counts[j] > 0:
                        exploit_counts[j] = max(0, exploit_counts[j] - 1)
            my_action = bool(history.my_actions[t])
            opp_actions_row = history.opponent_actions[t, :]
            coop_count = int(my_action) + int(np.sum(opp_actions_row.astype(int)))
            for j in range(n_opponents):
                observed = 1.0 if bool(opp_actions_row[j]) else 0.0
                p_js[j] = (1.0 - self.alpha) * p_js[j] + self.alpha * observed
            if my_action and coop_count < self.m:
                for j in range(n_opponents):
                    if not bool(opp_actions_row[j]):
                        punish_timers[j] = max(punish_timers[j], self.base_P)
                        exploit_counts[j] += 1
                        if exploit_counts[j] >= self.escalate_threshold:
                            punish_timers[j] = max(punish_timers[j], self.cap_P)
        effective_ps = []
        for j in range(n_opponents):
            if punish_timers[j] > 0:
                effective_ps.append(0.0)
            else:
                pj = p_js[j]
                if pj < 0.0:
                    pj = 0.0
                elif pj > 1.0:
                    pj = 1.0
                effective_ps.append(pj)
        pr_defect = self._prob_at_least_k(effective_ps, self.m)
        pr_coop = self._prob_at_least_k(effective_ps, max(0, self.m - 1))
        E_defect = 1.0 + self.k * pr_defect
        E_coop = self.k * pr_coop
        intend_C = E_coop > E_defect + self.margin
        if random.random() < self.epsilon:
            intend_C = not intend_C
        return Action.C if intend_C else Action.D

class Strategy_AGGRESSIVE_11(BaseStrategy):
    """
    Aggressive Pivotalist (AP)

    - Default: defect.
    - Cooperate only if your single cooperation meaningfully increases
      the chance of securing the group reward (pivotal), i.e. EU_C >= EU_D + delta.
    - Maintain per-opponent Bayesian estimates (Beta(alpha,beta)) with exponential
      decay on past rounds to compute current cooperation probabilities.
    - Use Poisson-binomial DP convolution to compute the distribution of how many
      opponents will cooperate.
    - Short, group-level punishment: if any opponent's current cooperation estimate
      drops by punish_threshold relative to a slowly-updated baseline, withhold
      cooperation for punish_length rounds. Punishments decay automatically.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.delta = 0.1
        self.alpha = 1.0
        self.beta = 1.0
        self.decay = 0.9
        self.punish_threshold = 0.3
        self.punish_length = 2
        self.baseline_lr = 0.05
        n = max(0, game_description.n_players - 1)
        prior_mean = self.alpha / (self.alpha + self.beta)
        self.baseline = np.full((n,), prior_mean, dtype=float)
        self.punish_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        n_opponents = max(0, n_players - 1)
        if history is None:
            completed = 0
        else:
            completed = history.round_number
        t = completed + 1
        if t == n_rounds:
            return Action.D
        if history is None or completed == 0 or n_opponents == 0:
            return Action.D
        opp_actions = np.asarray(history.opponent_actions, dtype=float)
        rounds_completed = opp_actions.shape[0]
        exponents = np.arange(rounds_completed - 1, -1, -1)
        weights = (self.decay ** exponents).astype(float)
        weighted_C = np.zeros((n_opponents,), dtype=float)
        for idx, w in enumerate(weights):
            weighted_C += w * opp_actions[idx, :]
        S = self.alpha + weighted_C
        total_weights = np.sum(weights)
        weighted_D = total_weights - weighted_C
        F = self.beta + weighted_D
        p = S / (S + F)
        p = np.clip(p, 1e-12, 1.0 - 1e-12)
        prob = np.zeros((n_opponents + 1,), dtype=float)
        prob[0] = 1.0
        for pj in p:
            new_prob = np.zeros_like(prob)
            new_prob[0] = prob[0] * (1.0 - pj)
            for x in range(1, n_opponents + 1):
                new_prob[x] = prob[x] * (1.0 - pj) + prob[x - 1] * pj
            prob = new_prob
        lower_D = max(m, 0)
        if lower_D <= n_opponents:
            P_D = float(np.sum(prob[lower_D:]))
        else:
            P_D = 0.0
        lower_C = max(m - 1, 0)
        if lower_C <= n_opponents:
            P_C = float(np.sum(prob[lower_C:]))
        else:
            P_C = 0.0
        EU_D = 1.0 + k * P_D
        EU_C = k * P_C
        drop_mask = p <= self.baseline * (1.0 - self.punish_threshold)
        if np.any(drop_mask):
            self.punish_timer = max(self.punish_timer, self.punish_length)
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
        elif EU_C >= EU_D + self.delta:
            action = Action.C
        else:
            action = Action.D
        self.baseline = (1.0 - self.baseline_lr) * self.baseline + self.baseline_lr * p
        return action

class Strategy_AGGRESSIVE_12(BaseStrategy):
    """
    Opportunistic Pivot-and-Exploit (OPE)

    Aggressive, immediate-EU maximizing strategy for the Collective Risk Dilemma.
    - Defaults to defect.
    - Estimates opponents' cooperation probabilities over a short window.
    - Computes exact distribution of how many opponents will cooperate (via convolution).
    - Compares immediate expected payoff of Cooperate vs Defect and chooses the higher.
    - Has a short "build" (recruitment) phase where it may cooperate if a coalition seems plausible.
    - Tracks times it was exploited (cooperated but threshold failed) and abandons cooperation after a small limit.
    - Marks specific opponents as less reliable when they defect in rounds that exploited you.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        r = int(self.game.n_rounds)
        self.w = int(min(10, max(1, math.floor(r / 4))))
        self.T_build = int(min(5, max(1, math.floor(r / 4)))) if r > 1 else 1
        self.R_min = 0.6
        self.exploit_limit = 2
        self.epsilon = 1e-06
        self.times_exploited = 0
        self.build_success = False
        self.build_success_count = 0
        self.always_defect = False
        self._decay = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            n_opp = max(0, self.game.n_players - 1)
            self._decay = np.ones(n_opp, dtype=float)
            return Action.D
        n_opp = max(0, self.game.n_players - 1)
        if self._decay is None or len(self._decay) != n_opp:
            self._decay = np.ones(n_opp, dtype=float)
        rounds_completed = history.round_number
        if rounds_completed >= 1:
            last_idx = -1
            my_last = bool(history.my_actions[last_idx])
            opp_last_actions = np.array(history.opponent_actions[last_idx, :], dtype=bool)
            total_cooperators = int(np.sum(opp_last_actions)) + (1 if my_last else 0)
            if my_last and total_cooperators < self.game.m:
                self.times_exploited += 1
                for j in range(n_opp):
                    if not opp_last_actions[j]:
                        self._decay[j] = max(0.0, self._decay[j] * 0.5)
                if self.times_exploited > self.exploit_limit:
                    self.always_defect = True
                    self.build_success = False
            last_round_number = rounds_completed
            if last_round_number <= self.T_build and my_last and (total_cooperators >= self.game.m):
                self.build_success_count += 1
                if self.build_success_count >= 2:
                    self.build_success = True
        if self.always_defect:
            return Action.D
        t_next = history.round_number + 1
        use_rounds = min(self.w, history.round_number)
        p_js = np.zeros(n_opp, dtype=float)
        if use_rounds > 0:
            recent = history.opponent_actions[-use_rounds:, :]
            counts = np.sum(recent, axis=0)
            p_js = counts.astype(float) / float(use_rounds)
            p_js = p_js * self._decay
            p_js = np.clip(p_js, 0.0, 1.0)
        else:
            p_js = np.zeros(n_opp, dtype=float)
        probs = np.array([1.0], dtype=float)
        for p in p_js:
            probs = np.convolve(probs, np.array([1.0 - float(p), float(p)], dtype=float))
        probs = np.maximum(probs, 0.0)
        probs_sum = np.sum(probs)
        if probs_sum > 0:
            probs = probs / probs_sum
        m = int(self.game.m)
        if m <= len(probs) - 1:
            P_at_least_m = float(np.sum(probs[m:]))
        else:
            P_at_least_m = 0.0
        if m - 1 <= len(probs) - 1:
            P_at_least_m_minus_1 = float(np.sum(probs[max(0, m - 1):]))
        else:
            P_at_least_m_minus_1 = 0.0
        if 0 <= m - 1 < len(probs):
            P_exactly_m_minus_1 = float(probs[m - 1])
        else:
            P_exactly_m_minus_1 = 0.0
        EU_C = P_at_least_m_minus_1 * float(self.game.k)
        EU_D = P_at_least_m * (float(self.game.k) + 1.0) + (1.0 - P_at_least_m) * 1.0
        prefer_coop = False
        if t_next <= self.T_build and (not self.build_success):
            count_likely = int(np.sum(p_js >= self.R_min))
            if count_likely >= m:
                prefer_coop = True
        will_cooperate = False
        if EU_C > EU_D + self.epsilon:
            will_cooperate = True
        elif t_next <= self.T_build and prefer_coop and (EU_C + self.epsilon >= EU_D):
            will_cooperate = True
        else:
            will_cooperate = False
        if P_at_least_m >= 0.5:
            will_cooperate = False
        return Action.C if will_cooperate else Action.D

class Strategy_AGGRESSIVE_13(BaseStrategy):
    """
    Aggressive Conditional Pivotal Defector (ACPD).

    Summary:
    - Default: defect.
    - Cooperate only when empirical probability that exactly m-1 opponents
      will cooperate (p_exact) exceeds 1/k (the analytically derived threshold).
    - Uses a recent-history window (with tiny Laplace smoothing) to estimate
      the distribution of opponent cooperations.
    - Tie-breaking randomization around the threshold, explicit exploitation
      when opponents already provide >= m cooperators frequently, and a
      protection mechanism to avoid repeatedly being the sole (failed) cooperator.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.window_size = min(10, max(1, game_description.n_rounds - 1))
        self.epsilon = 0.001
        self.alpha = 0.001
        self.tie_coop_prob = 0.5
        self.first_round_delta = 0.0
        self.R_fail = 3
        self.T_protect = 2
        self.protection_counter = 0
        self.p_ge_exploit_threshold = 0.9

    def __call__(self, state: GameState, history: None | PlayerHistory):
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None or history.round_number == 0:
            if self.first_round_delta > 0.0 and random.random() < self.first_round_delta:
                return Action.C
            return Action.D
        if self.protection_counter > 0:
            self.protection_counter = max(0, self.protection_counter - 1)
            return Action.D
        rounds_completed = history.round_number
        W = min(self.window_size, rounds_completed)
        start = max(0, rounds_completed - W)
        freq = [0.0] * n
        for tau in range(start, rounds_completed):
            s = int(np.sum(history.opponent_actions[tau, :]))
            if 0 <= s <= n - 1:
                freq[s] += 1.0
        for c in range(0, n):
            freq[c] += self.alpha
        W_eff = float(W) + self.alpha * float(n)
        idx = m - 1
        if 0 <= idx < n:
            p_exact = freq[idx] / W_eff
        else:
            p_exact = 0.0
        p_ge = sum((freq[c] for c in range(m, n))) / W_eff if m < n else 0.0
        failures = 0
        found_coop = 0
        for tau in range(rounds_completed - 1, -1, -1):
            if history.my_actions[tau]:
                found_coop += 1
                opp_coops = int(np.sum(history.opponent_actions[tau, :]))
                total_coops = opp_coops + 1
                if total_coops < m:
                    failures += 1
                if found_coop >= self.R_fail:
                    break
        if found_coop >= self.R_fail and failures >= self.R_fail:
            self.protection_counter = self.T_protect
            self.protection_counter = max(0, self.protection_counter - 1)
            return Action.D
        if p_ge >= self.p_ge_exploit_threshold:
            return Action.D
        threshold = 1.0 / k if k != 0 else float('inf')
        if abs(p_exact - threshold) <= self.epsilon:
            if random.random() < self.tie_coop_prob:
                return Action.C
            else:
                return Action.D
        if p_exact > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_14(BaseStrategy):
    """
    Aggressive, pivot-aware strategy for the Collective Risk Dilemma.

    - Defaults to defect.
    - Cooperates only when my single contribution is likely to be pivotal:
        cooperate iff Pr(others exactly m-1 cooperate) > 1/k + eps
      (uses recency-weighted estimates of opponents' cooperation probabilities).
    - Exploits near-certain cooperators (free-rides if >= m others are almost certain).
    - Tracks and marks persistent "suckers" (opponents who repeatedly cooperate in failed rounds)
      and treats them as non-contributors thereafter to avoid exploitation.
    - Uses DP convolution for exact Poisson-binomial pmf when feasible, otherwise normal approx.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.H = 5.0
        self.epsilon = 0.01
        self.p0 = 0.1
        self.sucker_threshold = 2
        self.suckers = set()
        self.sucker_scores = {}
        self.dp_exact_threshold = 60

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if k <= 1.0:
            return Action.D
        if history is None:
            t = 0
        else:
            t = history.round_number
        if t == r - 1:
            if history is not None and history.round_number > 0:
                self._update_sucker_tracking(history)
            return Action.D
        if history is None:
            probs = [self.p0] * (n - 1)
            pmf = self._poisson_binomial_pmf(probs)
            p_eq = pmf[m - 1] if 0 <= m - 1 < len(pmf) else 0.0
            if p_eq > 1.0 / k + self.epsilon:
                return Action.C
            return Action.D
        if history.round_number > 0:
            self._update_sucker_tracking(history)
        probs = []
        n_other = n - 1
        for j in range(n_other):
            if j in self.suckers:
                probs.append(0.0)
            else:
                p_j = self._estimate_prob(j, history)
                probs.append(p_j)
        high_coop_count = sum((1 for p in probs if p >= 0.9))
        if high_coop_count >= m:
            return Action.D
        pmf = self._poisson_binomial_pmf(probs)
        p_eq = pmf[m - 1] if 0 <= m - 1 < len(pmf) else 0.0
        low_coop_count = sum((1 for p in probs if p <= 0.1))
        if low_coop_count >= m and p_eq < 1e-06:
            return Action.D
        threshold = 1.0 / k + self.epsilon
        if p_eq > threshold:
            return Action.C
        return Action.D

    def _estimate_prob(self, opponent_idx: int, history: PlayerHistory) -> float:
        """
        Recency-weighted estimate of opponent's cooperation probability with smoothing.
        opponent_idx is in 0..n-2 corresponding to columns of history.opponent_actions.
        """
        rounds = history.round_number
        if rounds == 0:
            return self.p0
        t = rounds
        s_arr = np.arange(0, t)
        exponents = (s_arr - (t - 1)) / float(self.H)
        weights = 2.0 ** exponents
        total_weight = float(np.sum(weights))
        if total_weight <= 0:
            return self.p0
        coop_flags = history.opponent_actions[:, opponent_idx].astype(float)
        observed_weighted = float(np.dot(weights, coop_flags))
        p_hat = (observed_weighted + self.alpha) / (total_weight + 2.0 * self.alpha)
        if p_hat < 0.0:
            p_hat = 0.0
        elif p_hat > 1.0:
            p_hat = 1.0
        if history.round_number >= 2:
            my_actions = history.my_actions.astype(int)
            opp_actions = history.opponent_actions[:, opponent_idx].astype(int)
            idxs_my_c = np.where(my_actions == 1)[0]
            idxs_my_d = np.where(my_actions == 0)[0]
            if len(idxs_my_c) >= 1 and len(idxs_my_d) >= 1:
                coop_when_i_c = np.mean(opp_actions[idxs_my_c]) if len(idxs_my_c) > 0 else 0.0
                coop_when_i_d = np.mean(opp_actions[idxs_my_d]) if len(idxs_my_d) > 0 else 0.0
                if coop_when_i_c - coop_when_i_d > 0.5 and coop_when_i_c > 0.6:
                    p_hat = min(p_hat, 0.4)
        return float(p_hat)

    def _poisson_binomial_pmf(self, probs):
        """
        Return pmf array where pmf[k] = Pr(X = k) for X = sum of independent Bernoulli(probs).
        Uses exact DP (convolution) when len(probs) <= dp_exact_threshold, otherwise normal approx.
        """
        n = len(probs)
        if n == 0:
            return np.array([1.0])
        if n <= self.dp_exact_threshold:
            pmf = np.zeros(n + 1, dtype=float)
            pmf[0] = 1.0
            for p in probs:
                prev = pmf.copy()
                pmf[0] = prev[0] * (1.0 - p)
                for k in range(1, n + 1):
                    pmf[k] = prev[k] * (1.0 - p) + prev[k - 1] * p
            pmf = np.maximum(pmf, 0.0)
            s = pmf.sum()
            if s > 0:
                pmf = pmf / s
            return pmf
        mu = float(sum(probs))
        var = float(sum((p * (1.0 - p) for p in probs)))
        sigma = math.sqrt(max(var, 0.0))
        pmf = np.zeros(n + 1, dtype=float)
        if sigma < 1e-08:
            k = int(round(mu))
            if 0 <= k <= n:
                pmf[k] = 1.0
            return pmf

        def normal_cdf(x):
            return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
        for k in range(0, n + 1):
            a = (k - 0.5 - mu) / sigma
            b = (k + 0.5 - mu) / sigma
            pmf[k] = max(0.0, normal_cdf(b) - normal_cdf(a))
        s = pmf.sum()
        if s > 0:
            pmf = pmf / s
        return pmf

    def _update_sucker_tracking(self, history: PlayerHistory):
        """
        Inspect the most recent completed round and, if appropriate, increment sucker scores
        for opponents who cooperated when the group failed while we cooperated.
        """
        rounds = history.round_number
        if rounds == 0:
            return
        last_idx = rounds - 1
        my_last = bool(history.my_actions[last_idx])
        opp_last = history.opponent_actions[last_idx, :].astype(bool)
        total_cooperators = int(np.sum(opp_last)) + (1 if my_last else 0)
        if my_last and total_cooperators < self.game_description.m:
            for j, did_coop in enumerate(opp_last):
                if did_coop:
                    self.sucker_scores[j] = self.sucker_scores.get(j, 0) + 1
                    if self.sucker_scores[j] >= self.sucker_threshold:
                        self.suckers.add(j)
        if rounds % 50 == 0 and rounds > 0:
            to_remove = []
            for j in list(self.sucker_scores.keys()):
                self.sucker_scores[j] = max(0, self.sucker_scores[j] - 1)
                if self.sucker_scores[j] == 0:
                    to_remove.append(j)
            for j in to_remove:
                self.sucker_scores.pop(j, None)
                self.suckers.discard(j)

class Strategy_AGGRESSIVE_15(BaseStrategy):
    """
    Aggressive Pivotal Exploiter

    - Defaults to defect on the first round.
    - Estimates each opponent's cooperation probability with a Beta(alpha=1, beta=2) smoothing.
    - Computes the exact probability that exactly m-1 of the other n-1 players cooperate
      by convolving independent Bernoulli(p_j) distributions (dynamic programming).
    - Cooperates iff Pr[#others == m-1] > 1/k (ties break to defection).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._alpha = 1.0
        self._beta = 2.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        rounds_seen = int(history.round_number)
        if rounds_seen <= 0:
            return Action.D
        opp_actions = history.opponent_actions
        count_C = np.sum(opp_actions, axis=0).astype(float)
        denom = rounds_seen + self._alpha + self._beta
        p_list = ((count_C + self._alpha) / denom).tolist()
        if len(p_list) == 0:
            return Action.D
        dp = np.zeros(n, dtype=float)
        dp[0] = 1.0
        for p in p_list:
            prev = dp
            shifted = np.concatenate(([0.0], prev[:-1]))
            dp = prev * (1.0 - p) + shifted * p
        target = m - 1
        if not 0 <= target <= n - 1:
            prob_pivotal = 0.0
        else:
            prob_pivotal = float(dp[target])
        if prob_pivotal > 1.0 / float(k):
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_16(BaseStrategy):
    """
    Aggressive pivotal-defector: defect by default, cooperate only when the estimated
    probability that exactly m-1 other players cooperate exceeds 1/k (uses per-player
    empirical frequencies with Beta(a,a) smoothing and exact Poisson-binomial DP).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.prior_a = 0.5
        self.epsilon = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        if self.m > self.n or self.m <= 1:
            return Action.D
        if self.epsilon > 0.0 and random.random() < float(self.epsilon):
            return Action.C
        t_prev = history.round_number
        n_other = self.n - 1
        try:
            opp_actions = history.opponent_actions
            if opp_actions.ndim != 2 or opp_actions.shape[1] != n_other:
                return Action.D
        except Exception:
            return Action.D
        a = float(self.prior_a)
        denom = float(t_prev) + 2.0 * a
        s = np.sum(opp_actions[:t_prev, :], axis=0).astype(float)
        p_vec = (s + a) / denom
        probs = np.zeros(n_other + 1, dtype=float)
        probs[0] = 1.0
        for p in p_vec:
            new_probs = np.zeros_like(probs)
            new_probs[:n_other + 1] += probs * (1.0 - float(p))
            new_probs[1:n_other + 1] += probs[:n_other] * float(p)
            probs = new_probs
        target = self.m - 1
        if target < 0 or target > n_other:
            p_eq = 0.0
        else:
            p_eq = float(probs[target])
        threshold = 1.0 / float(self.k) if self.k != 0 else float('inf')
        if p_eq > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_17(BaseStrategy):
    """
    Pivotal-Calculus Aggressor (PCA)

    - Default: defect on first round and when uncertain.
    - Maintain exponential-smoothed cooperation probabilities for each opponent.
    - Compute P_piv = P(sum_{j != me} Bernoulli(p_j) = m - 1) via dynamic programming.
    - Cooperate iff P_piv > 1/k + epsilon.
    - If exploited (I cooperated and group still failed), increment exploitation counter and
      set punishment_remaining = min(remaining_rounds, ceil(beta * E_count)).
      While punishment_remaining > 0, unconditionally defect and decrement punishment_remaining.
    - Performs light de-escalation: after several consecutive successful rounds without my cooperation,
      reduce E_count by 1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.25
        self.epsilon = 0.01
        self.beta = 1.0
        self.deescalate_streak_threshold = 3
        n_players = game_description.n_players
        n_opponents = max(0, n_players - 1)
        self.p_j = [0.0 for _ in range(n_opponents)]
        self.E_count = 0
        self.punishment_remaining = 0
        self.success_streak_no_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        remaining_rounds = max(0, self.game_description.n_rounds - rounds_completed)
        n_players = self.game_description.n_players
        n_opponents = max(0, n_players - 1)
        m = self.game_description.m
        k = self.game_description.k
        p_est = [0.0 for _ in range(n_opponents)]
        if rounds_completed > 0:
            for t in range(rounds_completed):
                row = history.opponent_actions[t]
                for j in range(n_opponents):
                    observed = 1.0 if bool(row[j]) else 0.0
                    p_est[j] = (1.0 - self.alpha) * p_est[j] + self.alpha * observed
        self.p_j = p_est
        if rounds_completed >= 1:
            last_my_action = bool(history.my_actions[-1])
            last_op_actions = history.opponent_actions[-1]
            opp_coops_last = int(np.sum(last_op_actions))
            total_coops_last = opp_coops_last + (1 if last_my_action else 0)
            if last_my_action and total_coops_last < m:
                self.E_count += 1
                pun_len = math.ceil(self.beta * self.E_count)
                self.punishment_remaining = min(remaining_rounds, pun_len)
                self.success_streak_no_coop = 0
            else:
                if not last_my_action and total_coops_last >= m:
                    self.success_streak_no_coop += 1
                else:
                    self.success_streak_no_coop = 0
                if self.success_streak_no_coop >= self.deescalate_streak_threshold and self.E_count > 0:
                    self.E_count = max(0, self.E_count - 1)
                    self.success_streak_no_coop = 0
                    self.punishment_remaining = min(self.punishment_remaining, remaining_rounds)
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            return Action.D
        target = m - 1
        if target < 0 or target > n_opponents:
            P_piv = 0.0
        else:
            probs = [0.0] * (n_opponents + 1)
            probs[0] = 1.0
            for p in self.p_j:
                for s in range(n_opponents - 1, -1, -1):
                    probs[s + 1] += probs[s] * p
                    probs[s] *= 1.0 - p
            P_piv = probs[target]
        threshold = 1.0 / k + self.epsilon
        if P_piv > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_18(BaseStrategy):
    """
    Aggressive Pivotal Exploiter (APE)

    - Defects on the first round.
    - Estimates per-opponent cooperation probabilities from history with a small
      Bayesian prior (alpha).
    - Computes the exact Poisson-Binomial probability that exactly (m-1)
      opponents cooperate (so the agent would be pivotal).
    - Cooperates only if k * P_exact > 1 + beta, where beta biases toward
      defection and is increased in the final rounds. Small random flips
      (eps_rand) are applied for unpredictability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.1
        self.beta_base = 0.02
        self.eps_rand = 0.02
        self.tie_tol = 1e-06
        if self.game_description.n_rounds <= 2:
            self.beta_base *= 2.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = history.round_number + 1
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if m > n or m <= 0:
            return Action.D
        num_past = history.round_number
        if history.opponent_actions is None:
            return Action.D
        try:
            _, n_opponents = history.opponent_actions.shape
        except Exception:
            return Action.D
        if n_opponents != n - 1:
            return Action.D
        coop_counts = np.sum(history.opponent_actions, axis=0).astype(float)
        denom = float(num_past) + 2.0 * float(self.alpha)
        q_list = (coop_counts + float(self.alpha)) / denom
        q_list = np.clip(q_list, 0.0, 1.0)
        target = m - 1
        if target < 0 or target > n_opponents:
            return Action.D
        try:
            p = np.zeros(n_opponents + 1, dtype=float)
            p[0] = 1.0
            for q in q_list:
                qf = float(q)
                if qf <= 0.0:
                    continue
                if qf >= 1.0:
                    p[1:] = p[:-1]
                    p[0] = 0.0
                    continue
                prev = p.copy()
                p[0] = prev[0] * (1.0 - qf)
                for s in range(1, n_opponents + 1):
                    p[s] = prev[s] * (1.0 - qf) + prev[s - 1] * qf
            P_exact = float(p[target]) if 0 <= target <= n_opponents else 0.0
            if not 0.0 <= P_exact <= 1.0 or np.isnan(P_exact):
                return Action.D
        except Exception:
            return Action.D
        remaining = r - t + 1
        beta = self.beta_base * (1.0 + 5.0 * (1.0 if remaining <= 2 else 0.0))
        metric = k * P_exact
        threshold = 1.0 + beta
        if abs(metric - threshold) < self.tie_tol:
            action = Action.D
        elif metric > threshold:
            action = Action.C
        else:
            action = Action.D
        if random.random() < self.eps_rand:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_19(BaseStrategy):
    """
    Aggressive Pivotal-Exploit (APE)

    - Default: defect.
    - Exploit when community reward will be achieved without you.
    - Contribute only when plausibly pivotal and cooperating has positive expected payoff:
        cooperate iff k * delta > 1  where delta = Prob(exactly m-1 other cooperators).
    - Detect being exploited (you cooperated, group succeeded, at least one other defected)
      in a recent window and if exploit_count >= threshold enter grim punish_mode (defect forever).
    - Always defect in the final round.
    - Uses Beta(1,1) smoothing on recent window (W = min(10, r)) for opponent cooperation rates.
    - Computes exact poisson-binomial distribution via convolution (DP).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None:
            completed = 0
        else:
            completed = int(history.round_number)
        t = completed + 1
        remaining = r - t + 1
        if remaining <= 1:
            return Action.D
        W = min(10, r)
        if history is not None and completed > 0:
            start_idx = max(0, completed - W)
            exploit_count = 0
            opp_acts = history.opponent_actions
            my_acts = history.my_actions
            for idx in range(start_idx, completed):
                my_coop = bool(my_acts[idx])
                opp_coop_count = int(np.sum(opp_acts[idx, :]))
                total_coop = opp_coop_count + (1 if my_coop else 0)
                success = total_coop >= m
                some_other_defected = opp_coop_count < n - 1
                if my_coop and success and some_other_defected:
                    exploit_count += 1
            if r <= 10:
                exploit_threshold = 1
            else:
                exploit_threshold = math.ceil(0.2 * W)
                if exploit_threshold < 1:
                    exploit_threshold = 1
            if exploit_count >= exploit_threshold:
                self.punish_mode = True
        if self.punish_mode:
            return Action.D
        if history is None or completed == 0:
            p_js = np.full(n - 1, 0.5, dtype=float)
        else:
            recent_rounds = min(W, completed)
            start_idx = completed - recent_rounds
            opp_acts = history.opponent_actions
            recent_window = opp_acts[start_idx:completed, :] if recent_rounds > 0 else np.empty((0, n - 1))
            coop_counts = np.sum(recent_window, axis=0) if recent_rounds > 0 else np.zeros(n - 1, dtype=float)
            denom = recent_rounds + 2
            p_js = (coop_counts + 1.0) / denom
            p_js = np.clip(np.array(p_js, dtype=float), 0.0, 1.0)
        others = n - 1
        probs = np.zeros(others + 1, dtype=float)
        probs[0] = 1.0
        for p in p_js:
            new = np.zeros_like(probs)
            new[0] = probs[0] * (1.0 - p)
            for s in range(1, others + 1):
                new[s] = probs[s] * (1.0 - p) + probs[s - 1] * p
            probs = new
        if m <= others:
            P_without = float(np.sum(probs[m:]))
        else:
            P_without = 0.0
        if 0 <= m - 1 <= others:
            delta = float(probs[m - 1])
        else:
            delta = 0.0
        if P_without >= 0.999:
            return Action.D
        if k * delta > 1.0:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_21(BaseStrategy):
    """
    Aggressive pivotality-based strategy for the Collective Risk Dilemma.
    Defect by default; cooperate only when your single contribution is
    sufficiently likely to be pivotal (P(S_{-i}=m-1) > (1 + bias)/k).
    Uses EMA per-opponent estimates and tiny probing early on.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.6
        self.prior_p = 0.2
        self.bias = 0.01
        self.eps_probe = 0.02
        self.probe_rounds = min(3, max(0, self.r - 1))
        self.p_js = np.full(max(0, self.n - 1), self.prior_p, dtype=float)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            if random.random() < self.eps_probe:
                return Action.C
            else:
                return Action.D
        rounds_completed = history.round_number
        if rounds_completed >= 1:
            last_actions = np.asarray(history.opponent_actions[-1, :], dtype=float)
            expected_len = max(0, self.n - 1)
            if last_actions.shape[0] != expected_len:
                resized = np.full(expected_len, self.prior_p, dtype=float)
                limit = min(expected_len, last_actions.shape[0])
                if limit > 0:
                    resized[:limit] = last_actions[:limit]
                last_actions = resized
            for j in range(len(self.p_js)):
                obs = 1.0 if last_actions[j] else 0.0
                self.p_js[j] = (1.0 - self.alpha) * self.p_js[j] + self.alpha * obs
        if rounds_completed <= self.probe_rounds and random.random() < self.eps_probe:
            return Action.C
        m_minus = self.m - 1
        if m_minus < 0 or m_minus > max(0, self.n - 1):
            P_piv = 0.0
        else:
            dp_len = max(1, self.n)
            prob = [0.0] * dp_len
            prob[0] = 1.0
            for p in self.p_js:
                for s in range(dp_len - 1, 0, -1):
                    prob[s] = prob[s] * (1.0 - p) + prob[s - 1] * p
                prob[0] = prob[0] * (1.0 - p)
            if 0 <= m_minus < len(prob):
                P_piv = float(prob[m_minus])
            else:
                P_piv = 0.0
        decision_score = self.k * P_piv - 1.0 - self.bias
        if decision_score > 0.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_22(BaseStrategy):
    """
    Pivotal Exploiter (PE)

    Aggressive, adaptive exploiter:
    - Defaults to defect.
    - Maintains per-opponent exponential recency estimates of cooperation.
    - Cooperates only when estimated probability that exactly (m-1) others cooperate
      exceeds 1/k (with a mild endgame scaling).
    - Never cooperates in the very last round.
    - If opponents display sustained high cooperation (p_bar >= T_high) enters a short
      exploitation window (E_max rounds) of forced-defection (still allows cooperation
      if the pivotal test strongly indicates cooperating is beneficial).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.2
        self.p0 = 0.1
        self.T_high = 0.6
        self.E_max = 3
        self.gamma = 0.2
        n = max(2, int(self.game_description.n_players))
        self.n_minus_1 = n - 1
        self.p_js = np.full(self.n_minus_1, float(self.p0), dtype=float)
        self.last_seen_history_rounds = 0
        self.exploitation_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None:
            self.last_seen_history_rounds = 0
            if state.round_number >= r:
                return Action.D
            return Action.D
        completed = int(history.round_number)
        completed = min(completed, history.opponent_actions.shape[0])
        for idx in range(self.last_seen_history_rounds, completed):
            round_ops = history.opponent_actions[idx, :]
            if round_ops.shape[0] != self.n_minus_1:
                if round_ops.shape[0] > self.n_minus_1:
                    round_ops = round_ops[:self.n_minus_1]
                else:
                    pad = np.zeros(self.n_minus_1 - round_ops.shape[0], dtype=bool)
                    round_ops = np.concatenate([round_ops, pad])
            coop_array = round_ops.astype(float)
            self.p_js = (1.0 - self.alpha) * self.p_js + self.alpha * coop_array
            self.p_js = np.minimum(np.maximum(self.p_js, 0.0), 1.0)
            p_bar_now = float(np.mean(self.p_js)) if self.n_minus_1 > 0 else 0.0
            if p_bar_now >= self.T_high and self.exploitation_counter == 0:
                self.exploitation_counter = int(self.E_max)
            if self.exploitation_counter > 0:
                self.exploitation_counter -= 1
        self.last_seen_history_rounds = completed
        t = int(state.round_number)
        if t >= r:
            return Action.D
        p_bar = float(np.mean(self.p_js)) if self.n_minus_1 > 0 else 0.0
        target = max(0, m - 1)
        nm1 = self.n_minus_1
        P_piv = 0.0
        if 0 <= target <= nm1:
            comb = math.comb(nm1, target)
            p_pow = p_bar ** target if not (p_bar == 0.0 and target == 0) else 1.0
            q_pow = (1.0 - p_bar) ** (nm1 - target) if not (1.0 - p_bar == 0.0 and nm1 - target == 0) else 1.0
            P_piv = comb * p_pow * q_pow
        L = r - t + 1
        endgame_scale = 1.0 + float(self.gamma) * (1.0 - float(L) / float(r))
        threshold = 1.0 / k * endgame_scale
        intend_cooperate = False
        if P_piv > threshold:
            intend_cooperate = True
        else:
            intend_cooperate = False
        return Action.C if intend_cooperate else Action.D

class Strategy_AGGRESSIVE_23(BaseStrategy):
    """
    Pivotal Poisson-Binomial Aggressive Strategy.

    - Maintains per-opponent exponential moving estimates p_j of cooperation.
    - In each round computes P_exact = Prob(sum_{j!=i} Bernoulli(p_j) == m-1) via
      a Poisson-binomial dynamic program.
    - Cooperates only if (except for rare exploration) P_exact > 1/k (tie -> defect).
    - Small exploration probability eps to probe; optionally disabled in last round.
    - Updates p_j after observing each round using p_j <- (1-lambda)*p_j + lambda*I[coop].
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.lambda_lr = 0.4
        self.p0 = 0.1
        self.eps = 0.02
        n_players = int(self.game_description.n_players)
        self.n_opponents = max(0, n_players - 1)
        self.p_j = [self.p0 for _ in range(self.n_opponents)]
        self.observed_rounds = 0

    def _poisson_binomial_prob_exact(self, probs, target):
        """
        Compute probability that sum of independent Bernoulli(probs) equals target.
        probs: iterable of probabilities for each Bernoulli variable
        target: integer target sum
        Returns float probability.
        """
        n = len(probs)
        if target < 0 or target > n:
            return 0.0
        dp = [0.0] * (n + 1)
        dp[0] = 1.0
        for p in probs:
            for s in range(n - 1, -1, -1):
                prob_s = dp[s]
                if prob_s != 0.0:
                    dp[s + 1] += prob_s * p
                    dp[s] = prob_s * (1.0 - p)
                else:
                    dp[s] = 0.0
        return dp[target]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is not None:
            total_rounds = history.round_number
            while self.observed_rounds < total_rounds:
                r = self.observed_rounds
                if 0 <= r < history.opponent_actions.shape[0]:
                    row = history.opponent_actions[r, :]
                    length = min(self.n_opponents, row.shape[0])
                    for j in range(length):
                        observed_c = 1.0 if bool(row[j]) else 0.0
                        self.p_j[j] = (1.0 - self.lambda_lr) * self.p_j[j] + self.lambda_lr * observed_c
                self.observed_rounds += 1
        last_round = False
        try:
            last_round = state.round_number == int(self.game_description.n_rounds)
        except Exception:
            last_round = False
        eps_local = 0.0 if last_round else float(self.eps)
        if eps_local > 0.0 and random.random() < eps_local:
            return Action.C
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        target = m - 1
        if target < 0 or target > self.n_opponents:
            P_exact = 0.0
        else:
            P_exact = self._poisson_binomial_prob_exact(self.p_j, target)
        threshold = 1.0 / k if k != 0 else float('inf')
        if P_exact - threshold > 1e-12:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_24(BaseStrategy):
    """
    Aggressive Conditional Swing (ACS):
    Defect by default, exploit stable cooperators, and only cooperate when
    your single cooperation has a data-driven chance to be the decisive swing (k * Pr(others == m-1) > 1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_default = 20
        self.alpha = 1
        self.beta = 1
        self.exploit_threshold_gamma = 0.9

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if t >= r:
            return Action.D
        completed = int(history.round_number)
        if completed == 0:
            return Action.D
        N = min(self.W_default, completed)
        window_op_actions = history.opponent_actions[-N:, :]
        others_coops_array = np.sum(window_op_actions, axis=1).astype(int)
        others_coops_prev = int(np.sum(history.opponent_actions[-1, :]))
        if others_coops_prev >= m:
            return Action.D
        s_count = int(np.sum(others_coops_array == m - 1))
        alpha = float(self.alpha)
        beta = float(self.beta)
        p_hat = (s_count + alpha) / (N + alpha + beta)
        p_without = float(np.sum(others_coops_array >= m)) / float(N)
        p_with = float(np.sum(others_coops_array >= m - 1)) / float(N)
        coop_counts = np.sum(window_op_actions, axis=0)
        coop_rate = coop_counts / float(N)
        stable_cooperator_count = int(np.sum(coop_rate >= self.exploit_threshold_gamma))
        if stable_cooperator_count >= m:
            return Action.D
        if k * p_hat > 1.0:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_25(BaseStrategy):
    """
    Pivotal Exploiter with Focused Rescue (PEFR)

    Aggressive opportunistic strategy:
    - Default to defect (D).
    - Cooperate (C) only when (a) others are unlikely to meet the threshold without you
      but are likely to be at m-1 (you are likely pivotal), or (b) a recent missed pivotal
      opportunity (rescue) occurred and the pivotal probability is above a lower bar.
    - Never cooperate in the first round or the last round.
    - Estimates opponent cooperation probabilities from a recent window and computes
      exact Poisson-Binomial probabilities by dynamic programming.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if t == 1:
            return Action.D
        if t == r:
            return Action.D
        L = min(6, t - 1)
        P0_threshold = 0.6
        P1_threshold = 0.5
        S = min(3, t - 1)
        Rescue_P1_min = 0.3
        num_opponents = n - 1
        p_list = []
        if L <= 0:
            p_list = [0.5] * num_opponents
        else:
            for j in range(num_opponents):
                recent = history.opponent_actions[-L:, j]
                coop_count = int(np.sum(recent))
                p_j = float(coop_count) / float(L)
                p_list.append(p_j)
        prob = [0.0] * (num_opponents + 1)
        prob[0] = 1.0
        current_max = 0
        for p in p_list:
            for s in range(current_max, -1, -1):
                prob[s + 1] += prob[s] * p
                prob[s] *= 1.0 - p
            current_max += 1
        if m <= num_opponents:
            P0 = float(sum((prob[s] for s in range(m, num_opponents + 1))))
        else:
            P0 = 0.0
        lower = max(0, m - 1)
        if lower <= num_opponents:
            P1 = float(sum((prob[s] for s in range(lower, num_opponents + 1))))
        else:
            P1 = 0.0
        rescue = False
        if S >= 1:
            rounds_completed = history.round_number
            start_idx = max(0, rounds_completed - S)
            end_idx = rounds_completed
            for idx in range(start_idx, end_idx):
                others_coops = int(np.sum(history.opponent_actions[idx, :]))
                my_act = bool(history.my_actions[idx])
                if others_coops == m - 1 and (not my_act):
                    rescue = True
                    break
        if P0 >= P0_threshold:
            return Action.D
        if P1 >= P1_threshold:
            return Action.C
        if rescue and P1 >= Rescue_P1_min:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_26(BaseStrategy):
    """
    Aggressive Collective Risk strategy.

    - Default: defect (first and last rounds defect).
    - Cooperate only when your cooperation is likely pivotal (Pr[others = m-1] > 1/k),
      with smoothing and conservative bias when little data.
    - If probability others >= m is very high (>= 0.8), defect to free-ride.
    - Detect stable cooperation streaks of length L and "sabotage" them for S rounds
      (defect during sabotage windows).
    - Track wasted cooperations (you cooperated but group failed). After wasted_count_limit
      wasted cooperations, switch to permanent grudge (always defect).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 1.0
        self.sabotage_streak_threshold = 3
        self.sabotage_duration = 2
        self.wasted_count_limit = 2
        self.grudge_on_wasted_coop = True
        self.P_ge_threshold = 0.8
        self.confidence_rounds = 3
        self.early_extra_eps = 0.02
        self.eps = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_observed = int(history.round_number)
        current_round = int(state.round_number)
        others = self.n - 1
        if current_round >= self.r:
            return Action.D
        if rounds_observed > 0:
            coop_count_per_opponent = np.sum(history.opponent_actions, axis=0).astype(float)
        else:
            coop_count_per_opponent = np.zeros(others, dtype=float)
        wasted_coop_count = 0
        permanent_grudge = False
        if rounds_observed > 0:
            past_my = history.my_actions.astype(int)
            past_opp_sums = np.sum(history.opponent_actions.astype(int), axis=1)
            past_total_coops = past_opp_sums + past_my
            wasted_mask = (past_my == 1) & (past_total_coops < self.m)
            wasted_coop_count = int(np.sum(wasted_mask))
            if self.grudge_on_wasted_coop and wasted_coop_count >= self.wasted_count_limit:
                permanent_grudge = True
        else:
            wasted_coop_count = 0
            permanent_grudge = False
        if permanent_grudge:
            return Action.D
        consecutive_successes_ending = []
        if rounds_observed > 0:
            past_my = history.my_actions.astype(int)
            past_opp_sums = np.sum(history.opponent_actions.astype(int), axis=1)
            past_total_coops = past_opp_sums + past_my
            success_mask = past_total_coops >= self.m
            run = 0
            for s in success_mask:
                if s:
                    run += 1
                else:
                    run = 0
                consecutive_successes_ending.append(run)
        else:
            consecutive_successes_ending = []
        sabotage_active = False
        current_idx = rounds_observed
        L = self.sabotage_streak_threshold
        S = self.sabotage_duration
        for r_index, run_len in enumerate(consecutive_successes_ending):
            if run_len >= L:
                start_idx = r_index + 1
                end_idx = r_index + S
                if start_idx <= current_idx <= end_idx:
                    sabotage_active = True
                    break
        if sabotage_active:
            return Action.D
        denom = rounds_observed + 2.0 * self.alpha
        if denom <= 0:
            denom = 2.0
        p_list = []
        for j in range(others):
            p_j = (float(coop_count_per_opponent[j]) + self.alpha) / denom
            if p_j < 0.0:
                p_j = 0.0
            elif p_j > 1.0:
                p_j = 1.0
            p_list.append(float(p_j))
        P_eq = 0.0
        P_ge = 0.0
        target_k = self.m - 1
        max_exact_n = 60
        if others <= 0:
            if target_k == 0:
                P_eq = 1.0
            else:
                P_eq = 0.0
            P_ge = 1.0 if target_k < 0 else 0.0
        elif others <= max_exact_n:
            dp = [0.0] * (others + 1)
            dp[0] = 1.0
            for p in p_list:
                new_dp = [0.0] * (others + 1)
                for k in range(0, others + 1):
                    new_dp[k] += dp[k] * (1.0 - p)
                    if k + 1 <= others:
                        new_dp[k + 1] += dp[k] * p
                dp = new_dp
            if 0 <= target_k <= others:
                P_eq = dp[target_k]
            else:
                P_eq = 0.0
            P_ge = sum(dp[self.m:]) if self.m <= others else 0.0
        else:
            mean = sum(p_list)
            var = sum([p * (1.0 - p) for p in p_list])
            std = math.sqrt(max(var, 1e-12))

            def normal_cdf(x):
                return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
            if 0 <= target_k <= others:
                z1 = (target_k - 0.5 - mean) / std
                z2 = (target_k + 0.5 - mean) / std
                P_eq = max(0.0, normal_cdf(z2) - normal_cdf(z1))
            else:
                P_eq = 0.0
            z_ge = (self.m - 0.5 - mean) / std
            P_ge = max(0.0, 1.0 - normal_cdf(z_ge))
        if P_ge >= self.P_ge_threshold:
            return Action.D
        pivot_threshold = 1.0 / max(1.0, self.k)
        extra_eps = 0.0
        if rounds_observed < self.confidence_rounds:
            extra_eps = self.early_extra_eps
        if P_eq > pivot_threshold + extra_eps + self.eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_27(BaseStrategy):
    """
    Aggressive Pivotal Defector (APD)

    - Default = Defect.
    - Cooperates only when a single cooperation by self is plausibly pivotal
      and historical evidence indicates others are likely to cooperate.
    - If ever cooperated and the threshold still failed (sucker), enter a
      punishment phase of length F rounds.
    - Exploits rounds where others will meet the threshold without me.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.L = min(5, max(1, self.r - 1))
        self.theta = 0.6
        self.F = max(2, math.ceil(self.r / 4))
        self.punish_until = 0
        self.exploitation_count = 0
        self._last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is not None and history.round_number > self._last_processed_round:
            last_completed = history.round_number
            idx = last_completed - 1
            if idx >= 0:
                my_action_last = bool(history.my_actions[idx])
                opp_last = history.opponent_actions[idx, :] if history.opponent_actions.size > 0 else np.array([], dtype=bool)
                total_cooperators = int(my_action_last) + int(np.sum(opp_last))
                threshold_met = total_cooperators >= self.m
                if my_action_last and (not threshold_met):
                    self.punish_until = max(self.punish_until, last_completed + self.F)
                    self.exploitation_count = 0
                if my_action_last and threshold_met:
                    opp_defectors = int(np.sum(~opp_last))
                    if opp_defectors >= 1:
                        self.exploitation_count += 1
                        if self.exploitation_count >= 2:
                            self.punish_until = max(self.punish_until, last_completed + self.F)
            self._last_processed_round = history.round_number
        if t == self.r:
            return Action.D
        if t == 1:
            return Action.D
        if self.punish_until >= t and t <= self.r - self.F:
            return Action.D
        if history is None:
            return Action.D
        available_rounds = history.round_number
        last_k = min(self.L, available_rounds)
        n_opponents = self.n - 1
        if n_opponents <= 0:
            return Action.D
        p_list = []
        if last_k <= 0:
            p_list = [0.0] * n_opponents
        else:
            recent = history.opponent_actions[-last_k:, :]
            cooperations = np.sum(recent, axis=0)
            p_list = [float(cooperations[j]) / float(last_k) for j in range(n_opponents)]
        E = float(sum(p_list))
        if available_rounds >= 1:
            C_last = int(np.sum(history.opponent_actions[-1, :]))
            last_round_played = history.opponent_actions[-1, :]
        else:
            C_last = 0
            last_round_played = np.array([False] * n_opponents, dtype=bool)
        count_high = 0
        for j in range(n_opponents):
            played_last = bool(last_round_played[j]) if last_round_played.size > 0 else False
            if p_list[j] >= self.theta or played_last:
                count_high += 1
        if E >= self.m:
            return Action.D
        if E <= self.m - 2:
            return Action.D
        if E >= self.m - 1 and E < self.m:
            if count_high >= self.m - 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_28(BaseStrategy):
    """
    Aggro-Pivotal strategy for the Collective Risk Dilemma.

    - Default: defect.
    - Cooperate only when a single extra cooperation from self is likely to flip the round
      from failing to succeeding (pivotal).
    - Maintain per-opponent short-window frequency estimates with a small prior.
    - Small exploration probability to occasionally probe (default eps=0.02).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.L = 5
        self.p0 = 0.1
        self.eps = 0.02

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = float(self.game_description.m)
        n_opponents = max(0, n - 1)
        if history is None:
            p_js = np.full((n_opponents,), float(self.p0), dtype=float)
            E_no_me = float(np.sum(p_js))
            if random.random() < self.eps:
                return Action.C
            if E_no_me >= m:
                return Action.D
            if E_no_me + 1 >= m:
                return Action.C
            return Action.D
        T = int(history.round_number)
        if n_opponents == 0:
            E_no_me = 0.0
        else:
            if T == 0:
                p_js = np.full((n_opponents,), float(self.p0), dtype=float)
            else:
                window = min(self.L, T)
                recent = history.opponent_actions[-window:, :]
                counts = np.sum(recent, axis=0).astype(float)
                p_js = (counts + float(self.p0)) / float(window + 1)
            E_no_me = float(np.sum(p_js))
        if random.random() < self.eps:
            return Action.C
        if E_no_me >= m:
            return Action.D
        if E_no_me + 1 >= m:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_29(BaseStrategy):
    """
    Aggressive selfish-pivotal strategy for the Collective Risk Dilemma.

    - Defaults to defecting to keep the private unit.
    - Estimates each opponent's cooperation probability from observed history with a small prior.
    - Computes the probability that exactly m-1 opponents cooperate (pivotal event).
    - Cooperates only if k * P_pivotal > 1 (strict), otherwise defects.
    - Allows a tiny exploration probability to probe opponents early.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.prior_p = 0.1
        self.prior_weight = 1.0
        self.epsilon_explore = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.n
        n_others = max(0, n_players - 1)
        if history is None or history.round_number == 0:
            counts_C = [0] * n_others
            counts_rounds = 0
        else:
            opp_actions = np.asarray(history.opponent_actions)
            if opp_actions.size == 0:
                counts_C = [0] * n_others
                counts_rounds = 0
            else:
                counts_per_op = np.sum(opp_actions.astype(float), axis=0)
                counts_C = [int(x) for x in counts_per_op.tolist()]
                counts_rounds = history.round_number
        pjs = []
        for j in range(n_others):
            count_Cj = counts_C[j] if j < len(counts_C) else 0
            denom = float(counts_rounds) + float(self.prior_weight)
            if denom <= 0.0:
                pj = float(self.prior_p)
            else:
                pj = (float(count_Cj) + float(self.prior_weight) * float(self.prior_p)) / denom
            if pj < 0.0:
                pj = 0.0
            elif pj > 1.0:
                pj = 1.0
            pjs.append(float(pj))
        if n_others <= 0:
            P_piv = 0.0
        else:
            probs = [0.0] * (n_others + 1)
            probs[0] = 1.0
            for pj in pjs:
                new_probs = [0.0] * (n_others + 1)
                for y in range(0, n_others + 1):
                    if probs[y] == 0.0:
                        continue
                    new_probs[y] += probs[y] * (1.0 - pj)
                    if y + 1 <= n_others:
                        new_probs[y + 1] += probs[y] * pj
                probs = new_probs
            idx = self.m - 1
            if 0 <= idx <= n_others:
                P_piv = float(probs[idx])
            else:
                P_piv = 0.0
        if random.random() < float(self.epsilon_explore):
            return Action.C
        if float(self.k) * float(P_piv) > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_30(BaseStrategy):
    """
    Aggressive Threshold Enforcer (ATE)

    - Cooperate only when your cooperation is needed (or likely needed) to reach the threshold.
    - Defect to exploit when others are predicted to form the coalition without you.
    - If a past round failed by exactly one cooperator and some previously-reliable opponent
      defected in that round, start a short punishment phase (defect for L rounds),
      but never allow punishment to run through the final round.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(5, self.r)
        self.L = max(1, math.ceil(self.r / 4.0))
        self.punish_timer = 0
        self._last_round_processed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', None)
        if t is None:
            if history is None:
                t = 1
            else:
                t = history.round_number + 1
        if history is None or history.round_number == 0:
            return Action.D
        if history.round_number > self._last_round_processed:
            rounds_completed = history.round_number
            num_obs_before = min(self.W, max(0, rounds_completed - 1))
            if num_obs_before > 0:
                ops_before = history.opponent_actions[-num_obs_before - 1:-1 if num_obs_before >= 1 else None, :]
                rates_before = np.sum(ops_before, axis=0) / float(num_obs_before)
            else:
                rates_before = np.zeros((history.opponent_actions.shape[1],), dtype=float)
            last_my_action = bool(history.my_actions[-1])
            last_op_actions = history.opponent_actions[-1, :]
            total_cooperators_last = int(last_my_action) + int(np.sum(last_op_actions))
            if total_cooperators_last == self.m - 1:
                defector_mask = np.logical_not(last_op_actions)
                if defector_mask.any():
                    if rates_before.shape[0] == defector_mask.shape[0]:
                        offending = np.logical_and(defector_mask, rates_before >= 0.5)
                        if offending.any():
                            remaining_rounds = self.r - (t - 1)
                            max_punish = max(0, remaining_rounds - 1)
                            if max_punish > 0:
                                self.punish_timer = min(self.L, max_punish)
            self._last_round_processed = history.round_number
        if t == self.r:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        rounds_available = history.round_number
        num_obs_decision = min(self.W, rounds_available)
        if num_obs_decision > 0:
            ops_slice = history.opponent_actions[-num_obs_decision:, :]
            cooperation_rates = np.sum(ops_slice, axis=0) / float(num_obs_decision)
        else:
            cooperation_rates = np.zeros((history.opponent_actions.shape[1],), dtype=float)
        predicted_others = float(np.sum(cooperation_rates))
        if abs(predicted_others - (self.m - 0.5)) < 1e-09:
            return Action.C
        if predicted_others >= float(self.m):
            return Action.D
        if predicted_others < float(self.m - 1):
            return Action.D
        return Action.C

class Strategy_AGGRESSIVE_31(BaseStrategy):
    """
    Aggressive pivotal free-rider strategy for the Collective Risk Dilemma.

    - Maintains per-opponent EMA estimates of cooperation probability (p_j).
    - Each round computes Pm1 = Pr[#others == m-1] via Poisson–Binomial DP using p_j.
    - Cooperates iff k * Pm1 - 1 > 0 (i.e. Pm1 > 1/k). Tie-break: defect.
    - Uses small, decaying exploration (probabilistic probing) in early rounds or when estimates are unstable.
    - EMA updates are applied incrementally from observed history to avoid double updates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        self.n_opp = max(0, n - 1)
        self.p0 = 0.1
        self.lambda_ = 0.35
        self.epsilon_floor = 0.01
        self.epsilon_cap = 0.05
        self.T_learn = 4
        self.p_estimates = [self.p0 for _ in range(self.n_opp)]
        self.up_to_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):

        def epsilon_for_round(t: int) -> float:
            return min(self.epsilon_cap, max(self.epsilon_floor, 1.0 / (t + 1.0)))

        def poisson_binomial_pmf(ps, target):
            mlen = len(ps)
            if target < 0 or target > mlen:
                return 0.0
            dp = [0.0] * (mlen + 1)
            dp[0] = 1.0
            for p in ps:
                for k in range(mlen, -1, -1):
                    without = dp[k] * (1.0 - p)
                    withj = dp[k - 1] * p if k >= 1 else 0.0
                    dp[k] = without + withj
            return dp[target]
        if history is None:
            t = 1
            if self.game_description.m > self.game_description.n_players:
                return Action.D
            if random.random() < epsilon_for_round(t):
                return Action.C
            return Action.D
        rounds_completed = int(history.round_number)
        t = rounds_completed + 1
        if rounds_completed > self.up_to_round:
            for rr in range(self.up_to_round, rounds_completed):
                row = history.opponent_actions[rr, :]
                if self.n_opp == 0:
                    break
                for j in range(self.n_opp):
                    seen = 1.0 if bool(row[j]) else 0.0
                    self.p_estimates[j] = (1.0 - self.lambda_) * self.p_estimates[j] + self.lambda_ * seen
            self.up_to_round = rounds_completed
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if m > n:
            return Action.D
        ps = list(self.p_estimates)
        target = m - 1
        Pm1 = poisson_binomial_pmf(ps, target)
        unstable = False
        if rounds_completed <= self.T_learn:
            unstable = True
        elif self.n_opp > 0:
            center_frac = sum((1 for p in ps if 0.4 <= p <= 0.6)) / float(self.n_opp)
            if center_frac > 0.5:
                unstable = True
        eps = epsilon_for_round(t)
        if unstable and random.random() < eps:
            return Action.C
        threshold = 1.0 / k if k > 0 else float('inf')
        if Pm1 > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_32(BaseStrategy):
    """
    Aggressive Pivotal Free-rider with Targeted Punishment (APFTP)

    - Default defecting, freerides when the group likely reaches the threshold without you.
    - Will cooperate if your single contribution is likely to be pivotal (P_pivotal >= tau_piv).
    - Cooperates when expected-value of cooperating exceeds defecting by at least theta.
    - Enforces short, targeted punishments (T_punish rounds) if recent other cooperation is well below
      the required fraction.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_left = 0
        self.p0 = 0.2
        self.theta = 0.25
        self.tau_piv = 0.15
        self.R_check = 3
        self.T_punish = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if t == r:
            return Action.D
        if self.punish_left > 0:
            self.punish_left = max(0, self.punish_left - 1)
            return Action.D

        def _other_coop_counts_over_range(start_idx_inclusive: int, end_idx_inclusive: int):
            if history is None:
                return 0
            opp_slice = history.opponent_actions[start_idx_inclusive:end_idx_inclusive + 1, :]
            if opp_slice.size == 0:
                return 0
            return int(np.sum(opp_slice))
        if history is None or history.round_number == 0:
            p_hat = self.p0
        else:
            W = min(20, max(0, t - 1))
            W_eff = min(W, history.round_number)
            if W_eff <= 0:
                p_hat = self.p0
            else:
                start_idx = history.round_number - W_eff
                end_idx = history.round_number - 1
                total_other_coops = _other_coop_counts_over_range(start_idx, end_idx)
                denom = float(W_eff * (n - 1))
                if denom <= 0.0:
                    p_hat = self.p0
                else:
                    p_hat = total_other_coops / denom
                    if p_hat <= 0.0:
                        p_hat = max(p_hat, 0.01)
        n_other = n - 1

        def binom_pmf(j: int, N: int, p: float) -> float:
            if j < 0 or j > N:
                return 0.0
            comb = math.comb(N, j)
            return comb * p ** j * (1.0 - p) ** (N - j)
        lower_C = m - 1
        P_success_if_C = 0.0
        if lower_C <= n_other:
            for j in range(max(0, lower_C), n_other + 1):
                P_success_if_C += binom_pmf(j, n_other, p_hat)
        lower_D = m
        P_success_if_D = 0.0
        if lower_D <= n_other:
            for j in range(max(0, lower_D), n_other + 1):
                P_success_if_D += binom_pmf(j, n_other, p_hat)
        P_pivotal = 0.0
        if 0 <= m - 1 <= n_other:
            P_pivotal = binom_pmf(m - 1, n_other, p_hat)
        E_C = k * P_success_if_C
        E_D = 1.0 + k * P_success_if_D
        action = Action.D
        if P_pivotal >= self.tau_piv:
            action = Action.C
        elif E_C >= E_D + self.theta:
            action = Action.C
        else:
            action = Action.D
        if history is not None and history.round_number >= 1:
            recent = min(self.R_check, history.round_number)
            if recent >= 1:
                start_idx = history.round_number - recent
                end_idx = history.round_number - 1
                total_recent_other_coops = _other_coop_counts_over_range(start_idx, end_idx)
                recent_other_frac = float(total_recent_other_coops) / float(recent * (n - 1))
                c_frac = float(m) / float(n)
                if recent_other_frac < 0.6 * c_frac:
                    self.punish_left = int(self.T_punish)
        return action

class Strategy_AGGRESSIVE_33(BaseStrategy):
    """
    Aggressive Coalition-Formation (ACF)

    Aggressively forms small reliable coalitions to reach the threshold, exploits
    coalitions when not pivotal, punishes betrayers with a temporary global
    defection phase and blacklisting, and allows slow rehabilitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(20, self.r)
        self.alpha = 0.7
        self.Pun = max(2, math.floor(self.r / 5))
        self.Rehab = max(2, math.ceil(self.m / 2))
        self.L = 1
        self.blacklist = set()
        self.punish_timer = 0
        self.punish_rounds = set()
        self.last_expected_eligible = set()
        self.last_attempt_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        completed = history.round_number
        t_next = completed + 1
        n_opponents = self.n - 1
        if self.last_attempt_round is not None and self.last_attempt_round == completed:
            last_idx = completed - 1
            we_cooperated = bool(history.my_actions[last_idx]) if completed >= 1 else False
            if we_cooperated:
                opp_coops = int(np.sum(history.opponent_actions[last_idx, :])) if completed >= 1 else 0
                total_coops = opp_coops + 1
                if total_coops < self.m:
                    betrayers = set()
                    for j in self.last_expected_eligible:
                        if 0 <= j < n_opponents:
                            acted_coop = bool(history.opponent_actions[last_idx, j])
                            if not acted_coop:
                                betrayers.add(j)
                    for b in betrayers:
                        self.blacklist.add(b)
                    if len(betrayers) > 0:
                        self.punish_timer = self.Pun
        if self.punish_timer == 0 and len(self.blacklist) > 0:
            to_unblacklist = set()
            for j in list(self.blacklist):
                consec = 0
                for r_idx in range(completed - 1, -1, -1):
                    round_num = r_idx + 1
                    if round_num in self.punish_rounds:
                        continue
                    if bool(history.opponent_actions[r_idx, j]):
                        consec += 1
                        if consec >= self.Rehab:
                            to_unblacklist.add(j)
                            break
                    else:
                        break
            for j in to_unblacklist:
                self.blacklist.discard(j)
        if t_next > self.r - self.L:
            if self.punish_timer > 0:
                self.punish_timer -= 1
                self.punish_rounds.add(t_next)
            return Action.D
        window = min(self.W, max(0, t_next - 1))
        f = [0.0] * n_opponents
        last_action = [False] * n_opponents
        if window > 0:
            start_idx = completed - window
            for j in range(n_opponents):
                coop_count = int(np.sum(history.opponent_actions[start_idx:completed, j]))
                f[j] = coop_count / float(window)
            last_idx = completed - 1
            for j in range(n_opponents):
                last_action[j] = bool(history.opponent_actions[last_idx, j])
        else:
            f = [0.0] * n_opponents
            last_action = [False] * n_opponents
        Reliable = set((j for j in range(n_opponents) if f[j] >= self.alpha or last_action[j]))
        Eligible = set((j for j in Reliable if j not in self.blacklist))
        E = len(Eligible)
        if completed >= 1:
            last_idx = completed - 1
            prev_opp_coops = int(np.sum(history.opponent_actions[last_idx, :]))
            prev_we_coop = bool(history.my_actions[last_idx])
            prev_coops = prev_opp_coops + (1 if prev_we_coop else 0)
            prev_coops_ex_me = prev_coops - (1 if prev_we_coop else 0)
        else:
            prev_coops = 0
            prev_coops_ex_me = 0
        if self.punish_timer > 0:
            self.punish_timer -= 1
            self.punish_rounds.add(t_next)
            self.last_expected_eligible = set()
            self.last_attempt_round = None
            return Action.D
        if E >= self.m:
            self.last_expected_eligible = set()
            self.last_attempt_round = None
            return Action.D
        if E >= self.m - 1:
            self.last_expected_eligible = set(Eligible)
            self.last_attempt_round = t_next
            return Action.C
        if prev_coops_ex_me >= self.m:
            self.last_expected_eligible = set()
            self.last_attempt_round = None
            return Action.D
        self.last_expected_eligible = set()
        self.last_attempt_round = None
        return Action.D

class Strategy_AGGRESSIVE_34(BaseStrategy):
    """
    Aggressive Pivotal-Exploiter with Grim-Blacklist.

    - Start by defecting.
    - Predict opponents' cooperation using a short window (W up to 5) and last-round behavior.
    - Cooperate only when you are pivotal (predicted_cooperators == m - 1).
    - Free-ride (defect) when others are expected to meet the threshold without you or when even with you
      the threshold can't be reached.
    - Maintain a blacklist: if the threshold was met in a round and some opponents defected that round,
      add those defectors to the blacklist. Blacklisted players are treated as predicted non-cooperators
      until they cooperate in `probation_length` consecutive rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rho = 0.6
        self.W_max = 5
        self.probation_length = 2
        self.blacklisted = set()
        self.coop_streaks = {}
        self._last_history_len = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.blacklisted.clear()
            self.coop_streaks.clear()
            self._last_history_len = 0
            return Action.D
        if history.round_number < self._last_history_len:
            self.blacklisted.clear()
            self.coop_streaks.clear()
            self._last_history_len = 0
        n_players = self.game_description.n_players
        m = self.game_description.m
        total_completed = history.round_number
        for r_idx in range(self._last_history_len, total_completed):
            my_action = bool(history.my_actions[r_idx])
            opp_actions_row = history.opponent_actions[r_idx, :]
            opp_coops = int(opp_actions_row.sum())
            total_cooperators = int(my_action) + opp_coops
            if total_cooperators >= m:
                for opp_idx in range(opp_actions_row.shape[0]):
                    if not bool(opp_actions_row[opp_idx]):
                        self.blacklisted.add(opp_idx)
                        self.coop_streaks[opp_idx] = 0
            for opp_idx in range(opp_actions_row.shape[0]):
                if bool(opp_actions_row[opp_idx]):
                    self.coop_streaks[opp_idx] = self.coop_streaks.get(opp_idx, 0) + 1
                else:
                    self.coop_streaks[opp_idx] = 0
                if opp_idx in self.blacklisted and self.coop_streaks.get(opp_idx, 0) >= self.probation_length:
                    self.blacklisted.remove(opp_idx)
        self._last_history_len = total_completed
        t = total_completed + 1
        if t == 1:
            return Action.D
        W = min(self.W_max, t - 1)
        opp_count = n_players - 1
        predicted_cooperators = 0
        for j in range(opp_count):
            if j in self.blacklisted:
                continue
            if W > 0:
                recent = history.opponent_actions[-W:, j]
                f_j = float(recent.sum()) / float(W)
                cooperated_last = bool(history.opponent_actions[-1, j])
            else:
                f_j = 0.0
                cooperated_last = False
            if f_j >= self.rho or cooperated_last:
                predicted_cooperators += 1
        if predicted_cooperators >= m:
            return Action.D
        elif predicted_cooperators <= m - 2:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_35(BaseStrategy):
    """
    Aggressive Pivotal Maximizer (APM)

    - Default: defect (exploit rounds where others will meet the threshold without you).
    - Cooperate only when your cooperation is likely to swing the round from failure to success
      and doing so gives you a strictly higher expected payoff than defecting.
    - Maintain per-opponent trust estimates from recent history (windowed with pseudocounts).
    - Maintain punishment counters and short blacklists for opponents who repeatedly defect
      while you cooperated and the round failed.
    - Always defect in the final round (single-shot).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.p0 = 0.1
        self.s = 1.0
        self.eps = 0.01
        self.L_max = 20
        self.Ppunish = 3
        self.punish_threshold = 2
        self.blacklist_min_p = 0.01
        self.T_end = 1
        self.q_probe = 0.0
        self.n_opponents = max(0, self.n - 1)
        self.punish_count = np.zeros(self.n_opponents, dtype=np.int64)
        self.blacklist_until = np.zeros(self.n_opponents, dtype=np.int64)
        self.last_processed_rounds = 0

    def _estimate_p_j(self, j: int, history: PlayerHistory, t: int) -> float:
        """Estimate opponent j's cooperation probability for upcoming round t (1-based)."""
        if history is None or history.round_number == 0:
            base_p = self.p0
        else:
            L = min(self.L_max, history.round_number)
            start_idx = max(0, history.round_number - L)
            recent = history.opponent_actions[start_idx:history.round_number, j]
            count_c = int(np.sum(recent))
            num_obs = recent.shape[0]
            base_p = (count_c + self.s) / (num_obs + 2.0 * self.s)
        if self.blacklist_until is not None and j < len(self.blacklist_until):
            if int(self.blacklist_until[j]) > t:
                base_p = min(base_p, self.blacklist_min_p)
        return float(base_p)

    def _pb_distribution(self, p_list):
        """Compute Poisson-Binomial distribution for sum of independent Bernoullis with probs p_list.
        Returns numpy array P where P[x] = Prob(exactly x successes).
        """
        m = len(p_list)
        P = np.zeros(m + 1, dtype=float)
        P[0] = 1.0
        for p in p_list:
            new_P = P * (1.0 - p)
            new_P[1:] += P[:-1] * p
            P = new_P
        return P

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            if self.q_probe > 0.0 and random.random() < self.q_probe:
                return Action.C
            return Action.D
        completed = history.round_number
        for r_idx in range(self.last_processed_rounds, completed):
            my_act = bool(history.my_actions[r_idx])
            opps = history.opponent_actions[r_idx, :]
            total_cooperators = int(np.sum(opps) + (1 if my_act else 0))
            if my_act and total_cooperators < self.m:
                defectors = np.logical_not(opps)
                for j in range(self.n_opponents):
                    if defectors[j]:
                        self.punish_count[j] += 1
                        if self.punish_count[j] >= self.punish_threshold:
                            new_until = t + self.Ppunish
                            if self.blacklist_until[j] < new_until:
                                self.blacklist_until[j] = new_until
                            self.punish_count[j] = 0
            else:
                for j in range(self.n_opponents):
                    if self.punish_count[j] > 0:
                        self.punish_count[j] = max(0, self.punish_count[j] - 0)
        self.last_processed_rounds = completed
        if t > self.r - self.T_end:
            return Action.D
        if t >= self.r:
            return Action.D
        p_list = []
        for j in range(self.n_opponents):
            p_j = self._estimate_p_j(j, history, t)
            p_list.append(p_j)
        P = self._pb_distribution(p_list)
        lower_m_minus_1 = max(0, self.m - 1)
        if lower_m_minus_1 <= len(P) - 1:
            prob_others_ge_m_minus_1 = float(np.sum(P[lower_m_minus_1:]))
        else:
            prob_others_ge_m_minus_1 = 0.0
        lower_m = max(0, self.m)
        if lower_m <= len(P) - 1:
            prob_others_ge_m = float(np.sum(P[lower_m:]))
        else:
            prob_others_ge_m = 0.0
        EV_C = prob_others_ge_m_minus_1 * self.k
        EV_D = prob_others_ge_m * (1.0 + self.k) + (1.0 - prob_others_ge_m) * 1.0
        if EV_C > EV_D + self.eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_36(BaseStrategy):
    """
    Exploitative Pivot Strategy (EPS).

    Summary:
    - Default: defect.
    - Cooperate only when your cooperation is likely to be the pivot that raises
      the chance of meeting the threshold by more than 1/k (i.e., Prob_exact_m_minus_1 > 1/k).
    - Occasionally probe with small probability epsilon.
    - Always defect in the initial round and the final endgame rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.epsilon = 0.03
        self.L_max = 10

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = history.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if n < 2 or r < 1 or (not 0 < k):
            return Action.D
        endgame_H = min(3, max(1, r - 1))
        if t > r - endgame_H:
            return Action.D
        if t == r:
            return Action.D
        completed_rounds = history.round_number
        if completed_rounds <= 0:
            return Action.D
        L = min(self.L_max, completed_rounds)
        if L <= 0:
            return Action.D
        opp_actions = history.opponent_actions
        recent = opp_actions[-L:, :]
        n_opponents = n - 1
        try:
            counts = np.sum(recent.astype(np.int32), axis=0)
            p_list = (counts / float(L)).tolist()
        except Exception:
            p_list = []
            for j in range(n_opponents):
                cnt = 0
                for row in range(-L, 0):
                    if opp_actions[row, j]:
                        cnt += 1
                p_list.append(cnt / float(L))
        dist = [0.0] * (n_opponents + 1)
        dist[0] = 1.0
        for p in p_list:
            newdist = [0.0] * (n_opponents + 1)
            newdist[0] = dist[0] * (1.0 - p)
            for s in range(1, n_opponents + 1):
                newdist[s] = dist[s] * (1.0 - p) + dist[s - 1] * p
            dist = newdist
        prob_exact_m_minus_1 = 0.0
        idx = m - 1
        if 0 <= idx <= n_opponents:
            prob_exact_m_minus_1 = dist[idx]
        threshold = 1.0 / float(k) if k != 0 else float('inf')
        if prob_exact_m_minus_1 > threshold:
            return Action.C
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_37(BaseStrategy):
    """
    Pivotal Aggressive: defect by default, cooperate when (predictively) pivotal, punish clear pivotal defectors for a bounded time, probe rarely.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.T_win = min(5, self.r)
        self.L = min(max(1, math.ceil(self.r / 4)), max(0, self.r - 1))
        self.F = 2
        self.theta = 0.5
        self.p_probe = min(0.05, 1.0 / max(1, self.r))
        self.n_opp = max(0, self.n - 1)
        self.cheater_flag = [False] * self.n_opp
        self.punish_timer = [0] * self.n_opp
        self.coop_run = [0] * self.n_opp
        self._processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        completed = history.round_number
        for idx in range(self._processed_rounds, completed):
            for j in range(self.n_opp):
                if self.punish_timer[j] > 0:
                    self.punish_timer[j] = max(0, self.punish_timer[j] - 1)
            opp_actions_row = history.opponent_actions[idx, :] if self.n_opp > 0 else np.array([], dtype=bool)
            my_action = bool(history.my_actions[idx])
            for j in range(self.n_opp):
                acted_coop = bool(opp_actions_row[j])
                if acted_coop:
                    self.coop_run[j] = self.coop_run[j] + 1
                else:
                    self.coop_run[j] = 0
                if self.cheater_flag[j] and self.coop_run[j] >= self.F:
                    self.cheater_flag[j] = False
                    self.punish_timer[j] = 0
            total_cooperators = int(np.sum(opp_actions_row)) + (1 if my_action else 0)
            for j in range(self.n_opp):
                opp_j_coop = bool(opp_actions_row[j])
                cooperators_excl_j = total_cooperators - (1 if opp_j_coop else 0)
                if not opp_j_coop and cooperators_excl_j == self.m - 1:
                    self.cheater_flag[j] = True
                    self.punish_timer[j] = self.L
        self._processed_rounds = completed
        t = state.round_number
        last_round = t == self.r
        observed_rounds = completed
        coop_fracs = [0.0] * self.n_opp
        for j in range(self.n_opp):
            if observed_rounds == 0:
                coop_fracs[j] = 0.0
            else:
                lookback = min(self.T_win, observed_rounds)
                if lookback <= 0:
                    coop_fracs[j] = 0.0
                else:
                    op_actions = history.opponent_actions[-lookback:, j] if lookback > 0 else np.array([], dtype=bool)
                    coop_count = int(np.sum(op_actions)) if op_actions.size > 0 else 0
                    coop_fracs[j] = float(coop_count) / float(lookback)
        predicted = [False] * self.n_opp
        for j in range(self.n_opp):
            if last_round:
                predicted[j] = coop_fracs[j] >= self.theta
            else:
                predicted[j] = coop_fracs[j] >= self.theta and (not self.cheater_flag[j])
        predicted_others = int(sum((1 for v in predicted if v)))
        if predicted_others >= self.m:
            return Action.D
        if predicted_others == self.m - 1:
            if not last_round:
                predicted_others_recount = int(sum((1 for j in range(self.n_opp) if coop_fracs[j] >= self.theta and (not self.cheater_flag[j]))))
            else:
                predicted_others_recount = int(sum((1 for j in range(self.n_opp) if coop_fracs[j] >= self.theta)))
            if predicted_others_recount >= self.m:
                return Action.D
            pred_indices = [j for j in range(self.n_opp) if coop_fracs[j] >= self.theta and (last_round or not self.cheater_flag[j])]
            avg_group_frac = 0.0
            if len(pred_indices) > 0:
                avg_group_frac = sum((coop_fracs[j] for j in pred_indices)) / float(len(pred_indices))
            if abs(avg_group_frac - self.theta) < 1e-06 or (avg_group_frac <= self.theta + 0.01 and avg_group_frac >= self.theta - 0.01):
                q_consider = max(0.0, min(1.0, avg_group_frac / 2.0))
                if random.random() < q_consider:
                    return Action.C
                else:
                    return Action.D
            return Action.C
        if random.random() < self.p_probe:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_38(BaseStrategy):
    """
    Aggressive adaptive pivot strategy: cooperate only when k * Pr(exactly m-1 others cooperate) > 1;
    default-defect first round; permanently punish players who defected while you cooperated and the
    group reward was received.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.num_opponents = max(0, self.n - 1)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.C_counts = np.zeros(self.num_opponents, dtype=float)
        self.exploiters = set()
        self.prior_alpha = 1.0
        self.prior_beta = 1.0
        self.processed_rounds = 0

    def _poisson_binomial_mass(self, p_list, target):
        """
        Exact Poisson-binomial mass at 'target' for Bernoulli probabilities in p_list.
        Uses dynamic programming.
        """
        N = len(p_list)
        if target < 0 or target > N:
            return 0.0
        prob = [0.0] * (N + 1)
        prob[0] = 1.0
        for p in p_list:
            for s in range(N - 1, -1, -1):
                prob[s + 1] += prob[s] * p
                prob[s] *= 1.0 - p
        return prob[target]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        total_completed = history.round_number
        for r_idx in range(self.processed_rounds, total_completed):
            round_op_actions = history.opponent_actions[r_idx, :]
            if self.num_opponents > 0:
                self.C_counts += round_op_actions.astype(float)
            my_coop = bool(history.my_actions[r_idx])
            coop_others = int(np.sum(round_op_actions))
            total_cooperators = coop_others + (1 if my_coop else 0)
            threshold_met = total_cooperators >= self.m
            if my_coop and threshold_met:
                for j in range(self.num_opponents):
                    if not bool(round_op_actions[j]):
                        self.exploiters.add(j)
        self.processed_rounds = total_completed
        denom = total_completed + self.prior_alpha + self.prior_beta
        p_list = []
        for j in range(self.num_opponents):
            if j in self.exploiters:
                p = 0.0
            else:
                p = float(self.C_counts[j] + self.prior_alpha) / float(denom)
            p_list.append(p)
        target = self.m - 1
        Pr_X_eq = self._poisson_binomial_mass(p_list, target)
        if self.k * Pr_X_eq > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_39(BaseStrategy):
    """
    Aggressive Opportunistic Coercer (AOC)

    - Starts by defecting and always defects in the final round.
    - Estimates opponents' cooperation probabilities from a recent window of rounds.
    - Chooses C vs D by comparing one-shot expected payoffs (uses Poisson-Binomial exact distribution).
    - If the group recently failed while some others cooperated, enters a temporary punishment mode:
      defect for punishment_duration rounds. After a punishment block ends, release only if at least
      (m-1) other players have recent cooperation rates >= coop_threshold_for_release; otherwise extend.
    - Tie-breaking and ambiguity favor defection (exploitative).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        r = self.game_description.n_rounds
        self.L = min(5, max(0, r - 1))
        self.epsilon = 1e-06
        self.exploit_bias = 0.0
        self.punish_window = 2
        self.punish_min_fail_cooperators = 1
        self.punishment_duration = int(math.ceil(min(3, max(0.0, r / 4.0))))
        if self.punishment_duration <= 0:
            self.punishment_duration = 1
        self.coop_threshold_for_release = 0.6
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None or history.round_number == 0:
            return Action.D
        t = state.round_number
        if t >= r:
            return Action.D
        prior_in_punishment = self.punishment_timer > 0
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
        completed = history.round_number
        window = min(self.L, completed)
        if window <= 0:
            p_list = [0.0] * (n - 1)
        else:
            recent_opponent_actions = history.opponent_actions[-window:, :]
            coop_counts = np.sum(recent_opponent_actions.astype(float), axis=0)
            p_list = (coop_counts / float(window)).tolist()
        if prior_in_punishment and self.punishment_timer == 0:
            release_window = min(self.L, completed)
            if release_window <= 0:
                self.punishment_timer = self.punishment_duration
            else:
                recent = history.opponent_actions[-release_window:, :].astype(float)
                rates = np.sum(recent, axis=0) / float(release_window)
                num_good = int(np.sum(rates >= self.coop_threshold_for_release))
                if num_good < max(0, m - 1):
                    self.punishment_timer = self.punishment_duration
                else:
                    pass
        if self.punishment_timer == 0:
            lookback = min(self.punish_window, completed)
            triggered = False
            for u in range(1, lookback + 1):
                idx = -u
                other_coops = int(np.sum(history.opponent_actions[idx, :]))
                my_coop = bool(history.my_actions[idx])
                total_coops = other_coops + (1 if my_coop else 0)
                if total_coops < m and other_coops >= self.punish_min_fail_cooperators:
                    self.punishment_timer = self.punishment_duration
                    triggered = True
                    break
            if triggered:
                return Action.D
        if self.punishment_timer > 0:
            return Action.D
        num_opp = n - 1
        probs = np.zeros(num_opp + 1, dtype=float)
        probs[0] = 1.0
        for p in p_list:
            new = np.zeros_like(probs)
            for s in range(0, num_opp + 1):
                if probs[s] == 0.0:
                    continue
                new[s] += probs[s] * (1.0 - p)
                if s + 1 <= num_opp:
                    new[s + 1] += probs[s] * p
            probs = new

        def tail_prob(thresh):
            if thresh <= 0:
                return 1.0
            if thresh > num_opp:
                return 0.0
            return float(np.sum(probs[thresh:]))
        P_success_if_defect = tail_prob(m)
        P_success_if_cooperate = tail_prob(max(0, m - 1))
        E_defect = 1.0 + P_success_if_defect * k
        E_cooperate = P_success_if_cooperate * k
        if P_success_if_defect >= 0.7 - 1e-12:
            return Action.D
        if E_defect > E_cooperate + self.epsilon:
            return Action.D
        elif E_cooperate > E_defect + self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_40(BaseStrategy):
    """
    Greedy Pivotal Exploiter:
    - Default: defect.
    - Estimate per-opponent cooperation rates from history (prior 0.5 if no data).
    - Compute the Poisson-Binomial distribution for the number X of other cooperators.
    - Let p_pivot = Pr[X = m-1]. Cooperate iff k * p_pivot > 1 + margin (strict).
    - Small epsilon exploration to avoid full determinism.
    - Grim-lite punishment: if repeatedly exploited in a sliding window, switch to permanent defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.base_margin = 0.02
        self.epsilon = min(0.02, 1.0 / max(1, game_description.n_rounds))
        self.W = min(10, game_description.n_rounds)
        self.T_coop_threshold = 3
        self.P_exploit_fraction = 0.7
        self.exploitation_flag = False
        self.prior_p = 0.5

    def _poisson_binomial_dist(self, ps):
        """
        Compute distribution of sum of independent Bernoulli(ps).
        Returns numpy array dist where dist[k] = Pr[sum = k].
        """
        dist = np.array([1.0], dtype=float)
        for p in ps:
            p = float(p)
            new = np.zeros(len(dist) + 1, dtype=float)
            new[:-1] += dist * (1.0 - p)
            new[1:] += dist * p
            dist = new
        return dist

    def _update_exploitation_flag(self, history: PlayerHistory):
        """
        Inspect the last W rounds to detect if we've been exploited enough to trigger grim.
        Conditions:
          - In the window, if we've cooperated >= T_coop_threshold times AND
            >= P_exploit_fraction of those cooperations resulted in no group reward,
            then set exploitation_flag = True.
        """
        if history is None:
            return
        rounds_done = history.round_number
        if rounds_done == 0:
            return
        start = max(0, rounds_done - self.W)
        end = rounds_done
        my_actions_win = history.my_actions[start:end]
        opp_actions_win = history.opponent_actions[start:end, :]
        coop_rounds_idx = np.nonzero(my_actions_win)[0]
        S_coop_you = int(len(coop_rounds_idx))
        if S_coop_you >= self.T_coop_threshold:
            exploited_coop_fail = 0
            for idx in coop_rounds_idx:
                opp_coops = int(np.sum(opp_actions_win[idx, :]))
                total_coops = 1 + opp_coops
                if total_coops < self.game_description.m:
                    exploited_coop_fail += 1
            frac = exploited_coop_fail / max(1, S_coop_you)
            if frac >= self.P_exploit_fraction:
                self.exploitation_flag = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = float(self.game_description.k)
        rounds_done = 0 if history is None else history.round_number
        L = max(1, r - rounds_done)
        if history is not None:
            self._update_exploitation_flag(history)
        if self.exploitation_flag:
            return Action.D
        num_opponents = max(0, n - 1)
        p_list = []
        if history is None or rounds_done == 0:
            p_list = [self.prior_p] * num_opponents
        else:
            obs_counts = rounds_done
            coop_counts = np.sum(history.opponent_actions, axis=0)
            for j in range(num_opponents):
                cj = int(coop_counts[j])
                if obs_counts <= 0:
                    p_list.append(self.prior_p)
                else:
                    pj = float(cj) / float(obs_counts)
                    p_list.append(pj)
        if num_opponents == 0:
            dist = np.array([1.0])
        else:
            dist = self._poisson_binomial_dist(p_list)
        pivot_index = m - 1
        if 0 <= pivot_index < len(dist):
            p_pivot = float(dist[pivot_index])
        else:
            p_pivot = 0.0
        if m <= len(dist):
            p_atleast_m = float(np.sum(dist[m:]))
        else:
            p_atleast_m = 0.0
        margin = self.base_margin * (1.0 + 1.0 / max(1.0, float(L)))
        coop_threshold = (1.0 + margin) / max(1e-12, k)
        do_explore = random.random() < self.epsilon
        if do_explore:
            return Action.C
        if p_pivot > coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_41(BaseStrategy):
    """
    Aggressive Conditional Pivotal (ACP)

    - Small early build phase to signal cooperation (B rounds).
    - Defect in final E rounds and in last round.
    - Free-ride when others are expected to reach the threshold without you.
    - Cooperate when you are likely pivotal (expected_others >= m-1).
    - Grim trigger: if at least P failures among your last Q cooperative attempts, switch to permanent defection.
    - Deterministic and based only on game parameters and full history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.permanent_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        B = min(3, max(1, math.floor(r / 4)))
        E = min(2, math.floor(r / 4))
        Q = 4
        P = 2
        t = int(state.round_number)
        if history is not None and history.round_number > 0:
            coop_indices = list(np.where(history.my_actions.astype(int))[0])
            if len(coop_indices) >= 1:
                recent_coops = coop_indices[-Q:]
                failures = 0
                for idx in recent_coops:
                    total_coops = int(history.my_actions[idx]) + int(np.sum(history.opponent_actions[idx, :]))
                    if total_coops < m:
                        failures += 1
                if len(recent_coops) >= Q and failures >= P:
                    self.permanent_defect = True
        if t == r:
            return Action.D
        if E > 0 and t > r - E:
            return Action.D
        if self.permanent_defect:
            return Action.D
        if history is None or history.round_number == 0:
            if r >= 4 and m <= n - 1:
                return Action.C
            else:
                return Action.D
        W = min(5, t - 1) if t > 1 else 0
        expected_others = None
        if W > 0:
            opp_actions = history.opponent_actions
            available_rounds = history.round_number
            use_W = min(W, available_rounds)
            if use_W > 0:
                recent = opp_actions[-use_W:, :]
                total_coop_by_others = float(np.sum(recent.astype(int)))
                p_hat = total_coop_by_others / ((n - 1) * use_W)
                expected_others = p_hat * (n - 1)
            else:
                expected_others = None
        else:
            expected_others = None
        if expected_others is not None:
            if expected_others >= m:
                return Action.D
        if expected_others is not None:
            if expected_others >= m - 1:
                return Action.C
        if t <= B and r - t >= 2:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_42(BaseStrategy):
    """
    Aggressive Collective Risk strategy.

    - Default: defect.
    - Cooperate only when (a) likely pivotal and cooperation will likely produce the threshold,
      or (b) in the last round cooperating is immediately pivotal.
    - Tracks recent cooperation reliability per opponent over a sliding window L.
    - Flags and permanently punishes opponents who opportunistically defected when their
      cooperation would have secured the threshold (objective test).
    - Probes rarely with probability epsilon.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.L = max(1, min(10, max(1, self.r - 1)))
        self.p_thresh = 0.6
        self.epsilon = 0.03
        self.n_opponents = max(0, self.n - 1)
        self.flagged_traitor = [False] * self.n_opponents
        self.last_processed_round = 0
        self.mass_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        completed_rounds = history.round_number
        for round_idx in range(self.last_processed_round, completed_rounds):
            my_act = bool(history.my_actions[round_idx])
            opp_acts_row = history.opponent_actions[round_idx, :]
            total_cooperators = int(sum(opp_acts_row)) + (1 if my_act else 0)
            if total_cooperators < self.m:
                if my_act:
                    if total_cooperators >= self.m - 1:
                        for j in range(self.n_opponents):
                            if not bool(opp_acts_row[j]):
                                self.flagged_traitor[j] = True
                else:
                    others_coops = int(sum(opp_acts_row))
                    if others_coops >= self.m - 1:
                        for j in range(self.n_opponents):
                            if not bool(opp_acts_row[j]):
                                self.flagged_traitor[j] = True
        self.last_processed_round = completed_rounds
        flagged_count = sum((1 for f in self.flagged_traitor if f))
        if flagged_count > self.n_opponents / 2:
            self.mass_defect = True
        if self.mass_defect:
            return Action.D
        t = completed_rounds + 1
        final_round = t == self.r
        L = self.L
        avail = min(completed_rounds, L)
        reliabilities = []
        for j in range(self.n_opponents):
            if avail > 0:
                recent = history.opponent_actions[-avail:, j]
                coop_in_recent = int(np.sum(recent))
            else:
                coop_in_recent = 0
            missing = L - avail
            coop_adjusted = coop_in_recent + 0.5 * missing
            reliability_j = coop_adjusted / L
            reliabilities.append(reliability_j)
        effective_reliability = [0.0 if self.flagged_traitor[j] else reliabilities[j] for j in range(self.n_opponents)]
        predicted_indices = [j for j, er in enumerate(effective_reliability) if er >= self.p_thresh]
        predicted_coops = len(predicted_indices)
        last_round_nonflag_coops = 0
        last_round_other_coops = 0
        if completed_rounds >= 1:
            last_row = history.opponent_actions[-1, :]
            last_round_other_coops = int(np.sum(last_row))
            last_round_nonflag_coops = int(np.sum(np.logical_and(last_row, np.logical_not(np.array(self.flagged_traitor)))))
        if final_round:
            if last_round_other_coops >= self.m - 1 or predicted_coops >= self.m - 1:
                if last_round_other_coops >= self.m - 1 and last_round_nonflag_coops >= self.m - 1 or predicted_coops >= self.m - 1:
                    return Action.C
            return Action.D
        if predicted_coops >= self.m:
            return Action.D
        elif predicted_coops == self.m - 1:
            if predicted_coops >= self.m - 1:
                return Action.C
            else:
                return Action.D
        else:
            if random.random() < self.epsilon:
                return Action.C
            return Action.D

class Strategy_AGGRESSIVE_43(BaseStrategy):
    """
    Pivotal Aggressor (PA)

    Aggressive, exploitative strategy for the Collective Risk Dilemma.
    - Defaults to defect.
    - Estimates per-opponent cooperation probabilities from a recent window.
    - Computes the Poisson-binomial probability that exactly (m-1) others cooperate.
      Cooperates myopically iff that probability > 1/k (i.e., likely pivotal).
    - Detects retaliation (drop in opponents' cooperation after our defections) and,
      if repairing is economically justified, enters a short repair phase of cooperative moves.
    - Occasionally probes (small epsilon) by cooperating when the algorithm would defect.
    - Always defects in the final round. Disables repair when insufficient rounds remain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(30, self.r)
        self.epsilon = 0.02
        self.Q = min(20, self.r)
        self.delta = 0.1
        self.R_rep = 2
        self.L_min = 3
        self.p0 = 0.3
        self.repair_remaining = 0

    def _poisson_binomial_pmf(self, ps, target):
        """
        Compute Poisson-Binomial PMF Pr(sum_j X_j = target) for independent Bernoulli(ps).
        Uses dynamic programming convolution. ps is iterable of probabilities.
        """
        prob = np.array([1.0], dtype=float)
        for p in ps:
            new_prob = np.empty(len(prob) + 1, dtype=float)
            new_prob[0] = prob[0] * (1.0 - p)
            for s in range(1, len(new_prob)):
                a = prob[s] * (1.0 - p) if s < len(prob) else 0.0
                b = prob[s - 1] * p
                new_prob[s] = a + b
            prob = new_prob
        if 0 <= target < len(prob):
            return float(prob[target])
        return 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            rounds_completed = 0
        else:
            rounds_completed = history.round_number
        rounds_left = max(0, self.r - rounds_completed)
        if rounds_left <= 1:
            self.repair_remaining = 0
            return Action.D
        if self.repair_remaining > 0:
            self.repair_remaining -= 1
            return Action.C
        other_count = self.n - 1
        ps = []
        if history is None or history.opponent_actions.size == 0:
            ps = [self.p0] * other_count
        else:
            T = history.opponent_actions.shape[0]
            start = max(0, T - self.W)
            observed_window = history.opponent_actions[start:T, :]
            for j in range(other_count):
                col = observed_window[:, j]
                if col.size == 0:
                    ps.append(self.p0)
                else:
                    pj = float(np.mean(col))
                    sample_size = col.size
                    shrink_weight = min(5.0 / max(1.0, sample_size), 1.0) * 0.5
                    pj = (1.0 - shrink_weight) * pj + shrink_weight * self.p0
                    ps.append(pj)
        target = max(0, self.m - 1)
        if target > other_count:
            P_exact = 0.0
        else:
            P_exact = self._poisson_binomial_pmf(ps, target)
        threshold = 1.0 / max(1e-12, self.k)
        myopic_cooperate = P_exact > threshold
        decision = Action.C if myopic_cooperate else Action.D
        if rounds_left >= self.L_min and history is not None and (history.opponent_actions.size > 0):
            T = history.opponent_actions.shape[0]
            events_after_C = []
            events_after_D = []
            my_actions = history.my_actions
            opp_actions = history.opponent_actions
            for i in range(0, T - 1):
                next_round_frac = float(np.mean(opp_actions[i + 1, :]))
                if bool(my_actions[i]):
                    events_after_C.append(next_round_frac)
                else:
                    events_after_D.append(next_round_frac)
            if len(events_after_C) > 0 or len(events_after_D) > 0:
                recent_C = events_after_C[-self.Q:] if len(events_after_C) > 0 else []
                recent_D = events_after_D[-self.Q:] if len(events_after_D) > 0 else []
                if len(recent_C) >= 1 and len(recent_D) >= 1:
                    coop_rate_after_my_C = float(np.mean(recent_C))
                    coop_rate_after_my_D = float(np.mean(recent_D))
                    if coop_rate_after_my_D <= coop_rate_after_my_C - self.delta:
                        Delta = coop_rate_after_my_C - coop_rate_after_my_D
                        Delta_total = (self.n - 1) * Delta
                        estimated_future_benefit = rounds_left * self.k * Delta_total
                        repair_cost = self.R_rep * 1.0
                        if estimated_future_benefit > repair_cost:
                            decision = Action.C
                            self.repair_remaining = max(0, self.R_rep - 1)
                            return decision
        if decision == Action.D:
            if random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        return decision

class Strategy_AGGRESSIVE_44(BaseStrategy):
    """
    Aggressive Pivotal Enforcer (APE)

    Default: defect. Estimate opponents' cooperation probabilities from a sliding
    window of recent rounds. Monte-Carlo estimate the probability that others
    meet the threshold with and without your cooperation. Cooperate only if
    cooperating raises your immediate expected payoff (accounting for a small
    defection bias delta). Monitor "exploit events" (you cooperated, opponent
    defected, and the threshold was met) and short, targeted punishments of
    length P are applied to offenders (treat their cooperation probability as
    0 while punishing). Punishment escalates if exploitation persists and is
    forgiven when the opponent's observed cooperation rate improves.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.L = min(10, max(1, self.r - 1))
        self.M = 200
        self.delta = 0.01
        self.gamma = 0.3
        self.base_P = min(3, max(1, math.floor(self.r / 6)))
        self.P_default = self.base_P
        self.F = max(1, self.P_default)
        self.R_small = 2
        self.E_W = self.L
        self.punish_end = {}
        self.escalation = {}
        self._last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = getattr(state, 'round_number', 1)
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        n_opp = self.n - 1
        for j in range(n_opp):
            if j not in self.escalation:
                self.escalation[j] = self.P_default
            if j not in self.punish_end:
                self.punish_end[j] = 0

        def window_slice(length):
            if length <= 0:
                return slice(0, 0)
            start = max(0, rounds_completed - length)
            return slice(start, rounds_completed)
        Lw = min(self.L, rounds_completed)
        Ew = min(self.E_W, rounds_completed)
        p_js = np.zeros(n_opp, dtype=float)
        if Lw > 0:
            sl = window_slice(Lw)
            recent_opp = opp_actions[sl, :]
            counts = np.sum(recent_opp, axis=0)
            p_js = (1.0 + counts) / (2.0 + Lw)
        else:
            p_js[:] = 0.5
        exploit_counts = np.zeros(n_opp, dtype=int)
        if Ew > 0:
            sl_e = window_slice(Ew)
            recent_my = my_actions[sl_e]
            recent_opps = opp_actions[sl_e, :]
            total_coop = recent_my.astype(int)[:, None] + np.sum(recent_opps, axis=1)[:, None]
            you_coop = recent_my.astype(bool)[:, None]
            opp_defected = ~recent_opps.astype(bool)
            threshold_met = (total_coop[:, 0] >= self.m)[:, None]
            exploit_matrix = you_coop & opp_defected & threshold_met
            exploit_counts = np.sum(exploit_matrix, axis=0)
        exploit_rates = np.zeros(n_opp, dtype=float)
        if Ew > 0:
            exploit_rates = exploit_counts / float(Ew)
        if Lw > 0:
            sl_dec = window_slice(Lw)
            recent_my_all = my_actions[sl_dec]
            recent_opps_all = opp_actions[sl_dec, :]
            total_coop_all = recent_my_all.astype(int) + np.sum(recent_opps_all, axis=1)
            for idx_in_window in range(total_coop_all.shape[0]):
                s_total = int(total_coop_all[idx_in_window])
                if s_total < self.m:
                    if s_total + 1 >= self.m or s_total + 2 >= self.m:
                        row = recent_opps_all[idx_in_window, :].astype(bool)
                        defectors = ~row
                        for j in range(n_opp):
                            if defectors[j]:
                                if self.punish_end.get(j, 0) < current_round:
                                    Pj = max(1, int(self.escalation.get(j, self.P_default)))
                                    self.punish_end[j] = current_round + Pj - 1
        for j in range(n_opp):
            if exploit_rates[j] >= self.gamma:
                if self.punish_end.get(j, 0) < current_round:
                    Pj = max(1, int(self.escalation.get(j, self.P_default)))
                    remaining = max(0, self.r - current_round + 1)
                    Pj = min(Pj, max(1, remaining))
                    self.punish_end[j] = current_round + Pj - 1
        for j in range(n_opp):
            ended_before = self.punish_end.get(j, 0) < current_round
            if not ended_before and self.punish_end.get(j, 0) >= current_round:
                continue
            if ended_before and exploit_rates[j] >= self.gamma:
                remaining = max(0, self.r - current_round + 1)
                if remaining > 0:
                    current_Pj = max(1, int(self.escalation.get(j, self.P_default)))
                    newP = min(max(1, current_Pj * 2), max(1, remaining // 2))
                    if newP >= 1 and remaining >= 1:
                        self.escalation[j] = newP
                        self.punish_end[j] = current_round + newP - 1
            if p_js[j] > self.gamma / 2.0:
                self.punish_end[j] = 0
                self.escalation[j] = self.P_default
        punished_now = [False] * n_opp
        for j in range(n_opp):
            if self.punish_end.get(j, 0) >= current_round:
                punished_now[j] = True
        rounds_left = max(0, self.r - current_round + 1)
        myopic_mode = rounds_left <= self.R_small
        p_used = np.array(p_js, copy=True)
        if not myopic_mode:
            for j in range(n_opp):
                if punished_now[j]:
                    p_used[j] = 0.0
                elif self.escalation.get(j, self.P_default) > self.P_default and p_js[j] <= self.gamma / 2.0:
                    p_used[j] = 0.0
        if n_opp <= 0:
            P_without = 1.0 if 0 >= self.m else 0.0
            P_with = 1.0 if 0 >= self.m - 1 else 0.0
        else:
            rand_mat = np.random.random((self.M, n_opp))
            samp = (rand_mat < p_used[None, :]).astype(int)
            S_samples = np.sum(samp, axis=1)
            P_without = float(np.mean(S_samples >= self.m))
            P_with = float(np.mean(S_samples >= self.m - 1))
        EU_C = self.k * P_with
        EU_D = 1.0 + self.k * P_without
        if EU_D > EU_C + self.delta:
            choice = Action.D
        else:
            choice = Action.C
        self._last_processed_round = current_round
        return choice

class Strategy_AGGRESSIVE_45(BaseStrategy):
    """
    Aggressive Threshold Enforcer (ATE)

    Brief:
    - Probes briefly to signal willingness to cooperate.
    - In NORMAL mode, preserves marginal coalitions, exploits safe slack (C_prev >= m+1),
      tips when pivotal and enough consistent cooperators exist, otherwise defects.
    - If exploited (you cooperated but group failed to reach m), switches to PUNISH:
      defect until two consecutive completed rounds immediately preceding the decision
      both have cooperators >= m, then return to NORMAL.
    - Always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.T_probe = min(3, max(1, math.floor(self.r / 4)))
        self.Consistency_window = 3
        self.tau = 0.8
        self.mode = 'NORMAL'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)

        def cooperators_in_round_idx(idx: int) -> int:
            opp_sum = int(np.sum(history.opponent_actions[idx, :])) if history is not None and history.opponent_actions.size > 0 else 0
            my_coop = int(history.my_actions[idx]) if history is not None and history.my_actions.size > 0 else 0
            return opp_sum + my_coop
        if history is not None and history.round_number >= 1:
            last_idx = history.round_number - 1
            my_cooperated_last = bool(history.my_actions[-1]) if history.my_actions.size > 0 else False
            cooperators_last = cooperators_in_round_idx(last_idx)
            if my_cooperated_last and cooperators_last < self.m:
                self.mode = 'PUNISH'
        if t == self.r:
            return Action.D
        if self.mode == 'PUNISH':
            if history is not None and history.round_number >= 2:
                idx1 = history.round_number - 2
                idx2 = history.round_number - 1
                coop1 = cooperators_in_round_idx(idx1)
                coop2 = cooperators_in_round_idx(idx2)
                if coop1 >= self.m and coop2 >= self.m:
                    self.mode = 'NORMAL'
                else:
                    return Action.D
            else:
                return Action.D
        if self.mode == 'NORMAL':
            if t <= self.T_probe:
                return Action.C
            if history is None or history.round_number == 0:
                C_prev = 0
            else:
                C_prev = cooperators_in_round_idx(history.round_number - 1)
            if C_prev >= self.m + 1:
                return Action.D
            if C_prev == self.m:
                return Action.C
            if C_prev == self.m - 1:
                rounds_so_far = history.round_number if history is not None else 0
                last_w = min(self.Consistency_window, rounds_so_far)
                if last_w <= 0:
                    return Action.D
                recent_opponent = history.opponent_actions[-last_w:, :]
                coop_rates = np.mean(recent_opponent.astype(float), axis=0)
                reliable_count = int(np.sum(coop_rates >= self.tau))
                if reliable_count >= self.m - 1:
                    return Action.C
                else:
                    return Action.D
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_46(BaseStrategy):
    """
    Pivotal Exploiter (PE)

    Aggressive, adaptive strategy for the Collective Risk Dilemma.
    - Defaults to Defect (Action.D).
    - Rare probes (epsilon) to detect conditional cooperators.
    - Estimates each opponent's cooperation rate using a sliding window of up to L=20
      most recent rounds (fast adaptation).
    - Approximates the distribution of the number X of other cooperators by
      Binomial(n-1, p_bar) where p_bar is the mean opponent cooperation rate.
    - Cooperates iff Pr(X == m-1) > 1/k (strict inequality). Ties favour Defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = float(game_description.k)
        self.p0 = 0.1
        self.L_max = 20
        self._tol = 1e-12

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        eps_probe = min(0.02, 1.0 / max(10, self.r))
        if random.random() < eps_probe:
            return Action.C
        if t == 1 or history is None:
            p_bar = self.p0
        else:
            L = min(self.L_max, t - 1)
            opp_actions = history.opponent_actions
            try:
                rounds_recorded, n_opponents = opp_actions.shape
            except Exception:
                p_bar = self.p0
            else:
                L_actual = min(L, rounds_recorded) if rounds_recorded > 0 else 0
                if L_actual <= 0:
                    p_bar = self.p0
                else:
                    pjs = []
                    for j in range(n_opponents):
                        recent = opp_actions[-L_actual:, j]
                        coop_count = int(np.sum(recent))
                        pj = coop_count / float(L_actual)
                        pjs.append(pj)
                    if n_opponents < self.n - 1:
                        pad_count = self.n - 1 - n_opponents
                        pjs.extend([self.p0] * pad_count)
                    p_bar = float(np.mean(pjs))
        if p_bar < 0.0:
            p_bar = 0.0
        elif p_bar > 1.0:
            p_bar = 1.0
        n_minus_1 = max(0, self.n - 1)
        k_target = max(0, self.m - 1)
        if k_target < 0 or k_target > n_minus_1:
            q = 0.0
        else:
            comb = math.comb(n_minus_1, k_target)
            try:
                q = comb * p_bar ** k_target * (1.0 - p_bar) ** (n_minus_1 - k_target)
            except Exception:
                q = 0.0
        threshold = 1.0 / float(self.k) if self.k != 0 else float('inf')
        if q - threshold > self._tol:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_47(BaseStrategy):
    """
    Aggressive selfish adaptive strategy for the Collective Risk Dilemma.

    - Default: Defect.
    - Cooperate only when cooperating is privately expected to be profitable:
        cooperate iff k * P(exactly m-1 others cooperate) > 1 + epsilon.
    - Uses per-opponent probability estimates based on a sliding window with a Laplace prior.
    - Small exploration probability for early probing when data is scarce.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 1.0
        self.window = 50
        self.epsilon = 0.05
        self.p_explore = 0.01
        self.approx_threshold = 300

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        num_obs = history.round_number
        n_others = self.n - 1
        if num_obs < 2 and random.random() < self.p_explore:
            return Action.C
        W = min(self.window, num_obs)
        p_list = []
        if W <= 0:
            p_list = [self.alpha / (2.0 * self.alpha) for _ in range(n_others)]
        else:
            start_idx = max(0, num_obs - W)
            recent_actions = history.opponent_actions[start_idx:num_obs, :]
            for j in range(n_others):
                try:
                    successes = int(np.sum(recent_actions[:, j]))
                except Exception:
                    successes = 0
                trials = recent_actions.shape[0]
                p_j = (self.alpha + successes) / (2.0 * self.alpha + trials)
                p_list.append(float(p_j))
        target = self.m - 1
        if target < 0 or target > n_others:
            P_piv = 0.0
        elif n_others <= self.approx_threshold:
            probs = [0.0] * (n_others + 1)
            probs[0] = 1.0
            for p in p_list:
                prev = probs[:]
                probs[0] = prev[0] * (1.0 - p)
                for k in range(1, n_others + 1):
                    probs[k] = prev[k] * (1.0 - p) + prev[k - 1] * p
            P_piv = probs[target]
        else:
            mu = sum(p_list)
            sigma2 = sum((p * (1.0 - p) for p in p_list))
            if sigma2 <= 0.0:
                P_piv = 1.0 if abs(mu - target) < 1e-12 else 0.0
            else:
                sigma = math.sqrt(sigma2)

                def normal_cdf(z):
                    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))
                upper = (target + 0.5 - mu) / sigma
                lower = (target - 0.5 - mu) / sigma
                P_piv = normal_cdf(upper) - normal_cdf(lower)
                if P_piv < 0.0:
                    P_piv = 0.0
                elif P_piv > 1.0:
                    P_piv = 1.0
        if self.k * P_piv > 1.0 + self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_48(BaseStrategy):
    """
    Aggressive pivot strategy for the Collective Risk Dilemma.

    - Default: defect (free-ride).
    - Maintain an exponential moving average p_j for each opponent's cooperation probability.
    - Compute the Poisson-binomial probability P(X = m-1) that exactly m-1 other players cooperate.
    - Cooperate this round iff k * P(X = m-1) > 1 + delta (aggressive tie-break to defect).
    - First round: probe with small probability epsilon_probe (otherwise defect).
    - Update p_j after each completed round using alpha (EMA).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.epsilon_probe = 0.05
        self.delta = 0.005
        self.initial_guess = 0.1
        n = int(self.game_description.n_players)
        self.p_vec = np.full((max(0, n - 1),), float(self.initial_guess), dtype=float)

    def _poisson_binomial_p_eq(self, target_k: int, p_list):
        """
        Compute exact Poisson-binomial probability that sum of independent Bernoulli(p_list) equals target_k.
        Uses dynamic programming. p_list is iterable of probabilities (floats).
        """
        n = len(p_list)
        if target_k < 0 or target_k > n:
            return 0.0
        prob = [0.0] * (n + 1)
        prob[0] = 1.0
        for p in p_list:
            q = 1.0 - p
            for j in range(n, 0, -1):
                prob[j] = prob[j] * q + prob[j - 1] * p
            prob[0] = prob[0] * q
        return prob[target_k]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            if random.random() < self.epsilon_probe:
                return Action.C
            return Action.D
        last_op_actions = history.opponent_actions[-1, :]
        if len(self.p_vec) == len(last_op_actions):
            recent = np.asarray(last_op_actions, dtype=float)
            self.p_vec = (1.0 - self.alpha) * self.p_vec + self.alpha * recent
        else:
            obs_n = last_op_actions.shape[0]
            if obs_n > 0:
                new_p = np.full((obs_n,), float(self.initial_guess), dtype=float)
                min_n = min(obs_n, len(self.p_vec))
                if min_n > 0:
                    new_p[:min_n] = self.p_vec[:min_n]
                recent = np.asarray(last_op_actions, dtype=float)
                self.p_vec = (1.0 - self.alpha) * new_p + self.alpha * recent
            else:
                return Action.D
        m_minus_1 = int(self.game_description.m) - 1
        P_eq = self._poisson_binomial_p_eq(m_minus_1, list(self.p_vec))
        k = float(self.game_description.k)
        if k * P_eq > 1.0 + self.delta:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_49(BaseStrategy):
    """
    Aggressive, selfish contributor for the Collective Risk Dilemma.

    - Defaults to defect on the first round (no information).
    - Uses a recent window (up to W=10 rounds) of observed rounds to estimate
      the frequency that exactly m-1 other players cooperated.
    - Cooperates only when the expected incremental gain from cooperating is
      strictly positive:
          k * Pr(q = m-1) > 1
      (Tie-breaks go to Defect.)
    - Optionally more conservative when very few samples are available:
      if W < min_samples_for_margin, require k * p_eq_m1 > 1 + margin.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = float(game_description.k)
        self.window_max = 10
        self.min_samples_for_margin = 3
        self.margin = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        completed = history.round_number
        if completed == 0:
            return Action.D
        W = min(self.window_max, completed)
        if W == 0:
            return Action.D
        start = completed - W
        count_eq_m1 = 0
        for idx in range(start, completed):
            q = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            if q == self.m - 1:
                count_eq_m1 += 1
        p_eq_m1 = count_eq_m1 / W if W > 0 else 0.0
        threshold = 1.0
        if W < self.min_samples_for_margin:
            threshold += self.margin
        if self.k * p_eq_m1 > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_50(BaseStrategy):
    """
    Aggressive pivotal cooperating strategy for the Collective Risk Dilemma.

    - Default: defect.
    - Cooperate only when the estimated probability that exactly (m-1)
      other players cooperate this round (i.e., you are pivotal) exceeds 1/k.
    - Never cooperate on the final round.
    - Estimates each opponent's cooperation tendency via an exponential
      moving average (EMA) initialized with a low prior p0 (aggressive).
    - Uses a Poisson-binomial DP to compute the pivotal probability,
      and also compares with empirical frequency of exactly (m-1)
      other cooperators (takes the max) to guard against correlations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.1
        self.ema_lambda = 0.4
        self._eps = 1e-12

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = float(self.game_description.k)
        if state.round_number >= r - 1:
            return Action.D
        target = m - 1
        num_opponents = n - 1
        if target < 0 or target > num_opponents:
            return Action.D
        if history is None:
            p_list = [self.p0] * num_opponents
            rounds_observed = 0
        else:
            rounds_observed = history.round_number
            p_list = []
            opp_actions = history.opponent_actions
            for j in range(num_opponents):
                p = self.p0
                for t in range(rounds_observed):
                    a = 1.0 if bool(opp_actions[t, j]) else 0.0
                    p = (1.0 - self.ema_lambda) * p + self.ema_lambda * a
                p_list.append(float(p))
        dp = np.zeros(num_opponents + 1, dtype=float)
        dp[0] = 1.0
        for p in p_list:
            for s in range(num_opponents, 0, -1):
                dp[s] = dp[s] * (1.0 - p) + dp[s - 1] * p
            dp[0] = dp[0] * (1.0 - p)
        pb_p_eq = float(dp[target]) if 0 <= target <= num_opponents else 0.0
        empirical_freq = 0.0
        if history is not None and rounds_observed > 0:
            counts = np.sum(history.opponent_actions, axis=1)
            empirical_freq = float(np.sum(counts == target) / float(rounds_observed))
        p_eq_est = max(pb_p_eq, empirical_freq)
        threshold = 1.0 / k if k > 0.0 else float('inf')
        if p_eq_est > threshold + self._eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_51(BaseStrategy):
    """
    Aggressive pivot-seeking strategy for the Collective Risk Dilemma.

    - Default: defect on round 1 and always defect in the final round.
    - In intermediate rounds, estimate each opponent's cooperation probability
      using recency-weighted counts with Laplace smoothing.
    - Compute the Poisson–binomial probability that exactly m-1 other players
      cooperate. Cooperate iff that probability > 1/k (with a small tie bias
      toward defection). Small randomization can flip the chosen action.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.lam = 0.95
        self.eps_random = 0.02
        self.eps_tie = 1e-06

    def _poisson_binomial_p_equal(self, ps, target):
        """
        Exact Poisson-binomial probability that sum of independent Bernoulli(ps) equals target.
        ps: iterable of probabilities
        target: integer target sum
        """
        n = len(ps)
        if target < 0 or target > n:
            return 0.0
        q = [0.0] * (n + 1)
        q[0] = 1.0
        for p in ps:
            for s in range(n - 1, -1, -1):
                q[s + 1] += q[s] * p
                q[s] = q[s] * (1.0 - p)
        return q[target]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = int(self.game_description.n_players)
        n_rounds = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        current_round = int(state.round_number)
        rounds_remaining = n_rounds - current_round + 1
        if rounds_remaining == 1:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        opp_actions = history.opponent_actions
        num_opponents = opp_actions.shape[1]
        rounds_observed = history.round_number
        W = [0.0] * num_opponents
        S = [0.0] * num_opponents
        for i in range(rounds_observed):
            exp = rounds_observed - 1 - i
            weight = self.lam ** exp if exp > 0 else 1.0
            row = opp_actions[i]
            for j in range(num_opponents):
                if bool(row[j]):
                    W[j] += weight
                S[j] += weight
        ps = []
        for j in range(num_opponents):
            if S[j] <= 0.0:
                p_j = self.alpha / (2.0 * self.alpha) if self.alpha > 0 else 0.5
            else:
                p_j = (W[j] + self.alpha) / (S[j] + 2.0 * self.alpha)
            if p_j < 0.0:
                p_j = 0.0
            elif p_j > 1.0:
                p_j = 1.0
            ps.append(p_j)
        target = m - 1
        P_equals = self._poisson_binomial_p_equal(ps, target)
        threshold = 1.0 / k
        if P_equals > threshold + self.eps_tie:
            action = Action.C
        else:
            action = Action.D
        if random.random() < self.eps_random:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_52(BaseStrategy):
    """
    Hard Pivotal Defector with Grim Retaliation (HPD-GR)

    - Default: defect.
    - Cooperate only when your single contribution is plausibly pivotal (others = m-1 last round)
      and recent history shows at least one round in a short window where others were >= m-1.
    - If you ever cooperated and the threshold still failed that round, switch to permanent defection.
    - Always defect in the final round and on the first round (probe).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.persistent_defect_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        t = state.round_number if state is not None else 1
        if history is not None:
            num_completed = history.round_number
            for j in range(num_completed):
                others_coops = int(history.opponent_actions[j, :].sum()) if history.opponent_actions.size else 0
                my_coop = bool(history.my_actions[j])
                total_coops = others_coops + (1 if my_coop else 0)
                if my_coop and total_coops < m:
                    self.persistent_defect_flag = True
                    break
        if self.persistent_defect_flag:
            return Action.D
        if t == r:
            return Action.D
        if t == 1:
            return Action.D
        if history is None:
            O_last = math.floor((n - 1) / 2)
            reliability = 0
        else:
            if history.opponent_actions.size == 0:
                O_last = 0
            else:
                O_last = int(history.opponent_actions[-1, :].sum())
            w = min(3, t - 1)
            if w <= 0:
                reliability = 0
            else:
                opp_slice = history.opponent_actions[-w:, :] if history.opponent_actions.size else np.zeros((0, n - 1), dtype=bool)
                if opp_slice.size == 0:
                    reliability = 0
                else:
                    counts = np.sum(opp_slice, axis=1)
                    reliability = int(np.count_nonzero(counts >= m - 1))
        if O_last == m - 1 and reliability >= 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_53(BaseStrategy):
    """
    Threshold Opportunist with Aggressive Punishment (TOAP)

    Aggressively free-rides whenever safe, only cooperates when predicted to be
    pivotal (exactly m-1 other cooperators), and enters a short punishment mode
    if it was "suckered" (cooperated but threshold failed). Always defects in
    the final round. Uses a short recent-history window to estimate opponents'
    propensities to cooperate. Deterministic tie-breaking and conservative
    behavior in endgame rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        r = max(1, getattr(game_description, 'n_rounds', 1))
        self.W = min(5, max(0, r - 1))
        self.P = 3
        self.G = 1
        self.punish_timer = 0
        self.blacklist = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        t = getattr(state, 'round_number', 1)
        if history is None:
            return Action.D
        completed = history.round_number
        if completed >= 1:
            last_idx = completed - 1
            if bool(history.my_actions[last_idx]):
                last_opponent_coops = int(np.sum(history.opponent_actions[last_idx, :]))
                total_last_coops = 1 + last_opponent_coops
                if total_last_coops < m:
                    self.punish_timer = self.P
                    defectors = np.where(history.opponent_actions[last_idx, :] == False)[0]
                    for d in defectors:
                        self.blacklist.add(int(d))
        if t >= r:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer -= 1
            return Action.D
        if completed == 0:
            return Action.D
        W_prime = min(self.W, completed)
        if W_prime <= 0:
            return Action.D
        recent = history.opponent_actions[-W_prime:, :]
        counts = np.sum(recent, axis=0).astype(float)
        p_js = counts / float(W_prime)
        if len(self.blacklist) > 0:
            for bj in list(self.blacklist):
                if 0 <= bj < p_js.size:
                    p_js[int(bj)] = 0.0
        predicted_sum = float(np.sum(p_js))
        predicted_other_cooperators = int(math.floor(predicted_sum + 0.5))
        last_round_others = int(np.sum(history.opponent_actions[-1, :]))
        endgame_phase = t > r - self.G
        if predicted_other_cooperators >= m:
            return Action.D
        if predicted_other_cooperators <= m - 2:
            return Action.D
        if predicted_other_cooperators == m - 1:
            if endgame_phase:
                if last_round_others == m - 1:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        if last_round_others >= m:
            return Action.D
        if last_round_others == m - 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_54(BaseStrategy):
    """
    Aggressive Pivotal + Punish (APP)

    Defect by default; cooperate only when your cooperation is likely to be pivotal:
    compute the probability P_eq that exactly m-1 of the other n-1 players will
    cooperate. Cooperate iff P_eq >= 1/k (tie cooperates). If others will almost
    surely meet the threshold without you (P_ge_m >= 1 - eps) then defect.
    Permanently flag as exploiters any opponent who defected in a successful round;
    flagged players are treated as certain defectors (p_j = 0).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.3
        self.alpha = 0.8
        self.eps_near_certain = 1e-09

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        n_other = n - 1
        if history is None:
            p_list = [self.p0] * n_other
        else:
            rounds_seen = history.round_number
            if rounds_seen == 0:
                p_list = [self.p0] * n_other
            else:
                opp_actions = history.opponent_actions
                coop_counts = np.sum(opp_actions, axis=0).astype(float)
                exploiter_flags = np.zeros(n_other, dtype=bool)
                my_actions = history.my_actions
                for r_idx in range(rounds_seen):
                    total_coop = int(my_actions[r_idx]) + int(np.sum(opp_actions[r_idx, :]))
                    if total_coop >= m:
                        defections = np.logical_not(opp_actions[r_idx, :])
                        exploiter_flags = np.logical_or(exploiter_flags, defections)
                p_list = []
                for j in range(n_other):
                    if exploiter_flags[j]:
                        p_j = 0.0
                    else:
                        freq = float(coop_counts[j]) / float(rounds_seen) if rounds_seen > 0 else 0.0
                        p_j = self.alpha * freq + (1.0 - self.alpha) * self.p0
                    if p_j < 0.0:
                        p_j = 0.0
                    elif p_j > 1.0:
                        p_j = 1.0
                    p_list.append(float(p_j))
        prob = [0.0] * (n_other + 1)
        prob[0] = 1.0
        for p in p_list:
            for x in range(n_other, -1, -1):
                no_coop_contrib = prob[x] * (1.0 - p)
                coop_contrib = prob[x - 1] * p if x - 1 >= 0 else 0.0
                prob[x] = no_coop_contrib + coop_contrib
        idx_eq = m - 1
        if 0 <= idx_eq <= n_other:
            P_eq = prob[idx_eq]
        else:
            P_eq = 0.0
        P_ge_m = sum(prob[m:]) if m <= n_other else 0.0
        if P_ge_m >= 1.0 - self.eps_near_certain:
            return Action.D
        if P_eq >= 1.0 / float(k):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_55(BaseStrategy):
    """
    Aggressive Pivotal-Exploiter (APE)

    - Default: defect on first round and in ambiguous situations.
    - Estimate each opponent j as independent Bernoulli(f_j) where f_j is empirical
      cooperation frequency from past rounds.
    - If opponents on average cooperate a lot (avg_other_coop >= tau_exploit),
      enter permanent exploit mode (always defect).
    - Otherwise compute exact Poisson-Binomial distribution for the number of
      cooperating opponents and:
        * If Prob(others >= m) >= tau_pivot: defect (they will reach threshold)
        * Else if Prob(others >= m-1) >= tau_pivot: cooperate (you are likely pivotal)
        * Else: defect
    - Tie/ambiguity defaults to defect (aggressive).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.tau_exploit = 0.8
        self.tau_pivot = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n = self.game_description.n_players
        m = self.game_description.m
        rounds_done = history.round_number
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            return Action.D
        denom = max(1, rounds_done)
        counts = np.sum(opp_actions.astype(float), axis=0)
        f_js = (counts / float(denom)).tolist()
        avg_other_coop = float(np.mean(f_js))
        if avg_other_coop >= self.tau_exploit:
            return Action.D

        def prob_at_least(q: int, probs: list[float]) -> float:
            k = len(probs)
            if q <= 0:
                return 1.0
            if q > k:
                return 0.0
            dp = [0.0] * (k + 1)
            dp[0] = 1.0
            for p in probs:
                new_dp = dp[:]
                for s in range(len(dp) - 1, -1, -1):
                    without = dp[s] * (1.0 - p)
                    with_ = dp[s - 1] * p if s - 1 >= 0 else 0.0
                    new_dp[s] = without + with_
                dp = new_dp
            return float(sum(dp[q:]))
        P_at_least_m = prob_at_least(m, f_js)
        P_at_least_m_minus_1 = prob_at_least(max(0, m - 1), f_js)
        if P_at_least_m >= self.tau_pivot:
            return Action.D
        elif P_at_least_m_minus_1 >= self.tau_pivot:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_56(BaseStrategy):
    """
    Aggressive Pivotal-Reciprocal (APR)

    Defect by default. Free-ride whenever others suffice (count_predicted >= m).
    Cooperate only when pivotal (count_predicted == m-1) and predicted cooperators
    are high-confidence. Punish players who were predicted to cooperate but defected
    by excluding them for T_punish rounds; forgive after demonstrated cooperation.
    Occasionally probe with small probability p_probe to detect/rebuild cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.n_opponents = max(0, self.n - 1)
        self.punished_until = np.zeros(self.n_opponents, dtype=int)
        self.last_predicted = None
        self.predicted_round = 0
        self.perm_defect = False
        self.q_good = 0.7
        self.q_high = 0.8
        self.X_coop = 1
        self.T_punish = max(2, math.ceil(0.02 * self.r))
        self.p_probe = max(0.02, 1.0 / max(1.0, float(self.r)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        completed = 0 if history is None else int(history.round_number)
        if history is not None and completed >= 1 and (self.predicted_round == completed) and (self.last_predicted is not None):
            last_actions = history.opponent_actions[-1, :]
            betrayed = self.last_predicted & ~last_actions
            if betrayed.any():
                self.punished_until[betrayed] = completed + self.T_punish
        if history is not None and completed >= 1:
            for j in range(self.n_opponents):
                if self.punished_until[j] >= t:
                    if completed >= self.X_coop:
                        recent = history.opponent_actions[-self.X_coop:, j]
                        if recent.shape[0] == self.X_coop and recent.all():
                            self.punished_until[j] = 0
        if history is not None:
            L_collapse = min(5, max(0, completed))
            remaining = max(0, self.r - completed)
            if L_collapse > 0:
                recent_mean = float(np.mean(history.opponent_actions[-L_collapse:, :]))
                if recent_mean < 0.05 and remaining <= max(1, math.ceil(0.1 * self.r)):
                    self.perm_defect = True
        L_obs = min(5, max(0, t - 1))
        if L_obs <= 0 or history is None or history.round_number == 0:
            coop_rate = np.zeros(self.n_opponents, dtype=float)
        else:
            recent = history.opponent_actions[-L_obs:, :]
            coop_rate = np.array(np.mean(recent.astype(float), axis=0), dtype=float)
        predicted_cooperate = (self.punished_until < t) & (coop_rate >= self.q_good)
        count_predicted = int(np.sum(predicted_cooperate))
        if self.perm_defect:
            action = Action.D
        elif t == 1:
            action = Action.D
        elif random.random() < self.p_probe:
            action = Action.C
        elif count_predicted >= self.m:
            action = Action.D
        elif count_predicted == self.m - 1:
            if count_predicted == 0:
                action = Action.D
            else:
                predicted_rates = coop_rate[predicted_cooperate]
                high_conf_count = int(np.sum(predicted_rates >= self.q_high))
                if high_conf_count >= self.m - 1:
                    action = Action.C
                else:
                    action = Action.D
        else:
            action = Action.D
        self.last_predicted = np.array(predicted_cooperate, dtype=bool)
        self.predicted_round = t
        return action

class Strategy_AGGRESSIVE_57(BaseStrategy):
    """
    Aggressive Pivotalist:
    Defect by default; cooperate only when your single contribution is likely to be pivotal
    (k * Prob(others == m-1) > 1). Add a small exploration probability epsilon to occasionally
    cooperate and gather information. Estimates each opponent's cooperation probability with
    a small Bayesian shrinkage prior over a sliding window of recent rounds and computes the
    Poisson–binomial exactly (DP) for moderate opponent counts, falling back to a normal
    approximation for large counts.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.epsilon = 0.01
        self.alpha = 0.1
        self.beta = 0.9
        self.window = 50
        self.dp_threshold = 200

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        n_opponents = max(0, n - 1)
        if random.random() < self.epsilon:
            return Action.C
        if history is None or history.round_number == 0:
            p0 = self.alpha / (self.alpha + self.beta)
            p_list = [p0] * n_opponents
        else:
            W = self.window if self.window is not None else history.round_number
            rounds_to_use = min(W, history.round_number)
            if rounds_to_use <= 0:
                p_list = [self.alpha / (self.alpha + self.beta)] * n_opponents
            else:
                recent = history.opponent_actions[-rounds_to_use:, :]
                coop_counts = np.sum(recent.astype(float), axis=0) if recent.size else np.zeros(n_opponents)
                denom = self.alpha + self.beta + rounds_to_use
                p_list = [(self.alpha + float(coop_counts[j])) / denom for j in range(n_opponents)]
        target = m - 1
        if target < 0 or target > n_opponents:
            p_eq = 0.0
        elif n_opponents <= self.dp_threshold:
            p_eq = self._poisson_binomial_pmf(p_list, target)
        else:
            p_eq = self._poisson_binomial_normal_approx(p_list, target)
        if k * p_eq > 1.0:
            return Action.C
        return Action.D

    def _poisson_binomial_pmf(self, p_list, target):
        n = len(p_list)
        prob = [0.0] * (n + 1)
        prob[0] = 1.0
        for p in p_list:
            for t in range(n, 0, -1):
                prob[t] = prob[t] * (1.0 - p) + prob[t - 1] * p
            prob[0] = prob[0] * (1.0 - p)
        return prob[target] if 0 <= target <= n else 0.0

    def _poisson_binomial_normal_approx(self, p_list, target):
        mu = float(sum(p_list))
        var = float(sum([p * (1.0 - p) for p in p_list]))
        if var <= 0.0:
            return 1.0 if abs(mu - target) < 1e-09 else 0.0
        sigma = math.sqrt(var)

        def std_cdf(x):
            return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
        lo = (target - 0.5 - mu) / sigma
        hi = (target + 0.5 - mu) / sigma
        return max(0.0, min(1.0, std_cdf(hi) - std_cdf(lo)))

class Strategy_AGGRESSIVE_58(BaseStrategy):
    """
    Pivotal Threshold Exploiter (PTE)

    Defect by default. Cooperate only when the empirical probability that the
    other n-1 players will supply exactly m-1 cooperators (making your single
    cooperation pivotal) exceeds 1/k (estimated from a sliding window with
    Laplace smoothing). If you cooperate and the threshold still fails (you
    were suckered), enter a short retaliation block of unconditional defection;
    repeated suckering increases the retaliation length up to a cap, and
    retaliation length slowly forgivingly decays back toward the initial value.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_max = 20
        self.alpha = 0.5
        self.T_ret_initial = 3
        self.T_ret_increment = 2
        self.T_ret_max = 6
        self.retaliation_timer = 0
        self.current_T_ret = self.T_ret_initial
        self._prev_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        was_suckered = False
        if history.round_number >= 1:
            last_my_action = bool(history.my_actions[-1])
            last_X = int(np.sum(history.opponent_actions[-1, :]))
            if last_my_action and last_X <= m - 2:
                was_suckered = True
        if was_suckered:
            self.retaliation_timer = int(self.current_T_ret)
            self.current_T_ret = min(self.T_ret_max, int(self.current_T_ret + self.T_ret_increment))
        if self.retaliation_timer > 0:
            action = Action.D
            self.retaliation_timer -= 1
        else:
            t = history.round_number + 1
            W = min(self.W_max, max(0, t - 1))
            if W <= 0:
                action = Action.D
            else:
                recent_opponent = history.opponent_actions[-W:, :]
                counts = np.sum(recent_opponent, axis=1)
                count_piv = int(np.sum(counts == m - 1))
                p_emp = (count_piv + self.alpha) / (W + 2.0 * self.alpha)
                if p_emp > 1.0 / k:
                    action = Action.C
                else:
                    action = Action.D
        if self._prev_active and self.retaliation_timer == 0 and (not was_suckered):
            if self.current_T_ret > self.T_ret_initial:
                self.current_T_ret = max(self.T_ret_initial, int(self.current_T_ret - 1))
        self._prev_active = self.retaliation_timer > 0
        return action

class Strategy_AGGRESSIVE_59(BaseStrategy):
    """
    Selfish-pivotal strategy for the Collective Risk Dilemma.

    Defect by default; estimate each opponent's cooperation rate with Laplace smoothing,
    compute the Poisson–Binomial probability that exactly m-1 others cooperate (P_piv),
    and cooperate only when P_piv * k > 1 (i.e. P_piv > 1/k) and cooperating is not redundant
    (Pr(others >= m) < 0.9). Use a small, decaying probing probability early to learn opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.eps_max = 0.05
        self.p_atleast_cutoff = 0.9
        self.tiny_prob = 1e-12

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        if history is None:
            rounds_completed = 0
            upcoming_round = 1
        else:
            rounds_completed = history.round_number
            upcoming_round = rounds_completed + 1
        if upcoming_round >= r:
            eps = 0.0
        else:
            eps = min(self.eps_max, 1.0 / math.sqrt(1.0 + rounds_completed))
        if history is None:
            if random.random() < eps:
                return Action.C
            return Action.D
        n_opponents = n - 1
        if m > n:
            return Action.D
        if rounds_completed == 0:
            rounds_seen = 0
            coop_counts = [0] * n_opponents
        else:
            rounds_seen = rounds_completed
            coop_counts = [int(x) for x in np.sum(history.opponent_actions.astype(int), axis=0).tolist()]
        p_list = []
        for j in range(n_opponents):
            pj = (coop_counts[j] + self.alpha) / (rounds_seen + 2.0 * self.alpha)
            if pj < 0.0:
                pj = 0.0
            elif pj > 1.0:
                pj = 1.0
            p_list.append(float(pj))
        dp = [0.0] * (n_opponents + 1)
        dp[0] = 1.0
        current_max = 0
        for p in p_list:
            for s in range(current_max, -1, -1):
                prob_s = dp[s]
                if prob_s == 0.0:
                    dp[s + 1] += 0.0
                    dp[s] = dp[s] * (1.0 - p)
                else:
                    dp[s + 1] += prob_s * p
                    dp[s] = prob_s * (1.0 - p)
            current_max += 1
        idx_piv = m - 1
        if 0 <= idx_piv <= n_opponents:
            P_piv = dp[idx_piv]
        else:
            P_piv = 0.0
        P_atleast = 0.0
        if m <= n_opponents:
            P_atleast = sum(dp[m:])
        if P_piv < self.tiny_prob and P_atleast < self.tiny_prob:
            return Action.D
        if eps > 0.0 and random.random() < eps:
            return Action.C
        if P_piv * k > 1.0 and P_atleast < self.p_atleast_cutoff:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_60(BaseStrategy):
    """
    Aggressive free-rider: default-defect, cooperate only when your single contribution
    increases the chance of securing the group reward enough that k * delta > 1;
    punish players who let you be a sucker, probe occasionally, and always defect in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha = 0.4
        self.e_explore = 0.02
        self.epsilon_tie = 0.0
        self.punish_len = max(3, math.ceil(self.r / 6))
        self.n_opp = max(0, self.n - 1)
        self.p = np.full(self.n_opp, 0.5, dtype=float)
        self.blame = np.zeros(self.n_opp, dtype=bool)
        self.consecutive_coop = np.zeros(self.n_opp, dtype=int)
        self.punish_counter = 0

    def _poisson_binomial_p_ge(self, probs, threshold):
        """Exact DP for Poisson-Binomial: probability that sum of Bernoullis >= threshold."""
        n = len(probs)
        if threshold <= 0:
            return 1.0
        if threshold > n:
            return 0.0
        dp = [0.0] * (n + 1)
        dp[0] = 1.0
        for p in probs:
            q = 1.0 - p
            for k in range(n, 0, -1):
                dp[k] = dp[k] * q + dp[k - 1] * p
            dp[0] = dp[0] * q
        return sum(dp[threshold:])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = state.round_number
        if history.round_number >= 1:
            last_idx = -1
            last_opponent_actions = history.opponent_actions[last_idx, :]
            for j in range(self.n_opp):
                observed = 1.0 if bool(last_opponent_actions[j]) else 0.0
                self.p[j] = self.alpha * observed + (1.0 - self.alpha) * self.p[j]
            for j in range(self.n_opp):
                if bool(last_opponent_actions[j]):
                    self.consecutive_coop[j] = self.consecutive_coop[j] + 1
                else:
                    self.consecutive_coop[j] = 0
            if history.my_actions.size >= 1:
                i_coop_last = bool(history.my_actions[-1])
                others_coops = int(np.sum(history.opponent_actions[-1, :])) if self.n_opp > 0 else 0
                total_coops = others_coops + (1 if i_coop_last else 0)
                threshold_met = total_coops >= self.m
                if i_coop_last and (not threshold_met):
                    newly_blamed = False
                    for j in range(self.n_opp):
                        opp_coop = bool(history.opponent_actions[-1, j])
                        if not opp_coop and total_coops + 1 >= self.m:
                            if not self.blame[j]:
                                self.blame[j] = True
                                newly_blamed = True
                    if newly_blamed:
                        self.punish_counter = max(self.punish_counter, self.punish_len)
            if history.round_number >= 3:
                last_k = 3
                recent = history.opponent_actions[-last_k:, :]
                for j in range(self.n_opp):
                    if self.blame[j]:
                        if np.all(recent[:, j]):
                            self.blame[j] = False
                            self.consecutive_coop[j] = last_k
        if t >= self.r:
            if self.punish_counter > 0:
                self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        epsilon = self.epsilon_tie
        if t >= self.r - 1:
            epsilon = max(epsilon, 1.0)
        distrust_mult = 0.2
        effective_p = np.array([float(self.p[j]) * (distrust_mult if self.blame[j] else 1.0) for j in range(self.n_opp)], dtype=float)
        effective_p = np.clip(effective_p, 0.0, 1.0)
        P_without = self._poisson_binomial_p_ge(effective_p.tolist(), self.m)
        P_with = self._poisson_binomial_p_ge(effective_p.tolist(), max(0, self.m - 1))
        delta = P_with - P_without
        if self.k * delta > 1.0 + epsilon:
            return Action.C
        if random.random() < self.e_explore:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_61(BaseStrategy):
    """
    Aggressive opportunist for the Collective Risk Dilemma.

    - Maintains per-opponent empirical cooperation probabilities (Bayesian smoothed with a low prior p0).
    - Estimates the probability that exactly (m-1) of the other players cooperate (Poisson–binomial DP when feasible,
      otherwise binomial approximation using mean p).
    - Cooperates myopically iff k * P_exact > 1 (tie -> defect).
    - With small probability epsilon_explore flips the chosen action to probe.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.05
        self.epsilon_explore = 0.01
        self.prior_strength = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None or history.round_number == 0:
            p_js = [self.p0] * (n - 1)
        else:
            rounds_seen = history.round_number
            opp_actions = history.opponent_actions
            coop_counts = np.sum(opp_actions.astype(float), axis=0)
            prior_a = self.p0 * self.prior_strength
            prior_b = (1.0 - self.p0) * self.prior_strength
            denom = float(rounds_seen + prior_a + prior_b)
            if rounds_seen == 0:
                p_js = [self.p0] * (n - 1)
            else:
                p_js = [float((coop_counts[j] + prior_a) / denom) for j in range(n - 1)]
        n_other = n - 1
        target = m - 1
        if target < 0 or target > n_other:
            P_exact = 0.0
        elif n_other <= 40:
            probs = [0.0] * (n_other + 1)
            probs[0] = 1.0
            for p in p_js:
                prev = probs[:]
                probs[0] = prev[0] * (1.0 - p)
                for kidx in range(1, n_other + 1):
                    probs[kidx] = prev[kidx] * (1.0 - p) + prev[kidx - 1] * p
            P_exact = float(probs[target])
        else:
            p_bar = float(np.mean(np.array(p_js, dtype=float)))
            if p_bar <= 0.0:
                P_exact = 1.0 if target == 0 else 0.0
            elif p_bar >= 1.0:
                P_exact = 1.0 if target == n_other else 0.0
            else:
                coef = float(math.comb(n_other, target))
                P_exact = coef * p_bar ** target * (1.0 - p_bar) ** (n_other - target)
        action = Action.C if k * P_exact > 1.0 else Action.D
        if random.random() < self.epsilon_explore:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_62(BaseStrategy):
    """
    Aggressive, opportunistic free-rider / pivotal strategy for the Collective Risk Dilemma.

    - Uses a rolling window of recent rounds to build an empirical distribution over
      how many opponents (other players) cooperated in a round.
    - Computes expected payoffs for cooperating vs defecting and chooses the action
      that maximizes expected private payoff, breaking ties in favor of defection.
    - Applies heuristics: always defect on first and last rounds, exploit high probability
      of others meeting the threshold, avoid cooperating when cooperation is almost never
      pivotal, and randomize slightly in marginal cases to avoid deterministic coordination failures.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.default_W = 10
        self.p_free_ride = 0.8
        self.p_no_pivot = 0.1
        self.margin_strong_default = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None or history.round_number == 0:
            return Action.D
        if state.round_number >= r:
            return Action.D
        W = min(self.default_W, max(1, r - 1))
        rounds_observed = history.round_number
        use_last = min(W, rounds_observed)
        if use_last <= 0:
            return Action.D
        opp_actions = history.opponent_actions
        recent = opp_actions[-use_last:, :]
        s_counts = np.sum(recent, axis=1).astype(int)
        sample_count = len(s_counts)
        p_s = np.zeros(n, dtype=float)
        if sample_count > 0:
            binc = np.bincount(s_counts, minlength=n)
            p_s = binc / float(sample_count)
        idx_without = max(0, m)
        P_threshold_without_me = float(np.sum(p_s[idx_without:])) if idx_without < n else 0.0
        idx_with = max(0, m - 1)
        P_threshold_with_me = float(np.sum(p_s[idx_with:])) if idx_with < n else 0.0
        margin_strong = self.margin_strong_default
        if r == 2:
            margin_strong = 0.1
        E_C = k * P_threshold_with_me
        E_D = 1.0 + k * P_threshold_without_me
        if P_threshold_without_me >= self.p_free_ride:
            return Action.D
        if P_threshold_with_me <= self.p_no_pivot:
            return Action.D
        diff = E_C - E_D
        if diff >= margin_strong:
            return Action.C
        if E_C <= E_D:
            return Action.D
        p_coop = min(0.5, diff / margin_strong * 0.5)
        if random.random() < p_coop:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_63(BaseStrategy):
    """
    Aggressive Ambusher:
    - Default: defect.
    - Never cooperate in the final E rounds (E=1 by default).
    - Free-ride if opponents alone met the threshold in the previous round.
    - If opponents were exactly one short last round (pivotal case), cooperate only if
      their recent cooperation fraction over a short lookback L meets a consistency threshold alpha.
      If cooperating as pivotal, mark the next round to unconditionally defect (exploit).
    - If opponents were one short but fail the consistency test, defect to sabotage.
    - Occasionally probe with small probability epsilon to detect unconditional cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.m = int(game_description.m)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.E = 1
        self.L = min(5, max(1, self.r - 1))
        self.alpha = max(0.6, 0.9 * float(max(0, self.m - 1)) / float(max(1, self.n - 1)))
        self.epsilon = 0.03
        self.exploit_next_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.exploit_next_round = False
            return Action.D
        t = int(state.round_number)
        if self.exploit_next_round:
            self.exploit_next_round = False
            return Action.D
        if t > self.r - self.E:
            return Action.D
        completed = int(history.round_number)
        if completed == 0:
            return Action.D
        prev_idx = completed - 1
        c_prev_others = int(np.sum(history.opponent_actions[prev_idx, :]))
        if c_prev_others >= self.m:
            return Action.D
        start_idx = max(0, completed - self.L)
        lookback_rounds = completed - start_idx
        if lookback_rounds <= 0:
            coop_fraction = 0.0
        else:
            total_opponent_coop = float(np.sum(history.opponent_actions[start_idx:completed, :]))
            denom = float(lookback_rounds * (self.n - 1))
            coop_fraction = total_opponent_coop / denom if denom > 0.0 else 0.0
        if c_prev_others == self.m - 1:
            if coop_fraction >= self.alpha:
                self.exploit_next_round = True
                return Action.C
            else:
                return Action.D
        if c_prev_others == self.m - 2:
            pass
        if t <= self.r - self.E - 1 and random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_64(BaseStrategy):
    """
    Pivotal-Grim Aggressor (PGA)

    - Default: defect.
    - Cooperate only when your single cooperation is predicted to be pivotal
      (predicted number of other cooperators == m-1).
    - If at any past round you cooperated, the group succeeded (>= m cooperators),
      and at least one other player defected that round, enter permanent grim:
      never cooperate again.
    - Prediction: empirical cooperation rates of opponents (coop_count / rounds_so_far),
      sum them to get expected_others, round to nearest integer with .5 ties broken downward.
    - Deterministic, uses only observed history and game parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        n = self.game_description.n_players
        m = self.game_description.m
        rounds_completed = history.round_number
        exploited_flag = False
        for r_idx in range(rounds_completed):
            my_act = bool(history.my_actions[r_idx])
            if not my_act:
                continue
            opp_coop_vector = history.opponent_actions[r_idx, :]
            opp_cooperators = int(np.sum(opp_coop_vector))
            total_cooperators = 1 + opp_cooperators
            if total_cooperators >= m:
                if opp_cooperators < n - 1:
                    exploited_flag = True
                    break
        if exploited_flag:
            return Action.D
        opp_coop_counts = np.sum(history.opponent_actions[:rounds_completed, :], axis=0)
        expected_others = float(np.sum(opp_coop_counts) / float(rounds_completed))
        base = math.floor(expected_others)
        frac = expected_others - base
        if abs(frac - 0.5) < 1e-12:
            predicted_others = int(base)
        else:
            predicted_others = int(math.floor(expected_others + 0.5))
        if predicted_others < 0:
            predicted_others = 0
        elif predicted_others > n - 1:
            predicted_others = n - 1
        if predicted_others >= m:
            return Action.D
        if predicted_others <= m - 2:
            return Action.D
        return Action.C

class Strategy_AGGRESSIVE_65(BaseStrategy):
    """
    Aggressive, self-maximizing Collective Risk strategy:
    defect by default, free-ride on safe rounds, only be pivotal when trusted cooperators exist,
    and punish/ostracize defectors until they rebuild trust.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.tau = 0.6
        self.rebuild_len = 2
        self.n_opponents = max(0, self.n - 1)
        self._initial_punitive = [False] * self.n_opponents

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed == 0:
            return Action.D
        n_op = self.n_opponents
        punitive = [False] * n_op
        consec_coop = [0] * n_op
        opp_total_coops = [0] * n_op
        for rnd in range(rounds_completed):
            my_act = bool(history.my_actions[rnd])
            opp_row = history.opponent_actions[rnd]
            for j in range(n_op):
                if bool(opp_row[j]):
                    opp_total_coops[j] += 1
            total_coops_round = int(sum((1 for x in opp_row if bool(x)))) + (1 if my_act else 0)
            if my_act and total_coops_round < self.m:
                for j in range(n_op):
                    if not bool(opp_row[j]):
                        punitive[j] = True
                        consec_coop[j] = 0
            for j in range(n_op):
                if punitive[j]:
                    if bool(opp_row[j]):
                        consec_coop[j] += 1
                        if consec_coop[j] >= self.rebuild_len:
                            punitive[j] = False
                            consec_coop[j] = 0
                    else:
                        consec_coop[j] = 0
        coop_rate = [opp_total_coops[j] / float(rounds_completed) for j in range(n_op)]
        last_idx = rounds_completed - 1
        last_opp_row = history.opponent_actions[last_idx]
        last_my = bool(history.my_actions[last_idx])
        coop_count_last = int(sum((1 for x in last_opp_row if bool(x)))) + (1 if last_my else 0)
        if coop_count_last >= self.m:
            return Action.D
        if coop_count_last <= self.m - 2:
            return Action.D
        S_last = [j for j in range(n_op) if bool(last_opp_row[j])]
        trusted_in_S = 0
        for j in S_last:
            if coop_rate[j] >= self.tau and (not punitive[j]):
                trusted_in_S += 1
        if trusted_in_S >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_66(BaseStrategy):
    """
    Aggressive pivotal-cooperator strategy for the Collective Risk Dilemma.

    - Defects by default to exploit cooperators.
    - Maintains an EMA estimate (p_hat) of opponents' cooperation rate.
    - Cooperates only when the probability that exactly (m-1) of the other
      players cooperate (i.e., you are pivotal) is at least 1/k.
    - Adds a small exploration/probing probability (epsilon).
    - Downweights p_hat toward the end of the game with endgame_bias_beta.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_hat = 0.05
        self.alpha = 0.3
        self.epsilon = 0.03
        self.endgame_bias_beta = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k_reward = float(self.game_description.k)
        t = int(state.round_number) if hasattr(state, 'round_number') else 1
        if t < 1:
            t = 1
        if history is None or history.round_number == 0:
            if random.random() < self.epsilon:
                return Action.C
            return Action.D
        last_opponent_actions = history.opponent_actions[-1, :]
        coop_count_last = float(np.sum(last_opponent_actions))
        denom = float(max(1, n - 1))
        frac_coop_last = coop_count_last / denom
        self.p_hat = self.alpha * frac_coop_last + (1.0 - self.alpha) * float(self.p_hat)
        stage = (t - 1) / float(max(1, r - 1))
        p_adj = float(self.p_hat) * (1.0 - (1.0 - float(self.endgame_bias_beta)) * stage)
        if p_adj < 0.0:
            p_adj = 0.0
        elif p_adj > 1.0:
            p_adj = 1.0
        trials = n - 1
        successes = m - 1
        ProbPivotal = 0.0
        if 0 <= successes <= trials:
            if p_adj == 0.0:
                ProbPivotal = 1.0 if successes == 0 else 0.0
            elif p_adj == 1.0:
                ProbPivotal = 1.0 if successes == trials else 0.0
            else:
                comb = math.comb(trials, successes)
                ProbPivotal = comb * p_adj ** successes * (1.0 - p_adj) ** (trials - successes)
        else:
            ProbPivotal = 0.0
        threshold = 1.0 / float(k_reward) if k_reward > 0 else float('inf')
        if ProbPivotal >= threshold:
            return Action.C
        if random.random() < float(self.epsilon):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_67(BaseStrategy):
    """
    Opportunistic Pivotal Exploiter (OPE)

    Aggressive, selfish strategy: defect by default, model each opponent's
    cooperation frequency with Laplace smoothing, compute the exact
    probability that exactly (m-1) opponents will cooperate (DP convolution),
    and cooperate only when the expected marginal benefit k * P_exact
    clearly exceeds the one-round cost (1) by a defect_bias margin.
    Small randomized tie-breaking keeps behavior non-deterministic.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.delta = 0.05
        self.eps = 1e-06

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        rounds_seen = history.round_number
        n_opponents = max(0, n - 1)
        if n_opponents == 0:
            return Action.D
        if rounds_seen == 0 or history.opponent_actions.size == 0:
            counts = np.zeros(n_opponents, dtype=float)
        else:
            counts = np.sum(history.opponent_actions.astype(float), axis=0)
        denom = rounds_seen + 2.0 * self.alpha
        if denom <= 0:
            p_js = [0.5] * n_opponents
        else:
            p_js = [float((counts[j] + self.alpha) / denom) for j in range(n_opponents)]
        target = m - 1
        if target < 0 or target > n_opponents:
            P_exact = 0.0
        else:
            prob = [0.0] * (n_opponents + 1)
            prob[0] = 1.0
            for p in p_js:
                for s in range(n_opponents - 1, -1, -1):
                    prob[s + 1] += prob[s] * p
                    prob[s] *= 1.0 - p
            P_exact = prob[target]
        score = k * P_exact
        threshold = 1.0 + self.delta
        if score > threshold + self.eps:
            return Action.C
        elif score < threshold - self.eps:
            return Action.D
        else:
            return Action.C if random.random() < 0.1 else Action.D

class Strategy_AGGRESSIVE_68(BaseStrategy):
    """
    Aggressive Collective Risk strategy.

    - Default: defect.
    - Cooperate only when immediate expected payoff of cooperating strictly exceeds defecting.
    - Never cooperate on last round.
    - Probe occasionally early to discover cooperators.
    - Track exploit events (when I cooperated and round succeeded while an opponent defected).
      After L exploits by an opponent, blacklist them for T rounds (treat as certain defectors).
    - Uses a sliding window W for estimating opponents' cooperation probabilities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k_reward = float(game_description.k)
        self.W = min(10, max(1, self.r - 1))
        self.probe_prob_initial = 0.2
        self.probe_decay = 0.95
        self.L = 2
        self.T = 5
        self.eps = 1e-06
        self.num_opponents = max(0, self.n - 1)
        self.exploit_count = [0 for _ in range(self.num_opponents)]
        self.blacklist = [0 for _ in range(self.num_opponents)]
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is not None:
            rounds_completed = history.round_number
            for idx in range(self.last_processed_round, rounds_completed):
                my_act = bool(history.my_actions[idx])
                opp_row = history.opponent_actions[idx, :] if self.num_opponents > 0 else np.array([], dtype=np.bool_)
                others_coops = int(np.sum(opp_row.astype(np.int32))) if opp_row.size else 0
                total_coops = others_coops + (1 if my_act else 0)
                if my_act and total_coops >= self.m:
                    for j in range(self.num_opponents):
                        if not bool(opp_row[j]):
                            self.exploit_count[j] += 1
                            if self.exploit_count[j] >= self.L:
                                self.blacklist[j] = self.T
                for j in range(self.num_opponents):
                    if self.blacklist[j] > 0:
                        self.blacklist[j] -= 1
            self.last_processed_round = rounds_completed
        p_list = [0.5 for _ in range(self.num_opponents)]
        if history is None or history.round_number == 0:
            observed_window = 0
        else:
            observed_window = min(self.W, history.round_number)
        for j in range(self.num_opponents):
            if self.blacklist[j] > 0:
                p_list[j] = 0.0
            elif observed_window == 0:
                p_list[j] = 0.5
            else:
                recent = history.opponent_actions[-observed_window:, j]
                coop_count = int(np.sum(recent.astype(np.int32)))
                p_list[j] = coop_count / float(observed_window)
        if t >= self.r:
            return Action.D
        P = np.zeros(self.n, dtype=float)
        P[0] = 1.0
        for p in p_list:
            newP = np.zeros_like(P)
            max_k = self.n - 1
            for kcount in range(0, max_k + 1):
                if P[kcount] == 0.0:
                    continue
                newP[kcount] += P[kcount] * (1.0 - p)
                if kcount + 1 <= max_k:
                    newP[kcount + 1] += P[kcount] * p
            P = newP
        expected_C = 0.0
        expected_D = 0.0
        max_other = self.n - 1
        for X in range(0, max_other + 1):
            prob = float(P[X])
            if X + 1 >= self.m:
                payoff_C = float(self.k_reward)
            else:
                payoff_C = 0.0
            if X >= self.m:
                payoff_D = 1.0 + float(self.k_reward)
            else:
                payoff_D = 1.0
            expected_C += prob * payoff_C
            expected_D += prob * payoff_D
        if expected_C > expected_D + self.eps:
            return Action.C
        if abs(expected_C - expected_D) <= self.eps and t <= self.r / 2.0:
            p_probe = self.probe_prob_initial * self.probe_decay ** (t - 1)
            if random.random() < p_probe:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_69(BaseStrategy):
    """
    Aggressive Pivotal Defector (APD)

    Defect by default. Estimate opponents' per-round cooperation probability from history,
    compute the probability that exactly m-1 opponents cooperate (p_exact). Cooperate only
    when p_exact > 1/k (i.e., cooperating has higher immediate expected payoff). Track
    rounds in which you cooperated but the threshold still failed (ExploitCount). If
    ExploitCount >= T (default 2) switch to permanent defection for the remainder of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = 2
        self.permanent_defect = False
        self.exploit_count = 0
        self._eps = 1e-12

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if self.permanent_defect:
            return Action.D
        if history is None or history.round_number == 0:
            if n - 1 > 0:
                p_hat = (m - 1) / (n - 1)
            else:
                p_hat = 0.0
            if p_hat <= self._eps:
                p_exact = 1.0 if m - 1 == 0 else 0.0
            elif 1.0 - p_hat <= self._eps:
                p_exact = 1.0 if m - 1 == n - 1 else 0.0
            else:
                coeff = math.comb(n - 1, m - 1)
                p_exact = coeff * p_hat ** (m - 1) * (1.0 - p_hat) ** (n - 1 - (m - 1))
            return Action.C if p_exact > 1.0 / k else Action.D
        rounds_prev = history.round_number
        total_opponent_coops = int(np.sum(history.opponent_actions)) if rounds_prev > 0 else 0
        denom = rounds_prev * (n - 1)
        if denom > 0:
            p_hat = float(total_opponent_coops) / float(denom)
        else:
            p_hat = (m - 1) / (n - 1) if n - 1 > 0 else 0.0
        if p_hat <= self._eps:
            p_exact = 1.0 if m - 1 == 0 else 0.0
        elif 1.0 - p_hat <= self._eps:
            p_exact = 1.0 if m - 1 == n - 1 else 0.0
        else:
            coeff = math.comb(n - 1, m - 1)
            p_exact = coeff * p_hat ** (m - 1) * (1.0 - p_hat) ** (n - 1 - (m - 1))
        exploit_count = 0
        for r in range(rounds_prev):
            my_coop = bool(history.my_actions[r])
            opp_coops_this_round = int(np.sum(history.opponent_actions[r, :]))
            total_coops_this_round = opp_coops_this_round + (1 if my_coop else 0)
            if my_coop and total_coops_this_round < m:
                exploit_count += 1
        self.exploit_count = exploit_count
        if self.exploit_count >= self.T:
            self.permanent_defect = True
            return Action.D
        if p_exact > 1.0 / k:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_70(BaseStrategy):
    """
    Pivotal Exploiter

    Aggressive, adaptive strategy for the Collective Risk Dilemma.
    - Cooperates only when the probability of being pivotal (exactly m-1 other cooperators)
      is large enough so that k * P_exact > 1.
    - Otherwise defects, but with a small exploration probability to probe for persistent
      cooperators (epsilon). No exploration in the final round.
    - Maintains punishment implicitly by scanning history: if you cooperated in a past round
      and the group still failed (< m cooperators), all defectors that round are marked as
      exploiters. Exploiters' estimated cooperation probabilities are severely discounted.
    - Uses a defection-biased prior p0 and smoothing parameter beta to estimate per-opponent
      cooperation probabilities. Computes the Poisson-binomial probability P_exact via DP.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.02
        self.beta = 1.0
        self.min_epsilon = 0.01
        self.discount_for_exploiter = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        n_opp = n - 1
        if history is None:
            t_obs = 0
        else:
            t_obs = history.round_number
        if t_obs == 0:
            coop_counts = np.zeros(n_opp, dtype=float)
        else:
            coop_counts = np.sum(history.opponent_actions, axis=0).astype(float)
        exploit_flags = np.zeros(n_opp, dtype=bool)
        if t_obs > 0:
            for rr in range(t_obs):
                my_coop = bool(history.my_actions[rr])
                if not my_coop:
                    continue
                opps_this_round = history.opponent_actions[rr, :]
                total_coops = 1 + int(np.sum(opps_this_round))
                if total_coops < m:
                    defectors = ~opps_this_round
                    exploit_flags |= defectors
        denom = float(t_obs + self.beta)
        p_j_raw = (coop_counts + self.beta * self.p0) / denom
        p_j_raw = np.clip(p_j_raw, 0.0, 1.0)
        p_j = p_j_raw.copy()
        if n_opp > 0:
            p_j[exploit_flags] = p_j[exploit_flags] * self.discount_for_exploiter
        m_minus_1 = m - 1
        if m_minus_1 < 0 or m_minus_1 > n_opp:
            P_exact = 0.0
        else:
            prob = [0.0] * (n_opp + 1)
            prob[0] = 1.0
            for pj in p_j:
                for s in range(n_opp - 1, -1, -1):
                    add = prob[s] * pj
                    prob[s + 1] += add
                    prob[s] *= 1.0 - pj
            P_exact = prob[m_minus_1]
        upcoming_round = t_obs + 1
        if upcoming_round >= r:
            epsilon = 0.0
        else:
            epsilon = max(self.min_epsilon, 1.0 / float(r))
        if k * P_exact > 1.0:
            return Action.C
        if epsilon > 0.0 and random.random() < float(epsilon):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_71(BaseStrategy):
    """
    Pivotal Bully (PB)

    Aggressive, self-serving strategy for the Collective Risk Dilemma.
    - Default: defect.
    - Predict others will repeat their last action.
    - Cooperate only when your single cooperation is predicted to be pivotal
      (i.e., predicted_other_coops == m - 1), unless currently in punishment.
    - Small-probability probes (ε) when your cooperation would not be pivotal to
      explore opponent behavior, but probes are suspended until opponents
      "earn back" trust after a punishment window.
    - Track being a "sucker" (you cooperated but group still failed). After
      T_exploit such sucker-events, enter punishment for F rounds refusing
      pivotal cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.epsilon = 0.02
        self.F = 3
        self.T_exploit = 2
        self.exploit_count = 0
        self.punishment_timer = 0
        self.last_sucker_total = 0
        self.punishment_snapshot_freq = None
        self.wait_for_repair = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            if self.punishment_timer > 0:
                self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        rounds = history.round_number
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        last_actions = opp_actions[-1, :].astype(bool)
        predicted_other_coops = int(np.sum(last_actions))
        if rounds > 0:
            freq_j = np.sum(opp_actions.astype(int), axis=0) / float(rounds)
        else:
            freq_j = np.zeros(self.n - 1, dtype=float)
        opponent_coop_counts_per_round = np.sum(opp_actions.astype(int), axis=1)
        my_coop_mask = my_actions.astype(bool)
        failed_after_my_coop_mask = my_coop_mask & (opponent_coop_counts_per_round + 1 < self.m)
        sucker_total = int(np.sum(failed_after_my_coop_mask))
        if sucker_total > self.last_sucker_total:
            new_suckers = sucker_total - self.last_sucker_total
            self.exploit_count += int(new_suckers)
            self.last_sucker_total = sucker_total
        if self.punishment_timer == 0 and self.exploit_count >= self.T_exploit:
            self.punishment_timer = int(self.F)
            self.punishment_snapshot_freq = np.array(freq_j, dtype=float)
            self.exploit_count = 0
            self.wait_for_repair = True
        punishment_active = self.punishment_timer > 0
        action = Action.D
        if predicted_other_coops == self.m - 1 and (not punishment_active):
            action = Action.C
        elif predicted_other_coops <= self.m - 2:
            probe_allowed = True
            if self.wait_for_repair and self.punishment_snapshot_freq is not None:
                improved = np.any(freq_j > self.punishment_snapshot_freq + 1e-12)
                probe_allowed = bool(improved)
                if improved:
                    self.wait_for_repair = False
                    self.punishment_snapshot_freq = None
            if probe_allowed and random.random() < self.epsilon:
                action = Action.C
            else:
                action = Action.D
        else:
            action = Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            if self.punishment_timer == 0:
                pass
        return action

class Strategy_AGGRESSIVE_72(BaseStrategy):
    """
    Aggressive Threshold Opportunist (ATO)

    - Defaults to defect (especially in the first round).
    - Estimates each opponent's cooperation probability with Laplace smoothing (alpha=1).
    - Computes exact Poisson-Binomial distribution (DP convolution) for the number of cooperating
      opponents, then computes the pivot probability q_pivot = Pr(exactly m-1 others cooperate).
    - Cooperates iff k * q_pivot > 1 (ties -> defect).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._alpha = 1
        self._aggressive_first_round = True
        self._eps = 1e-12

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if n < 2 or r <= 1 or (not 1 < m < n) or (k <= 1):
            return Action.D
        if history is None or history.round_number == 0:
            if self._aggressive_first_round:
                return Action.D
        T = 0 if history is None else history.round_number
        others_count = n - 1
        if history is None:
            p_list = [0.5] * others_count
        else:
            opp_actions = history.opponent_actions
            if T == 0:
                p_list = [0.5] * others_count
            elif opp_actions.ndim != 2 or opp_actions.shape[1] != others_count:
                p_list = [0.5] * others_count
            else:
                counts = np.sum(opp_actions.astype(float), axis=0)
                denom = float(T + 2 * self._alpha)
                p_list = [float((counts[j] + self._alpha) / denom) for j in range(others_count)]
        q = np.zeros(others_count + 1, dtype=float)
        q[0] = 1.0
        for p in p_list:
            new_q = np.zeros_like(q)
            new_q[0] = q[0] * (1.0 - p)
            for k_idx in range(1, others_count + 1):
                new_q[k_idx] = q[k_idx] * (1.0 - p) + q[k_idx - 1] * p
            q = new_q
        pivot_index = m - 1
        if 0 <= pivot_index <= others_count:
            q_pivot = float(q[pivot_index])
        else:
            q_pivot = 0.0
        if k * q_pivot - 1.0 > self._eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_73(BaseStrategy):
    """
    Pivotal Aggressor

    Aggressive myopic strategy for the Collective Risk Dilemma:
    - Estimates each opponent's cooperation probability with exponential recency weighting and a pessimistic prior.
    - Computes the probability P_exact that exactly (m-1) of the other players will cooperate.
    - Cooperates this round iff k * P_exact > 1 (strict), otherwise defects. Tie-breaks favor defection.
    - Always defects in the final round.
    - Detects "betrayals" in past rounds where this player cooperated expecting success but the group failed;
      players who defected despite being expected to cooperate are punished for a short duration by lowering their
      estimated p_j and by defecting while any punishments are active.
    - Uses dynamic programming to compute the exact heterogeneous Bernoulli sum distribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.decay = 0.9
        self.p_init = 0.1
        self.prior_alpha = 1e-06
        self.p_min = 0.01
        self.eps = 1e-06
        self.p_expected_threshold = 0.5
        self.punish_factor = 0.1
        self.punish_duration = 2
        n = int(self.game_description.n_players)
        self.n_opponents = max(0, n - 1)
        self.punish_timers = np.zeros(self.n_opponents, dtype=int)
        self.punish_multiplier = np.ones(self.n_opponents, dtype=float)
        self.last_history_rounds = 0

    def _compute_weighted_counts(self, opponent_actions: NDArray[np.bool_], rounds_considered: int):
        """
        Compute weighted coop counts and total weights for each opponent using exponential recency weighting
        over opponent_actions[:rounds_considered, :].
        Returns (count_coop, count_total) arrays of length n_opponents (float).
        """
        if self.n_opponents == 0:
            return (np.array([]), np.array([]))
        count_coop = np.full(self.n_opponents, self.prior_alpha * self.p_init, dtype=float)
        count_total = np.full(self.n_opponents, self.prior_alpha, dtype=float)
        T = int(rounds_considered)
        if T <= 0:
            return (count_coop, count_total)
        exponents = np.arange(T - 1, -1, -1)
        weights = (self.decay ** exponents).astype(float)
        actions_float = opponent_actions[:T, :].astype(float)
        weighted_coops = np.dot(weights, actions_float)
        weighted_total = np.sum(weights)
        count_coop += weighted_coops
        count_total += weighted_total
        return (count_coop, count_total)

    def _compute_pjs(self, opponent_actions: NDArray[np.bool_], rounds_considered: int):
        """
        Compute p_j estimates for each opponent using weighted counts and apply punishment multipliers.
        Returns array length n_opponents.
        """
        if self.n_opponents == 0:
            return np.array([])
        count_coop, count_total = self._compute_weighted_counts(opponent_actions, rounds_considered)
        pjs = count_coop / (count_total + 0.0)
        pjs = np.maximum(pjs, self.p_min)
        pjs = np.minimum(pjs, 1.0 - self.eps)
        pjs = pjs * self.punish_multiplier
        pjs = np.maximum(pjs, self.p_min)
        return pjs

    def _prob_exact_by_dp(self, pjs, target_s):
        """
        Dynamic programming convolution to compute probability exactly target_s successes
        among independent Bernoulli(p_j) for opponents.
        pjs: iterable of p_j, length = n_opponents
        target_s: integer (m-1)
        """
        n = len(pjs)
        if target_s < 0 or target_s > n:
            return 0.0
        prob = np.zeros(n + 1, dtype=float)
        prob[0] = 1.0
        for p in pjs:
            prev = prob.copy()
            prob[0] = prev[0] * (1.0 - p)
            for s in range(1, n + 1):
                prob[s] = prev[s] * (1.0 - p) + prev[s - 1] * p
        return float(prob[target_s])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_completed = int(history.round_number)
        if rounds_completed > self.last_history_rounds:
            for t in range(self.last_history_rounds, rounds_completed):
                self.punish_timers = np.maximum(self.punish_timers - 1, 0)
                expired = self.punish_timers == 0
                if expired.any():
                    self.punish_multiplier[expired] = 1.0
                if self.n_opponents > 0:
                    pjs_before = self._compute_pjs(history.opponent_actions, t)
                else:
                    pjs_before = np.array([])
                my_coop = bool(history.my_actions[t])
                if not my_coop:
                    continue
                target = self.game_description.m - 1
                P_exact_before = 0.0
                if self.n_opponents > 0:
                    P_exact_before = self._prob_exact_by_dp(pjs_before, target)
                if not self.game_description.k * P_exact_before > 1.0:
                    continue
                opp_coops_this_round = int(np.sum(history.opponent_actions[t, :])) if self.n_opponents > 0 else 0
                total_coops = opp_coops_this_round + 1
                if total_coops >= self.game_description.m:
                    continue
                if self.n_opponents > 0:
                    defected_mask = np.logical_not(history.opponent_actions[t, :])
                    expected_mask = pjs_before >= self.p_expected_threshold
                    betrayers = np.logical_and(defected_mask, expected_mask)
                    for j in np.nonzero(betrayers)[0]:
                        self.punish_timers[j] = self.punish_duration
                        self.punish_multiplier[j] = max(self.punish_factor, self.p_min)
            self.last_history_rounds = rounds_completed
        pjs = self._compute_pjs(history.opponent_actions, rounds_completed)
        if self.n_opponents > 0 and np.any(self.punish_timers > 0):
            return Action.D
        if int(state.round_number) >= int(self.game_description.n_rounds) - 1:
            return Action.D
        target = self.game_description.m - 1
        P_exact = 0.0
        if self.n_opponents > 0:
            P_exact = self._prob_exact_by_dp(pjs, target)
        if self.game_description.k * P_exact > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_74(BaseStrategy):
    """
    Pivotal Aggressor with Trigger Punishment (PATP)

    - Default: defect on first and last rounds.
    - Estimate opponents' cooperation probabilities with Laplace smoothing.
    - Compute probability P_exact that exactly m-1 other players cooperate.
    - Cooperate only if k * P_exact > 1 (i.e. P_exact > 1/k).
    - Maintain bounded, targeted punishment when players free-ride on successful rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.num_opponents = max(0, self.n - 1)
        self.coop_count = [0] * self.num_opponents
        self.blacklist = [False] * self.num_opponents
        self.forgive_counter = [0] * self.num_opponents
        self.T_punish_remaining = 0
        self.alpha = 1
        P_len_candidate = max(2, math.floor(self.r / 6)) if self.r >= 6 else max(1, min(2, self.r - 1))
        self.P_len = min(P_len_candidate, max(1, self.r - 1))
        self.L = 2
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = int(state.round_number)
        if history is None or history.round_number == 0:
            return Action.D
        total_completed = history.round_number
        for rr in range(self.last_processed_round, total_completed):
            my_a = bool(history.my_actions[rr])
            opp_row = history.opponent_actions[rr]
            for j in range(self.num_opponents):
                if bool(opp_row[j]):
                    self.coop_count[j] += 1
            num_opp_coops = int(sum((1 for j in range(self.num_opponents) if bool(opp_row[j]))))
            total_cooperators = (1 if my_a else 0) + num_opp_coops
            if total_cooperators >= self.m:
                for j in range(self.num_opponents):
                    if not bool(opp_row[j]):
                        self.blacklist[j] = True
                        if self.P_len > self.T_punish_remaining:
                            self.T_punish_remaining = self.P_len
            for j in range(self.num_opponents):
                if self.blacklist[j]:
                    if bool(opp_row[j]):
                        self.forgive_counter[j] += 1
                        if self.forgive_counter[j] >= self.L:
                            self.blacklist[j] = False
                            self.forgive_counter[j] = 0
                    else:
                        self.forgive_counter[j] = 0
            if self.T_punish_remaining > 0:
                self.T_punish_remaining -= 1
        self.last_processed_round = total_completed
        if current_round == self.r:
            return Action.D
        if current_round == 1:
            return Action.D
        prior_rounds = current_round - 1
        p_list = []
        if prior_rounds <= 0:
            p_list = [0.5] * self.num_opponents
        else:
            denom = prior_rounds + 2 * self.alpha
            for j in range(self.num_opponents):
                p_j = (self.coop_count[j] + self.alpha) / denom
                if p_j < 0.0:
                    p_j = 0.0
                elif p_j > 1.0:
                    p_j = 1.0
                p_list.append(p_j)
        if self.T_punish_remaining > 0:
            return Action.D
        target_k = self.m - 1
        P_exact = 0.0
        if target_k < 0 or target_k > self.num_opponents:
            P_exact = 0.0
        else:
            prob = [0.0] * (self.num_opponents + 1)
            prob[0] = 1.0
            for p in p_list:
                prev = prob[:]
                prob[0] = prev[0] * (1.0 - p)
                for x in range(1, self.num_opponents + 1):
                    prob[x] = prev[x] * (1.0 - p) + prev[x - 1] * p
            P_exact = prob[target_k]
        if self.k * P_exact > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_75(BaseStrategy):
    """
    Pivotal Aggressor (PA)

    Always defect by default; only cooperate when the estimated probability that
    you are pivotal (others will supply exactly m-1 cooperators) is high enough
    that the expected gain from cooperating exceeds the sure private gain from
    defecting. Uses Laplace/Beta(1,1) smoothing to estimate opponents' cooperation
    probabilities, computes the Poisson-binomial probability of being pivotal
    (exact DP for moderate n, normal approximation for large n), and includes a
    vanishing exploration probability to gather information.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        observed_rounds = 0 if history is None else history.round_number
        current_t = observed_rounds + 1
        opponents = max(0, self.n - 1)
        target = self.m - 1
        if history is None or observed_rounds == 0:
            C_counts = np.zeros(opponents, dtype=float)
        else:
            C_counts = np.sum(history.opponent_actions.astype(float), axis=0)
        denom = 2.0 + float(observed_rounds)
        if denom <= 0:
            p_j = np.full(opponents, 0.5)
        elif opponents > 0:
            p_j = (1.0 + C_counts) / denom
        else:
            p_j = np.array([])
        P_piv = 0.0
        if target < 0 or target > opponents:
            P_piv = 0.0
        elif opponents == 0:
            P_piv = 1.0 if target == 0 else 0.0
        elif opponents > 50:
            mu = float(np.sum(p_j))
            var = float(np.sum(p_j * (1.0 - p_j)))
            if var <= 0.0:
                P_piv = 1.0 if abs(mu - target) < 1e-12 else 0.0
            else:
                sigma = math.sqrt(var)

                def phi(x):
                    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
                upper = (target + 0.5 - mu) / sigma
                lower = (target - 0.5 - mu) / sigma
                P_piv = max(0.0, phi(upper) - phi(lower))
        else:
            probs = [0.0] * (opponents + 1)
            probs[0] = 1.0
            for p in p_j:
                for s in range(opponents - 1, -1, -1):
                    if probs[s] != 0.0:
                        contrib = probs[s] * p
                        probs[s + 1] += contrib
                        probs[s] -= contrib
            P_piv = float(probs[target])
        epsilon = min(0.1, 1.0 / float(current_t + 1))
        if random.random() < epsilon:
            return Action.C
        if self.k * P_piv > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_76(BaseStrategy):
    """
    Aggressive Pivotal Greedy (APG)

    - Defaults to defection.
    - Maintains a pooled short-window estimate q_hat of opponents' cooperation rate.
    - Computes expected immediate payoff of Cooperate vs Defect using binomial probabilities.
    - Cooperates only if EU_C > EU_D + epsilon (ties -> defect).
    - Detects "exploitable failures" in the most recent completed round and, if found,
      marks defecting opponents as culprits and punishes by unconditionally defecting
      for the next P_pun rounds. Punishment is bounded and then forgiven.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.Lmax = 10
        self.q0 = 0.2
        self.epsilon = 1e-09
        self.P_pun = 3
        self.culprits = set()
        self.pun_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        n_opp = n - 1

        def binprob(q: float, a: int, b: int) -> float:
            if a <= 0 and b >= n_opp:
                return 1.0
            if q <= 0.0:
                return 1.0 if a <= 0 <= b else 0.0
            if q >= 1.0:
                return 1.0 if a <= n_opp <= b else 0.0
            a_clamped = max(a, 0)
            b_clamped = min(b, n_opp)
            if a_clamped > b_clamped:
                return 0.0
            if n_opp <= 60:
                prob = 0.0
                for j in range(a_clamped, b_clamped + 1):
                    prob += math.comb(n_opp, j) * q ** j * (1 - q) ** (n_opp - j)
                return float(min(max(prob, 0.0), 1.0))
            else:
                mu = n_opp * q
                var = n_opp * q * (1 - q)
                if var <= 0:
                    val = int(round(mu))
                    return 1.0 if a_clamped <= val <= b_clamped else 0.0
                sigma = math.sqrt(var)

                def normal_cdf(x):
                    return 0.5 * (1.0 + math.erf((x - mu) / (sigma * math.sqrt(2.0))))
                lower = a_clamped - 0.5
                upper = b_clamped + 0.5
                prob = normal_cdf(upper) - normal_cdf(lower)
                return float(min(max(prob, 0.0), 1.0))
        if history is not None and history.round_number >= 1:
            last_my_action = bool(history.my_actions[-1])
            last_op_actions = history.opponent_actions[-1, :]
            total_coop = int(np.sum(last_op_actions)) + (1 if last_my_action else 0)
            others_coop = int(np.sum(last_op_actions))
            if total_coop < m and others_coop >= max(0, m - 1):
                culprits_now = {idx for idx, act in enumerate(last_op_actions) if not bool(act)}
                if len(culprits_now) > 0:
                    self.culprits = culprits_now
                    self.pun_remaining = self.P_pun
        if self.pun_remaining > 0:
            action = Action.D
            self.pun_remaining -= 1
            if self.pun_remaining == 0:
                self.culprits = set()
            return action
        if history is None or history.round_number == 0:
            t = 1
        else:
            t = history.round_number + 1
        if t == 1 or history is None or history.round_number == 0:
            q_hat = float(self.q0)
        else:
            L = min(t - 1, self.Lmax)
            recent_op = history.opponent_actions[-L:, :]
            total_C_by_others = int(np.sum(recent_op))
            denom = float(max(1, n_opp * L))
            q_hat = float(total_C_by_others) / denom
        if m <= 0:
            P_others_ge_m = 1.0
        else:
            P_others_ge_m = binprob(q_hat, m, n_opp)
        if m - 1 <= 0:
            P_others_ge_m1 = 1.0
        else:
            P_others_ge_m1 = binprob(q_hat, m - 1, n_opp)
        EU_D = 1.0 + k * float(P_others_ge_m)
        EU_C = k * float(P_others_ge_m1)
        if EU_C > EU_D + self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_77(BaseStrategy):
    """
    Pivotal Exploit (Aggressive)

    Aggressive, myopic strategy for the Collective Risk Dilemma.
    - Maintains per-opponent cooperation probabilities p_j (exponential smoothing).
    - Defects by default (low prior p0) and only cooperates when doing so
      meaningfully increases immediate expected payoff (pivotal).
    - Punishes defectors more harshly when they exploit you (faster decay).
    - Breaks ties in favour of defection via a small epsilon bias.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(self.game.n_players)
        self.r = int(self.game.n_rounds)
        self.m = int(self.game.m)
        self.k = float(self.game.k)
        self.p0 = 0.2
        self.alpha = 0.3
        self.eps = 0.001
        self.n_opponents = max(0, self.n - 1)
        self.p = np.full((self.n_opponents,), float(self.p0), dtype=float)
        self._last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return self._decide_using_current_beliefs()
        rounds_observed = history.round_number
        for ridx in range(self._last_processed_round, rounds_observed):
            my_action = bool(history.my_actions[ridx])
            opp_row = np.array(history.opponent_actions[ridx, :], dtype=bool)
            opp_coops = int(opp_row.sum())
            group_failed_despite_my_coop = False
            if my_action:
                total_cooperators = opp_coops + 1
                if total_cooperators < self.m:
                    group_failed_despite_my_coop = True
            for j in range(self.n_opponents):
                observed = 1.0 if opp_row[j] else 0.0
                if group_failed_despite_my_coop and observed == 0.0:
                    alpha_used = min(1.0, self.alpha * 2.0)
                else:
                    alpha_used = self.alpha
                self.p[j] = (1.0 - alpha_used) * self.p[j] + alpha_used * observed
        self._last_processed_round = rounds_observed
        return self._decide_using_current_beliefs()

    def _decide_using_current_beliefs(self) -> Action:
        """
        Compute PMF for number of cooperating opponents under independent Bernoulli model
        with parameters self.p, then compute U_C and U_D and choose according to rule.
        """
        if self.n_opponents == 0:
            u_c = self.k if self.m <= 1 else 0.0
            u_d = 1.0
            if u_c > u_d + self.eps:
                return Action.C
            else:
                return Action.D
        approx_threshold = 200
        if self.n_opponents > approx_threshold:
            p_bar = float(np.mean(self.p))
            pmf = [0.0] * (self.n_opponents + 1)
            pmf[0] = (1.0 - p_bar) ** self.n_opponents
            for s in range(1, self.n_opponents + 1):
                prev = pmf[s - 1]
                if prev == 0.0:
                    comb = math.comb(self.n_opponents, s)
                    pmf[s] = comb * p_bar ** s * (1 - p_bar) ** (self.n_opponents - s)
                else:
                    ratio = (self.n_opponents - (s - 1)) / s * (p_bar / (1.0 - p_bar)) if 1.0 - p_bar > 0 else 0.0
                    pmf[s] = prev * ratio
        else:
            pmf = [1.0] + [0.0] * self.n_opponents
            for p_j in self.p:
                next_pmf = [0.0] * (len(pmf) + 1)
                for s in range(len(pmf)):
                    next_pmf[s] += pmf[s] * (1.0 - p_j)
                    next_pmf[s + 1] += pmf[s] * p_j
                pmf = next_pmf
        P_ge_m = 0.0
        start = max(self.m, 0)
        if start <= self.n_opponents:
            P_ge_m = sum(pmf[start:])
        P_eq_mminus1 = 0.0
        idx = self.m - 1
        if 0 <= idx <= self.n_opponents:
            P_eq_mminus1 = pmf[idx]
        U_C = (P_ge_m + P_eq_mminus1) * self.k
        U_D = 1.0 + P_ge_m * self.k
        if U_C > U_D + self.eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_78(BaseStrategy):
    """
    Aggressive Pivotal Grim (APG)

    - Defect by default and start aggressively (defect in round 1).
    - Predict others' cooperations this round using the previous round's opponent cooperators.
    - Cooperate only when my single cooperation is predicted to be pivotal (m-1 opponents cooperated last round).
    - Free-ride (defect) when opponents can meet the threshold without me (last_cooperators_excluding_me >= m).
    - If ever exploited (I cooperated in a successful round while at least one opponent defected), switch to permanent defection (grim).
    - Optional conservative single probe: in round 2, if no opponents cooperated in round 1, play a single C once to detect latent cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.condemned = False
        self.optional_probe_enabled = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is not None and history.round_number > 0:
            last_my_action = bool(history.my_actions[-1])
            last_opponent_actions = history.opponent_actions[-1, :]
            total_cooperators = int(last_my_action) + int(np.sum(last_opponent_actions))
            if not self.condemned and last_my_action and (total_cooperators >= self.game_description.m):
                if np.any(last_opponent_actions == False):
                    self.condemned = True
        if self.condemned:
            return Action.D
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == 1:
            return Action.D
        if history is None or history.round_number == 0:
            last_cooperators_excluding_me = 0
            last_opponent_actions = np.array([], dtype=bool)
        else:
            last_opponent_actions = history.opponent_actions[-1, :]
            last_cooperators_excluding_me = int(np.sum(last_opponent_actions))
        m = self.game_description.m
        if self.optional_probe_enabled and t == 2 and (last_cooperators_excluding_me == 0):
            return Action.C
        if last_cooperators_excluding_me >= m:
            return Action.D
        elif last_cooperators_excluding_me == m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_79(BaseStrategy):
    """
    Aggressive pivot-based Collective Risk Dilemma strategy.

    - Default: defect, punish persistent free-riders, probe rarely.
    - Cooperate only when estimated probability that others produce exactly (m-1)
      cooperators (p_piv) exceeds 1/k (strict), except rare probes.
    - Maintains per-opponent empirical cooperation rates (sliding window),
      free-ride histories, and time-limited blacklists. Uses Poisson-Binomial DP
      to compute exact pivotal probabilities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_base = 20
        self.eps_probe = 0.03
        self.L_blacklist = 5
        self.L_forgive = 3
        self.T_free = 0.5
        self.p0 = 0.1
        self.n = int(game_description.n_players)
        self.n_opponents = max(0, self.n - 1)
        self.free_ride_histories = [list() for _ in range(self.n_opponents)]
        self.blacklisted_until = [0 for _ in range(self.n_opponents)]
        self._processed_rounds = 0

    def _poisson_binomial_pmf(self, probs):
        """
        Compute Poisson-Binomial PMF for sum of independent Bernoullis with success probs.
        Returns a list P where P[x] = P(sum == x).
        Simple DP O(n^2) with Python lists / numpy.
        """
        n = len(probs)
        if n == 0:
            return [1.0]
        dp = [1.0]
        for p in probs:
            new = [0.0] * (len(dp) + 1)
            new[0] = dp[0] * (1.0 - p)
            for j in range(1, len(new)):
                old_j = dp[j] if j < len(dp) else 0.0
                old_jm1 = dp[j - 1] if j - 1 < len(dp) else 0.0
                new[j] = old_j * (1.0 - p) + old_jm1 * p
            dp = new
        return dp

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        r_total = int(self.game_description.n_rounds)
        if history is None:
            if random.random() < self.eps_probe:
                return Action.C
            return Action.D
        rounds_completed = history.round_number
        try:
            rounds_array = history.opponent_actions.shape[0]
        except Exception:
            rounds_array = rounds_completed
        rounds_completed = min(rounds_completed, rounds_array)
        for r_idx in range(self._processed_rounds, rounds_completed):
            my_c = bool(history.my_actions[r_idx])
            opps_row = history.opponent_actions[r_idx, :] if self.n_opponents > 0 else np.array([], dtype=bool)
            total_cooperators = int(my_c) + int(np.sum(opps_row))
            for j in range(self.n_opponents):
                opp_coop = bool(opps_row[j])
                free_ride = not opp_coop and total_cooperators >= m
                self.free_ride_histories[j].append(1 if free_ride else 0)
                if len(self.free_ride_histories[j]) > max(200, 5 * self.W_base):
                    self.free_ride_histories[j] = self.free_ride_histories[j][-max(200, 5 * self.W_base):]
            current_round_num = r_idx + 1
            W_for_check = min(self.W_base, current_round_num)
            min_free_needed = max(2, math.ceil(0.3 * max(1, W_for_check)))
            for j in range(self.n_opponents):
                recent = self.free_ride_histories[j][-W_for_check:] if W_for_check > 0 else []
                count_free = sum(recent)
                if count_free >= min_free_needed:
                    new_until = current_round_num + self.L_blacklist + 1
                    if new_until > self.blacklisted_until[j]:
                        self.blacklisted_until[j] = new_until
                if len(history.opponent_actions) >= self.L_forgive:
                    start_idx = max(0, r_idx - self.L_forgive + 1)
                    recent_actions = history.opponent_actions[start_idx:r_idx + 1, j]
                    if len(recent_actions) >= self.L_forgive and bool(np.all(recent_actions)):
                        self.blacklisted_until[j] = 0
                        self.free_ride_histories[j] = []
        self._processed_rounds = rounds_completed
        W = min(self.W_base, max(0, t - 1))
        p_list = []
        if W == 0:
            p_list = [self.p0 for _ in range(self.n_opponents)]
        else:
            avail = history.opponent_actions
            rounds_avail = avail.shape[0]
            take = min(W, rounds_avail)
            if take == 0:
                p_list = [self.p0 for _ in range(self.n_opponents)]
            else:
                recent = avail[-take:, :]
                frac = np.mean(recent.astype(float), axis=0)
                for j in range(self.n_opponents):
                    pj = float(frac[j]) if take > 0 else self.p0
                    if self.blacklisted_until[j] > t:
                        pj = 0.0
                    p_list.append(pj)
        if len(p_list) != self.n_opponents:
            p_list = [self.p0 for _ in range(self.n_opponents)]
        pmf = self._poisson_binomial_pmf(p_list)
        idx_piv = m - 1
        p_piv = pmf[idx_piv] if 0 <= idx_piv < len(pmf) else 0.0
        if m <= self.n_opponents:
            P_atleast_m = float(sum(pmf[m:]))
        else:
            P_atleast_m = 0.0
        if t >= r_total:
            margin = 0.05
            if p_piv > 1.0 / k + margin:
                return Action.C
            return Action.D
        if random.random() < self.eps_probe:
            return Action.C
        if p_piv > 1.0 / k:
            return Action.C
        if P_atleast_m > 0.5:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_80(BaseStrategy):
    """
    Aggressive self-interested pivotal strategy for the Collective Risk Dilemma.

    Summary:
    - Estimate per-opponent cooperation probabilities using a Beta(1,1) smoothed frequency estimate.
    - Compute the Poisson–Binomial distribution for the number of other cooperators.
    - Let P_piv = Pr(others cooperate = m-1). Cooperate iff P_piv > 1/k (tie -> defect).
    - First round (no history) -> defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._alpha = 1.0
        self._beta = 1.0
        self._tol = 1e-12

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        n_other = n - 1
        target_idx = m - 1
        if target_idx < 0 or target_idx > n_other:
            return Action.D
        T = history.round_number
        opp_actions = history.opponent_actions
        if T == 0:
            S = np.zeros(n_other, dtype=float)
        else:
            S = np.sum(opp_actions.astype(float), axis=0)
        denom = float(T) + self._alpha + self._beta
        p_js = [(float(S[j]) + self._alpha) / denom for j in range(n_other)]
        dp = np.zeros(n_other + 1, dtype=float)
        dp[0] = 1.0
        for p in p_js:
            for x in range(n_other, 0, -1):
                dp[x] = dp[x] * (1.0 - p) + dp[x - 1] * p
            dp[0] = dp[0] * (1.0 - p)
        P_piv = float(dp[target_idx])
        threshold = 1.0 / k if k != 0.0 else float('inf')
        if P_piv - threshold > self._tol:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_81(BaseStrategy):
    """
    Aggressive exploit-first Collective Risk strategy.

    - Defaults to defect.
    - Estimates per-opponent cooperation rates via Laplace smoothing.
    - Computes distribution over number of cooperating opponents by convolution.
    - Cooperates only when expected payoff of cooperating exceeds defecting by theta.
    - Enters a short spite (punishment) window S after being sucker-played (you cooperated but threshold failed).
    - Uses a larger margin in the last round to bias further toward defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.1
        r = max(1, int(self.game_description.n_rounds))
        self.S_default = min(5, max(1, math.floor(r / 4)))
        self._theta = self.theta
        self._S = self.S_default

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = history.round_number
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if t == 0:
            return Action.D
        S = int(self._S)
        last_sucker_idx = -1
        for round_idx in range(t):
            my_coop = bool(history.my_actions[round_idx])
            opp_coops = int(np.sum(history.opponent_actions[round_idx, :])) if history.opponent_actions.size else 0
            total_coops = opp_coops + (1 if my_coop else 0)
            if my_coop and total_coops < m:
                last_sucker_idx = round_idx
        if last_sucker_idx >= 0:
            spite_timer = max(0, S - (t - last_sucker_idx))
        else:
            spite_timer = 0
        if t > 0:
            if history.opponent_actions.size == 0:
                coop_counts = np.zeros(n - 1, dtype=float)
            else:
                coop_counts = np.sum(history.opponent_actions[:t, :], axis=0).astype(float)
            p_js = (1.0 + coop_counts) / (2.0 + float(t))
        else:
            p_js = np.full((n - 1,), 0.5, dtype=float)
        q = np.zeros(n, dtype=float)
        q[0] = 1.0
        for p in p_js:
            new_q = np.zeros_like(q)
            for s in range(0, n):
                mass = q[s]
                if mass == 0.0:
                    continue
                new_q[s] += mass * (1.0 - p)
                if s + 1 < n:
                    new_q[s + 1] += mass * p
            q = new_q
        lower_with_C = max(0, m - 1)
        if lower_with_C <= n - 1:
            P_with_C = float(np.sum(q[lower_with_C:n]))
        else:
            P_with_C = 0.0
        lower_without_C = max(0, m)
        if lower_without_C <= n - 1:
            P_without_C = float(np.sum(q[lower_without_C:n]))
        else:
            P_without_C = 0.0
        E_coop = P_with_C * k
        E_def = 1.0 + P_without_C * k
        last_round = t == r - 1
        theta = float(self._theta)
        if last_round:
            theta_effective = theta * 5.0
        else:
            theta_effective = theta
        if spite_timer > 0:
            if E_coop >= E_def + max(10.0 * theta, 10.0 * theta_effective):
                return Action.C
            else:
                return Action.D
        if E_coop >= E_def + theta_effective:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_82(BaseStrategy):
    """
    Pivotal Exploiter with Retaliatory Grim (PERG).

    - Default: defect.
    - Only cooperate when your single contribution is likely pivotal (others ~ m-1)
      AND recent history shows high confidence that others will supply m-1 cooperators.
    - If you cooperate and the threshold still fails (you were betrayed), enter a finite
      punishment mode (punish_timer) where you defect for several rounds to deny the
      group the collective reward.
    - Always defect in the final `final_rounds_no_coop` rounds (end-game defection).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r - 1))
        self.P_param = max(1, int(round(0.25 * self.r)))
        self.trust_threshold = 0.8
        self.final_rounds_no_coop = 1
        self.punish_timer = 0
        self._last_processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        completed_rounds = 0
        if history is not None:
            completed_rounds = int(history.round_number)
        rounds_advanced = max(0, completed_rounds - self._last_processed_rounds)
        if rounds_advanced > 0:
            self.punish_timer = max(0, int(self.punish_timer) - rounds_advanced)
            self._last_processed_rounds = completed_rounds
        if history is not None and completed_rounds > 0:
            try:
                my_last_action = bool(history.my_actions[-1])
                opp_last_actions = history.opponent_actions[-1, :]
            except Exception:
                my_last_action = False
                opp_last_actions = np.zeros((max(0, self.n - 1),), dtype=bool)
            total_cooperators_last = int(np.sum(opp_last_actions) + (1 if my_last_action else 0))
            if my_last_action and total_cooperators_last < self.m:
                remaining_rounds = max(0, self.r - completed_rounds)
                self.punish_timer = min(self.P_param, remaining_rounds)
        t = int(state.round_number) if state is not None else completed_rounds + 1
        if t > self.r - self.final_rounds_no_coop:
            return Action.D
        if self.punish_timer > 0:
            return Action.D
        if completed_rounds == 0:
            return Action.D
        t_hist = min(self.W, completed_rounds)
        if t_hist <= 0:
            estimated_other_cooperators = 0.0
            conf = 0.0
        else:
            recent_opp = history.opponent_actions[-t_hist:, :]
            recent_counts = np.sum(recent_opp.astype(np.int64), axis=1)
            estimated_other_cooperators = float(np.mean(recent_counts))
            pivotal_threshold = max(0, self.m - 1)
            conf = float(np.mean((recent_counts >= pivotal_threshold).astype(np.float64))) if t_hist > 0 else 0.0
        if estimated_other_cooperators >= self.m:
            return Action.D
        elif estimated_other_cooperators <= self.m - 2:
            return Action.D
        elif conf >= self.trust_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_83(BaseStrategy):
    """
    Aggressive, self-interested Collective Risk strategy.
    Defaults to defect, estimates opponent cooperation rates, defects unless cooperating
    clearly raises immediate expected payoff (with small probing and escalation).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.gamma = 0.01
        self.epsilon0 = 0.05
        self.W = max(5, math.floor(self.r / 6))
        self.escalated = False

    def _poisson_binomial_exact_tail(self, ps, threshold):
        L = len(ps)
        if threshold <= 0:
            return 1.0
        if L == 0:
            return 1.0 if threshold <= 0 else 0.0
        prob = [0.0] * (L + 1)
        prob[0] = 1.0
        for p in ps:
            for s in range(L, 0, -1):
                prob[s] = prob[s] * (1.0 - p) + prob[s - 1] * p
            prob[0] = prob[0] * (1.0 - p)
        tail = 0.0
        th = int(threshold)
        if th <= 0:
            tail = 1.0
        else:
            for s in range(th, L + 1):
                tail += prob[s]
        if tail < 0.0:
            tail = 0.0
        if tail > 1.0:
            tail = 1.0
        return tail

    def _poisson_binomial_approx_tail(self, ps, threshold):
        mu = sum(ps)
        sigma2 = sum((p * (1.0 - p) for p in ps))
        if sigma2 <= 1e-08:
            s_det = int(round(mu))
            return 1.0 if s_det >= threshold else 0.0
        z = (threshold - 0.5 - mu) / math.sqrt(sigma2)
        Phi = 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))
        tail = 1.0 - Phi
        if tail < 0.0:
            tail = 0.0
        if tail > 1.0:
            tail = 1.0
        return tail

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number) if state is not None else 1
        num_opponents = self.n - 1
        if history is None:
            if random.random() < self.epsilon0:
                return Action.C
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed <= 0:
            if random.random() < self.epsilon0:
                return Action.C
            return Action.D
        opp_actions = history.opponent_actions
        counts = np.sum(opp_actions, axis=0) if opp_actions.size > 0 else np.zeros((num_opponents,), dtype=float)
        counts_list = [float(x) for x in np.asarray(counts).flatten().tolist()]
        denom = float(rounds_completed)
        p_list = []
        if denom <= 0.0:
            p_list = [0.5] * num_opponents
        else:
            for c in counts_list:
                p_list.append(c / denom)
        use_exact = num_opponents <= 50
        m_val = self.m
        threshold_without = m_val
        threshold_with = m_val - 1
        if use_exact:
            P_without = self._poisson_binomial_exact_tail(p_list, threshold_without)
            P_with = self._poisson_binomial_exact_tail(p_list, threshold_with)
        else:
            P_without = self._poisson_binomial_approx_tail(p_list, threshold_without)
            P_with = self._poisson_binomial_approx_tail(p_list, threshold_with)
        E_D = 1.0 + self.k * P_without
        E_C = 0.0 + self.k * P_with
        if self.r <= 1:
            eps_t = max(self.epsilon0, 0.01)
        else:
            frac = (t - 1) / max(1, self.r - 1)
            eps_t = max(self.epsilon0 * (1.0 - frac), 0.01)
        if not self.escalated and rounds_completed >= self.W:
            cumulative_actual = float(np.sum(history.my_payoffs)) if history.my_payoffs.size > 0 else 0.0
            opps = history.opponent_actions
            always_defect_total = 0.0
            for r_idx in range(opps.shape[0]):
                opps_this = opps[r_idx, :]
                coop_count_others = int(np.sum(opps_this))
                payoff_if_defect = 1.0 + self.k * (1 if coop_count_others >= self.m else 0)
                always_defect_total += payoff_if_defect
            if cumulative_actual + 1e-09 < always_defect_total:
                self.gamma = 1000000.0
                self.escalated = True
        if random.random() < eps_t:
            return Action.C
        if E_C > E_D + self.gamma:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_84(BaseStrategy):
    """
    OPPORTUNISTIC PIVOT + TARGETED EXPLOIT (OPTE)

    Aggressive history-driven strategy for the Collective Risk Dilemma:
    - Default to defect.
    - Cooperate only when your single contribution is likely pivotal (P(X = m-1) > 1/k + delta).
    - Free-ride when others likely meet the threshold without you.
    - If repeatedly "suckered" (you cooperated but threshold failed), enter a short punishment block of defections.
    - Add a small exploration probability to flip actions occasionally.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = min(10, max(1, self.game_description.n_rounds))
        self.alpha = 1.0
        self.exploit_prob_threshold = 0.9
        self.delta = 0.02
        self.P_cooperate_threshold_offset = self.delta
        self.L = 2
        self.T_ret = max(1, min(5, int(math.ceil(3 * (self.game_description.n_rounds / max(1, 20))))))
        self.eps_explore = 0.01
        self.sucker_counter = 0
        self.punishment_timer = 0
        self.last_history_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = float(self.game_description.k)
        if n < 2:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        newly_observed = history.round_number - self.last_history_round
        if newly_observed > 0:
            for t in range(self.last_history_round, history.round_number):
                if self.punishment_timer > 0:
                    self.punishment_timer = max(0, self.punishment_timer - 1)
                else:
                    my_action = bool(history.my_actions[t])
                    opps = history.opponent_actions[t, :]
                    total_cooperators = int(my_action) + int(opps.sum())
                    if my_action and total_cooperators < m:
                        self.sucker_counter += 1
                    elif self.sucker_counter > 0:
                        self.sucker_counter -= 1
                if self.sucker_counter >= self.L and self.punishment_timer == 0:
                    self.punishment_timer = int(self.T_ret)
                    self.sucker_counter = 0
            self.last_history_round = history.round_number
        if self.punishment_timer > 0:
            chosen = Action.D
        else:
            observed = min(self.W, history.round_number)
            n_opponents = n - 1
            if n_opponents < max(0, m - 1):
                chosen = Action.D
            else:
                p_list = []
                if observed <= 0:
                    for _ in range(n_opponents):
                        p_j = (0 + self.alpha) / (0 + 2.0 * self.alpha)
                        p_list.append(float(p_j))
                else:
                    start_idx = max(0, history.round_number - observed)
                    recent_op_actions = history.opponent_actions[start_idx:history.round_number, :]
                    for j in range(n_opponents):
                        coop_j = int(recent_op_actions[:, j].sum())
                        p_j = (coop_j + self.alpha) / (observed + 2.0 * self.alpha)
                        p_list.append(float(p_j))
                prob = [1.0] + [0.0] * n_opponents
                for p in p_list:
                    new_len = len(prob) + 1
                    new = [0.0] * new_len
                    for k_idx in range(len(prob)):
                        no = prob[k_idx] * (1.0 - p)
                        yes = prob[k_idx] * p
                        new[k_idx] += no
                        new[k_idx + 1] += yes
                    prob = new
                prob = prob[:n_opponents + 1]
                idx_m1 = m - 1
                P_m1 = prob[idx_m1] if 0 <= idx_m1 <= n_opponents else 0.0
                P_atleastm_without_you = 0.0
                if m <= n_opponents:
                    P_atleastm_without_you = float(sum(prob[m:]))
                else:
                    P_atleastm_without_you = 0.0
                threshold = 1.0 / max(1.0, k) + self.P_cooperate_threshold_offset
                if P_m1 > threshold:
                    chosen = Action.C
                elif P_atleastm_without_you >= self.exploit_prob_threshold:
                    chosen = Action.D
                else:
                    chosen = Action.D
        if random.random() < self.eps_explore:
            chosen = Action.C if chosen == Action.D else Action.D
        return chosen

class Strategy_AGGRESSIVE_85(BaseStrategy):
    """
    Pivotal Opportunist (aggressive).

    Defects by default and only cooperates when the probability that exactly m-1
    opponents cooperate (so that your single cooperation is pivotal) multiplied
    by the public reward k exceeds the private endowment forgone (1) plus a small
    aggression margin delta. Grim-trigger punishment: if you ever cooperated and
    the threshold failed that round, set exploited and defect forever. Free-rides
    when others will almost certainly meet the threshold without you (FR).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.3
        self.delta = 0.0
        self.FR = 0.9
        self.p0 = 0.2
        self.grim = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k_reward = self.game_description.k
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == r:
            return Action.D
        n_opponents = max(0, n - 1)
        if history is None or history.round_number == 0:
            p_j = [self.p0] * n_opponents
            exploited = False
        else:
            exploited = False
            p_j = [self.p0] * n_opponents
            rounds_seen = history.round_number
            for round_idx in range(rounds_seen):
                my_coop = bool(history.my_actions[round_idx])
                opp_coops = int(np.sum(history.opponent_actions[round_idx, :]))
                total_coops = opp_coops + (1 if my_coop else 0)
                if my_coop and total_coops < m:
                    exploited = True
                for j in range(n_opponents):
                    obs = 1.0 if bool(history.opponent_actions[round_idx, j]) else 0.0
                    p_j[j] = (1.0 - self.alpha) * p_j[j] + self.alpha * obs
        if exploited and self.grim:
            return Action.D
        if n_opponents == 0:
            return Action.D
        P = [0.0] * (n_opponents + 1)
        P[0] = 1.0
        for pj in p_j:
            pj = float(pj)
            for k_idx in range(len(P) - 1, -1, -1):
                prob_k_if_def = P[k_idx] * (1.0 - pj)
                prob_k_if_coop = P[k_idx - 1] * pj if k_idx - 1 >= 0 else 0.0
                P[k_idx] = prob_k_if_def + prob_k_if_coop
        m_minus_1 = m - 1
        if 0 <= m_minus_1 <= n_opponents:
            Pr_exactly_m_minus1 = P[m_minus_1]
        else:
            Pr_exactly_m_minus1 = 0.0
        if m <= n_opponents:
            Pr_others_ge_m = sum(P[m:])
        else:
            Pr_others_ge_m = 0.0
        if Pr_others_ge_m >= self.FR:
            return Action.D
        if k_reward * Pr_exactly_m_minus1 > 1.0 + self.delta:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_86(BaseStrategy):
    """
    Aggressive Pivotal Exploiter (APE)

    Summary:
    - Default to defect on the first round.
    - Estimate each opponent's cooperation probability with Laplace smoothing from history.
    - Compute exact Poisson-binomial distribution (DP convolution) for number of OTHER cooperators.
    - Play C only if the immediate expected payoff of cooperating (k * Pr(others >= m-1))
      strictly exceeds defecting (1 + k * Pr(others >= m)). Tie -> D.
    - Maintain an anti-exploitation heuristic: if an opponent defects in a large fraction
      of rounds where you cooperated (and you suffered from those cooperations), mark them
      as exploiters and set their p_j = 0 forever in future estimates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.R_exploit_frac = 0.8
        self.min_coop_obs = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_played = history.round_number
        if rounds_played == 0:
            return Action.D
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        n_opponents = opp_actions.shape[1]
        coop_counts = np.sum(opp_actions.astype(np.int64), axis=0)
        denom = 2 + rounds_played
        p_js = (1.0 + coop_counts.astype(np.float64)) / float(denom)
        exploiters_mask = np.zeros(n_opponents, dtype=bool)
        you_coop_mask = my_actions.astype(bool)
        times_you_cooperated = int(np.sum(you_coop_mask))
        if times_you_cooperated >= self.min_coop_obs:
            defections_when_you_coop = np.sum(~opp_actions & you_coop_mask[:, None], axis=0).astype(np.int64)
            total_cooperators_each_round = np.sum(opp_actions.astype(np.int64), axis=1) + my_actions.astype(np.int64)
            if times_you_cooperated > 0:
                failed_when_you_coop = np.sum((total_cooperators_each_round[you_coop_mask] < m).astype(np.int64))
                frac_failed = float(failed_when_you_coop) / float(times_you_cooperated)
            else:
                frac_failed = 0.0
            if frac_failed >= 0.5:
                exploiter_indices = np.where(defections_when_you_coop / float(times_you_cooperated) >= self.R_exploit_frac)[0]
                exploiters_mask[exploiter_indices] = True
        p_js[exploiters_mask] = 0.0
        if n_opponents == 0:
            return Action.D
        dp = np.zeros(n_opponents + 1, dtype=np.float64)
        dp[0] = 1.0
        for p in p_js:
            if p <= 0.0:
                dp = dp * 1.0
                continue
            if p >= 1.0:
                new_dp = np.zeros_like(dp)
                new_dp[1:] = dp[:-1]
                dp = new_dp
                continue
            new_dp = dp * (1.0 - p)
            new_dp[1:] += dp[:-1] * p
            dp = new_dp
        upper = n_opponents
        P_m = 0.0
        P_m_1 = 0.0
        if m <= upper:
            P_m = float(np.sum(dp[m:]))
        else:
            P_m = 0.0
        if m - 1 <= upper:
            lower_idx = max(0, m - 1)
            P_m_1 = float(np.sum(dp[lower_idx:]))
        else:
            P_m_1 = 0.0
        U_defect = 1.0 + k * P_m
        U_cooperate = k * P_m_1
        if U_cooperate > U_defect:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_87(BaseStrategy):
    """
    Aggressive pivotal free-rider for the Collective Risk Dilemma.

    - Never cooperates in the final round.
    - Starts by defecting (uses a low prior on round 1).
    - Estimates each opponent's probability of cooperating by Laplace smoothing
      from past actions: p_j = (c_j + 1) / (T + 2) (unless first round prior).
    - Computes the exact distribution of the number of other cooperators (DP).
    - Cooperates in non-final rounds iff k * Prob_exact(m-1 others) > 1.
      Ties broken in favor of defection.
    - Optional tiny maintenance epsilon to rarely cooperate when the group
      is almost certain to reach the threshold without me but appears
      stably cooperative (to preserve future free-riding opportunities).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prior_p = 0.1
        self.maintenance_epsilon = 0.02
        self.maintenance_threshold = 0.95
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            T = 0
        else:
            T = int(history.round_number)
        t = T + 1
        if t >= self.r:
            return Action.D
        n = self.n
        m = self.m
        k = self.k
        n_opponents = n - 1
        if n_opponents <= 0:
            return Action.D
        if history is None or T == 0:
            p_list = [self.prior_p] * n_opponents
        else:
            opp_actions = history.opponent_actions
            counts = np.sum(opp_actions.astype(int), axis=0)
            denom = float(T + 2)
            p_list = [float((int(counts[j]) + 1) / denom) for j in range(n_opponents)]
        prob = np.array([1.0], dtype=float)
        for p in p_list:
            newprob = np.zeros(len(prob) + 1, dtype=float)
            newprob[:-1] += prob * (1.0 - p)
            newprob[1:] += prob * p
            prob = newprob
        idx = m - 1
        Prob_exact = float(prob[idx]) if 0 <= idx < len(prob) else 0.0
        if m <= len(prob) - 1:
            Prob_no_me = float(np.sum(prob[m:]))
        else:
            Prob_no_me = 0.0
        if k * Prob_exact > 1.0:
            return Action.C
        if Prob_no_me >= self.maintenance_threshold and self.maintenance_epsilon > 0.0:
            try:
                high_count = sum((1 for p in p_list if p >= 0.9))
                fraction_high = high_count / float(n_opponents)
            except Exception:
                fraction_high = 0.0
            if T >= 3 and fraction_high >= 0.6:
                if random.random() < self.maintenance_epsilon:
                    return Action.C
        return Action.D

class Strategy_AGGRESSIVE_88(BaseStrategy):
    """
    Aggressive Pivotal-Only (APO) strategy.

    - Default: defect on first round.
    - Estimate opponents' cooperation probabilities with Laplace smoothing (alpha0).
    - Compute p_exact = Prob[#other cooperators == m-1] assuming independent Bernoulli opponents.
    - Cooperate only if k * p_exact > 1 + delta (aggressive margin). Tie -> defect.
    - If ever cooperated in a round that still failed (total cooperators < m), enter permanent
      punishment_mode (grim): defect forever.
    - Uses full history by default; optional sliding window can be set via self.window.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.alpha0 = 1.0
        self.delta = 0.1
        self.window = None
        self.punishment_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if not self.punishment_mode:
            my_actions = np.asarray(history.my_actions).astype(bool)
            opp_actions = np.asarray(history.opponent_actions).astype(bool)
            if my_actions.size > 0:
                opp_coop_counts = opp_actions.sum(axis=1)
                total_cooperators = opp_coop_counts + my_actions.astype(int)
                suckered_mask = my_actions & (total_cooperators < self.m)
                if np.any(suckered_mask):
                    self.punishment_mode = True
        if self.punishment_mode:
            return Action.D
        rounds_seen = history.round_number
        if rounds_seen <= 0:
            return Action.D
        opp_actions = np.asarray(history.opponent_actions).astype(bool)
        if self.window is not None and self.window > 0:
            W = int(self.window)
            if rounds_seen > W:
                recent = opp_actions[-W:, :]
                rounds_considered = W
            else:
                recent = opp_actions
                rounds_considered = rounds_seen
            coop_counts = recent.sum(axis=0).astype(float)
        else:
            coop_counts = opp_actions.sum(axis=0).astype(float)
            rounds_considered = rounds_seen
        denom = float(rounds_considered) + 2.0 * float(self.alpha0)
        if denom <= 0.0:
            p_js = np.full(self.n - 1, 0.0)
        else:
            p_js = (coop_counts + float(self.alpha0)) / denom
            p_js = np.minimum(np.maximum(p_js, 0.0), 1.0)
        dp = np.array([1.0], dtype=float)
        for p in p_js:
            new_dp = np.zeros(len(dp) + 1, dtype=float)
            new_dp[:-1] += dp * (1.0 - float(p))
            new_dp[1:] += dp * float(p)
            dp = new_dp
        target = self.m - 1
        if target < 0 or target >= len(dp):
            p_exact = 0.0
        else:
            p_exact = float(dp[target])
        threshold = 1.0 + float(self.delta)
        lhs = float(self.k) * p_exact
        if lhs > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_89(BaseStrategy):
    """
    Aggressive Threshold Enforcer (ATE)

    - Starts by cooperating round 1 to signal willingness.
    - Tracks per-opponent cooperation counts (recent-window) to estimate
      cooperation probabilities.
    - Cooperates when likely to be pivotal (Prob_others_at_least(m-1) >= 0.5).
    - Defects when threshold is likely without you (Prob_others_at_least(m) >= 0.5)
      or by default when not pivotal.
    - If in any completed round the total cooperators == m-1, any opponent who
      defected in that round is marked as an offender. Offenders trigger a grim
      punishment: the strategy defects for the remainder of the tournament
      (except it still may cooperate in the very last round if doing so is
      pivotally profitable).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = max(0, game_description.n_players - 1)
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.offenders = set()
        self.punished = False
        self.W = max(3, math.floor(self.r / 4))
        self.coop_counts = np.zeros(self.n, dtype=int)
        self.processed_rounds = 0

    def _update_from_history(self, history: PlayerHistory):
        """
        Process any newly completed rounds in history to update coop_counts,
        offender detection, and punished flag. Ensures idempotence by tracking
        processed_rounds.
        """
        rounds_observed = history.round_number
        if rounds_observed <= self.processed_rounds:
            return
        for r_idx in range(self.processed_rounds, rounds_observed):
            if self.n > 0:
                opp_row = history.opponent_actions[r_idx, :]
            else:
                opp_row = np.zeros(0, dtype=bool)
            my_act = bool(history.my_actions[r_idx])
            if self.n > 0:
                self.coop_counts += opp_row.astype(int)
            total_coops = int(np.sum(opp_row.astype(int)) + (1 if my_act else 0))
            if total_coops == self.m - 1:
                for j in range(self.n):
                    if not bool(opp_row[j]):
                        self.offenders.add(j)
                if len(self.offenders) > 0:
                    self.punished = True
        self.processed_rounds = rounds_observed

    def _estimate_pjs(self, history: PlayerHistory):
        """
        Estimate p_j for each opponent using the last min(W, rounds_observed) rounds.
        Uses recency-weighted average (linear weights) to emphasize recent behaviour.
        Returns list of length n with floats in [0,1]. If there are no opponents,
        returns empty list.
        """
        rounds_observed = history.round_number
        if self.n == 0:
            return []
        window_len = min(self.W, rounds_observed)
        if window_len <= 0:
            return [0.5] * self.n
        window = history.opponent_actions[-window_len:, :]
        weights = np.arange(1, window_len + 1, dtype=float)
        weights = weights / np.sum(weights)
        window_float = window.astype(float)
        weighted = np.dot(weights, window_float)
        pjs = np.clip(weighted, 0.0, 1.0)
        return pjs.tolist()

    def _prob_others_at_least(self, pjs, q):
        """
        Given list pjs length n (probabilities each opponent cooperates),
        compute probability that number of cooperating opponents >= q.
        Uses dynamic programming (convolution) to get exact distribution.
        """
        if q <= 0:
            return 1.0
        if len(pjs) == 0:
            return 1.0 if 0 >= q else 0.0
        dist = [1.0]
        for p in pjs:
            new_len = len(dist) + 1
            new_dist = [0.0] * new_len
            for k in range(new_len):
                if k < len(dist):
                    new_dist[k] += dist[k] * (1.0 - p)
                if k - 1 >= 0:
                    new_dist[k] += dist[k - 1] * p
            dist = new_dist
        if q >= len(dist):
            return 0.0
        prob = sum(dist[q:])
        return max(0.0, min(1.0, prob))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        self._update_from_history(history)
        last_round = t == self.r
        if self.punished and (not last_round):
            return Action.D
        pjs = self._estimate_pjs(history)
        P_pivotal = self._prob_others_at_least(pjs, max(0, self.m - 1))
        P_threshold_without_me = self._prob_others_at_least(pjs, self.m)
        if last_round:
            if P_pivotal >= 0.5:
                return Action.C
            else:
                return Action.D
        if P_threshold_without_me >= 0.5:
            return Action.D
        if P_pivotal >= 0.5:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_90(BaseStrategy):
    """
    Aggressive opportunistic adaptive strategy for the Collective Risk Dilemma.

    - Defaults to defecting unless cooperating is selfishly justified by the
      estimated probability of being pivotal (exactly m-1 other cooperators).
    - Maintains a recency-weighted, Laplace-smoothed estimate of P(otherC == m-1).
    - Uses a small, decaying exploration probability to gather data early.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = float(game_description.k)
        self.gamma = 0.95
        self.alpha = 0.5
        self.alpha0 = 1.0
        self.eps0 = 0.06
        self.eps_decay = 0.95
        self.weighted_count_m1 = 0.0
        self.total_weight = 0.0
        self.last_processed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        try:
            next_round = int(state.round_number)
        except Exception:
            next_round = history.round_number + 1 if history is not None else 1
        if history is not None:
            completed = int(history.round_number)
            for idx in range(self.last_processed, completed):
                opp_row = history.opponent_actions[idx]
                otherC = int(np.sum(opp_row))
                indicator = 1.0 if otherC == self.m - 1 else 0.0
                self.weighted_count_m1 = self.gamma * self.weighted_count_m1 + indicator
                self.total_weight = self.gamma * self.total_weight + 1.0
                self.last_processed += 1
        if next_round >= self.r:
            epsilon = 0.0
        else:
            epsilon = float(self.eps0 * self.eps_decay ** max(0, next_round - 1))
        if self.total_weight <= 0.0:
            B_hat = float(self.alpha / self.alpha0)
        else:
            B_hat = float((self.alpha + self.weighted_count_m1) / (self.alpha0 + self.total_weight))
        if random.random() < epsilon:
            return Action.C
        threshold = 1.0 / max(1e-12, self.k)
        if B_hat > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_91(BaseStrategy):
    """
    Aggressive adaptive strategy for the Collective Risk Dilemma.

    - Default: defect.
    - Use empirical frequencies (Jeffreys prior) to estimate each opponent's cooperation prob.
    - Compute Poisson-Binomial distribution over other cooperators.
    - Compare immediate expected payoff of cooperating vs defecting; cooperate only if it strictly
      increases immediate expected payoff (ties -> defect).
    - If I cooperated and the group failed to reach the threshold (I was "suckered"),
      set a short punishment window during which I defect to deter exploitation.
    - Always defect in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prior_alpha = 0.5
        self.tie_bias = 0.01
        self.punishment_length = min(3, max(1, self.game_description.n_rounds))
        self.punishment_timer = 0
        self._in_punishment_phase_last_decision = False
        self._last_observed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        r = int(self.game_description.n_rounds)
        if history is not None:
            completed_rounds = int(history.round_number)
            if completed_rounds > self._last_observed_rounds:
                if self._in_punishment_phase_last_decision and self.punishment_timer > 0:
                    self.punishment_timer = max(0, self.punishment_timer - 1)
                if completed_rounds >= 1:
                    last_idx = completed_rounds - 1
                    my_last_action = bool(history.my_actions[last_idx])
                    others_last_coop = int(np.sum(history.opponent_actions[last_idx, :]))
                    total_cooperators = others_last_coop + (1 if my_last_action else 0)
                    if my_last_action and total_cooperators < m:
                        self.punishment_timer = self.punishment_length
                self._last_observed_rounds = completed_rounds
                self._in_punishment_phase_last_decision = False
        if t >= r:
            self._in_punishment_phase_last_decision = False
            return Action.D
        if self.punishment_timer > 0:
            self._in_punishment_phase_last_decision = True
            return Action.D
        if t == 1 or history is None:
            self._in_punishment_phase_last_decision = False
            return Action.D
        obs_rounds = t - 1
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            coop_counts = np.zeros(n - 1, dtype=float)
        else:
            coop_counts = np.sum(opp_actions.astype(float), axis=0)
        denom = obs_rounds + 2.0 * self.prior_alpha
        if denom <= 0:
            denom = 1.0
        p_js = ((coop_counts + self.prior_alpha) / denom).tolist()
        n_opponents = n - 1
        probs = [0.0] * (n_opponents + 1)
        probs[0] = 1.0
        for p in p_js:
            new_probs = probs[:]
            for x in range(0, n_opponents + 1):
                if probs[x] == 0.0:
                    continue
                new_probs[x] += 0.0
            new_probs = [0.0] * (n_opponents + 1)
            for x in range(0, n_opponents + 1):
                prob_x = probs[x]
                if prob_x == 0.0:
                    continue
                new_probs[x] += prob_x * (1.0 - p)
                if x + 1 <= n_opponents:
                    new_probs[x + 1] += prob_x * p
            probs = new_probs

        def sum_from(start_idx):
            if start_idx > n_opponents:
                return 0.0
            start = max(0, start_idx)
            return float(sum(probs[start:]))
        P_others_ge_m_minus_1 = sum_from(m - 1)
        P_others_ge_m = sum_from(m)
        pi_C = k * P_others_ge_m_minus_1
        pi_D = 1.0 + k * P_others_ge_m
        if pi_C > pi_D + self.tie_bias:
            self._in_punishment_phase_last_decision = False
            return Action.C
        else:
            self._in_punishment_phase_last_decision = False
            return Action.D

class Strategy_AGGRESSIVE_92(BaseStrategy):
    """
    Aggressive pivotal/cooperator-exploiting strategy for the Collective Risk Dilemma.

    - Default: defect on the first opportunity (aggressive).
    - Estimate the empirical distribution of the number S of other players who cooperate
      using past rounds (opponent_actions). Use Laplace smoothing (beta).
    - Compute expected payoff of cooperating vs defecting this round:
        E_C = k * P(S >= m-1)
        E_D = 1 + (k-1) * P(S >= m)
      Cooperate iff E_C > E_D + eps. Tie-break favors defection (aggressive).
    - Uses only public history and given game parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.beta = 1.0
        self.eps = 0.001
        self.decay = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            return Action.D
        others_coops = np.sum(opp_actions, axis=1).astype(int)
        if self.decay != 1.0 and self.decay > 0.0:
            rounds = others_coops.shape[0]
            weights = np.array([self.decay ** (rounds - 1 - i) for i in range(rounds)], dtype=float)
            counts = np.zeros(n, dtype=float)
            for x, w in zip(others_coops, weights):
                if 0 <= x < n:
                    counts[x] += w
        else:
            counts = np.bincount(others_coops, minlength=n).astype(float)
        pseudo = counts + float(self.beta)
        total = float(np.sum(pseudo))
        if total <= 0.0:
            return Action.D
        p_s = pseudo / total
        if m <= 0:
            P_m = 1.0
        elif m >= n:
            P_m = 0.0
        else:
            P_m = float(np.sum(p_s[m:]))
        if m - 1 <= 0:
            P_mminus1 = 1.0
        elif m - 1 >= n:
            P_mminus1 = 0.0
        else:
            P_mminus1 = float(np.sum(p_s[max(0, m - 1):]))
        E_C = k * P_mminus1
        E_D = 1.0 + (k - 1.0) * P_m
        if E_C > E_D + self.eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_93(BaseStrategy):
    """
    Aggressive pivotal/cohort-based strategy for the Collective Risk Dilemma.

    - Defect by default.
    - Estimate opponents' cooperation frequencies and act only when cooperation is
      likely to be pivotal or safe.
    - Maintain a PunishedSet (with timers) for persistent defectors; withhold cooperation
      until rehabilitation.
    - Exploit when the threshold is met without me.
    - Become strictly self-interested in the final L rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.p_good_0 = 0.7
        self.p_bad = 0.25
        self.F = max(2, math.ceil(self.r / 10))
        self.P = max(2, math.ceil(self.r / 10))
        self.L = min(3, max(0, self.r - 1))
        self.p_conf_base = 0.5
        self.p_avg = 0.6
        self.epsilon = 0.02
        self.punish_timers = [0] * max(0, self.n - 1)
        self.last_expected_cooperators = set()
        self.permanent_refuse = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_timers = [max(0, t - 1) for t in self.punish_timers]
            self.last_expected_cooperators.clear()
            return Action.D
        completed = history.round_number
        t = completed + 1
        num_opponents = max(0, self.n - 1)
        if num_opponents == 0:
            return Action.D
        self.punish_timers = [max(0, tt - 1) for tt in self.punish_timers]
        opp_actions = history.opponent_actions
        if completed > 0:
            coop_counts = np.sum(opp_actions, axis=0).astype(float)
        else:
            coop_counts = np.zeros((num_opponents,), dtype=float)
        denom = max(1, completed)
        freq = (coop_counts / float(denom)).tolist()
        lastC = []
        if completed > 0:
            for j in range(num_opponents):
                col = opp_actions[:, j]
                true_indices = np.nonzero(col)[0]
                if true_indices.size == 0:
                    lastC.append(0)
                else:
                    lastC.append(int(true_indices[-1]) + 1)
        else:
            lastC = [0] * num_opponents
        E_others = float(np.sum(freq))
        reduction = (t - 1) / max(1, self.r - 1) * 0.5
        p_good_t = self.p_good_0 * max(0.4, 1.0 - reduction)
        punished_flags = [tt > 0 for tt in self.punish_timers]
        GoodCount = 0
        BadCount = 0
        for j in range(num_opponents):
            if not punished_flags[j] and freq[j] >= p_good_t:
                GoodCount += 1
            last_round_defected = True
            if completed > 0:
                last_round_defected = not bool(opp_actions[-1, j])
            recent_defection_flag = last_round_defected or lastC[j] < max(0, t - 1)
            if freq[j] <= self.p_bad and recent_defection_flag:
                BadCount += 1
        for j in range(num_opponents):
            window = min(completed, self.F)
            if window > 0:
                recent = opp_actions[-window:, j]
                if freq[j] <= self.p_bad and np.sum(recent) == 0:
                    self.punish_timers[j] = max(self.punish_timers[j], self.P)
            else:
                pass
        half_needed = math.ceil(self.F / 2)
        for j in range(num_opponents):
            if self.punish_timers[j] > 0 and completed > 0:
                window = min(completed, self.F)
                recent = opp_actions[-window:, j]
                if np.sum(recent) >= half_needed:
                    self.punish_timers[j] = 0
        if len(self.last_expected_cooperators) > 0 and completed > 0:
            for j in list(self.last_expected_cooperators):
                if j < 0 or j >= num_opponents:
                    continue
                if not bool(opp_actions[-1, j]):
                    self.punish_timers[j] = max(self.punish_timers[j], self.P)
            self.last_expected_cooperators.clear()
        punished_flags = [tt > 0 for tt in self.punish_timers]
        punished_count = sum((1 for f in punished_flags if f))
        known_defectors = sum((1 for f in freq if f <= self.p_bad))
        if punished_count + known_defectors >= self.n - self.m + 1:
            self.permanent_refuse = True
        if self.permanent_refuse:
            if t <= max(2, math.ceil(self.r / 5)) and random.random() < self.epsilon:
                return Action.C
            return Action.D
        if self.L > 0 and t > self.r - self.L:
            if E_others >= self.m:
                return Action.D
            elif math.floor(E_others) == self.m - 1:
                candidates = []
                for j in range(num_opponents):
                    if not punished_flags[j]:
                        candidates.append((j, freq[j], lastC[j]))
                candidates.sort(key=lambda x: (-x[1], -x[2]))
                if len(candidates) < self.m - 1:
                    return Action.D
                candidate_set = candidates[:self.m - 1]
                freqs_candidate = [c[1] for c in candidate_set]
                p_end_conf = max(0.5, 0.5 + 0.2 * float(self.r - t) / max(1, self.r - 1))
                if min(freqs_candidate) >= p_end_conf:
                    self.last_expected_cooperators = set((c[0] for c in candidate_set))
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        if t <= max(2, math.ceil(self.r / 5)) and random.random() < self.epsilon:
            return Action.C
        nonpunished_good = 0
        for j in range(num_opponents):
            if not punished_flags[j] and freq[j] >= p_good_t:
                nonpunished_good += 1
        if nonpunished_good >= self.m and E_others >= self.m:
            return Action.D
        if E_others >= self.m:
            return Action.D
        if E_others <= self.m - 2:
            return Action.D
        if math.floor(E_others) == self.m - 1:
            candidates = []
            for j in range(num_opponents):
                if not punished_flags[j]:
                    candidates.append((j, freq[j], lastC[j]))
            if len(candidates) < self.m - 1:
                return Action.D
            candidates.sort(key=lambda x: (-x[1], -x[2]))
            candidate_set = candidates[:self.m - 1]
            freqs_candidate = [c[1] for c in candidate_set]
            confidence = min(freqs_candidate)
            average = float(np.mean(freqs_candidate)) if len(freqs_candidate) > 0 else 0.0
            p_conf = self.p_conf_base * max(0.5, 1.0 / max(1.0, self.k))
            if confidence >= p_conf or average >= self.p_avg:
                self.last_expected_cooperators = set((c[0] for c in candidate_set))
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_94(BaseStrategy):
    """
    Pivotal-Exploiter strategy for the Collective Risk Dilemma.

    Summary:
    - Aggressively defects by default to keep the private payoff = 1.
    - Estimates each opponent's cooperation probability via Laplace smoothing (beta=0.5).
    - Computes the probability p_exact that exactly (m-1) OTHER players will cooperate
      (dynamic programming convolution over heterogenous Bernoulli p_j).
    - Cooperates iff p_exact * k > 1 + gamma (default gamma=0.02).
    - Performs minimal stochastic probing early (epsilon_probe default 0.03), decaying
      with remaining rounds and limited to early rounds to gather data.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.beta = 0.5
        self.epsilon_probe = 0.03
        self.gamma = 0.02
        self.early_rounds_threshold = max(3, math.ceil(self.r / 10))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t_current = 1
        else:
            t_current = history.round_number + 1
        if history is None:
            epsilon = self.epsilon_probe * math.sqrt(max(0.0, (self.r - 0) / max(1, self.r)))
            if t_current <= self.early_rounds_threshold and random.random() < epsilon:
                return Action.C
            return Action.D
        rounds_seen = history.round_number
        remaining = max(0, self.r - rounds_seen)
        epsilon = self.epsilon_probe * math.sqrt(remaining / max(1, self.r))
        if t_current <= self.early_rounds_threshold and random.random() < epsilon:
            return Action.C
        if rounds_seen == 0:
            p_js = np.full(self.n - 1, self.beta / (2 * self.beta), dtype=float)
        else:
            coop_counts = np.sum(history.opponent_actions, axis=0)
            denom = rounds_seen + 2 * self.beta
            p_js = (coop_counts + self.beta) / denom
            p_js = np.array(p_js, dtype=float)
        L = self.m - 1
        num_opponents = self.n - 1
        prob = [0.0] * (num_opponents + 1)
        prob[0] = 1.0
        processed = 0
        for pj in p_js:
            for s in range(processed, -1, -1):
                p_s = prob[s]
                prob[s + 1] += p_s * pj
                prob[s] = p_s * (1.0 - pj)
            processed += 1
        p_exact = prob[L] if 0 <= L <= num_opponents else 0.0
        if p_exact * self.k > 1.0 + self.gamma:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_95(BaseStrategy):
    """
    Pivotal Retaliator (aggressive, adaptive)

    - Default: defect except when my single cooperation is pivotal (p == m-1)
      and all predicted pivotal cooperators are trusted (trust >= 0).
    - Free-ride when predicted trusted cooperators already meet the threshold.
    - If I cooperated and the threshold was met while some players defected,
      punish those defectors by decrementing their trust.
    - Forgive a negative-trust player after FORGIVE_WINDOW consecutive cooperations.
    - Probe with small probability PROBE_EPS to detect changes.
    - Always defect on first and final rounds.
    """
    PUNISH_PENALTY = 1
    FORGIVE_WINDOW = 2
    PROBE_EPS = 0.03

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_opponents = max(0, self.game_description.n_players - 1)
        self.trust = [0 for _ in range(self.n_opponents)]
        self.consec_coop = [0 for _ in range(self.n_opponents)]
        self._last_processed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            current_round = 1
        else:
            current_round = history.round_number + 1
        if history is not None and history.round_number > self._last_processed:
            for round_idx in range(self._last_processed, history.round_number):
                my_coop = bool(history.my_actions[round_idx])
                opp_row = history.opponent_actions[round_idx, :] if self.n_opponents > 0 else np.array([], dtype=bool)
                total_coops = int(my_coop) + int(np.sum(opp_row))
                threshold_met = total_coops >= self.game_description.m
                if threshold_met and my_coop:
                    for j in range(self.n_opponents):
                        if not bool(opp_row[j]):
                            self.trust[j] -= self.PUNISH_PENALTY
                            self.consec_coop[j] = 0
                        else:
                            self.consec_coop[j] += 1
                else:
                    for j in range(self.n_opponents):
                        if bool(opp_row[j]):
                            self.consec_coop[j] += 1
                        else:
                            self.consec_coop[j] = 0
                for j in range(self.n_opponents):
                    if self.trust[j] < 0 and self.consec_coop[j] >= self.FORGIVE_WINDOW:
                        self.trust[j] = 0
            self._last_processed = history.round_number
        if current_round == 1 or current_round == self.game_description.n_rounds:
            return Action.D
        predicted_cooperators = set()
        if history is not None and history.round_number > 0 and (self.n_opponents > 0):
            last_opp_actions = history.opponent_actions[-1, :]
            for j in range(self.n_opponents):
                if bool(last_opp_actions[j]) and self.trust[j] >= 0:
                    predicted_cooperators.add(j)
        p = len(predicted_cooperators)
        action = Action.D
        if p >= self.game_description.m:
            action = Action.D
        elif p == self.game_description.m - 1:
            action = Action.C
        else:
            action = Action.D
        if action == Action.D and random.random() < self.PROBE_EPS:
            action = Action.C
        return action

class Strategy_AGGRESSIVE_96(BaseStrategy):
    """
    Pivotal Aggressor (PA)

    Aggressive, adaptive strategy for the Collective Risk Dilemma.
    - Default: defect.
    - Cooperate only when your single cooperation is likely to be pivotal:
      Pr(exactly m-1 of the other n-1 players cooperate) > 1/k.
    - Estimates each opponent's cooperation probability from recent history
      using a sliding window with Laplace smoothing (alpha=beta=1).
    - Last round: always defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.window_default = min(50, max(1, game_description.n_rounds))
        self.alpha = 1.0
        self.beta = 1.0
        self.p0 = 0.2
        self.epsilon = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = float(self.game_description.k)
        if state.round_number >= r - 1:
            return Action.D
        T_completed = history.round_number
        W = min(self.window_default, r)
        observed_T = min(T_completed, W)
        start_idx = max(0, T_completed - observed_T)
        n_opponents = max(0, n - 1)
        p_list = []
        if observed_T <= 0:
            p_list = [self.p0] * n_opponents
        else:
            recent = history.opponent_actions[start_idx:start_idx + observed_T, :]
            if n_opponents == 1 and recent.ndim == 1:
                recent = recent.reshape(observed_T, 1)
            for j in range(n_opponents):
                count_j = int(np.sum(recent[:, j]))
                T_j = observed_T
                if T_j <= 0:
                    p_j = self.p0
                else:
                    p_j = (count_j + self.alpha) / (T_j + self.alpha + self.beta)
                p_list.append(float(p_j))
        max_s = n_opponents
        arr = [0.0] * (max_s + 1)
        arr[0] = 1.0
        current_max = 0
        for p in p_list:
            for s in range(current_max, -1, -1):
                prob_s = arr[s]
                if prob_s == 0.0:
                    arr[s] = prob_s * (1.0 - p)
                    continue
                arr[s + 1] += prob_s * p
                arr[s] = prob_s * (1.0 - p)
            current_max += 1
        s_target = m - 1
        prob_pivotal = 0.0
        if 0 <= s_target <= max_s:
            prob_pivotal = arr[s_target]
        threshold = 1.0 / k if k != 0.0 else float('inf')
        if prob_pivotal > threshold:
            if self.epsilon > 0.0 and random.random() < self.epsilon:
                return Action.D
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_97(BaseStrategy):
    """
    Selfish Opportunist for Collective Risk Dilemma.

    Defect-by-default, estimate each opponent's cooperation probability from a short
    rolling window (with Bayesian smoothing toward a low prior), compute the
    heterogeneous-binomial distribution of how many others will cooperate, and
    choose the action (C or D) that maximizes my immediate expected payoff.
    Small-probability exploration (higher early, lower later) flips the chosen
    action occasionally to probe opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prior_coop = 0.1
        self.prior_weight = 1.0
        self.max_window = 20
        self.eps_early = 0.05
        self.eps_late = 0.02

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if n - 1 < m:
            return Action.D
        completed = 0 if history is None else int(history.round_number)
        t = completed + 1
        eps = self.eps_early if t <= 5 else self.eps_late
        n_opp = n - 1
        H = min(self.max_window, completed)
        p_js = [self.prior_coop] * n_opp
        if history is not None and H > 0:
            recent = history.opponent_actions[-H:, :] if H > 0 else history.opponent_actions
            for j in range(n_opp):
                coop_count = int(np.sum(recent[:, j]))
                p_js[j] = (coop_count + self.prior_weight * self.prior_coop) / (H + self.prior_weight)
        else:
            p_js = [self.prior_coop] * n_opp
        pmf = np.zeros(n_opp + 1, dtype=float)
        pmf[0] = 1.0
        for p in p_js:
            new_pmf = np.zeros_like(pmf)
            new_pmf += pmf * (1.0 - p)
            new_pmf[1:] += pmf[:-1] * p
            pmf = new_pmf
        total_prob = float(np.sum(pmf))
        if total_prob > 0:
            pmf /= total_prob
        exp_payoff_C = 0.0
        exp_payoff_D = 0.0
        for x in range(n_opp + 1):
            prob = float(pmf[x])
            payoff_C = k if x + 1 >= m else 0.0
            payoff_D = 1.0 + k if x >= m else 1.0
            exp_payoff_C += prob * payoff_C
            exp_payoff_D += prob * payoff_D
        if exp_payoff_C > exp_payoff_D:
            action = Action.C
        else:
            action = Action.D
        if random.random() < eps:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_98(BaseStrategy):
    """
    Aggressive Threshold Enforcer (ATE)

    Aggressively exploits safe free-riding opportunities, only cooperates when pivotal
    cooperation is likely to pay off (based on recent reliability of others) or when
    others already met the threshold without you. Uses finite but substantial group-level
    punishments triggered by betrayals and threshold failures; forgives only after sustained
    good behaviour.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.L = min(5, max(0, self.r - 1))
        self.P_base = max(1, math.floor(0.2 * self.r))
        self.tau = 0.6
        self.F = 2
        self.punish_until_round = 0
        self.post_punish_good_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', 1)
        if history is None or history.round_number == 0:
            return Action.D
        last_round_idx = history.round_number - 1
        rounds_completed = history.round_number
        if rounds_completed >= 1:
            opp_coops_last = int(np.sum(history.opponent_actions[last_round_idx, :]))
            my_coop_last = int(bool(history.my_actions[last_round_idx]))
            total_coops_last = opp_coops_last + my_coop_last
            if last_round_idx + 1 <= self.punish_until_round and self.punish_until_round > 0:
                if total_coops_last >= self.m:
                    self.post_punish_good_count += 1
                else:
                    self.post_punish_good_count = 0
                if self.post_punish_good_count >= self.F:
                    self.punish_until_round = 0
                    self.post_punish_good_count = 0
        C_others_last = int(np.sum(history.opponent_actions[last_round_idx, :]))
        divisor = min(self.L, rounds_completed)
        n_opponents = self.n - 1
        recent_coop_freq = [0.5] * n_opponents
        if divisor > 0:
            slice_start = rounds_completed - divisor
            recent_slice = history.opponent_actions[slice_start:rounds_completed, :]
            for j in range(n_opponents):
                recent_coop_freq[j] = float(np.sum(recent_slice[:, j])) / float(divisor)
        trusted_set = {j for j in range(n_opponents) if recent_coop_freq[j] >= self.tau}
        last_round_threshold_met = int(np.sum(history.opponent_actions[last_round_idx, :])) + int(bool(history.my_actions[last_round_idx])) >= self.m
        if t <= self.punish_until_round and self.punish_until_round > 0:
            return Action.D
        if C_others_last >= self.m:
            return Action.D
        is_last_round = t == self.r
        if C_others_last == self.m - 1:
            S = [j for j in range(n_opponents) if history.opponent_actions[last_round_idx, j]]
            if len(S) > 0:
                avg_reliability_S = float(np.mean([recent_coop_freq[j] for j in S]))
            else:
                avg_reliability_S = 0.0
            if is_last_round:
                if avg_reliability_S >= 0.5:
                    return Action.C
                else:
                    return Action.D
            k_dominance = self.k - 1.0 >= 0.5 * float(self.P_base)
            if avg_reliability_S >= self.tau or k_dominance:
                return Action.C
            else:
                num_exploiters_estimate = sum((1 for j in S if recent_coop_freq[j] < self.tau))
                punish_length = self.P_base * max(1, num_exploiters_estimate)
                self.punish_until_round = min(self.r, t + int(punish_length) - 1)
                self.post_punish_good_count = 0
                return Action.D
        if C_others_last < self.m - 1:
            betrayed_trusted = [j for j in trusted_set if not history.opponent_actions[last_round_idx, j]]
            if last_round_threshold_met and len(betrayed_trusted) > 0:
                punish_length = self.P_base * max(1, len(betrayed_trusted))
                self.punish_until_round = min(self.r, t + int(punish_length) - 1)
                self.post_punish_good_count = 0
            return Action.D
        if C_others_last == self.m - 1:
            S = [j for j in range(n_opponents) if history.opponent_actions[last_round_idx, j]]
            if len(S) > 0:
                avg_reliability_S = float(np.mean([recent_coop_freq[j] for j in S]))
                if avg_reliability_S >= self.tau:
                    return Action.C
        return Action.D

class Strategy_AGGRESSIVE_100(BaseStrategy):
    """
    Aggressive pivotal-cooperator strategy for the Collective Risk Dilemma.

    - Maintains per-opponent EWMA estimates of cooperation (initialized with tiny prior alpha).
    - At each round computes the probability p_eq that exactly (m-1) opponents cooperate.
      Uses exact Poisson-binomial DP for moderate opponent counts, falling back to a normal
      approximation for large counts.
    - Cooperates iff p_eq > 1/k (ties broken toward defection). Beliefs update after observing
      opponents each round via EWMA with recency weight beta.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.02
        self.beta = 0.4
        self.hesitation_eps = 0.0
        self.n_opponents = max(0, self.n - 1)
        self.EWMA = [self.alpha for _ in range(self.n_opponents)]
        self._last_update_round = 0
        self._tol = 1e-12

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        completed = 0 if history is None else history.round_number
        if history is not None and completed > self._last_update_round:
            start = self._last_update_round
            end = completed
            if self.n_opponents > 0:
                for t in range(start, end):
                    round_actions = history.opponent_actions[t]
                    for j in range(self.n_opponents):
                        observed = 1.0 if bool(round_actions[j]) else 0.0
                        self.EWMA[j] = self.beta * observed + (1.0 - self.beta) * self.EWMA[j]
            self._last_update_round = completed
        p_list = []
        for j in range(self.n_opponents):
            p_j = self.alpha + (1.0 - self.alpha) * self.EWMA[j]
            if p_j < 0.0:
                p_j = 0.0
            elif p_j > 1.0:
                p_j = 1.0
            p_list.append(p_j)
        target = self.m - 1
        p_eq = 0.0
        if target < 0 or target > self.n_opponents:
            p_eq = 0.0
        elif self.n_opponents <= 200:
            prob = [0.0] * (self.n_opponents + 1)
            prob[0] = 1.0
            for p in p_list:
                for s in range(self.n_opponents - 1, -1, -1):
                    prob[s + 1] += prob[s] * p
                    prob[s] = prob[s] * (1.0 - p)
            p_eq = prob[target]
        else:
            mu = float(sum(p_list))
            var = float(sum((p * (1.0 - p) for p in p_list)))
            if var <= 1e-12:
                mode = int(round(mu))
                p_eq = 1.0 if mode == target else 0.0
            else:
                sigma = math.sqrt(var)
                upper = (target + 0.5 - mu) / sigma
                lower = (target - 0.5 - mu) / sigma

                def std_cdf(x):
                    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
                p_eq = max(0.0, std_cdf(upper) - std_cdf(lower))
        threshold = 1.0 / self.k + self.hesitation_eps
        if p_eq > threshold + self._tol:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_101(BaseStrategy):
    """
    Aggressive Pivotal Trigger (APT)

    - Defect by default, cooperate only when your single cooperation is plausibly
      decisive to secure the public reward (based on recent opponent behavior).
    - Use a grim trigger: if you ever cooperated and the round still failed to
      reach the threshold m, defect forever thereafter.
    - Always defect in the final round.
    - Estimates opponents' cooperation probabilities from a recent memory window
      (Laplace smoothing) and treats opponents as independent Bernoulli draws.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._punish = False
        self.L_max = 10
        self.p_exploit = 0.5
        self.p_coop = 0.65

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is not None and history.round_number >= 1 and (not self._punish):
            last_idx = history.round_number - 1
            my_last_coop = bool(history.my_actions[last_idx])
            opps_last = int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size else 0
            total_cooperators_last = opps_last + (1 if my_last_coop else 0)
            if my_last_coop and total_cooperators_last < m:
                self._punish = True
        if state is not None and state.round_number >= r:
            return Action.D
        if self._punish:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        L = min(self.L_max, max(0, t - 1))
        if L == 0:
            p_js = np.zeros(self.game_description.n_players - 1, dtype=float)
        else:
            opp_actions = history.opponent_actions
            rounds_available = opp_actions.shape[0]
            use_L = min(L, rounds_available)
            if use_L <= 0:
                p_js = np.zeros(self.game_description.n_players - 1, dtype=float)
            else:
                recent = opp_actions[-use_L:, :]
                coops = np.sum(recent.astype(int), axis=0)
                observed = use_L
                p_js = (coops + 1.0) / (observed + 2.0)
        n_others = n - 1
        probs = np.zeros(n_others + 1, dtype=float)
        probs[0] = 1.0
        for j in range(n_others):
            p = float(p_js[j]) if j < len(p_js) else 0.0
            new_probs = np.zeros_like(probs)
            new_probs += probs * (1.0 - p)
            new_probs[1:] += probs[:-1] * p
            probs = new_probs
        if m <= 0:
            P_already = 1.0
        elif m > n_others:
            P_already = 0.0
        else:
            P_already = float(np.sum(probs[m:]))
        pivot_threshold = max(0, m - 1)
        if pivot_threshold <= 0:
            P_pivotal = 1.0
        elif pivot_threshold > n_others:
            P_pivotal = 0.0
        else:
            P_pivotal = float(np.sum(probs[pivot_threshold:]))
        if P_already >= self.p_exploit:
            return Action.D
        elif P_pivotal >= self.p_coop:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_102(BaseStrategy):
    """
    Aggressive Pivotal Exploiter (APE)

    - Defects by default (aggressive/tie-break toward defection).
    - Estimates each opponent's cooperation probability from a recent rolling window
      with Laplace-like smoothing.
    - Computes the Poisson-Binomial probability that exactly (m-1) other players
      cooperate this round (the "pivotal" event).
    - Cooperates iff P_hat > 1/k (with a tiny numerical tolerance), otherwise defects.
    - Includes a tiny exploration probability (epsilon) that decays with rounds to
      occasionally cooperate for learning; exploration overrides the default rule.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.L_default = 10
        self.alpha = 0.1
        self.epsilon = 0.01
        self.tiny_eps = 1e-12
        self.tie_break = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        eps_t = self.epsilon / math.sqrt(max(1.0, float(t)))
        if random.random() < eps_t:
            return Action.C
        if history is None or history.round_number == 0:
            return Action.D
        completed = history.round_number
        opp_actions = history.opponent_actions
        if opp_actions is None:
            return Action.D
        n_opponents = opp_actions.shape[1]
        if n_opponents != self.n - 1:
            pass
        L = min(self.L_default, completed)
        start_idx = max(0, completed - L)
        p_list = []
        for j in range(n_opponents):
            H_j = opp_actions[start_idx:completed, j]
            observed_rounds_j = int(H_j.shape[0])
            coop_count_j = int(np.sum(H_j)) if observed_rounds_j > 0 else 0
            denom = observed_rounds_j + 2.0 * self.alpha
            if denom <= 0.0:
                p_j = 0.0
            else:
                p_j = (coop_count_j + self.alpha) / denom
            p_j = max(0.0, min(1.0, float(p_j)))
            p_list.append(p_j)
        target = self.m - 1
        if target < 0 or target > n_opponents:
            P_hat = 0.0
        else:
            dp = np.zeros(n_opponents + 1, dtype=float)
            dp[0] = 1.0
            current_max = 0
            for p in p_list:
                upper = current_max + 1
                for x in range(upper, -1, -1):
                    prob_no = dp[x] * (1.0 - p) if x <= current_max else 0.0
                    prob_yes = dp[x - 1] * p if x - 1 >= 0 and x - 1 <= current_max else 0.0
                    dp[x] = prob_no + prob_yes
                current_max += 1
            P_hat = float(dp[target])
        threshold = 1.0 / max(1e-12, self.k)
        if P_hat > threshold + self.tiny_eps:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_103(BaseStrategy):
    """
    Aggressive Pivotal Exploiter (APE)

    Aggressive, data-driven strategy for the Collective Risk Dilemma.
    - Default: defect.
    - Cooperates only when empirical probability of being pivotal (others == m-1)
      makes cooperation payoff-maximizing according to the pivotal inequality:
        k * P(other_count == m-1) > 2 + margin_coop * punish_multiplier
    - Probes rarely with probability epsilon_probe to detect latent cooperators.
    - Punishes wasted cooperation by increasing margin_coop via punish_multiplier
      and reducing probe rate; slowly decays these adjustments over time.
    - Always defects on the first and final round (no future leverage).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(50, max(10, self.r))
        self.margin_coop = 0.1
        self.baseline_epsilon = 0.02
        self.epsilon_probe = float(self.baseline_epsilon)
        self.punish_multiplier = 1.0
        self.min_samples = 5
        self.max_punish_multiplier = 10.0
        self.max_epsilon = 0.2
        self.min_epsilon = 1e-06

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == 1 or t == self.r:
            return Action.D
        if history is not None and history.round_number >= 1:
            last_idx = history.round_number - 1
            my_last_coop = bool(history.my_actions[last_idx])
            try:
                opp_coops_last = int(np.sum(history.opponent_actions[last_idx, :]))
            except Exception:
                opp_coops_last = 0
            total_coops_last = opp_coops_last + (1 if my_last_coop else 0)
            if my_last_coop and total_coops_last < self.m:
                self.punish_multiplier = min(self.max_punish_multiplier, self.punish_multiplier * 1.5)
                self.epsilon_probe = max(self.min_epsilon, float(self.epsilon_probe * 0.7))
            else:
                self.punish_multiplier = max(1.0, self.punish_multiplier * 0.98)
                if self.epsilon_probe < self.baseline_epsilon:
                    self.epsilon_probe = min(self.baseline_epsilon, float(self.epsilon_probe / 0.98))
                else:
                    self.epsilon_probe = max(self.baseline_epsilon, float(self.epsilon_probe * 0.98))
            if self.epsilon_probe > self.max_epsilon:
                self.epsilon_probe = self.max_epsilon
            if self.epsilon_probe < self.min_epsilon:
                self.epsilon_probe = self.min_epsilon
        if history is None:
            if random.random() < self.epsilon_probe:
                return Action.C
            return Action.D
        completed_rounds = history.round_number
        S = min(self.W, max(0, completed_rounds))
        if S < self.min_samples:
            if random.random() < self.epsilon_probe:
                return Action.C
            return Action.D
        start_idx = completed_rounds - S
        end_idx = completed_rounds
        window_opponent_actions = history.opponent_actions[start_idx:end_idx, :]
        try:
            c_array = np.sum(window_opponent_actions, axis=1).astype(float)
        except Exception:
            if random.random() < self.epsilon_probe:
                return Action.C
            return Action.D
        p_ge_m = float(np.count_nonzero(c_array >= float(self.m))) / float(S)
        p_ge_mminus1 = float(np.count_nonzero(c_array >= float(self.m - 1))) / float(S)
        p_eq_mminus1 = float(np.count_nonzero(c_array == float(self.m - 1))) / float(S)
        if p_ge_m >= 0.6:
            return Action.D
        effective_margin = 2.0 + self.margin_coop * self.punish_multiplier
        if self.k * p_eq_mminus1 > effective_margin:
            return Action.C
        if random.random() < self.epsilon_probe:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_104(BaseStrategy):
    """
    Pivotal-Exploit strategy for the Collective Risk Dilemma.

    - Estimates per-opponent cooperation probability using a Beta(1,2) prior.
    - Computes probability of being pivotal (exactly m-1 other cooperators).
    - Cooperates iff k * P_pivotal > 1 (ties -> defect).
    - Adds a small decaying probing probability epsilon_t to occasionally cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(self.game.n_players)
        self.r = int(self.game.n_rounds)
        self.m = int(self.game.m)
        self.k = float(self.game.k)
        self.alpha = 1.0
        self.beta = 2.0
        self.epsilon0 = 0.05
        self.c = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state is None:
            t = 1
        else:
            t = max(1, int(state.round_number))
        if history is None or history.round_number == 0:
            T = 0
            S = 0
        else:
            T = int(history.round_number)
            S = int(history.opponent_actions.sum())
        if T == 0:
            p_hat = self.alpha / (self.alpha + self.beta)
        else:
            denom = (self.n - 1) * T + (self.alpha + self.beta)
            if denom <= 0:
                p_hat = self.alpha / (self.alpha + self.beta)
            else:
                p_hat = (S + self.alpha) / denom
        k_choose = 0
        if 0 <= self.m - 1 <= self.n - 1:
            k_choose = math.comb(self.n - 1, self.m - 1)
        else:
            k_choose = 0
        exp1 = self.m - 1
        exp2 = self.n - 1 - (self.m - 1)
        P_pivotal = k_choose * p_hat ** exp1 * (1.0 - p_hat) ** exp2 if k_choose > 0 else 0.0
        epsilon_t = min(self.epsilon0, self.c / math.sqrt(max(1, t)))
        if random.random() < epsilon_t:
            return Action.C
        if self.k * P_pivotal > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_105(BaseStrategy):
    """
    Pivotal-Grim strategy for the Collective Risk Dilemma.

    - Aggressive exploitation: defect when others are predicted to meet the threshold.
    - Never waste a contribution when the threshold cannot be met.
    - In pivotal situations (your cooperation can flip the outcome), cooperate only if
      recent group behavior justifies trust; otherwise defect and (if a avoidable
      failure occurs) punish blameable defectors for P_punish rounds.
    - Occasional small-probability probes to detect hidden cooperators.
    - Always defect in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.L_window = min(5, self.r)
        self.tau_trust = 0.6
        self.P_punish = min(5, self.r)
        self.epsilon_probe = 0.03
        self.punishment_timer = 0
        self.blame_set = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if self.punishment_timer == 0 and len(self.blame_set) > 0:
            self.blame_set.clear()
        if history is not None and history.round_number > 0 and (self.punishment_timer == 0):
            last_idx = history.round_number - 1
            opp_coops_last = int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size else 0
            my_coop_last = int(bool(history.my_actions[last_idx]))
            total_coops_last = opp_coops_last + my_coop_last
            if total_coops_last == self.m - 1 and my_coop_last == 0:
                defectors = [j for j in range(self.n - 1) if not bool(history.opponent_actions[last_idx, j])]
                window_size = min(self.L_window, history.round_number)
                start = max(0, history.round_number - window_size)
                recent_coop_counts = np.zeros(self.n - 1, dtype=float)
                if window_size > 0:
                    recent_window = history.opponent_actions[start:history.round_number, :]
                    recent_coop_counts = np.sum(recent_window, axis=0).astype(float) / float(window_size)
                blame_candidates = [j for j in defectors if recent_coop_counts[j] > 0.0]
                if len(blame_candidates) == 0:
                    blame_candidates = defectors[:]
                if len(blame_candidates) > 0:
                    self.blame_set = set(blame_candidates)
                    self.punishment_timer = int(self.P_punish)
        if t == self.r:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if random.random() < self.epsilon_probe:
            return Action.C
        if history is None or history.round_number == 0:
            return Action.D
        rounds_completed = history.round_number
        window_size = min(self.L_window, rounds_completed)
        start = max(0, rounds_completed - window_size)
        num_opponents = self.n - 1
        coop_rates = np.zeros(num_opponents, dtype=float)
        if window_size > 0:
            recent_window = history.opponent_actions[start:rounds_completed, :]
            coop_rates = np.sum(recent_window, axis=0).astype(float) / float(window_size)
        predicted_others = float(np.sum(coop_rates))
        predicted_floor = math.floor(predicted_others + 1e-09)
        group_recent_avg = float(np.mean(coop_rates)) if coop_rates.size > 0 else 0.0
        recent_success_count = 0
        if window_size > 0:
            recent_success_count = 0
            for rr in range(start, rounds_completed):
                opp_coops = int(np.sum(history.opponent_actions[rr, :])) if history.opponent_actions.size else 0
                my_coop = int(bool(history.my_actions[rr]))
                total_coops = opp_coops + my_coop
                if total_coops >= self.m:
                    recent_success_count += 1
        recent_success_frac = float(recent_success_count) / float(max(1, window_size))
        if predicted_floor >= self.m:
            return Action.D
        if predicted_floor <= self.m - 2:
            return Action.D
        if predicted_floor == self.m - 1:
            if group_recent_avg >= self.tau_trust or recent_success_frac >= 0.5:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_106(BaseStrategy):
    """
    Aggressive selfish pivot strategy for the Collective Risk Dilemma.

    - Maintains a recency-weighted cooperation probability p_j for each opponent.
    - Computes the exact Poisson-binomial probability that exactly (m-1) opponents
      will cooperate this round using dynamic programming.
    - Cooperates iff k * P_exact > 1 (aggressive tie-breaking: defect on ties).
    - Rapidly adapts beliefs with exponential smoothing (alpha).
    - Optionally applies small probing in early rounds (p_probe), and reduces
      beliefs in the endgame by an endgame_factor to avoid last-round suckers.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.p0 = 0.15
        self.p_probe = 0.0
        self.endgame_factor = 0.5
        self.epsilon_tolerance = 1e-12
        self.epsilon_randomize = 1e-06
        self.p_opponents = None

    def _initialize_or_rebuild_beliefs(self, history: None | PlayerHistory):
        """
        Initialize or rebuild internal beliefs from prior and full history.
        This ensures correctness even if the strategy object is recreated.
        """
        n_players = self.game_description.n_players
        num_opponents = n_players - 1
        p = [float(self.p0) for _ in range(num_opponents)]
        if history is not None and history.round_number > 0:
            rounds = history.round_number
            for t in range(rounds):
                row = history.opponent_actions[t]
                for j in range(min(len(row), num_opponents)):
                    if bool(row[j]):
                        p[j] = self.alpha * 1.0 + (1.0 - self.alpha) * p[j]
                    else:
                        p[j] = self.alpha * 0.0 + (1.0 - self.alpha) * p[j]
        self.p_opponents = p

    def _poisson_binomial_p_exact(self, probs, target):
        """
        Compute Prob(sum Bernoulli(probs) == target) exactly by DP convolution.
        probs: iterable of probabilities for each Bernoulli
        target: integer target sum
        """
        n = len(probs)
        if target < 0 or target > n:
            return 0.0
        probs_dp = [0.0] * (n + 1)
        probs_dp[0] = 1.0
        for p in probs:
            p = float(p)
            new_dp = [0.0] * (n + 1)
            new_dp[0] = probs_dp[0] * (1.0 - p)
            for s in range(1, n + 1):
                new_dp[s] = probs_dp[s] * (1.0 - p) + probs_dp[s - 1] * p
            probs_dp = new_dp
        return probs_dp[target]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        gd = self.game_description
        n = gd.n_players
        m = gd.m
        k = float(gd.k)
        r = gd.n_rounds
        if self.p_opponents is None or (history is not None and len(self.p_opponents) != n - 1):
            self._initialize_or_rebuild_beliefs(history)
        if history is not None and history.round_number > 0 and (self.p_opponents is not None):
            if len(self.p_opponents) == history.opponent_actions.shape[1]:
                last_row = history.opponent_actions[-1]
                for j in range(len(self.p_opponents)):
                    if bool(last_row[j]):
                        self.p_opponents[j] = self.alpha * 1.0 + (1.0 - self.alpha) * self.p_opponents[j]
                    else:
                        self.p_opponents[j] = self.alpha * 0.0 + (1.0 - self.alpha) * self.p_opponents[j]
            else:
                self._initialize_or_rebuild_beliefs(history)
        completed_rounds = 0 if history is None else history.round_number
        current_round = completed_rounds + 1
        p_use = list(self.p_opponents) if self.p_opponents is not None else [float(self.p0)] * (n - 1)
        if current_round >= max(1, r - 1):
            p_use = [float(p) * float(self.endgame_factor) for p in p_use]
        early_round_limit = math.ceil(r / 3.0)
        if self.p_probe > 0.0 and current_round <= early_round_limit:
            if random.random() < float(self.p_probe):
                return Action.C
        target = m - 1
        P_exact = self._poisson_binomial_p_exact(p_use, target)
        lhs = k * P_exact
        distance = abs(lhs - 1.0)
        if distance <= self.epsilon_randomize:
            if random.random() < 0.9:
                return Action.D
            else:
                return Action.C
        if lhs > 1.0 + self.epsilon_tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_107(BaseStrategy):
    """
    Aggressive pivotal cooperater.

    Cooperate only when your cooperation is likely to be pivotal (i.e., your single
    cooperation flips the round from no reward to reward). Estimates opponents'
    cooperation probabilities from recent history with Laplace smoothing, computes
    the Poisson–Binomial distribution (or a normal approximation for many opponents),
    and cooperates iff Pr(S = m-1) > 1/k. Probe very rarely in the first round
    with probability eps_probe to gather data. Tie-breaks favor defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_max = min(20, max(1, self.game_description.n_rounds))
        self.eps_probe = min(0.05, 1.0 / max(1.0, float(self.game_description.n_rounds)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None:
            if getattr(state, 'round_number', 1) == r:
                return Action.D
            if random.random() < self.eps_probe:
                return Action.C
            return Action.D
        completed_rounds = int(history.round_number)
        current_round = getattr(state, 'round_number', completed_rounds + 1)
        if current_round > r:
            current_round = r
        T = min(self.W_max, completed_rounds)
        if T <= 0:
            return Action.D
        opp_actions = history.opponent_actions
        if opp_actions.ndim != 2:
            return Action.D
        n_opponents = opp_actions.shape[1]
        p_list = []
        recent = opp_actions[-T:, :]
        coop_counts = np.sum(recent, axis=0)
        for j in range(n_opponents):
            C_count = int(coop_counts[j])
            p_j = (C_count + 1.0) / (T + 2.0)
            if p_j < 0.0:
                p_j = 0.0
            elif p_j > 1.0:
                p_j = 1.0
            p_list.append(float(p_j))
        target_s = m - 1
        if target_s < 0 or target_s > n_opponents:
            P_eq = 0.0
        elif n_opponents > 60:
            mu = float(sum(p_list))
            var = float(sum((p * (1.0 - p) for p in p_list)))
            if var <= 1e-12:
                S_det = int(round(mu))
                P_eq = 1.0 if S_det == target_s else 0.0
            else:
                sigma = math.sqrt(var)

                def std_normal_cdf(x):
                    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
                upper = (target_s + 0.5 - mu) / sigma
                lower = (target_s - 0.5 - mu) / sigma
                P_eq = float(max(0.0, min(1.0, std_normal_cdf(upper) - std_normal_cdf(lower))))
        else:
            prob = np.zeros(n_opponents + 1, dtype=float)
            prob[0] = 1.0
            for p in p_list:
                for s in range(n_opponents - 1, -1, -1):
                    if prob[s] != 0.0:
                        prob[s + 1] += prob[s] * p
                        prob[s] *= 1.0 - p
                    else:
                        prob[s] *= 1.0 - p
            P_eq = float(prob[target_s])
        threshold = 1.0 / k if k != 0.0 else float('inf')
        if P_eq > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_108(BaseStrategy):
    """
    Aggressive Pivotal Exploiter (APE)

    - Cooperates only when a single cooperation is likely to be pivotal (turns a failure into success).
    - Defects when others already secure the threshold.
    - Maintains short, targeted punishments for opponents who unexpectedly defect and cause failures.
    - Occasionally probes to find exploitable cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(10, max(1, self.r - 1))
        self.p_reliable = 0.75
        self.p_blame = 0.5
        self.T_punish = min(3, max(1, math.ceil(self.r / 10)))
        self.p_probe = min(0.08, 1.0 / max(1, math.floor(max(1, self.r) / 5.0)))
        self.punish_until = [0] * (self.n - 1)
        self.last_probe_round = -9999
        self.probes_done = 0
        self.max_probes = 6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = max(1, int(state.round_number))
        if t == self.r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        rounds_done = history.round_number
        window_len = min(self.W, rounds_done)
        window_start = rounds_done - window_len
        opp_actions = history.opponent_actions
        if window_len > 0:
            recent_slice = opp_actions[window_start:rounds_done, :]
            coop_count = np.sum(recent_slice, axis=0).astype(float)
            p_j = (coop_count / float(window_len)).tolist()
        else:
            coop_count = np.zeros(self.n - 1, dtype=float)
            p_j = [0.0] * (self.n - 1)
        if rounds_done >= 1:
            last_round_others = int(np.sum(opp_actions[-1, :]))
        else:
            last_round_others = 0
        reliable_set = {j for j in range(self.n - 1) if p_j[j] >= self.p_reliable}
        reliable_count = len(reliable_set)
        if rounds_done >= 1:
            my_prev = bool(history.my_actions[-1])
            total_prev = int(last_round_others) + (1 if my_prev else 0)
            if last_round_others >= max(0, self.m - 1) and total_prev < self.m:
                blame_candidates = []
                for j in range(self.n - 1):
                    defected_last = not bool(opp_actions[-1, j])
                    if not defected_last:
                        continue
                    was_reliable = p_j[j] >= self.p_blame
                    cooperated_prev_prev = False
                    if rounds_done >= 2:
                        cooperated_prev_prev = bool(opp_actions[-2, j])
                    if was_reliable or cooperated_prev_prev:
                        blame_candidates.append(j)
                for j in blame_candidates:
                    new_until = t + self.T_punish - 1
                    if new_until > self.punish_until[j]:
                        self.punish_until[j] = new_until
        punished_now = [self.punish_until[j] >= t for j in range(self.n - 1)]
        alpha = 0.8
        sum_pj = float(sum(p_j))
        smooth_est = alpha * float(last_round_others) + (1.0 - alpha) * sum_pj
        expected_others = int(math.floor(smooth_est + 1e-09))
        action = Action.D
        if expected_others >= self.m:
            action = Action.D
        elif expected_others == max(0, self.m - 1):
            expected_cooperators_indices = []
            if rounds_done >= 1:
                last_coop_idxs = [j for j in range(self.n - 1) if bool(opp_actions[-1, j])]
                if len(last_coop_idxs) <= expected_others:
                    expected_cooperators_indices = last_coop_idxs[:]
                else:
                    sorted_idxs = sorted(last_coop_idxs, key=lambda jj: p_j[jj], reverse=True)
                    expected_cooperators_indices = sorted_idxs[:expected_others]
            if len(expected_cooperators_indices) < expected_others:
                all_sorted = sorted(range(self.n - 1), key=lambda jj: p_j[jj], reverse=True)
                for jj in all_sorted:
                    if jj not in expected_cooperators_indices:
                        expected_cooperators_indices.append(jj)
                    if len(expected_cooperators_indices) >= expected_others:
                        break
            problematic = False
            for jj in expected_cooperators_indices:
                if punished_now[jj] or p_j[jj] < self.p_blame:
                    problematic = True
                    break
            if not problematic:
                action = Action.C
            else:
                action = Action.D
        else:
            action = Action.D
        active_punishments = any(punished_now)
        time_since_last_probe = t - self.last_probe_round
        if action == Action.D and t < self.r and (not active_punishments):
            can_probe_now = self.probes_done < self.max_probes and time_since_last_probe >= max(1, self.W)
            if can_probe_now and random.random() < self.p_probe:
                action = Action.C
                self.last_probe_round = t
                self.probes_done += 1
        return action

class Strategy_AGGRESSIVE_109(BaseStrategy):
    """
    Exploit-and-Pivot strategy.

    - Aggressively defects by default to exploit cooperators.
    - Cooperates only when empirical probability that others are exactly m-1 (i.e. you would be pivotal)
      exceeds 1/k (so that expected payoff of cooperating > defecting).
    - Uses an exponential recency weighting (EMA-like) to make the pivot-frequency estimate adaptive.
    - First round (no history) returns D.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.epsilon = 1e-06
        self.half_life = 3.0
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        T = history.round_number
        if self.n <= 1 or self.m <= 0 or self.m >= self.n:
            return Action.D
        opp_actions = history.opponent_actions[:T, :]
        s_array = np.sum(opp_actions, axis=1).astype(float)
        if T == 1:
            weights = np.array([1.0], dtype=float)
        else:
            ages = T - 1 - np.arange(T, dtype=float)
            ln2 = math.log(2.0)
            decay = -ln2 / max(1e-12, self.half_life)
            weights = np.exp(decay * ages)
        total_weight = float(np.sum(weights))
        pivot_mask = s_array == float(self.m - 1)
        count_pivotal = float(np.sum(weights[pivot_mask]))
        p_hat = (count_pivotal + self.epsilon) / (total_weight + 2.0 * self.epsilon)
        if p_hat > 1.0 / self.k:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_110(BaseStrategy):
    """
    Aggressive pivotal-cooperator strategy for the Collective Risk Dilemma.

    - Defects by default and only cooperates when the posterior probability of
      being exactly pivotal (others = m-1) is high enough that the expected
      benefit of cooperating exceeds defecting by more than `margin`.
    - Uses independent per-opponent Beta posteriors (alpha_C=0.5, alpha_D=1.5 by default)
      updated from observed opponent actions to estimate each opponent's cooperation probability.
    - Computes the distribution of the number of other cooperators via convolution
      (independence assumption) and applies the EV decision rule described in the spec.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prior_alpha_C = 0.5
        self.prior_alpha_D = 1.5
        self.margin = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        n_opponents = n - 1
        if n_opponents <= 0:
            return Action.D
        if history is None:
            p_list = [self.prior_alpha_C / (self.prior_alpha_C + self.prior_alpha_D)] * n_opponents
        else:
            rounds_completed = history.round_number
            if rounds_completed == 0:
                coop_counts = np.zeros(n_opponents, dtype=int)
            else:
                coop_counts = np.sum(history.opponent_actions, axis=0).astype(int)
                if coop_counts.shape[0] > n_opponents:
                    coop_counts = coop_counts[:n_opponents]
                elif coop_counts.shape[0] < n_opponents:
                    pad = np.zeros(n_opponents - coop_counts.shape[0], dtype=int)
                    coop_counts = np.concatenate([coop_counts, pad])
            p_list = []
            for j in range(n_opponents):
                alphaC = self.prior_alpha_C + int(coop_counts[j])
                alphaD = self.prior_alpha_D + int(rounds_completed - int(coop_counts[j]))
                denom = alphaC + alphaD
                p_j = alphaC / denom if denom > 0 else 0.0
                p_list.append(float(p_j))
        prob = np.array([1.0], dtype=float)
        for p_j in p_list:
            prob = np.convolve(prob, np.array([1.0 - p_j, p_j], dtype=float))
        idx_exact = m - 1
        if 0 <= idx_exact < prob.size:
            P_exact = float(prob[idx_exact])
        else:
            P_exact = 0.0
        if m < prob.size:
            P_atleast = float(np.sum(prob[m:]))
        else:
            P_atleast = 0.0
        EV_defect = 1.0 + k * P_atleast
        EV_cooperate = k * (P_atleast + P_exact)
        if EV_cooperate > EV_defect + self.margin:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_111(BaseStrategy):
    """
    Aggressive, opportunistic strategy for the Collective Risk Dilemma.

    - Maintains an exponential moving-average (EMA) estimate P of the probability
      that exactly m-1 of the other players will cooperate in a given round.
    - Cooperates in the next round only if cooperating has positive immediate
      expected value: P * k > 1 (with a small epsilon margin; ties go to Defect).
    - Uses a responsive EMA update from the most recently completed round.
    - If the agent cooperated in the last round and the threshold was not met
      (i.e., the agent was 'exploited'), the agent halves P and enters a short
      punishment period of G rounds during which it defects unconditionally.
    - Default first-round action is Defect (history is None).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.lambda_rate = 0.3
        self.small_prior = 0.05
        self.epsilon = 0.01
        self.punishment_length = 3
        self.P = float(self.small_prior)
        self.punishment_counter = 0
        self.last_observed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is not None and history.round_number > self.last_observed_rounds:
            if history.round_number >= 1:
                others_count_last = int(np.sum(history.opponent_actions[-1, :]))
                I = 1 if others_count_last == m - 1 else 0
                self.P = self.lambda_rate * I + (1.0 - self.lambda_rate) * self.P
                my_last_action_was_C = bool(history.my_actions[-1])
                total_last = others_count_last + (1 if my_last_action_was_C else 0)
                if my_last_action_was_C and total_last < m:
                    self.P = float(self.P) * 0.5
                    self.punishment_counter = max(self.punishment_counter, self.punishment_length)
            self.last_observed_rounds = history.round_number
        if history is None:
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if self.P * k > 1.0 + self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_112(BaseStrategy):
    """
    Opportunistic-Pivotal Aggressor

    Default: defect every round except when the estimated probability that exactly
    m-1 other players will cooperate exceeds 1/k, in which case cooperate.
    Uses per-opponent recent-window Laplace-smoothed frequency estimates (aggressive
    unseen prior) and an optional tiny probing probability epsilon that flips the
    chosen action to avoid perfect predictability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.prior_alpha = 1.0
        self.unseen_prior = 0.05
        self.w = int(min(max(5, math.floor(self.r / 4)), 50))
        self.epsilon = min(0.02, 1.0 / (10.0 * max(1, self.r)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if history.opponent_actions.size == 0:
            base_choice = Action.D
            if random.random() < self.epsilon:
                return Action.C if base_choice == Action.D else Action.D
            return base_choice
        num_opponents = history.opponent_actions.shape[1]
        rounds_observed = history.round_number
        window_len = min(self.w, rounds_observed)
        p_list = []
        if window_len == 0:
            p_list = [self.unseen_prior] * num_opponents
        else:
            window = history.opponent_actions[-window_len:, :]
            coop_counts = np.sum(window, axis=0)
            for j in range(num_opponents):
                t_j = int(window_len)
                c_j = int(coop_counts[j])
                if t_j == 0:
                    p_j = self.unseen_prior
                else:
                    p_j = (c_j + self.prior_alpha) / (t_j + 2.0 * self.prior_alpha)
                p_list.append(float(p_j))
        s_target = self.m - 1
        if s_target < 0 or s_target > num_opponents:
            prob_exact_m1 = 0.0
        else:
            dist = [0.0] * (num_opponents + 1)
            dist[0] = 1.0
            max_seen = 0
            for p in p_list:
                newdist = [0.0] * (num_opponents + 1)
                for x in range(0, max_seen + 1):
                    prob_x = dist[x]
                    if prob_x == 0.0:
                        continue
                    newdist[x] += prob_x * (1.0 - p)
                    newdist[x + 1] += prob_x * p
                max_seen += 1
                dist = newdist
            prob_exact_m1 = dist[s_target]
        threshold = 1.0 / max(1e-12, self.k)
        if prob_exact_m1 > threshold:
            chosen = Action.C
        else:
            chosen = Action.D
        if random.random() < self.epsilon:
            chosen = Action.C if chosen == Action.D else Action.D
        return chosen

class Strategy_AGGRESSIVE_113(BaseStrategy):
    """
    Aggressive Opportunistic Swing (AOS)

    Default: defect. Estimate the probability that exactly m-1 other players
    cooperated in a given round (i.e., that you're the pivotal/swing vote).
    Cooperate only when that estimated probability exceeds 1/k (so that the
    expected gain k * P > 1). Allow extremely rare, decaying probes early
    in the game to gather data; never probe in the last round. Use an
    exponential recency weight (lambda) and a small smoothing mass (alpha).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.lam = 0.9
        self.alpha = 1.0
        r = max(1, getattr(game_description, 'n_rounds', 1))
        self.eps_init = min(0.05, 1.0 / max(1, r))
        self.T_probe = min(3, max(0, r - 1))
        self.n = getattr(game_description, 'n_players')
        self.r = r
        self.m = getattr(game_description, 'm')
        self.k = getattr(game_description, 'k')

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        last_round = t == self.r
        if history is None or history.round_number == 0:
            P_hat = 0.0
        else:
            opp_actions = np.asarray(history.opponent_actions)
            X_other = np.sum(opp_actions, axis=1).astype(float)
            T_obs = X_other.shape[0]
            exponents = np.arange(T_obs - 1, -1, -1)
            weights = (self.lam ** exponents).astype(float)
            W_total = float(self.alpha) + float(np.sum(weights))
            mask = X_other == float(self.m - 1)
            W_E = float(np.sum(weights[mask]))
            P_hat = W_E / W_total if W_total > 0.0 else 0.0
        probe = False
        if not last_round and self.T_probe > 0 and (t <= self.T_probe):
            eps = self.eps_init * (1.0 - float(t - 1) / float(self.T_probe))
            if eps > 0.0 and random.random() < eps:
                probe = True
        if probe:
            return Action.C
        threshold = 1.0 / float(self.k) if self.k != 0 else float('inf')
        if P_hat > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_114(BaseStrategy):
    """
    Aggressive Opportunist

    - Defaults to defect (Action.D) except when cooperating increases immediate expected payoff
      or when you're very likely to be pivotal.
    - Uses a short rolling window with Laplace smoothing to estimate opponents' cooperative tendencies.
    - Probes rarely (epsilon) in non-final rounds to learn; never probes in final round.
    - Parameters (tunable): W_max=10, alpha=1, margin=0.01, epsilon=0.03, P_pivot=0.90.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_max = 10
        self.alpha = 1.0
        self.margin = 0.01
        self.epsilon = 0.03
        self.P_pivot = 0.9

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        t = state.round_number
        final_round = t == r
        if history is None or history.round_number == 0:
            epsilon_effective = 0.0 if final_round else self.epsilon
            if random.random() < epsilon_effective:
                return Action.C
            return Action.D
        rounds_played = history.round_number
        W = min(self.W_max, rounds_played)
        if W <= 0:
            epsilon_effective = 0.0 if final_round else self.epsilon
            if random.random() < epsilon_effective:
                return Action.C
            return Action.D
        opp_actions = history.opponent_actions
        use_actions = opp_actions[-W:, :]
        Trials = use_actions.shape[0]
        if Trials == 0:
            epsilon_effective = 0.0 if final_round else self.epsilon
            if random.random() < epsilon_effective:
                return Action.C
            return Action.D
        others_counts = np.sum(use_actions, axis=1)
        S_m = int(np.sum(others_counts >= m))
        S_m1 = int(np.sum(others_counts >= m - 1))
        S_eq = int(np.sum(others_counts == m - 1))
        denom = Trials + 2.0 * self.alpha
        f_m = (S_m + self.alpha) / denom
        f_m1 = (S_m1 + self.alpha) / denom
        prob_pivotal = (S_eq + self.alpha) / denom
        E_defect = 1.0 + k * f_m
        E_cooperate = k * f_m1
        epsilon_effective = 0.0 if final_round else self.epsilon
        if prob_pivotal >= self.P_pivot:
            return Action.C
        if E_cooperate > E_defect + self.margin:
            return Action.C
        if random.random() < epsilon_effective:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_115(BaseStrategy):
    """
    Aggressive pivotal contributor for the Collective Risk Dilemma.

    - Maintains an EMA belief p_j for each opponent's probability to cooperate.
    - Computes the Poisson-binomial probability that exactly (m-1) others cooperate (P_piv).
    - Cooperates iff P_piv > 1/k (ties -> defect). With tiny prob epsilon, flips action to
      break symmetry.
    - Beliefs updated each round via EMA: p_j <- (1-alpha) * p_j + alpha * 1{j cooperated}.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prior = 0.1
        self.alpha = 0.3
        self.epsilon = 0.01
        self.clamp_min = 1e-06
        self.clamp_max = 1.0 - 1e-06
        self.n = int(game_description.n_players)
        self.n_opp = max(0, self.n - 1)
        if self.n_opp > 0:
            self.p_vec = np.full(self.n_opp, float(self.prior), dtype=float)
        else:
            self.p_vec = np.array([], dtype=float)
        self.last_updated_round = 0

    def _clamp(self, x: float) -> float:
        if x < self.clamp_min:
            return self.clamp_min
        if x > self.clamp_max:
            return self.clamp_max
        return x

    def _update_beliefs_from_history(self, history: PlayerHistory):
        rounds_observed = history.round_number
        if self.n_opp == 0:
            self.last_updated_round = rounds_observed
            return
        for r_idx in range(self.last_updated_round, rounds_observed):
            row = history.opponent_actions[r_idx, :]
            row_float = np.array(row, dtype=float)
            self.p_vec = (1.0 - self.alpha) * self.p_vec + self.alpha * row_float
            self.p_vec = np.clip(self.p_vec, self.clamp_min, self.clamp_max)
        self.last_updated_round = rounds_observed

    def _poisson_binomial_pmf_exact(self, p_array: np.ndarray, target: int) -> float:
        n = p_array.size
        if target < 0 or target > n:
            return 0.0
        prob = np.zeros(n + 1, dtype=float)
        prob[0] = 1.0
        for p in p_array:
            for s in range(n - 1, -1, -1):
                prob[s + 1] += prob[s] * p
                prob[s] *= 1.0 - p
        return float(prob[target])

    def _poisson_binomial_pmf_normal_approx(self, p_array: np.ndarray, target: int) -> float:
        mu = float(np.sum(p_array))
        var = float(np.sum(p_array * (1.0 - p_array)))
        if var <= 1e-10:
            rounded = int(round(mu))
            return 1.0 if rounded == target else 0.0
        sigma = math.sqrt(var)
        low = (target - 0.5 - mu) / sigma
        high = (target + 0.5 - mu) / sigma

        def std_norm_cdf(x):
            return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))
        return max(0.0, min(1.0, std_norm_cdf(high) - std_norm_cdf(low)))

    def _compute_P_piv(self) -> float:
        m = int(self.game_description.m)
        target = m - 1
        if self.n_opp == 0:
            return 1.0 if target == 0 else 0.0
        if self.n_opp <= 200:
            return self._poisson_binomial_pmf_exact(self.p_vec, target)
        else:
            return self._poisson_binomial_pmf_normal_approx(self.p_vec, target)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is not None:
            self._update_beliefs_from_history(history)
        P_piv = self._compute_P_piv()
        k = float(self.game_description.k)
        threshold = 1.0 / k if k != 0.0 else float('inf')
        intended_cooperate = True if P_piv > threshold else False
        if random.random() < self.epsilon:
            intended_cooperate = not intended_cooperate
        return Action.C if intended_cooperate else Action.D

class Strategy_AGGRESSIVE_116(BaseStrategy):
    """
    Aggressive, adaptive Collective Risk strategy.

    - Default is to defect.
    - Maintain an EMA estimate p_j for each opponent's cooperation probability.
    - Compute exact probability P_eq that exactly (m-1) of the other players cooperate
      (using convolution). Cooperate only if k * P_eq > 1 (strict), except for small
      decaying exploration probability.
    - If the probability the others already reach the threshold (P_atleast) is very high,
      defect to free-ride.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game = game_description
        self.n = int(self.game.n_players)
        self.r = int(self.game.n_rounds)
        self.m = int(self.game.m)
        self.k = float(self.game.k)
        self.n_opp = max(0, self.n - 1)
        self.alpha = 0.35
        self.p0 = 0.1
        self.p = np.full(self.n_opp, self.p0, dtype=float)
        self.eps0 = min(0.05, 3.0 / max(1, self.r))
        self.explore_decay_rounds = 3
        self.free_ride_threshold = 0.9
        self.margin = 0.0
        self.rounds_seen = 0

    def _epsilon_for_round(self, t: int) -> float:
        if t <= 0:
            return 0.0
        if t > self.explore_decay_rounds:
            return 0.0
        factor = (self.explore_decay_rounds - (t - 1)) / float(self.explore_decay_rounds)
        return self.eps0 * max(0.0, factor)

    def _compute_prob_distribution(self, p_vec: np.ndarray) -> np.ndarray:
        """
        Compute distribution prob[k] = Prob(exactly k cooperators among opponents)
        using convolution. Returns array of length (n_opp + 1).
        """
        if p_vec.size == 0:
            prob = np.zeros(1, dtype=float)
            prob[0] = 1.0
            return prob
        prob = np.array([1.0], dtype=float)
        for pj in p_vec:
            prob = np.convolve(prob, np.array([1.0 - pj, pj], dtype=float))
        return prob

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.m <= 0 or self.m > self.n:
            eps = self._epsilon_for_round(t)
            if random.random() < eps:
                return Action.C
            return Action.D
        if history is None:
            eps = self._epsilon_for_round(t)
            if random.random() < eps:
                return Action.C
            return Action.D
        self.rounds_seen = history.round_number
        if self.n_opp > 0 and history.opponent_actions.size != 0:
            last_actions = history.opponent_actions[-1, :]
            if last_actions.shape[0] != self.n_opp:
                min_len = min(last_actions.shape[0], self.n_opp)
                vals = np.zeros(self.n_opp, dtype=float)
                vals[:min_len] = last_actions[:min_len].astype(float)
                last_vals = vals
            else:
                last_vals = last_actions.astype(float)
            self.p = (1.0 - self.alpha) * self.p + self.alpha * last_vals
        self.p = np.clip(self.p, 0.0, 1.0)
        prob = self._compute_prob_distribution(self.p)
        target = self.m - 1
        if 0 <= target <= self.n_opp:
            P_eq = float(prob[target])
        else:
            P_eq = 0.0
        if self.m <= 0:
            P_atleast = 1.0
        elif self.m > self.n_opp:
            P_atleast = 0.0
        else:
            P_atleast = float(np.sum(prob[self.m:])) if self.m <= self.n_opp else 0.0
        eps = self._epsilon_for_round(t)
        if random.random() < eps:
            return Action.C
        if P_atleast > self.free_ride_threshold:
            return Action.D
        if self.k * P_eq > 1.0 + self.margin:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_117(BaseStrategy):
    """
    Aggressive Opportunistic Pivotal (AOP)

    Default: defect. Estimate opponents' cooperation probabilities with Laplace
    smoothing. Compute probability that exactly m-1 of the other players will
    cooperate (Pr_exact) using a binomial approximation with p_bar (mean of
    per-opponent estimates). Cooperate only if k * Pr_exact > 1 + margin (strict),
    otherwise defect. If you cooperate and the threshold fails (you were exploited)
    repeatedly (>= E_tol), switch to permanent defection (punishment_mode).

    Optional early mobilization is available but disabled by default.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.smoothing_prior = 1.0
        self.margin = 0.0
        self.E_tol = 1
        self.eps = 0.02
        self.use_mobilize = False
        self.mobilize_prob = 0.5
        self.exploited_count = 0
        self.punishment_mode = False
        self.processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        others = n - 1
        t = getattr(state, 'round_number', None)
        if t is None:
            t = history.round_number + 1 if history is not None else 1
        if history is not None:
            completed_rounds = history.round_number
            for idx in range(self.processed_rounds, completed_rounds):
                my_act = bool(history.my_actions[idx])
                opp_row = history.opponent_actions[idx]
                coop_opponents = int(np.sum(opp_row))
                total_cooperators = coop_opponents + (1 if my_act else 0)
                if my_act and total_cooperators < m:
                    self.exploited_count += 1
                    if self.exploited_count >= self.E_tol:
                        self.punishment_mode = True
            self.processed_rounds = completed_rounds
        if history is None or history.round_number == 0:
            p_bar = 0.5
            p_js = np.full(others, 0.5) if others > 0 else np.array([])
            rounds_seen = 0
        else:
            rounds_seen = history.round_number
            coop_counts = np.sum(history.opponent_actions.astype(np.int64), axis=0)
            denom = float(rounds_seen + 2)
            p_js = (coop_counts + 1.0) / denom
            if p_js.size == 0:
                p_bar = 0.0
            else:
                p_bar = float(np.mean(p_js))

        def binom_pmf(k_succ: int, trials: int, p: float) -> float:
            if k_succ < 0 or k_succ > trials:
                return 0.0
            try:
                comb = math.comb(trials, k_succ)
            except Exception:
                comb = 1
                kk = k_succ
                rr = trials
                for a in range(kk):
                    comb *= rr - a
                for a in range(1, kk + 1):
                    comb //= a
            return comb * p ** k_succ * (1.0 - p) ** (trials - k_succ)
        target_k = m - 1
        if others <= 0:
            Pr_exact = 0.0
        else:
            Pr_exact = binom_pmf(target_k, others, p_bar)
        Pr_ge = 0.0
        if others > 0:
            for kk in range(m, others + 1):
                Pr_ge += binom_pmf(kk, others, p_bar)
        if self.punishment_mode:
            return Action.D
        if t >= r:
            margin_current = max(self.margin, 0.0)
        else:
            margin_current = self.margin
        cooperate_by_pivot = k * Pr_exact > 1.0 + margin_current
        action = Action.C if cooperate_by_pivot else Action.D
        if not cooperate_by_pivot and (not self.punishment_mode) and self.use_mobilize:
            if t <= max(1, math.floor(r / 3)):
                threshold_fraction = m / float(n) - self.eps
                if p_bar >= threshold_fraction:
                    if random.random() < self.mobilize_prob:
                        action = Action.C
        return action

class Strategy_AGGRESSIVE_118(BaseStrategy):
    """
    Aggressive Pivotal Opportunist (APO)

    - Default: DEFECT.
    - Keeps per-opponent smoothed reliabilities and finite punishments derived from history.
    - Cooperates only when (a) others likely reach the threshold without me (free-ride -> D),
      (b) my cooperation is likely to be pivotal (I make the difference -> C),
      (c) a small coalition of reliable, unpunished opponents can be formed with me -> C.
    - Final round: only cooperate if my cooperation almost certainly secures the reward.
    - First round: defect (epsilon_probe = 0).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = min(20, max(1, game_description.n_rounds))
        self.alpha = 1.0 / float(self.W)
        self.P_free = 0.6
        self.P_piv = 0.5
        self.q_reliable = 0.8
        self.P_punish = min(3, math.floor(game_description.n_rounds / 5)) if game_description.n_rounds > 0 else 1
        self.epsilon_probe = 0.0
        self.prior = 0.5
        self._last_history_rounds_seen = 0

    def _normal_cdf(self, x: float) -> float:
        return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))

    def _prob_at_least(self, s: int, qs: list[float]) -> float:
        """
        Approximate Prob(number of successes >= s) for Poisson-Binomial with means qs
        using normal approximation with continuity correction.
        qs: list of probabilities for each Bernoulli.
        """
        if s <= 0:
            return 1.0
        n = len(qs)
        if s > n:
            return 0.0
        mu = float(sum(qs))
        var = float(sum([q * (1.0 - q) for q in qs]))
        if var <= 1e-08:
            return 1.0 if mu + 1e-09 >= s else 0.0
        sigma = math.sqrt(var)
        z = (s - 0.5 - mu) / sigma
        return max(0.0, min(1.0, 1.0 - self._normal_cdf(z)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if n < 2 or not 1 < m < n:
            return Action.D
        if history is None or history.round_number == 0:
            if self.epsilon_probe > 0.0 and random.random() < self.epsilon_probe:
                return Action.C
            return Action.D
        rounds_completed = history.round_number
        opponents = n - 1
        reliabilities: list[float] = []
        recent_window = min(self.W, rounds_completed)
        alpha = self.alpha
        for j in range(opponents):
            if recent_window <= 0:
                recent_freq = 0.0
            else:
                recent_slice = history.opponent_actions[-recent_window:, j] if recent_window > 0 else np.array([], dtype=bool)
                recent_freq = float(np.sum(recent_slice)) / float(recent_window) if recent_window > 0 else 0.0
            qj = alpha * recent_freq + (1.0 - alpha) * self.prior
            qj = max(0.0, min(1.0, qj))
            reliabilities.append(qj)
        punish_remaining: list[int] = [0] * opponents
        last_fail_round: list[int | None] = [None] * opponents
        for t in range(rounds_completed):
            my_act = bool(history.my_actions[t])
            opp_row = history.opponent_actions[t, :]
            coop_count_t = int(my_act) + int(np.sum(opp_row))
            if coop_count_t == m - 1:
                for j in range(opponents):
                    if not bool(opp_row[j]):
                        last_fail_round[j] = t
        for j in range(opponents):
            t = last_fail_round[j]
            if t is not None:
                remaining = max(0, self.P_punish - (rounds_completed - 1 - t))
                punish_remaining[j] = remaining
        q_adj: list[float] = []
        for j in range(opponents):
            if punish_remaining[j] > 0:
                qjprime = min(reliabilities[j], 0.1)
            else:
                qjprime = reliabilities[j]
            q_adj.append(qjprime)
        if state.round_number == r - 1:
            prob_with_me = self._prob_at_least(m - 1, q_adj)
            if prob_with_me >= 0.9:
                return Action.C
            return Action.D
        P_without = self._prob_at_least(m, q_adj)
        if P_without >= self.P_free:
            return Action.D
        P_pivotal = self._prob_at_least(m - 1, q_adj)
        if P_pivotal >= self.P_piv:
            return Action.C
        sorted_idx = sorted(range(opponents), key=lambda jj: q_adj[jj], reverse=True)
        cum = 0.0
        S = []
        cooperators_needed = m - 1
        formed_coalition = False
        for idx in sorted_idx:
            S.append(idx)
            cum += q_adj[idx]
            if cum >= cooperators_needed:
                all_reliable = all((reliabilities[j] >= self.q_reliable and punish_remaining[j] == 0 for j in S))
                if all_reliable:
                    formed_coalition = True
                break
        if formed_coalition:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_119(BaseStrategy):
    """
    Pivotal Exploiter with Reluctant Provision (PERP)

    - Never cooperate in first or last round.
    - If others already reached the threshold last round, defect (free-ride).
    - If exactly m-1 others cooperated last round, cooperate only if those exact players
      cooperated in every one of the previous T = min(3, t-1) rounds.
    - If fewer than m-1 cooperated last round, normally defect, but on deterministic probe
      rounds (t % probe_interval == 0 with probe_interval = max(3, floor(r/5))) cooperate
      if at least m-1 other players cooperated at least once in the recent T rounds.
    - Deterministic and uses only observable history and public parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            t = 1
        else:
            t = history.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if t == 1:
            return Action.D
        if t == r:
            return Action.D
        if history is None:
            return Action.D
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            L_prev = 0
        else:
            L_prev = int(np.sum(opp_actions[-1, :]))
        T = min(3, t - 1)
        if L_prev >= m:
            return Action.D
        if L_prev == m - 1:
            last_row = opp_actions[-1, :]
            S_needed_indices = [j for j, val in enumerate(last_row) if val]
            if T <= 0:
                return Action.D
            recent = opp_actions[-T:, :]
            reliable = True
            for j in S_needed_indices:
                if not bool(np.all(recent[:, j])):
                    reliable = False
                    break
            if reliable:
                return Action.C
            else:
                return Action.D
        probe_interval = max(3, math.floor(r / 5))
        if probe_interval > 0 and t % probe_interval == 0:
            if T <= 0:
                return Action.D
            recent = opp_actions[-T:, :]
            counts = np.sum(recent, axis=0)
            count_signal = int(np.sum(counts >= 1))
            if count_signal >= m - 1:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_120(BaseStrategy):
    """
    Predatory Conditional Defector (PCD)

    Aggressive, exploitation-focused strategy for the Collective Risk Dilemma.
    Defaults to defecting. Cooperates only when empirical evidence indicates a
    strict expected benefit now (EV_cooperate >= EV_defect) or when a small
    sacrifice reliably tips a threshold thanks to a detectable set of
    exploitable or unconditional cooperators. Never cooperates in the final
    round(s). Uses a short sliding window of history to estimate conditional
    cooperation patterns of opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.L = min(10, max(0, self.r - 1))
        self.exploit_delta = 0.25
        self.p_confident = 0.75
        self.tip_threshold = 1.0 / max(1e-09, self.k)
        self.last_safe_rounds = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = state.round_number
        if t >= self.r - self.last_safe_rounds + 1:
            return Action.D
        total_rounds_played = history.round_number
        w = min(self.L, total_rounds_played)
        if w <= 0:
            return Action.D
        start_idx = total_rounds_played - w
        my_actions_window = np.asarray(history.my_actions[start_idx:total_rounds_played], dtype=bool)
        opp_actions_window = np.asarray(history.opponent_actions[start_idx:total_rounds_played, :], dtype=bool)
        n_opponents = opp_actions_window.shape[1]
        coop_counts = np.sum(opp_actions_window, axis=1) + my_actions_window.astype(int)
        threshold_met = coop_counts >= self.m
        mask_def = ~my_actions_window
        denom_def = np.sum(mask_def)
        if denom_def > 0:
            p_def = float(np.sum(threshold_met & mask_def) / denom_def)
        else:
            p_def = 0.0
        mask_coop = my_actions_window
        denom_coop = np.sum(mask_coop)
        if denom_coop > 0:
            p_coop = float(np.sum(threshold_met & mask_coop) / denom_coop)
        else:
            p_coop = 0.0
        if p_def >= self.p_confident:
            return Action.D
        if p_def < 1.0 / max(1e-09, self.k) and p_coop < 1.0 / max(1e-09, self.k):
            return Action.D
        EV_defect = 1.0 + p_def * self.k
        EV_cooperate = p_coop * self.k
        exploitable_count = 0
        unconditional_coop_count = 0
        prev_actions = []
        for abs_idx in range(start_idx, total_rounds_played):
            if abs_idx >= 1:
                prev_actions.append(bool(history.my_actions[abs_idx - 1]))
            else:
                prev_actions.append(None)
        for j in range(n_opponents):
            coop_after_C = []
            coop_after_D = []
            for idx_in_window, abs_idx in enumerate(range(start_idx, total_rounds_played)):
                prev = prev_actions[idx_in_window]
                if prev is None:
                    continue
                acted = bool(history.opponent_actions[abs_idx, j])
                if prev:
                    coop_after_C.append(1 if acted else 0)
                else:
                    coop_after_D.append(1 if acted else 0)
            Pj_C = float(np.mean(coop_after_C)) if len(coop_after_C) > 0 else 0.0
            Pj_D = float(np.mean(coop_after_D)) if len(coop_after_D) > 0 else 0.0
            overall_coop = float(np.mean(opp_actions_window[:, j])) if w > 0 else 0.0
            if Pj_C - Pj_D >= self.exploit_delta and Pj_C >= 0.6:
                exploitable_count += 1
            if overall_coop >= 0.9:
                unconditional_coop_count += 1
        supportive_exploiters = exploitable_count + unconditional_coop_count
        if EV_cooperate >= EV_defect:
            return Action.C
        delta_p = p_coop - p_def
        needed_supporters = max(0, self.m - 1)
        if delta_p >= self.tip_threshold and supportive_exploiters >= needed_supporters:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_121(BaseStrategy):
    """
    Aggressive Pivotalist with Retaliatory Calibration (APRC)

    - Maintains EMA estimates P_j of each opponent's cooperation probability.
    - Computes exact Poisson-Binomial distribution for the number of other cooperators.
    - Chooses the action (C or D) that maximizes expected payoff, breaking ties in favor of D.
    - Detects 'culprits' who single-handedly turned a would-be success into a failure,
      punishes them temporarily by assuming very low cooperation probability, and forgives
      after they cooperate or after a bounded timer expires.
    - Defaults to Defect on the very first decision (history is None).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.n_opp = max(0, n - 1)
        self.alpha = 0.4
        self.eps = 0.02
        self.T_base = 3
        self.delta_frac = 0.1
        self.P = np.full(self.n_opp, 0.5, dtype=float)
        self.culprits = {}
        self.processed_rounds = 0

    def _poisson_binomial_probs(self, ps):
        """
        Compute exact distribution of sum of independent Bernoulli(ps).
        Returns numpy array probs where probs[s] = Pr(sum == s).
        """
        probs = np.array([1.0], dtype=float)
        for p in ps:
            new = np.zeros(len(probs) + 1, dtype=float)
            new[:-1] += probs * (1.0 - p)
            new[1:] += probs * p
            probs = new
        return probs

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed > 0:
            obs_n_opp = history.opponent_actions.shape[1]
            if obs_n_opp != self.n_opp:
                self.n_opp = obs_n_opp
                oldP = self.P
                self.P = np.full(self.n_opp, 0.5, dtype=float)
                up_to = min(len(oldP), self.n_opp)
                if up_to > 0:
                    self.P[:up_to] = oldP[:up_to]
        for t in range(self.processed_rounds, rounds_completed):
            opp_actions_t = history.opponent_actions[t, :] if self.n_opp > 0 else np.array([], dtype=bool)
            my_action_t = bool(history.my_actions[t])
            if self.n_opp > 0:
                cooperated = opp_actions_t.astype(float)
                self.P = (1.0 - self.alpha) * self.P + self.alpha * cooperated
                self.P = np.clip(self.P, self.eps, 1.0 - self.eps)
            coop_count = int(np.sum(opp_actions_t)) + (1 if my_action_t else 0)
            remaining_after_t = max(0, r - (t + 1))
            if coop_count < m:
                for j in range(self.n_opp):
                    if not opp_actions_t[j]:
                        if coop_count + 1 >= m:
                            T = min(self.T_base, remaining_after_t)
                            if T > 0:
                                self.culprits[j] = T
                            else:
                                self.culprits.pop(j, None)
            to_remove = []
            for j in list(self.culprits.keys()):
                if t < rounds_completed and self.n_opp > 0 and history.opponent_actions[t, j]:
                    to_remove.append(j)
                else:
                    self.culprits[j] -= 1
                    if self.culprits[j] <= 0:
                        to_remove.append(j)
            for j in to_remove:
                self.culprits.pop(j, None)
        self.processed_rounds = rounds_completed
        remaining_rounds = max(0, r - rounds_completed)
        if remaining_rounds <= 0:
            return Action.D
        apply_punishment_override = remaining_rounds > 1
        if self.n_opp > 0:
            probs_no_override = self._poisson_binomial_probs(self.P)
        else:
            probs_no_override = np.array([1.0], dtype=float)
        prob_success_without_me = float(np.sum(probs_no_override[m:])) if m <= len(probs_no_override) - 1 else 0.0
        if prob_success_without_me >= 0.999:
            return Action.D
        prob_pivotal = float(probs_no_override[m - 1]) if 0 <= m - 1 < len(probs_no_override) else 0.0
        if prob_pivotal >= 0.999:
            return Action.C
        p_calc = self.P.copy()
        if apply_punishment_override and len(self.culprits) > 0:
            for j in self.culprits.keys():
                if 0 <= j < len(p_calc):
                    p_calc[j] = 0.05
            p_calc = np.clip(p_calc, self.eps, 1.0 - self.eps)
        probs = self._poisson_binomial_probs(p_calc) if self.n_opp > 0 else np.array([1.0], dtype=float)
        s_vals = np.arange(len(probs))
        payoff_C = np.where(s_vals + 1 >= m, k, 0.0)
        payoff_D = np.where(s_vals >= m, 1.0 + k, 1.0)
        E_C = float(np.sum(probs * payoff_C))
        E_D = float(np.sum(probs * payoff_D))
        delta = self.delta_frac * (1.0 + k)
        if apply_punishment_override and len(self.culprits) > 1 and (abs(E_C - E_D) <= delta):
            return Action.D
        if E_C > E_D:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_122(BaseStrategy):
    """
    Opportunistic pivotal strategy for the Collective Risk Dilemma.

    Cooperate only when your single action is likely to be pivotal (i.e., when
    the probability that exactly m-1 other players cooperate exceeds 1/k).
    Estimates per-opponent cooperation probabilities with a Beta(1,1) (Laplace)
    prior from observed history and computes the Poisson–binomial exactly via
    dynamic programming. Tie-breaks toward defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.n < 2 or not 1 < self.m < self.n or self.k <= 1.0:
            return Action.D
        opponents = self.n - 1
        t = 0 if history is None else int(history.round_number)
        if history is None or t == 0:
            coop_counts = np.zeros(opponents, dtype=int)
        else:
            arr = history.opponent_actions.astype(int)
            coop_counts = np.sum(arr, axis=0).astype(int)
            if coop_counts.shape[0] != opponents:
                if coop_counts.shape[0] > opponents:
                    coop_counts = coop_counts[:opponents]
                else:
                    padded = np.zeros(opponents, dtype=int)
                    padded[:coop_counts.shape[0]] = coop_counts
                    coop_counts = padded
        denom = 2 + t
        p_j = (1.0 + coop_counts.astype(float)) / float(denom)
        target = self.m - 1
        if target < 0 or target > opponents:
            return Action.D
        prob = np.zeros(opponents + 1, dtype=float)
        prob[0] = 1.0
        for pj in p_j:
            pj = float(pj)
            one_minus = 1.0 - pj
            for s in range(opponents - 1, -1, -1):
                add = prob[s] * pj
                prob[s + 1] += add
                prob[s] *= one_minus
        P_piv = float(prob[target])
        eps = 1e-12
        threshold = 1.0 / float(self.k)
        if P_piv - threshold > eps:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_123(BaseStrategy):
    """
    Pivotal Aggressor

    Default: defect. Cooperate only when the empirical, recency-weighted probability that
    the other players provide exactly m-1 cooperators (so your single cooperation is pivotal)
    exceeds the rational cutoff 1/k (plus a small margin). Otherwise defect, but perform
    rare, decaying probes (random cooperations) to discover or create pivotal opportunities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.2
        self.margin = 0.02
        self.probe_margin = 0.02
        self.epsilon_probe_base = max(0.02, 0.5 / max(1, self.r))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        pivot_threshold = 1.0 / max(1e-12, self.k)
        if history is None:
            if random.random() < self.epsilon_probe_base:
                return Action.C
            return Action.D
        num_past = int(history.round_number)
        if num_past <= 0:
            if random.random() < self.epsilon_probe_base:
                return Action.C
            return Action.D
        past_actions = np.array(history.opponent_actions)
        if past_actions.ndim == 1:
            past_actions = past_actions.reshape(1, -1)
        cooperators_per_round = np.sum(past_actions.astype(float), axis=1)
        target_count = max(0, self.m - 1)
        indicators = (cooperators_per_round == target_count).astype(float)
        indices = np.arange(num_past)
        exponents = (num_past - 1 - indices).astype(float)
        weights = (self.alpha * (1.0 - self.alpha) ** exponents).astype(float)
        weight_sum = float(np.sum(weights))
        if weight_sum <= 0:
            weights = np.ones(num_past, dtype=float)
            weight_sum = float(num_past)
        P_piv_hat = float(np.dot(weights, indicators) / weight_sum)
        if P_piv_hat > pivot_threshold + self.margin:
            return Action.C
        decay_factor = 1.0 - float(max(0, t - 1)) / max(1.0, float(self.r))
        epsilon = float(self.epsilon_probe_base) * max(0.0, decay_factor)
        if num_past < 6:
            epsilon = min(0.5, epsilon * 3.0)
        if abs(P_piv_hat - pivot_threshold) < self.probe_margin:
            epsilon = min(0.5, epsilon * 5.0)
        if random.random() < epsilon:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_124(BaseStrategy):
    """
    Aggressive, payoff-maximizing Collective Risk strategy.

    - Probes on first round (plays C).
    - Maintains per-opponent short-window cooperation estimates (L = min(5, r)) with a prior.
    - Computes exact distribution of other cooperators via DP and cooperates only when
      k * P_m1 > 1 + epsilon_coop (pivotal benefit outweighs private endowment).
    - Punishes identified betrayers (defected when pivotal) by defecting for T_punish rounds.
      Punishment timers decay each round; punished players cooperating two consecutive rounds
      after punishment start are forgiven early.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.L = min(5, self.r)
        self.p0 = 0.5
        self.prior_weight = 1.0
        self.epsilon_coop = 0.01
        self.T_punish = 3
        self.punish_timers = [0] * max(0, self.n - 1)
        self.coop_streaks = [0] * max(0, self.n - 1)
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            if len(self.punish_timers) != max(0, self.n - 1):
                self.punish_timers = [0] * max(0, self.n - 1)
                self.coop_streaks = [0] * max(0, self.n - 1)
            return Action.C
        num_opponents = max(0, self.n - 1)
        if len(self.punish_timers) != num_opponents:
            self.punish_timers = [0] * num_opponents
            self.coop_streaks = [0] * num_opponents
        completed = history.round_number
        for round_idx in range(self.last_processed_round, completed):
            for j in range(num_opponents):
                if self.punish_timers[j] > 0:
                    self.punish_timers[j] = max(0, self.punish_timers[j] - 1)
            my_act = bool(history.my_actions[round_idx]) if round_idx < len(history.my_actions) else False
            opp_row = history.opponent_actions[round_idx, :] if round_idx < history.opponent_actions.shape[0] else np.array([], dtype=bool)
            if opp_row.shape[0] != num_opponents:
                opp_row = np.array([False] * num_opponents, dtype=bool)
            for j in range(num_opponents):
                acted_coop = bool(opp_row[j])
                if self.punish_timers[j] > 0:
                    if acted_coop:
                        self.coop_streaks[j] += 1
                        if self.coop_streaks[j] >= 2:
                            self.punish_timers[j] = 0
                            self.coop_streaks[j] = 0
                    else:
                        self.coop_streaks[j] = 0
                else:
                    self.coop_streaks[j] = 0
            if self.k > 1:
                total_opp_coops = int(np.sum(opp_row))
                for j in range(num_opponents):
                    acted_coop = bool(opp_row[j])
                    if acted_coop:
                        continue
                    coops_excl_j = int(my_act) + (total_opp_coops - (1 if acted_coop else 0))
                    if coops_excl_j == self.m - 1:
                        self.punish_timers[j] = max(self.punish_timers[j], self.T_punish)
                        self.coop_streaks[j] = 0
            self.last_processed_round += 1
        current_round = int(state.round_number)
        if any((t > 0 for t in self.punish_timers)):
            return Action.D
        if current_round == 1:
            return Action.C
        p_js = []
        L_eff = min(self.L, history.round_number)
        for j in range(num_opponents):
            if L_eff <= 0:
                num_coop = 0
            else:
                opp_col = history.opponent_actions[max(0, history.round_number - L_eff):history.round_number, j]
                num_coop = int(np.sum(opp_col)) if opp_col.size > 0 else 0
            p_j = (num_coop + self.prior_weight * self.p0) / (L_eff + self.prior_weight)
            if p_j < 0.0:
                p_j = 0.0
            elif p_j > 1.0:
                p_j = 1.0
            p_js.append(float(p_j))
        max_others = num_opponents
        dp = [0.0] * (max_others + 1)
        dp[0] = 1.0
        for p in p_js:
            new_dp = [0.0] * (max_others + 1)
            for t in range(0, max_others + 1):
                if dp[t] == 0.0:
                    continue
                new_dp[t] += dp[t] * (1.0 - p)
                if t + 1 <= max_others:
                    new_dp[t + 1] += dp[t] * p
            dp = new_dp
        P_m1 = 0.0
        if 0 <= self.m - 1 <= max_others:
            P_m1 = dp[self.m - 1]
        P_succ_without_me = 0.0
        if self.m <= max_others:
            P_succ_without_me = sum((dp[t] for t in range(self.m, max_others + 1)))
        if self.k * P_m1 > 1.0 + self.epsilon_coop:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_125(BaseStrategy):
    """
    Aggressive Pivotal Defector (APD)

    - Default: defect.
    - Cooperate only when your single contribution is expected to be pivotal (others ≈ m-1)
      and recent history does not indicate your cooperations were wasted.
    - Rare early probes when cooperation seems unlikely but reward k is large.
    - If repeatedly exploited (you cooperated but threshold repeatedly failed), withdraw cooperation
      temporarily and require confirmation (two pivotal predictions or p_avg increase) before resuming.
    - Final round: no probes; cooperate only if actually pivotal and not recently exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.prev_p_avg = None
        self.exploit_flag = False
        self.exploit_block_until = 0
        self.pivotal_consec = 0
        self.no_coop_since_exploit = 0
        self.p0 = 0.1
        self.confirm_delta = 0.05
        self.confirm_required_consec = 2
        self.no_coop_timeout = 3
        self.probe_early_discount = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            return Action.D
        T = min(5, t - 1)
        start_idx = t - 1 - T
        opp_slice = history.opponent_actions[start_idx:start_idx + T, :]
        if opp_slice.size == 0:
            p_avg = float(self.p0)
        else:
            p_avg = float(np.mean(opp_slice))
        prev_p = None if self.prev_p_avg is None else float(self.prev_p_avg)
        expected_others = p_avg * (self.n - 1)
        L = min(3, t - 1)
        my_recent = history.my_actions[-L:]
        opp_recent = history.opponent_actions[-L:, :]
        count_my_coop_in_L = int(np.sum(my_recent))
        wasted_coop_count = 0
        if count_my_coop_in_L > 0:
            for idx in range(L):
                if my_recent[idx]:
                    total_coop = int(np.sum(opp_recent[idx, :])) + 1
                    if total_coop < self.m:
                        wasted_coop_count += 1
        exploited_recent = False
        if count_my_coop_in_L > 0 and wasted_coop_count > count_my_coop_in_L / 2:
            exploited_recent = True
        if exploited_recent:
            self.exploit_flag = True
            self.exploit_block_until = t
            self.pivotal_consec = 0
            self.no_coop_since_exploit = 0
        is_final_round = t == self.r
        action = Action.D
        if expected_others >= self.m:
            action = Action.D
        else:
            predicted_pivotal = round(expected_others) == self.m - 1
            if self.exploit_flag and t <= self.exploit_block_until:
                action = Action.D
                self.no_coop_since_exploit += 1
            elif is_final_round:
                if predicted_pivotal and (not exploited_recent) and (not self.exploit_flag):
                    action = Action.C
                else:
                    action = Action.D
            elif predicted_pivotal:
                if self.exploit_flag:
                    increased = False
                    if prev_p is not None and p_avg - prev_p >= self.confirm_delta:
                        increased = True
                    self.pivotal_consec = self.pivotal_consec + 1
                    if self.pivotal_consec >= self.confirm_required_consec or increased:
                        self.exploit_flag = False
                        self.pivotal_consec = 0
                        self.no_coop_since_exploit = 0
                        action = Action.C
                    else:
                        action = Action.D
                        self.no_coop_since_exploit += 1
                        if self.no_coop_since_exploit >= self.no_coop_timeout:
                            self.exploit_flag = False
                            self.pivotal_consec = 0
                else:
                    action = Action.C
            elif t >= self.r - 1:
                action = Action.D
            else:
                base_factor = min(1 + (self.k - 1), 5.0)
                p_probe = 0.02 * base_factor
                p_probe = max(0.01, min(p_probe, 0.2))
                p_probe = p_probe * (1.0 - float(t - 1) / float(self.r))
                if self.exploit_flag and self.no_coop_since_exploit < self.no_coop_timeout:
                    action = Action.D
                    self.no_coop_since_exploit += 1
                    if self.no_coop_since_exploit >= self.no_coop_timeout:
                        self.exploit_flag = False
                        self.pivotal_consec = 0
                elif random.random() < p_probe:
                    action = Action.C
                    if self.exploit_flag:
                        self.no_coop_since_exploit = 0
                else:
                    action = Action.D
                    if self.exploit_flag:
                        self.no_coop_since_exploit += 1
                        if self.no_coop_since_exploit >= self.no_coop_timeout:
                            self.exploit_flag = False
                            self.pivotal_consec = 0
        self.prev_p_avg = p_avg
        return action

class Strategy_AGGRESSIVE_126(BaseStrategy):
    """
    Aggressive, expectation-maximizing strategy for the Collective Risk Dilemma.

    - Uses per-opponent empirical cooperation rates with a Jeffreys prior (alpha=beta=0.5 by default).
    - Computes the Poisson-Binomial distribution over how many OTHER players will cooperate this round.
    - Chooses the immediate-action with higher expected immediate payoff, breaking ties in favor of defection.
    - Has explicit exploitation and pivotal overrides:
        * If others are almost certain to meet the threshold without you (P(K>=m) >= T_exploit) -> defect (free-ride).
        * If others are likely to leave you pivotal (P(K==m-1) >= T_pivot) -> cooperate.
    - Small-probing: with tiny probability epsilon, invert the chosen action when expected-payoff difference is very small.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.beta = 0.5
        self.T_exploit = 0.95
        self.T_pivot = 0.5
        self.epsilon = 0.01
        self.delta = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        reward = self.game_description.k
        if history is None or history.round_number == 0:
            N_obs = 0
            p_opp = np.full((n - 1,), self.alpha / (self.alpha + self.beta), dtype=float)
        else:
            N_obs = history.round_number
            coop_counts = np.sum(history.opponent_actions.astype(float), axis=0)
            denom = float(N_obs + self.alpha + self.beta)
            p_opp = (coop_counts + self.alpha) / denom
        dp = np.zeros(n, dtype=float)
        dp[0] = 1.0
        for p in p_opp:
            new_dp = np.zeros_like(dp)
            for k in range(n - 1):
                prob = dp[k]
                if prob != 0.0:
                    new_dp[k] += prob * (1.0 - p)
                    new_dp[k + 1] += prob * p
            new_dp += dp[-1] * (1.0 - p)
            dp = new_dp
        total_prob = dp.sum()
        if total_prob > 0:
            dp = dp / total_prob
        else:
            dp = np.full_like(dp, 1.0 / dp.size)
        if m <= n - 1:
            P_ge_m = float(dp[m:].sum())
        else:
            P_ge_m = 0.0
        if 0 <= m - 1 <= n - 1:
            P_eq_m1 = float(dp[m - 1])
        else:
            P_eq_m1 = 0.0
        E_coop = reward * float(dp[m - 1:].sum()) if m - 1 <= n - 1 else 0.0
        E_def = 1.0 + reward * P_ge_m
        if P_ge_m >= self.T_exploit:
            action = Action.D
        elif P_eq_m1 >= self.T_pivot:
            action = Action.C
        elif E_def >= E_coop:
            action = Action.D
        else:
            action = Action.C
        if abs(E_def - E_coop) < self.delta and random.random() < self.epsilon:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_127(BaseStrategy):
    """
    Aggressive Pivotal Enforcer (APE)

    - Aggressive default to defect; cooperate only when cooperating strictly increases immediate expected payoff.
    - Enforcer: track opponents' cooperation frequencies, punish players who defected when your cooperation was decisive by
      lowering their effective cooperation probability for a punishment window and raising your cooperation margin.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(10, max(1, self.r - 1))
        self.H = max(1, math.floor(self.r / 4))
        self.epsilon = 0.01
        self.epsilon_punish = 0.2
        self.p0 = 0.5
        self.p_min = 0.05
        self.delta_p = 0.2
        self.redemption_threshold = 0.8
        self.penalty = [0.0] * (self.n - 1)
        self.last_defect_when_pivotal = [False] * (self.n - 1)
        self.punish_until = 0
        self.processed_rounds = 0

    def _prob_at_least(self, p_list, threshold):
        """Exact convolution DP for probability at least `threshold` successes among Bernoulli trials with probs p_list."""
        mlen = len(p_list)
        if threshold <= 0:
            return 1.0
        if threshold > mlen:
            return 0.0
        probs = [0.0] * (mlen + 1)
        probs[0] = 1.0
        for p in p_list:
            next_probs = [0.0] * (mlen + 1)
            for s in range(mlen + 1):
                if probs[s] == 0.0:
                    continue
                next_probs[s] += probs[s] * (1.0 - p)
                if s + 1 <= mlen:
                    next_probs[s + 1] += probs[s] * p
            probs = next_probs
        return sum(probs[threshold:])

    def _compute_empirical_p(self, history: PlayerHistory, j: int, upto_rounds: int):
        """
        Compute empirical cooperation frequency for opponent j over up to last W rounds
        using only observed rounds (upto_rounds = history.round_number).
        """
        if history is None or upto_rounds <= 0:
            return self.p0
        num_obs = min(self.W, upto_rounds)
        arr = history.opponent_actions
        if num_obs <= 0:
            return self.p0
        recent = arr[-num_obs:, j]
        coop_count = int(recent.sum())
        return coop_count / float(num_obs)

    def __call__(self, state: GameState, history: None | PlayerHistory):
        t = int(state.round_number)
        if history is not None:
            total_completed = history.round_number
            while self.processed_rounds < total_completed:
                rr = self.processed_rounds
                my_act = bool(history.my_actions[rr])
                opp_row = history.opponent_actions[rr, :]
                sum_other_coops = int(opp_row.sum())
                total_coops = (1 if my_act else 0) + sum_other_coops
                succeeded = total_coops >= self.m
                if my_act and succeeded:
                    if sum_other_coops == self.m - 1:
                        for j in range(self.n - 1):
                            if not bool(opp_row[j]):
                                self.last_defect_when_pivotal[j] = True
                                self.penalty[j] = min(1.0, self.penalty[j] + self.delta_p)
                                self.punish_until = max(self.punish_until, rr + 1 + self.H)
                self.processed_rounds += 1
            for j in range(self.n - 1):
                if self.last_defect_when_pivotal[j]:
                    freq = 0.0
                    if history.round_number > 0:
                        num_obs = min(self.W, history.round_number)
                        freq = float(history.opponent_actions[-num_obs:, j].sum()) / float(num_obs)
                    if freq > self.redemption_threshold:
                        self.last_defect_when_pivotal[j] = False
                        self.penalty[j] = 0.0
        p_list = []
        for j in range(self.n - 1):
            if history is None or (history is not None and history.round_number == 0):
                emp = self.p0
            else:
                emp = self._compute_empirical_p(history, j, history.round_number)
            adjusted = max(self.p_min, min(1.0, emp - self.penalty[j]))
            p_list.append(adjusted)
        P_without = self._prob_at_least(p_list, self.m)
        P_with = self._prob_at_least(p_list, max(0, self.m - 1))
        EV_defect = 1.0 + self.k * P_without
        EV_cooperate = self.k * P_with
        if t >= self.r:
            margin = 0.0
        elif t <= self.punish_until:
            margin = self.epsilon_punish
        else:
            margin = self.epsilon
        if EV_cooperate > EV_defect + margin:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_128(BaseStrategy):
    """
    Pivotal Exploiter (Aggressive)

    - Predicts opponents will repeat their last action (first round: all D).
    - Marks opponents as "unreliable" if you cooperated but the group still failed;
      unreliable players are treated as predicted defectors in future predictions.
    - Cooperates only when your cooperation is predicted to be pivotal
      (predicted_others == m - 1). Otherwise defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n_opponents = max(0, self.game_description.n_players - 1)
        self.unreliable = np.zeros(n_opponents, dtype=bool)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        last_my_action = bool(history.my_actions[-1])
        last_op_actions = history.opponent_actions[-1, :]
        total_cooperators_last = int(last_my_action) + int(np.sum(last_op_actions))
        if last_my_action and total_cooperators_last < self.game_description.m:
            defectors_mask = np.logical_not(last_op_actions)
            if defectors_mask.shape == self.unreliable.shape:
                self.unreliable = np.logical_or(self.unreliable, defectors_mask)
            else:
                n_opponents = defectors_mask.shape[0]
                new_unreliable = np.zeros(n_opponents, dtype=bool)
                min_len = min(len(self.unreliable), n_opponents)
                if min_len > 0:
                    new_unreliable[:min_len] = self.unreliable[:min_len]
                new_unreliable = np.logical_or(new_unreliable, defectors_mask)
                self.unreliable = new_unreliable
        predicted_actions = np.array(history.opponent_actions[-1, :], dtype=bool)
        if predicted_actions.shape != self.unreliable.shape:
            n_pred = predicted_actions.shape[0]
            n_unrel = self.unreliable.shape[0]
            if n_unrel < n_pred:
                pad = np.zeros(n_pred - n_unrel, dtype=bool)
                self.unreliable = np.concatenate([self.unreliable, pad])
            elif n_unrel > n_pred:
                self.unreliable = self.unreliable[:n_pred]
        if self.unreliable.size > 0:
            predicted_actions = np.where(self.unreliable, False, predicted_actions)
        predicted_others = int(np.sum(predicted_actions))
        m = self.game_description.m
        if predicted_others >= m:
            return Action.D
        if predicted_others == m - 1:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_7(BaseStrategy):
    """
    Opportunistic Exploiter with Targeted Punishment (OETP)

    Aggressive, recency-adaptive strategy for the Collective Risk Dilemma.
    Defaults favor defection unless cooperation is decisively in the player's
    immediate expected interest. Exploits stable cooperators, targets persistent
    defectors with short punishments, uses recency-weighted estimates of opponents'
    cooperation probabilities, and always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.7
        self.margin = 0.0
        self.coop_exploit_thresh = 0.8
        self.defect_thresh = 0.2
        self.H_default = 5
        self.P = 2
        self.punish_margin = 0.15
        self.p0 = 0.2
        self.test_coop_prob = 0.2
        self.final_window_L = 2

    def _normal_cdf(self, x: float) -> float:
        return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))

    def _prob_sum_bernoulli(self, ps, threshold):
        """
        Compute Prob(sum_{i} Bernoulli(ps[i]) >= threshold).
        Use exact convolution DP for moderate sizes; use normal approx for larger.
        ps: iterable of probabilities
        threshold: integer threshold (k)
        """
        ps = list(ps)
        m = len(ps)
        if threshold <= 0:
            return 1.0
        if threshold > m:
            return 0.0
        if m <= 60:
            dist = np.array([1.0], dtype=float)
            for p in ps:
                dist = np.convolve(dist, [1.0 - p, p])
            if threshold <= len(dist) - 1:
                return float(dist[threshold:].sum())
            else:
                return 0.0
        else:
            mu = sum(ps)
            var = sum((p * (1.0 - p) for p in ps))
            if var <= 0.0:
                if mu + 1e-12 >= threshold:
                    return 1.0
                else:
                    return 0.0
            sigma = math.sqrt(var)
            z = (threshold - 0.5 - mu) / sigma
            return 1.0 - self._normal_cdf(z)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None:
            t = 1
        else:
            t = int(history.round_number) + 1
        if t >= r:
            return Action.D
        if history is None:
            return Action.D
        M = n - 1
        p_list = []
        completed_rounds = int(history.round_number)
        if completed_rounds == 0:
            p_list = [self.p0] * M
        else:
            L = completed_rounds
            exponents = np.arange(L - 1, -1, -1, dtype=float)
            weights = (self.alpha ** exponents).astype(float)
            opp_actions = np.array(history.opponent_actions, dtype=bool)
            for j in range(M):
                col = opp_actions[:, j]
                weighted_total = float(weights.sum())
                if weighted_total <= 0.0:
                    p = self.p0
                else:
                    weighted_C = float((weights * col).sum())
                    p = weighted_C / weighted_total
                p_list.append(float(p))
        P_defect = self._prob_sum_bernoulli(p_list, m)
        P_coop = self._prob_sum_bernoulli(p_list, max(0, m - 1))
        E_D = 1.0 + k * P_defect
        E_C = k * P_coop
        SC_indices = [idx for idx, p in enumerate(p_list) if p >= self.coop_exploit_thresh]
        if len(SC_indices) >= m:
            return Action.D
        SD_indices = [idx for idx, p in enumerate(p_list) if p <= self.defect_thresh]
        most_recent_detection_round = None
        if SD_indices and completed_rounds > 0:
            H = min(self.H_default, completed_rounds)
            start_round = max(1, completed_rounds - H + 1)
            for s in range(completed_rounds, start_round - 1, -1):
                if not history.my_actions[s - 1]:
                    continue
                opps_coop = int(np.sum(history.opponent_actions[s - 1, :]))
                total_coop = opps_coop + 1
                if total_coop >= m:
                    continue
                sd_defected = False
                for idx in SD_indices:
                    if not bool(history.opponent_actions[s - 1, idx]):
                        sd_defected = True
                        break
                if sd_defected:
                    most_recent_detection_round = s
                    break
        in_punishment = False
        if most_recent_detection_round is not None:
            s = most_recent_detection_round
            if t > s and t <= s + self.P:
                in_punishment = True
        effective_margin = float(self.margin)
        punish_margin = float(self.punish_margin)
        if in_punishment:
            effective_margin = max(effective_margin, punish_margin)
            if random.random() < self.test_coop_prob:
                return Action.C
        L = int(self.final_window_L)
        if t >= r - L + 1:
            effective_margin = max(effective_margin, 0.05)
            punish_margin = max(punish_margin, 0.25)
            if in_punishment:
                effective_margin = max(effective_margin, punish_margin)
        if E_C > E_D + effective_margin:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_20(BaseStrategy):
    """
    Predatory Pivotal Strategy (PPS)

    Aggressive, self-interested strategy for the Collective Risk Dilemma.
    Defaults to defect. Only cooperates when reasonably confident it will be
    pivotal (i.e. predicted other cooperators == m-1 with supporting history).
    Punishes players who let it be a "sucker" (it cooperated but threshold failed)
    by refusing to cooperate for a fixed punishment window P. Occasionally probes
    with small probability epsilon when history is scarce.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.W = min(5, max(0, self.r - 1))
        self.P = min(3, max(1, math.floor(self.r / 4)))
        self.epsilon = 0.03
        self.punishment_timers = {}
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = state.round_number
        completed = history.round_number
        n_opponents = max(0, self.n - 1)
        for r_idx in range(self.last_processed_round, completed):
            to_delete = []
            for p, timer in list(self.punishment_timers.items()):
                new_timer = timer - 1
                if new_timer <= 0:
                    to_delete.append(p)
                else:
                    self.punishment_timers[p] = new_timer
            for p in to_delete:
                self.punishment_timers.pop(p, None)
            my_coop = bool(history.my_actions[r_idx])
            if n_opponents > 0:
                opp_row = history.opponent_actions[r_idx, :]
                opp_coop_array = np.array(opp_row, dtype=np.bool_)
                opp_coop_count = int(np.sum(opp_coop_array))
            else:
                opp_coop_array = np.array([], dtype=np.bool_)
                opp_coop_count = 0
            total_cooperators = (1 if my_coop else 0) + opp_coop_count
            if my_coop and total_cooperators < self.m:
                for p_index in range(n_opponents):
                    if p_index < len(opp_coop_array) and (not bool(opp_coop_array[p_index])):
                        prev = self.punishment_timers.get(p_index, 0)
                        self.punishment_timers[p_index] = max(prev, self.P)
            for p_index in list(self.punishment_timers.keys()):
                if p_index < len(opp_coop_array):
                    if not bool(opp_coop_array[p_index]):
                        prev = self.punishment_timers.get(p_index, 0)
                        self.punishment_timers[p_index] = max(prev, self.P)
        self.last_processed_round = completed
        recent_counts = []
        if completed > 0 and n_opponents > 0:
            start_idx = max(0, completed - self.W)
            for idx in range(start_idx, completed):
                recent_counts.append(int(np.sum(np.array(history.opponent_actions[idx, :], dtype=np.bool_))))
        elif completed > 0:
            start_idx = max(0, completed - self.W)
            for idx in range(start_idx, completed):
                recent_counts.append(0)
        last_count = recent_counts[-1] if len(recent_counts) > 0 else 0
        if len(recent_counts) == 0:
            predicted_others = 0
        elif self.r <= 10:
            predicted_others = int(last_count)
        else:
            median_val = float(np.median(np.array(recent_counts)))
            predicted_others = int(math.floor(median_val + 1e-06))
        reliability = 0.0
        if len(recent_counts) > 0:
            ge_count = sum((1 for c in recent_counts if c >= predicted_others))
            reliability = ge_count / len(recent_counts)
        if t == self.r:
            if predicted_others >= self.m:
                return Action.D
            elif predicted_others == self.m - 1:
                cond1 = len(recent_counts) > 0 and last_count >= self.m - 1
                cond2 = sum((1 for c in recent_counts if c >= self.m - 1)) >= 1
                if cond1 or cond2:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        if len(self.punishment_timers) > 0:
            return Action.D
        if predicted_others >= self.m:
            return Action.D
        if predicted_others == self.m - 1:
            cond_last = len(recent_counts) > 0 and last_count >= self.m - 1
            cond_repeat = sum((1 for c in recent_counts if c >= self.m - 1)) >= 2
            if cond_last or cond_repeat:
                return Action.C
            else:
                return Action.D
        if predicted_others < self.m - 1:
            no_reliable_history = completed <= self.W
            if no_reliable_history and random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_99(BaseStrategy):
    """
    Aggressive Greedy Pivotal Defector (AGPD)

    - Maintains per-opponent EMA estimates of cooperation probabilities.
    - Computes Poisson-Binomial (via DP convolution) for opponents cooperating.
    - Cooperates only when expected one-shot payoff from cooperating exceeds defecting
      by a margin delta. Otherwise defects.
    - Starts by defecting. If cooperated and the group still failed to reach the threshold,
      enters a short punitive window of guaranteed defection for Lpun rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.2
        self.delta = 0.05
        self.p0 = 0.5
        self.num_opps = max(0, self.n - 1)
        if self.num_opps > 0:
            self.p_js = np.full(self.num_opps, self.p0, dtype=float)
        else:
            self.p_js = np.array([], dtype=float)
        Lpun = min(3, max(1, math.floor(self.r / 6))) if self.r > 0 else 1
        self.Lpun = int(Lpun)
        self.punishment_counter = 0
        self.processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        completed = history.round_number
        for t in range(self.processed_rounds, completed):
            if self.num_opps > 0:
                round_actions = history.opponent_actions[t, :]
                for j in range(self.num_opps):
                    observed = 1.0 if bool(round_actions[j]) else 0.0
                    self.p_js[j] = (1.0 - self.alpha) * self.p_js[j] + self.alpha * observed
            my_played = bool(history.my_actions[t])
            opp_coops = int(np.sum(history.opponent_actions[t, :])) if self.num_opps > 0 else 0
            total_cooperators = opp_coops + (1 if my_played else 0)
            if my_played and total_cooperators < self.m:
                self.punishment_counter = self.Lpun
        self.processed_rounds = completed
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        num = self.num_opps
        if num == 0:
            if self.m <= 1:
                P_with = 1.0
            else:
                P_with = 0.0
            if self.m <= 0:
                P_without = 1.0
            else:
                P_without = 0.0
        else:
            q = np.zeros(num + 1, dtype=float)
            q[0] = 1.0
            for j in range(num):
                p = float(self.p_js[j])
                q_new = np.zeros_like(q)
                q_new[0] = q[0] * (1.0 - p)
                for c in range(1, num + 1):
                    q_new[c] = q[c] * (1.0 - p) + q[c - 1] * p
                q = q_new
            lower_without = max(self.m, 0)
            if lower_without > num:
                P_without = 0.0
            else:
                P_without = float(np.sum(q[lower_without:]))
            lower_with = max(self.m - 1, 0)
            if lower_with > num:
                P_with = 0.0
            else:
                P_with = float(np.sum(q[lower_with:]))
        E_C = -1.0 + self.k * P_with
        E_D = +1.0 + self.k * P_without
        if E_C > E_D + self.delta:
            return Action.C
        else:
            return Action.D