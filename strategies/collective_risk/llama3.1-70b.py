"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: Client
- Model: llama3.1:70b
- Game: collective_risk
"""

from dataclasses import dataclass
from enum import Enum
import math
import numpy as np
from numpy.typing import NDArray
import random

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState




class Strategy_COLLECTIVE_1(BaseStrategy):
    """
    Adaptive Cooperative Escalation Strategy.
    
    This strategy balances individual self-interest with the need for cooperation 
    to achieve the community reward by adapting to the actions of others and 
    escalating cooperation when necessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.m / game_description.n_players
        self.prev_coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        self.prev_coop_count = sum(prev_round_actions)
        p = self.prev_coop_count / self.game_description.n_players
        if random.random() < p or self.prev_coop_count >= self.coop_threshold * self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if history.my_payoffs[-1] == self.game_description.k + 1:
            self.coop_threshold += 0.1 * self.coop_threshold
        elif self.prev_coop_count < self.game_description.m:
            self.coop_threshold -= 0.05 * self.coop_threshold
        return action

class Strategy_COLLECTIVE_2(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.p_punish = 0.7
        self.prev_m_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.p_init else Action.D
        opponent_actions = history.opponent_actions[-1]
        m_prev = np.sum(opponent_actions)
        total_coop_payoff_prev = self.game_description.k * np.sum(history.my_actions[-1]) + 1
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action:
                total_coop_payoff_prev += self.game_description.k
        if m_prev >= self.game_description.m or total_coop_payoff_prev >= self.game_description.k * m_prev:
            cooperate = True
        elif random.random() < self.p_punish:
            cooperate = False
        else:
            cooperate = random.random() < self.p_init
        if self.prev_m_prev is not None:
            if m_prev > self.prev_m_prev:
                self.p_init += 0.1
            elif m_prev < self.prev_m_prev:
                self.p_init -= 0.1
        self.prev_m_prev = m_prev
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = random.random() < 0.9
        elif self.game_description.n_players == 2:
            cooperate = True
        elif m_prev + np.sum(history.my_actions[-1]) >= self.game_description.m:
            cooperate = True
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_3(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective risk management.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_threshold_met = None
        self.total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.previous_threshold_met = history.my_actions[-1] and sum(history.opponent_actions[-1, :]) + 1 >= self.game_description.m
        self.total_cooperators += int(self.previous_threshold_met)
        if self.previous_threshold_met:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1 and self.total_cooperators + 1 < self.game_description.m:
            return Action.D
        else:
            p_defect = 1 - self.game_description.k / (self.game_description.k + 1)
            return Action.C if random.random() > p_defect else Action.D

class Strategy_COLLECTIVE_4(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to the game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5
        self.punishment_target = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        m_prev = np.sum(history.opponent_actions[-1, :] == True)
        avg_payoff_prev = np.mean(history.my_payoffs)
        if avg_payoff_prev < 1.5:
            return Action.D
        elif m_prev >= self.game_description.m:
            return Action.C
        elif history.my_actions[-1] == False and m_prev >= self.game_description.m:
            self.punishment_target = True
            return Action.D
        if avg_payoff_prev > 1.5 and m_prev >= self.game_description.m:
            self.cooperation_probability += 0.05
            self.cooperation_probability = min(self.cooperation_probability, 1)
        elif self.punishment_target:
            self.cooperation_probability -= 0.05
            self.cooperation_probability = max(self.cooperation_probability, 0)
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_5(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy is a collective, adaptive, and robust approach to playing the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5 if game_description.m <= game_description.n_players / 2 else 0
        self.collective_payoff_history = []
        self.cooperate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        previous_payoff = sum((history.my_payoffs[-1] for _ in range(self.game_description.n_players)))
        current_cooperators = sum((1 for opponent_action in history.opponent_actions[-1, :] if opponent_action))
        if current_cooperators >= self.game_description.m:
            cooperation_probability = min(max(self.cooperation_probability, 0), 1)
            return Action.C if random.random() < cooperation_probability else Action.D
        else:
            return Action.D
        new_payoff = sum((history.my_payoffs[-1] for _ in range(self.game_description.n_players)))
        self.collective_payoff_history.append(new_payoff)
        if len(self.collective_payoff_history) > 1 and new_payoff > self.collective_payoff_history[-2]:
            self.cooperation_probability += 0.05
        elif len(self.collective_payoff_history) > 1 and new_payoff <= self.collective_payoff_history[-2]:
            self.cooperation_probability -= 0.05
        if state.round_number == self.game_description.n_rounds - 1 and sum((1 for x in self.cooperate_history if x == Action.C)) >= (self.game_description.n_rounds - 1) / 2:
            return Action.C
        self.cooperate_history.append(Action.C if current_cooperators >= self.game_description.m else Action.D)

class Strategy_COLLECTIVE_6(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.
    
    This strategy aims to balance individual self-interest with collective cooperation, 
    adapting to the game's history and parameters. It ensures robustness against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_coop_level = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if prev_coop_level == self.game_description.m - 1:
            return Action.C
        coop_diff = self.game_description.m - prev_coop_level
        defect_prob = coop_diff / (self.game_description.n_players - self.game_description.m)
        if self.game_description.n_players == self.game_description.m:
            return Action.D
        if random.random() < defect_prob:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_7(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective risk management by adapting to the observed behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            if self.game_description.m <= self.game_description.n_players / 2 or self.game_description.m == 1:
                return Action.C
            else:
                return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.game_description.m:
            cooperate_prob = (state.round_number / self.game_description.n_rounds) ** self.alpha
            if random.random() < cooperate_prob:
                return Action.C
        coop_prop = prev_cooperators / self.game_description.n_players
        if coop_prop >= self.alpha:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_8(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy for Collective Risk Dilemma games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperations = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n, m, k, r = (self.game_description.n_players, self.game_description.m, self.game_description.k, self.game_description.n_rounds)
        if state.round_number == 0:
            return Action.C if m <= n / 2 else Action.D
        CR = sum(self.cooperations) / (n * state.round_number)
        θ = m / n * (k - 1) / k
        prev_cooperators = sum(history.opponent_actions[-1, :])
        action = Action.C if CR >= θ or prev_cooperators >= m else Action.D if prev_cooperators < m and CR != θ else random.choice([Action.C, Action.D])
        if state.round_number == r - 1:
            my_payoffs_C = sum(history.my_payoffs) + k
            my_payoffs_D = sum(history.my_payoffs)
            action = Action.C if my_payoffs_C > my_payoffs_D else Action.D
        self.cooperations.append(1 if action == Action.C else 0)
        return action

class Strategy_COLLECTIVE_9(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to the behavior of other players.
    It promotes cooperation while responding to changes in the level of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_threshold = game_description.m
        self.k_reward = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] & history.my_actions[-1])
        total_previous_cooperators = previous_cooperators + (history.my_actions[-1] and 1 or 0)
        if total_previous_cooperators >= self.m_threshold:
            return Action.C
        else:
            p_defect = (self.m_threshold - total_previous_cooperators) / (self.n_players - self.m_threshold)
            return Action.D if random.random() < p_defect else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            expected_payoff_C = self.k_reward * total_previous_cooperators
            expected_payoff_D = 1 + self.k_reward * (self.m_threshold - 1)
            return Action.C if expected_payoff_C > expected_payoff_D else Action.D

class Strategy_COLLECTIVE_10(BaseStrategy):
    """
    Adaptive Cooperation (AC) strategy for the Collective Risk Dilemma game.
    
    This strategy adapts its cooperation probability based on the previous round's outcome,
    encouraging sustained cooperation when beneficial and adjusting to avoid exploitation by defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize the Adaptive Cooperation strategy with a given game description.

        Args:
            game_description (CollectiveRiskDescription): The game description containing n_players, n_rounds, m, and k.
        """
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determine the action to take in a given game state and history.

        Args:
            state (GameState): The current game state containing the round number.
            history (None | PlayerHistory): The player's history of actions, payoffs, and opponents' actions and payoffs.

        Returns:
            Action: The chosen action, either COOPERATE or DEFECT.
        """
        if state.round_number == 0:
            return self._choose_action(self.cooperation_probability)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_coop = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        self.cooperation_probability = self._calculate_p_curr(prev_coop)
        return self._choose_action(self.cooperation_probability)

    def _calculate_p_curr(self, prev_coop: int) -> float:
        """
        Calculate the cooperation probability for the current round based on the previous round's outcome.

        Args:
            prev_coop (int): The number of cooperators in the previous round.

        Returns:
            float: The calculated cooperation probability.
        """
        n = self.game_description.n_players
        m = self.game_description.m
        if prev_coop >= m:
            return 0.7 + (prev_coop - m) / n * 0.3
        else:
            return max(0, (m - prev_coop) / n)

    def _choose_action(self, probability: float) -> Action:
        """
        Choose an action based on a given cooperation probability.

        Args:
            probability (float): The cooperation probability.

        Returns:
            Action: The chosen action, either COOPERATE or DEFECT.
        """
        if random.random() < probability:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_11(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors while promoting 
    collective cooperation and robustness through initial cooperation, history-dependent 
    cooperation, punishment, forgiveness, and reward escalation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []
        self.cooperators = 0
        self.punished_players = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_actions = history.opponent_actions[-1] if history else []
        self.cooperators = sum(prev_actions)
        if self.cooperators >= self.game_description.m:
            action = Action.C
        elif self.cooperators > 0:
            action = Action.D
        else:
            action = Action.C
        for player, prev_action in enumerate(prev_actions):
            if prev_action == False and self.cooperators >= self.game_description.m:
                self.punished_players.append(player)
            elif prev_action == True and player in self.punished_players:
                self.punished_players.remove(player)
        if len(self.history) > 1 and self.cooperators >= self.game_description.m and (sum(history.opponent_actions[-2, :]) >= self.game_description.m):
            self.game_description.k += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        self.history.append(action)
        return action

class Strategy_COLLECTIVE_12(BaseStrategy):
    """
    Adaptive Cooperation Strategy for Collective Risk Dilemma.
    
    This strategy balances cooperation and self-interest, promoting a collective approach while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_c = 0
        self.APP = 0
        self.uncooperative_players = []
        self.cooperation_likelihood = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.m_c = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if len(history.my_payoffs) >= 5:
            recent_payoffs = history.my_payoffs[-5:]
            self.APP = np.mean(recent_payoffs)
            if self.APP > 1.5:
                self.cooperation_likelihood += 0.1
                self.cooperation_likelihood = min(self.cooperation_likelihood, 1.0)
            elif self.APP < 1.0:
                self.cooperation_likelihood -= 0.1
                self.cooperation_likelihood = max(self.cooperation_likelihood, 0.0)
        if history is not None and len(history.opponent_actions) >= 3:
            recent_opponent_actions = history.opponent_actions[-3:, :]
            for i in range(self.game_description.n_players - 1):
                if all(recent_opponent_actions[:, i] == False) and self.m_c >= self.game_description.m:
                    self.uncooperative_players.append(i)
                    break
        if len(self.uncooperative_players) > 0:
            self.cooperation_likelihood -= 0.2
            self.cooperation_likelihood = max(self.cooperation_likelihood, 0.0)
        if random.random() < self.cooperation_likelihood:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_13(BaseStrategy):
    """
    Adaptive Collective Risk (ACR) Strategy for Collective Risk Dilemma.
    
    The ACR strategy aims to balance individual self-interest with collective risk management. 
    It adapts to the game's history and opponent behaviors while maintaining a robust and cooperative approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_cooperation = False
        self.m_contributors = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        last_round_contributors = sum(history.opponent_actions[-1, :])
        self.m_contributors = last_round_contributors + (history.my_actions[-1] == Action.C)
        if history.my_payoffs.size > 0:
            self.last_round_cooperation = history.my_payoffs[-1] > 0
        if self.last_round_cooperation and self.m_contributors >= self.game_description.m:
            return Action.C
        elif not self.last_round_cooperation:
            return random.choice([Action.D, Action.C])
        else:
            return Action.C

class Strategy_COLLECTIVE_14(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma.
    
    This strategy prioritizes cooperation when the collective threshold is met or exceeded, 
    ensuring that the group achieves the reward. By adapting to the previous round's 
    cooperation level, ACT encourages others to cooperate while also avoiding exploitation 
    by defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if num_cooperators_prev_round == self.game_description.m:
            return Action.C
        if num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            p_defect = (self.game_description.m - num_cooperators_prev_round) / self.game_description.m
            return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_15(BaseStrategy):
    """
    A collective risk dilemma adaptive strategy that prioritizes cooperation when 
    the collective threshold is met or likely to be met, while adapting to individual 
    payoffs and opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_probability = 0.5

    def calculate_payoff(self, actions: NDArray[np.bool_], action: Action) -> float:
        """Calculate payoff for a player given the actions of all players."""
        cooperators = sum(actions)
        payoff = 1 - int(action == Action.C)
        if cooperators >= self.game_description.m:
            payoff += self.game_description.k
        return payoff

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_actions = history.opponent_actions[-1]
        my_prev_action = history.my_actions[-1]
        prev_cooperators = sum(prev_actions)
        prev_payoff = self.calculate_payoff(np.concatenate(([my_prev_action], prev_actions)), Action.C if my_prev_action else Action.D)
        if prev_cooperators >= math.ceil(self.game_description.m / 2) or prev_payoff >= (1 + self.game_description.k) / 2:
            return Action.C
        if prev_cooperators < self.game_description.m:
            return Action.D
        adjust_prob = 0.1
        if state.round_number > 1:
            last_two_payoffs = [self.calculate_payoff(np.concatenate(([history.my_actions[i]], history.opponent_actions[i])), Action.C if history.my_actions[i] else Action.D) for i in range(-2, 0)]
            avg_last_two_payoffs = sum(last_two_payoffs) / 2
            if avg_last_two_payoffs >= (1 + self.game_description.k) / 2:
                adjust_prob = -adjust_prob
        self.coop_probability = max(0, min(1, self.coop_probability + adjust_prob))
        return Action.C if random.random() < self.coop_probability else Action.D

class Strategy_COLLECTIVE_16(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Adjustment (ACTA) strategy for Collective Risk dilemma.

    This strategy promotes collective cooperation by establishing an initial cooperative tone,
    adjusting the cooperation threshold based on previous outcomes, introducing a degree of defection
    to "punish" non-cooperative behavior, and encouraging sustained cooperation through consecutive
    cooperation rewards.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_threshold = game_description.m
        self.last_round_outcome = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.last_round_outcome = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.last_round_outcome >= self.game_description.m:
            action = Action.C
        else:
            p_defect = 1 - self.game_description.k / (self.game_description.n_players + self.game_description.k)
            action = Action.D if random.random() < p_defect else Action.C
        if self.last_round_outcome >= self.game_description.m:
            self.cooperate_threshold += 1
            if self.cooperate_threshold > self.game_description.n_players - 1:
                self.cooperate_threshold = self.game_description.n_players - 1
        if self.last_round_outcome < self.game_description.m / 2:
            self.cooperate_threshold = self.game_description.m
        return action

class Strategy_COLLECTIVE_17(BaseStrategy):
    """
    A strategy that adapts to the level of cooperation observed in previous rounds,
    gradually increasing the likelihood of defection when faced with insufficient cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if any((np.sum(history.opponent_actions[:i, :]) >= self.game_description.m for i in range(state.round_number))):
                return Action.C
            else:
                return Action.D
        cooperators = np.sum(history.opponent_actions[-1, :])
        p_defect = (self.game_description.m - cooperators) / self.game_description.n_players
        if random.random() < p_defect:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_18(BaseStrategy):
    """
    Adaptive Cooperation Strategy for Collective Risk Dilemma.
    
    This strategy prioritizes cooperation when the threshold is met or exceeded, 
    acknowledging that collective success benefits all players. By adapting to the 
    group's behavior and adjusting cooperation probability accordingly, this 
    strategy promotes a culture of mutual support while remaining robust against 
    exploitation by defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_round_cooperators < self.game_description.m:
            action = Action.D
        else:
            action = Action.C if random.random() < self.cooperate_prob else Action.D
        if prev_round_cooperators >= self.game_description.m and self.cooperate_prob < 0.9:
            self.cooperate_prob += 0.1
        elif prev_round_cooperators < self.game_description.m and self.cooperate_prob > 0.1:
            self.cooperate_prob -= 0.1
        return action

class Strategy_COLLECTIVE_19(BaseStrategy):
    """
    Adaptive Cooperation strategy for the Collective Risk Dilemma game.
    This strategy aims to balance individual self-interest with collective well-being while adapting to the evolving game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        last_round_cooperators = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] else sum(history.opponent_actions[-1, :])
        threshold_met = last_round_cooperators >= self.game_description.m
        if threshold_met:
            return Action.C
        elif len(history.my_payoffs) > 2:
            second_last_round_cooperators = sum(history.opponent_actions[-2, :]) + 1 if history.my_actions[-2] else sum(history.opponent_actions[-2, :])
            cooperators_increasing_or_stable = last_round_cooperators >= second_last_round_cooperators
            if cooperators_increasing_or_stable:
                return Action.D if random.random() < 0.5 else Action.C
            elif last_round_cooperators < second_last_round_cooperators:
                return Action.C if random.random() < 0.75 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_20(BaseStrategy):
    """
    Balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.first_round_played = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if not self.first_round_played or (state.round_number == 0 and history is None):
            self.first_round_played = True
            return Action.C
        prev_outcome = history.opponent_actions[-1, :]
        num_coop_prev = sum(prev_outcome)
        if num_coop_prev >= self.game_description.m:
            return Action.C
        else:
            p_defect = 1 - self.game_description.k / (self.game_description.n_players + self.game_description.k)
            if random.random() < p_defect:
                return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_21(BaseStrategy):
    """
    The Adaptive Cooperative Escalation (ACE) strategy aims to balance individual self-interest with collective cooperation,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = []
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1]
        num_cooperators_last_round = sum(opponent_actions_last_round)
        if len(self.cooperators) >= self.game_description.m:
            return Action.C
        if any((opponent_action for opponent_action in opponent_actions_last_round)):
            coop_prob = num_cooperators_last_round / self.game_description.n_players * self.game_description.k
            return Action.C if random.random() < coop_prob else Action.D
        if all((not opponent_action for opponent_action in opponent_actions_last_round)):
            return Action.C
        if len(self.cooperators) >= self.game_description.m and history.my_payoffs[-1] == 0:
            defect_counter = 2
            while defect_counter > 0:
                action = Action.D
                defect_counter -= 1
                return action
        return Action.D

class Strategy_COLLECTIVE_22(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.pi_avg = 0.0
        self.num_cooperators_prev = 0
        self.pi_last_w_rounds = []
        self.w = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        pi_current = history.my_payoffs[-1]
        self.pi_last_w_rounds.append(pi_current)
        if len(self.pi_last_w_rounds) > self.w:
            self.pi_last_w_rounds.pop(0)
        if state.round_number >= self.w:
            pi_avg = sum(self.pi_last_w_rounds) / len(self.pi_last_w_rounds)
            return Action.C if pi_avg >= self.game_description.k / 2 else Action.D
        else:
            num_cooperators_prev = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            if num_cooperators_prev >= self.game_description.m:
                return Action.C
            elif state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            else:
                return Action.D

class Strategy_COLLECTIVE_23(BaseStrategy):
    """
    Adaptive Collective Risk (ACR) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_C = 0.5
        self.p_D = 0.5
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.p_C else Action.D
        coop_rate_prev = np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        collective_payoff_prev = np.mean(history.my_payoffs)
        if state.round_number > 1:
            self.avg_payoff += collective_payoff_prev / state.round_number
            if coop_rate_prev < self.game_description.m / self.game_description.n_players:
                self.p_C = min(self.p_C + 0.05, 0.7)
                self.p_D = max(self.p_D - 0.05, 0.3)
            else:
                self.p_C = max(self.p_C - 0.05, 0.3)
                self.p_D = min(self.p_D + 0.05, 0.7)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if coop_rate_prev >= self.game_description.m / self.game_description.n_players else Action.D
        return random.choice([Action.C, Action.D]) if random.random() < self.p_C else Action.D

class Strategy_COLLECTIVE_24(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective optimality by 
    cooperating initially, adapting cooperation probability based on past outcomes, 
    punishing defection when the threshold is not met, and randomly cooperating or 
    defecting to gather information about other players' strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0.0
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.m_prev = (opponent_cooperators + (history.my_actions[-1] == Action.C)) / self.game_description.n_players
        if opponent_cooperators < self.game_description.m:
            p_d = 1 - self.game_description.k / (self.game_description.n_players + self.game_description.k)
            action = Action.D if random.random() < p_d else Action.C
        elif self.m_prev >= self.game_description.m / self.game_description.n_players:
            p_c = max(0.5, self.m_prev)
            action = Action.C if random.random() < p_c else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            action = Action.C if opponent_cooperators >= self.game_description.m else Action.D
        elif all(history.opponent_actions[-2, :] == False) and history.my_actions[-2] == False:
            self.consecutive_defections += 1
            action = Action.C if random.random() < 0.5 else Action.D
        else:
            action = Action.C if random.random() < 0.5 else Action.D
        if random.random() < 0.1:
            return Action.C if random.random() < 0.5 else Action.D
        return action

class Strategy_COLLECTIVE_25(BaseStrategy):
    """
    A strategy that adapts to the behavior of other players by balancing individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = (game_description.k - 1) / game_description.k
        self.q = 1 - (game_description.m - 1) / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators > self.game_description.m:
            self.p = min(1, self.p + 0.1)
        elif opponent_cooperators < self.game_description.m:
            self.q = max(0, self.q - 0.1)
        if opponent_cooperators >= self.game_description.m:
            return Action.C if random.random() < self.p else Action.D
        else:
            return Action.C if random.random() > self.q else Action.D

class Strategy_COLLECTIVE_26(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma.
    This strategy prioritizes cooperation while adapting to the collective behavior of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.collective_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.collective_cooperators = sum(history.opponent_actions[-1, :])
        collective_cooperation_rate = self.collective_cooperators / self.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if collective_cooperation_rate >= self.m / self.n_players:
            return Action.C
        else:
            p = 0.5 + (self.m - self.collective_cooperators) / self.n_players
            return Action.D if random.random() < p else Action.C

class Strategy_COLLECTIVE_27(BaseStrategy):
    """
    Adaptive Collective Concession (ACC) strategy.

    This strategy adapts to the game's history and parameters by adjusting a concession threshold.
    It balances individual self-interest with collective welfare by conceding when the threshold is met or exceeded,
    while being cautious in later rounds to avoid failing to meet the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.concession_threshold = 0.5 * game_description.m / game_description.n_players
        self.adjustment_parameter = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if abs(self.game_description.m - self.game_description.n_players / 2) < 1 else Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        own_cooperation_last_round = history.my_actions[-1]
        if opponent_cooperators >= self.game_description.m:
            self.concession_threshold -= self.adjustment_parameter
        else:
            self.concession_threshold += self.adjustment_parameter
        avg_cooperators = np.mean(history.opponent_actions, axis=1).mean()
        if state.round_number in [self.game_description.n_rounds - 2, self.game_description.n_rounds - 1]:
            return Action.C if opponent_cooperators >= self.game_description.m / 2 else Action.D
        elif avg_cooperators <= self.concession_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_30(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.adaptive_threshold = game_description.m
        self.prev_cooperators = 0
        self.cooperation_likelihood = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1]):
            self.cooperation_likelihood = min(1.0, self.cooperation_likelihood + 0.1)
        if sum(history.opponent_actions[-1]) >= self.game_description.m:
            if sum(history.opponent_actions[-1]) >= self.adaptive_threshold - 1:
                self.adaptive_threshold = max(2, self.adaptive_threshold - 1)
                return Action.C
            else:
                return Action.D
        else:
            self.adaptive_threshold = min(self.game_description.n_players - 1, self.adaptive_threshold + 1)
            return Action.D
        if random.random() < self.cooperation_likelihood:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_31(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective risk management by adapting cooperation based on game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        last_half_rounds = max(1, state.round_number // 2)
        avg_cooperators = np.mean(np.sum(history.opponent_actions[-last_half_rounds:], axis=1)) + history.my_actions[-last_half_rounds:].sum() / last_half_rounds
        theta = self.game_description.m if avg_cooperators >= self.game_description.m else math.ceil(avg_cooperators + 1)
        num_cooperators = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions is not None else 0)
        if num_cooperators >= theta:
            return Action.C
        elif history.my_actions is not None and np.mean(history.my_actions) >= 0.5:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_32(BaseStrategy):
    """
    A strategy that adapts to the observed cooperation level in a collective risk dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if history is not None and len(history.opponent_actions) > 0:
            last_round_cooperators = sum(history.opponent_actions[-1, :])
            thr = max(m, last_round_cooperators * (1 + (k - 1) / k))
            if thr <= n:
                return Action.C
            else:
                return Action.D
        if self.game_description.n_players <= m:
            return Action.C
        return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_33(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = game_description.m / game_description.n_players
        self.opponent_defect_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_coop else Action.D
        t_coop = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if t_coop >= self.game_description.m:
            self.p_coop = 1
        else:
            self.p_coop -= 0.05
            if self.p_coop < 0:
                self.p_coop = 0
        if self.game_description.k > 2 and sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1] >= self.game_description.m * self.game_description.k:
            self.p_coop += 0.1
            if self.p_coop > 1:
                self.p_coop = 1
        opponent_defected = sum(history.opponent_actions[-1, :]) < self.game_description.m and (not history.my_actions[-1])
        if opponent_defected:
            self.opponent_defect_count += 1
            self.p_coop -= 0.2 * (self.opponent_defect_count / state.round_number)
            if self.p_coop < 0:
                self.p_coop = 0
        if state.round_number == self.game_description.n_rounds - 1 and sum(history.my_actions) + sum(sum((opponent_action for opponent_action in history.opponent_actions))) >= self.game_description.m:
            return Action.C
        return Action.C if random.random() < self.p_coop else Action.D

class Strategy_COLLECTIVE_34(BaseStrategy):
    """
    A collective strategy that adapts to the game's history and parameters while maintaining a cooperative mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.m / self.n_players else Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        total_payoff = sum(history.my_payoffs)
        p_coop = self.m / self.n_players
        if cooperators >= self.m:
            p_coop += 0.1
            p_coop = min(p_coop, 1)
        else:
            p_coop -= 0.05
            p_coop = max(p_coop, 0)
        if cooperators < self.m and random.random() < 0.5:
            return Action.D
        if total_payoff >= self.n_players * self.k:
            p_coop = 1
        if state.round_number == self.n_rounds - 1 and cooperators < self.m:
            return Action.D if random.random() < 0.5 else Action.C
        return Action.C if random.random() < p_coop else Action.D

class Strategy_COLLECTIVE_35(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    This collective strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    It aims to cooperate when cooperation is likely to succeed and defect when it's not.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the ACC strategy.

        Args:
            game_description (CollectiveRiskDescription): Game description containing number of players, rounds, minimum cooperators needed, and reward factor.
        """
        self.game_description = game_description
        self.avg_coops_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Returns the action for the given game state and history.

        Args:
            state (GameState): Current game state.
            history (None | PlayerHistory): History of actions and payoffs.

        Returns:
            Action: C (cooperate) or D (defect)
        """
        if state.round_number == 0:
            return Action.C
        avg_coops = np.mean(self.avg_coops_history)
        recent_success = history.my_payoffs[-1] > self.game_description.k + 1 - self.game_description.m / self.game_description.n_players
        defectors_prev_round = sum(~history.opponent_actions[-1, :])
        defect_reward = defectors_prev_round >= self.game_description.n_players // 2
        coop_punishment = history.my_payoffs[-1] < self.game_description.k + 1 - self.game_description.m / self.game_description.n_players
        if avg_coops >= self.game_description.m * 0.8:
            action = Action.C
        elif recent_success:
            action = Action.C
        elif defect_reward and avg_coops >= self.game_description.m * 0.85:
            action = Action.C
        elif coop_punishment and avg_coops >= self.game_description.m * 0.9:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        cooperators = sum(history.opponent_actions[-1, :]) + int(action == Action.C)
        self.avg_coops_history.append(cooperators / self.game_description.n_players)
        return action

class Strategy_COLLECTIVE_36(BaseStrategy):
    """
    Adaptive Collective Threshold strategy.
    
    This strategy prioritizes achieving the collective goal while adapting to 
    the behavior of other players. By cooperating initially and adjusting based 
    on outcomes, we aim to encourage others to cooperate, punish those who defect 
    excessively, and balance individual self-interest with collective benefit.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_c = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        threshold_met = sum(history.opponent_actions[-1, :] + [True]) >= self.game_description.m
        if threshold_met:
            self.p_c += 0.05
            return Action.C if random.random() < self.p_c else Action.D
        else:
            p_d = (self.game_description.m - 1) / (self.game_description.n_players - 1)
            return Action.D if random.random() < p_d else Action.C

class Strategy_COLLECTIVE_37(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C if n_players <= 10 else Action.D
        if state.round_number == r - 1:
            return Action.D
        theta = max(m / n_players, 0.5) * (k - 1) / k
        if n_players <= 3:
            return Action.C if m / n_players >= 0.5 else Action.D
        avg_coop_rate = self.calculate_avg_coop_rate(history, state.round_number)
        if avg_coop_rate >= theta:
            return Action.C
        else:
            return Action.D

    def calculate_avg_coop_rate(self, history: PlayerHistory, current_round: int) -> float:
        """
        Calculate the average cooperation rate up to the given round number.
        
        Parameters:
        - history (PlayerHistory): The game's history.
        - current_round (int): The current round number.
        
        Returns:
        - avg_coop_rate (float): The average cooperation rate up to the given round number.
        """
        total_cooperations = sum(sum(history.opponent_actions[:current_round, :]))
        total_possible_actions = current_round * self.game_description.n_players
        if total_possible_actions == 0:
            return 0.0
        return total_cooperations / total_possible_actions

class Strategy_COLLECTIVE_38(BaseStrategy):
    """
    A strategy that adapts its cooperation level based on the game's history and parameters.
    It aims to balance individual self-interest with collective welfare by adjusting a dynamic cooperation threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        success = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        reward = history.my_payoffs[-1]
        theta = max(self.game_description.m - 1, min(self.game_description.n_players - 1, (success + reward / self.game_description.k) / self.game_description.n_players))
        random_number = random.uniform(0, 1)
        if random_number <= theta:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_39(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Collective Cooperation (ACC)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        p_defect = prev_cooperators / self.game_description.n_players
        if prev_cooperators >= self.game_description.m:
            self.consecutive_defections = 0
            return Action.C
        else:
            self.consecutive_defections += int(history.my_actions[-1] == False)
            if random.random() < p_defect and (not self.consecutive_defections > self.game_description.n_players / 2):
                return Action.D
            if self.consecutive_defections > self.game_description.n_players / 2:
                self.consecutive_defections = 0
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1 or prev_cooperators < self.game_description.m:
            return Action.C

class Strategy_COLLECTIVE_40(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma strategy.

    This strategy balances individual self-interest with collective risk management by adapting to the history of player actions.
    It encourages cooperation in the initial round, rewards continued cooperation when the threshold is met or close to being met,
    and introduces a punishment mechanism to prevent exploitation and maintain cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_period = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperation_quotient = num_cooperators_prev_round / (self.game_description.n_players - 1)
        if self.punishment_period > 0:
            action = Action.D
            self.punishment_period -= 1
        elif cooperation_quotient >= (self.game_description.m - 1) / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if num_cooperators_prev_round < self.game_description.m and self.punishment_period == 0:
            self.punishment_period = 1
        if state.round_number >= 3 and all(history.my_actions[-3:] == False) and (self.punishment_period > 0):
            self.punishment_period = 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_42(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) strategy for the Collective Risk Dilemma game.
    
    This strategy prioritizes collective convergence by adapting to the cooperation rate in previous rounds.
    By cooperating when the threshold is met or exceeded, players create an incentive for others to cooperate as well.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def count_C_actions(self, round_history: NDArray[np.bool_]) -> int:
        """
        Count the number of C actions in a given round history.
        
        Parameters:
        - round_history (NDArray[np.bool_]): Boolean array representing the actions taken by all players in the previous round.
            True/1 means COOPERATE (Action.C) and False/0 means DEFECT (Action.D).
        
        Returns:
        int: The number of C actions in the given round history.
        """
        return sum(round_history)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            p_init = self.m / self.n
            return Action.C if random.random() < p_init else Action.D
        c_prev = self.count_C_actions(history.opponent_actions[-1, :]) / self.n
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif c_prev >= self.m / self.n:
            p_coop = self.k / (self.k + 1)
            return Action.C if random.random() < p_coop else Action.D
        else:
            p_defect = 1 - self.k / (self.k + 1)
            return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_43(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.contribution_rate_threshold = game_description.m / game_description.n_players
        self.cooperation_likelihood = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        current_contribution_rate = cooperators / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if current_contribution_rate >= self.contribution_rate_threshold:
            return Action.C
        defect_probability = (self.contribution_rate_threshold - current_contribution_rate) / self.contribution_rate_threshold
        if random.random() < defect_probability * (1 - self.cooperation_likelihood):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_44(BaseStrategy):
    """
    A strategy that adapts to the game's history by balancing individual self-interest with collective trust.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.trust_index = 0.5
        self.alpha = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperative_actions = sum(history.opponent_actions[-1, :]) + 1
        total_actions = self.game_description.n_players
        self.trust_index = self.alpha * self.trust_index + (1 - self.alpha) * (cooperative_actions / total_actions)
        if state.round_number <= self.game_description.n_rounds // 2:
            return Action.C if self.trust_index >= 0.5 else Action.D
        else:
            return Action.C if self.trust_index >= 0.7 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff_cooperation = self.game_description.k * (self.game_description.n_players - 1)
            private_payoff = self.game_description.n_players
            if total_payoff_cooperation < private_payoff:
                return Action.D

class Strategy_COLLECTIVE_45(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma game that adapts to the cooperation rate of opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = game_description.m / game_description.n_players
        self.CR_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        CR = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        self.CR_history.append(CR)
        if len(self.CR_history) >= 5 and all((cr > self.game_description.m / self.game_description.n_players for cr in self.CR_history[-5:])):
            return Action.C
        if state.round_number < int(0.1 * self.game_description.n_rounds) and CR < self.game_description.m / self.game_description.n_players:
            return Action.D
        if CR >= self.T:
            return Action.C
        else:
            defect_prob = (1 - CR) / (1 - self.T)
            return Action.D if random.random() < defect_prob else Action.C
        alpha = 0.5
        self.T = alpha * self.T + (1 - alpha) * CR
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_46(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    
    This strategy aims to balance individual self-interest with collective cooperation by adapting 
    cooperation rates based on the previous round's outcome and encouraging initial cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        prev_cooperators = sum(history.my_actions[-1] + history.opponent_actions[-1, :])
        if prev_cooperators < self.game_description.m:
            cooperate_prob = (self.game_description.m - prev_cooperators) / (self.game_description.n_players - prev_cooperators)
        else:
            cooperate_prob = 1
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = sum(history.my_payoffs)
            return Action.C if total_payoff * self.game_description.k > self.game_description.n_players else Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_47(BaseStrategy):
    """
    The ACG strategy is designed to balance individual self-interest with collective risk management,
    adapting to the game's history and parameters. This strategy prioritizes cooperation while being 
    robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_payoff = 0
        self.coop_prob = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_coop_count = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_coop_count >= self.game_description.m:
            action = Action.C
        else:
            self.coop_prob = (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players
            action = Action.C if random.random() < self.coop_prob else Action.D
        if history.my_payoffs.size > 0:
            self.avg_payoff += (history.my_payoffs[-1] - self.avg_payoff) / state.round_number
        threshold = 1 + self.game_description.k / 2
        if self.avg_payoff < threshold:
            self.coop_prob = min(1, self.coop_prob + 0.1)
        elif self.avg_payoff > threshold and state.round_number != self.game_description.n_rounds - 1:
            self.coop_prob = max(0, self.coop_prob - 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_48(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 2:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_round_coops >= self.game_description.m:
            return Action.C
        elif prev_round_coops > sum(history.opponent_actions[-2, :]) + history.my_actions[-2]:
            return Action.C
        else:
            if state.round_number == self.game_description.n_rounds - 1 and prev_round_coops == self.game_description.m - 1:
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_49(BaseStrategy):
    """
    A collective strategy that balances individual and collective interests 
    while adapting to changing opponent behaviors in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < min(5, self.game_description.n_rounds):
            return Action.C
        if history is None:
            return Action.D
        num_cooperators = sum(history.opponent_actions[-1, :])
        coop_payoffs = np.extract(history.my_actions, history.my_payoffs)
        avg_coop_payoff = coop_payoffs.mean() if len(coop_payoffs) > 0 else 0
        if num_cooperators > self.threshold:
            self.threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if any((sum(round) >= self.game_description.m for round in history.opponent_actions)) else Action.D
        elif len(history.my_payoffs) < self.threshold:
            return Action.C
        return Action.C if avg_coop_payoff >= 1 and num_cooperators >= self.threshold else Action.D

class Strategy_COLLECTIVE_50(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy.

    This strategy prioritizes collective cooperation while adapting to the behavior of other players.
    It initially cooperates, monitors contributions, and adjusts its behavior to balance individual and collective interests.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_action = Action.C
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.n_rounds // 4:
            return Action.C
        if history is not None:
            m_t = sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == True)
        else:
            m_t = self.game_description.m // 2
        if m_t >= self.game_description.m:
            if self.prev_action == Action.C:
                return Action.C
            elif self.prev_action == Action.D:
                return Action.C
        else:
            p = (self.game_description.m - m_t) / self.game_description.n_players
            if self.prev_action == Action.C and random.random() < p:
                return Action.D
            elif self.prev_action == Action.D:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if m_t == self.game_description.m - 1 and self.prev_action == Action.D:
            return Action.C if random.random() < 0.5 else Action.D
        self.prev_action = Action.C
        return Action.C

class Strategy_COLLECTIVE_51(BaseStrategy):
    """
    Adaptive Collective Risk (ACR) strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective well-being by adapting to the number of cooperators and defectors in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_last = 0
        self.d_last = 0
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m_last = sum(history.opponent_actions[-1, :] + [True])
        d_last = self.game_description.n_players - m_last
        if m_last >= self.game_description.m:
            action = True
        else:
            p = (1 - m_last / self.game_description.n_players) * (1 - self.game_description.k / (self.game_description.k + 1))
            action = random.random() < p
        if action:
            self.m_last += 1
        else:
            self.d_last += 1
        if m_last < self.game_description.m:
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = 0
        if self.consecutive_defections > math.sqrt(self.game_description.n_rounds):
            action = False
        if state.round_number == self.game_description.n_rounds - 1 and m_last >= self.game_description.m:
            return Action.C
        if self.game_description.m > self.game_description.n_players / 2:
            p = (1 - m_last / self.game_description.n_players) * (1 - self.game_description.k / (2 * self.game_description.k + 1))
            action = random.random() < p
        return Action.C if action else Action.D

class Strategy_COLLECTIVE_52(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game.
    This strategy balances individual self-interest with the need for cooperation to achieve a higher collective payoff.
    It adapts its cooperation probability based on the success rate of cooperation over time.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize the ATBC strategy.

        Args:
            game_description (CollectiveRiskDescription): A dataclass containing the game's parameters.
                - n_players: The number of players in the game.
                - n_rounds: The number of rounds in the game.
                - m: The minimum number of cooperators required to achieve a higher collective payoff.
                - k: The reward factor if the threshold is met.

        Returns:
            None
        """
        self.game_description = game_description
        self.k_success = 0
        self.w = 5
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the next action based on the current game state and history.

        Args:
            state (GameState): The current game state.
                - round_number: The current round number.

            history (None | PlayerHistory): The player's history in the game, or None if it's the first round.
                - my_actions: A boolean array of this player's actions, indexed by round.
                - my_payoffs: A float array of this player's payoffs, indexed by round.
                - opponent_actions: A 2D boolean array of opponents' actions, indexed by [round, player].
                - opponent_payoffs: A 2D float array of opponents' payoffs, indexed by [round, player].

        Returns:
            Action: The next action to take (either COOPERATE or DEFECT).
        """
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        m_prev = sum(history.opponent_actions[-1, :] == True)
        if m_prev >= self.game_description.m:
            self.k_success += 1
        else:
            self.k_success -= 1
        if self.k_success > self.w / 2:
            self.p_coop = min(1, self.p_coop + 0.1)
        elif self.k_success < self.w / 4:
            self.p_coop = max(0, self.p_coop - 0.1)
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.C
        return Action.C if random.random() < self.p_coop else Action.D

class Strategy_COLLECTIVE_53(BaseStrategy):
    """
    A collective strategy that adapts to the history of play and robustly handles various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        threshold_met = num_cooperators >= self.game_description.m
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.C
        if history.my_actions.shape[0] > 1 and threshold_met and (num_cooperators >= self.game_description.m):
            return Action.C
        else:
            p = 1 - num_cooperators / self.game_description.n_players
            if random.random() < p:
                return Action.D
            else:
                return Action.C
        if history.opponent_actions.shape[0] == 1 and history.my_payoffs.shape[0] > 0:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions, axis=0)
        opponent_averages = np.mean(history.opponent_payoffs, axis=0)
        sorted_opponents = np.argsort(-opponent_cooperations * opponent_averages)
        for opponent in sorted_opponents:
            if history.opponent_actions[-1, opponent]:
                return Action.C

class Strategy_COLLECTIVE_54(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) Strategy for Collective Risk Dilemma.

    This strategy adapts to the observed behavior of other players by:
    1. Initially probing for cooperation and adjusting accordingly.
    2. Gradually escalating cooperation in response to increasing average cooperation rates.
    3. Cooperating in the final round if the average cooperation rate is sufficient, as there's no risk of future retaliation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize ACE strategy with game description.

        Args:
            game_description (CollectiveRiskDescription): Game parameters.
        """
        self.game_description = game_description
        self.prev_round_coop = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determine action based on current state and history.

        Args:
            state (GameState): Current game state.
            history (None | PlayerHistory): History of actions and payoffs.

        Returns:
            Action: Chosen action (C or D).
        """
        if state.round_number == 0:
            coop_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < coop_prob else Action.D
        prev_round_coop_rates = history.opponent_actions[-1, :]
        ACR = np.mean(prev_round_coop_rates)
        if state.round_number <= self.game_description.n_rounds // 2:
            coop_prob = ACR + 0.5 * (1 - ACR)
        elif state.round_number < self.game_description.n_rounds - 1:
            coop_prob = ACR + 0.75 * (1 - ACR)
        else:
            coop_prob = 1.0 if ACR >= self.game_description.m / self.game_description.n_players else 0.0
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_COLLECTIVE_55(BaseStrategy):
    """
    A collective strategy designed to balance individual self-interest with collective risk aversion in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5 if game_description.m / game_description.n_players > 0.7 else 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        prev_round_my_action = history.my_actions[-1]
        prev_round_payoffs = history.opponent_payoffs[-1, :]
        cooperation_rate = np.sum(prev_round_opponent_actions) / self.game_description.n_players
        coop_payoff = np.sum(prev_round_payoffs[prev_round_opponent_actions])
        if cooperation_rate == self.cooperation_threshold:
            return Action.C if random.random() < 0.5 else Action.D
        if cooperation_rate >= self.cooperation_threshold or coop_payoff > (self.game_description.n_players - self.game_description.m) * (1 + self.game_description.k):
            return Action.C
        if state.round_number == self.game_description.n_rounds:
            if np.sum(prev_round_opponent_actions) >= self.game_description.m or (self.game_description.n_rounds == 2 and history.my_actions[0]):
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_56(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with group cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_score = 0
        self.rounds_since_last_success = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperations = sum(history.opponent_actions[-1, :]) + 1
        previous_round_success = previous_round_cooperations >= self.game_description.m
        if previous_round_success:
            self.cooperation_score += previous_round_cooperations
            self.rounds_since_last_success = 0
        else:
            self.rounds_since_last_success += 1
        if self.cooperation_score > self.game_description.m and self.game_description.m < self.game_description.n_players - 1:
            self.game_description.m += 1
        elif self.cooperation_score < self.game_description.m and self.game_description.m > 1:
            self.game_description.m -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if previous_round_success or (state.round_number < self.game_description.n_rounds - 1 and random.random() > (1 - self.game_description.k / (self.game_description.k + 1)) ** self.rounds_since_last_success):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_57(BaseStrategy):
    """
    The Adaptive Cooperative Threshold Strategy balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m
        self.cooperation_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_players == 2:
            return self.handle_two_player_game(state, history)
        if state.round_number == 0 or history is None:
            return Action.C
        if self.previous_round_threshold_met(history):
            action = Action.C
        else:
            action = Action.D
        if self.collective_reward_achieved(history):
            self.threshold += 1
        else:
            self.threshold -= 1
            if self.threshold < self.game_description.m:
                self.threshold = self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

    def handle_two_player_game(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        else:
            opponent_action = history.opponent_actions[-1, 0]
            return Action.C if opponent_action else Action.D

    def previous_round_threshold_met(self, history: PlayerHistory) -> bool:
        cooperators = sum(history.opponent_actions[-1, :]) + 1
        return cooperators >= self.threshold

    def collective_reward_achieved(self, history: PlayerHistory) -> bool:
        total_payoff = sum(history.my_payoffs[-1:]) + sum(history.opponent_payoffs[-1, :])
        return total_payoff > self.game_description.n_players * self.game_description.k

class Strategy_COLLECTIVE_58(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective cooperation,
    adapting to the dynamics of the game while promoting a culture of mutual support.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            action = True
        else:
            prev_cooperators = sum(history.opponent_actions[-1, :])
            if prev_cooperators >= self.game_description.m:
                action = True
            elif random.random() < (self.game_description.m - prev_cooperators) / self.game_description.n_players:
                action = False
            else:
                action = True
            for opponent_action in history.opponent_actions[-1, :]:
                if not opponent_action and random.random() < 0.5:
                    action = False
            if state.round_number >= self.game_description.n_rounds - 2:
                prev_cooperators = sum(history.opponent_actions[-1, :])
                if prev_cooperators < self.game_description.m:
                    action = False
        self.history.append(action)
        return Action.C if action else Action.D

class Strategy_COLLECTIVE_59(BaseStrategy):
    """
    A collective strategy called "Adaptive Cooperative Threshold" (ACT) that balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            coop_prob = (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players
            return Action.C if random.random() < coop_prob else Action.D
        self.prev_coop_count = sum((1 for a in history.my_actions if a == True))
        prev_round_opponent_coops = sum(history.opponent_actions[-1, :])
        if self.game_description.k > 2 and prev_round_opponent_coops >= self.game_description.m - 1:
            return Action.C
        elif self.prev_coop_count < self.game_description.m:
            return Action.D
        else:
            return Action.C if state.round_number != self.game_description.n_rounds else Action.D

class Strategy_COLLECTIVE_60(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for Collective Risk Dilemma.
    This strategy balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_adj_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        t = state.round_number
        if t == 0:
            return Action.C if random.random() < m / n else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        CS = np.sum(opponent_actions) + (history.my_actions[-1] == True)
        self.threshold_adj_count += 1
        if self.threshold_adj_count >= r // 2:
            avg_cooperation_rate = np.mean(history.my_actions)
            if avg_cooperation_rate < 0.5 and m < n - 1:
                m += 1
            elif avg_cooperation_rate > 0.8 and m > 1:
                m -= 1
            self.threshold_adj_count = 0
        cooperate_prob = max(0.5, CS / n) if CS >= m else min(1, (n - CS) / n)
        if t == r - 1:
            return Action.C
        if CS == m:
            cooperate_prob = 0.5
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_61(BaseStrategy):
    """
    Adaptive Cooperativity (AC) strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective well-being, 
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = 0.5
        self.rounds_since_success = 0
        self.adaptive_defect_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 0.1 * self.game_description.n_rounds:
            return Action.C
        prev_round_payoffs = history.my_payoffs[-1] + history.opponent_payoffs[-1]
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.game_description.m and np.mean(prev_round_payoffs[history.opponent_actions[-1, :]]) > np.mean(prev_round_payoffs[~history.opponent_actions[-1, :]]):
            self.rounds_since_success = 0
            return Action.C
        punishment_prob = self.rounds_since_success / self.game_description.n_rounds * 0.5 if self.game_description.n_rounds != 0 else 0
        action = random.random() < punishment_prob
        self.rounds_since_success += 1
        for opponent_action in history.opponent_actions[-1, :]:
            self.p_coop = max(0, min(self.p_coop + (opponent_action * 2 - 1) / 10, 1))
        if self.p_coop < 0.3 and self.adaptive_defect_rounds == 0:
            self.adaptive_defect_rounds = int(0.1 * self.game_description.n_rounds)
            return Action.D
        self.adaptive_defect_rounds -= 1
        return Action.C if action else Action.D

    def _get_payoff(self, history: PlayerHistory):
        return history.my_payoffs[-1]

class Strategy_COLLECTIVE_62(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy prioritizes cooperation while adapting to the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = self.game_description.m / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperation_probability else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if previous_round_cooperators >= self.game_description.m:
            self.cooperation_probability = min(self.cooperation_probability * 1.1, 1)
        else:
            self.cooperation_probability = max(self.cooperation_probability * 0.9, self.game_description.m / self.game_description.n_players)
        for opponent_actions in history.opponent_actions[-1, :]:
            if not opponent_actions and previous_round_cooperators < self.game_description.m:
                pass
        if state.round_number == self.game_description.n_rounds - 1:
            cooperators_in_half = sum(np.sum(history.opponent_actions + history.my_actions[:, None], axis=0) >= self.game_description.m) / state.round_number
            return Action.C if cooperators_in_half > 0.5 else Action.D
        return random.choice([Action.C, Action.D]) if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_63(BaseStrategy):
    """
    The Adaptive Cooperation (AC) strategy balances individual self-interest with collective well-being by adaptively adjusting cooperation levels based on game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round = history.opponent_actions[-1]
        num_cooperators = sum(prev_round)
        if self.game_description.m == self.game_description.n_players or num_cooperators >= self.game_description.m:
            return Action.C
        elif num_cooperators > 0:
            p_c = (self.game_description.k - 1) / (self.game_description.n_players - self.game_description.m + self.game_description.k)
            if random.random() < p_c:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and any(history.opponent_actions[:, :].sum(axis=0) > 0):
            return Action.C

class Strategy_COLLECTIVE_64(BaseStrategy):
    """
    Adaptive Cooperate-to-Reward (ACR) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] | np.array([True]))
        previous_defectors = self.game_description.n_players - previous_cooperators
        if previous_cooperators >= self.game_description.m:
            p = self.game_description.k * previous_cooperators / (self.game_description.n_players * self.game_description.k)
            return Action.C if random.random() < p else Action.D
        else:
            q = previous_defectors / self.game_description.n_players
            return Action.D if random.random() < q else Action.C
        total_payoff = sum(history.my_payoffs) + np.sum(history.opponent_payoffs, axis=0).mean()
        threshold_payoff = self.game_description.k * self.game_description.m / 2
        if total_payoff < threshold_payoff:
            return Action.C

class Strategy_COLLECTIVE_65(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that is adaptive and robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_history = history.opponent_actions[state.round_number - 1]
        self.prev_coop_count = sum(prev_round_history)
        if random.random() < 0.05:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            if self.prev_coop_count >= math.ceil(self.game_description.m / 2):
                return Action.C
            else:
                return Action.D
        if self.prev_coop_count >= math.ceil(self.game_description.m / 2):
            return Action.C
        elif self.prev_coop_count < self.game_description.m / 2:
            return Action.D
        if sum(history.opponent_actions[state.round_number - 1]) == 0:
            return Action.C if random.random() < 0.75 else Action.D

class Strategy_COLLECTIVE_66(BaseStrategy):
    """
    Adaptive Collective Responsibility (ACR) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.h = 3
        self.delta_p = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.m / self.n_players else Action.D
        recent_cooperations = np.sum(history.opponent_actions[-min(self.h, state.round_number):, :], axis=0)
        average_cooperation_ratio = np.mean(recent_cooperations / min(self.h, state.round_number))
        p_c = average_cooperation_ratio / (1 + self.k * (self.m - 1)) if average_cooperation_ratio > 0.5 else 0
        my_actions = np.array([action == Action.C for action in history.my_actions])
        opponents_actions = history.opponent_actions[-1, :]
        if state.round_number > 1:
            previous_round_cooperators = sum(opponents_actions) + int(my_actions[-2])
            if previous_round_cooperators < self.m and (not my_actions[-2]):
                p_c += self.delta_p
            elif previous_round_cooperators >= self.m and my_actions[-2]:
                p_c -= self.delta_p
        if state.round_number == self.game_description.n_rounds - 1:
            p_c += self.delta_p
        return Action.C if random.random() < max(0, min(p_c, 1)) else Action.D

class Strategy_COLLECTIVE_67(BaseStrategy):
    """
    This strategy balances individual self-interest with collective well-being,
    adapting to the game's history and parameters. It prioritizes cooperation
    while being robust to a wide range of opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_ratio = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_round_cooperators >= self.game_description.m:
            action = Action.C
        else:
            self.cooperation_ratio = np.mean(history.opponent_actions + history.my_actions[:, None] > 0)
            p = max(0.01, min(1 - self.cooperation_ratio, 0.99))
            action = Action.C if random.random() < p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_68(BaseStrategy):
    """
    A cooperative strategy for Collective Risk Dilemma that adapts to the group's behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_ratio = 0.0
        self.cooperation_prev_round = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            prob_coop = self.game_description.m / self.game_description.n_players
            cooperate = random.random() < prob_coop
        else:
            cr_prev_round = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
            prob_coop = (cr_prev_round >= self.game_description.m / self.game_description.n_players) * (self.game_description.k / (self.game_description.k + 1)) + (cr_prev_round < self.game_description.m / self.game_description.n_players) * ((1 + self.game_description.k) / (2 + self.game_description.k))
            if state.round_number == self.game_description.n_rounds - 1:
                prob_coop = cr_prev_round >= self.game_description.m / self.game_description.n_players
            cooperate = random.random() < prob_coop
            self.cooperation_prev_round.append(cooperate)
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_69(BaseStrategy):
    """
    The Adaptive Collective Cooperation strategy aims to balance individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.consecutive_coop = 0
        self.collective_coop_count = 0
        self.action_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_action_prev = history.my_actions[-1]
        collective_coop_count = sum(opponent_actions) + (my_action_prev == True)
        if self.action_prev is not None:
            self.consecutive_coop += 1 if self.action_prev else -1
        if collective_coop_count >= self.game_description.m and my_action_prev == True:
            action = Action.C
        elif collective_coop_count < self.game_description.m and my_action_prev == True:
            action = Action.D
        elif collective_coop_count >= self.game_description.m and my_action_prev == False:
            action = Action.C
        elif collective_coop_count < self.game_description.m and my_action_prev == False:
            if self.consecutive_coop <= -2:
                self.coop_prob *= 0.8
            action = Action.D
        else:
            action = Action.C
        if self.consecutive_coop >= 2 and self.coop_prob < 0.9:
            self.coop_prob *= 1.1
        self.action_prev = my_action_prev
        self.collective_coop_count = collective_coop_count
        return action

class Strategy_COLLECTIVE_70(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective well-being by adapting cooperation based on observed behavior and game parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m / game_description.n_players
        self.delta_ct = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        proportion_cooperators = cooperators / self.game_description.n_players
        if proportion_cooperators >= self.cooperation_threshold:
            self.cooperation_threshold += self.delta_ct
        else:
            self.cooperation_threshold -= self.delta_ct
        cooperate_prob = self.cooperation_threshold
        return Action.C if random.random() < cooperate_prob and state.round_number < self.game_description.n_rounds - 1 else Action.D

class Strategy_COLLECTIVE_71(BaseStrategy):
    """
    Adaptive Cooperative Threshold Strategy for Collective Risk Dilemma.

    This strategy adapts to the game's dynamics by adjusting the cooperation threshold based on the history of cooperation.
    It prioritizes cooperation when it is likely to succeed, while avoiding exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        prev_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] and 1 or 0)
        if self.cooperation_threshold is None:
            self.cooperation_threshold = self.game_description.m - 1 + math.floor((self.game_description.n_players - self.game_description.m + 1) / 2)
        if prev_coops >= self.game_description.m:
            self.cooperation_threshold -= 1
        elif prev_coops < self.game_description.m:
            self.cooperation_threshold += 1
        if prev_coops == 1 and (not history.my_actions[-1]):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if any((sum(history.opponent_actions[i, :]) + (history.my_actions[i] and 1 or 0) >= self.game_description.m for i in range(state.round_number))):
                return Action.C
            else:
                return Action.D
        if prev_coops >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_72(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy aims to balance individual 
    self-interest with collective cooperation to maximize overall payoffs. It adapts 
    to the game's history and opponent behaviors while maintaining a robust, non-exploitable approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.game_description.m == 1:
            return Action.C
        if self.game_description.k <= 1:
            return Action.D
        prev_coop_count = sum([1 for h in history.opponent_actions[-1, :] if h])
        if state.round_number < self.game_description.n_rounds - 1:
            if prev_coop_count >= self.game_description.m:
                return Action.C
            else:
                p_defect = (self.game_description.m - prev_coop_count) / self.game_description.n_players
                return Action.D if random.random() < p_defect else Action.C
        return Action.D

class Strategy_COLLECTIVE_73(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Threshold (ACT)
    
    The ACT strategy aims to balance individual self-interest with collective risk management, 
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_players == 2:
            return Action.C
        if self.game_description.m == self.game_description.n_players:
            return Action.C
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if prev_cooperators >= self.game_description.m or (prev_cooperators < self.game_description.m and np.mean([p for p, a in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if a]) > np.mean([p for p, a in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if not a])):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and prev_cooperators >= self.game_description.m:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_74(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    It encourages cooperation while being robust against exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_successes_threshold = math.floor(game_description.n_rounds / 4)
        self.recent_failures_threshold = math.ceil(game_description.n_rounds / 8)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players // 2 or self.game_description.n_players == 2 or (self.game_description.m in [1, 2] and self.game_description.n_players > 4):
                return Action.C
            else:
                return Action.D
        if history.my_actions.size == state.round_number - 1:
            return Action.C
        consecutive_successes = sum([all(history.opponent_actions[i, :] | history.my_actions[i]) and self.game_description.m <= np.sum(history.opponent_actions[i, :] | history.my_actions[i]) for i in range(max(0, state.round_number - self.consecutive_successes_threshold), state.round_number)])
        recent_failures = sum([not all(history.opponent_actions[i, :] | history.my_actions[i]) or not self.game_description.m <= np.sum(history.opponent_actions[i, :] | history.my_actions[i]) for i in range(max(0, state.round_number - self.recent_failures_threshold), state.round_number)])
        if consecutive_successes >= self.consecutive_successes_threshold:
            return Action.C
        elif recent_failures >= self.recent_failures_threshold:
            return Action.D
        avg_payoff_threshold_met = np.mean([history.my_payoffs[i] for i in range(state.round_number) if all(history.opponent_actions[i, :] | history.my_actions[i]) and self.game_description.m <= np.sum(history.opponent_actions[i, :] | history.my_actions[i])])
        avg_payoff_threshold_not_met = np.mean([history.my_payoffs[i] for i in range(state.round_number) if not all(history.opponent_actions[i, :] | history.my_actions[i]) or not self.game_description.m <= np.sum(history.opponent_actions[i, :] | history.my_actions[i])])
        if avg_payoff_threshold_met > avg_payoff_threshold_not_met:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.game_description.m / 2 or (state.round_number == self.game_description.n_rounds - 1 and all(history.opponent_actions[-1, :] | history.my_actions[-1]) and (self.game_description.m <= np.sum(history.opponent_actions[-1, :] | history.my_actions[-1]))):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_75(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        last_round = history.opponent_actions[-1]
        c = sum(last_round)
        if c >= self.game_description.m:
            p_coop = (1 + self.game_description.k) / (2 + self.game_description.k)
            return Action.C if random.random() < p_coop else Action.D
        else:
            p_defect = (self.game_description.n_players - self.game_description.m) / self.game_description.n_players
            return Action.D if random.random() < p_defect else Action.C
        self.adjust_probability(c, self.game_description.m, self.game_description.n_players, self.game_description.k)

    def adjust_probability(self, c: int, m: int, n: int, k: float):
        if c >= m:
            self.coop_prob += (k - 1) / (2 + k)
        else:
            self.coop_prob -= (m - c) / n

class Strategy_COLLECTIVE_76(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Adjustment (ACTA) strategy.
    
    Cooperate in the first round to establish a baseline of cooperation, 
    then adaptively cooperate based on the observed cooperation rate. 
    Adjust the threshold value every 5 rounds to maintain alignment with 
    the collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_adjustment_rounds = []
        self.cooperation_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if len(self.threshold_adjustment_rounds) > 0 and (state.round_number - self.threshold_adjustment_rounds[-1]) % 5 == 0:
            avg_coop_rate = np.mean(np.array([sum(history.opponent_actions[i, :]) for i in range(max(0, state.round_number - 4), state.round_number)]))
            if avg_coop_rate > 0.7 and self.game_description.m > 1:
                self.game_description.m -= 1
            elif avg_coop_rate < 0.3 and self.game_description.m < self.game_description.n_players - 1:
                self.game_description.m += 1
        if prev_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_77(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma (ACRD) strategy.

    This strategy prioritizes cooperation when the collective benefit is within reach,
    while adapting to the behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.p_prev = (opponent_cooperators + 1) / self.game_description.n_players if history is not None else None
        if history is not None and history.my_payoffs[-1] == self.game_description.k:
            self.p_prev += 0.05
        elif history is not None:
            self.p_prev -= 0.05
        if self.p_prev > 0.9:
            self.p_prev = 1
        elif self.p_prev < 0.1:
            self.p_prev = 0
        if state.round_number == self.game_description.n_rounds - 1 and any(history.my_payoffs >= self.game_description.k):
            return Action.C
        if opponent_cooperators == self.game_description.m - 1:
            return Action.C if random.random() < 0.5 else Action.D
        return Action.C if random.random() < self.p_prev else Action.D

class Strategy_COLLECTIVE_78(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for Collective Risk Dilemma.

    This strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_c = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_c else Action.D
        prev_outcome = np.sum(history.opponent_actions[-1, :])
        if prev_outcome >= self.game_description.m:
            action = Action.C if random.random() < self.p_c else Action.D
        else:
            action = Action.D if random.random() < 1 - self.p_c else Action.C
        collective_payoff = np.sum(history.my_payoffs)
        if collective_payoff > 0 and state.round_number > 1:
            prev_payoff = history.my_payoffs[-2]
            self.p_c += 0.05 if collective_payoff >= prev_payoff else -0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif state.round_number < self.game_description.m:
            return Action.D
        return action

class Strategy_COLLECTIVE_79(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy for the Collective Risk Dilemma.
    
    This strategy encourages cooperation while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = []
        self.exploration_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            return Action.C
        prev_cooperators_count = sum(history.opponent_actions[-1, :] + [True])
        self.prev_cooperators.append(prev_cooperators_count)
        if state.round_number == 1:
            return Action.C
        elif self.exploration_phase and len(self.prev_cooperators) < self.game_description.m:
            p_explore = 0.5
            action = Action.C if random.random() < p_explore else Action.D
        elif prev_cooperators_count >= self.game_description.m:
            action = Action.C
        else:
            punishment_rounds = min(2, len(self.prev_cooperators))
            action = Action.D if punishment_rounds > 0 else Action.C
        if state.round_number == self.game_description.n_rounds - 1 and prev_cooperators_count >= self.game_description.m - 1:
            return Action.C
        self.exploration_phase = len(self.prev_cooperators) < self.game_description.m and (not self.exploration_phase)
        return action

class Strategy_COLLECTIVE_80(BaseStrategy):
    """
    A strategy that adapts its cooperation threshold based on the game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.H_c = 0
        self.H_d = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators = self.game_description.m
        k_reward = self.game_description.k
        r_rounds = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            cooperate_prob = m_cooperators / n_players
        else:
            T = (self.H_c + 1) / (self.H_c + self.H_d + 2) if k_reward > 2 else m_cooperators / n_players
            cooperate_prob = T
        if state.round_number == r_rounds - 1:
            return Action.D
        action = Action.C if random.random() < cooperate_prob else Action.D
        if history is not None and len(history.my_actions) > 0:
            num_cooperators = sum(history.opponent_actions[-1, :] == True)
            if num_cooperators >= m_cooperators:
                self.H_c += 1
            else:
                self.H_d += 1
        return action

class Strategy_COLLECTIVE_82(BaseStrategy):
    """
    Collective Risk Dilemma strategy using Adaptive Threshold Cooperation (ATC).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.beta = 1.2
        self.delta_alpha = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        prev_coop_rate = sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1)
        goal_achieved = sum(history.opponent_actions[-1, :] + history.my_actions[-1:]) >= self.game_description.m
        self.alpha += self.delta_alpha * (goal_achieved - 0.5)
        cooperate_prob = self.beta * prev_coop_rate if prev_coop_rate >= self.alpha * self.game_description.m / self.game_description.n_players else 0
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_83(BaseStrategy):
    """
    Adaptive Collective Defense (ACD) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = game_description.m / game_description.n_players
        self.defect_response_prob = (game_description.n_players - game_description.m) / game_description.n_players
        self.reinforce_coop_prob = (game_description.k - 1) / (game_description.k + 1)
        self.adaptive_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_cooperators < self.adaptive_threshold:
            cooperate = random.random() >= self.defect_response_prob
        elif prev_cooperators >= self.adaptive_threshold:
            cooperate = random.random() < self.reinforce_coop_prob
        if prev_cooperators < self.adaptive_threshold:
            self.adaptive_threshold -= 1
        elif prev_cooperators >= self.adaptive_threshold:
            self.adaptive_threshold += 1
        self.adaptive_threshold = max(1, min(self.game_description.n_players - 1, self.adaptive_threshold))
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = prev_cooperators >= self.adaptive_threshold
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_84(BaseStrategy):
    """
    A hybrid strategy for the Collective Risk Dilemma game.
    
    This strategy combines elements of tit-for-tat, reputation tracking,
    and threshold-dependent cooperation to adapt to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k
        self.total_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coop_count = sum(history.opponent_actions[-1, :])
        coop_prob = min(self.m_threshold / self.n_players, 1 - (self.total_rounds - state.round_number) / self.total_rounds)
        reputation_scores = np.zeros((self.n_players,))
        for round_actions in history.opponent_actions:
            for i, action in enumerate(round_actions):
                if action == True:
                    reputation_scores[i] += 1
                else:
                    reputation_scores[i] -= 1
        if prev_round_coop_count >= self.m_threshold or np.any(reputation_scores > 0):
            coop_prob += 0.1
        if state.round_number == self.total_rounds - 1:
            return Action.D
        if random.random() < coop_prob:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_85(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: "Adaptive Collective Threshold" (ACT)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        cr = prev_coop_count / self.game_description.n_players
        if prev_coop_count < self.game_description.m:
            action = Action.D
        elif cr >= self.theta:
            action = Action.C
        else:
            prob_defect = (1 - cr) / (1 - self.theta)
            action = Action.D if random.random() < prob_defect else Action.C
        self.theta = 0.5 * (self.theta + cr)
        return action

class Strategy_COLLECTIVE_86(BaseStrategy):
    """
    Adaptive Cooperativity Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        k_reward_factor = self.game_description.k
        if state.round_number < 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperators = sum(history.my_actions) + np.sum(np.sum(history.opponent_actions, axis=0))
            if total_cooperators < m_cooperators_needed * (self.game_description.n_rounds - 1):
                return Action.C
            else:
                return Action.D
        cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_last_round < m_cooperators_needed:
            p_cooperate = min(0.5, (m_cooperators_needed - cooperators_last_round) / n_players)
        else:
            p_cooperate = max(0.5, (cooperators_last_round - m_cooperators_needed + 1) / n_players)
        return Action.C if random.random() < p_cooperate else Action.D

class Strategy_COLLECTIVE_87(BaseStrategy):
    """
    The AC strategy aims to balance individual payoffs with collective risk management,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.m:
            return Action.C
        if history is not None:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            total_cooperators = opponent_cooperators + int(bool(history.my_actions[-1]))
            cooperation_ratio = total_cooperators / self.game_description.n_players
            if cooperation_ratio > 0.5 or (state.round_number >= self.game_description.m and total_cooperators >= self.game_description.m):
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_88(BaseStrategy):
    """
    This strategy aims to balance individual self-interest with collective risk management by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0
        self.τ = game_description.m
        self.c_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        if history is not None:
            self.p = (self.p * (state.round_number - 1) + sum(history.opponent_actions[-1, :] == True)) / state.round_number
            if self.p >= self.game_description.m / self.game_description.n_players:
                self.τ = max(self.game_description.m - 1, 0)
            else:
                self.τ = self.game_description.m
        if history is not None and sum(history.opponent_actions[-1, :] == True) >= self.τ and (self.game_description.k > 1 + (self.game_description.n_players - self.game_description.m) / self.game_description.n_players):
            return Action.C
        else:
            return Action.D
        if history is not None:
            self.c_prev = sum(history.opponent_actions[-1, :] == True)

class Strategy_COLLECTIVE_89(BaseStrategy):
    """
    This strategy implements the "Adaptive Collective Responsibility" approach.
    It adapts to deviations from desired cooperation levels and rewards collective responsibility.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_round_cooperators = 0
        self.previous_round_reward = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number <= self.game_description.m:
            return Action.C
        if history is not None and len(history.my_actions) > 0:
            self.previous_round_cooperators = sum(history.opponent_actions[-1, :])
            previous_round_payoff = history.my_payoffs[-1]
            self.previous_round_reward = max(previous_round_payoff - 1, 0)
        p_defect = 0.5 * (1 - self.previous_round_cooperators / self.game_description.n_players)
        action = Action.D if random.random() < p_defect else Action.C
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if self.previous_round_reward > 0:
            p_cooperate = 0.5 * (1 + self.previous_round_reward / self.game_description.k)
            action = Action.C if random.random() < p_cooperate else Action.D
        return action

class Strategy_COLLECTIVE_90(BaseStrategy):
    """
    Adaptive Cooperation with Memory (ACM) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_outcome = history.opponent_actions[-1, :]
        num_cooperators = sum(prev_round_outcome)
        if self.game_description.n_players == 2 and self.game_description.m == 1:
            return Action.C
        if math.isclose(self.game_description.k, 1):
            return random.choice([Action.C, Action.D])
        if num_cooperators >= self.game_description.m:
            return Action.C
        elif 0 < num_cooperators < self.game_description.m:
            p = 1 - self.game_description.k / (self.game_description.n_players + self.game_description.k)
            if random.random() < p:
                return Action.D
            else:
                return Action.C
        return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            prev_prev_round_outcome = history.opponent_actions[-2, :]
            num_cooperators_last = sum(prev_prev_round_outcome)
            if num_cooperators_last < self.game_description.m:
                return Action.D

class Strategy_COLLECTIVE_91(BaseStrategy):
    """
    Collective Strategy: "Conditional Cooperator with Adaptive Threshold" (CCAT)

    Prioritizes collective success while maintaining individual self-interest.
    Adapts to the history of cooperation and uses probabilistic decisions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_cooperators >= self.game_description.m:
            my_action = True
        elif prev_cooperators > self.game_description.m / 2:
            p = (self.game_description.k - 1) / (self.game_description.n_players - self.game_description.m)
            my_action = random.random() < p
        elif state.round_number == self.game_description.n_rounds - 1 and prev_cooperators >= self.game_description.m - 1:
            my_action = True
        else:
            my_action = False
        return Action.C if my_action else Action.D

class Strategy_COLLECTIVE_92(BaseStrategy):
    """
    The Adaptive Collective Guardian (ACG) strategy balances individual self-interest with collective well-being.
    It adapts to the game's history and opponent behaviors, aiming for a robust balance between cooperation and self-protection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_cooperators = None
        self.last_payoff = None
        self.defect_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        last_round_actions = history.opponent_actions[-1, :]
        self.last_round_cooperators = sum(last_round_actions) + (history.my_actions[-1] == Action.C)
        self.last_payoff = history.my_payoffs[-1]
        if self.last_round_cooperators >= self.game_description.m and self.last_payoff >= 1 + self.game_description.k / 2:
            return Action.C
        if self.last_round_cooperators < self.game_description.m:
            self.defect_counter = min(2, self.defect_counter + 1)
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and any(last_round_actions):
            return Action.C
        self.defect_counter = max(0, self.defect_counter - 1)
        if self.defect_counter > 0:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_93(BaseStrategy):
    """
    Adaptive Collective Risk strategy for the Collective Risk Dilemma game.
    
    Initially cooperates, then adapts to the level of cooperation observed in previous rounds,
    prioritizing collective benefit when threshold is met or close to being met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize AdaptiveCollectiveRisk strategy.

        :param game_description: Game description containing parameters n_players, n_rounds, m, and k
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Make a decision based on the current game state and player history.

        :param state: Current game state (round number)
        :param history: Player's action and payoff history
        :return: Chosen action (Cooperate or Defect)
        """
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.game_description.m:
            action = Action.C
        elif prev_cooperators < math.ceil(self.game_description.m / 2):
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_94(BaseStrategy):
    """
    This strategy balances individual self-interest with collective risk aversion through an adaptive Cooperation Quotient threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_quotient_threshold = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players % 2 != 0 else Action.D
        cooperators_prev_round = sum(history.opponent_actions[-1, :])
        cooperation_quotient = (cooperators_prev_round + history.my_actions[-1]) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if cooperators_prev_round >= self.game_description.m else Action.D
        elif cooperation_quotient > self.cooperation_quotient_threshold or (cooperation_quotient == self.cooperation_quotient_threshold and cooperators_prev_round >= self.game_description.m):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_95(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for cooperation to achieve the collective goal.
    It adapts to the game's history and is robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        prev_cooperators = sum(prev_round_actions)
        prev_payoff = np.sum(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        if prev_cooperators >= self.game_description.m:
            return Action.C
        elif prev_cooperators < self.game_description.m and prev_payoff > 0:
            return Action.D
        elif prev_cooperators < self.game_description.m and prev_payoff <= 0:
            if any(prev_round_actions):
                return Action.C
            else:
                return Action.D
        elif state.round_number >= self.game_description.n_rounds - 2:
            return Action.C

class Strategy_COLLECTIVE_96(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_outcome = history.opponent_actions[-1, :]
        num_cooperators_prev = sum(prev_round_outcome)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if num_cooperators_prev < self.game_description.m:
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = 0
        p = (self.game_description.k - 1) / self.game_description.k + min(self.consecutive_defections * 0.1, 0.9)
        if num_cooperators_prev >= self.game_description.m:
            return Action.C if random.random() < p else Action.D
        else:
            q = self.game_description.m / self.game_description.n_players
            return Action.D if random.random() < q else Action.C

class Strategy_COLLECTIVE_97(BaseStrategy):
    """
    Adaptive Threshold Cooperativity Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_coops = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_coops >= self.game_description.m:
            self.threshold = self.game_description.m
        elif self.threshold < self.game_description.n_players - 1:
            self.threshold += 1
        r_is_odd = state.round_number % 2 == 1
        prev_action_was_d = not history.my_actions[-1]
        enough_coop_in_prev_round = prev_coops >= self.game_description.m
        if r_is_odd or (prev_coops < self.game_description.m and self.threshold >= self.game_description.m) or (prev_action_was_d and enough_coop_in_prev_round):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_98(BaseStrategy):
    """
    A collective strategy that adapts to the game's history by balancing individual self-interest with the need for collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.cumulative_payoff_C = 0
        self.cumulative_payoff_D = 0
        self.history_m_c = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            m_c = sum((1 for h in self.history_m_c if h >= self.game_description.m))
            p_C = self.alpha * (1 + (m_c - self.game_description.m) / (self.game_description.n_rounds - 1))
            action = Action.C if random.random() < p_C else Action.D
            cooperators_in_last_round = sum((1 for a in history.opponent_actions[-1, :] if a))
            self.history_m_c.append(cooperators_in_last_round + 1)
            payoff_C = 1 - 1 + self.game_description.k if cooperators_in_last_round + 1 >= self.game_description.m else 1 - 1
            payoff_D = 1 + self.game_description.k if cooperators_in_last_round >= self.game_description.m else 1
            self.cumulative_payoff_C += payoff_C
            self.cumulative_payoff_D += payoff_D
            self.alpha = max(self.alpha - 0.05, 0) if m_c < self.game_description.m else min(self.alpha + 0.05, 1)
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1 and self.cumulative_payoff_C > self.cumulative_payoff_D:
            action = Action.C
        return action

class Strategy_COLLECTIVE_99(BaseStrategy):
    """
    The Adaptive Cooperation strategy balances individual self-interest with collective well-being,
    adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.0
        self.prev_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        t = state.round_number - 1
        if t < r / 3:
            if self.prev_payoff > 1 + k / 2:
                return Action.C
            else:
                p = (m - 1) / n_players
                return Action.D if random.random() < p else Action.C
        elif t < 2 * r / 3:
            opponent_cooperation_rate = np.mean(history.opponent_actions[t - 1, :])
            if opponent_cooperation_rate > 0.5:
                return Action.C
            else:
                p = (m - 1) / n_players
                return Action.D if random.random() < p else Action.C
        elif t < r:
            current_cooperation_level = np.sum(history.opponent_actions[t, :] | history.my_actions[t])
            if current_cooperation_level >= m:
                return Action.C
            else:
                p = (m - 1) / n_players
                return Action.D if random.random() < p else Action.C
        elif t == r - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_100(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy adapts to the game's history and opponents' behaviors, 
    making it robust against various strategies. It balances individual self-interest with the need for collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_coop_count = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_coop_count >= self.game_description.m:
            return Action.C if prev_coop_count >= self.game_description.m / 2 else Action.D
        return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_101(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    
    The ACC strategy aims to balance individual self-interest with collective risk aversion by adapting to the game's history and parameters. It promotes cooperation while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        CT = self.game_description.m - num_cooperators_last_round
        expected_payoff_C = (0 if history.my_actions[-1] else 1) + self.game_description.k if num_cooperators_last_round >= self.game_description.m - 1 else 0
        expected_payoff_D = 1 if history.my_actions[-1] else 0
        if CT > 0 and expected_payoff_C > expected_payoff_D:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_102(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Threshold-based Punishment (ACTP)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperated_last_round = True
        self.punished_last_round = False
        self.consecutive_defection = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (self.consecutive_defection and all(history.opponent_actions[-1] == False)):
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        threshold_met = num_cooperators >= math.ceil(self.game_description.m / 2)
        if threshold_met:
            self.consecutive_defection = False
            return Action.C
        elif not self.punished_last_round and num_cooperators < self.game_description.m:
            self.punished_last_round = True
            self.cooperated_last_round = False
            return Action.D
        else:
            self.consecutive_defection = all(history.opponent_actions[-1] == False)
            if not self.consecutive_defection or history.my_actions[-1]:
                self.punished_last_round = False
                self.cooperated_last_round = True
                return Action.C
            self.cooperated_last_round = False
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        consecutive_cooperation = sum(history.my_actions[-3:]) == 3 and all((num_cooperators >= self.game_description.m for _ in range(3)))
        if consecutive_cooperation:
            return Action.C
        rare_event = num_cooperators < math.ceil(self.game_description.m / 4) and sum(history.my_actions[-10:]) / (state.round_number + 1) < 0.1
        if rare_event:
            self.game_description.m /= 2
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_103(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) Strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective cooperation, 
    adapting to the game's history and parameters. It encourages initial cooperation, 
    adjusts its cooperation threshold based on historical payoffs, and adapts to opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m
        self.avg_historical_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] & np.ones_like(history.opponent_actions[-1, :]))
        if prev_cooperators < self.cooperation_threshold:
            action = Action.D
        else:
            action = Action.C
            collective_payoff = sum(history.my_payoffs) + np.sum(history.opponent_payoffs, axis=0)
            avg_payoff = np.mean(collective_payoff)
            if avg_payoff < self.avg_historical_payoff:
                self.cooperation_threshold -= 1
            elif avg_payoff > self.avg_historical_payoff:
                self.cooperation_threshold += 1
            historical_cooperation_rate = np.mean(history.my_actions)
            if random.random() < historical_cooperation_rate:
                action = Action.C
        return action

class Strategy_COLLECTIVE_104(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Response (ATR)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.defect_count = 0
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        for opponent_action in history.opponent_actions[-1, :]:
            if opponent_action:
                self.coop_count += 1
            else:
                self.defect_count += 1
        if self.coop_count >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_105(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) Strategy for Collective Risk Dilemma.

    ACO aims to balance individual self-interest with collective risk management by adapting to the group's behavior over time.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.historical_cooperation_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_cooperations = np.sum(history.opponent_actions[:-1], axis=(0, 1))
        total_rounds = state.round_number - 1
        self.historical_cooperation_rate = (self.historical_cooperation_rate * (total_rounds - 1) + opponent_cooperations) / total_rounds
        if self.historical_cooperation_rate >= self.game_description.m / self.game_description.n_players:
            return Action.C if random.random() < (self.game_description.k - 1) / self.game_description.k else Action.D
        else:
            return Action.D if random.random() < self.game_description.m / self.game_description.n_players / self.historical_cooperation_rate else Action.C
        if random.random() < 0.05:
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_106(BaseStrategy):
    """
    The Adaptive Collective Risk strategy promotes collective cooperation by encouraging initial cooperation,
    observing and responding to the actions of others, and adapting to changing conditions while maintaining
    a focus on cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = (game_description.m - 1) / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            coop_prob = m / n
            return Action.C if random.random() < coop_prob else Action.D
        elif 1 <= state.round_number < r // 3:
            avg_coop_rate = np.mean(history.my_actions[:state.round_number])
            opponent_avg_coop_rates = np.mean(history.opponent_actions[:state.round_number, :], axis=0)
            if avg_coop_rate + np.sum(opponent_avg_coop_rates) > self.theta * n:
                return Action.C
            else:
                return Action.D
        else:
            previous_payoff = history.my_payoffs[state.round_number - 1]
            avg_payoff = np.mean(history.my_payoffs[max(0, state.round_number - r // 3):state.round_number])
            if previous_payoff >= avg_payoff:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_107(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if self.game_description.m == 1:
            return Action.C
        elif self.game_description.m == self.game_description.n_players:
            return Action.D
        p = max(0, 1 - num_cooperators_prev_round / self.game_description.m)
        if num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        elif random.random() < p:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_108(BaseStrategy):
    """
    This strategy prioritizes collective responsibility and adapts to the behavior of other players.
    It initially cooperates with a probability proportional to m/n, where m is the minimum number 
    of cooperators needed and n is the total number of players. If cooperation fails, it uses a 
    punishment mechanism to discourage free-riding. The learning component updates cooperation 
    probability based on past experiences, promoting a balance between cooperation and self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = 0.0
        self.p_punish = 1 - (game_description.k - 1) / (game_description.n_players - game_description.m + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.p_coop = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < self.p_coop else Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_round_coops >= self.game_description.m:
            action = Action.C
        elif prev_round_coops < self.game_description.m:
            action = Action.D if random.random() < self.p_punish else Action.C
        prev_action = history.my_actions[-1]
        prev_outcome = 'success' if prev_round_coops >= self.game_description.m else 'failure'
        if prev_action and prev_outcome == 'success':
            self.p_coop += 0.1
        elif not prev_action and prev_outcome == 'success':
            self.p_coop -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            prev_round_coops = sum(history.opponent_actions[-2, :]) + history.my_actions[-2]
            action = Action.C if prev_round_coops >= self.game_description.m - 1 else Action.D
        elif math.isclose(self.p_coop, 0.5):
            action = Action.C
        return action

class Strategy_COLLECTIVE_109(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    This strategy adapts to various opponent behaviors and depends only on game parameters and history.
    It promotes cooperation while adapting to the group's behavior, balancing individual self-interest with collective benefit.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + 1
        if state.round_number == self.game_description.n_rounds - 1:
            successful_cooperation = sum((1 for i in range(state.round_number) if sum((1 for action in history.opponent_actions[i, :] if action)) + 1 >= self.game_description.m))
            return Action.C if successful_cooperation >= state.round_number / 2 else Action.D
        if prev_round_cooperators >= self.game_description.m:
            p = (self.game_description.k - 1) / self.game_description.k
            return Action.C if random.random() < p else Action.D
        x = prev_round_cooperators - 1
        p = (self.game_description.m - x) / (self.game_description.n_players - self.game_description.m)
        return Action.C if random.random() > p else Action.D

class Strategy_COLLECTIVE_110(BaseStrategy):
    """
    A collective strategy called "Adaptive Collective Risk Management" (ACRM) 
    for the Collective Risk Dilemma game. It balances individual self-interest with 
    collective risk management by adapting to the evolving behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        round_number = state.round_number
        if round_number == 0:
            return Action.C if n_players > m else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        c_prev = sum(opponent_actions)
        cooperate_prob = max(0.2, min(0.8, 1 - (k - 1) / 10))
        if c_prev >= m:
            return Action.C
        if round_number > 1 and history.my_payoffs[-1] < self.game_description.k and (self.cooperate_count > round_number / 2):
            return Action.D
        if random.random() < cooperate_prob:
            self.cooperate_count += 1
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_111(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma game that adapts to the game's history 
    and prioritizes collective cooperation while preventing exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        m_threshold = self.game_description.m
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        majority_cooperated = opponent_cooperations >= (n_players + 1) // 2
        threshold_reached = opponent_cooperations == m_threshold - 1
        exploiter_detected = False
        if history is not None:
            my_previous_defections = np.sum(~history.my_actions)
            exploiter_detected = my_previous_defections > 0.25 * state.round_number
        if majority_cooperated or threshold_reached:
            return Action.C
        elif exploiter_detected:
            return Action.D
        else:
            if state.round_number == self.game_description.n_rounds - 1 and opponent_cooperations >= m_threshold:
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_112(BaseStrategy):
    """
    A strategy that adapts to the collective behavior of opponents in a Collective Risk Dilemma game.
    It prioritizes cooperation while adapting to opponents' actions and exploring alternative strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = 0
        self.cooperation_probability = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if previous_round_cooperators < self.game_description.m / 2:
            self.cooperation_probability -= 0.1
        elif previous_round_cooperators > self.game_description.m / 2:
            self.cooperation_probability += 0.1
        self.cooperation_probability = max(0, min(self.cooperation_probability, 1))
        if random.random() < self.cooperation_probability:
            action = Action.C
        else:
            action = Action.D
        if random.random() < 0.05 and action == Action.C:
            action = Action.D
        self.cooperate_count += action == Action.C
        return action

class Strategy_COLLECTIVE_113(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Threshold (ACT)
    
    Decision Rules:
    - Initial Round: Cooperate (C) to establish a cooperative tone and encourage others to do the same.
    - Subsequent Rounds: Use a weighted average of past cooperation rates to determine the likelihood of cooperating in the current round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.convergence_param = 0.5
        self.epsilon = 1e-06

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(opponent_actions)
        p_c = num_cooperators / self.game_description.n_players
        p_m = self.game_description.m / self.game_description.n_players
        if abs(p_c - p_m) < self.epsilon:
            if p_c > p_m:
                return Action.C if random.random() < (p_c + self.convergence_param) / 2 else Action.D
            else:
                return Action.C if random.random() < 1 - (1 - p_c + self.convergence_param) / 2 else Action.D
        elif p_c >= p_m:
            return Action.C if random.random() < (p_c + 1) / 2 else Action.D
        else:
            return Action.C if random.random() > (1 - p_c) / 2 else Action.D
        total_cooperators = sum(history.my_actions)
        return Action.C if total_cooperators >= self.game_description.m else Action.D

class Strategy_COLLECTIVE_114(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy.

    This strategy prioritizes cooperation when it's likely to be reciprocated and when collective outcomes are superior.
    By adapting to the group's behavior and punishing free-riders, ACO promotes a shared sense of responsibility
    and encourages players to work together towards the common goal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.prev_total_payoff = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_rounds - state.round_number == 1:
            return Action.C
        prev_opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.prev_cooperators += history.my_actions[-1]
        if prev_opponent_cooperators >= self.game_description.m // 2 or self.prev_total_payoff > self.game_description.n_players:
            return Action.C
        if prev_opponent_cooperators < self.game_description.m - 1:
            return Action.D
        if prev_opponent_cooperators >= self.game_description.m // 2:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_115(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.successful_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        theta_t = self.successful_rounds / state.round_number * (self.game_description.m / self.game_description.n_players)
        prev_coop = sum(history.opponent_actions[-1, :])
        prev_defect = self.game_description.n_players - 1 - prev_coop
        if history.my_payoffs[-1] > 0:
            self.successful_rounds += 1
        if prev_coop >= theta_t or history.my_payoffs[-1] > 0:
            return Action.C
        elif prev_defect >= self.game_description.n_players - self.game_description.m or history.my_payoffs[-1] <= 0:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1 and prev_coop < self.game_description.m:
            return Action.D
        elif history.my_payoffs[-1] > 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_116(BaseStrategy):
    """
    The Adaptive Cooperator strategy prioritizes cooperation when it is likely to lead to a successful outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number < self.game_description.n_rounds // 2:
            num_cooperators = sum(history.opponent_actions[-1, :])
            m = self.game_description.m
            n = self.game_description.n_players
            if (m - 1) / n != 0:
                p = 1 - (m - 1) / n
            else:
                p = 1
            if num_cooperators >= m:
                return Action.C
            elif random.random() > p:
                return Action.D
            else:
                return Action.C
        avg_payoff = np.mean(history.my_payoffs[-5:])
        k = self.game_description.k
        if avg_payoff >= k:
            return Action.C
        elif random.random() < 1 - (self.game_description.m - 1) / self.game_description.n_players:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_117(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for collective cooperation to achieve the threshold reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = game_description.m / game_description.n_players
        self.CT = game_description.m / game_description.n_players * (game_description.k / (game_description.k + 1))
        self.alpha = 0.1
        self.p_coop = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        avg_payoff_t = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        prev_coop_proportion = sum(history.opponent_actions[-1, :] | (history.my_actions[-1] == 1)) / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds - 1:
            if prev_coop_proportion >= self.CT:
                action = Action.C
            else:
                action = Action.D
            self.p_coop = max(0, min(1, self.p_init + self.alpha * (avg_payoff_t / self.game_description.k)))
        else:
            action = Action.D
        return action

class Strategy_COLLECTIVE_118(BaseStrategy):
    """
    Adaptive Threshold Cooperator strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.defection_count = [0] * game_description.n_players
        self.total_cooperations = 0
        self.cooperation_rate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        current_cooperations = sum(history.opponent_actions[-1, :])
        self.total_cooperations += current_cooperations
        self.cooperation_rate_history.append(current_cooperations / self.game_description.n_players)
        T = self.game_description.m - 1 + (self.game_description.k - 1) * (self.total_cooperations / (state.round_number * self.game_description.n_players))
        cooperate = current_cooperations >= T
        if not cooperate and history.my_actions[-1] == True:
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if not opponent_action:
                    self.defection_count[i] += 1
        elif any(self.defection_count):
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if opponent_action and self.defection_count[i]:
                    self.defection_count[i] = 0
        if state.round_number % 5 == 0:
            cooperation_rate = np.mean(self.cooperation_rate_history)
            target_cooperation_rate = 0.5
            T += 0.1 * (cooperation_rate - target_cooperation_rate)
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_119(BaseStrategy):
    """
    Adaptive Collective Escalation (ACE) strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective benefit, adapting to various opponent behaviors while maintaining a strong focus on achieving the cooperative threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5
        self.opponent_cooperation_history = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :] + [True])
        threshold_met = cooperators >= self.game_description.m
        if not threshold_met:
            escalation_factor = (self.game_description.m - cooperators) * 0.1
            self.cooperation_probability += escalation_factor
        elif threshold_met:
            self.cooperation_probability = max(self.cooperation_probability, 0.9)
        for opponent_index, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action and (not threshold_met):
                punishment_factor = 0.2
                if opponent_index in self.opponent_cooperation_history:
                    self.opponent_cooperation_history[opponent_index] -= punishment_factor
        for opponent_index, opponent_cooperation in enumerate(history.opponent_actions[-1, :]):
            if opponent_index not in self.opponent_cooperation_history:
                self.opponent_cooperation_history[opponent_index] = 0.5
            self.opponent_cooperation_history[opponent_index] += (opponent_cooperation * 2 - 1) * 0.01
        for opponent_probability in self.opponent_cooperation_history.values():
            if opponent_probability > 0.5:
                self.cooperation_probability += 0.01
            elif opponent_probability < 0.2:
                self.cooperation_probability -= 0.01
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if cooperators == self.game_description.m:
            prioritize_cooperation = max(self.opponent_cooperation_history.values())
            if prioritize_cooperation > 0.5:
                self.cooperation_probability += 0.1
        action = random.choices([Action.C, Action.D], weights=[self.cooperation_probability, 1 - self.cooperation_probability])[0]
        return action

class Strategy_COLLECTIVE_120(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation.

    This strategy balances individual self-interest with the need for collective cooperation,
    adapting to various opponent behaviors and game histories.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.H = 0
        self.T = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_coops >= self.game_description.m:
            action = Action.C
        else:
            p = (1 - self.game_description.k) * (self.H / self.T) if self.T > 0 else 1
            action = Action.D if random.random() < p else Action.C
        if prev_coops >= self.game_description.m:
            self.H += 1
        self.T += 1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        return action

class Strategy_COLLECTIVE_121(BaseStrategy):
    """
    Adaptive Collective Responsibility Strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective responsibility 
    to achieve optimal outcomes in the game. It relies on observing and responding 
    to the actions of other players, rather than relying on pre-defined coordination 
    mechanisms or assumptions about opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_rounds_cooperated = 0
        self.total_previous_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + 1
        self.previous_rounds_cooperated += int(history.my_actions[-1])
        self.total_previous_rounds += 1
        RR = previous_round_cooperators / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if RR >= self.game_description.m / self.game_description.n_players:
            return Action.C
        p = 1 - self.previous_rounds_cooperated / max(self.total_previous_rounds, 1)
        p = max(p, 0.5)
        if random.random() < p:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_122(BaseStrategy):
    """
    A strategy that adapts to the group's performance by adjusting cooperation probabilities based on previous outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.p = None
        self.m_eff = self.m
        self.δ = 0.1
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.p = self.m / self.n
            cooperate = random.random() < self.p
            self.history.append([cooperate] + [random.choice([True, False]) for _ in range(self.n - 1)])
            return Action.C if cooperate else Action.D
        previous_outcome = sum(history.opponent_actions[-1, :] + history.my_actions[-1:])
        if previous_outcome >= self.m_eff:
            self.p = min(self.p + self.δ, 1)
        else:
            self.p = max(self.p - self.δ, self.m / self.n)
        avg_coop_rate = sum([sum(round) for round in self.history[-5:]]) / (5 * self.n)
        if avg_coop_rate > 0.5 and self.m_eff < self.n:
            self.m_eff += 1
        elif avg_coop_rate < 0.5 and self.m_eff > 1:
            self.m_eff -= 1
        if state.round_number == self.r - 1 and all((sum(round) < self.m for round in self.history)):
            return Action.D
        cooperate = random.random() < self.p
        self.history.append([cooperate] + [random.choice([True, False]) for _ in range(self.n - 1)])
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_123(BaseStrategy):
    """
    A collective risk dilemma strategy that adapts to opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round = history.opponent_actions[-1]
        num_cooperators = sum(prev_round)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if num_cooperators >= self.m:
            return Action.C if sum(prev_round[:-1]) >= self.m - 1 else Action.D
        elif num_cooperators == 0:
            return Action.C if random.random() < 0.5 else Action.D
        elif num_cooperators == 1:
            return Action.C if random.random() < (self.k - 1) / self.k else Action.D
        else:
            return Action.C if random.random() < (self.k - 1) / self.k else Action.D

class Strategy_COLLECTIVE_124(BaseStrategy):
    """
    The ACR strategy aims to balance individual self-interest with collective risk management.
    It adapts to the game's history and opponent behavior while promoting cooperation when beneficial.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.m_threshold = game_description.m
        self.k_reward = game_description.k
        self.p_coop = (self.k_reward - 1) / self.k_reward

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_outcome = history.opponent_actions[-1, :]
        coop_count = sum(prev_round_outcome)
        if self.n_players == 2 and state.round_number == self.game_description.n_rounds - 1:
            opponent_prev_action = prev_round_outcome[0]
            return Action.D if opponent_prev_action else Action.C
        if self.m_threshold == 1:
            return Action.C
        if coop_count >= self.m_threshold:
            return Action.C if random.random() < self.p_coop else Action.D
        else:
            p_defect = 1 - self.m_threshold / self.n_players
            return Action.D if random.random() < p_defect else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            coop_count_last_round = sum(history.opponent_actions[-2, :])
            if coop_count_last_round >= self.m_threshold:
                return Action.C

class Strategy_COLLECTIVE_125(BaseStrategy):
    """
    Implement the Adaptive Cooperation (AC) strategy for Collective Risk Dilemma.
    
    AC prioritizes cooperation when it benefits the collective, while being cautious not to exploit others' cooperation.
    By observing the previous round's outcome, AC adapts to changing circumstances and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_round_outcome = None
        self.avg_payoffs_C = []
        self.avg_payoffs_D = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.prev_round_outcome = sum(history.opponent_actions[-1, :] == True)
        my_payoff = history.my_payoffs[-1]
        opponent_payoffs_C = [payoff for action, payoff in zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :]) if action]
        opponent_payoffs_D = [payoff for action, payoff in zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :]) if not action]
        avg_payoff_C = np.mean(opponent_payoffs_C + [my_payoff if history.my_actions[-1] else 0])
        avg_payoff_D = np.mean(opponent_payoffs_D)
        if len(opponent_payoffs_C) == 0:
            avg_payoff_C = my_payoff if history.my_actions[-1] else 0
        if len(opponent_payoffs_D) == 0:
            avg_payoff_D = 0
        self.avg_payoffs_C.append(avg_payoff_C)
        self.avg_payoffs_D.append(avg_payoff_D)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.prev_round_outcome >= self.game_description.m:
            if avg_payoff_C == avg_payoff_D:
                return Action.C
            elif avg_payoff_C > avg_payoff_D:
                return Action.C
            else:
                return Action.D
        prev_prev_round_outcome = sum(history.opponent_actions[-2, :] == True) if state.round_number > 1 else 0
        if self.prev_round_outcome < self.game_description.m and self.prev_round_outcome > prev_prev_round_outcome:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_126(BaseStrategy):
    """
    The ACC strategy balances individual self-interest with collective well-being by adaptively responding to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = {}
        self.punish_threshold = 0.5
        self.punish_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.game_description.m:
            action = Action.C
        else:
            defect_prob = (self.game_description.m - num_cooperators_prev_round) / (self.game_description.n_players - 1)
            action = random.choices([Action.C, Action.D], weights=[1 - defect_prob, defect_prob])[0]
        opponent_id = 0
        if opponent_id not in self.opponent_cooperation_rates:
            self.opponent_cooperation_rates[opponent_id] = 0.5
        self.opponent_cooperation_rates[opponent_id] += (history.opponent_actions[-1, opponent_id] - self.opponent_cooperation_rates[opponent_id]) / state.round_number
        if self.opponent_cooperation_rates[opponent_id] < self.punish_threshold:
            action = Action.D
            self.punish_rounds += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_127(BaseStrategy):
    """
    The Adaptive Collective Guardian (ACG) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_round_cooperation = 0
        self.average_cooperation_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        previous_round_actions = history.opponent_actions[-1, :]
        opponent_cooperation_last_round = sum(previous_round_actions)
        if state.round_number <= r // 3:
            if self.previous_round_cooperation < m:
                return Action.D
            else:
                return Action.C
        elif state.round_number <= 2 * r // 3:
            num_previous_rounds = min(5, state.round_number)
            previous_rounds_actions = history.opponent_actions[-num_previous_rounds:, :]
            self.average_cooperation_level = np.mean(sum(previous_rounds_actions))
            if self.average_cooperation_level >= m - 1:
                return Action.C
            else:
                return Action.D
        elif opponent_cooperation_last_round >= m:
            return Action.C
        else:
            return Action.D
        self.previous_round_cooperation = opponent_cooperation_last_round + (1 if history.my_actions[-1] else 0)

class Strategy_COLLECTIVE_128(BaseStrategy):
    """
    A dynamic strategy that balances individual self-interest with collective well-being,
    leveraging game parameters and history to inform cooperation decisions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m >= self.game_description.n_players / 2 else Action.D
        if history is not None:
            my_actions = np.array(history.my_actions, dtype=np.bool_)
            opponent_actions = np.array(history.opponent_actions, dtype=np.bool_)
            total_payoff = sum(history.my_payoffs)
            prev_cooperation_rate = np.sum(opponent_actions[-1]) / self.game_description.n_players
            if prev_cooperation_rate >= self.game_description.m / self.game_description.n_players:
                return Action.C if random.random() < (self.game_description.k - 1) / self.game_description.k else Action.D
            elif total_payoff < self.game_description.k:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and history is not None:
            prev_cooperation_rate = np.sum(np.array(history.opponent_actions, dtype=np.bool_)[:-1]) / self.game_description.n_players
            return Action.C if prev_cooperation_rate >= self.game_description.m / self.game_description.n_players else Action.D
        return Action.D

class Strategy_COLLECTIVE_129(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) Strategy.

    This strategy prioritizes collective success over individual gain.
    By cooperating initially and escalating cooperation when necessary,
    ACE encourages other players to do the same, increasing the likelihood of meeting the cooperation threshold and achieving a higher payoff for all players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold_met = False
        self.escalation_triggered = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators >= self.game_description.m:
            self.cooperation_threshold_met = True
            self.escalation_triggered = False
            return Action.C
        if not self.cooperation_threshold_met and state.round_number > 1:
            previous_cooperators = sum(history.opponent_actions[-2, :])
            if previous_cooperators < self.game_description.m:
                self.escalation_triggered = True
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_130(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.

    This strategy prioritizes cooperation when the collective cooperation level is above a dynamic threshold.
    It adapts to the number of cooperators in previous rounds, promoting a self-reinforcing cycle of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_threshold = game_description.m
        self.prev_cooperators = None
        self.temp_defect_prob = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        self.prev_cooperators = prev_cooperators
        if prev_cooperators >= self.m_threshold:
            action = Action.C
        else:
            action = Action.D
        if prev_cooperators >= self.game_description.m:
            self.m_threshold = max(self.m_threshold - 1, 1)
        elif prev_cooperators < self.game_description.m:
            self.m_threshold = min(self.m_threshold + 1, self.game_description.n_players - 1)
        if action == Action.D and prev_cooperators >= self.game_description.m:
            self.temp_defect_prob = 0.75
            action = Action.D if random.random() < self.temp_defect_prob else Action.C
        if state.round_number == self.game_description.n_rounds - 1 and prev_cooperators >= self.m_threshold:
            return Action.C
        return action

class Strategy_COLLECTIVE_131(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy.
    
    Balances individual self-interest with collective optimality by adapting cooperation probability based on previous round outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.0
        self.prev_round_outcome = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.p = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < self.p else Action.D
        self.prev_round_outcome = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if self.prev_round_outcome >= self.game_description.m:
            self.p = 1 - 1 / self.game_description.n_players
        else:
            self.p = (self.game_description.n_players - self.game_description.m) / self.game_description.n_players
        delta = 1 / (2 * self.game_description.n_players)
        if self.prev_round_outcome >= self.game_description.m and history.my_actions[-1] == 1:
            self.p += delta
        elif self.prev_round_outcome < self.game_description.m and history.my_actions[-1] == 0:
            self.p -= delta
        if state.round_number == self.game_description.n_rounds - 1:
            self.p = self.game_description.m / self.game_description.n_players
        return Action.C if random.random() < self.p else Action.D

class Strategy_COLLECTIVE_132(BaseStrategy):
    """
    A strategy that initially assumes goodwill and adaptively adjusts based on group performance 
    and individual behaviors observed over rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_actions_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if last_round_cooperators >= self.game_description.m:
            return Action.C
        elif any(history.opponent_actions[-1, :]):
            opponents_who_did_not_cooperate = np.where(~history.opponent_actions[-1, :])[0]
            opponents_past_actions = history.opponent_actions[:-1, opponents_who_did_not_cooperate]
            if np.any(opponents_past_actions):
                return Action.C if (opponents_past_actions == 1).mean() > 0.5 else Action.D
        elif state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        return Action.D

    def __postcall__(self, state: GameState, history: None | PlayerHistory):
        if history is not None and len(self.my_actions_history) < state.round_number + 1:
            self.my_actions_history.append(history.my_actions[-1])

class Strategy_COLLECTIVE_133(BaseStrategy):
    """
    Adaptive Collective Risk Management strategy.
    
    This strategy adapts to various opponent behaviors and aligns with the collective mindset,
    balancing individual payoff maximization with collective risk management through cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.successful_rounds = 0
        self.cooperation_rates = [0.0] * game_description.n_players

    def calculate_expected_payoff(self, history: PlayerHistory) -> float:
        """
        Calculate the expected payoff for cooperating in the current round.
        
        This calculation considers the game's payoff structure, the reward factor k,
        and the observed behaviors (cooperation rates) of other players.
        """
        prob_cooperate = np.mean(history.opponent_actions)
        expected_payoff = 0.5 * (1 + self.game_description.k) * prob_cooperate + 0.5 * (1 - prob_cooperate)
        return expected_payoff

    def calculate_fraction_of_highly_cooperative_players(self, history: PlayerHistory) -> float:
        """
        Calculate the fraction of players with high cooperation rates in successful rounds.
        
        This calculation is used to determine if there's a collective inclination towards cooperation.
        """
        highly_cooperative_players = sum((1 for rate in self.cooperation_rates if rate >= 0.5))
        fraction = highly_cooperative_players / self.game_description.n_players
        return fraction

    def update_history(self, history: PlayerHistory):
        """
        Update the successful rounds and cooperation rates based on the current round's actions.
        
        This is used to maintain a record of past behaviors for future decision-making.
        """
        if sum(history.opponent_actions[-1, :]) >= self.game_description.m:
            self.successful_rounds += 1
        for i in range(self.game_description.n_players):
            self.cooperation_rates[i] = np.mean(history.opponent_actions[:, i])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            expected_cooperating_payoff = self.calculate_expected_payoff(history)
            expected_defecting_payoff = 1.0
            fraction_of_highly_cooperative_players = self.calculate_fraction_of_highly_cooperative_players(history)
            if expected_cooperating_payoff > expected_defecting_payoff or fraction_of_highly_cooperative_players >= 0.5:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_134(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation.
    
    Balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperation_probability = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < self.cooperation_probability else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == True)
        if previous_round_cooperators >= self.game_description.m:
            self.cooperation_probability = max(0, (self.game_description.k - 1) / self.game_description.k)
        else:
            self.cooperation_probability = min(1, (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players)
        if history.my_actions[-1] == True and previous_round_cooperators >= self.game_description.m:
            self.cooperation_probability += 0.05
        elif history.my_actions[-1] == False and previous_round_cooperators < self.game_description.m:
            self.cooperation_probability -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            expected_payoff_cooperate = (self.game_description.k - 1) / self.game_description.k * self.game_description.m + (1 - history.my_actions[-1]) * self.game_description.k
            expected_payoff_defect = (history.my_actions[-1] == True) + (1 - history.my_actions[-1]) * self.game_description.k
            return Action.C if expected_payoff_cooperate > expected_payoff_defect else Action.D
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_135(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_minus_1 = max(0, game_description.m - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_outcomes = history.my_actions[-1] + history.opponent_actions[-1]
        threshold_met_prev = sum(prev_round_outcomes) >= self.game_description.m
        player_payoff_prev = 1 - int(history.my_actions[-1]) + (self.game_description.k if threshold_met_prev else 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif threshold_met_prev and player_payoff_prev >= self.game_description.k + 1 - int(history.my_actions[-1]):
            if sum(prev_round_outcomes[:-1]) >= self.m_minus_1:
                return Action.C
            else:
                return Action.D
        elif not threshold_met_prev:
            if player_payoff_prev == 1:
                return Action.D
            elif player_payoff_prev >= self.game_description.k + 1 - int(history.my_actions[-1]):
                return Action.C

class Strategy_COLLECTIVE_136(BaseStrategy):
    """
    Adaptive Cooperation strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        prev_round_coop = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        threshold_met = prev_round_coop >= self.game_description.m
        if threshold_met:
            self.coop_prob += 0.1
            self.coop_prob = min(self.coop_prob, 1)
        else:
            self.coop_prob -= 0.1
            self.coop_prob = max(self.coop_prob, self.game_description.m / self.game_description.n_players)
        if prev_round_coop > self.game_description.m:
            self.coop_prob += 0.05
            self.coop_prob = min(self.coop_prob, 1)
        elif prev_round_coop < self.game_description.m:
            self.coop_prob -= 0.05
            self.coop_prob = max(self.coop_prob, self.game_description.m / self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if threshold_met or abs(prev_round_coop - self.game_description.m) <= 2 else Action.D
        return Action.C if random.random() < self.coop_prob else Action.D

class Strategy_COLLECTIVE_137(BaseStrategy):
    """
    ACO Strategy: Balance individual self-interest with collective well-being by encouraging initial cooperation, 
    adapting to others' actions, and focusing on long-term benefits.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        M_t = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        if len(history.my_payoffs) < state.round_number:
            return Action.C
        avg_payoff_coop = np.mean([history.opponent_payoffs[-1, i] for i in range(self.game_description.n_players - 1) if history.opponent_actions[-1, i]])
        avg_payoff_defect = np.mean([history.opponent_payoffs[-1, i] for i in range(self.game_description.n_players - 1) if not history.opponent_actions[-1, i]])
        if M_t >= self.game_description.m or (avg_payoff_coop > avg_payoff_defect and avg_payoff_coop != 0):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if M_t >= self.game_description.m else Action.D
        return Action.D

class Strategy_COLLECTIVE_138(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.H_c = 0.0
        self.H_d = 0.0
        self.N = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number > 0 and history is None):
            return Action.C
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        self.N += 1
        if state.round_number < self.game_description.n_rounds - 1:
            if num_cooperators_last_round < self.game_description.m:
                return Action.D
            else:
                p = self.H_c / (self.H_c + self.H_d) if self.H_c + self.H_d > 0.0 else 0.5
                return Action.C if random.random() < p else Action.D
        else:
            return Action.C

        def update_payoff_history(action: bool, payoff: float):
            if action:
                self.H_c += payoff
            else:
                self.H_d += payoff

class Strategy_COLLECTIVE_139(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters by balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.tau = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperation_rate = self.calculate_cooperation_rate(history)
        if history.my_payoffs[-1] > 1:
            self.tau *= 0.9
        else:
            self.tau *= 1.1
        if cooperation_rate >= self.tau or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

    def calculate_cooperation_rate(self, history: PlayerHistory):
        num_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        return num_cooperators / self.n_players

class Strategy_COLLECTIVE_140(BaseStrategy):
    """
    The Adaptive Cooperation strategy aims to balance individual self-interest with collective benefit,
    adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if prev_round_coops >= self.game_description.m:
            p = prev_round_coops / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        else:
            q = 1 - prev_round_coops / self.game_description.m
            if prev_round_coops == self.game_description.m - 1:
                return Action.C if random.random() < 0.5 else Action.D
            return Action.C if random.random() > q else Action.D

class Strategy_COLLECTIVE_141(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma that adapts to the group's behavior.
    Encourages initial cooperation and punishes non-cooperative behavior temporarily.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_actions = history.opponent_actions[-1, :]
        num_cooperators = sum((1 for action in previous_round_actions if action))
        CI = num_cooperators / self.game_description.n_players
        if CI >= 0.5:
            return Action.C
        if num_cooperators >= self.game_description.m and any((not action for action in previous_round_actions)):
            return Action.D
        return Action.D

    def last_round(self, state: GameState) -> bool:
        """
        Check if this is the last round.
        """
        return state.round_number == self.game_description.n_rounds - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.last_round(state):
            return Action.C
        previous_round_actions = history.opponent_actions[-1, :]
        num_cooperators = sum((1 for action in previous_round_actions if action))
        CI = num_cooperators / self.game_description.n_players
        if CI >= 0.5:
            return Action.C
        if num_cooperators >= self.game_description.m and any((not action for action in previous_round_actions)):
            return Action.D
        if num_cooperators >= self.game_description.m and CI < 0.5:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_142(BaseStrategy):
    """
    Adaptive Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = max(0.5, (game_description.m - 1) / game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        c_prev = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if c_prev >= self.game_description.m:
            cooperate_prob = max(0.5, self.p_coop)
        else:
            cooperate_prob = min(0.5, 1 - (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players)
        action = Action.C if random.random() < cooperate_prob else Action.D
        if c_prev < self.game_description.m:
            self.p_coop = max(0, self.p_coop - 0.1)
        elif c_prev >= self.game_description.m:
            self.p_coop = min(1, self.p_coop + 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if c_prev < self.game_description.m else Action.D
        return action

class Strategy_COLLECTIVE_143(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        n_players = self.game_description.n_players
        if state.round_number > 1 and prev_cooperators < self.game_description.m and (sum(history.opponent_actions[-2, :]) + int(history.my_actions[-2]) < self.game_description.m):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        p_defect = 1 - prev_cooperators / n_players
        if prev_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_144(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy.
    Balances individual self-interest with collective well-being by adaptively responding to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.game_description.m:
            return Action.C
        else:
            defect_prob = (self.game_description.m - prev_cooperators) / self.game_description.n_players
            if random.random() < defect_prob:
                return Action.D
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_145(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy for Collective Risk Dilemma.
    Balances individual self-interest with collective well-being, adapting to the game's dynamics and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.collective_payoff_history = []
        self.opponent_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if len(self.collective_payoff_history) < state.round_number:
            self.collective_payoff_history.append(history.my_payoffs[-1])
        if self.opponent_actions is None or len(self.opponent_actions) < state.round_number:
            self.opponent_actions = np.zeros((state.round_number + 1, self.game_description.n_players - 1), dtype=bool)
        self.opponent_actions[state.round_number - 1] = history.opponent_actions[-1]
        if len(self.collective_payoff_history) > 0:
            previous_round_cooperated = not history.my_actions[-1]
            previous_opponent_defected = np.any(history.opponent_actions[-1])
            if previous_round_cooperated and previous_opponent_defected:
                return Action.D
            elif not previous_round_cooperated and np.any(self.opponent_actions[state.round_number - 1]):
                return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            successful_collective_payoffs = sum((1 for payoff in self.collective_payoff_history if payoff >= self.game_description.m))
            return Action.C if successful_collective_payoffs >= self.game_description.n_rounds / 2 else Action.D
        previous_round_success = self.collective_payoff_history[-1] >= self.game_description.m
        if previous_round_success:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        else:
            return Action.C if random.random() > 0.5 else Action.D

class Strategy_COLLECTIVE_146(BaseStrategy):
    """
    A strategy that adapts its cooperation probability based on the collective effort's success rate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_prob = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_prob else Action.D
        previous_round_success = sum(history.opponent_actions[-1, :]) >= self.game_description.m
        if previous_round_success:
            action = Action.C
        else:
            cooperation_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
            self.cooperation_prob = max(0, min(1, self.cooperation_prob * cooperation_rate))
            action = Action.C if random.random() < self.cooperation_prob else Action.D
        if state.round_number % 5 == 0:
            avg_cooperation_rate = sum((sum(history.opponent_actions[i, :]) for i in range(state.round_number - 4, state.round_number))) / (self.game_description.n_players * 5)
            if avg_cooperation_rate > self.game_description.m / self.game_description.n_players:
                self.cooperation_prob += 0.1
            elif avg_cooperation_rate < self.game_description.m / self.game_description.n_players:
                self.cooperation_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C if previous_round_success else Action.D
        consecutive_failures = sum((1 for i in range(max(0, state.round_number - 3), state.round_number) if not sum(history.opponent_actions[i, :]) >= self.game_description.m))
        if consecutive_failures >= 3:
            action = Action.D
        return action

class Strategy_COLLECTIVE_147(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy prioritizes collective success 
    over individual gain, adapting to changing circumstances while maintaining a strong 
    inclination towards cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_prob = 0.5

    def adjust_cooperation_probability(self, avg_payoff):
        if avg_payoff > self.game_description.k:
            self.cooperation_prob += 0.1
        elif avg_payoff < self.game_description.k:
            self.cooperation_prob -= 0.1
        self.cooperation_prob = max(0, min(1, self.cooperation_prob))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number <= min(self.game_description.n_rounds // 4, 3):
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            if opponent_cooperators >= self.game_description.m:
                return Action.C
            else:
                return random.choices([Action.C, Action.D], weights=[0.2, 0.8])[0]
        avg_payoff = np.mean(history.my_payoffs[-min(self.game_description.n_rounds // 4, 3):])
        self.adjust_cooperation_probability(avg_payoff)
        if avg_payoff > self.game_description.k:
            return random.choices([Action.C, Action.D], weights=[0.8, 0.2])[0]
        else:
            return random.choices([Action.C, Action.D], weights=[0.4, 0.6])[0]
        if state.round_number == self.game_description.n_rounds - 1:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            if opponent_cooperators >= self.game_description.m:
                return Action.C

class Strategy_COLLECTIVE_148(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.5
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        total_players = history.opponent_actions.shape[1] + 1
        self.cooperation_rate = (cooperators + 1) / total_players if state.round_number > 0 else self.cooperation_rate
        self.threshold = self.game_description.m - 1 if self.cooperation_rate >= 0.5 else self.game_description.m + 1
        return Action.C if cooperators + 1 >= self.threshold else Action.D

class Strategy_COLLECTIVE_149(BaseStrategy):
    """
    The Adaptive Cooperation strategy balances individual self-interest with collective well-being.
    It adapts to changing game dynamics and opponent behaviors, prioritizing collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.coop_prob = game_description.m / game_description.n_players
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        prev_outcome = sum(history.opponent_actions[-1, :] | (history.my_actions[-1] == 1))
        if prev_outcome >= self.game_description.m:
            self.coop_prob = max(0.5, 1 - 1 / self.game_description.k)
        else:
            self.coop_prob = min(0.8, (self.game_description.n_players - self.game_description.m) / self.game_description.n_players)
        self.coop_prob = self._adjust_cooperation_probability(prev_outcome, self.coop_prob)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.coop_prob else Action.D

    def _adjust_cooperation_probability(self, prev_outcome: int, coop_prob: float) -> float:
        """
        Adjust cooperation probability based on previous round's outcome.
        
        If the collective goal was achieved, increase the cooperation probability by 0.1 (up to a maximum of 1).
        If the collective goal was not achieved, decrease the cooperation probability by 0.1 (down to a minimum of 0).
        """
        if prev_outcome >= self.game_description.m:
            return min(1, coop_prob + 0.1)
        else:
            return max(0, coop_prob - 0.1)

class Strategy_COLLECTIVE_150(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy in the Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective cooperation, 
    using a dynamic decision-making process that responds to the actions of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_payoff_prev_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        num_cooperators_last_round = sum(opponent_actions_last_round)
        my_action_last_round = history.my_actions[-1]
        if my_action_last_round:
            self.my_payoff_prev_round = self.game_description.k - 1 + (1 if num_cooperators_last_round >= self.game_description.m else 0)
        else:
            self.my_payoff_prev_round = 1 + (self.game_description.k if num_cooperators_last_round >= self.game_description.m else 0)
        if num_cooperators_last_round >= self.game_description.m:
            action = Action.C
        elif self.my_payoff_prev_round < self.game_description.k - 1 and num_cooperators_last_round < self.game_description.m:
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1 and num_cooperators_last_round >= self.game_description.m:
            action = Action.D
        return action

class Strategy_COLLECTIVE_151(BaseStrategy):
    """
    The Adaptive Cooperative Resilience strategy aims to balance individual self-interest with collective cooperation,
    adapting to changing circumstances while promoting a resilient cooperative outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def calculate_average_payoffs(self, history: PlayerHistory) -> tuple[float, float]:
        if len(history.my_actions) == 1:
            return (0, 0)
        cooperators_payoff_sum = sum([payoff for i, payoff in enumerate(history.opponent_payoffs[-2]) if history.opponent_actions[-2][i]])
        defectors_payoff_sum = sum([payoff for i, payoff in enumerate(history.opponent_payoffs[-2]) if not history.opponent_actions[-2][i]])
        n_cooperators = sum(history.opponent_actions[-2])
        n_defectors = self.game_description.n_players - 1 - n_cooperators
        average_cooperator_payoff = cooperators_payoff_sum / n_cooperators if n_cooperators > 0 else 0
        average_defector_payoff = defectors_payoff_sum / n_defectors if n_defectors > 0 else 0
        return (average_cooperator_payoff, average_defector_payoff)

    def adapt_cooperation_probability(self, average_cooperator_payoff: float, average_defector_payoff: float) -> None:
        adaptation = math.copysign(1, average_cooperator_payoff - average_defector_payoff)
        self.cooperation_probability = min(max(self.cooperation_probability + 0.1 * adaptation, 0), 1)

    def exploitation_prevention(self, history: PlayerHistory) -> bool:
        if len(history.my_actions) < 2 or not all(history.opponent_actions[-1]):
            return False
        previous_round_payoff = history.opponent_payoffs[-1][0]
        return self.game_description.k == previous_round_payoff

    def endgame_cooperate(self, history: PlayerHistory) -> bool:
        if len(history.my_actions) < 2 or not all(history.opponent_actions[-1]):
            return False
        successful_outcomes = sum([1 for i in range(len(history.opponent_payoffs)) if self.game_description.m <= sum(map(int, history.opponent_actions[i]))])
        return successful_outcomes >= len(history.my_actions) // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperation_probability else Action.D
        average_cooperator_payoff, average_defector_payoff = self.calculate_average_payoffs(history)
        self.adapt_cooperation_probability(average_cooperator_payoff, average_defector_payoff)
        previous_round_success = sum(map(int, history.opponent_actions[-1])) >= self.game_description.m
        if previous_round_success and average_cooperator_payoff > average_defector_payoff:
            return Action.C
        elif self.exploitation_prevention(history):
            return random.choice([Action.C, Action.D]) if random.random() < 0.2 else Action.D
        elif state.round_number == self.game_description.n_rounds - 1 and self.endgame_cooperate(history):
            return Action.C
        return random.choice([Action.C, Action.D]) if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_152(BaseStrategy):
    """
    Adaptive Cooperate-Defect (ACD) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions, axis=0)
        avg_coop_rate = np.mean(opponent_cooperations)
        coop_threshold = self.game_description.m / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if avg_coop_rate > coop_threshold or (avg_coop_rate == coop_threshold and random.random() < 0.5):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_153(BaseStrategy):
    """
    Adaptive Cooperation strategy for Collective Risk Dilemma.

    This strategy adapts to opponents' behaviors by cooperating when others do,
    encouraging opponents to contribute when necessary, and balancing self-interest
    and collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = math.floor(game_description.m / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.game_description.k >= 1 else Action.D
        elif prev_cooperators < math.ceil(self.theta):
            return Action.C if random.random() < 0.25 else Action.D
        elif prev_cooperators >= self.game_description.m:
            return Action.C
        elif prev_cooperators > math.ceil(self.theta):
            return Action.D
        else:
            p = (1 - self.theta / self.game_description.m) * (1 + self.game_description.k / (self.game_description.k + 1))
            return Action.C if random.random() < p else Action.D

class Strategy_COLLECTIVE_154(BaseStrategy):
    """
    Adaptive Collective Responsibility Strategy.

    This strategy prioritizes collective success by encouraging initial cooperation,
    reciprocating cooperation when the threshold is met, and adapting to the group's performance.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators >= self.game_description.m:
            self.cooperation_probability = min(1, self.cooperation_probability + 0.1)
            action = Action.C if random.random() < self.cooperation_probability else Action.D
        else:
            self.cooperation_probability = max(0, self.cooperation_probability - 0.1)
            action = Action.C if random.random() < self.cooperation_probability else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_155(BaseStrategy):
    """
    The Adaptive Threshold Cooperation strategy prioritizes collective success over individual gain.
    By adapting to the number of cooperators in previous rounds and adjusting cooperation probability accordingly,
    ATC promotes a balanced approach that encourages cooperation while avoiding exploitation by defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.memory_parameter = 0
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperation_probability else Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.game_description.m:
            action = Action.C
        else:
            cooperating_payoffs = np.sum([payoff for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if action])
            defecting_payoffs = np.sum([payoff for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if not action])
            action = Action.C if cooperating_payoffs >= defecting_payoffs else Action.D
        if self.memory_parameter >= 2:
            if opponent_cooperators < self.game_description.m:
                self.cooperation_probability -= 0.1
            else:
                self.cooperation_probability += 0.1
        if opponent_cooperators < self.game_description.m:
            self.memory_parameter += 1
        else:
            self.memory_parameter = 0
        return action

class Strategy_COLLECTIVE_156(BaseStrategy):
    """
    Adaptive Collective Defense (ACD) strategy for the Collective Risk Dilemma.
    Balances individual self-interest with collective risk management, adapting to the evolving game environment and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if len(history.opponent_payoffs) < state.round_number:
            return Action.D
        total_payoff = np.sum(history.opponent_payoffs[-1, :] + [history.my_payoffs[-1]])
        if num_cooperators < self.game_description.m:
            return Action.D
        elif total_payoff >= self.game_description.n_players * self.game_description.k:
            return Action.C
        else:
            p = (self.game_description.m - 1) / (self.game_description.n_players - 1)
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_158(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)

    This strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_cooperators_prev_round = 0
        self.avg_payoff = 0
        self.collective_payoff_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        self.n_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        self.collective_payoff_prev_round = history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :])
        p_C = 0.5
        if self.n_cooperators_prev_round < self.game_description.m:
            p_C = (self.game_description.m - self.n_cooperators_prev_round) / self.game_description.n_players
        elif self.game_description.k >= 2:
            p_C = 0.8
        else:
            p_C = 0.5
        if history.my_payoffs.size > 1:
            self.avg_payoff += (self.collective_payoff_prev_round - self.avg_payoff) / state.round_number
            if self.collective_payoff_prev_round < self.avg_payoff:
                p_C = max(0.2, p_C - 0.1)
            elif self.collective_payoff_prev_round > self.avg_payoff:
                p_C = min(0.8, p_C + 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            p_C = 0.9
        return Action.C if random.random() < p_C else Action.D

class Strategy_COLLECTIVE_159(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy.

    This strategy adapts to the evolving game dynamics by adjusting its threshold for cooperation.
    It prioritizes cooperation when others cooperate, and decreases the threshold if average cooperation rates are low.
    In the endgame, it ensures that players do not miss opportunities for reward in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.avg_m = 0
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.m_prev = sum(history.opponent_actions[-1, :])
            self.total_payoff += self.calculate_payoff(Action.D, self.m_prev, self.game_description.k)
            if state.round_number % 5 == 0:
                self.avg_m = np.mean([sum(history.opponent_actions[t, :]) for t in range(max(0, state.round_number - 5), state.round_number)])
                if self.avg_m < 0.5 * self.game_description.m:
                    self.game_description.m -= 1
            if state.round_number == self.game_description.n_rounds - 1:
                if self.total_payoff <= self.game_description.k:
                    return Action.C
                else:
                    return Action.D
        if self.m_prev >= self.game_description.m:
            return Action.C
        return Action.D

    def calculate_payoff(self, action: Action, m_prev: int, k: float) -> float:
        if m_prev < self.game_description.m:
            return 1 - (action == Action.C)
        else:
            return 1 - (action == Action.C) + k

class Strategy_COLLECTIVE_160(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy.
    
    This strategy encourages cooperation by adapting to the collective behavior of opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.successful_rounds = 0
        self.cooperation_probability = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        if sum(history.opponent_actions[-1, :]) >= self.game_description.m:
            cooperate = random.random() < max(0.5, self.game_description.k / (self.game_description.k + 1))
        else:
            cooperate = random.random() < min(0.5, 1 / self.game_description.k)
        if history.my_payoffs[-1] > np.mean(history.my_payoffs[:-1]):
            self.successful_rounds += 1
            self.cooperation_probability = min(self.cooperation_probability + 0.05, 1)
        else:
            self.cooperation_probability = max(self.cooperation_probability - 0.05, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if sum(history.my_payoffs > np.mean(history.my_payoffs)) >= self.game_description.n_rounds // 2 else Action.D
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_161(BaseStrategy):
    """
    Adaptive Threshold Response (ATR) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players, m, k, r = (self.game_description.n_players, self.game_description.m, self.game_description.k, self.game_description.n_rounds)
        if state.round_number == 0:
            return Action.C if m > n_players / 2 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        punish = self.prev_cooperators is not None and self.prev_cooperators >= m and (prev_cooperators < m)
        explore = state.round_number % 3 == 0
        if prev_cooperators >= m:
            return Action.C
        elif prev_cooperators < m and (not punish) and (not explore):
            return Action.D
        elif punish:
            self.prev_cooperators = None
            return Action.D
        else:
            return Action.C
        self.prev_cooperators = prev_cooperators

class Strategy_COLLECTIVE_162(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy aims to balance individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coop_rate = 0
        self.avg_opponent_coop_rate = 0
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        prev_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        x = min(5, self.game_description.n_rounds // 2)
        avg_opponent_coop_rate = np.mean(np.any(history.opponent_actions[max(-x, -len(history.opponent_actions)):, :], axis=0))
        if prev_coop_rate >= self.game_description.m / self.game_description.n_players or (avg_opponent_coop_rate >= 0.5 and self.punishment_counter == 0):
            action = Action.C
        else:
            self.punishment_counter += 1
            action = Action.D
            y = min(2, self.game_description.n_rounds // 4)
            if self.punishment_counter >= y:
                self.punishment_counter = 0
        z = min(3, self.game_description.n_rounds // 4)
        if state.round_number > self.game_description.n_rounds - z and avg_opponent_coop_rate >= 0.5:
            action = Action.C
        return action

class Strategy_COLLECTIVE_163(BaseStrategy):
    """
    A hybrid strategy combining tit-for-tat and gradient ascent to balance individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_score = 0
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        cooperation_index = opponent_cooperators / self.game_description.n_players
        if cooperation_index >= 0.5:
            self.cooperation_score += 1
        else:
            self.cooperation_score -= 1
        if state.round_number % (self.game_description.n_players // 2) == 0:
            if self.cooperation_score > self.game_description.n_players / 2:
                self.threshold = max(0, self.threshold - 0.1)
            elif self.cooperation_score < -self.game_description.n_players / 2:
                self.threshold = min(1, self.threshold + 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if cooperation_index >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_164(BaseStrategy):
    """
    The Adaptive Collective Threshold strategy balances individual self-interest with collective well-being by adapting to the group's cooperation level and adjusting its behavior accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coop = 0
        self.avg_coop_rate = 0

    def calculate_avg_coop_rate(self, round_num: int, history: PlayerHistory) -> float:
        coop_count = 0
        for i in range(max(1, round_num - 5), round_num + 1):
            coop_count += sum(history.opponent_actions[i, :] == True)
        avg_coop_rate = coop_count / ((round_num - max(1, round_num - 5) + 1) * self.game_description.n_players)
        return avg_coop_rate

    def update_avg_coop_rate(self, avg_coop_rate: float, round_num: int, history: PlayerHistory) -> float:
        new_avg_coop_rate = (avg_coop_rate * (round_num - 2) + sum(history.opponent_actions[round_num - 1, :] == True)) / (round_num - 1)
        return new_avg_coop_rate

    def count_cooperators(self, round_num: int, history: PlayerHistory) -> int:
        coop_count = sum(history.opponent_actions[round_num, :] == True)
        return coop_count

    def total_payoff_from_C(self, history: PlayerHistory) -> float:
        total_payoff = 0
        for t in range(1, self.game_description.n_rounds + 1):
            if self.count_cooperators(t - 1, history) + 1 >= self.game_description.m:
                total_payoff += self.game_description.k - 1
        return total_payoff

    def total_payoff_from_D(self, history: PlayerHistory) -> float:
        total_payoff = 0
        for t in range(1, self.game_description.n_rounds + 1):
            if self.count_cooperators(t - 1, history) >= self.game_description.m:
                total_payoff += self.game_description.k + 1
        return total_payoff

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= 5:
            self.prev_coop = self.count_cooperators(state.round_number - 1, history)
            if self.prev_coop >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        else:
            self.avg_coop_rate = self.calculate_avg_coop_rate(state.round_number - 1, history)
            if self.avg_coop_rate >= 0.7:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds and self.total_payoff_from_C(history) > self.total_payoff_from_D(history):
            return Action.C

class Strategy_COLLECTIVE_165(BaseStrategy):
    """
    Adaptive Cooperative Escalation Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.previous_cooperators = sum(history.opponent_actions[-1, :])
        if self.previous_cooperators >= self.game_description.m - 1:
            return Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            if self.previous_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_166(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = game_description.m / game_description.n_players
        self.threshold_met_prev_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        num_cooperators_prev = sum(history.opponent_actions[-1, :])
        if self.threshold_met_prev_round is None:
            self.threshold_met_prev_round = num_cooperators_prev >= self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if not self.threshold_met_prev_round and state.round_number > 1 and (history.my_payoffs[-2] <= history.my_payoffs[-1]):
            return Action.D
        elif num_cooperators_prev >= self.game_description.m - 1:
            return Action.C
        else:
            return Action.D
        self.threshold_met_prev_round = num_cooperators_prev >= self.game_description.m

class Strategy_COLLECTIVE_167(BaseStrategy):
    """
    Adaptive Collective Maximization Strategy for the Collective Risk Dilemma game.
    This strategy adapts to various opponent behaviors and aims to balance individual incentives with collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.just_punished = False

    def count_c(self, actions: NDArray[np.bool_]) -> int:
        return sum(actions)

    def is_increasing_trend(self, history: PlayerHistory) -> bool:
        if len(history.my_actions) < 2:
            return True
        current_rate = self.count_c(history.opponent_actions[-1, :]) / self.game_description.n_players
        previous_rate = self.count_c(history.opponent_actions[-2, :]) / self.game_description.n_players
        return current_rate > previous_rate

    def is_decreasing_trend(self, history: PlayerHistory) -> bool:
        if len(history.my_actions) < 2:
            return False
        current_rate = self.count_c(history.opponent_actions[-1, :]) / self.game_description.n_players
        previous_rate = self.count_c(history.opponent_actions[-2, :]) / self.game_description.n_players
        return current_rate < previous_rate

    def should_punish(self, history: PlayerHistory) -> bool:
        if len(history.my_actions) < 1:
            return False
        prev_coop_rate = self.count_c(history.opponent_actions[-1, :]) / self.game_description.n_players
        return self.count_c(history.opponent_actions[-1, :]) < self.game_description.m and (self.is_increasing_trend(history) or self.is_decreasing_trend(history))

    def just_punished(self, history: PlayerHistory) -> bool:
        if len(history.my_actions) == 0:
            return False
        return not history.my_actions[-1]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.just_punished = False
            return Action.C
        prev_coop_rate = self.count_c(history.opponent_actions[-1, :]) / self.game_description.n_players
        if prev_coop_rate >= self.game_description.m / self.game_description.n_players:
            self.just_punished = False
            return Action.C
        if self.is_increasing_trend(history):
            self.just_punished = False
            return Action.C
        if self.should_punish(history):
            self.just_punished = True
            return Action.D
        if self.just_punished:
            self.just_punished = False
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_168(BaseStrategy):
    """
    The Adaptive Collective Contribution strategy encourages cooperation in the 
    collective risk dilemma by adapting to various opponent behaviors and aligning 
    with the collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and history is None):
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] | np.array([True]))
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if prev_round_cooperators < self.game_description.m / 2:
            p_defect = 1 - self.game_description.k / (self.game_description.k + 1)
            return Action.D if random.random() < p_defect else Action.C
        elif prev_round_cooperators >= self.game_description.m:
            p_cooperate = self.game_description.k / (self.game_description.k + 1)
            return Action.C if random.random() < p_cooperate else Action.D
        elif prev_round_cooperators == self.game_description.m // 2:
            return Action.C if random.random() < 0.5 else Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_169(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.consecutive_failure = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperations = sum(history.opponent_actions[-1, :] + [Action.C.value])
        self.threshold_met = previous_round_cooperations >= self.game_description.m
        if not self.threshold_met:
            self.consecutive_failure += 1
        else:
            self.consecutive_failure = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif self.consecutive_failure >= 2:
            return random.choice([Action.C, Action.D])
        else:
            if self.threshold_met:
                p_coop = min(self.game_description.k / (self.game_description.k + 1), 1)
            else:
                p_coop = min((self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players, 1)
            return Action.C if random.random() < p_coop else Action.D

class Strategy_COLLECTIVE_170(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being 
    by adapting to the evolving game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5
        self.avg_C = 0
        self.success_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        if state.round_number <= 3:
            opponent_actions = history.opponent_actions[-1, :]
            my_action = history.my_actions[-1]
            self.avg_C += sum(opponent_actions) + my_action
            if sum(opponent_actions) + my_action >= self.game_description.m:
                self.success_count += 1
            self.avg_C /= state.round_number + 1
        else:
            if self.avg_C >= 0.5 and self.success_count > 1:
                self.p = 1 - (self.game_description.n_players - self.game_description.m) / self.game_description.n_players
            else:
                self.p = (self.game_description.n_players - self.game_description.m) / self.game_description.n_players
            payoff = history.my_payoffs[-1]
            if payoff >= self.game_description.k + 1:
                self.p = min(1, self.p + 0.1)
            elif payoff < 1:
                self.p = max(0, self.p - 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.p else Action.D

class Strategy_COLLECTIVE_171(BaseStrategy):
    """
    A collective strategy called "Adaptive Collective Risk" (ACR).
    It balances individual self-interest with collective well-being,
    while adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_threshold = game_description.m <= game_description.n_players / 2
        self.failure_count = 0
        self.exploitation_detected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.cooperate_threshold else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        my_payoff = history.my_payoffs[-1]
        opponent_payoffs = np.mean(history.opponent_payoffs[-1, :], axis=0)
        reciprocal_cooperation = opponent_cooperations >= self.game_description.m and np.mean(opponent_payoffs[opponent_cooperations > 0]) > my_payoff
        collective_effort_failed = opponent_cooperations < self.game_description.m
        if not self.exploitation_detected:
            avg_cooperative_payoff = np.mean(history.my_payoffs[:state.round_number][history.my_actions[:state.round_number]])
            self.exploitation_detected = my_payoff < avg_cooperative_payoff - 0.5 * self.game_description.k
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if opponent_cooperations >= self.game_description.m / 2 else Action.D
        if collective_effort_failed:
            self.failure_count += 1
        elif self.failure_count > 0 and (not collective_effort_failed):
            self.failure_count = 0
        if reciprocal_cooperation or (not collective_effort_failed and opponent_cooperations >= self.game_description.m):
            return Action.C
        elif collective_effort_failed or (self.exploitation_detected and state.round_number < self.game_description.n_rounds - 2) or (self.failure_count > self.game_description.n_rounds // 4 and (not reciprocal_cooperation)):
            return Action.D
        return Action.C if opponent_payoffs.mean() >= my_payoff else Action.D

class Strategy_COLLECTIVE_172(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Threshold-Triggered Defection (ACTD)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if self.game_description.n_players - previous_round_cooperators < self.game_description.m:
            return Action.D
        if previous_round_cooperators >= self.game_description.m:
            return Action.C
        p_defect = (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players
        if random.random() < p_defect:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if previous_round_cooperators == self.game_description.m:
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_173(BaseStrategy):
    """
    Adaptive Cooperation with Retribution (ACR) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.retribution_due = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.my_actions) == 0):
            return Action.C
        prev_cooperators = sum((1 for action in history.opponent_actions[-1] if action))
        if prev_cooperators >= self.game_description.m:
            return Action.C
        elif prev_cooperators > 0:
            return Action.D
        else:
            prob_coop = 1 / (self.game_description.n_players - self.game_description.m + 1)
            return Action.C if random.random() < prob_coop else Action.D
        if history is not None and len(history.my_actions) > 0:
            prev_threshold_met = sum((1 for action in history.opponent_actions[-2] if action)) >= self.game_description.m
            prev_opponent_defected = any((not action for action in history.opponent_actions[-2]))
            if prev_threshold_met and prev_opponent_defected:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_174(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective risk aversion, adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploration_phase = True
        self.convergence_detected = False
        self.fixed_strategy = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number <= math.ceil(0.2 * self.game_description.n_rounds):
            self.exploration_phase = True
            return [Action.C, Action.D][state.round_number % 2]
        if not self.convergence_detected:
            last_three_rounds = history.opponent_actions[-3:, :]
            all_cooperated = np.all(last_three_rounds == 1)
            all_defected = np.all(last_three_rounds == 0)
            if all_cooperated or all_defected:
                self.convergence_detected = True
                self.fixed_strategy = Action.C if all_cooperated else Action.D
        if self.convergence_detected:
            return self.fixed_strategy
        prev_cooperators = np.sum(history.opponent_actions[-1, :] == 1)
        if prev_cooperators >= self.game_description.m and self.game_description.k >= 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_175(BaseStrategy):
    """
    A collective risk dilemma strategy that adapts to the group's cooperation level.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        coop_rate = prev_cooperators / self.game_description.n_players
        if coop_rate >= self.game_description.m / self.game_description.n_players:
            return Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            return Action.D
        elif self.game_description.k > 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_176(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
        if previous_cooperators >= self.game_description.m - 1:
            return Action.C
        if previous_cooperators < self.game_description.m and self.consecutive_defections < 2:
            self.consecutive_defections += 1
            return Action.D
        if self.game_description.k > 1.5 and sum((1 for action in history.opponent_actions[-1, :] if not action)) > self.game_description.n_players / 2:
            return Action.C
        if self.consecutive_defections >= 3:
            self.consecutive_defections = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            any_cooperators = any((sum((1 for action in history.opponent_actions[i, :] if action)) >= self.game_description.m for i in range(len(history.opponent_actions))))
            return Action.C if any_cooperators else Action.D
        return Action.D

class Strategy_COLLECTIVE_177(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for Collective Risk Dilemma.
    This strategy adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.escalation_factor = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        prev_threshold_met = sum(history.opponent_actions[-1, :] | (history.my_actions[-1] == True)) >= self.game_description.m
        if prev_threshold_met:
            self.escalation_factor = 0
            return Action.C
        else:
            self.escalation_factor += 1
            if self.escalation_factor == self.game_description.m:
                self.escalation_factor = 0
                return Action.C
            elif state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return Action.D

class Strategy_COLLECTIVE_178(BaseStrategy):
    """
    A collective-minded strategy that adapts to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_attempts = 0
        self.success_count = 0
        self.threshold_success_rate = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        previous_round_cooperation = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_round_cooperation >= self.game_description.m:
            self.success_count += 1
        self.coop_attempts += 1
        success_rate = self.success_count / self.coop_attempts if self.coop_attempts > 0 else 0
        if success_rate > self.threshold_success_rate or (success_rate == self.threshold_success_rate and random.random() < 0.5):
            action = Action.C
        else:
            action = Action.D
        if previous_round_cooperation < self.game_description.m:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_179(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_round_cooperators >= self.game_description.m:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        return Action.D if random.random() > (self.game_description.n_players - self.game_description.m) / self.game_description.n_players else Action.C

class Strategy_COLLECTIVE_180(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma Strategy.
    
    This strategy aims to strike a balance between collective cooperation and individual protection against exploitation, 
    adapting its approach as the game progresses to maximize payoffs within the constraints of the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperated_last = True
        self.total_cooperators = 0
        self.learning_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if not self.learning_phase and state.round_number < 6:
            self.learning_phase = True
            cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(self.cooperated_last)
            self.total_cooperators += cooperators_last_round
            if self.total_cooperators >= (state.round_number + 1) / 2 * self.game_description.m:
                return Action.C
            else:
                return Action.D
        rounds_remaining = self.game_description.n_rounds - state.round_number
        if rounds_remaining <= self.game_description.n_rounds * 0.2:
            average_payoff = np.mean(history.my_payoffs) if history.my_payoffs.size else 0
            if average_payoff < 1 - 0.5 + self.game_description.k / 2:
                return Action.D
        cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(self.cooperated_last)
        if cooperators_last_round >= self.game_description.m:
            return Action.C
        elif cooperators_last_round > 0 and state.round_number < self.game_description.n_rounds // 2:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_181(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    
    This strategy starts with cooperation to set a cooperative tone, then adapts 
    its behavior based on the number of cooperators in previous rounds. It has 
    special rules for edge cases and prioritizes collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :])
        if len(history.my_actions) > 0 and history.my_actions[-1]:
            self.prev_cooperators += 1
        if state.round_number == self.game_description.n_rounds - 1:
            if self.game_description.n_rounds % 2 == 1:
                return Action.C
        if self.game_description.n_players == 1:
            return Action.D
        elif self.prev_cooperators == self.game_description.n_players - 1:
            p_coorp = 0.5
            return Action.C if random.random() < p_coorp else Action.D
        else:
            p_defect = 1 - self.prev_cooperators / self.game_description.m
            return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_182(BaseStrategy):
    """
    Adaptive Threshold Cooperator strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 1 or history is None:
            return Action.C if random.random() < self.m / self.n else Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if previous_cooperators >= self.m:
            return Action.C if random.random() < self.k / (self.k + 1) else Action.D
        elif previous_cooperators == self.m - 1:
            return Action.D if random.random() < 0.5 else Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.C

class Strategy_COLLECTIVE_184(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.w = 5
        self.cooperation_threshold = self.m - 1
        self.num_cooperators_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.my_actions) > 0:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            self.num_cooperators_history.append(num_cooperators_prev_round)
            if state.round_number >= self.m - 1:
                avg_cooperation_rate = np.mean(self.num_cooperators_history[-self.w:]) / self.n_players
                self.cooperation_threshold = (self.m - 1) * (avg_cooperation_rate / (self.m / self.n_players)) if avg_cooperation_rate <= self.m / self.n_players else self.m - 1
            cooperate = False
            if num_cooperators_prev_round >= self.cooperation_threshold or (state.round_number > self.game_description.n_rounds - self.m + 1 and num_cooperators_prev_round < self.m):
                cooperate = True
            return Action.C if cooperate else Action.D
        return Action.C

class Strategy_COLLECTIVE_185(BaseStrategy):
    """
    Adaptive Cooperation strategy for Collective Risk Dilemma.
    
    This strategy adapts cooperation probability based on previous round's outcome,
    encouraging continued cooperation when successful and reducing it when not effective.
    It also prevents excessive cooperation when others are not contributing.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_outcome = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_outcome >= self.game_description.m:
            self.coop_prob = min(self.coop_prob + 0.1, 0.9)
        elif prev_outcome < self.game_description.m / 2:
            return Action.D
        else:
            self.coop_prob = max(self.coop_prob - 0.1, 0.1)
        if random.random() < self.coop_prob:
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_COLLECTIVE_186(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy.
    
    Aims to balance individual self-interest with collective cooperation, adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_d = 1 - game_description.m / game_description.n_players
        self.w = min(5, game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_payoff = np.sum(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        if prev_payoff >= self.game_description.k * self.game_description.m:
            action = Action.C
        else:
            action = Action.D if random.random() < self.p_d else Action.C
            avg_coop_level = np.mean(np.sum(history.opponent_actions[-self.w:, :], axis=1) + history.my_actions[-self.w:])
            if avg_coop_level < self.game_description.m / 2:
                self.p_d -= 0.1
                self.p_d = max(0, self.p_d)
            elif avg_coop_level >= self.game_description.m:
                self.p_d += 0.1
                self.p_d = min(1, self.p_d)
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_COLLECTIVE_187(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy adapts to opponent behaviors 
    while promoting collective cooperation and minimizing individual risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_rate = None
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_actions_last_round = history.opponent_actions[-1]
        total_cooperators_last_round = sum(opponent_actions_last_round)
        coop_rate_t_minus_1 = (total_cooperators_last_round + 1) / self.game_description.n_players
        if coop_rate_t_minus_1 >= self.game_description.m / self.game_description.n_players:
            self.p_coop = 0.8
        else:
            p_defect = 0.6
            if self.coop_rate is not None and coop_rate_t_minus_1 > self.coop_rate:
                self.p_coop += 0.1
            elif self.coop_rate is not None and coop_rate_t_minus_1 < 0.8 * self.coop_rate:
                p_defect += 0.15
        action = Action.C if random.random() < self.p_coop else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        self.coop_rate = coop_rate_t_minus_1
        return action

class Strategy_COLLECTIVE_188(BaseStrategy):
    """
    Adaptive Cooperate-to-Threshold (ACT) strategy for Collective Risk Dilemma.
    
    This strategy adapts cooperation levels based on game history and parameters,
    balancing individual self-interest with collective risk aversion.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = game_description.m
        self.failed_cooperation_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if previous_round_cooperators >= self.T:
            return Action.C
        else:
            self.failed_cooperation_count += 1
            if self.failed_cooperation_count >= 3:
                self.T += 1
            return Action.D
        collective_payoff = np.sum(history.my_payoffs) + np.sum(history.opponent_payoffs[-1, :])
        if collective_payoff >= self.game_description.k:
            self.T -= 1 if self.T > self.game_description.m else 0
        else:
            self.T += 1 if self.T < self.game_description.n_players - 1 else 0
        if previous_round_cooperators >= self.game_description.m:
            self.failed_cooperation_count = 0

class Strategy_COLLECTIVE_189(BaseStrategy):
    """
    Adaptive Cooperation with Punishment Strategy.
    
    This strategy prioritizes cooperation and adapts to the actions of others by punishing free-riders.
    It encourages others to cooperate while minimizing individual risk of being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.punish_mode = False
        self.rounds_since_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.threshold_met and history.my_actions[-1]:
            return Action.C
        elif not self.threshold_met and history.my_actions[-1]:
            self.punish_mode = True
            return Action.D
        elif history.my_actions[-1] == False and prev_round_coops >= self.game_description.m:
            return Action.C
        if self.punish_mode:
            self.rounds_since_punish += 1
            if prev_round_coops >= self.game_description.m or self.rounds_since_punish >= 3:
                self.punish_mode = False
                self.rounds_since_punish = 0
                return Action.C
        self.threshold_met = prev_round_coops >= self.game_description.m
        return Action.D

class Strategy_COLLECTIVE_190(BaseStrategy):
    """
    Adaptive Cooperative Threshold Strategy for Collective Risk Dilemma.
    
    This strategy adapts to game parameters and history by adjusting its cooperation probability based on average payoffs.
    It prioritizes cooperation when collective benefit is evident, aiming for a balance between individual self-interest and collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize the ACT strategy with the given game description.

        :param game_description: The CollectiveRiskDescription containing n_players, n_rounds, m, and k.
        """
        self.game_description = game_description
        self.p_c = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determine the action for the given state and history.

        :param state: The current GameState containing round_number.
        :param history: The PlayerHistory or None if in the first round.
        :return: The chosen Action (C or D).
        """
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if state.round_number == 0:
            return Action.C if random.random() < self.p_c else Action.D
        avg_payoff = np.mean(history.my_payoffs[-1:] + history.opponent_payoffs[-1, :])
        if avg_payoff >= self.game_description.k:
            self.p_c += 0.1
        else:
            self.p_c -= 0.1
        self.p_c = max(0, min(self.p_c, 1))
        if history.my_actions[-1] and sum(history.opponent_actions[-1, :]) <= self.game_description.m - 2:
            return Action.C
        return Action.C if random.random() < self.p_c else Action.D

class Strategy_COLLECTIVE_191(BaseStrategy):
    """
    This strategy adapts to various opponent behaviors and depends solely on the game parameters and history.
    It promotes collective success while being mindful of individual self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        prev_round_outcome = history.opponent_actions[-1, :]
        num_cooperators_prev = sum(prev_round_outcome)
        if num_cooperators_prev >= m:
            return Action.C
        elif num_cooperators_prev > 0:
            p = 1 - num_cooperators_prev / n
            return Action.D if random.random() < p else Action.C
        else:
            q = m / n
            return Action.C if random.random() < q else Action.D
        if state.round_number == r - 1:
            total_cooperators = sum((1 for i in range(state.round_number) if sum(history.opponent_actions[i, :]) >= m))
            return Action.C if total_cooperators >= m else Action.D

class Strategy_COLLECTIVE_192(BaseStrategy):
    """
    Adaptive Cooperation with Collective Mindset strategy for the Collective Risk Dilemma game.
    This strategy balances individual self-interest with collective well-being and adapts to the group's behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr_window_size = game_description.n_players
        self.convergence_window_size = 5
        self.acr_history = []
        self.ap_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            return Action.C
        if len(self.acr_history) < self.acr_window_size:
            cooperators = sum(history.opponent_actions[-1, :])
            acr = (cooperators + 1) / (self.game_description.n_players + 1)
            self.acr_history.append(acr)
            return Action.C
        avg_acr = np.mean(self.acr_history)
        if avg_acr >= self.game_description.m / self.game_description.n_players:
            return Action.C
        else:
            defect_prob = (self.game_description.m - avg_acr * self.game_description.n_players) / (self.game_description.n_players - self.game_description.m + 1)
            if random.random() < defect_prob:
                return Action.D
            return Action.C
        if len(self.ap_history) >= self.convergence_window_size:
            avg_ap = np.mean(self.ap_history[-self.convergence_window_size:])
            if avg_ap >= self.game_description.k - 1:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players < self.game_description.m:
            return Action.D

class Strategy_COLLECTIVE_193(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators = [0] * game_description.n_rounds
        self.total_payoff = [0.0] * game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_num_cooperators = self.num_cooperators[state.round_number - 1]
        prev_total_payoff = self.total_payoff[state.round_number - 1]
        if prev_num_cooperators >= self.game_description.m:
            action = Action.C
        elif prev_num_cooperators > self.game_description.m - 1 and prev_num_cooperators <= self.game_description.n_players - 1:
            action = Action.C
        else:
            action = Action.D
        self.num_cooperators[state.round_number] += 1 if action == Action.C else 0
        self.total_payoff[state.round_number] += 1.0 if action == Action.C else 0.0 + self.game_description.k
        if state.round_number == self.game_description.n_rounds - 1:
            if self.total_payoff[state.round_number] >= self.game_description.n_players * self.game_description.k:
                return Action.C
        return action

class Strategy_COLLECTIVE_194(BaseStrategy):
    """
    A hybrid strategy combining elements of tit-for-tat, win-stay-lose-shift, and a threshold-based cooperation mechanism.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        payoff_history = history.my_payoffs
        avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        if sum(prev_round_actions) >= self.game_description.m and sum(prev_round_actions) >= self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D
        prev_action = history.my_actions[-1]
        payoff = payoff_history[-1]
        if payoff > avg_payoff:
            return prev_action
        else:
            return Action.C if prev_action == False else Action.D
        coop_count = sum((1 for action in prev_round_actions if action))
        if coop_count >= self.game_description.m - 1 and sum(prev_round_actions) < self.game_description.n_players // 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif state.round_number < 3 and sum(prev_round_actions) < self.game_description.m:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_195(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Memory (ACM).
    
    The ACM strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and the opponent's behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_coop = 0
        self.num_defect = 0
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        self.num_coop = sum(opponent_actions)
        self.num_defect = len(opponent_actions) - self.num_coop
        if self.num_coop >= self.game_description.m:
            self.total_payoff = self.game_description.k * self.game_description.n_players
        else:
            self.total_payoff = self.game_description.n_players - self.num_coop + self.game_description.k / 2 * self.game_description.n_players
        if self.num_coop >= self.game_description.m:
            return Action.C
        elif self.total_payoff > self.game_description.k / 2 * self.game_description.n_players and self.num_coop < self.game_description.m:
            return Action.C
        elif self.num_defect >= self.num_coop:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_196(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective benefits by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            return Action.C
        prev_round = history.opponent_actions[-1]
        num_coop_prev = sum(prev_round)
        if num_coop_prev >= (self.game_description.n_players + 1) // 2:
            return Action.C
        elif num_coop_prev < self.game_description.m and self.game_description.k > 1.5:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_197(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy for Collective Risk Dilemma.
    Balances individual self-interest with collective cooperation to achieve optimal payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5
        self.q = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.m == self.game_description.n_players / 2 and all(history.opponent_actions[0, :] == history.opponent_actions[0, 0]):
            return random.choice([Action.C, Action.D])
        collective_threshold_met = sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :]) >= self.game_description.m
        if collective_threshold_met:
            action = Action.C
        elif any(history.opponent_actions[-1, :]):
            action = Action.D if random.random() < self.p else Action.C
        else:
            action = Action.C if random.random() < self.q else Action.D
        if not collective_threshold_met and any(history.opponent_actions[-1, :]):
            self.p = min(1.0, self.p + 0.1)
        elif collective_threshold_met:
            self.p = max(0.0, self.p - 0.1)
        if all((not coop for coop in history.opponent_actions[-1, :])) and history.my_payoffs[-1] + self.game_description.k > history.my_payoffs[-1]:
            self.q = min(1.0, self.q + 0.1)
        else:
            self.q = max(0.0, self.q - 0.1)
        return action

class Strategy_COLLECTIVE_198(BaseStrategy):
    """
    Adaptive Cooperation strategy for the Collective Risk Dilemma game.
    
    This strategy aims to balance individual self-interest with collective risk management, 
    adapting to the game's history and parameters. It prioritizes cooperation when it is likely 
    to succeed, while also being cautious not to contribute excessively when others are not cooperating.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= self.game_description.n_players:
            if state.round_number == 0:
                return Action.C
            previous_round_actions = history.opponent_actions[-1, :] if history else []
            majority_action = np.mean(previous_round_actions) >= 0.5 if previous_round_actions.size > 0 else False
            return Action.C if majority_action else Action.D
        elif self.game_description.n_players < state.round_number < self.game_description.n_rounds - self.game_description.n_players:
            previous_round_cooperators = sum(history.opponent_actions[-1, :] if history else [])
            return Action.C if previous_round_cooperators >= self.game_description.m // 2 else Action.D
        else:
            rounds_with_m_plus_cooperators = sum(np.sum(history.opponent_actions, axis=1) >= self.game_description.m)
            total_cooperation_payoff = rounds_with_m_plus_cooperators * self.game_description.k
            potential_defection_gain = (self.game_description.n_rounds - state.round_number) * 1
            return Action.C if total_cooperation_payoff > potential_defection_gain else Action.D

class Strategy_COLLECTIVE_199(BaseStrategy):
    """
    The Adaptive Threshold Reciprocity (ATR) strategy.
    
    This strategy encourages early cooperation, responds to reciprocity,
    and adapts to the game's history to maintain cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        prev_coop = np.sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1)
        if prev_coop >= self.game_description.m / self.game_description.n_players:
            cooperation_prob = self.game_description.k / (self.game_description.k + 1)
        else:
            cooperation_prob = self.cooperation_probability
        avg_payoff = np.mean(history.my_payoffs) if history.my_payoffs.size > 0 else 0
        if avg_payoff < self.game_description.k / 2:
            self.cooperation_probability *= 0.9
        elif avg_payoff > self.game_description.k:
            self.cooperation_probability *= 1.1
        if state.round_number == self.game_description.n_rounds - 1:
            expected_payoff = self.game_description.k + (history.my_payoffs.size > 0) * np.mean(history.my_payoffs)
            if np.sum(history.my_payoffs) >= expected_payoff:
                return Action.D
        return Action.C if random.random() < cooperation_prob else Action.D

class Strategy_COLLECTIVE_200(BaseStrategy):
    """
    Adaptive Threshold Cooperation Strategy.
    
    This strategy adapts to the game's history and parameters, balancing individual self-interest with collective well-being.
    It promotes cooperation while being robust against a wide range of opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def calculate_acr(self, history: PlayerHistory) -> float:
        """Calculate the average cooperation rate (ACR)."""
        if len(history.my_actions) == 0:
            return 0.0
        cooperation_rates = []
        for t in range(len(history.my_actions)):
            cooperators = sum((1 for a in [history.my_actions[t]] + history.opponent_actions[t, :].tolist() if a))
            cooperation_rate = cooperators / self.game_description.n_players
            cooperation_rates.append(cooperation_rate)
        return np.mean(cooperation_rates)

    def calculate_ds(self, history: PlayerHistory) -> list:
        """Calculate the defection streaks (DS)."""
        if len(history.my_actions) == 0:
            return [0] * self.game_description.n_players
        ds = [0] * self.game_description.n_players
        for t in range(len(history.my_actions)):
            for i, a in enumerate([history.my_actions[t]] + history.opponent_actions[t, :].tolist()):
                if not a:
                    ds[i] += 1
                else:
                    ds[i] = 0
        return ds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        acr = self.calculate_acr(history)
        ds = self.calculate_ds(history)
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        if any((d >= 2 for d in ds)):
            return Action.C
        if acr >= self.game_description.m / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_201(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.convergence_phase = False
        self.adapt_defect_count = 0
        self.success_rate_window = []
        self.m_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= math.ceil(self.game_description.n_rounds / 4):
            return Action.C
        if not self.convergence_phase and history is not None:
            self.convergence_phase = True
            self.m_prev = sum(history.opponent_actions[-1, :]) + 1
        if history is not None:
            self.m_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if len(self.success_rate_window) > math.floor(self.game_description.n_rounds / 2):
            self.success_rate_window.pop(0)
        self.success_rate_window.append(self.m_prev >= self.game_description.m)
        success_rate = sum(self.success_rate_window) / len(self.success_rate_window)
        if success_rate < 0.5:
            self.adapt_defect_count += 1
            return Action.D
        if state.round_number >= self.game_description.n_rounds - math.ceil(self.game_description.n_rounds / 4) + 1:
            collective_payoff_prev = (self.m_prev >= self.game_description.m) * self.game_description.k + (self.m_prev < self.game_description.m) * 1
            if collective_payoff_prev >= self.game_description.k * self.game_description.m:
                return Action.C
        if self.m_prev >= self.game_description.m:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_202(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being,
    adapting to the dynamics of the game and the behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        if opponent_cooperations >= self.game_description.m:
            self.p = min(0.9, self.p + 0.1)
        elif self.game_description.n_players > 1:
            my_payoff = history.my_payoffs[-1]
            avg_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
            if my_payoff < avg_opponent_payoff:
                self.p = max(0.1, self.p - 0.1)
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.p else Action.D

class Strategy_COLLECTIVE_203(BaseStrategy):
    """
    The Adaptive Cooperation strategy aims to balance individual self-interest with collective risk management by adaptively responding to the game's history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if prev_cooperators >= self.game_description.m:
            prob_coop = self.game_description.k / (self.game_description.k + 1)
            return Action.C if random.random() < prob_coop else Action.D
        return Action.D

class Strategy_COLLECTIVE_204(BaseStrategy):
    """
    A strategy that prioritizes cooperation while adapting to the collective behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.6
        self.x = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players == 2:
            if state.round_number < self.game_description.n_rounds - 1:
                return Action.C
            else:
                return Action.D
        elif self.game_description.m == 1:
            return Action.C
        prev_CCR = np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players + history.my_actions[-1]
        prev_CCR /= self.game_description.n_players
        if prev_CCR >= self.theta:
            return Action.C
        else:
            consecutive_rounds_below_theta = 0
            for i in range(state.round_number - 1, max(0, state.round_number - 10), -1):
                if (np.sum(history.opponent_actions[i, :]) / self.game_description.n_players + history.my_actions[i]) / self.game_description.n_players < self.theta:
                    consecutive_rounds_below_theta += 1
            p_defect = consecutive_rounds_below_theta / self.game_description.n_rounds * (1 - self.theta)
            if random.random() < p_defect:
                return Action.D
        consecutive_defections = 0
        for i in range(state.round_number - 1, max(0, state.round_number - 10), -1):
            if not history.my_actions[i]:
                consecutive_defections += 1
        if consecutive_defections > self.x and prev_CCR < self.theta:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_205(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Learning (ACL)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        cooperators = sum(opponent_actions)
        coop_percentage = cooperators / self.n_players
        theta = (self.m - 1) / self.n_players
        if coop_percentage >= theta:
            return Action.C
        elif coop_percentage < theta and self.k > 2:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_206(BaseStrategy):
    """
    Adaptive Collective Threshold strategy for the Collective Risk Dilemma.
    
    This strategy prioritizes achieving the collective threshold while being adaptive to changing circumstances.
    It tracks the proportion of cooperative actions and adjusts the cooperation decision accordingly, 
    promoting a balanced approach that considers both individual payoffs and collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_coop_count = sum(history.opponent_actions[-1, :])
        if prev_coop_count >= self.game_description.m:
            self.T += 0.1
        else:
            self.T -= 0.1
        coop_payoff = 1 - 1 + self.game_description.k * self.T
        private_payoff = 1
        cooperate_prob = coop_payoff >= private_payoff
        if prev_coop_count < self.game_description.m:
            cooperate_prob *= 0.8
        if coop_payoff == private_payoff:
            cooperate_prob = 0.5
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_208(BaseStrategy):
    """
    Adaptive Collective Risk strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = game_description.m / game_description.n_players
        self.prev_coop_count = 0
        self.avg_payoff_diff = 0
        self.defection_threshold = -0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :] | np.array([True])) - 1
        self.prev_coop_count = prev_round_cooperators
        if prev_round_cooperators >= self.game_description.m:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        my_prev_payoffs = history.my_payoffs[-1] if len(history.my_payoffs) > 0 else 0
        coop_payoff, defect_payoff = self.get_payoffs(history)
        payoff_diff = coop_payoff - defect_payoff
        self.avg_payoff_diff += payoff_diff
        if self.avg_payoff_diff < self.defection_threshold:
            return Action.D if random.random() > self.game_description.k / (self.game_description.k + 1) else Action.C
        else:
            return Action.C if random.random() < self.coop_prob else Action.D

    def get_payoffs(self, history: PlayerHistory):
        my_coop_rounds = np.where(history.my_actions == True)[0]
        coop_payoff = np.mean([history.my_payoffs[i] for i in my_coop_rounds])
        my_defect_rounds = np.where(history.my_actions == False)[0]
        defect_payoff = np.mean([history.my_payoffs[i] for i in my_defect_rounds])
        return (coop_payoff, defect_payoff)

class Strategy_COLLECTIVE_209(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) strategy.
    
    ACC prioritizes cooperation while adapting to the dynamics of the game and opponent behaviors.
    By tracking the proportion of cooperators and adjusting the cooperation threshold, ACC encourages convergence towards collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.T_c = game_description.m / game_description.n_players
        self.rewards = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        p_c = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if p_c > self.prev_cooperators / self.game_description.n_players:
            return Action.C
        defect_prob = (self.T_c - p_c) / (1 - self.T_c)
        if p_c < 0.2:
            return Action.D
        elif p_c > 0.8:
            return Action.C
        return Action.D if random.random() < defect_prob else Action.C

    def update(self, history: PlayerHistory):
        self.prev_cooperators = sum(history.opponent_actions[-1, :])
        total_reward = np.mean(history.my_payoffs)
        self.rewards.append(total_reward)
        if len(self.rewards) > 5:
            average_reward = np.mean(self.rewards[:-5])
            if total_reward > average_reward:
                self.T_c *= 0.95
            elif total_reward < average_reward:
                self.T_c *= 1.1

    def get_action(self, state: GameState, history: None | PlayerHistory) -> Action:
        action = self.__call__(state, history)
        if history is not None:
            self.update(history)
        return action

class Strategy_COLLECTIVE_210(BaseStrategy):
    """
    A strategy that adapts to the number of cooperators in the previous round.
    It initially cooperates and then adjusts based on the number of cooperators,
    with a mechanism to gradually increase cooperation when it's most needed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5
        self.adjustment_factor = 10 / 100

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_payoff = np.sum(history.my_payoffs[-1])
        if opponent_cooperators >= self.game_description.m / 2:
            return Action.C
        elif total_payoff == 0 and opponent_cooperators < self.game_description.m / 2:
            p = (self.game_description.m - opponent_cooperators) / (self.game_description.n_players - opponent_cooperators)
            if random.random() < p:
                return Action.C
        elif total_payoff > 0 and opponent_cooperators >= self.game_description.m:
            self.cooperation_probability = min(self.cooperation_probability + self.adjustment_factor, 1.0)
        else:
            self.cooperation_probability = max(self.cooperation_probability - self.adjustment_factor, 0.0)
        if random.random() < self.cooperation_probability:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_212(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_quota = game_description.m
        self.adaptive_cooperation_rate = 0.5
        self.previous_cooperation_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        cooperation_quota_t = self.cooperation_quota
        adaptive_cooperation_rate_t = self.calculate_adaptive_cooperation_rate(self.previous_cooperation_rates)
        if len(history.opponent_actions) == 0 or sum(history.opponent_actions[-1, :]) < cooperation_quota_t:
            cooperate_prob = 0.0
        elif adaptive_cooperation_rate_t > 0.5:
            cooperate_prob = adaptive_cooperation_rate_t
        else:
            cooperate_prob = 0.5
        if random.random() < cooperate_prob:
            action = Action.C
        else:
            action = Action.D
        if history is not None and len(history.opponent_actions) > 0:
            self.previous_cooperation_rates.append(sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1))
        return action

    def calculate_adaptive_cooperation_rate(self, previous_cooperation_rates):
        if not previous_cooperation_rates:
            return 0.5
        avg_coop_rate = sum(previous_cooperation_rates) / len(previous_cooperation_rates)
        return avg_coop_rate

    def calculate_cooperation_rate(self, history: PlayerHistory):
        return sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1)

class Strategy_COLLECTIVE_213(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts based on observed behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.observation_phase_length = max(2, math.floor(game_description.n_rounds / 3))
        self.avg_cooperators = 0
        self.sum_cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= self.observation_phase_length:
            return Action.D
        if state.round_number == self.observation_phase_length + 1:
            self.avg_cooperators = np.mean(np.sum(history.opponent_actions[:self.observation_phase_length, :], axis=0))
        if history is not None:
            self.sum_cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if state.round_number > self.observation_phase_length:
            if self.avg_cooperators < self.game_description.m:
                threshold = max(1, self.game_description.m - 1)
            else:
                threshold = self.game_description.m
            if state.round_number == self.game_description.n_rounds:
                return Action.C
            elif self.sum_cooperators_last_round >= threshold:
                return Action.C
            else:
                return Action.D
        prob_defect_coop = 0.1
        if self.sum_cooperators_last_round > self.game_description.m and random.random() < prob_defect_coop:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_214(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.game_description.m:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        else:
            return Action.C if random.random() > (self.game_description.n_players - self.game_description.m) / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = sum(history.my_payoffs)
            return Action.C if self.game_description.k >= total_payoff else Action.D

class Strategy_COLLECTIVE_215(BaseStrategy):
    """
    The ACC strategy is designed to balance individual self-interest with collective cooperation,
    adapting to the behavior of other players and the game's history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :] if history is not None else []
        my_prev_action = history.my_actions[-1] if history is not None else False
        CR = self.calculate_cooperation_rate(prev_round_actions)
        SR = self.calculate_success_rate(history)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif CR >= 0.5 and SR > 0 or (state.round_number in [self.game_description.n_rounds - 2, self.game_description.n_rounds - 1] and SR > 0 and (CR >= 0.5)):
            return Action.C
        else:
            p = 1 - self.game_description.m / self.game_description.n_players
            if random.random() < p:
                return Action.D
            else:
                return Action.C

    def calculate_cooperation_rate(self, actions: NDArray[np.bool_]):
        cooperate_count = sum(actions) + 1
        return cooperate_count / self.game_description.n_players

    def calculate_success_rate(self, history: PlayerHistory):
        if history is None:
            return 0
        success_count = 0
        for i in range(len(history.opponent_actions)):
            round_cooperate_count = sum(history.opponent_actions[i, :]) + int(history.my_actions[i])
            if round_cooperate_count >= self.game_description.m:
                success_count += 1
        return success_count / len(history.opponent_actions)

class Strategy_COLLECTIVE_216(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    This strategy balances individual adaptability with collective cooperation,
    making it a robust strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = 0
        self.r = 0
        self.p = game_description.k / (game_description.k + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.r += 1
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        threshold_met = previous_round_cooperators >= self.game_description.m
        if state.round_number < 5:
            if self.game_description.m < self.game_description.n_players / 2:
                self.p = 0.7
            else:
                self.p = 0.3
        elif threshold_met:
            self.p = min(1, self.p + 0.1)
        elif random.random() < 0.5:
            return Action.D
        else:
            self.p = max(0, self.p - 0.1)
        if threshold_met:
            self.T += 1
        if self.T / self.r > 0.5:
            self.p = min(1, self.p + 0.1)
        elif self.T / self.r < 0.3:
            self.p = max(0, self.p - 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if threshold_met else Action.D
        return Action.C if random.random() < self.p else Action.D

class Strategy_COLLECTIVE_217(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective cooperation, 
    adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.c_prev = 0
        self.p_coop = self.p_init

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        num_coop_prev_round = np.sum(history.opponent_actions[-1, :] != False)
        self.c_prev = num_coop_prev_round / self.game_description.n_players
        if self.c_prev >= self.game_description.m / self.game_description.n_players:
            self.p_coop = max(0.5, min(1, self.game_description.k * self.c_prev))
            action = Action.C if random.random() < self.p_coop else Action.D
        else:
            action = Action.D
        num_coop_curr_round = np.sum(history.opponent_actions[-1, :] != False) + (Action.C == action)
        c_curr = num_coop_curr_round / self.game_description.n_players
        if c_curr >= self.game_description.m / self.game_description.n_players:
            self.p_coop += 0.05 if self.p_coop < 1 else 0
        else:
            self.p_coop -= 0.05 if self.p_coop > 0 else 0
        return action

class Strategy_COLLECTIVE_218(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective responsibility,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        prev_round_actions = history.opponent_actions[-1, :] if history.my_actions.size > 0 else [False] * n_players
        threshold_met = sum(prev_round_actions) + (history.my_actions[-1] if history.my_actions.size > 0 else False) >= m
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperators = sum(history.my_actions) + sum(prev_round_actions)
            cooperate_payoff = 1 - int(history.my_actions[-1]) + k if total_cooperators >= m else 1 - int(history.my_actions[-1])
            defect_payoff = 1
            return Action.C if cooperate_payoff > defect_payoff else Action.D
        if threshold_met:
            p = (k - 1) / k
            return Action.C if random.random() < p else Action.D
        num_cooperators_prev_round = sum(prev_round_actions)
        if num_cooperators_prev_round >= m / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_219(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for Collective Risk Dilemma.
    
    ACT balances individual self-interest with collective well-being by adapting to the number of cooperators and defectors in each round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m > self.game_description.n_players / 2 else Action.D
        prev_m = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if state.round_number > 1:
            self.coop_payoff = self.game_description.k * (self.game_description.n_players - prev_m) / self.game_description.n_players
        if prev_m >= self.game_description.m - 1 or self.coop_payoff > self.game_description.m - prev_m:
            return Action.C
        elif prev_m < self.game_description.m:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C if prev_m >= self.game_description.m else Action.D
        return Action.D

class Strategy_COLLECTIVE_220(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Cooperativity (ATC)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_number = 0
        self.previous_round_payoffs = []
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        threshold_met = len([payoff for payoff in self.previous_round_payoffs if payoff >= self.game_description.k]) >= self.game_description.m
        if threshold_met:
            action = Action.C
        else:
            num_cooperators = sum((1 for payoff in self.previous_round_payoffs if payoff == 0))
            self.p = (self.game_description.m - num_cooperators) / self.game_description.n_players
            action = Action.D if random.random() < self.p else Action.C
        self.round_number += 1
        self.previous_round_payoffs.append(self._calculate_payoff(action, history))
        if state.round_number % 3 == 0:
            avg_payoff = sum(self.previous_round_payoffs[-3:]) / 3
            self.p = max(0, self.p - 0.1) if avg_payoff < 1.5 else min(1, self.p + 0.1)
        if state.round_number == self.game_description.n_rounds:
            return Action.C
        return action

    def _calculate_payoff(self, action: Action, history: PlayerHistory) -> float:
        num_cooperators = sum((1 for opponent_action in history.opponent_actions[-1, :] if opponent_action))
        if num_cooperators >= self.game_description.m:
            return 1 - int(action == Action.C) + self.game_description.k
        else:
            return 1 - int(action == Action.C)

class Strategy_COLLECTIVE_221(BaseStrategy):
    """
    Adaptive Threshold Cooperation strategy.

    This strategy promotes collective cooperation by encouraging initial cooperation,
    rewarding successful cooperation with increased payoffs, and adapting the threshold value based on game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = []
        self.prev_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players >= 2 * self.game_description.m else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        prev_round_payoffs = history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :])
        if prev_round_cooperators >= self.game_description.m or prev_round_payoffs > self.game_description.n_players * (1 - self.game_description.k):
            return Action.C

        def adjust_threshold(m):
            if len(self.prev_cooperators) >= m:
                return max(1, m - 1)
            else:
                return min(self.game_description.n_players - 1, m + 1)
        self.prev_cooperators = [i for i in range(self.game_description.n_players) if history.opponent_actions[-1, i] == 1]
        self.prev_payoffs = history.my_payoffs.tolist() + history.opponent_payoffs[-1, :].tolist()
        self.game_description.m = adjust_threshold(self.game_description.m)
        return Action.D

class Strategy_COLLECTIVE_222(BaseStrategy):
    """
    The Adaptive Collective Convergence (ACC) strategy aims to adaptively converge on a cooperative outcome 
    while being robust to various opponent behaviors. It balances individual self-interest with collective benefit, 
    using a dynamic threshold-based approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        cooperation_rate = cooperators / self.game_description.n_players
        if state.round_number % 2 == 0:
            self.cooperation_threshold = max(0.4, min(0.6, cooperation_rate + (self.game_description.k - 1) / (self.game_description.n_players * self.game_description.k)))
        if cooperation_rate >= self.cooperation_threshold or cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_223(BaseStrategy):
    """
    The Adaptive Collective Defense (ACD) strategy prioritizes collective defense by 
    establishing a foundation for cooperation, encouraging players to maintain cooperation, 
    punishing defectors, and fostering a sense of shared responsibility.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.defector_players = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperation_level = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if previous_cooperation_level >= self.game_description.m:
            if history.my_payoffs[-1] > 0 and history.my_actions[-1]:
                for player in range(self.game_description.n_players - 1):
                    if not history.opponent_actions[-1, player]:
                        self.defector_players.add(player)
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            cooperation_history = sum(history.my_actions)
            if cooperation_history >= self.game_description.n_rounds / 2:
                return Action.C
            else:
                return Action.D
        if self.game_description.n_players == 2:
            return Action.C
        elif self.game_description.m == 1:
            return Action.C if history.my_actions[-1] else Action.D
        return Action.D

class Strategy_COLLECTIVE_224(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Threshold Adjustment (ACTA)
    
    This strategy implements adaptive cooperation based on the collective 
    cooperation rate and adjusts its decision-making to prioritize mutual benefits.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_theta = 0.5
        self.punishment_delta = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        n_cooperators = np.sum(opponent_actions)
        p_coop = n_cooperators / self.game_description.n_players
        if p_coop >= self.threshold_theta:
            return Action.C if n_cooperators >= self.game_description.m - 1 else Action.D
        elif n_cooperators < self.game_description.m + 1:
            return Action.D
        else:
            return Action.C
        n_defectors = self.game_description.n_players - 1 - np.sum(opponent_actions)
        if n_defectors > self.punishment_delta:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and p_coop >= self.threshold_theta:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_225(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_round_cooperators = []
        self.my_prev_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1]
        num_coops_prev = sum(prev_round_opponent_actions)
        my_payoff_last_round = history.my_payoffs[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if num_coops_prev >= self.game_description.m or (num_coops_prev == self.game_description.m - 1 and self.my_prev_action is True):
            action = True
        elif prev_round_opponent_actions[0] is False and self.my_prev_action is True:
            action = False
        elif prev_round_opponent_actions[0] is True and my_payoff_last_round == 1 - self.game_description.k:
            action = True
        else:
            action = False
        if state.round_number == self.game_description.n_rounds - 2 and num_coops_prev < self.game_description.m - 1:
            action = False
        self.prev_round_cooperators.append(prev_round_opponent_actions)
        self.my_prev_action = action
        return Action(action)

class Strategy_COLLECTIVE_226(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy.
    
    This strategy aims to balance individual self-interest with collective cooperation 
    to achieve the highest total payoff over multiple rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        self.M = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.M >= self.game_description.m:
            return Action.C
        elif self.game_description.k > 2 and self.M < self.game_description.m:
            close_to_m = abs(self.M - self.game_description.m) <= math.ceil(0.1 * self.game_description.n_players)
            if close_to_m:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_227(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters, balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_defections = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if self.opponent_defections is None:
            self.opponent_defections = [0] * self.game_description.n_players
        previous_cooperate_count = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        previous_total_payoff = np.sum(history.my_payoffs) + np.sum(history.opponent_payoffs)
        if previous_cooperate_count >= self.game_description.m / 2 or previous_total_payoff >= self.game_description.n_players * self.game_description.k / 2:
            action = Action.C
        elif any((defections >= 2 for defections in self.opponent_defections)):
            action = Action.D
        else:
            action = Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action:
                self.opponent_defections[i] += 1
        if state.round_number == self.game_description.n_rounds - 1:
            previous_cooperate_count = sum(history.opponent_actions[-2, :] | history.my_actions[-2])
            previous_total_payoff = np.sum(history.my_payoffs) + np.sum(history.opponent_payoffs)
            if previous_total_payoff >= self.game_description.n_players * self.game_description.k / 2:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_COLLECTIVE_228(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_count = 0
        self.average_payoff_cooperators = 0
        self.round_number = 0
        self.defected_last_round = False
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None:
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            self.cooperators_count = opponent_cooperations + 1 if history.my_actions[-1] else opponent_cooperations
            payoffs_last_round = np.mean(history.my_payoffs[-3:]) if len(history.my_payoffs) >= 3 else np.mean(history.my_payoffs)
            self.average_payoff_cooperators = (self.average_payoff_cooperators * (len(history.my_payoffs) - 1) + payoffs_last_round) / len(history.my_payoffs)
        if self.cooperators_count < self.game_description.m:
            if self.defected_last_round:
                self.consecutive_defections += 1
            else:
                self.consecutive_defections = 0
            if self.consecutive_defections >= 2:
                return Action.C
            else:
                self.defected_last_round = True
                return Action.D
        else:
            self.defected_last_round = False
            self.consecutive_defections = 0
            if self.average_payoff_cooperators > self.game_description.k / 2:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_229(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def cooperate(self, t, history) -> bool:
        if t == 1:
            return random.random() < 0.5
        theta_t = (self.game_description.m - 1) / (self.game_description.n_players - 1)
        successful_coop_rounds = sum((1 for round in history.my_actions[:-1] if round))
        avg_payoff = np.mean(history.my_payoffs[-2:])
        return successful_coop_rounds >= theta_t and avg_payoff <= self.game_description.k + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if self.cooperate(state.round_number + 1, history) else Action.D

class Strategy_COLLECTIVE_230(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy.

    This strategy adapts to the number of cooperators in previous rounds, 
    promoting a culture of mutual support and encouraging others to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = (game_description.k - 1) / game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :] + (history.my_actions[-1],))
        if num_cooperators_prev_round < self.game_description.m:
            return Action.D
        else:
            action = random.choices([Action.C, Action.D], weights=[self.p, 1 - self.p])[0]
            prev_num_cooperators = sum(history.opponent_actions[-2, :] + (history.my_actions[-2],)) if state.round_number > 1 else 0
            if num_cooperators_prev_round >= self.game_description.m and (num_cooperators_prev_round > prev_num_cooperators or state.round_number == 1):
                self.p = min(self.p + 0.05, 1)
            elif num_cooperators_prev_round < self.game_description.m:
                self.p = max(self.p - 0.05, 0)
            return action
        if state.round_number == self.game_description.n_rounds:
            return Action.C

class Strategy_COLLECTIVE_231(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        last_round_opponent_actions = history.opponent_actions[-1]
        cooperators_last_round = sum(last_round_opponent_actions)
        if cooperators_last_round > self.n_players / 2:
            return Action.C
        elif cooperators_last_round < self.m and 1 - int(history.my_actions[-1]) <= self.k:
            return Action.C
        elif sum(~last_round_opponent_actions) > self.n_players / 2 and history.my_payoffs[-1] <= 1:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_232(BaseStrategy):
    """
    The ACC strategy prioritizes cooperation when it is likely to succeed and adapts to changing circumstances.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_defections_threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if previous_cooperators >= self.game_description.m:
            return Action.C
        elif previous_cooperators > self.game_description.n_players / 2:
            p = (self.game_description.m - previous_cooperators) / (self.game_description.n_players - previous_cooperators)
            return Action.C if random.random() < p else Action.D
        consecutive_defections = sum((1 for i, action in enumerate(history.my_actions[-self.consecutive_defections_threshold:]) if not action))
        if consecutive_defections > self.consecutive_defections_threshold:
            return Action.D
        if history.opponent_payoffs.shape[0] >= 2 and sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1]) >= self.game_description.m:
            p = (self.game_description.m - previous_cooperators) / (self.game_description.n_players - previous_cooperators)
            return Action.C if random.random() < min(p * 1.1, 1) else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_233(BaseStrategy):
    """
    A strategy that adapts to the game parameters and history by maintaining cooperation 
    when the threshold is met, defecting when others don't cooperate, and testing cooperation 
    after consecutive defections.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_defection_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        if self.consecutive_defection_threshold < len([prev_round for prev_round in history.my_actions if not prev_round]):
            return Action.C
        if previous_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_234(BaseStrategy):
    """
    Adaptive collective risk dilemma strategy with exploration.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.tot_cooperators = 0
        self.defection_detected = False
        self.last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return random.choice([Action.C, Action.D]) if random.random() < self.m / self.n_players else Action.D
        else:
            n_rounds = len(history.my_actions)
            last_round_cooperators = sum(history.opponent_actions[-1, :])
            self.tot_cooperators += last_round_cooperators
            avg_cooperation_rate = self.tot_cooperators / (n_rounds * self.n_players)
            if last_round_cooperators < self.m:
                self.defection_detected = True
            if avg_cooperation_rate > 0.5 or (n_rounds >= 2 and sum(history.opponent_actions[-1, :]) > sum(history.opponent_actions[-2, :])):
                return Action.C
            elif self.defection_detected:
                return Action.D
            else:
                if random.random() < 0.1:
                    return Action.C
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if last_round_cooperators < self.m and state.round_number > 3:
            return Action.D

class Strategy_COLLECTIVE_235(BaseStrategy):
    """
    Adaptive Cooperation with Memory Strategy for Collective Risk Dilemma.
    
    This strategy prioritizes collective success while adapting to changing circumstances 
    and opponent behaviors. It promotes a culture of cooperation and mutual benefit by 
    cooperating initially, punishing freeloaders, and forgiving past transgressions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_successes = 0
        self.recent_failure = False
        self.freeloaders = []
        self.forgiveness_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            recent_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            self.consecutive_successes += 1 if recent_cooperators >= self.game_description.m else 0
            self.recent_failure = not recent_cooperators >= self.game_description.m
            opponent_payoffs = history.opponent_payoffs[-1, :]
            average_payoff = np.mean(opponent_payoffs)
            self.freeloaders = [i for i, payoff in enumerate(opponent_payoffs) if payoff > average_payoff]
            self.forgiveness_rounds += 1 if recent_cooperators >= self.game_description.m else 0
        if self.consecutive_successes >= 2:
            return Action.C
        elif self.recent_failure:
            return Action.D
        elif len(self.freeloaders) > 0:
            return Action.D
        elif self.forgiveness_rounds >= 3 and history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_236(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Cooperate (ATC)
    
    This strategy adapts to the collective behavior of the group while promoting cooperation when possible.
    It updates its cooperation probability based on the observed number of cooperators in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.0
        self.delta = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        previous_cooperators = sum(history.opponent_actions[-1, :])
        if previous_cooperators >= self.game_description.m:
            self.alpha += self.delta
        else:
            self.alpha -= self.delta
        previous_defectors = self.game_description.n_players - 1 - previous_cooperators
        p_C = 1 / (1 + math.exp(-self.alpha * (self.game_description.m - previous_defectors) / self.game_description.n_players))
        if random.random() < p_C:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and self.alpha > 0.5:
            return Action.C
        if history.my_payoffs.sum() < self.game_description.m * self.game_description.k and state.round_number < 4:
            self.alpha = 0.0
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_237(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy.
    Balances individual self-interest with collective cooperation, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1]
        num_cooperators = sum(prev_round_opponent_actions)
        if num_cooperators >= self.game_description.m:
            return Action.C
        p_defect = 1 - self.game_description.k / self.game_description.m
        if random.random() < p_defect:
            return Action.D
        if num_cooperators == self.game_description.m - 1:
            p_cooperate = self.game_description.k / self.game_description.m
            if random.random() < p_cooperate:
                return Action.C
        if len(history.my_actions) > 2 and all((not action for action in history.my_actions[-3:])):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_238(BaseStrategy):
    """
    Adaptive Cooperation with History-Dependent Threshold strategy.
    
    This strategy adapts to changing opponent behaviors and encourages cooperation when it is more likely to succeed.
    It starts with a cooperative approach, then adjusts its threshold based on the previous round's outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_m = game_description.m
        self.previous_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_opponent_actions = history.opponent_actions[-1, :]
        self.previous_cooperators = sum(previous_opponent_actions) + int(history.my_actions[-1])
        if self.previous_cooperators >= self.game_description.m:
            self.threshold_m = max(2, self.threshold_m - 1)
        else:
            self.threshold_m = min(self.game_description.n_players - 1, self.threshold_m + 1)
        if self.previous_cooperators < self.threshold_m:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_239(BaseStrategy):
    """
    The Adaptive Collective Thrift (ACT) strategy is designed to balance individual self-interest with collective risk aversion in a repeated, simultaneous-move game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.prev_defectors = game_description.n_players
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        my_last_payoff = self.game_description.k if sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :]) >= self.game_description.m else 0
        average_payoff = np.mean(history.my_payoffs)
        coop_prob_adjustment = (self.total_payoff < average_payoff) * 0.1
        if self.prev_cooperators < self.game_description.m:
            coop_prob = max((self.game_description.m - self.prev_cooperators) / self.game_description.n_players + coop_prob_adjustment, 0)
        else:
            defect_prob = (self.prev_defectors + 1) / self.game_description.n_players
            coop_prob = min(1 - defect_prob + coop_prob_adjustment, 1)
        if state.round_number == self.game_description.n_rounds - 1 and self.prev_cooperators >= self.game_description.m - 1:
            return Action.C
        action = random.random() < coop_prob
        if history.my_actions[-1]:
            self.prev_cooperators += 1
        else:
            self.prev_defectors -= 1
        self.total_payoff += my_last_payoff
        return Action.C if action else Action.D

class Strategy_COLLECTIVE_240(BaseStrategy):
    """
    The Adaptive Collective Risk (ACR) strategy balances individual self-interest with collective well-being,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.coop_prob = game_description.m / game_description.n_players
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        avg_payoff_prev = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        if avg_payoff_prev >= (1 + self.game_description.k) / 2:
            self.coop_prob = max(self.game_description.m / self.game_description.n_players, 0.5)
        else:
            self.coop_prob = min(1 - self.game_description.m / self.game_description.n_players, 0.5)
        if state.round_number % 3 == 0 and state.round_number > 0:
            avg_payoff_3r = np.mean(history.my_payoffs[-3:] + history.opponent_payoffs[-3:, :].mean(axis=1))
            if avg_payoff_3r >= (1 + self.game_description.k) / 2:
                self.coop_prob += 0.1
            else:
                self.coop_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            avg_payoff_3r = np.mean(history.my_payoffs[-3:] + history.opponent_payoffs[-3:, :].mean(axis=1))
            return Action.C if avg_payoff_3r >= (1 + self.game_description.k) / 2 else Action.D
        elif all((not action for action in history.my_actions[-2:])) and all((not opponent_action for opponent_action in history.opponent_actions[-2:, :].flatten())):
            self.coop_prob = self.game_description.m / self.game_description.n_players
        return Action.C if random.random() < self.coop_prob else Action.D

class Strategy_COLLECTIVE_242(BaseStrategy):
    """
    A collective strategy that adapts cooperation levels based on past game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = (game_description.m - 1) / (game_description.n_players - 1)
        self.sensitivity_parameter = 0
        self.total_cooperation_payoff = 0
        self.total_game_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(history.my_payoffs) > 0:
            self.sensitivity_parameter = self.total_cooperation_payoff / sum(history.my_payoffs)
            self.cooperation_threshold *= 1 + self.sensitivity_parameter * (self.game_description.k - 1)
        if len(history.opponent_actions) > 0:
            num_cooperators = np.sum(np.concatenate((history.opponent_actions[-1, :], [history.my_actions[-1]])))
        else:
            num_cooperators = self.game_description.n_players
        if num_cooperators < self.game_description.m and random.random() < (self.game_description.m - num_cooperators) / self.game_description.n_players:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif num_cooperators >= self.cooperation_threshold * self.game_description.n_players:
            return Action.D
        else:
            return Action.C
        if len(history.my_payoffs) > 0:
            self.total_game_payoff += history.my_payoffs[-1]
            if history.my_actions[-1]:
                self.total_cooperation_payoff += history.my_payoffs[-1]

class Strategy_COLLECTIVE_243(BaseStrategy):
    """
    The Adaptive Collective Defender (ACD) strategy balances individual self-interest 
    with collective cooperation, adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number < math.floor(self.game_description.n_rounds / 3):
            cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            avg_cooperator_payoff = np.mean([payoff for payoff, action in zip(history.my_payoffs, history.my_actions) if action])
            avg_defector_payoff = np.mean([payoff for payoff, action in zip(history.my_payoffs, history.my_actions) if not action])
            if cooperators_last_round >= self.game_description.m - 1 or (cooperators_last_round < self.game_description.m - 1 and avg_cooperator_payoff > avg_defector_payoff):
                return Action.C
            else:
                return Action.D
        elif math.floor(self.game_description.n_rounds / 3) <= state.round_number < math.floor(2 * self.game_description.n_rounds / 3):
            cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            avg_cooperator_payoff_over_r6 = np.mean([payoff for payoff, action in zip(history.my_payoffs[-math.floor(self.game_description.n_rounds / 6):], history.my_actions[-math.floor(self.game_description.n_rounds / 6):]) if action])
            avg_defector_payoff_over_r6 = np.mean([payoff for payoff, action in zip(history.my_payoffs[-math.floor(self.game_description.n_rounds / 6):], history.my_actions[-math.floor(self.game_description.n_rounds / 6):]) if not action])
            project_success_rate = sum((1 for i in range(state.round_number) if sum([action for action in history.opponent_actions[i, :]]) + (history.my_actions[i] == True) >= self.game_description.m)) / state.round_number
            if cooperators_last_round >= self.game_description.m and avg_cooperator_payoff_over_r6 > avg_defector_payoff_over_r6 and (project_success_rate >= 0.5):
                return Action.C
            else:
                return Action.D
        elif math.floor(2 * self.game_description.n_rounds / 3) <= state.round_number < self.game_description.n_rounds - 1:
            cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            avg_cooperator_payoff_over_r = np.mean([payoff for payoff, action in zip(history.my_payoffs, history.my_actions) if action])
            avg_defector_payoff_over_r = np.mean([payoff for payoff, action in zip(history.my_payoffs, history.my_actions) if not action])
            if cooperators_last_round >= self.game_description.m or avg_cooperator_payoff_over_r > avg_defector_payoff_over_r:
                return Action.C
            elif sum((1 for i in range(state.round_number) if sum([action for action in history.opponent_actions[i, :]]) + (history.my_actions[i] == True) >= self.game_description.m)) / state.round_number < 0.5 and state.round_number - math.floor((self.game_description.n_rounds - self.game_description.m + 1) / 2) > 0:
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            project_success_rate = sum((1 for i in range(state.round_number) if sum([action for action in history.opponent_actions[i, :]]) + (history.my_actions[i] == True) >= self.game_description.m)) / state.round_number
            cooperation_leads_to_higher_payoff = np.mean(history.my_payoffs[-math.floor(self.game_description.n_rounds / 6):]) > np.mean([payoff for payoff, action in zip(history.my_payoffs, history.my_actions) if not action])
            if project_success_rate >= 0.5 and cooperation_leads_to_higher_payoff:
                return Action.C
            else:
                return Action.D
        elif self.game_description.m == 1:
            return Action.C

class Strategy_COLLECTIVE_244(BaseStrategy):
    """
    The Adaptive Threshold Cooperator (ATC) strategy prioritizes cooperation while protecting against exploitation and adapting to the collective behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1]
        prev_cooperators = sum(prev_opponent_actions)
        prev_defectors = self.game_description.n_players - 1 - prev_cooperators
        is_last_round = state.round_number == self.game_description.n_rounds - 1
        if prev_cooperators >= math.ceil(self.game_description.m / 2):
            return Action.C
        elif prev_cooperators < math.ceil(self.game_description.m / 2) and prev_defectors > math.floor((self.game_description.n_players - self.game_description.m) / 2):
            return Action.D
        elif is_last_round:
            successful_rounds = sum((1 for actions in history.opponent_actions if sum(actions) + 1 >= self.game_description.m))
            if successful_rounds >= math.ceil(self.game_description.n_rounds / 2):
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_245(BaseStrategy):
    """
    A strategy that adapts to the game's history and opponent behaviors,
    prioritizing collective cooperation when the reward is certain and defects
    when the risk is high or uncertain.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_payoffs = [0] * game_description.n_rounds
        self.p_cooperate = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(prev_round_opponent_actions) + (history.my_actions[-1] == True)
        self.p_cooperate = 0.7 + 0.3 * (num_cooperators / self.game_description.n_players)
        if state.round_number > 2:
            avg_payoff = sum(self.round_payoffs[:state.round_number // 2]) / (state.round_number // 2)
            if history.my_payoffs[-1] > avg_payoff:
                self.p_cooperate += 0.05
            elif history.my_payoffs[-1] < avg_payoff:
                self.p_cooperate -= 0.05
        self.p_cooperate = max(0, min(self.p_cooperate, 1))
        if random.random() < self.p_cooperate:
            return Action.C
        else:
            return Action.D

    def calculate_payoff(self, n: int, m: int, k: float, actions: NDArray[np.bool_], t: int) -> float:
        num_cooperators = sum(actions[t - 1])
        if num_cooperators >= m:
            return 0 + k
        else:
            return 1 - actions[t - 1][self.game_description.n_players - 1] + 0

class Strategy_COLLECTIVE_246(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)

    This strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        prev_coop_met = sum(history.opponent_actions[-1, :] | history.my_actions[-1]) >= self.game_description.m
        if not prev_coop_met and any((not action for action in history.opponent_actions[-1, :])):
            self.T += 1 if history.my_actions[-1] else -1
            self.T = max(0, min(self.T, self.game_description.n_players))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        elif self.T >= self.game_description.m and self.game_description.k >= 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_247(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for 
    collective cooperation to achieve a threshold and reap a reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_cooperations = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            cooperators_t = sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == True)
        else:
            cooperators_t = 0
        if cooperators_t >= self.game_description.m:
            return Action.C
        if history is not None and len(history.opponent_actions) > 1:
            previous_cooperators_t = sum(history.opponent_actions[-2, :] == True) + (history.my_actions[-2] == True)
            total_cooperations_increase = cooperators_t > previous_cooperators_t
        else:
            total_cooperations_increase = False
        if not total_cooperations_increase and self.total_cooperations > 0:
            return Action.D
        self.total_cooperations += 1
        return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if any(history.my_actions):
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_248(BaseStrategy):
    """
    Adaptive Collective Risk (ACR) strategy for the Collective Risk Dilemma game.

    This strategy aims to balance individual self-interest with group cooperation,
    adapting to the game's dynamics by observing previous rounds' outcomes and adjusting behavior accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = [0] * game_description.n_players
        self.threshold = game_description.m
        self.tau = 0.5
        self.payoffs = []
        self.collective_payoff_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        if opponent_cooperation_rate >= self.tau or sum(self.cooperation_rate) >= self.threshold:
            cooperate = True
        elif sum(self.cooperation_rate) < self.threshold:
            cooperate = random.random() < self.game_description.m / self.game_description.n_players
        else:
            cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = True
        self.cooperation_rate[state.round_number % self.game_description.n_players] = int(cooperate)
        prev_round_payoff = sum(history.my_payoffs) / len(history.my_payoffs)
        self.collective_payoff_history.append(prev_round_payoff)
        if state.round_number > self.game_description.n_rounds // 2:
            avg_payoff_last_r_div_2_rounds = np.mean(self.collective_payoff_history[-self.game_description.n_rounds // 2:])
            if prev_round_payoff > avg_payoff_last_r_div_2_rounds:
                self.threshold += 1
            elif prev_round_payoff < avg_payoff_last_r_div_2_rounds:
                self.threshold -= 1
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_249(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game, 
    which balances individual self-interest with collective risk management.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_prev = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.n_players <= 10:
                return Action.C
            else:
                return Action.D
        c_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        self.p_prev = c_prev / self.game_description.n_players
        if self.game_description.m <= math.ceil(self.p_prev * self.game_description.n_players) or self.game_description.k > 1 / self.p_prev:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            expected_payoff_C = 1 - history.my_actions[-1] + self.game_description.k
            expected_payoff_D = 1 - history.my_actions[-1]
            if expected_payoff_C > expected_payoff_D:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_250(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)
    
    This strategy balances individual self-interest with collective risk management, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_payoff_threshold = 1.2 * game_description.n_players
        self.p_conscience = min(0.5, game_description.m / game_description.n_players)
        self.round_num = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_num = state.round_number
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        prev_total_payoff = np.sum(history.my_payoffs) + np.sum(history.opponent_payoffs)
        if self.round_num <= 3:
            return Action.C
        elif prev_cooperators < self.game_description.m and prev_total_payoff < self.total_payoff_threshold:
            return Action.C
        elif prev_cooperators >= self.game_description.m and self.game_description.k > 1.5:
            return Action.C
        elif random.random() < self.p_conscience:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_251(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy balances individual self-interest with collective cooperation.
    It adapts to the game's history and parameters, promoting a cooperative mindset while being robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_tracker = 0
        self.defection_penalty = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        self.cooperation_tracker += 1 if previous_round_cooperators >= self.game_description.m else 0
        if previous_round_cooperators < self.game_description.m:
            self.defection_penalty = min(self.defection_penalty + 0.05, 1)
        threshold_met = self.cooperation_tracker >= self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if threshold_met:
            cooperation_probability = max(0.5, min(0.8, 0.8 - self.defection_penalty / 10))
        else:
            cooperation_probability = max(0.5, min(1 - (self.game_description.n_players - self.game_description.m) / self.game_description.n_players - self.defection_penalty / 10, 0.8))
        return Action.C if random.random() < cooperation_probability else Action.D

class Strategy_COLLECTIVE_252(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop = sum((1 for t in range(state.round_number) if np.sum(history.opponent_actions[t, :]) >= self.game_description.m)) / state.round_number
        threshold = (self.game_description.m - 1) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_coop <= threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_253(BaseStrategy):
    """
    A strategy that balances cooperation and self-interest while adapting to the history of play.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_t = 0
        self.f_t = 0
        self.s_t = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        my_last_action = history.my_actions[-1]
        self.m_t += 1 if my_last_action else 0
        self.f_t += 1 if self.m_t < self.game_description.m and state.round_number > 0 else 0
        self.s_t += 1 if opponent_cooperations >= self.game_description.m else 0
        if self.m_t >= self.game_description.m:
            return Action.C
        if self.f_t > 0.2 * self.game_description.n_rounds:
            return Action.D
        elif opponent_cooperations > 0:
            return Action.C
        else:
            cooperation_probability = max(0.3, min(0.5, 1 - state.round_number / self.game_description.n_rounds))
            return Action.C if random.random() < cooperation_probability else Action.D
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.C if opponent_cooperations > 0 else Action.D

class Strategy_COLLECTIVE_254(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma that adapts to the game's history and parameters.
    It balances individual self-interest with collective risk management while adapting to diverse opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        num_cooperators_prev = sum(prev_round)
        if num_cooperators_prev == 0:
            p = 0.75
            return Action.C if random.random() < p else Action.D
        elif num_cooperators_prev >= self.game_description.m:
            return Action.C
        else:
            defect_prob = (self.game_description.m - num_cooperators_prev) / self.game_description.m
            if random.random() < defect_prob:
                return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_255(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy aims to balance individual self-interest 
    with collective cooperation, adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def calculate_cooperation_rate(self, actions, n):
        cooperations = sum(actions)
        return cooperations / n

    def get_opponent_cooperation_rate(self, actions):
        return self.calculate_cooperation_rate(actions[actions != 0], len(actions) - 1)

    def collective_threshold_met(self, actions, m):
        return sum(actions) >= m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C if m <= n / 2 else Action.D
        CR = self.calculate_cooperation_rate(history.opponent_actions[-1, :] | history.my_actions[-1], n)
        if CR >= m / n and state.round_number < r - 1:
            return Action.C
        opponent_CR = self.get_opponent_cooperation_rate(history.opponent_actions[-1, :])
        if opponent_CR > 0.5:
            return Action.C
        if not self.collective_threshold_met(history.opponent_actions[-1, :] | history.my_actions[-1], m):
            return Action.D
        if state.round_number == r - 1 and opponent_CR >= m / n:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_256(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective welfare,
    adapts to changing circumstances, and maintains robustness against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif prev_cooperators == 0:
            return Action.C
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_257(BaseStrategy):
    """
    Adaptive Cooperation Strategy: 
    Balances individual self-interest with collective risk management, adapting to changes in the game's history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.successful_rounds = 0
        self.total_rounds = 0
        self.consecutive_defections = 0
        self.cautious_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_rounds += 1
        prev_cooperators = sum(history.opponent_actions[-2, :] if history.my_actions.shape[0] > 1 else [False])
        if prev_cooperators >= self.game_description.m:
            self.successful_rounds += 1
        elif all((not opponent_action for opponent_action in history.opponent_actions[-1, :])):
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = 0
        if self.consecutive_defections >= 2 and (not self.cautious_mode):
            self.cautious_mode = True
        elif prev_cooperators >= self.game_description.m or state.round_number == self.game_description.n_rounds - 1:
            self.cautious_mode = False
        if self.cautious_mode:
            return Action.C if random.random() < 0.5 else Action.D
        elif prev_cooperators >= self.game_description.m:
            p = self.successful_rounds / self.total_rounds if self.successful_rounds > 0 else 0
            return Action.C if random.random() < p else Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_258(BaseStrategy):
    """
    This strategy implements the Adaptive Collective Optimism (ACO) approach.
    It balances individual self-interest with collective well-being, promoting cooperation while adapting to the actions of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_payoff = 0
        self.cooperate_prob = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        previous_round_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(previous_round_actions)
        my_payoffs = np.array(history.my_payoffs)
        avg_payoff = np.mean(my_payoffs)
        if num_cooperators < self.game_description.m:
            defect_prob = 1 - self.game_description.m / self.game_description.n_players
            return Action.D if random.random() < defect_prob else Action.C
        if avg_payoff >= self.game_description.k / 2:
            action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1 and avg_payoff < self.game_description.k / 2:
            action = Action.D
        else:
            action = Action.D
        if num_cooperators >= self.game_description.m:
            self.avg_payoff = self.game_description.k / 2
        return action

class Strategy_COLLECTIVE_259(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for Collective Risk Dilemma.

    This strategy balances individual self-interest with collective well-being.
    It adapts cooperation levels based on the group's performance, promoting robust and resilient cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = self.game_description.m / self.game_description.n_players
        self.average_cooperation_rate = None
        self.previous_average_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[self.cooperation_probability, 1 - self.cooperation_probability])[0]
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        total_players = self.game_description.n_players - 1
        self.average_cooperation_rate = (opponent_cooperations + int(history.my_actions[-1])) / (total_players + 1)
        if self.average_cooperation_rate >= self.game_description.m / self.game_description.n_players:
            return Action.C
        delta_rounds = max(1, self.game_description.n_rounds // 5)
        if state.round_number % delta_rounds == 0 and history.my_payoffs.size > 0:
            average_payoff = np.mean(history.my_payoffs[-delta_rounds:])
            threshold = 1 + self.game_description.k / 2
            adjustment = -0.05 if average_payoff < threshold else 0.05
            self.cooperation_probability += adjustment
            self.cooperation_probability = max(0, min(self.cooperation_probability, 1))
        return random.choices([Action.C, Action.D], weights=[self.cooperation_probability, 1 - self.cooperation_probability])[0]

class Strategy_COLLECTIVE_260(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Social Learning.
    
    This strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.t = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        self.m_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.m_prev >= self.game_description.m:
            return Action.C
        if history.my_payoffs[-1] < 1:
            return Action.D
        if history.my_payoffs[-1] >= self.game_description.k:
            return Action.C
        return Action.D

    def update(self):
        self.t += 1

class Strategy_COLLECTIVE_261(BaseStrategy):
    """
    Collective Risk Dilemma strategy using adaptive cooperativity.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_threshold_met = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] >= self.game_description.m
        consecutive_coop_count = self.consecutive_coop_count
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_threshold_met or (consecutive_coop_count == self.game_description.m - 1 and (not prev_threshold_met)):
            self.consecutive_coop_count = min(consecutive_coop_count + 1, self.game_description.m)
            return Action.C
        elif history.my_payoffs[-1] > 1:
            self.consecutive_coop_count = 0
            return Action.C
        else:
            self.consecutive_coop_count = 0
            return Action.D

class Strategy_COLLECTIVE_262(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors 
    and focuses on maximizing the group's payoff while being robust.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = 0
        self.window_size = 5
        self.avg_cooperate_window = []
        self.defect_prob = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        M = sum(history.opponent_actions[-1, :])
        if M >= self.game_description.m:
            action = Action.C
        else:
            p = (self.game_description.m - M) / self.game_description.n_players
            self.defect_prob = min(self.defect_prob + 0.2, 1)
            action = Action.D if random.random() < p * self.defect_prob else Action.C
        self.cooperate_count += 1 if action == Action.C else 0
        self.avg_cooperate_window.append(self.cooperate_count / (state.round_number + 1))
        if len(self.avg_cooperate_window) > self.window_size:
            self.avg_cooperate_window.pop(0)
        M_avg = sum(self.avg_cooperate_window) / len(self.avg_cooperate_window)
        if M_avg < self.game_description.m and self.defect_prob < 1:
            self.defect_prob += 0.2
        return action

class Strategy_COLLECTIVE_263(BaseStrategy):
    """
    A collective strategy called "Adaptive Threshold Cooperation" (ATC). 
    ATC balances individual self-interest with collective cooperation, adapting to game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_outcome = history.my_payoffs[-1]
        threshold_met = previous_round_outcome > 1 - self.game_description.k
        opponent_cooperation_last = sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1)
        if state.round_number >= 2:
            opponent_cooperation_second_last = sum(history.opponent_actions[-2, :]) / (self.game_description.n_players - 1)
        else:
            opponent_cooperation_second_last = 0
        punishment_active = False
        if history.my_payoffs[-1] < previous_round_outcome and state.round_number > 1:
            punishment_active = True
        if threshold_met:
            if opponent_cooperation_last >= opponent_cooperation_second_last or state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            else:
                return Action.D
        elif opponent_cooperation_last > opponent_cooperation_second_last and (not punishment_active):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_264(BaseStrategy):
    """
    A collective risk monitor strategy that balances individual self-interest 
    with the goal of achieving collective success in the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        if state.round_number < self.game_description.n_rounds:
            p_defect = (self.game_description.m - prev_round_cooperators) / self.game_description.n_players
            return Action.D if random.random() < p_defect else Action.C
        total_payoff_so_far = np.sum(history.my_payoffs)
        if total_payoff_so_far >= (state.round_number - 1) * self.game_description.k:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_265(BaseStrategy):
    """
    Adaptive Collective Conscience strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_probability = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_probability else Action.D
        previous_round_cooperation_rate = np.mean(history.opponent_actions[-1, :] + history.my_actions[-1])
        if previous_round_cooperation_rate >= self.game_description.m / self.game_description.n_players:
            cooperate_probability = 0.8
        else:
            cooperate_probability = 0.2
        if self.game_description.k > 2 and np.mean(history.my_payoffs[-1]) + self.game_description.k > 1:
            cooperate_probability += 0.1
        elif self.game_description.k < 1.5 and np.mean(history.my_payoffs[-1]) <= 1:
            cooperate_probability -= 0.1
        cooperate_probability = max(0, min(cooperate_probability, 1))
        return Action.C if random.random() < cooperate_probability else Action.D

class Strategy_COLLECTIVE_266(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective responsibility,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_round_cooperators <= self.game_description.m - 1:
            return Action.C
        if self.game_description.k * prev_round_cooperators >= self.game_description.m:
            coop_prob = prev_round_cooperators / self.game_description.n_players
            return Action.C if random.random() < coop_prob else Action.D
        defect_prob = (self.game_description.m - prev_round_cooperators) / self.game_description.m
        return Action.D if random.random() < defect_prob else Action.C
        if state.round_number == self.game_description.n_rounds:
            return Action.C if prev_round_cooperators >= self.game_description.m else Action.D

class Strategy_COLLECTIVE_267(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) strategy for the Collective Risk Dilemma.
    
    ACC balances individual self-interest with the need for cooperation by adapting to 
    the actions of others and prioritizing cooperation when the collective reward is within reach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_cooperators = 0
        self.previous_payoff = 0
        self.threshold = math.floor(game_description.m / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + 1
        current_payoff = self.calculate_payoff(history)
        if current_payoff > self.previous_payoff:
            self.threshold = math.ceil((self.game_description.m + 1) / 2)
        if previous_round_cooperators < self.game_description.m and state.round_number < self.game_description.n_rounds - 1:
            return Action.D
        if previous_round_cooperators < self.threshold:
            action = Action.D
        else:
            action = Action.C
        self.previous_cooperators = previous_round_cooperators
        self.previous_payoff = current_payoff
        return action

    def calculate_payoff(self, history: PlayerHistory) -> float:
        """
        Calculate the player's payoff in the most recent round.
        
        :param history: The player's history.
        :return: The player's payoff in the most recent round.
        """
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + 1
        if previous_round_cooperators < self.game_description.m:
            return 1 - int(bool(history.my_actions[-1]))
        else:
            return 1 - int(bool(history.my_actions[-1])) + self.game_description.k

class Strategy_COLLECTIVE_268(BaseStrategy):
    """
    Adaptive Cooperation strategy for Collective Risk Dilemma game.

    This strategy balances individual self-interest with collective well-being,
    adapting cooperation rates based on collective success and individual payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        prev_round_cooperate = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        threshold_met = prev_round_cooperate >= self.game_description.m
        if threshold_met:
            cooperate_prob = self.game_description.k / (self.game_description.k + 1)
        else:
            cooperate_prob = 1 - self.game_description.m / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = sum(history.my_payoffs)
            expected_cooperate_payoff = self.game_description.k * (state.round_number + 1)
            cooperate_prob = 1 if total_payoff < expected_cooperate_payoff else 0
        if prev_round_cooperate == self.game_description.m:
            cooperate_prob = 0.5
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_269(BaseStrategy):
    """
    Adaptive Collective Optimism strategy.
    
    This strategy adapts to the history of play and opponent behaviors,
    promoting cooperation among players in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.adaptive_thresholds = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] & history.my_actions[-1:])
        total_payoff = sum(np.where(history.my_actions[-1:], 1 - self.k, 1))
        if prev_cooperators < self.m:
            return Action.D
        elif total_payoff >= self.n_players * (1 + self.k) / 2:
            for opponent in range(self.n_players):
                if history.opponent_actions[-1, opponent] == False and prev_cooperators >= self.m:
                    self.adaptive_thresholds[opponent] = self.adaptive_thresholds.get(opponent, 0) + 1
            return Action.C
        else:
            if all((threshold <= self.m for threshold in self.adaptive_thresholds.values())):
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_270(BaseStrategy):
    """
    A strategy for Collective Risk Dilemma that balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = 0
        self.coop_rate_threshold = game_description.m / game_description.n_players
        self.sustain_rounds = 3
        self.punish_rounds = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] == True)
        coop_rate = previous_cooperators / self.game_description.n_players
        if coop_rate >= self.coop_rate_threshold or (self.punish_rounds and state.round_number < history.my_payoffs.shape[0] + 3):
            action = Action.C
        else:
            action = Action.D
        self.cooperators += 1 if action == Action.C else 0
        self.update_coop_rate_threshold(previous_cooperators)
        if previous_cooperators >= self.game_description.m and state.round_number < self.game_description.n_rounds - self.sustain_rounds:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if previous_cooperators < self.game_description.m and (not self.punish_rounds):
            self.punish_rounds = True
        return action

    def update_coop_rate_threshold(self, cooperators: int):
        if cooperators >= self.game_description.m:
            self.coop_rate_threshold = min(1, self.coop_rate_threshold + 0.1)
        else:
            self.coop_rate_threshold = max(self.game_description.m / self.game_description.n_players, self.coop_rate_threshold - 0.1)

    def reset(self):
        self.__init__(self.game_description)

class Strategy_COLLECTIVE_271(BaseStrategy):
    """
    Adaptive Collective Risk strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m / game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.m else Action.D
        c_prev = np.mean(history.opponent_actions[-1, :])
        if c_prev >= self.m:
            prob_c = max(0.5, c_prev * self.k / (self.k + 1))
        else:
            prob_c = min(0.5, (1 - c_prev) / (1 - self.m))
        if history.my_payoffs[-1] < self.k and (not history.my_actions[-1]):
            prob_d = 0.5
        else:
            prob_d = 0
        if history.my_payoffs[-1] >= self.k and history.my_actions[-1]:
            prob_c = 0.5
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D if np.mean(history.my_payoffs) < (self.game_description.n_players - 1) * self.k else Action.C
        return Action.C if random.random() < prob_c else Action.D

class Strategy_COLLECTIVE_273(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        majority_cooperated = prev_cooperators >= self.game_description.n_players / 2
        threshold_at_risk = prev_cooperators < self.game_description.m and prev_cooperators > self.game_description.n_players / 4
        if threshold_at_risk:
            prob = (self.game_description.m - prev_cooperators) / (self.game_description.n_players - self.game_description.n_players / 4)
            return Action.C if random.random() < prob else Action.D
        opponents_avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        if opponents_avg_payoff > 1.5 * self.game_description.k or prev_cooperators <= self.game_description.n_players / 4:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if majority_cooperated else Action.D

class Strategy_COLLECTIVE_274(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy.
    
    This strategy adapts to various opponent behaviors and prioritizes collective success by:
    - Cooperating initially to establish trust.
    - Encouraging cooperation when the threshold is within reach.
    - Punishing defectors to maintain accountability.
    - Rewarding continued cooperation with increased payoffs.
    - Adapting to changing game conditions and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1]
        prev_cooperators = sum(prev_opponent_actions)
        threshold_met = prev_cooperators >= self.game_description.m / 2
        punish_defector = any((not action and history.my_payoffs[-1] > 0 for action in prev_opponent_actions))
        total_payoff_from_cooperation = sum((self.game_description.k * num_cooperators for num_cooperators, payoff in zip(history.opponent_actions.sum(axis=1), history.my_payoffs)))
        potential_gain_from_defecting = self.game_description.n_rounds - state.round_number
        risk_averse_defection = state.round_number >= 0.8 * self.game_description.n_rounds
        if threshold_met and any(prev_opponent_actions):
            return Action.C
        elif punish_defector:
            return Action.D
        elif total_payoff_from_cooperation > potential_gain_from_defecting:
            return Action.C
        elif risk_averse_defection and prev_cooperators < self.game_description.m:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_275(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    It adapts to the game's history and parameters, ensuring robustness against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        p = (1 - (self.game_description.k - 1) / self.game_description.k) * (previous_round_cooperators / self.game_description.n_players)
        if random.random() < p or previous_round_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_276(BaseStrategy):
    """
    An adaptive collective strategy for the Collective Risk Dilemma game.
    It initially cooperates, then adapts to the average cooperation rate of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.first_round_cooperation = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and self.first_round_cooperation):
            self.first_round_cooperation = False
            return Action.C
        if state.round_number < self.game_description.n_rounds - 1:
            opponent_actions_prev_round = history.opponent_actions[-1, :]
            avg_coop_prev_round = np.mean(opponent_actions_prev_round)
            num_cooperators_prev_round = sum(history.my_actions) + sum(opponent_actions_prev_round)
            if avg_coop_prev_round >= 0.5 and num_cooperators_prev_round < self.game_description.m:
                return Action.C
            elif avg_coop_prev_round >= 0.5:
                return Action.D
        else:
            return Action.D
        if self.game_description.m == 1:
            return Action.C
        elif self.game_description.k <= 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_277(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)
    
    This strategy prioritizes collective success by encouraging initial cooperation, 
    adapting the cooperation threshold based on game progress, and triggering defection 
    when the cooperation rate is too low or payoffs are unbalanced.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coop_rate = 0.0
        self.coop_threshold = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_rounds = state.round_number
        total_cooperators = np.sum(history.my_actions[:prev_rounds]) + np.sum(np.sum(history.opponent_actions[:prev_rounds, :], axis=1))
        self.prev_coop_rate = total_cooperators / (self.game_description.n_players * prev_rounds)
        self.coop_threshold += (1 - self.game_description.m / self.game_description.n_players) * ((state.round_number + 1) / self.game_description.n_rounds)
        meet_threshold = self.prev_coop_rate >= self.coop_threshold
        num_cooperators = np.sum(history.my_actions[:prev_rounds]) + np.sum(np.any(history.opponent_actions[:prev_rounds, :], axis=0))
        avg_payoff_per_player_prev_round = np.mean(np.concatenate((history.my_payoffs[prev_rounds - 1:prev_rounds], history.opponent_payoffs[prev_rounds - 1, :])))
        defection_trigger = num_cooperators < self.game_description.m or avg_payoff_per_player_prev_round < (1 + self.game_description.k) / 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if meet_threshold else Action.D
        return Action.C if meet_threshold and (not defection_trigger) else Action.D

class Strategy_COLLECTIVE_278(BaseStrategy):
    """
    The Adaptive Threshold strategy is a collective approach that balances individual self-interest 
    with the need for cooperation to achieve the group's goal. It uses a dynamic threshold to 
    determine when to cooperate or defect, based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players % 2 == 1 else Action.D
        if history.my_actions[-1] == True:
            self.prev_cooperators += 1
        prev_round_cooperators = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_round_cooperators >= self.game_description.m:
            self.cooperation_threshold = 0.5
        else:
            denominator = max(self.game_description.n_players, 1)
            self.cooperation_threshold = 1 - (self.game_description.m - prev_round_cooperators) / denominator
        if prev_round_cooperators >= self.game_description.m and random.random() <= self.cooperation_threshold:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and np.any(np.cumsum(history.my_actions) + np.sum(history.opponent_actions, axis=1) >= self.game_description.m):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_279(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma game.
    This strategy adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.game_description.m == 1:
            return Action.C
        elif self.game_description.n_players == 2:
            return Action.C if state.round_number % 2 == 0 else Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        p_defect = (self.game_description.m - num_cooperators_prev_round) / (self.game_description.n_players - 1)
        if num_cooperators_prev_round >= self.game_description.m / 2:
            return Action.C
        else:
            return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_280(BaseStrategy):
    """
    Adaptive Cooperation with Threshold-based Adjustment (ACTA) strategy.
    
    This strategy prioritizes cooperation while being robust to various opponent behaviors.
    By initially cooperating and adjusting based on the number of cooperators, ACTA promotes collective benefits.
    The tolerance threshold allows for gradual adjustments to maintain cooperation when deviations occur.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_cooperators = 0
        self.tolerance_threshold = math.ceil(0.1 * self.game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.previous_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.game_description.n_rounds - state.round_number == 1:
            return Action.C
        if self.previous_cooperators < self.game_description.m:
            if self.previous_cooperators >= self.game_description.m - self.tolerance_threshold:
                p = (self.game_description.m - self.previous_cooperators) / self.tolerance_threshold
                return Action.C if random.random() < p else Action.D
            return Action.D
        elif self.previous_cooperators >= self.game_description.m:
            return Action.C

class Strategy_COLLECTIVE_281(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for the Collective Risk Dilemma game.
    
    Initially cooperates to encourage others, then adapts based on previous rounds' outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.pc_prev = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        pc_curr = sum(history.my_actions[-1] + history.opponent_actions[-1, :]) / self.game_description.n_players
        if pc_curr >= self.game_description.m / self.game_description.n_players:
            action = Action.C
            self.pc_prev += 0.1
        else:
            action = Action.D
            self.pc_prev -= 0.1
        self.pc_prev = max(0, min(self.pc_prev, 1))
        return action

class Strategy_COLLECTIVE_282(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being 
    by adaptively responding to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.w = min(game_description.n_rounds // 2, game_description.n_rounds)
        self.theta = 0.5
        self.cooperation_rate = [None] * (game_description.n_rounds + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.m >= self.n_players - 1 else Action.D
        my_actions = np.array([a == Action.C for a in history.my_actions], dtype=np.bool_)
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        cooperation_rate_array = opponent_cooperations + my_actions >= self.m
        self.cooperation_rate[state.round_number] = int(cooperation_rate_array[-1])
        p_m = np.mean(self.cooperation_rate[1:state.round_number + 1])
        cooperate = p_m > self.theta
        if p_m > self.theta:
            self.theta -= 0.05
        elif p_m < self.theta:
            self.theta += 0.05
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_283(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 3
        self.rho = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff = self.calculate_avg_payoff(history)
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_cooperators >= self.game_description.m or self.game_description.k >= avg_payoff:
            return Action.C
        elif prev_cooperators < self.game_description.m and history.my_payoffs[-1] < avg_payoff:
            return Action.D
        else:
            return Action.C

    def calculate_avg_payoff(self, history: PlayerHistory):
        if len(history.my_payoffs) <= self.theta:
            return np.mean(history.my_payoffs)
        else:
            return np.mean(history.my_payoffs[-self.theta:])

class Strategy_COLLECTIVE_284(BaseStrategy):
    """
    Adaptive Collective Threshold strategy balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CR = 0.0
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_actions = history.opponent_actions[-1, :]
        self.CR = np.mean(prev_actions)
        if sum(prev_actions) >= self.game_description.m:
            self.theta = min(1, self.theta + 0.1)
        else:
            self.theta = max(0, self.theta - 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if self.CR >= self.theta else Action.D

class Strategy_COLLECTIVE_285(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma game that balances individual self-interest with collective cooperation.
    It adapts to the game's history and opponent behaviors, using a combination of tit-for-tat, grim trigger, and reciprocity-based strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_defected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        num_cooperators = sum(prev_round)
        if self.opponent_defected:
            self.opponent_defected = False
            for prev_round in reversed(history.opponent_actions[:-1]):
                if prev_round[0] == 1 and prev_round[1:] == 0:
                    return Action.D
            self.opponent_defected = True
        if num_cooperators >= self.game_description.m:
            if sum(prev_round[1:]) >= (self.game_description.n_players - 1) / 2:
                return Action.C
            else:
                return Action.D
        else:
            p = (self.game_description.m - num_cooperators) / self.game_description.n_players
            if random.random() < p and (not self.opponent_defected):
                return Action.C
            elif history.my_actions[-1] == 0:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_286(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CCR_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number <= self.game_description.n_rounds // 4:
            return Action.C if state.round_number % 2 == 1 else Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1) + history.my_actions
        total_actions = self.game_description.n_players * state.round_number
        CCR = np.mean(opponent_cooperations)
        if CCR > self.CCR_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_287(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        avg_coop = np.mean(history.opponent_actions, axis=1).sum()
        action = Action.C if avg_coop >= self.game_description.m else Action.D
        if len(history.my_actions) > 2 and history.opponent_actions[-1].sum() < self.game_description.m:
            if self.prev_action == Action.D:
                action = Action.C
            elif self.prev_action == Action.C:
                action = Action.D
        if random.random() < 0.1:
            action = Action.D if action == Action.C else Action.C
        self.prev_action = action
        return action

class Strategy_COLLECTIVE_288(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) strategy aims to balance individual self-interest with collective risk aversion.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_counter = 0
        self.prev_coops = []
        self.convergence_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or len(self.prev_coops) == 0:
            return Action.C
        self.round_counter = state.round_number
        opponent_actions = history.opponent_actions[-1, :]
        num_prev_coops = sum(opponent_actions) + (history.my_actions[-1] if history else False)
        self.prev_coops.append(num_prev_coops)
        if self.round_counter > min(self.game_description.n_rounds / 2, 5):
            if num_prev_coops >= self.convergence_threshold:
                action = Action.C
            else:
                p = (1 - self.convergence_threshold / self.game_description.n_players) ** 2
                action = Action.D if random.random() < p else Action.C
            if len(self.prev_coops) > 5:
                recent_rewards = [sum(history.opponent_payoffs[-i, :]) + history.my_payoffs[-i] for i in range(1, min(len(history.my_payoffs), 6))]
                if sum(recent_rewards) >= 0.75 * len(recent_rewards) * self.game_description.k:
                    self.convergence_threshold += 1
                elif sum(recent_rewards) < 0.25 * len(recent_rewards) * self.game_description.k:
                    self.convergence_threshold -= 1
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_289(BaseStrategy):
    """
    The ACC strategy balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters. This approach ensures a robust 
    response to various opponent behaviors while maintaining a collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        num_coop_last_round = sum(history.opponent_actions[-1, :])
        if state.round_number > 1:
            self.avg_coop = (self.avg_coop * (state.round_number - 2) + num_coop_last_round) / (state.round_number - 1)
        else:
            self.avg_coop = num_coop_last_round
        if self.avg_coop >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        opponent_is_exploiting = np.any(np.all(history.opponent_actions == 0, axis=0)) and state.round_number > 2
        if opponent_is_exploiting:
            if self.avg_coop < self.game_description.m - 1:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff_coop = np.sum(history.my_payoffs) + self.game_description.k * (self.game_description.m / self.game_description.n_players)
            total_payoff_defect = np.sum(history.my_payoffs) + 1
            if total_payoff_coop >= total_payoff_defect:
                action = Action.C
        return action

class Strategy_COLLECTIVE_290(BaseStrategy):
    """
    A collective strategy called "Adaptive Threshold Cooperativity" (ATC) that balances individual self-interest with collective welfare by adapting cooperation levels based on game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players >= 2 * self.game_description.m else Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        p_C = min(1, self.game_description.k * (cooperators / self.game_description.m))
        if cooperators >= self.game_description.m:
            return Action.C if random.random() < p_C else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_291(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    Combines elements of reciprocity, adaptability, and collective thinking to promote cooperation 
    while being robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = game_description.m / game_description.n_players
        self.prev_cooperators = 0
        self.prev_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        if self.prev_cooperators >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D if random.random() < 1 - self.cooperate_prob else Action.C
        average_payoff = np.mean(history.my_payoffs)
        if history is not None and len(history.my_payoffs) > 0:
            if self.prev_payoff > average_payoff and self.prev_cooperators >= self.game_description.m:
                self.cooperate_prob += 0.05
            elif self.prev_payoff < average_payoff and self.prev_cooperators < self.game_description.m:
                self.cooperate_prob -= 0.05
        if state.round_number == self.game_description.n_rounds - 1 and self.prev_cooperators >= self.game_description.m:
            action = Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :] if history else [])
        self.prev_payoff = 1 - int(action == Action.D) + self.game_description.k * int(self.prev_cooperators >= self.game_description.m)
        return action

class Strategy_COLLECTIVE_292(BaseStrategy):
    """
    The Adaptive Collective Convergence (ACC) strategy is designed to navigate the collective risk dilemma by adapting 
    to the game's history and promoting convergence towards cooperation. ACC balances individual self-interest with 
    collective well-being, ensuring a robust and effective approach in the face of diverse opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CR = 0.0
        self.converged = False
        self.cooperations = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        self.CR = sum(self.cooperations) / self.game_description.n_players
        self.converged = self.CR >= self.game_description.m / self.game_description.n_players
        if self.converged:
            action = Action.C if random.random() < 0.8 else Action.D
        else:
            p_defect = 0.2 + 0.4 * (1 - self.CR)
            action = Action.D if random.random() < p_defect else Action.C
        if random.random() < 0.05:
            action = random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.converged else Action.D
        if self.CR < 0.2:
            action = random.choice([Action.C, Action.D])
        self.cooperations.append(action == Action.C)
        return action

class Strategy_COLLECTIVE_293(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters.
    It aims to maximize overall payoff while being robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        self.prev_cooperators = sum(opponent_actions)
        if history.my_payoffs[-1] < self.game_description.k and self.prev_cooperators < self.game_description.m:
            self.p = min(self.p + 0.1, 0.9)
        if self.prev_cooperators >= self.game_description.m:
            return Action.C
        self.p = (state.round_number - sum((1 for i in range(state.round_number) if sum(history.opponent_actions[i, :]) < self.game_description.m))) / state.round_number
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.prev_cooperators >= self.game_description.m else Action.D
        return Action.D if random.random() < self.p else Action.C

class Strategy_COLLECTIVE_294(BaseStrategy):
    """
    The Adaptive Collective Risk Averter (ACRA) strategy is designed to promote collective risk aversion.
    It initially probes other players' behaviors with a moderate cooperation rate and adapts to the group's 
    cooperation rate, adjusting its own cooperation probability accordingly. When the threshold is met, it 
    prioritizes cooperation to maintain the collective effort.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_cooperation_rate = None
        self.last_threshold_met = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        self.last_cooperation_rate = (opponent_cooperations + history.my_actions[-1]) / self.game_description.n_players
        self.last_threshold_met = self.last_cooperation_rate >= self.game_description.m / self.game_description.n_players
        if self.last_threshold_met:
            cooperate_prob = 1 - self.last_cooperation_rate
            return random.choices([Action.C, Action.D], weights=[cooperate_prob, 1 - cooperate_prob])[0]
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_295(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Learning (ACTL) strategy.
    
    This strategy prioritizes cooperation when it's likely to succeed and adapts 
    to changes in opponent behavior. By initially cooperating, we encourage others 
    to do the same, creating a collective effort. As the game progresses, our strategy 
    becomes more adaptive, balancing cooperation with caution. In the final rounds, 
    we prioritize cooperation if previous efforts have been successful.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_payoff = self._calculate_payoff(history, state.round_number - 1)
        self.total_payoff += prev_payoff
        if state.round_number <= math.floor(self.game_description.n_rounds / 2):
            previous_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if previous_round_cooperators < self.game_description.m:
                return Action.D
            else:
                p = (self.game_description.k - 1) / self.game_description.k
                return Action.C if random.random() < p else Action.D
        else:
            previous_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if state.round_number == self.game_description.n_rounds and previous_round_cooperators >= self.game_description.m:
                return Action.C
            elif self.total_payoff >= math.floor(self.game_description.n_rounds / 2) * (self.game_description.k - 1):
                return Action.C
            else:
                return Action.D

    def _calculate_payoff(self, history: PlayerHistory, round_number: int) -> float:
        """Calculate the payoff for this player in a given round."""
        cooperators = sum(history.opponent_actions[round_number, :]) + history.my_actions[round_number]
        if cooperators < self.game_description.m:
            return 1 - history.my_actions[round_number]
        else:
            return 1 - history.my_actions[round_number] + self.game_description.k

class Strategy_COLLECTIVE_296(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma game that adapts to the collective cooperation rate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        collective_cooperation_rate = np.mean(history.opponent_actions[-1, :] | (history.my_actions[-1] == Action.C))
        if collective_cooperation_rate >= self.cooperation_threshold:
            average_payoff = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
            payoff_from_cooperating = 0 + self.game_description.k
            return Action.C if payoff_from_cooperating >= average_payoff else Action.D
        else:
            return Action.D
        if collective_cooperation_rate > self.cooperation_threshold:
            self.cooperation_threshold -= 1
        elif collective_cooperation_rate < self.cooperation_threshold:
            self.cooperation_threshold += 1

    def _last_round(self, history: PlayerHistory) -> Action:
        average_payoff = np.mean(np.concatenate((history.my_payoffs[-2:], history.opponent_payoffs[-2, :])))
        payoff_from_cooperating = 0 + self.game_description.k
        return Action.C if payoff_from_cooperating >= average_payoff else Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return self._last_round(history)
        collective_cooperation_rate = np.mean(history.opponent_actions[-1, :] | (history.my_actions[-1] == Action.C))
        if collective_cooperation_rate >= self.cooperation_threshold:
            average_payoff = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
            payoff_from_cooperating = 0 + self.game_description.k
            return Action.C if payoff_from_cooperating >= average_payoff else Action.D
        else:
            return Action.D
        if collective_cooperation_rate > self.cooperation_threshold:
            self.cooperation_threshold -= 1
        elif collective_cooperation_rate < self.cooperation_threshold:
            self.cooperation_threshold += 1

class Strategy_COLLECTIVE_297(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        threshold_met = prev_round_cooperators >= self.m_threshold
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if threshold_met:
            p = min(1, self.k_factor / (self.k_factor + 1))
            return Action.C if random.random() < p else Action.D
        else:
            q = max(0, (self.n_players - self.m_threshold) / self.n_players)
            return Action.D if random.random() < q else Action.C

class Strategy_COLLECTIVE_298(BaseStrategy):
    """
    The Adaptive Collective Risk (ACR) strategy aims to balance individual self-interest with collective cooperation 
    to achieve the optimal payoff. It adapts to the game's history and opponent behaviors while maintaining a collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.game_description.m:
            self.cooperation_threshold += 1
        else:
            self.cooperation_threshold -= 1
            if self.cooperation_threshold < 1:
                self.cooperation_threshold = 1
        if history.my_actions[-1] == False and sum(history.opponent_actions[-1, :]) == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_299(BaseStrategy):
    """
    Adaptive Collective Convergence strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = game_description.m / game_description.n_players
        self.epsilon = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        prev_coops = sum(history.opponent_actions[-1, :])
        self.p = (prev_coops + 1) / self.game_description.n_players
        if self.p >= self.game_description.m / self.game_description.n_players:
            cooperate = True
        else:
            cooperate = False
        if prev_coops >= self.game_description.m:
            self.p += self.epsilon
        else:
            self.p -= self.epsilon
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_300(BaseStrategy):
    """
    A strategy that adapts to the collective cooperation level and escalates cooperation when necessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = 0
        self.m_avg = 0
        self.p = 0.5
        self.w = 3
        self.f = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if history is None or len(history.opponent_actions) == 0:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :]) + 1
        self.m_avg = (self.m_avg * (state.round_number - 2) + m_t) / (state.round_number - 1)
        if m_t >= self.T:
            return Action.C
        elif self.m_avg / self.game_description.n_players < self.f:
            self.p += 0.1
            if random.random() < self.p:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D
        if m_t > self.game_description.m:
            self.T = min(self.T + 1, self.game_description.n_players)
        elif m_t < self.game_description.m and self.T > 0:
            self.T -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_301(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective risk management by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate = True
        self.prev_threshold_met = False
        self.prev_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.m == 1:
            return Action.C
        elif self.game_description.k <= 1:
            return Action.D
        if state.round_number == 0:
            self.cooperate = True
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + self.cooperate
        total_payoff = history.my_payoffs[-1]
        self.prev_threshold_met = num_cooperators >= self.game_description.m
        self.prev_payoff = total_payoff
        if state.round_number < self.game_description.n_rounds - 1:
            if self.prev_threshold_met and self.prev_payoff >= 1:
                return Action.C
            else:
                return Action.C if random.random() > 0.5 else Action.D
        else:
            return Action.D

    def calculate_payoff(self, action: bool, num_cooperators: int) -> float:
        """
        Calculate the payoff for a given action and number of cooperators.
        
        :param action: True if cooperate (C), False if defect (D)
        :param num_cooperators: Number of players who cooperated
        :return: Payoff value
        """
        if num_cooperators < self.game_description.m:
            return 1 - int(action)
        else:
            return 1 - int(action) + self.game_description.k

class Strategy_COLLECTIVE_302(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy for Collective Risk Dilemma.
    
    This strategy prioritizes collective well-being by encouraging initial cooperation, 
    adapting to the group's performance, and adjusting behavior accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = game_description.k / (game_description.k + 1)
        self.q = 1 - game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions.size > 0 else False)
        if cooperators < self.game_description.m:
            return Action.C if random.random() < 0.5 else Action.D
        elif cooperators >= self.game_description.m:
            return Action.C if random.random() < self.p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_303(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_trigger = game_description.m / 2
        self.avg_payoff_C = 0.0
        self.avg_payoff_D = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3 or (state.round_number == 3 and history is None):
            return Action.C
        prev_actions = history.opponent_actions[-1, :]
        num_coop_prev_round = sum(prev_actions)
        total_payoff_prev_round = np.sum(history.opponent_payoffs[-1, :])
        avgcoop_payoff = 0.0 if num_coop_prev_round == 0 else np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1, :], prev_actions) if action])
        avgdefect_payoff = total_payoff_prev_round - avgcoop_payoff * num_coop_prev_round
        avgdefect_payoff /= self.game_description.n_players - 1 - num_coop_prev_round
        self.coop_trigger += (num_coop_prev_round - self.coop_trigger) * 0.1
        prev_my_action = history.my_actions[-1]
        self.avg_payoff_C = (self.avg_payoff_C * (state.round_number - 3) + avgcoop_payoff if prev_my_action else 0.0) / (state.round_number - 2)
        self.avg_payoff_D = (self.avg_payoff_D * (state.round_number - 3) + avgdefect_payoff if not prev_my_action else 0.0) / (state.round_number - 2)
        if num_coop_prev_round >= self.coop_trigger and self.avg_payoff_C > self.avg_payoff_D:
            return Action.C
        if not prev_my_action and total_payoff_prev_round < self.avg_payoff_C * num_coop_prev_round:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_304(BaseStrategy):
    """
    A strategy for Collective Risk Dilemma that adapts to the game's progression,
    balancing individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_payoff = self.calculate_previous_round_payoff(history)
        theta = (self.game_description.n_rounds - state.round_number) // 2
        if self.consecutive_coop >= theta:
            return Action.C
        if prev_payoff == self.calculate_successful_payoff():
            return Action.C if prev_payoff >= (1 + self.game_description.k) / 2 else Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and sum(history.opponent_actions[-1, :]) >= self.game_description.m:
            return Action.C

    def calculate_previous_round_payoff(self, history: PlayerHistory) -> float:
        previous_round = history.my_actions.shape[0] - 1
        opponent_coop_count = sum(history.opponent_actions[previous_round, :])
        if opponent_coop_count >= self.game_description.m:
            return 1 - history.my_actions[previous_round] + self.game_description.k
        else:
            return 1 - history.my_actions[previous_round]

    def calculate_successful_payoff(self) -> float:
        return 1 - self.game_description.m / self.game_description.n_players + self.game_description.k

class Strategy_COLLECTIVE_305(BaseStrategy):
    """
    A strategy that adaptively adjusts cooperation levels based on game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met_before = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        prev_round_coop = np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if prev_round_coop >= self.game_description.m / self.game_description.n_players:
            return Action.C
        else:
            return Action.D
        if np.sum(history.opponent_actions[-1, :]) >= self.game_description.m:
            self.threshold_met_before = True
        if np.sum(~history.opponent_actions[-1, :]) > self.game_description.n_players // 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.threshold_met_before else Action.D

class Strategy_COLLECTIVE_306(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players <= 5 else Action.D
        prev_coop_players = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        action = Action.C if prev_coop_players >= self.cooperation_threshold else Action.D
        if random.random() < 0.05:
            action = random.choice([Action.C, Action.D])
        if prev_coop_players >= self.game_description.m:
            self.cooperation_threshold = min(self.cooperation_threshold + 1, self.game_description.n_players - 1)
        else:
            self.cooperation_threshold = max(self.cooperation_threshold - 1, self.game_description.m)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if prev_coop_players >= self.game_description.m else Action.D
        return action

class Strategy_COLLECTIVE_307(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for Collective Risk Dilemma.

    This strategy balances individual self-interest with collective cooperation.
    It initially cooperates unconditionally, then adapts based on the previous round's outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.t = 0
        self.m_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.t <= self.n_players / 2:
            return Action.C
        if history is not None:
            self.m_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.m_prev >= self.m:
            p = (self.k - 1) / self.k
            action = Action.C if random.random() < p else Action.D
        else:
            q = (self.n_players - self.m) / self.n_players
            action = Action.D if random.random() < q else Action.C
        payoff_from_cooperating = 1 - 1 + self.k
        expected_payoff_from_defecting = 1 - 0 + self.k
        if self.k < 2 or payoff_from_cooperating < 0.5 * expected_payoff_from_defecting:
            action = Action.D
        if state.round_number == self.r - 1:
            return Action.C
        if self.n_players == 1:
            return Action.C
        self.t += 1
        return action

class Strategy_COLLECTIVE_308(BaseStrategy):
    """
    Adaptive Cooperation with Dynamic Threshold (ACDT) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.epsilon = 0.01
        self.punishment_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        p = num_cooperators / self.n_players
        tau = (self.m - 1) / self.n_players + self.epsilon
        if state.round_number < self.game_description.n_rounds and p >= tau:
            self.punishment_flag = False
            return Action.C
        elif history.my_payoffs[-1] < 1:
            self.punishment_flag = True
            return Action.D
        else:
            if self.punishment_flag and state.round_number > 2 and (sum(history.my_actions[-3:-1]) == 2 * True):
                self.punishment_flag = False
            return Action.C if not self.punishment_flag else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_309(BaseStrategy):
    """
    A strategy that adapts to the cooperation level of other players in a collective risk dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.defect_streak = 0
        self.avg_cooperators = []
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.D
        m_prev = np.sum(history.opponent_actions[-1, :] + (history.my_actions[-1] == True))
        self.defect_streak += 1 if np.all(~history.opponent_actions[-1, :]) and history.my_actions[-1] == False else -self.defect_streak
        self.avg_cooperators.append(m_prev)
        self.avg_cooperators = self.avg_cooperators[-min(state.round_number // 2, 10):]
        avg_cooperators = np.mean(self.avg_cooperators) if len(self.avg_cooperators) > 0 else 0
        if m_prev >= self.game_description.m or self.defect_streak > 2 * self.game_description.n_rounds / self.game_description.n_players:
            return Action.C
        elif avg_cooperators > self.game_description.m:
            self.cooperation_probability = min(self.cooperation_probability + 0.1, 1)
        else:
            self.cooperation_probability = max(self.cooperation_probability - 0.1, 0)
        if state.round_number == self.game_description.n_rounds - 1 or (self.defect_streak > 2 * self.game_description.n_rounds / self.game_description.n_players and random.random() < 0.5):
            return Action.C
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_310(BaseStrategy):
    """
    An implementation of the Adaptive Collective Cooperation strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum((1 for action in history.opponent_actions[-1] if action))
        if prev_round_coops >= self.game_description.m:
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.C if prev_round_coops >= self.game_description.m else Action.D
            return Action.C
        p_defect = max(0, min(1, (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players))
        if self.game_description.n_players == 1:
            return Action.D
        return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_311(BaseStrategy):
    """
    Adaptive Cooperation Strategy.
    
    This strategy balances individual self-interest with collective cooperation by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def calculate_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Calculate the Cooperation Rate from previous rounds.

        :param history: This player's history.
        :return: The cooperation rate (between 0 and 1).
        """
        if len(history.my_actions) == 0:
            return 0.0
        cooperative_actions = sum((1 for action in history.my_actions if action))
        total_actions = len(history.my_actions)
        return cooperative_actions / total_actions

    def calculate_avg_defect_payoff(self, history: PlayerHistory) -> float:
        """
        Calculate the average payoff received by defecting players in previous rounds.

        :param history: This player's history.
        :return: The average defect payoff (0 if no defects).
        """
        if len(history.my_actions) == 0 or not any((not action for action in history.my_actions)):
            return 0.0
        defect_payoffs = [payoff for action, payoff in zip(history.my_actions, history.my_payoffs) if not action]
        return sum(defect_payoffs) / len(defect_payoffs)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m >= self.game_description.n_players // 2 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            cooperation_rate = self.calculate_cooperation_rate(history)
            return Action.C if cooperation_rate >= (self.game_description.m - 1) / self.game_description.n_players else Action.D
        cooperation_rate = self.calculate_cooperation_rate(history)
        avg_defect_payoff = self.calculate_avg_defect_payoff(history)
        if history.my_actions[-1]:
            return Action.C if cooperation_rate >= (self.game_description.m - 1) / self.game_description.n_players or self.game_description.k > avg_defect_payoff else Action.D
        else:
            return Action.D if cooperation_rate < (self.game_description.m - 1) / self.game_description.n_players and self.game_description.k <= avg_defect_payoff else Action.C

class Strategy_COLLECTIVE_312(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy for Collective Risk Dilemma games.
    Balances individual self-interest with collective well-being by adapting to game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.successful_rounds = 0
        self.total_rounds = 0
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        self.total_rounds += 1
        if prev_round_cooperators >= self.game_description.m:
            self.successful_rounds += 1
        if self.successful_rounds < self.total_rounds / 2:
            self.threshold = min(self.threshold + 1, self.game_description.n_players - 1)
        elif self.successful_rounds > self.total_rounds / 2:
            self.threshold = max(self.threshold - 1, self.game_description.m)
        p = self.successful_rounds / self.total_rounds if self.total_rounds > 0 else 0
        delta = 0.1
        if prev_round_cooperators < self.threshold:
            return Action.D
        elif random.random() < p + delta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_313(BaseStrategy):
    """
    This strategy adapts to various opponent behaviors in a Collective Risk Dilemma game.
    It prioritizes collective cooperation while escalating cooperation when the game's total payoff falls short of the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_threshold = game_description.m
        self.k_factor = game_description.k
        self.rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.m_threshold > self.n_players / 2 else Action.D
        total_payoff = sum(history.my_payoffs)
        theta = self.k_factor * (self.rounds - 1)
        number_of_cooperators_last_round = sum(history.opponent_actions[-1, :])
        if total_payoff < self.k_factor:
            p = (self.m_threshold - number_of_cooperators_last_round) / self.n_players
            return Action.C if random.random() > p else Action.D
        if total_payoff < theta:
            p = min(1, (theta - total_payoff) / theta)
            return Action.C if random.random() < p else Action.D
        if state.round_number == self.rounds - 1:
            return Action.C if self.m_threshold > self.n_players / 2 else Action.D
        if self.rounds - state.round_number < self.m_threshold and number_of_cooperators_last_round < self.m_threshold:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_314(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy.

    This strategy adapts to the game's history, prioritizing cooperation when the collective threshold is within reach or has been met in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_round_cooperators >= self.game_description.m:
            return Action.C
        elif prev_round_cooperators > 0 and prev_round_cooperators < self.game_description.m:
            return Action.C if random.random() < (self.game_description.m - prev_round_cooperators) / self.game_description.n_players else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_315(BaseStrategy):
    """
    A strategy that adapts to the cooperation level of other players,
    encouraging collective success by punishing non-cooperative behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and self.game_description.m == 1):
            return Action.D if self.game_description.m == 1 else Action.C
        prev_round_opponent_coops = history.opponent_actions[-1, :].sum()
        total_prev_round_coops = int(prev_round_opponent_coops + history.my_actions[-1])
        if state.round_number < self.game_description.n_rounds - 1:
            if total_prev_round_coops >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_316(BaseStrategy):
    """
    The Adaptive Collective Contribution strategy prioritizes collective well-being by 
    encouraging cooperation when the threshold is met or exceeded, adjusting the 
    cooperation strategy to maintain a stable level of cooperation, and maximizing 
    individual payoffs only in situations where it doesn't compromise collective interests.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.previous_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        threshold_adjusted = False
        if self.previous_cooperators < self.game_description.m and state.round_number >= 2:
            prev_prev_round_coops = sum(history.opponent_actions[-2, :]) + history.my_actions[-2]
            if prev_prev_round_coops < self.game_description.m:
                threshold_adjusted = True
        if self.previous_cooperators >= self.game_description.m or (threshold_adjusted and self.previous_cooperators >= math.ceil(self.game_description.n_players / 2)):
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_317(BaseStrategy):
    """
    Adaptive Cooperation with Threshold-based Punishment (ACTP) strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_cooperation_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_history = history.my_actions[state.round_number - 1]
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds:
            return Action.C
        elif opponent_cooperators >= self.game_description.m:
            return Action.C
        elif previous_round_history:
            return Action.D
        elif state.round_number == 1:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_318(BaseStrategy):
    """
    An adaptive cooperate-defect strategy for the collective risk dilemma.
    This strategy aims to balance individual self-interest with collective risk management by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.6
        self.lambda_ = game_description.m - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m > self.game_description.n_players / 2 else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [True if history.my_actions[-1] == Action.C.value else False])
        collective_reward_achieved = prev_round_cooperators >= self.game_description.m
        individual_payoff_higher_than_private = state.round_number > 0 and history.my_payoffs[state.round_number - 1] > 1 + self.game_description.k
        if collective_reward_achieved and individual_payoff_higher_than_private:
            return Action.C if prev_round_cooperators / self.game_description.n_players >= self.theta else Action.D
        elif not collective_reward_achieved:
            return Action.C if prev_round_cooperators >= self.lambda_ else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if collective_reward_achieved else Action.D

class Strategy_COLLECTIVE_319(BaseStrategy):
    """
    Collective strategy for the Collective Risk Dilemma game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < m / n_players else Action.D
        num_coop_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        CR = num_coop_prev_round / n_players
        if CR >= m / n_players:
            return Action.C if random.random() < k / (k + 1) else Action.D
        elif num_coop_prev_round < m and CR > 0.5 * m / n_players:
            return Action.C if random.random() < 0.5 else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C if num_coop_prev_round >= m else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_320(BaseStrategy):
    """
    The ACC strategy aims to balance individual self-interest with collective well-being,
    ensuring a robust and adaptive approach to cooperation in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.tau = 0.6
        self.w = min(5, game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_cooperators >= self.game_description.m:
            return Action.C
        elif random.random() < 0.5:
            return Action.D
        else:
            opponent_cooperation_rates = []
            for i in range(self.w):
                round_number = state.round_number - 1 - i
                if round_number >= 0:
                    cooperators_in_round = sum(history.opponent_actions[round_number, :])
                    opponent_cooperation_rate = cooperators_in_round / self.game_description.n_players
                    opponent_cooperation_rates.append(opponent_cooperation_rate)
            if not opponent_cooperation_rates:
                return Action.C
            average_opponent_cooperation_rate = np.mean(opponent_cooperation_rates)
            if average_opponent_cooperation_rate > self.tau:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif previous_round_cooperators == self.game_description.m - 1:
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_321(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma game.
    
    This strategy seeds cooperation in the first round, then adapts to observed behavior by adjusting the cooperation probability and threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.k_avg = None
        self.coop_threshold = game_description.m
        self.coop_prob = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        if self.k_avg is not None and self.k_avg > 1:
            self.coop_prob *= 1.05
        elif self.k_avg is not None:
            self.coop_prob /= 1.05
        if history is not None and history.opponent_actions.shape[0] > 0:
            observed_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            if observed_cooperators < self.game_description.m:
                self.coop_threshold -= 1
            elif observed_cooperators > self.game_description.m:
                self.coop_threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if history is not None and sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C) >= self.game_description.m - 1 else Action.D
        if history is not None and history.my_payoffs.shape[0] > 0:
            coop_rounds = np.where(history.my_actions == True)[0]
            avg_coop_payoff = np.mean(history.my_payoffs[coop_rounds])
            self.k_avg = avg_coop_payoff / (1 - self.game_description.m / self.game_description.n_players)
        return Action.C if random.random() < self.coop_prob and (history is None or sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C) >= self.coop_threshold) else Action.D

class Strategy_COLLECTIVE_322(BaseStrategy):
    """
    A strategy that adapts to the collective behavior of opponents in a Collective Risk Dilemma game.
    Initially cooperates, then adjusts its actions based on whether the threshold was met and how many players cooperated.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_outcome = np.sum(history.opponent_actions[-1, :]) + (Action.C.value in history.my_actions)
        self.coop_count = prev_outcome
        if prev_outcome >= self.game_description.m:
            action = Action.C
        elif self.coop_count > 0:
            p = min(1, self.coop_count / self.game_description.m)
            action = Action.D if random.random() < p else Action.C
        else:
            p = min(1, self.coop_count / self.game_description.m) if self.coop_count > 0 else 0.5
            action = Action.D if random.random() < p else Action.C
        if state.round_number == self.game_description.n_rounds - 1 and self.total_payoff > self.game_description.k:
            return Action.D
        prev_payoff = self.game_description.k + 1 if prev_outcome >= self.game_description.m else 1
        self.total_payoff += prev_payoff
        return action

class Strategy_COLLECTIVE_323(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: "Adaptive Threshold".
    
    A collective, adaptive approach that balances individual self-interest with 
    the need for cooperation to achieve the collective reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        my_prev_action = bool(history.my_actions[-1])
        all_prev_actions = np.append(prev_round_actions, my_prev_action)
        prev_coops = np.sum(all_prev_actions)
        if prev_coops >= self.m / 2:
            return Action.C
        if prev_coops >= self.m:
            return Action.C
        else:
            return Action.D
        if state.round_number >= self.n_rounds - 1:
            total_coops = np.sum(history.my_actions)
            if total_coops >= self.m * self.n_rounds / 2:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_324(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation in the Collective Risk Dilemma game.
    
    This strategy adapts to the evolving game dynamics by cooperating when the threshold is met 
    and defecting when it's not, promoting a self-sustaining cycle of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_c = 0.8
        self.p_d = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] and 1 or 0)
        if prev_round_cooperators >= self.game_description.m:
            return Action.C if random.random() < self.p_c else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C if random.random() < 1 - self.p_d else Action.D

class Strategy_COLLECTIVE_325(BaseStrategy):
    """
    This strategy implements the Adaptive Threshold Cooperation approach.
    It prioritizes cooperation and adapts to the group's performance by tracking the number of cooperators,
    adjusting its threshold for cooperation to balance individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        prev_my_payoff = history.my_payoffs[-1]
        cooperators = sum(prev_opponent_actions) + 1 if history.my_actions[-1] else sum(prev_opponent_actions)
        if state.round_number > 1 and (not history.my_actions[-1]) and (cooperators < self.game_description.m):
            return Action.D
        if state.round_number > 1 and history.my_actions[-2] and (cooperators >= self.threshold):
            return Action.C
        if cooperators < self.threshold and prev_my_payoff == 1:
            self.threshold += 1
        elif cooperators >= self.threshold and prev_my_payoff > 1 + self.game_description.k:
            self.threshold -= 1
            self.threshold = max(self.threshold, 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if cooperators >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_326(BaseStrategy):
    """
    A dynamic strategy that balances individual self-interest with collective responsibility.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        prev_round = history.opponent_actions[-1, :]
        prev_coops = sum(prev_round)
        if prev_coops >= self.game_description.m and state.round_number > 0:
            return Action.C
        defector_rounds = 0
        for round in history.opponent_actions[-2:]:
            if sum(round) < self.game_description.n_players / 2:
                defector_rounds += 1
        if defector_rounds >= 2:
            self.cooperation_probability -= (defector_rounds - 1) / self.game_description.n_players
        x = prev_coops
        if self.game_description.m < x < self.game_description.n_players - 1:
            ratio = x / self.game_description.m
            if ratio > 0.5:
                self.cooperation_probability += (ratio - 0.5) / self.game_description.n_players
            else:
                self.cooperation_probability -= (0.5 - ratio) / self.game_description.n_players
        if state.round_number >= self.game_description.n_rounds - 2:
            coop_window = history.opponent_actions[-2:]
            if any((sum(round) >= self.game_description.m for round in coop_window)):
                return Action.C
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_327(BaseStrategy):
    """
    A collective risk dilemma strategy that adapts to the group's behavior.
    It prioritizes cooperation when it's likely to be successful and balances individual self-interest with collective risk aversion.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        self.prev_cooperators = sum(opponent_actions)
        if self.prev_cooperators >= math.ceil(self.game_description.m / 2):
            return Action.C
        p = 1 - (self.game_description.k - 1) / (self.game_description.n_players - self.game_description.m)
        if random.random() < p:
            return Action.D
        q = 0.5
        if random.random() < q and self.prev_cooperators < self.game_description.m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_328(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperate-or-Defect (ACD)

    This strategy aims to balance individual self-interest with collective well-being.
    It encourages opponents to cooperate, identifies the minimum number of cooperators needed for a successful outcome,
    and adjusts its behavior based on recent payoffs and opponent actions.

    :param game_description: CollectiveRiskDescription instance
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.avg_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number <= self.game_description.n_rounds // 2:
            self.m_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            return Action.C if self.m_prev >= self.game_description.m else Action.D
        if state.round_number < self.game_description.n_rounds - 1:
            payoffs = history.my_payoffs[-2:]
            self.avg_payoff = np.mean(payoffs) if len(payoffs) == 2 else payoffs[0]
            p_c = max(0.5, self.m_prev / self.game_description.n_players)
            return Action.C if random.random() < p_c and self.avg_payoff >= self.game_description.k / 2 + 1 else Action.D
        return Action.C if self.m_prev >= self.game_description.m - 1 else Action.D

class Strategy_COLLECTIVE_329(BaseStrategy):
    """
    A strategy that adapts to changing game conditions and prioritizes collective cooperation while avoiding exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_outcome = self._assess_previous_round(history)
        cooperation_rate = self._update_cooperation_rate(history)
        if previous_outcome == 'success':
            action = Action.C
        elif previous_outcome == 'failure' and cooperation_rate > 0.5:
            action = Action.C
        else:
            action = Action.D
        if state.round_number + 1 == self.game_description.n_rounds:
            expected_average_payoff = self.game_description.k / self.game_description.n_players * self.game_description.n_rounds
            if self.total_payoff < expected_average_payoff:
                action = Action.C
        if history is not None:
            self.total_payoff += history.my_payoffs[-1]
        return action

    def _assess_previous_round(self, history: PlayerHistory) -> str:
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if previous_round_cooperators >= self.game_description.m:
            return 'success'
        else:
            return 'failure'

    def _update_cooperation_rate(self, history: PlayerHistory) -> float:
        previous_round_opponent_cooperations = sum(history.opponent_actions[-1, :])
        cooperation_rate = previous_round_opponent_cooperations / (self.game_description.n_players - 1)
        return cooperation_rate

class Strategy_COLLECTIVE_330(BaseStrategy):
    """
    Adaptive Collective Escalation (ACE) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = game_description.m / game_description.n_players
        self.escalation_step = 1 / (game_description.n_players * game_description.n_rounds)
        self.max_cooperation_probability = 0.9
        self.threshold_met_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        if self.game_description.n_rounds - state.round_number <= 1:
            return Action.D
        if self.threshold_met_last_round:
            return Action.C
        else:
            defect_probability = 1 - self.game_description.k / (self.game_description.n_players * self.game_description.n_rounds)
            action = Action.C if random.random() > defect_probability else Action.D
            if action == Action.D and self.cooperation_probability < self.max_cooperation_probability:
                self.cooperation_probability += self.escalation_step
        if history is not None:
            self.threshold_met_last_round = np.sum(history.opponent_actions[-1, :]) + 1 >= self.game_description.m
        return action

class Strategy_COLLECTIVE_331(BaseStrategy):
    """
    A strategy that adapts to the cooperation level in the previous round.
    It encourages collective cooperation while allowing for occasional deviations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_players == 2 or self.game_description.m == self.game_description.n_players:
            return Action.C
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
        threshold_met = prev_round_cooperators >= self.game_description.m
        if threshold_met and state.round_number < self.game_description.n_rounds - 1:
            p = (self.game_description.k - 1) / self.game_description.k
            return Action.C if random.random() < p else Action.D
        elif not threshold_met and state.round_number < self.game_description.n_rounds - 1:
            return Action.D if random.random() < 0.5 else Action.C
        return Action.C

class Strategy_COLLECTIVE_332(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Cooperation (ATC)

    The ATC strategy aims to balance individual self-interest with collective risk management,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_last_round = 0
        self.p_defect = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            opponent_actions = history.opponent_actions[-1, :]
            self.cooperators_last_round = sum(opponent_actions) + 1
            for opponent in opponent_actions:
                if opponent == False:
                    self.p_defect += 0.1
                elif opponent == True:
                    self.p_defect -= 0.1
        if self.cooperators_last_round < self.game_description.m:
            self.p_defect = (self.game_description.m - self.cooperators_last_round) / (self.game_description.n_players - self.game_description.m)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < self.p_defect:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_333(BaseStrategy):
    """
    Adaptive Collective Risk strategy balances individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate = True
        self.previous_payoff = 0.0
        self.C_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        my_previous_payoff = self.calculate_payoff(history.my_actions[-1], history.opponent_actions[-1])
        threshold_met = sum(history.opponent_actions[-1]) >= self.game_description.m
        if threshold_met:
            if my_previous_payoff >= 1 + self.game_description.k / 2:
                self.cooperate = True
            else:
                self.cooperate = False
        elif self.C_prev < self.game_description.m / 2:
            self.cooperate = random.random() < 0.7
        elif self.C_prev >= self.game_description.m:
            self.cooperate = random.random() < 0.3
        else:
            self.cooperate = random.random() < 0.5
        average_payoff = np.mean(history.opponent_payoffs[-1])
        if my_previous_payoff < average_payoff:
            self.cooperate = False
        if state.round_number == self.game_description.n_rounds - 1 and threshold_met:
            return Action.C
        self.C_prev = sum(history.opponent_actions[-1])
        return Action.C if self.cooperate else Action.D

    def calculate_payoff(self, my_action: bool, opponent_actions: NDArray[np.bool_]) -> float:
        threshold_met = sum(opponent_actions) + int(my_action) >= self.game_description.m
        payoff = 1 - int(my_action)
        if threshold_met:
            payoff += self.game_description.k
        return payoff

class Strategy_COLLECTIVE_334(BaseStrategy):
    """
    The Adaptive Threshold Cooperation (ATC) strategy adapts cooperation levels based on game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = max(1, game_description.m - game_description.n_players // 2)
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        total_payoff_prev_round = np.sum(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_cooperators >= self.cooperation_threshold or total_payoff_prev_round > self.game_description.k / 2 * self.game_description.n_players:
            action = Action.C
        elif prev_cooperators < self.cooperation_threshold and total_payoff_prev_round <= self.game_description.k / 2 * self.game_description.n_players:
            action = Action.D
        else:
            action = Action.D
        if prev_cooperators >= self.game_description.m:
            self.cooperation_threshold -= 1
        else:
            self.cooperation_threshold += 1
        self.prev_cooperators = prev_cooperators
        return action

class Strategy_COLLECTIVE_335(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for Collective Risk Dilemma.
    
    This strategy adapts to the previous round's outcome, balancing individual interests 
    with collective benefits and encouraging cooperation when the threshold is met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        if self.prev_cooperators < self.game_description.m:
            return Action.D
        else:
            p = (self.game_description.k - 1) / self.game_description.k
            return Action.C if random.random() < p else Action.D

class Strategy_COLLECTIVE_336(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M = game_description.m

    def update_threshold(self, history: PlayerHistory):
        num_cooperators = sum(history.opponent_actions[-1, :]) + 1
        if num_cooperators >= self.M:
            return max(self.game_description.m, self.M - 1)
        else:
            return min(self.game_description.n_players, self.M + 1)

    def estimate_cooperators(self, history: PlayerHistory):
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(opponent_actions)
        return num_cooperators

    def should_cooperate(self, state: GameState, history: PlayerHistory):
        if state.round_number == 0:
            return len(history.my_actions) >= self.game_description.n_players - self.game_description.m
        self.M = self.update_threshold(history)
        expected_cooperators = self.estimate_cooperators(history)
        previous_defector = not history.opponent_actions[-1, 0]
        return expected_cooperators >= self.M or (previous_defector and expected_cooperators >= self.game_description.n_players - self.M)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if not self.should_cooperate(state, history):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_337(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0.0
        self.confidence_level = 0.5
        self.learning_rate = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[self.game_description.m / self.game_description.n_players, 1 - self.game_description.m / self.game_description.n_players])[0]
        successful_round = history.my_payoffs[-1] > 1
        self.threshold = (1 - self.learning_rate) * self.threshold + self.learning_rate * int(successful_round)
        if state.round_number < self.game_description.n_rounds - 1 and self.threshold > self.confidence_level and np.any(history.opponent_actions[-1, :]):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_338(BaseStrategy):
    """
    The Adaptive Collective Guardian strategy adapts to various opponent behaviors and aligns with the collective mindset.
    It encourages cooperation when the threshold is relatively low, adapts to opponents' behavior, and implements a gentle punishment mechanism.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.consecutive_defection = 0
        self.punishment_rounds = 2
        self.cooperate_prob = (game_description.k - 1) / (game_description.k + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        if state.round_number < 4:
            cooperators = sum(history.opponent_actions[-1, :])
            self.avg_coop += cooperators / (state.round_number - 1)
            return Action.C if self.avg_coop >= self.game_description.m else Action.D
        threshold_met = history.my_payoffs[-1] == 1 + self.game_description.k
        if threshold_met:
            self.cooperate_prob = (self.game_description.k - 1) / (self.game_description.k + 1)
            defect_prob = 2 / (self.game_description.k + 1)
            return Action.C if random.random() < self.cooperate_prob else Action.D
        else:
            self.cooperate_prob = 1 / (self.game_description.n_players - self.game_description.m + 1)
            defect_prob = (self.game_description.n_players - self.game_description.m) / (self.game_description.n_players - self.game_description.m + 1)
            return Action.C if random.random() < self.cooperate_prob else Action.D
        opponent_defected = not history.opponent_actions[-1, 0]
        if self.consecutive_defection >= 2 and opponent_defected:
            self.punishment_rounds += 1
            return Action.D if random.random() < (self.game_description.n_players - self.game_description.m) / (self.game_description.n_players - self.game_description.m + 1) else Action.C
            self.consecutive_defection = 0
        total_coop_payoff = sum(history.my_payoffs[history.my_actions])
        total_defect_payoff = sum(history.my_payoffs[~history.my_actions])
        return Action.C if total_coop_payoff >= total_defect_payoff else Action.D

class Strategy_COLLECTIVE_339(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective benefits by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.avg_payoff = 0.0
        self.w = 5
        self.threshold_m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.m:
            return Action.C
        if history is not None and len(history.opponent_actions) > 0:
            self.coop_count = sum(history.opponent_actions[-1, :])
        if state.round_number > self.game_description.m:
            if state.round_number > self.w:
                payoffs = history.my_payoffs if history is not None else []
                self.avg_payoff = np.mean(payoffs[-self.w:]) if len(payoffs) >= self.w else 0.0
                if self.avg_payoff > (self.game_description.k + 1) / 2:
                    self.threshold_m += 1
                elif self.threshold_m > 1:
                    self.threshold_m -= 1
        if state.round_number < self.game_description.n_rounds - 1:
            return Action.C if self.coop_count >= self.threshold_m else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_340(BaseStrategy):
    """
    A collective risk dilemma strategy that adapts to the cooperation levels and payoffs of opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.p = 0.5
        self.CS = 0
        self.PS = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        if state.round_number < self.m:
            previous_round_actions = history.opponent_actions[-1, :]
            if any(previous_round_actions):
                return Action.C
            return Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        self.CS = previous_round_cooperators / self.n_players
        previous_round_payoffs = np.sum(history.opponent_payoffs[-1, :], axis=0)
        self.PS = np.mean(previous_round_payoffs)
        if self.CS * self.k > (1 - self.CS) * (self.n_players - self.m + 1) and self.PS / self.n_players >= (self.k - 1) / 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_341(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for 
    collective cooperation to achieve the threshold reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met_recently = False
        self.cooperation_frequency = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        recent_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        total_players = self.game_description.n_players
        cooperation_ratio = recent_cooperators / total_players
        if cooperation_ratio < self.game_description.m / total_players:
            self.cooperation_frequency = max(self.cooperation_frequency - 0.05, 0)
        else:
            self.cooperation_frequency = min(self.cooperation_frequency + 0.05, 1)
        recent_threshold_met = recent_cooperators >= self.game_description.m
        if self.threshold_met_recently and (not recent_threshold_met):
            self.threshold_met_recently = False
            return Action.D
        if state.round_number < self.game_description.n_rounds - 2:
            if recent_threshold_met:
                self.threshold_met_recently = True
                return Action.C
            if random.random() < self.cooperation_frequency:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_342(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Threshold-Based Defection.
    
    This strategy adapts to the game's history and parameters, balancing individual self-interest 
    with collective risk aversion by cooperating when the threshold is met or likely to be met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players, m, _, _ = (self.game_description.n_players, self.game_description.m, self.game_description.k, self.game_description.n_rounds)
        if state.round_number == 0:
            return Action.C
        prev_threshold_met = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] >= m
        if prev_threshold_met:
            return Action.C
        elif any([history.opponent_actions[-1, i] for i in range(n_players - 1)]):
            p = max(0, min(1, (n_players - m + 1) / n_players))
            return Action.D if random.random() < p else Action.C
        elif state.round_number == self.game_description.n_rounds - 2:
            return Action.D
        elif prev_threshold_met and sum(history.opponent_actions[-1, :]) + history.my_actions[-1] < m:
            p = max(0, min(1, (m - 1) / m))
            return Action.C if random.random() < p else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_343(BaseStrategy):
    """
    This strategy aims to balance individual payoff maximization with contributing to the collective good.
    It adapts the cooperation threshold based on the history of contributions, encouraging players to cooperate when it's likely to lead to a better collective outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperated_last_round = False
        self.m_t = 0
        self.theta_t = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.m_t >= self.game_description.m:
            self.theta_t = 1 - (self.game_description.m - self.m_t) / self.game_description.n_players
        else:
            self.theta_t = 0
        random_value = random.random()
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        others_cooperated = opponent_cooperators > 0
        if self.theta_t > random_value and (others_cooperated or state.round_number == 1):
            action = Action.C
            self.cooperated_last_round = True
        else:
            action = Action.D
            self.cooperated_last_round = False
        if history is not None:
            self.m_t = sum(history.my_actions[:state.round_number + 1]) + sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1 and any((self.theta_t > random.random() for _ in range(state.round_number))):
            return Action.C
        return action

class Strategy_COLLECTIVE_344(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        previous_cooperators = sum(prev_opponent_actions)
        if previous_cooperators >= self.game_description.m:
            action = Action.C
        else:
            p_defect = (self.game_description.m - previous_cooperators) / self.game_description.n_players
            if random.random() < p_defect:
                action = Action.D
            else:
                action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff_C = self.game_description.k * (self.game_description.n_rounds - state.round_number)
            private_payoff_D = self.game_description.n_rounds - state.round_number
            if total_payoff_C > private_payoff_D:
                action = Action.C
        return action

class Strategy_COLLECTIVE_345(BaseStrategy):
    """
    This strategy adapts to the evolving behavior of other players by balancing individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_round_cooperators = 0
        self.my_previous_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            self.previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            if self.previous_round_cooperators >= self.game_description.m:
                return Action.C
            elif self.previous_round_cooperators > 0:
                p_defect = 1 - (self.game_description.k - 1) / self.game_description.k
                return Action.D if random.random() < p_defect else Action.C
            else:
                p_cooperate = 1 / (self.game_description.n_players - self.game_description.m + 1)
                return Action.C if random.random() < p_cooperate else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.previous_round_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_346(BaseStrategy):
    """
    The ACC strategy is designed to balance individual self-interest with collective cooperation,
    adapting to the behavior of other players and the game's history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.d_consecutive = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :])
        CI = m_prev / self.game_description.n_players * (self.game_description.k - 1)
        action = Action.D
        if m_prev >= self.game_description.m or CI > self.alpha:
            action = Action.C
        if history.my_actions[-1]:
            self.d_consecutive[0] = 0
        else:
            self.d_consecutive[0] += 1
        if self.d_consecutive[0] >= 2:
            action = Action.D
        self.alpha += (1 - self.alpha) * (CI - self.alpha)
        return action

class Strategy_COLLECTIVE_347(BaseStrategy):
    """
    Adaptive Cooperation with Memory Strategy.

    This strategy encourages cooperation in early rounds, adapts to the level of cooperation observed among other players,
    and implements a punishment mechanism to discourage free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M = 0
        self.K_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        obs_actions = history.opponent_actions[-1, :]
        self.M = sum((1 for a in obs_actions if a)) + (history.my_actions[-1] == Action.C)
        self.K_met = self.M >= self.game_description.m
        if self.M >= self.game_description.m and self.K_met:
            return Action.C
        elif not self.K_met and self.M < self.game_description.m:
            return Action.D
        elif self.K_met and self.M < self.game_description.m:
            return Action.C if random.random() < 0.5 else Action.D
        return Action.D

class Strategy_COLLECTIVE_348(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective cooperation, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate = None
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperate = True if self.game_description.m <= self.game_description.n_players / 2 else False
            return Action.C if self.cooperate else Action.D
        prev_payoff = history.my_payoffs[-1]
        m_t = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_payoff > 1 + self.game_description.k / 2:
            self.cooperate = True
        elif prev_payoff <= 1 + self.game_description.k / 2 and m_t < self.game_description.m:
            self.cooperate = False
        else:
            self.cooperate = not self.cooperate
        if state.round_number == self.game_description.n_rounds - 1:
            self.cooperate = True if prev_payoff > 1 + self.game_description.k / 2 else False
        if history.my_actions[-1] == False and all(history.opponent_actions[-1, :] == False):
            self.consecutive_defections += 1
            if self.consecutive_defections > self.game_description.n_rounds // 4:
                self.cooperate = True
                self.consecutive_defections = 0
        return Action.C if self.cooperate else Action.D

class Strategy_COLLECTIVE_349(BaseStrategy):
    """
    Adaptive Collective Risk (ACR) strategy for the Collective Risk Dilemma game.
    
    This strategy prioritizes cooperation and collective success while adapting to the behaviors of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = [0] * (game_description.n_rounds + 1)
        self.opp_coop_count = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if state.round_number < 5:
            self.coop_count[state.round_number] = sum(history.opponent_actions[-1, :] + [True])
            self.opp_coop_count[state.round_number % self.game_description.n_players] += self.coop_count[state.round_number]
            return Action.D
        if state.round_number >= 5:
            if self.coop_count[state.round_number - 2] >= self.game_description.m - 1:
                return Action.C
            elif sum(self.opp_coop_count) / self.game_description.n_players > 0.5:
                return Action.C
            else:
                return Action.D
            if state.round_number % 4 == 0 and sum(self.coop_count[state.round_number - 3:state.round_number + 1]) < self.game_description.m * 2:
                for _ in range(2):
                    return Action.D
        if state.round_number >= self.game_description.n_rounds - 1:
            if self.coop_count[state.round_number - 2] >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_350(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma.
    
    Initially cooperates with a probability of m/n, then adapts to the level of cooperation observed in subsequent rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = 0
        self.previous_payoff = 0
        self.cooperation_probability = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        self.cooperators = sum(history.opponent_actions[-1, :])
        if self.cooperators >= self.game_description.m:
            cooperation_decision = True
        elif self.cooperators > 0:
            self.cooperation_probability += 0.1
            if self.cooperation_probability > 1:
                self.cooperation_probability = 1
            cooperation_decision = random.random() < self.cooperation_probability
        else:
            cooperation_decision = False
        if self.cooperators < self.game_description.m - 1 and history.my_payoffs[-1] * (1 - int(cooperation_decision)) > np.mean(history.my_payoffs):
            cooperation_decision = False
        return Action.C if cooperation_decision else Action.D

class Strategy_COLLECTIVE_351(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective risk management.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.delta = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if state.round_number < 3:
            return Action.C
        elif previous_round_cooperators >= m:
            return Action.C
        else:
            p_defect = 1 - k / (n_players + k)
            if random.random() < p_defect:
                return Action.D
            else:
                return Action.C
        if history.my_payoffs[-1] > 1 - int(history.my_actions[-1]):
            self.alpha = min(self.alpha + self.delta, 1)
        else:
            self.alpha = max(self.alpha - self.delta, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_352(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters by balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.average_payoff = 0.0
        self.previous_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        k_threshold_factor = self.game_description.k
        n_rounds = self.game_description.n_rounds
        if state.round_number <= n_players:
            action = Action.C
        elif n_players < state.round_number <= 2 * n_players:
            action = Action.C if state.round_number % 2 == 0 else Action.D
        else:
            self.average_payoff = np.mean(history.my_payoffs[max(0, state.round_number - n_players):])
            self.previous_cooperators = sum(history.opponent_actions[-1, :])
            if self.average_payoff > k_threshold_factor:
                action = Action.C
            elif self.previous_cooperators < m_cooperators_needed and state.round_number != n_rounds - 1:
                action = Action.D
            else:
                action = Action.C
        if state.round_number == n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_353(BaseStrategy):
    """
    This strategy is designed to balance individual self-interest with collective well-being,
    while being robust to various opponent behaviors. It encourages cooperation and punishes defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CR = 0
        self.p = 1 - game_description.k / (game_description.k + 1)
        self.cooperation_probability = 0.5
        self.threshold_adjustment_rounds = math.ceil(game_description.n_rounds / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :] if history else []
        cooperators = sum(opponent_actions)
        self.CR = cooperators / self.game_description.n_players
        if self.CR >= self.game_description.m / self.game_description.n_players:
            return Action.C
        elif len(history.my_payoffs) > self.threshold_adjustment_rounds and sum((self.CR for _ in range(-self.threshold_adjustment_rounds, 0))) < self.game_description.m * self.threshold_adjustment_rounds / self.game_description.n_players:
            self.game_description.m -= 1
        elif len(history.my_payoffs) > 0 and history.my_payoffs[-1] == 1 - self.cooperation_probability + self.game_description.k:
            self.cooperation_probability = min(1, self.cooperation_probability + 0.05)
        if random.random() < self.p:
            return Action.D
        elif random.random() < self.cooperation_probability:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_354(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for Collective Risk Dilemma.
    
    Balances individual self-interest with collective cooperation, adapting to 
    the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_in_previous_round = 0
        self.average_payoff_per_player = 0
        self.defection_probability_p = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 2:
            return Action.C
        self.cooperators_in_previous_round = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] else sum(history.opponent_actions[-1, :])
        if state.round_number <= self.game_description.n_rounds // 2:
            if self.cooperators_in_previous_round >= self.game_description.m:
                return Action.C
            else:
                self.defection_probability_p = (self.game_description.m - self.cooperators_in_previous_round) / self.game_description.n_players
                return Action.D if random.random() < self.defection_probability_p else Action.C
        average_payoff_per_player = np.mean(history.my_payoffs[-5:])
        if average_payoff_per_player < 1.5:
            self.defection_probability_p += 0.1
        if self.cooperators_in_previous_round >= self.game_description.m:
            return Action.C
        else:
            p = max(self.defection_probability_p, (self.game_description.m - self.cooperators_in_previous_round) / self.game_description.n_players)
            return Action.D if random.random() < p else Action.C
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.C

class Strategy_COLLECTIVE_355(BaseStrategy):
    """
    The Adaptive Collective Threshold Strategy balances individual self-interest 
    with collective cooperation to achieve a mutually beneficial outcome in the 
    Collective Risk Dilemma game. It adapts its behavior based on the group's 
    performance and prioritizes cooperation when the collective reward is within reach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_adjustment = 0.5
        self.previous_round_outcome = None

    def calculate_cooperation_rate(self, previous_round_outcome: PlayerHistory) -> float:
        """Calculate the cooperation rate in the given round."""
        total_cooperators = np.sum(previous_round_outcome.my_actions[-1]) + np.sum(previous_round_outcome.opponent_actions[-1])
        return total_cooperators / self.game_description.n_players

    def update_threshold_adjustment(self, previous_round_outcome: PlayerHistory) -> None:
        """Update the threshold adjustment factor based on the given round's outcome."""
        cooperation_rate = self.calculate_cooperation_rate(previous_round_outcome)
        if previous_round_outcome.my_payoffs[-1] > 1 and cooperation_rate >= 0.75:
            self.threshold_adjustment *= 1.1
        elif previous_round_outcome.my_payoffs[-1] <= 1 and cooperation_rate < 0.5:
            self.threshold_adjustment *= 0.9

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        previous_cooperation_rate = self.calculate_cooperation_rate(history)
        self.update_threshold_adjustment(history)
        adjusted_m = math.ceil(self.game_description.m * (1 + self.threshold_adjustment))
        if history.my_payoffs[-1] > 1:
            return Action.C if previous_cooperation_rate >= 0.75 else Action.D
        else:
            return Action.C if previous_cooperation_rate >= 0.5 else Action.D

class Strategy_COLLECTIVE_356(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy for the Collective Risk Dilemma game.
    
    This strategy encourages cooperation by initially cooperating, adapting to recent outcomes,
    and considering expected payoffs in the last round. It promotes collective success in the face of uncertainty.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.m_avg = 0
        self.threshold = game_description.m
        self.w = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.update_m_avg(history)
        if self.m_prev >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        if self.m_avg >= self.game_description.m:
            self.threshold += 1
        else:
            self.threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            expected_payoff_C = self.calculate_expected_payoff(Action.C, history)
            expected_payoff_D = self.calculate_expected_payoff(Action.D, history)
            if expected_payoff_C > expected_payoff_D:
                action = Action.C
            else:
                action = Action.D
        return action

    def update_m_avg(self, history: PlayerHistory):
        recent_cooperators = [sum(history.opponent_actions[i, :]) for i in range(max(0, len(history.my_actions) - self.w), len(history.my_actions))]
        self.m_prev = sum(recent_cooperators[-1:])
        if len(recent_cooperators) > 1:
            self.m_avg = np.mean(recent_cooperators[:-1])

    def calculate_expected_payoff(self, action: Action, history: PlayerHistory):
        if action == Action.C:
            return 0 + self.game_description.k if self.m_prev >= self.threshold else 0
        else:
            return 1 + self.game_description.k if self.m_prev >= self.threshold else 1

class Strategy_COLLECTIVE_357(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma (ACRD) strategy.

    This strategy initially cooperates with a probability proportional to the minimum number of cooperators needed,
    then adapts based on game history. It incorporates reciprocal altruism and punishment mechanisms.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_min_cooperators = game_description.m
        self.k_reward_factor = game_description.k
        self.r_total_rounds = game_description.n_rounds
        self.p_init = self.m_min_cooperators / self.n_players
        self.p_recip = self.k_reward_factor / (self.k_reward_factor + 1)
        self.p_punish = 1 - self.m_min_cooperators / self.n_players
        self.num_coop_rounds = 0
        self.p_avgcoop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        elif state.round_number == self.r_total_rounds - 1:
            prev_round_coop = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            return Action.C if prev_round_coop >= self.m_min_cooperators else Action.D
        prev_round_coop = sum(history.opponent_actions[-1, :])
        threshold_met_prev_round = prev_round_coop >= self.m_min_cooperators - history.my_actions[-1]
        if state.round_number > 0:
            self.num_coop_rounds += int(threshold_met_prev_round)
            self.p_avgcoop = (self.p_avgcoop * (state.round_number - 1) + self.num_coop_rounds) / state.round_number
            if self.p_avgcoop > 0.5:
                self.p_recip = min(self.p_recip * 1.1, 1)
        coop_prob = self.p_init if state.round_number == 0 else self.p_recip if threshold_met_prev_round else 1 - self.p_punish
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_COLLECTIVE_358(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that balances individual self-interest with collective risk management.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def get_payoff(self, action: Action, num_cooperators: int) -> float:
        if action == Action.C:
            if num_cooperators >= self.game_description.m:
                return self.game_description.k
            else:
                return -1
        elif num_cooperators < self.game_description.m:
            return 1
        else:
            return self.game_description.k + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        total_payoff_prev_round = sum((self.get_payoff(Action(int(action)), num_cooperators_prev_round) for action in np.append(history.opponent_actions[-1, :], history.my_actions[-1])))
        p_defect = (self.game_description.m - num_cooperators_prev_round) / (self.game_description.n_players - self.game_description.m + 1)
        if random.random() < p_defect:
            return Action.D
        elif total_payoff_prev_round >= self.game_description.n_players * self.game_description.k / 2:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_359(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Memory (ACTM) strategy.
    
    This strategy prioritizes collective success while incorporating individual self-interest. 
    It adapts to the group's performance and adjusts behavior accordingly, promoting a balance between cooperation and competition.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_memory = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if state.round_number < 5 and prev_cooperators < self.game_description.m:
            self.threshold_memory = -1
        if prev_cooperators >= self.game_description.m + self.threshold_memory:
            return Action.C
        elif self.game_description.k > 2 and prev_cooperators > self.game_description.m + self.threshold_memory:
            return Action.C if random.random() < (self.game_description.k - 1) / self.game_description.k else Action.D
        else:
            deficiency = max(0, self.game_description.m - prev_cooperators)
            defect_prob = min(deficiency / self.game_description.n_players, 1)
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            return Action.D if random.random() < defect_prob else Action.C

class Strategy_COLLECTIVE_360(BaseStrategy):
    """
    The ACC strategy aims to balance individual self-interest with collective welfare by adaptively responding to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_round_cooperators = 0
        self.previous_round_payoff = 0.0
        self.cooperation_probability = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        previous_round_payoff = self.calculate_payoff(history.my_actions[-1], self.game_description.n_players, self.game_description.m, self.game_description.k)
        if history.opponent_payoffs is not None:
            previous_round_opponent_payoffs = np.mean(history.opponent_payoffs[-1, :])
        else:
            previous_round_opponent_payoffs = 0.0
        if self.previous_round_cooperators < self.game_description.m:
            return Action.D
        else:
            opponent_defected = False
            for opponent in history.opponent_actions[-1, :]:
                if not opponent:
                    opponent_defected = True
                    break
            if opponent_defected:
                self.cooperation_probability *= 0.5
            if previous_round_payoff > 1.5 * self.game_description.n_players or (previous_round_opponent_payoffs > 1.5 and previous_round_opponent_payoffs != 0):
                self.cooperation_probability *= 0.8
        opponent_punishments = []
        for i, opponent in enumerate(history.opponent_actions[-1, :]):
            if not opponent:
                punishment_probability = 0.7
                opponent_punishments.append((i, punishment_probability))
        if len(opponent_punishments) > 0 and random.random() < opponent_punishments[0][1]:
            return Action.D
        if self.cooperation_probability >= 0.5:
            return Action.C
        else:
            return Action.D

    @staticmethod
    def calculate_payoff(action, n_players: int, m: int, k: float) -> float:
        if action:
            c_i = 1
        else:
            c_i = 0
        cooperators = sum([c_i] + [random.choice([True, False]) for _ in range(n_players - 1)])
        payoff = 1 - c_i
        if cooperators >= m:
            payoff += k
        return payoff

class Strategy_COLLECTIVE_361(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for Collective Risk Dilemma game.
    
    This strategy adapts to the cooperation level of opponents, gradually increasing 
    or decreasing its own cooperation threshold based on observed behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_prime = np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        self.threshold *= 0.9 + 0.1 * (m_prime > self.game_description.m)
        if m_prime >= self.game_description.m:
            p_coop = self.game_description.k / (self.game_description.k + 1)
        else:
            p_coop = 1 - (m_prime / self.game_description.n_players) ** 2
        return Action.C if random.random() < p_coop else Action.D

class Strategy_COLLECTIVE_362(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == self.game_description.n_rounds - 1 and (history.my_actions[-1] or sum(history.opponent_actions[-1, :]))):
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        threshold_met = any((sum(round_cooperators) >= self.game_description.m for round_cooperators in history.opponent_actions))
        total_payoff_beneficial = sum(history.my_payoffs[-1:]) + sum(history.opponent_payoffs[-1, :]) > self.game_description.n_players * 1
        if cooperators >= self.game_description.m or total_payoff_beneficial:
            return Action.C
        punished_recently = history.my_actions[-1] == False and sum(history.opponent_actions[-2, :]) < self.game_description.m
        consecutive_defection = sum((sum(round_cooperators) <= self.game_description.m - 1 for round_cooperators in history.opponent_actions[-3:])) >= 2
        if punished_recently or consecutive_defection:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_363(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) strategy for the Collective Risk Dilemma.
    
    This strategy adapts to the game's history and parameters while promoting cooperation.
    It initially cooperates, explores opponents' behavior, and adjusts based on average payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize ACC strategy with game description.

        :param game_description: Game description containing n_players, n_rounds, m, and k.
        """
        self.game_description = game_description
        self.coop_rounds = 0
        self.avg_coop_payoff = 0
        self.avg_defect_payoff = 0
        self.warning_given = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide action based on current state and history.

        :param state: Current game state.
        :param history: History of actions and payoffs (None for the first round).
        :return: Chosen action (Action.C or Action.D).
        """
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if self.coop_rounds < self.game_description.m and state.round_number <= 5:
            return Action.D
        if history.opponent_payoffs.shape[0] > 0:
            coop_payments = np.sum(history.my_payoffs) + np.sum(np.extract(history.opponent_actions[-1, :], history.opponent_payoffs))
            defect_payments = np.sum(np.extract(~history.opponent_actions[-1, :], history.opponent_payoffs))
            avg_coop_payment = coop_payments / (prev_cooperators + 1e-06)
            avg_defect_payment = defect_payments / (self.game_description.n_players - prev_cooperators + 1e-06)
            self.avg_coop_payoff, self.avg_defect_payoff = (avg_coop_payment, avg_defect_payment)
        if self.avg_coop_payoff > self.avg_defect_payoff + 1:
            return Action.C
        else:
            return Action.D
        if prev_cooperators >= self.game_description.m and state.round_number < self.game_description.n_rounds - 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_defected_last_round = np.any(~history.opponent_actions[-1, :])
        if opponent_defected_last_round and (not self.warning_given):
            self.warning_given = True
            return Action.D
        self.coop_rounds += 1 if history.my_actions[-1] else 0
        return Action.D

class Strategy_COLLECTIVE_364(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Enforcement (ATE)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        elif state.round_number < n:
            prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            if prev_cooperators < m:
                return Action.D
        else:
            prev_payoffs = np.array([0.0 if x == False else 1 - k for x in history.opponent_actions[-1, :]])
            prev_payoffs = np.append(prev_payoffs, [0.0 if history.my_actions[-1] == False else 1 - k])
            total_prev_payoff = sum(prev_payoffs)
            avg_opponent_cooperators = sum(history.opponent_actions[-1, :]) / (n - 1)
            if total_prev_payoff >= k * (n - 1):
                if prev_cooperators >= m:
                    return Action.C
                else:
                    p = (m - prev_cooperators) / n
                    return random.choices([Action.D, Action.C], weights=[p, 1 - p])[0]
            avg_payoff_last_n_rounds = np.mean(history.my_payoffs[-n:])
            if avg_payoff_last_n_rounds >= k * (n - 1):
                return Action.C
        if state.round_number == r:
            prev_total_payoff = sum([0.0 if x == False else 1 - k for x in history.opponent_actions[-1, :]])
            prev_total_payoff += [0.0 if history.my_actions[-1] == False else 1 - k][-1]
            if prev_total_payoff <= k * (n - 1):
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_365(BaseStrategy):
    """
    This strategy adapts to recent history and adjusts cooperation probabilities 
    based on the community's performance, aiming to balance individual self-interest 
    with collective cooperation to achieve the community reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        C_prev = sum(history.opponent_actions[-1, :])
        p = 0.0
        if C_prev >= self.game_description.m:
            p = min(self.game_description.k - 1, self.game_description.k * C_prev / self.game_description.n_players)
        else:
            p = max(0, 1 - self.game_description.k * C_prev / (self.game_description.n_players - self.game_description.m))
        cooperate = random.random() < p
        if history.my_payoffs[-1] > 1:
            self.cooperation_probability += 0.1
        else:
            self.cooperation_probability -= 0.1
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_366(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for Collective Risk Dilemma.
    
    Balances individual self-interest with cooperative behavior, adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] | np.bool_(history.my_actions[-1]))
        if state.round_number < self.game_description.n_rounds - 1:
            if history.my_payoffs is None or len(history.my_payoffs) <= 1:
                return Action.C
            previous_previous_cooperators = sum(history.opponent_actions[-2, :] | np.bool_(history.my_actions[-2]))
            if previous_cooperators >= self.game_description.m or previous_cooperators > previous_previous_cooperators:
                return Action.C
            return Action.D
        else:
            total_payoff_C = sum(history.my_payoffs) + self.game_description.k
            total_payoff_D = sum(history.my_payoffs) + 1
            if total_payoff_C > total_payoff_D:
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_367(BaseStrategy):
    """
    A strategy that adapts to the game's history and opponent behaviors,
    balancing individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
        avg_prev_round_payoff = np.mean(history.my_payoffs[-1] if len(history.my_payoffs) > 0 else [0])
        opponent_avg_prev_round_payoff = np.mean(history.opponent_payoffs[-1, :].sum(axis=0) if len(history.opponent_payoffs) > 0 and len(history.opponent_payoffs[-1]) > 0 else [0])
        if prev_round_cooperators >= self.game_description.m and avg_prev_round_payoff > opponent_avg_prev_round_payoff:
            return Action.C
        elif prev_round_cooperators < self.game_description.m or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_368(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.α = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            p_coop = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < p_coop else Action.D
        prev_round_actions = history.opponent_actions[-1, :]
        p_prev = np.mean(prev_round_actions)
        coop_threshold = self.game_description.m / self.game_description.n_players - self.α * (self.game_description.m / self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif p_prev >= coop_threshold:
            action = Action.C
        else:
            action = Action.D
        if history.my_payoffs is not None and len(history.my_payoffs) > 0:
            collective_payoff = np.mean([x + y for x, y in zip(history.opponent_payoffs[-1], [history.my_payoffs[-1]] * self.game_description.n_players)])
            if collective_payoff >= self.game_description.k / 2:
                self.α *= 0.9
            elif collective_payoff < self.game_description.k / 2:
                self.α *= 1.1
        return action

class Strategy_COLLECTIVE_369(BaseStrategy):
    """
    The Adaptive Collective Risk (ACR) strategy balances individual self-interest with collective well-being.
    It adapts to the game's history and parameters, prioritizing cooperation when it benefits the group.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = None
        self.coop_prob = 0.5
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.history = history
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_round_coops = sum(self.history.opponent_actions[-1, :] + [bool(self.history.my_actions[-1])])
        prev_round_payoff = self.history.my_payoffs[-1]
        avg_payoff = np.mean(np.concatenate((self.history.my_payoffs, self.history.opponent_payoffs[-1])))
        if prev_round_coops >= self.game_description.m and prev_round_payoff <= avg_payoff:
            action = Action.C
        elif prev_round_coops < self.game_description.m and self.coop_prob > 0:
            self.coop_prob *= self.game_description.k / (self.game_description.k + 1)
            action = Action.C if random.random() < self.coop_prob else Action.D
        else:
            action = Action.D
        if random.random() < self.epsilon:
            action = Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1 and prev_round_coops < self.game_description.m:
            action = Action.D
        return action

class Strategy_COLLECTIVE_370(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game.
    
    This strategy starts with a probabilistic initial cooperation, 
    then adapts to the dynamics of the game by adjusting its probability 
    of cooperation based on whether the group's average cooperation rate 
    meets or exceeds the required threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = game_description.m / game_description.n_players
        self.p_coop = self.p_init
        self.delta_p = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperations = np.sum(history.opponent_actions[state.round_number - 1])
        c_avg = (opponent_cooperations + (history.my_actions[state.round_number - 1] == True)) / self.game_description.n_players
        if c_avg >= self.game_description.m / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if c_avg >= self.game_description.m / self.game_description.n_players and self.p_coop < 1:
            self.p_coop += self.delta_p
        elif c_avg < self.game_description.m / self.game_description.n_players and self.p_coop > 0:
            self.p_coop -= self.delta_p
        return action

class Strategy_COLLECTIVE_371(BaseStrategy):
    """
    This strategy aims to balance individual self-interest with collective well-being,
    adapting to the game's dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_coop = 0
        self.opp_defects = []
        self.p_forgive = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= 2:
            self.num_coop = sum(history.opponent_actions[-1, :])
            return Action.C
        elif state.round_number == 3:
            if self.num_coop < self.game_description.m:
                return Action.D
            else:
                return Action.C
        else:
            self.num_coop = sum(history.opponent_actions[-1, :])
            if self.num_coop >= self.game_description.m:
                return Action.C
            else:
                p_defect = 1 - self.num_coop / self.game_description.m
                if random.random() < p_defect:
                    return Action.D
                else:
                    return Action.C
            for opponent, action in enumerate(history.opponent_actions[-1, :]):
                if not action and self.num_coop >= self.game_description.m:
                    self.opp_defects.append(opponent)
                elif action and opponent in self.opp_defects:
                    self.p_forgive = 0.5 + self.num_coop / self.game_description.m
                    if random.random() < self.p_forgive:
                        return Action.C
            if state.round_number == self.game_description.n_rounds - 1 and self.num_coop >= self.game_description.m:
                return Action.C

class Strategy_COLLECTIVE_372(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    This strategy aims to balance individual self-interest with collective cooperation, 
    adapting to the game's history and parameters. It ensures robustness against various opponent behaviors 
    while promoting overall group success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_opponents_cooperate = sum(history.opponent_actions[-1, :])
        if len(history.my_actions) > 1 and (not all(history.my_actions)):
            consecutive_defections = 0
            for i in range(len(history.my_actions) - 2, -1, -1):
                if history.my_actions[i]:
                    break
                consecutive_defections += 1
        else:
            consecutive_defections = 0
        last_round_total_payoff = sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        expected_payoff_all_cooperate = (self.game_description.k - 1) * self.game_description.n_players
        if last_round_opponents_cooperate >= self.game_description.m:
            return Action.C
        elif last_round_total_payoff > expected_payoff_all_cooperate:
            return Action.C
        elif consecutive_defections >= 2:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1 and sum(history.my_payoffs) + sum([p for p in history.opponent_payoffs[-1, :]]) > expected_payoff_all_cooperate * (state.round_number + 1):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_373(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    This strategy balances individual self-interest with collective cooperation, adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = 0
        self.total_payoff = 0
        self.punish = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = self.cooperators
        if history.opponent_actions is not None:
            self.cooperators = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if history.my_payoffs is not None:
            self.total_payoff += history.my_payoffs[-1]
        if previous_cooperators >= self.game_description.m and self.total_payoff > 0:
            action = Action.C
        elif self.punish:
            action = Action.D
        else:
            action = Action.C
        if self.cooperators < self.game_description.m and (history.my_payoffs is None or history.my_payoffs[-1] < 0):
            self.punish = True
        elif self.cooperators >= self.game_description.m or (history.my_payoffs is not None and history.my_payoffs[-1] > 0):
            self.punish = False
        if action == Action.D:
            if history.opponent_actions is not None and np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C) >= previous_cooperators:
                action = Action.C
            elif self.total_payoff > 0:
                action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_375(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for Collective Risk Dilemma games.
    
    This strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    It encourages cooperation when it's likely to succeed based on the proportion of cooperators and the reward factor k.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize the strategy with the game description.

        :param game_description: The description of the Collective Risk Dilemma game.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the action to take based on the current state and history.

        :param state: The current state of the game.
        :param history: The history of actions and payoffs for all players.
        :return: The chosen action (C or D).
        """
        if state.round_number == 0:
            return Action.C
        prev_coop_proportion = self._count_prev_coops(history) / self.n_players
        cooperation_threshold = self.m / self.n_players * (1 + (self.k - 1) / self.k)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D if prev_coop_proportion < self.m / self.n_players else Action.C
        return Action.C if prev_coop_proportion >= cooperation_threshold else Action.D

    def _count_prev_coops(self, history: PlayerHistory) -> int:
        """
        Count the number of cooperators in the previous round.

        :param history: The history of actions and payoffs for all players.
        :return: The number of cooperators in the previous round.
        """
        prev_round_actions = history.opponent_actions[-1, :]
        return sum(prev_round_actions) + (history.my_actions[-1] == 1)

class Strategy_COLLECTIVE_377(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for Collective Risk Dilemma.
    
    This strategy aims to balance individual self-interest with collective well-being,
    promoting a stable and cooperative outcome in the face of uncertainty and diverse opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperated_last_round = False
        self.total_payoff_last_round = 0.0
        self.num_defectors_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_last_round = sum(history.opponent_actions[-1, :] | np.array([self.cooperated_last_round]))
        majority_cooperated_last_round = num_cooperators_last_round > self.game_description.n_players / 2
        reward_achieved_last_round = self.total_payoff_last_round >= 1 + self.game_description.k
        consistently_defected_player = state.round_number > self.game_description.n_rounds // 2 and all((not history.opponent_actions[t, i] for t in range(state.round_number - 1) for i in range(self.game_description.n_players - 1)))
        if num_cooperators_last_round >= self.game_description.m:
            return Action.C
        elif self.total_payoff_last_round <= 1 and self.num_defectors_last_round > 0:
            return Action.D
        elif majority_cooperated_last_round and reward_achieved_last_round:
            return Action.C
        else:
            if consistently_defected_player:
                return Action.D
            return Action.C

class Strategy_COLLECTIVE_378(BaseStrategy):
    """
    Adaptive Threshold Cooperation Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.success_count = 0
        self.coop_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if np.random.rand() < self.coop_prob else Action.D
        m_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if m_prev >= self.game_description.m:
            self.coop_prob = 0.8
        else:
            self.coop_prob = 0.4
        if self.success_count > 0.7 * self.game_description.n_rounds:
            self.coop_prob -= 0.1
            self.coop_prob = max(self.coop_prob, 0.5)
        elif self.success_count < 0.3 * self.game_description.n_rounds:
            self.coop_prob += 0.1
            self.coop_prob = min(self.coop_prob, 0.8)
        if state.round_number == self.game_description.n_rounds - 1:
            self.coop_prob = 0.9
        action = Action.C if np.random.rand() < self.coop_prob else Action.D
        if action == Action.C and m_prev >= self.game_description.m:
            self.success_count += 1
        return action

class Strategy_COLLECTIVE_379(BaseStrategy):
    """
    The Adaptive Collective Risk (ACR) strategy balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.h = [0] * game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            p = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        theta = max(0, min(1, self.h[state.round_number - 2] - (self.game_description.m - 1) / self.game_description.n_players))
        p = theta
        action = Action.C if random.random() < p else Action.D
        if sum(self.h[:state.round_number - 1]) < self.game_description.m:
            q = 0.5
            action = Action.D if random.random() < q else Action.C
        cooperators = sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == True)
        self.h[state.round_number - 1] = cooperators / self.game_description.n_players
        return action

class Strategy_COLLECTIVE_380(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    This strategy aims to balance individual self-interest with collective well-being by adapting to the evolving cooperation dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.success_rounds = 0
        self.total_rounds = 0
        self.punishment_rounds = 2
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [True])
        prev_payoffs = history.my_payoffs[-1]
        if prev_cooperators >= self.game_description.m:
            self.success_rounds += 1
        self.total_rounds += 1
        avg_prev_payoff = np.mean(history.opponent_payoffs[-1, :])
        if self.success_rounds / self.total_rounds >= 0.5 or (state.round_number < 5 and prev_cooperators > 0):
            return Action.C
        elif prev_cooperators >= self.game_description.m - 1 or (prev_cooperators == self.game_description.m - 2 and history.my_payoffs[-1] <= avg_prev_payoff + self.epsilon):
            return Action.C
        elif prev_payoffs > avg_prev_payoff + self.epsilon:
            self.punishment_rounds += 1
            return Action.D if self.punishment_rounds > 0 else Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_381(BaseStrategy):
    """
    The Adaptive Collective Threshold strategy balances individual self-interest with collective cooperation.
    It adapts to the game's history and parameters, introducing a punishment mechanism when necessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        ct = self.game_description.m / self.game_description.n_players * (previous_round_cooperators / self.game_description.n_players)
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        expected_full_cooperation_payoff = (self.game_description.k - 1) * self.game_description.n_players + self.game_description.n_players
        if history.my_payoffs[-1] < expected_full_cooperation_payoff:
            self.punishment_rounds_remaining = 3
        if ct >= 0.5 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_382(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)
    
    This strategy balances individual self-interest with collective risk management,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        total_payoff_last_round = np.sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        if cooperators_last_round >= self.game_description.m or total_payoff_last_round >= (self.game_description.n_players - 1) * self.game_description.k + (self.game_description.n_players - self.game_description.m):
            return Action.C
        if cooperators_last_round < self.game_description.m or total_payoff_last_round < (self.game_description.n_players - 1) * self.game_description.k + (self.game_description.n_players - self.game_description.m):
            return Action.D
        if random.random() < (self.game_description.n_players - self.game_description.m) / self.game_description.n_players:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_383(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy aims to balance individual self-interest with collective benefit.
    It adapts to the game's history and parameters, ensuring a robust response to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_defections = 0
        self.cooperation_threshold = math.floor(game_description.m / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_round_cooperators < self.game_description.m:
            self.consecutive_defections += 1
            return Action.D
        else:
            self.consecutive_defections = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.consecutive_defections > self.game_description.m - 1:
            self.consecutive_defections = 0
            return Action.C
        recent_cooperation = sum(history.opponent_actions[-2:, :].sum(axis=0) + history.my_actions[-2:])
        if recent_cooperation >= self.cooperation_threshold:
            return Action.C
        avg_payoff_last_quarter = np.mean(history.my_payoffs[-math.floor(self.game_description.n_rounds / 4):])
        current_payoff = history.my_payoffs[-1]
        if avg_payoff_last_quarter > current_payoff and self.cooperation_threshold < self.game_description.m:
            self.cooperation_threshold += 1
        elif avg_payoff_last_quarter < current_payoff and self.cooperation_threshold > math.floor(self.game_description.m / 2):
            self.cooperation_threshold -= 1
        return Action.C if prev_round_cooperators >= self.cooperation_threshold else Action.D

class Strategy_COLLECTIVE_384(BaseStrategy):
    """
    An adaptive collective strategy that balances individual self-interest with 
    collective well-being by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.threshold = math.floor(game_description.m / 2)
        self.last_round_outcome = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :] != False)
        self.threshold = max(math.floor(self.game_description.m / 2), min(self.game_description.n_players - 1, self.threshold + self.last_round_outcome * 2 - 1))
        self.last_round_outcome = sum(history.my_payoffs[-1:]) >= 0
        if self.prev_cooperators >= self.threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_385(BaseStrategy):
    """
    Collective Risk Dilemma strategy: Adaptive Threshold Cooperator (ATC).
    
    Encourages cooperation by cooperating initially and adapting to the observed level of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_half_ceiling = math.ceil(game_description.m / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum((1 for action in history.opponent_actions[-1] if action))
        if prev_cooperators >= self.m_half_ceiling:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_386(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.
    This strategy prioritizes collective cooperation by adapting to opponent behaviors and game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_adj = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_coop_counts = sum(history.opponent_actions[-1, :])
        n_def = self.game_description.n_players - opponent_coop_counts - 1
        if opponent_coop_counts >= self.game_description.m:
            action = Action.C
        else:
            p = (self.game_description.m - opponent_coop_counts) / (self.game_description.n_players - self.game_description.m)
            action = Action.D if random.random() < p else Action.C
        if state.round_number > 1 and history is not None:
            prev_round_actions = history.opponent_actions[-2, :]
            prev_cooperators = sum(prev_round_actions)
            if prev_cooperators >= self.game_description.m:
                self.m_adj = max(self.m_adj - 1, self.game_description.m)
            else:
                self.m_adj = min(self.m_adj + 1, self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        return action

class Strategy_COLLECTIVE_387(BaseStrategy):
    """
    Adaptive Collective Risk strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C if m <= n_players / 2 else Action.D
        avg_coops = self._calculate_average_cooperators(history)
        if state.round_number == r - 1:
            return Action.D
        if avg_coops >= m:
            return Action.C
        last_round_actions = history.opponent_actions[-1, :]
        if sum(last_round_actions) < m and (not self._did_i_cooperate_last_round(history)):
            if any((action for action in last_round_actions)):
                return Action.C
        return Action.D

    def _calculate_average_cooperators(self, history: PlayerHistory) -> float:
        """Calculate the average number of cooperators in previous rounds."""
        total_coops = sum((sum(round) for round in history.opponent_actions))
        return total_coops / (len(history.opponent_actions) * self.game_description.n_players)

    def _did_i_cooperate_last_round(self, history: PlayerHistory) -> bool:
        """Check if this player cooperated in the last round."""
        return history.my_actions[-1]

class Strategy_COLLECTIVE_388(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy balances individual self-interest with collective cooperation.
    It adapts to the game's history and opponent behaviors, encouraging cooperation while punishing free-riding.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= 4:
            return Action.C if state.round_number % 2 == 0 else Action.D
        else:
            total_cooperations = sum(history.my_actions) + sum(sum(history.opponent_actions))
            CQ = total_cooperations / ((state.round_number - 1) * self.game_description.n_players)
            if CQ >= self.game_description.m / self.game_description.n_players:
                cooperate = True
            else:
                cooperate = False
            if sum(history.opponent_actions[-1, :]) < self.game_description.m:
                if history.my_actions[-1]:
                    cooperate = False
                else:
                    cooperate = True
            if state.round_number == self.game_description.n_rounds - 1:
                cooperate = True
            return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_389(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with 
    collective risk management by adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_round_num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_round_num_cooperators >= self.game_description.m:
            return Action.C
        else:
            p_defect = (self.game_description.m - prev_round_num_cooperators) / self.game_description.n_players
            if math.isclose(p_defect, 0.0):
                return Action.D
            if random.random() < p_defect:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_391(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) Strategy.
    
    This strategy prioritizes cooperation while adapting to the collective behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.w = 3
        self.e = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        avg_cooperators = np.mean(history.opponent_actions[-min(self.w, state.round_number):, :].sum(axis=1))
        if avg_cooperators >= m / 2 or (state.round_number - 1) % self.e == 0:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 0 and history.my_actions[-1] and (np.sum(history.opponent_actions[-1, :]) < m):
            action = Action.D
        if state.round_number == r - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_392(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy for Collective Risk Dilemma.
    Balances individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.sum_of_cooperators = 0
        self.CT = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        if history.my_actions[-1]:
            self.sum_of_cooperators += 1
        if self.sum_of_cooperators < self.CT:
            action = Action.C
        else:
            action = Action.D
        if np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1] >= self.game_description.m:
            new_CT = min(self.game_description.n_players, self.CT + 1)
        else:
            new_CT = max(self.game_description.m, self.CT - 1)
        self.CT = new_CT
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_COLLECTIVE_393(BaseStrategy):
    """
    Adaptive Threshold Cooperation Strategy.
    
    Initially cooperates and then adjusts its cooperation level based on the number of rounds remaining,
    promoting collective success while balancing individual interests.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        T_t = self.game_description.m - (self.game_description.m - 1) * ((self.game_description.n_rounds - state.round_number - 1) / self.game_description.n_rounds)
        if history is not None:
            self.M = sum(history.opponent_actions[-1, :]) + 1
        if self.M >= math.ceil(T_t):
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_394(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective benefits 
    while adapting to varying opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coops = []
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_coop else Action.D
        prev_m_coops = sum(history.opponent_actions[-1, :])
        self.prev_coops.append(prev_m_coops)
        if prev_m_coops < self.game_description.m:
            action = Action.D
        else:
            self.p_coop = self.game_description.m / self.game_description.n_players * self.game_description.k / (self.game_description.k + 1)
            action = Action.C if random.random() < self.p_coop else Action.D
        if state.round_number > 2 and len(self.prev_coops) > 1 and (self.prev_coops[-2] >= self.game_description.m):
            if history.my_actions[-1]:
                p_coop_punish = 0.5
                action = Action.D if random.random() < p_coop_punish else Action.C
        if state.round_number < 3 and len(self.prev_coops) > 1 and (self.prev_coops[-1] == 0):
            self.p_coop += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        return action

class Strategy_COLLECTIVE_395(BaseStrategy):
    """
    The Adaptive Collective Cooperation strategy adapts to various opponent behaviors 
    and aligns with the collective mindset in a Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_convergence_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperations = history.opponent_actions[-1, :]
        prev_coop_count = sum(prev_cooperations)
        if prev_coop_count >= self.game_description.m / 2:
            action = Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            p_defect = (self.game_description.m - prev_coop_count) / (self.game_description.n_players - 1)
            action = Action.D if random.random() < p_defect else Action.C
        else:
            action = Action.D if prev_coop_count < self.game_description.m else Action.C
        if state.round_number <= 5 and random.random() < 0.1:
            action = Action.C
        if len(history.opponent_actions) >= 3 and sum(prev_cooperations[-3:]) > self.game_description.m / 2 * 3:
            self.coop_convergence_count += 1
            if self.coop_convergence_count == 3:
                adjusted_threshold = self.game_description.m - 1
                action = Action.C if prev_coop_count >= adjusted_threshold else Action.D
        return action

class Strategy_COLLECTIVE_396(BaseStrategy):
    """
    A strategy that adapts to the level of cooperation in previous rounds,
    ensuring individual actions contribute to achieving the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :])
        if prev_coop_count == 0 and random.random() < 1 / n_players:
            return Action.C
        p_defect = 1 - prev_coop_count / m
        if prev_coop_count >= m:
            return Action.C
        elif random.random() < p_defect:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_397(BaseStrategy):
    """
    Collective Risk Dilemma strategy based on adaptive collective optimism.
    Prioritizes cooperation when possible, adapting to the behavior of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] is True)
        if m_prev >= self.game_description.m:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = np.sum(history.my_payoffs) + np.sum(history.opponent_payoffs)
            if total_payoff >= self.game_description.k * self.game_description.n_players:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_398(BaseStrategy):
    """
    Adaptive Cooperativity strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        previous_round_payoff = history.my_payoffs[-1]
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if num_cooperators_last_round >= self.game_description.m:
            return Action.C
        elif previous_round_payoff > 1.5 and num_cooperators_last_round < self.game_description.m:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_399(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.coop_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < self.coop_prob else Action.D
        prev_outcome = history.my_payoffs[-1] > 0
        if prev_outcome:
            self.coop_prob = min(self.coop_prob + 0.1, 1)
        else:
            self.coop_prob = max(self.coop_prob - 0.1, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if history.my_payoffs.sum() + self.game_description.k >= self.game_description.n_rounds * self.game_description.k else Action.D
        if prev_outcome:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        else:
            return Action.D if random.random() < (self.game_description.n_players - self.game_description.m) / self.game_description.n_players else Action.C

class Strategy_COLLECTIVE_400(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to the game environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        cooperators_prev = sum((1 for action in history.opponent_actions[-1] if action)) + 1 if history.my_actions[-1] else 0
        if cooperators_prev >= self.game_description.m:
            p_c = 1 - 1 / self.game_description.k
        else:
            p_c = self.game_description.m / self.game_description.n_players
        if history.my_payoffs.size > 0:
            avg_payoff_hist = np.mean(history.my_payoffs[:-1])
            if state.round_number < self.game_description.n_rounds - 1:
                if sum(history.opponent_payoffs[-1]) + history.my_payoffs[-1] > avg_payoff_hist * self.game_description.n_players:
                    p_c += 0.1
                elif sum(history.opponent_payoffs[-1]) + history.my_payoffs[-1] < avg_payoff_hist * self.game_description.n_players:
                    p_c -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if cooperators_prev == self.game_description.m:
            return random.choice([Action.C, Action.D])
        return Action.C if random.random() < p_c else Action.D

class Strategy_COLLECTIVE_401(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coops = 0
        self.prev_reward = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_rounds % 2 == 0 else Action.D
        m_t = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        coop = False
        if m_t >= self.game_description.m or self.prev_reward:
            coop = True
        elif m_t > 0:
            coop = not history.my_actions[-1]
        reward = self.game_description.k == history.my_payoffs[-1] + (not history.my_actions[-1])
        if not reward and history.my_actions[-1] and any((not a for a in history.opponent_actions[-1, :])):
            coop = False
        if state.round_number == self.game_description.n_rounds - 1:
            coop = True
        self.prev_coops = m_t
        self.prev_reward = reward
        return Action.C if coop else Action.D

class Strategy_COLLECTIVE_402(BaseStrategy):
    """
    A collective strategy that balances individual payoffs with collective success while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.0
        self.m_eff = float(game_description.m)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        payoffs = np.sum(history.my_payoffs) if history.my_payoffs.size > 0 else 0.0
        self.alpha = payoffs / float(self.game_description.n_players)
        self.m_eff = self.game_description.m + self.alpha * (self.game_description.n_players - self.game_description.m)
        p = self.game_description.k / (self.game_description.k + 1) if m_t >= self.game_description.m else 0.0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if m_t >= self.m_eff else Action.D
        else:
            return Action.C if random.random() < p else Action.D

class Strategy_COLLECTIVE_403(BaseStrategy):
    """
    The Adaptive Collective Risk strategy balances individual self-interest with collective risk aversion,
    adapting to the behavior of other players over time.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.0
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m > self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators_prev = sum(opponent_actions)
        num_defectors_prev = self.game_description.n_players - 1 - num_cooperators_prev
        avg_payoff_last_half = np.mean(history.my_payoffs[-self.game_description.n_rounds // 2:])
        if state.round_number > self.game_description.n_rounds // 2 and len(self.history) > 0:
            current_payoff = history.my_payoffs[-1]
            if current_payoff > avg_payoff_last_half:
                self.p = min(1.0, self.p + 0.1)
            elif current_payoff < avg_payoff_last_half:
                self.p = max(0.0, self.p - 0.1)
        if num_cooperators_prev >= self.game_description.m:
            self.p = (self.game_description.m - 1) / self.game_description.n_players
        elif num_defectors_prev >= self.game_description.n_players // 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators_prev >= self.game_description.m - 1 and len(self.history) > 0:
                return Action.C
            else:
                return Action.D
        return Action.C if random.random() < self.p else Action.D
        self.history.append(Action.C.value)

class Strategy_COLLECTIVE_404(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.

    This strategy prioritizes collective well-being by initially cooperating to establish a cooperative atmosphere.
    It then adapts to the evolving game dynamics, taking into account the previous round's outcome and the average cooperation rate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate = True
        self.avg_coop_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_outcome = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_round_outcome >= self.game_description.m:
            action = Action.C
        elif prev_round_outcome > (self.game_description.n_players - self.game_description.m) / 2:
            action = Action.D if self.cooperate else Action.C
            self.cooperate = not self.cooperate
        elif state.round_number > 2 * self.game_description.n_rounds // 3 and self.avg_coop_rate < 0.7:
            action = Action.D
            self.cooperate = False
        else:
            action = Action.C
        if state.round_number > 1:
            prev_prev_round_outcome = np.sum(history.opponent_actions[-2, :]) + (history.my_actions[-2] == True)
            self.avg_coop_rate += (prev_round_outcome - prev_prev_round_outcome) / (self.game_description.n_players * state.round_number)
        return action

class Strategy_COLLECTIVE_405(BaseStrategy):
    """
    A strategy that adapts to the game's history, balancing individual incentives with collective goals.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :] == True)
        if previous_round_cooperators >= self.game_description.m:
            self.p_coop = min(0.9, self.p_coop + 0.1)
        else:
            self.p_coop = max(0.1, self.p_coop - 0.1)
        if self.game_description.k > 2:
            defector_payoffs = history.opponent_payoffs[-1, :][history.opponent_actions[-1, :] == False]
            avg_defector_payoff = np.mean(defector_payoffs) if len(defector_payoffs) > 0 else 0
            if history.my_payoffs[-1] > avg_defector_payoff:
                self.p_coop = min(0.9, self.p_coop + 0.05)
            else:
                self.p_coop = max(0.1, self.p_coop - 0.05)
        if previous_round_cooperators < self.game_description.m:
            cooperate_prob = 0.3
        else:
            cooperate_prob = self.p_coop
        if state.round_number >= self.game_description.n_rounds - 2:
            cooperate_prob = 1.0
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_406(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with the need for collective cooperation,
    making it robust against various opponent behaviors and suitable for a tournament setting.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        total_players = self.game_description.n_players - 1
        cooperation_rate = (opponent_cooperations + 1) / (total_players + 1)
        self.cooperation_rate_history.append(cooperation_rate)
        m = self.game_description.m
        n = self.game_description.n_players
        k = self.game_description.k
        theta = (m - 1) / (n * (k - 1))
        if state.round_number == self.game_description.n_rounds - 1:
            if cooperation_rate >= m / n:
                return Action.C
        if cooperation_rate >= theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_407(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to the history of play and is robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.p_coop = 0.8
        self.p_defect = 0.2
        self.p_end = 0.9
        self.m_t = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        m_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history is not None else False)
        if m_prev >= self.m_t:
            cooperate = random.random() < self.p_coop
        else:
            cooperate = random.random() > self.p_defect
        if state.round_number < self.game_description.n_rounds - 1:
            if m_prev >= self.m_t and sum(history.my_payoffs) + (history.opponent_payoffs[-1, :].sum() if history is not None else 0) > len(history.my_actions):
                self.m_t += 1
            elif m_prev < self.m_t and sum(history.my_payoffs) + (history.opponent_payoffs[-1, :].sum() if history is not None else 0) <= len(history.my_actions):
                self.m_t -= 1
                if self.m_t < 1:
                    self.m_t = 1
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = random.random() < self.p_end
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_409(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective risk management.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0

    def estimate_expected_cooperators(self, history: PlayerHistory) -> int:
        if history.my_actions.size == 0:
            return self.game_description.n_players // 2
        else:
            recent_rounds = min(5, history.my_actions.size)
            opponent_cooperation_rate = np.mean(history.opponent_actions[-recent_rounds:, :])
            return math.floor(self.game_description.n_players * (1 - opponent_cooperation_rate))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.n_players < 10:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            previous_cooperators = sum(history.opponent_actions[-1, :])
            if previous_cooperators >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        expected_cooperators = self.estimate_expected_cooperators(history)
        if expected_cooperators < self.game_description.m:
            self.threshold = min(self.threshold + 1, self.game_description.n_players - 1)
        elif expected_cooperators >= self.game_description.m:
            self.threshold = max(self.threshold - 1, 0)
        if expected_cooperators < self.threshold:
            return Action.C
        if history.my_payoffs.size > 0 and history.my_payoffs[-1] > 2 and (not history.my_actions[-1]):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_410(BaseStrategy):
    """
    Adaptive Cooperation (AC) strategy for the Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective well-being,
    making it robust against various opponent behaviors. It encourages cooperation 
    while protecting against exploitation, making it a competitive strategy in the tournament.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperated_last_round = False
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        self.prev_cooperators = sum(prev_opponent_actions) + self.cooperated_last_round
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.prev_cooperators >= self.game_description.m and (not self.cooperated_last_round):
            return Action.C
        elif self.prev_cooperators < self.game_description.m and self.cooperated_last_round:
            return Action.D
        else:
            return Action(self.cooperated_last_round)
        self.cooperated_last_round = action == Action.C

class Strategy_COLLECTIVE_411(BaseStrategy):
    """
    This strategy balances individual self-interest with collective risk aversion,
    adapting to the behavior of other players over time.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n_players = game_description.n_players
        self.num_consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(opponent_actions)
        if num_cooperators < self.m:
            self.num_consecutive_defections += 1
        else:
            self.num_consecutive_defections = 0
        if self.num_consecutive_defections >= 2 or (num_cooperators == self.m - 1 and random.random() < 0.5):
            return Action.C
        if num_cooperators >= self.m:
            return Action.C
        elif sum(~opponent_actions) > self.n_players / 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and num_cooperators < self.m:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_413(BaseStrategy):
    """
    A hybrid strategy balancing individual self-interest with collective well-being,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players == 2 or self.game_description.n_rounds == 2:
            if self.game_description.n_rounds != 2 or (self.game_description.k > 2 and state.round_number == 1):
                return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if m_prev >= self.game_description.m:
            return Action.C
        p_defect = 1 - m_prev / self.game_description.m
        return Action.D if random.random() < p_defect else Action.C

    def last_round_call(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_players == 2:
            return Action.C
        total_cooperators = sum(sum(history.opponent_actions[:state.round_number - 1, :]) + history.my_actions[:state.round_number - 1])
        if total_cooperators >= self.game_description.m * (self.game_description.n_rounds - 1):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_414(BaseStrategy):
    """
    Balances individual self-interest with collective well-being by adaptively responding to others' actions and escalating cooperation when it's safe.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_cooperators = []
        self.round_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [Action.C.value])
        prev_round_payoff = history.my_payoffs[-1]
        self.round_cooperators.append(prev_round_cooperators)
        self.round_payoffs.append(prev_round_payoff)
        if state.round_number == n_players - 1:
            return Action.D
        avg_prev_payoffs = np.mean(self.round_payoffs[-(n_players // 2):])
        if prev_round_cooperators >= m:
            action = Action.C
        else:
            action = Action.D
        collective_payoff = (prev_round_cooperators - m + 1) * k
        if collective_payoff > avg_prev_payoffs and action == Action.C and (n_players > prev_round_cooperators):
            action = Action.C
        return action

class Strategy_COLLECTIVE_415(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters by balancing individual self-interest with collective risk aversion.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        n_players = self.game_description.n_players
        reward_received = n_cooperators >= self.game_description.m
        if n_cooperators <= self.game_description.m:
            return Action.C
        elif reward_received:
            p_coop = n_cooperators / n_players
            return Action.C if random.random() < p_coop else Action.D
        elif not history.opponent_actions[-1, 0] and (not reward_received):
            return Action.D if random.random() < 0.8 else Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif n_cooperators == n_players // 2:
            return Action.C if random.random() < 0.5 else Action.D
        else:
            return Action.C if random.random() < 0.5 else Action.D

class Strategy_COLLECTIVE_416(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5
        self.q = 0.5
        self.threshold_met_streak = 0
        self.threshold_not_met_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        num_cooperators_prev_round = sum(prev_round_actions)
        prev_threshold_met = num_cooperators_prev_round >= self.game_description.m
        if prev_threshold_met:
            self.p = num_cooperators_prev_round / self.game_description.n_players
            action = Action.C if random.random() < self.p else Action.D
        else:
            num_defectors_prev_round = self.game_description.n_players - num_cooperators_prev_round
            self.q = 1 - num_defectors_prev_round / self.game_description.n_players
            action = Action.D if random.random() < self.q else Action.C
        if prev_threshold_met:
            self.threshold_met_streak += 1
            if self.threshold_met_streak >= 3:
                self.p = min(1, self.p + 0.1)
        else:
            self.threshold_not_met_streak += 1
            if self.threshold_not_met_streak >= 3:
                self.q = max(0, self.q - 0.1)
        return action

class Strategy_COLLECTIVE_417(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    
    This strategy aims to balance individual self-interest with collective benefits while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
        m = self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1:
            any_prev_coop = np.any([np.all(history.my_actions[:t] == True) and np.sum(history.opponent_actions[t, :]) >= m - 1 for t in range(state.round_number)])
            return Action.C if any_prev_coop else Action.D
        elif prev_round_cooperators >= m:
            return Action.C
        elif prev_round_cooperators < m - 1:
            n = self.game_description.n_players
            defect_prob = (m - 1 - prev_round_cooperators) / (n - prev_round_cooperators)
            return Action.D if random.random() < defect_prob else Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_418(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy for the Collective Risk Dilemma.

    This strategy balances individual self-interest with collective cooperation,
    adapting to the game's dynamics and opponent behaviors while ensuring a robust
    and collective approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_t = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number <= 3:
            return random.choice([Action.C, Action.D])
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round < self.game_description.m:
            action = Action.D
        else:
            self.p_t = min(1, self.game_description.k / (self.game_description.k + 1))
            action = random.choices([Action.C, Action.D], weights=[self.p_t, 1 - self.p_t])[0]
        if num_cooperators_prev_round >= self.game_description.m:
            self.p_t += 0.1
        else:
            self.p_t -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators_prev_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        return action

class Strategy_COLLECTIVE_419(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy adapts to various opponent behaviors 
    and aligns with the collective mindset in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_cooperators < self.game_description.m and (not self.punish):
            if state.round_number > 2 and sum(history.opponent_actions[-2, :]) + int(history.my_actions[-2]) < self.game_description.m:
                self.punish = True
        if self.punish and prev_cooperators >= self.game_description.m:
            self.punish = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if prev_cooperators < self.game_description.m and (not self.punish):
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_420(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.ACR = None
        self.cooperation_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        elif state.round_number <= self.game_description.n_rounds // 4:
            if history is not None and len(history.my_actions) > 0:
                self.ACR = np.mean(np.append(history.my_actions, history.opponent_actions.sum(axis=1)))
            return Action.C
        elif self.ACR is not None:
            if self.cooperation_threshold is None:
                self.cooperation_threshold = self.game_description.m - 1 if self.ACR >= 0.5 else self.game_description.m + 1
            cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            return Action.C if cooperators >= self.cooperation_threshold or state.round_number == self.game_description.n_rounds - 1 else Action.D
        return Action.D

class Strategy_COLLECTIVE_421(BaseStrategy):
    """
    The Adaptive Collective Risk strategy encourages early cooperation and adapts the cooperation threshold based on previous rounds' outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = 0
        self.previous_cooperation_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        opponent_cooperations = history.opponent_actions[-1, :].sum()
        total_cooperations = int(history.my_actions[-1]) + opponent_cooperations
        self.previous_cooperation_count = total_cooperations
        if total_cooperations < self.game_description.m:
            self.T = self.game_description.m - 1
        else:
            self.T = max(self.game_description.m - 1, total_cooperations * (self.game_description.k / (self.game_description.k + 1)))
        if state.round_number == self.game_description.n_rounds and opponent_cooperations < self.game_description.m:
            return Action.C
        if opponent_cooperations <= self.T or (opponent_cooperations == self.T and random.random() < 0.5):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_422(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    This strategy balances individual self-interest with collective cooperation, 
    adapting to opponents' behaviors and prioritizing cooperation when collective benefits are high.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n_rounds = self.game_description.n_rounds
        r = state.round_number
        if r <= math.floor(n_rounds / 3):
            return Action.C if r % 2 == 1 else Action.D
        prev_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        m = self.game_description.m
        n_players = self.game_description.n_players
        p_defect = (m - prev_coops) / (n_players - 1)
        p_defect = min(max(p_defect, 0), 1)
        if random.random() < p_defect:
            return Action.D
        total_coop_payoff = sum(history.my_payoffs[-math.floor(n_rounds / 3):])
        expected_defect_payoff = (1 + self.game_description.k * (m - 1) / n_players) * (n_players - prev_coops)
        if r > math.floor(2 * n_rounds / 3):
            return Action.C if total_coop_payoff > expected_defect_payoff else Action.D
        return Action.C

class Strategy_COLLECTIVE_423(BaseStrategy):
    """
    Adaptive Collective Threshold strategy for the Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective benefits while being robust to various opponent behaviors.
    It adapts to the number of cooperators in previous rounds and encourages cooperation when the threshold is within reach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.m_threshold <= self.n_players / 2 else Action.D
        x = sum(history.opponent_actions[-1, :] | np.array([True]))
        θ = max(self.m_threshold - 1, x)
        π_c = 1 + self.k_reward_factor * (x >= self.m_threshold)
        π_d = 1
        if π_c >= π_d + self.k_reward_factor * (self.m_threshold - θ) / self.n_players:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1 and x >= self.m_threshold - 1:
            return Action.C
        elif np.sum(history.opponent_actions[-1, :]) == 0:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_424(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual and collective interests by adapting to the game's dynamics, 
    rewarding cooperation when beneficial, and punishing deviations from cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_t = 0
        self.total_payoff_i = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= self.game_description.m:
            return Action.C
        if history is not None:
            self.cooperators_t = sum(history.opponent_actions[-1, :] == True)
            self.total_payoff_i += np.mean(history.my_payoffs)
        if self.cooperators_t >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        if self.game_description.k > 1.5 * (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players:
            cooperation_threshold = self.game_description.m - 1
        else:
            cooperation_threshold = self.game_description.m
        if history is not None and len(history.my_payoffs) > 0:
            avg_opponent_payoff = np.mean(np.sum(history.opponent_payoffs, axis=0))
            if history.my_payoffs[-1] < avg_opponent_payoff:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        return action

class Strategy_COLLECTIVE_425(BaseStrategy):
    """
    A strategy that adapts to various opponent behaviors and focuses on achieving the collective goal.
    It initially cooperates, then adjusts its cooperation probability based on the number of previous round's cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if cooperators >= self.game_description.m:
            p = cooperators / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        else:
            q = 0.5
            return Action.C if random.random() < q else Action.D

class Strategy_COLLECTIVE_426(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    This strategy balances individual self-interest with the need for cooperation to achieve a higher reward.
    It adapts to the game's history and opponent behaviors, ensuring robustness against various strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        coop_rate = np.mean(history.opponent_actions[-1, :])
        theta = (self.game_description.m - 1) / self.game_description.n_players
        if math.isclose(coop_rate, theta):
            return Action.C if random.random() < 0.5 else Action.D
        if coop_rate >= theta:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_427(BaseStrategy):
    """
    The Adaptive Cooperation strategy aims to balance individual self-interest with collective welfare by adapting to the evolving cooperation dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.prev_cooperators = None
        self.prev_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        self.prev_cooperators = np.sum(prev_round_opponent_actions) + (history.my_actions[-1] == Action.C)
        self.prev_payoff = history.my_payoffs[-1]
        avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        if self.prev_payoff > avg_payoff and self.prev_cooperators >= self.game_description.m:
            self.cooperate_prob -= 0.1
        if self.prev_cooperators >= self.game_description.m / 2 and self.prev_payoff < avg_payoff:
            return Action.C
        elif self.prev_cooperators < self.game_description.m and self.prev_payoff > avg_payoff:
            return Action.D
        if self.prev_cooperators >= self.game_description.m:
            self.cooperate_prob += 0.1
        elif self.prev_cooperators < self.game_description.m:
            self.cooperate_prob -= 0.1
        self.cooperate_prob = max(0.1, min(self.cooperate_prob, 0.9))
        if random.random() < self.cooperate_prob:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.prev_cooperators >= self.game_description.m and self.prev_payoff < avg_payoff:
                return Action.C

class Strategy_COLLECTIVE_428(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    This strategy aims to balance individual self-interest with collective cooperation, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.p = game_description.k / (game_description.k + 1)
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        M = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if M >= self.game_description.m:
            self.p = min(0.9, self.p * 1.1)
        else:
            self.p = max(0.1, self.p * 0.9)
        if M >= self.game_description.m or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif random.random() < self.p:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_429(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n = game_description.n_players
        self.p = self.m / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        prev_threshold_met = sum(history.opponent_actions[-1, :] + [True]) >= self.m
        if prev_threshold_met:
            self.p = min(1.0, self.p + 0.1)
        else:
            self.p = max(0.0, self.p - 0.1)
        return Action.C if random.random() < self.p else Action.D

class Strategy_COLLECTIVE_430(BaseStrategy):
    """
    The ACT strategy balances individual self-interest with collective benefit,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        prev_avg_payoff = np.mean(history.opponent_payoffs[-1, :] + [history.my_payoffs[-1]])
        if self.punishment_phase:
            self.punishment_phase = False
            return Action.D
        if prev_cooperators >= self.game_description.m and prev_avg_payoff > 1:
            return Action.C
        p = 0.1
        if random.random() < p:
            return Action.C if random.random() < 0.5 else Action.D
        if prev_cooperators < self.game_description.m and history.my_actions[-1] == False:
            self.punishment_phase = True
        return Action.C

class Strategy_COLLECTIVE_431(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.prev_round_cooperators = None
        self.prev_round_defectors = None
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        self.prev_round_cooperators = sum(prev_round_actions) + 1
        self.prev_round_defectors = self.game_description.n_players - self.prev_round_cooperators
        if self.prev_round_cooperators >= self.game_description.m:
            return Action.C
        else:
            p = 1 - self.prev_round_defectors / self.game_description.n_players
            return Action.C if random.random() < p else Action.D

class Strategy_COLLECTIVE_433(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation (AC)
    
    Encourages collective cooperation by initially cooperating, 
    then responding to the previous round's outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        cooperators = sum(prev_round)
        threshold_met = cooperators >= self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if threshold_met:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_434(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Reciprocity (ATR)
    
    ATR aligns with the collective mindset by encouraging initial cooperation, 
    reciprocating cooperation when others are cooperating, punishing defection 
    when others are not cooperating, and adapting to changes in opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.epsilon = 0.05
        self.delta = 0.1
        self.cooperation_rate_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        num_cooperators_prev_round = sum(opponent_actions_prev_round)
        cooperation_rate_prev = (num_cooperators_prev_round + 1) / self.game_description.n_players
        self.cooperation_rate_prev = cooperation_rate_prev
        if cooperation_rate_prev >= self.game_description.m / self.game_description.n_players:
            cooperate_prob = 1 - self.epsilon
        else:
            cooperate_prob = self.delta
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperators_prev_rounds = sum(history.my_actions) + np.sum(history.opponent_actions, axis=0)
            if np.any(total_cooperators_prev_rounds >= self.game_description.m):
                return Action.C
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_435(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.r_c = 0.5
        self.p = self.game_description.m / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        cooperators = sum(history.opponent_actions[-1, :]) + 1
        threshold_met = cooperators >= self.game_description.m
        if threshold_met:
            self.p = self.game_description.k / (self.game_description.k + 1)
        else:
            self.p = 1 - self.game_description.m / self.game_description.n_players
        prev_cooperation_rate = sum(history.my_actions) / state.round_number if history.my_actions.size > 0 else 0.5
        current_cooperation_rate = cooperators / self.game_description.n_players
        self.r_c += (current_cooperation_rate - prev_cooperation_rate) * 0.05
        if self.r_c > self.game_description.m / self.game_description.n_players:
            self.p = min(self.p + 0.05, 1)
        elif self.r_c < self.game_description.m / self.game_description.n_players:
            self.p = max(self.p - 0.05, 0)
        return Action.C if random.random() < self.p else Action.D

class Strategy_COLLECTIVE_436(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy balances individual self-interest with collective cooperation.
    It adapts to the game's history and parameters, prioritizing cooperation while allowing for individual self-protection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        total_cooperations = np.sum(history.my_actions[:-1]) + np.sum(np.any(history.opponent_actions[:-1, :], axis=0))
        previous_cooperators = np.count_nonzero(history.opponent_actions[-1, :])
        if state.round_number < self.game_description.n_rounds - 2:
            cooperation_rate = total_cooperations / (state.round_number * self.game_description.m)
            if cooperation_rate > 0.5:
                return Action.C if self.game_description.k >= 1 else Action.D
            elif cooperation_rate < 0.5 and previous_cooperators >= self.game_description.m - 1:
                return Action.C
        if history is not None and np.count_nonzero(history.my_actions[-2:]) + np.count_nonzero(np.any(history.opponent_actions[-2:, :], axis=0)) < self.game_description.m:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if previous_cooperators >= self.game_description.m - 1 else Action.D
        return Action.D

class Strategy_COLLECTIVE_437(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy balances individual self-interest with collective welfare,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        if self.k > 0:
            self.p = self.k / (self.k + 1)
        else:
            self.p = 0.0
        self.q = 1 - self.m / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_threshold_met = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True) >= self.m
        if prev_round_threshold_met:
            return Action.C if random.random() < self.p else Action.D
        else:
            return Action.D if random.random() < self.q else Action.C
        consecutive_defections = sum((1 for action in history.my_actions[-self.n_rounds // 2:] if not action))
        if consecutive_defections > self.n_rounds // 2:
            return Action.C if random.random() < 1 - self.q else Action.D
        if state.round_number == self.n_rounds - 1:
            prev_round_threshold_met = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True) >= self.m
            return Action.C if prev_round_threshold_met else Action.D
        if self.m == self.n_players:
            return Action.C
        elif self.k == 1.0:
            return Action.D

class Strategy_COLLECTIVE_438(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return Action.C
        previous_opponent_actions = history.opponent_actions[-1, :]
        cooperators = np.sum(previous_opponent_actions) + (history.my_actions[-1] == True)
        defectors = n_players - cooperators
        if state.round_number < 2:
            return Action.C
        elif cooperators >= m / 2:
            return Action.C
        elif defectors >= n_players - m:
            return Action.D
        elif state.round_number > r // 2:
            avg_payoff = np.mean(history.my_payoffs)
            if avg_payoff < (1 + k) / 2:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C
        if state.round_number >= r - 2:
            total_payoff = np.sum(history.my_payoffs)
            if total_payoff >= (1 + k) / 2 * (state.round_number + 1):
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_439(BaseStrategy):
    """
    Adaptive Cooperation with Reward-Sensitive Learning (ACRSL) strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperators = 0.0
        self.p_defect = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + 1
        self.avg_cooperators = (1 - 0.1) * self.avg_cooperators + 0.1 * num_cooperators
        if num_cooperators < self.game_description.m:
            return Action.D if random.random() < self.p_defect else Action.C
        num_defectors = self.game_description.n_players - num_cooperators
        p_cooperate = 1 - 1 / self.game_description.k * num_defectors
        if all(history.my_actions[-2:]) and all(history.opponent_actions[-2:, :].flatten()):
            p_cooperate += 0.2
        return Action.C if random.random() < p_cooperate else Action.D

    def last_round(self):
        return Action.D

class Strategy_COLLECTIVE_440(BaseStrategy):
    """
    This strategy prioritizes cooperation and adaptability in the Collective Risk Dilemma game.
    It encourages initial cooperation, responds to changes in cooperation levels, and prioritizes 
    collective benefits while minimizing individual risk of exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if previous_cooperators >= self.game_description.m:
            return Action.C
        if state.round_number > 1:
            previous_previous_cooperators = sum(history.opponent_actions[-2, :]) + int(history.my_actions[-2])
            if previous_cooperators > previous_previous_cooperators:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff_C = sum(history.my_payoffs[history.my_actions]) + self.game_description.k * history.my_actions.sum()
            total_payoff_D = sum(history.my_payoffs[~history.my_actions])
            return Action.C if total_payoff_C >= total_payoff_D else Action.D
        return Action.D

class Strategy_COLLECTIVE_442(BaseStrategy):
    """
    This strategy aims to balance individual self-interest with collective well-being 
    by adapting to the evolving dynamics of cooperation and defection within the group.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.m == 1:
            return Action.C
        if self.game_description.n_players == 2 and state.round_number == self.game_description.n_rounds - 1:
            opponent_cooperated_last_round = history.opponent_actions[-1, 0]
            return Action.D if opponent_cooperated_last_round else Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [Action.C == self._get_my_action(state.round_number - 1, history)])
        if prev_cooperators >= self.game_description.m:
            p_C = (self.game_description.k - 1) / self.game_description.k
            return Action.C if random.random() < p_C else Action.D
        prev_defectors = self.game_description.n_players - prev_cooperators
        prev_prev_cooperators = sum(history.opponent_actions[-2, :] + [Action.C == self._get_my_action(state.round_number - 2, history)])
        prev_prev_defectors = self.game_description.n_players - prev_prev_cooperators
        increasing_cooperation = prev_cooperators > prev_prev_cooperators or abs(prev_cooperators - prev_defectors) < abs(prev_prev_cooperators - prev_prev_defectors)
        if increasing_cooperation:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

    def _get_my_action(self, round_number: int, history: PlayerHistory) -> Action:
        for i, action in enumerate(history.my_actions[:round_number + 1]):
            if action == True:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_443(BaseStrategy):
    """
    The Adaptive Cooperation strategy balances individual self-interest with collective well-being 
    in the Collective Risk Dilemma game by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return Action.C
        t = state.round_number
        if t < r / 2:
            prev_coops = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
            if prev_coops >= m:
                return Action.C
            else:
                return Action.D
        avg_payoff_window_size = min(5, t)
        my_avg_payoff = np.mean(history.my_payoffs[max(-avg_payoff_window_size + 1, -len(history.my_payoffs)):])
        if my_avg_payoff > (k + 1) / 2:
            return Action.C
        else:
            return Action.D
        total_coops = sum(history.opponent_actions[:, :] | history.my_actions)
        if total_coops >= m * r / 2:
            return Action.C
        else:
            return Action.D
        if t == r - 1:
            if total_coops + 1 >= m * r / 2:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_444(BaseStrategy):
    """
    Adaptive Threshold Enforcement (ATE) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history else False)
        if m_prev >= self.game_description.m:
            return Action.C
        elif state.round_number > 1:
            prev_cooperators = sum(history.opponent_actions[-2, :]) + (history.my_actions[-2] if history else False)
            if m_prev > prev_cooperators:
                return Action.C
            elif m_prev < prev_cooperators:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_445(BaseStrategy):
    """
    A strategy for Collective Risk Dilemma games that adapts to the cooperation history of its opponents.
    
    It starts by cooperating with a probability proportional to the minimum number of cooperators needed, 
    then adjusts this probability based on whether the collective goal was met in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = game_description.m / game_description.n_players
        self.H_c = 0
        self.H_d = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if previous_round_cooperators >= self.game_description.m:
            self.H_c += 1
            cooperate = True
        else:
            self.H_d += 1
            cooperate = random.random() < self.cooperation_probability
        if self.H_c > self.H_d:
            self.cooperation_probability = min(1, self.cooperation_probability + 0.1)
        elif self.H_c < self.H_d:
            self.cooperation_probability = max(0, self.cooperation_probability - 0.1)
        if state.round_number == self.game_description.n_rounds - 1 and previous_round_cooperators >= self.game_description.m:
            cooperate = True
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_446(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_threshold = game_description.m
        self.k_reward = game_description.k
        self.p_C = (self.k_reward - 1) / self.k_reward if self.m_threshold <= self.n_players // 2 else 0
        self.p_D = (self.n_players - self.m_threshold + 1) / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.m_threshold <= self.n_players // 2 else Action.D
        prev_outcome = sum(history.opponent_actions[-1, :]) >= self.m_threshold and history.my_payoffs[-1] > 0
        if prev_outcome:
            self.p_C += 0.05
        else:
            self.p_C -= 0.05
        self.p_C = max(0, min(self.p_C, 1))
        return Action.C if random.random() < self.p_C else Action.D

class Strategy_COLLECTIVE_447(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy.

    This strategy balances individual self-interest with collective cooperation,
    adapting to the group's behavior while prioritizing the achievement and maintenance
    of the cooperative threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.last_round_cooperators is None:
            self.last_round_cooperators = sum(history.opponent_actions[-1, :])
        p_defect = 0.5 - (self.game_description.m - self.last_round_cooperators) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif self.game_description.n_players == 2:
            return Action.C
        if random.random() < p_defect:
            action = Action.D
        else:
            action = Action.C
        self.last_round_cooperators = sum(history.opponent_actions[-1, :]) + (action == Action.C)
        return action

class Strategy_COLLECTIVE_448(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma.
    Prioritizes cooperation when it is likely to yield the collective reward while adapting to changing opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.ccr = 0.5
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        self.ccr = (self.ccr * (self.game_description.n_players - 1) + history.my_actions[-1]) / self.game_description.n_players
        if self.ccr > self.threshold / self.game_description.n_players:
            self.threshold += 1
        else:
            self.threshold -= 1
        if self.ccr >= self.threshold / self.game_description.n_players:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        elif state.round_number > self.game_description.n_rounds // 2 and np.sum(history.my_actions) > self.game_description.n_rounds // 2:
            return Action.C
        else:
            return Action.D if random.random() < 0.5 else Action.C

class Strategy_COLLECTIVE_449(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective cooperation while adapting to changing opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.c_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if history is not None:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            total_players = self.game_description.n_players - 1
            if total_players > 0:
                self.c_prev = (opponent_cooperators + int(history.my_actions[-1])) / self.game_description.n_players
        tau = max(self.game_description.m / self.game_description.n_players, self.c_prev) if self.c_prev is not None else 0.5
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D if tau < self.game_description.m / self.game_description.n_players else Action.C
        return Action.C if random.random() < tau else Action.D

class Strategy_COLLECTIVE_450(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective cooperation to maximize overall payoffs.
    It adapts to the game's history and opponent behaviors, making it robust and competitive.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        p = (self.game_description.m - prev_cooperators) / self.game_description.n_players
        if p < 0:
            p = 0
        if random.random() < p:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_451(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters. It aims to foster an environment
    where cooperation can emerge and thrive, even among diverse opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.n = game_description.n_players
        self.payoffs = []
        self.cooperation_threshold = self.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.cooperation_threshold:
            p = prev_cooperators / self.n
            return Action.C if random.random() < p else Action.D
        else:
            p = 1 - prev_cooperators / self.m
            return Action.D if random.random() < p else Action.C
        if state.round_number % min(5, self.r // 2) == 0:
            avg_payoff = np.mean(self.payoffs[-min(5, len(self.payoffs)):])
            if avg_payoff < (self.k + 1) / 2 and self.cooperation_threshold < self.n - 1:
                self.cooperation_threshold += 1
            elif avg_payoff > self.k and self.cooperation_threshold > self.m:
                self.cooperation_threshold -= 1
        if state.round_number == self.r - 1:
            return Action.C if sum(self.payoffs) < self.r * self.k / 2 else Action.D
        return Action.C if random.random() < 0.51 else Action.D

class Strategy_COLLECTIVE_452(BaseStrategy):
    """
    Adaptive Cooperation strategy for Collective Risk Dilemma.
    Balances individual self-interest with collective risk management,
    adapting to various opponent behaviors and maximizing overall payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponents_cooperated = sum(history.opponent_actions[-1, :])
        if opponents_cooperated >= self.threshold:
            self.punishment_phase = False
            return Action.C
        if not self.punishment_phase and any((not x for x in history.my_actions)) and (opponents_cooperated > 0):
            self.punishment_phase = True
        if self.punishment_phase:
            if history.my_actions[-1]:
                return Action.D
            else:
                return Action.C
        if state.round_number % 3 == 0 and state.round_number > 2:
            last_three_rounds = history.opponent_actions[-3:, :]
            cooperators_last_three_rounds = np.sum(last_three_rounds, axis=1)
            if sum(cooperators_last_three_rounds >= self.threshold) >= 2:
                self.threshold += 1
            else:
                self.threshold = max(self.threshold - 1, 1)
        return Action.D

class Strategy_COLLECTIVE_453(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective cooperation, 
    adapting to varying opponent behaviors while prioritizing a collaborative atmosphere.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate = True
        self.consecutive_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_threshold_met = sum(history.opponent_actions[-1, :]) >= self.game_description.m
        if previous_threshold_met:
            self.cooperate = True
        elif sum(history.opponent_actions[-1, :]) > 0:
            self.cooperate = False
        else:
            self.cooperate = random.random() < 0.5
        if self.consecutive_coop > self.game_description.m / 2:
            self.cooperate = False
            self.consecutive_coop = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.consecutive_coop += 1 if self.cooperate else 0
        return Action.C if self.cooperate else Action.D

class Strategy_COLLECTIVE_454(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy.
    
    This strategy balances cooperation and defection based on past performance, 
    adapting to the game's dynamics by introducing an adaptive threshold (θ) 
    and adding a small randomization factor (ε).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.adaptive_threshold = None
        self.previous_average_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        reward_factor = self.game_description.k
        delta_theta = 0.05
        epsilon_delta = 0.1
        if state.round_number == 0:
            initial_cooperation_probability = m_cooperators_needed / n_players
            return Action.C if random.random() < initial_cooperation_probability else Action.D
        previous_round_actions = history.opponent_actions[-1, :]
        opponent_cooperations = sum(previous_round_actions)
        if self.adaptive_threshold is None:
            self.adaptive_threshold = m_cooperators_needed / n_players
        average_payoff = np.mean(history.my_payoffs)
        self.previous_average_payoff = average_payoff
        if average_payoff >= 1 + reward_factor / 2:
            updated_theta = max(0, min(1, self.adaptive_threshold - delta_theta))
        else:
            updated_theta = max(0, min(1, self.adaptive_threshold + delta_theta))
        self.adaptive_threshold = updated_theta
        cooperation_probability = self.adaptive_threshold
        epsilon = random.uniform(-epsilon_delta, epsilon_delta)
        final_cooperation_probability = max(0, min(1, cooperation_probability + epsilon))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < final_cooperation_probability else Action.D

class Strategy_COLLECTIVE_455(BaseStrategy):
    """
    The ACRD strategy aims to balance individual self-interest with collective cooperation,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if prev_coop_count >= self.game_description.m:
            return Action.C
        elif prev_coop_count > 0:
            return Action.D
        if prev_coop_count >= self.game_description.m:
            self.cooperation_threshold += 1
        else:
            self.cooperation_threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_coop_count == 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_456(BaseStrategy):
    """
    The Adaptive Cooperation strategy adapts to the behavior of other players and the game's history,
    balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :] | [False])
        self.p = 0.5 + num_cooperators_prev_round / self.game_description.n_players * 0.3
        if random.random() < self.p:
            action = Action.C
        else:
            action = Action.D
        if num_cooperators_prev_round >= self.game_description.m:
            self.p = min(0.9, self.p + 0.1)
        else:
            self.p = max(0.1, self.p - 0.1)
        if state.round_number == self.game_description.n_rounds - 1 and sum((action == Action.C for action in history.my_actions)) >= self.game_description.n_rounds / 2:
            return Action.C
        return action

class Strategy_COLLECTIVE_457(BaseStrategy):
    """
    The Adaptive Collective Risk (ACR) strategy adapts to the game's history and parameters,
    ensuring robustness against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.epsilon = 0.05
        self.gamma = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return self.initial_cooperation()
        avg_coop = self.calculate_average_cooperators(history)
        previous_action = self.get_previous_action(history)
        if avg_coop >= self.game_description.m:
            return self.cooperate_with_probability(1 - self.epsilon, history)
        elif avg_coop < self.game_description.m and (not previous_action):
            return self.defect_with_probability(1 - self.gamma, history)
        else:
            return self.adjust_contribution(avg_coop, history)

    def initial_cooperation(self) -> Action:
        probability = self.game_description.m / self.game_description.n_players
        return Action.C if random.random() < probability else Action.D

    def calculate_average_cooperators(self, history: PlayerHistory) -> float:
        opponent_actions = history.opponent_actions[:-1]
        cooperators = np.sum(opponent_actions, axis=1)
        avg_coop = np.mean(cooperators)
        return avg_coop

    def get_previous_action(self, history: PlayerHistory) -> bool | None:
        if len(history.my_actions) > 0:
            return not history.my_actions[-1]
        else:
            return None

    def cooperate_with_probability(self, probability: float, history: PlayerHistory) -> Action:
        return Action.C if random.random() < probability else Action.D

    def defect_with_probability(self, probability: float, history: PlayerHistory) -> Action:
        return Action.D if random.random() < probability else Action.C

    def adjust_contribution(self, avg_coop: float, history: PlayerHistory) -> Action:
        previous_payoff = history.my_payoffs[-1]
        k = self.game_description.k
        delta_p = (k - previous_payoff) / 2
        new_probability = max(0.5, min(1 - delta_p, 1))
        return Action.C if random.random() < new_probability else Action.D

class Strategy_COLLECTIVE_458(BaseStrategy):
    """
    A collective strategy called "Adaptive Threshold Cooperation" (ATC) that balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m_t_1 = sum(history.opponent_actions[-1, :])
        cr = m_t_1 / self.game_description.n_players
        if cr >= 0.5 and (self.game_description.n_rounds - state.round_number) % 2 == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif m_t_1 > 0 and history.my_payoffs[-1] > np.mean(history.opponent_payoffs, axis=1)[-1]:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_459(BaseStrategy):
    """
    A strategy that aims to balance individual self-interest with collective well-being 
    by adapting to the game's history and parameters in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 1.0
        self.successful_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_outcome = sum(history.my_actions[-1:] + history.opponent_actions[-1, :])
        if previous_round_outcome >= self.game_description.m:
            action = Action.C
        else:
            self.cooperation_probability = (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players
            if random.random() < self.cooperation_probability:
                action = Action.C
            else:
                action = Action.D
        if previous_round_outcome >= self.game_description.m:
            self.successful_rounds += 1
        if self.successful_rounds / (state.round_number + 1) > 0.5 and self.cooperation_probability < 1.0:
            self.cooperation_probability = min(self.cooperation_probability + 0.1, 1.0)
        elif self.successful_rounds / (state.round_number + 1) < 0.2 and self.cooperation_probability > 0.0:
            self.cooperation_probability = max(self.cooperation_probability - 0.1, 0.0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_460(BaseStrategy):
    """
    A collective strategy called "Adaptive Threshold Cooperation" (ATC) 
    that balances individual self-interest with the need for collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        reward_factor = self.game_description.k
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        prev_cooperators = sum(prev_round_opponent_actions)
        if prev_cooperators < m_cooperators_needed:
            p_c = m_cooperators_needed / n_players
        else:
            p_c = 1 - (reward_factor - 1) / reward_factor
        avg_defector_payoff = np.mean(history.opponent_payoffs[-1, ~prev_round_opponent_actions])
        if history.my_payoffs[-1] < avg_defector_payoff and prev_cooperators < m_cooperators_needed:
            p_d = 0.5
        else:
            p_d = 0
        avg_cooperator_payoff = np.mean(history.opponent_payoffs[-1, prev_round_opponent_actions])
        if prev_cooperators >= m_cooperators_needed and history.my_payoffs[-1] > avg_cooperator_payoff:
            p_c = 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if prev_cooperators >= m_cooperators_needed else Action.D
        random_num = random.random()
        if random_num < p_c:
            return Action.C
        elif random_num < p_c + p_d:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_461(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective cooperation.
    It adapts to the game's history and parameters, prioritizing cooperation while minimizing losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        last_round = state.round_number == self.game_description.n_rounds - 1
        if n_players == m:
            return Action.C
        elif last_round and prev_round_coops >= m:
            return Action.C
        elif prev_round_coops < m and (not history.my_actions[-1]):
            return Action.D
        if prev_round_coops >= m:
            return Action.C
        elif prev_round_coops > 0:
            coop_prob = prev_round_coops / n_players
            return Action.C if random.random() < coop_prob else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_462(BaseStrategy):
    """
    Adaptive Collective Risk Averter (ACRA) strategy.

    This strategy balances individual self-interest with collective risk aversion,
    adapting to the behavior of other players over time.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploration_rate = 0.6
        self.early_rounds_threshold = math.ceil(0.1 * game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        my_payoff = history.my_payoffs[-1]
        max_payoff = self.game_description.k + (1 - int(my_payoff))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif num_cooperators < self.game_description.m:
            return Action.C
        elif num_cooperators >= self.game_description.m and my_payoff != max_payoff:
            return Action.D
        elif state.round_number < self.early_rounds_threshold:
            return Action.C if random.random() < self.exploration_rate else Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_464(BaseStrategy):
    """
    A strategy that combines cooperation and defection to achieve collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.C_rate = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + int(bool(self.game_description.n_players - history.my_actions[-1]))
        if state.round_number < min(self.game_description.m - 1, self.game_description.n_rounds // 2):
            return Action.C
        elif prev_coop_count < self.game_description.m - 1:
            prob_defect = max(0.0, min((self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players, 1.0))
            return Action.D if random.random() < prob_defect else Action.C
        elif prev_coop_count >= self.game_description.m and sum(history.my_payoffs) - history.my_payoffs[-1] + self.game_description.k > 0:
            self.C_rate = min(self.C_rate + 0.1, 1)
            return Action.C if random.random() < self.C_rate else Action.D
        elif prev_coop_count >= self.game_description.m - 1:
            return Action.C if random.random() < self.C_rate else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if prev_coop_count >= self.game_description.m - 1 else Action.D

class Strategy_COLLECTIVE_465(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective cooperation, 
    adapting to the game's history and parameters. It encourages cooperation when 
    it is likely to lead to a better outcome for all players while avoiding exploitation 
    by free-riding opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met_count = 0
        self.adaptive_threshold = max(2, int(game_description.n_rounds / 5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_outcome = history.opponent_actions[-1]
        coop_count_prev = sum(prev_round_outcome)
        coop_count_prev_prev = sum(history.opponent_actions[-2]) if state.round_number > 1 else 0
        threshold_met = coop_count_prev >= self.game_description.m
        if threshold_met:
            avg_payoff = np.mean(history.my_payoffs[-1] + self.game_description.k * threshold_met)
            action = Action.C if avg_payoff > 1 else Action.D
            self.threshold_met_count += 1
        else:
            action = Action.C if coop_count_prev >= coop_count_prev_prev else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C if threshold_met else Action.D
        elif self.game_description.n_players == 1:
            action = Action.D
        if self.threshold_met_count >= self.adaptive_threshold:
            action = Action.C
            self.threshold_met_count = 0
        return action

class Strategy_COLLECTIVE_466(BaseStrategy):
    """
    The Adaptive Cooperative Threshold strategy encourages cooperation by adapting to past cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = None
        self.delta = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        cooperators_t = sum((1 for action in history.opponent_actions[-1, :] if action))
        if self.cooperation_threshold is None:
            self.cooperation_threshold = (self.game_description.m - 1) / self.game_description.n_players
        else:
            self.cooperation_threshold += cooperators_t / self.game_description.n_players
        reward_received = cooperators_t >= self.game_description.m
        if reward_received:
            self.cooperation_threshold += self.delta * self.game_description.k
        return Action.C if cooperators_t + 1 >= self.cooperation_threshold else Action.D

class Strategy_COLLECTIVE_467(BaseStrategy):
    """
    Adaptive Cooperation (AC) strategy for the Collective Risk Dilemma game.
    
    This strategy balances individual self-interest with collective well-being,
    adapting to the game's dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.explore_rounds = 2
        self.punish_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.explore_rounds:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_coop_count = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if state.round_number % 5 == 0 and state.round_number > 0:
            self.reassess_opponent_behavior(history)
            self.adjust_cooperation_strategy()
        if prev_coop_count >= self.game_description.m:
            return Action.C
        punish_factor = (self.game_description.m - prev_coop_count) / self.game_description.n_players
        return Action.D if random.random() < self.punish_prob * punish_factor else Action.C

    def reassess_opponent_behavior(self, history: PlayerHistory):
        opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        self.punish_prob *= (1 + np.mean(opponent_cooperation_rates)) / 2

    def adjust_cooperation_strategy(self):
        pass

class Strategy_COLLECTIVE_468(BaseStrategy):
    """
    A strategy that adapts to the cooperation level of opponents in a collective risk dilemma game.
    It encourages cooperation when the collective goal is achievable and punishes individual defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.c_prev = 0.0
        self.opponent_behavior = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
        else:
            if history is not None:
                opponent_actions = history.opponent_actions[-1, :]
                my_action = history.my_actions[-1]
                c_prev = np.mean(opponent_actions) + my_action
                if c_prev >= self.game_description.m:
                    for i, opponent in enumerate(opponent_actions):
                        if not opponent:
                            if i not in self.opponent_behavior:
                                self.opponent_behavior[i] = [0.5]
                            self.opponent_behavior[i].append(0.4)
                self.c_prev = c_prev
            if self.c_prev >= self.game_description.m:
                cooperate_prob = 1 - (1 - self.c_prev) / (self.game_description.n_players - self.game_description.m + 1)
            else:
                cooperate_prob = (self.game_description.m - (self.game_description.n_players - self.game_description.m)) / self.game_description.n_players
        for i, behavior in self.opponent_behavior.items():
            if len(behavior) > 1:
                avg_defection_prob = np.mean(behavior)
                cooperate_prob *= 1 - avg_defection_prob
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        action = random.random() < cooperate_prob
        return Action.C if action else Action.D

class Strategy_COLLECTIVE_469(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for Collective Risk Dilemma.

    This strategy prioritizes collective success while adapting to the behavior of others.
    By initially cooperating, we encourage others to do the same and establish a baseline level of cooperation.
    We then reciprocate cooperation if the collective threshold is met or if some players cooperate.
    If the threshold is consistently not met, we escalate our cooperation to motivate others to follow suit.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.escalation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_threshold_met = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1) >= self.game_description.m
        prev_cooperation_level = np.mean(np.concatenate((history.opponent_actions[-1, :], [history.my_actions[-1]])))
        if prev_threshold_met:
            return Action.C
        elif prev_cooperation_level > 0 and prev_cooperation_level < 1:
            defect_prob = 1 - self.game_description.k / (self.game_description.k + 1)
            if self.escalation_rounds >= 3:
                return Action.C
            else:
                self.escalation_rounds += 1
                return Action.D if random.random() < defect_prob else Action.C
        else:
            self.escalation_rounds = 0
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_470(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.cooperation_probability = 0.5

    def initial_exploration(self) -> float:
        if self.game_description.n_rounds == 1:
            return 0.5
        elif self.m_prev < self.game_description.m:
            return 0.2
        else:
            return 0.6

    def adaptive_cooperation(self) -> float:
        if self.m_prev < self.game_description.m:
            return 0.2
        elif self.m_prev == self.game_description.m:
            return 0.6
        else:
            return 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperation_probability = 0.5
            action = np.random.choice([Action.C, Action.D], p=[self.cooperation_probability, 1 - self.cooperation_probability])
            return action
        if history is not None:
            self.m_prev = sum(history.opponent_actions[-1, :] == Action.C)
        if state.round_number <= 3:
            self.cooperation_probability = self.initial_exploration()
        else:
            self.cooperation_probability = self.adaptive_cooperation()
        if state.round_number == self.game_description.n_rounds - 1:
            if self.m_prev < self.game_description.m:
                return Action.D
            else:
                return Action.C
        action = np.random.choice([Action.C, Action.D], p=[self.cooperation_probability, 1 - self.cooperation_probability])
        return action

class Strategy_COLLECTIVE_471(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Threshold (ACT)
    
    This strategy balances individual self-interest with collective well-being, 
    adapting to the game's history and parameters. The goal is to cooperate when 
    it benefits the group while minimizing personal risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_outcome = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        self.p = prev_round_outcome / self.game_description.n_players
        if random.random() < self.p:
            action = Action.C
        else:
            action = Action.D
        self.history.append(action.value)
        return action

class Strategy_COLLECTIVE_472(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy.
    
    This strategy adapts to the actions of other players and adjusts its behavior 
    based on whether the threshold of cooperation is met or not.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.observation_phase_completed = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if not self.observation_phase_completed:
            n_cooperators = sum(history.opponent_actions[-1, :])
            if n_cooperators >= self.game_description.m or state.round_number == math.ceil(self.game_description.n_rounds / 2):
                self.observation_phase_completed = True
                return Action.C
            action = Action.D
            next_round_action = Action.C if n_cooperators >= self.game_description.m - 1 else Action.D
        else:
            n_cooperators = sum(history.opponent_actions[-1, :])
            if state.round_number == self.game_description.n_rounds - 1:
                action = Action.C
            elif n_cooperators >= self.game_description.m:
                action = Action.C
            elif n_cooperators >= self.game_description.m - 1:
                action = Action.C
            else:
                action = Action.D
            if n_cooperators == self.game_description.m - 1 and random.random() < 0.5:
                action = Action.D
        return action

class Strategy_COLLECTIVE_473(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m
        self.previous_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.previous_cooperators = sum(history.opponent_actions[-1, :])
        if self.previous_cooperators >= self.cooperation_threshold:
            self.cooperation_threshold = max(self.game_description.m, self.previous_cooperators + 1)
        else:
            self.cooperation_threshold = self.game_description.m
        if state.round_number < self.game_description.n_rounds - 1:
            if self.previous_cooperators >= self.cooperation_threshold:
                return Action.C
            elif self.previous_cooperators < self.cooperation_threshold and history.my_actions[-1]:
                return Action.C
            else:
                return Action.D
        elif self.previous_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_474(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    This strategy initially cooperates to establish a cooperative tone, 
    then adapts its cooperation threshold based on historical performance.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m / game_description.n_players
        self.avg_payoff_window = 5
        self.avg_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        p_t = (prev_cooperators + 1) / self.game_description.n_players
        if state.round_number % self.avg_payoff_window == 0:
            avg_payoff = np.mean(self.avg_payoffs)
            expected_defect_payoff = 1 + self.game_description.k / 2
            if avg_payoff > expected_defect_payoff:
                self.threshold += 0.05
            else:
                self.threshold -= 0.05
        if p_t < self.game_description.m / self.game_description.n_players:
            self.threshold -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if p_t >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        my_payoff = history.my_payoffs[-1]
        self.avg_payoffs.append(my_payoff)
        if len(self.avg_payoffs) > self.avg_payoff_window:
            self.avg_payoffs.pop(0)
        return action

class Strategy_COLLECTIVE_475(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Risk Assessment.
    
    Balances individual self-interest with collective cooperation using risk-assessment approach 
    to adapt to changing opponent behaviors and game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m_threshold = game_description.m
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        ctr = (self.m_threshold - n_cooperators) / self.m_threshold
        if ctr <= 0.5:
            return Action.C
        elif ctr > 0.5 and state.round_number < self.n_rounds - 1:
            return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.C
        n_cooperators_prev = sum(history.opponent_actions[-2, :]) + history.my_actions[-2]
        if n_cooperators >= n_cooperators_prev * (2 / 3):
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_476(BaseStrategy):
    """
    Adaptive Threshold Tracker (ATT) strategy for the Collective Risk Dilemma game.
    This strategy focuses on adapting to the collective behavior and reinforcing cooperation when it is more likely to succeed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_count = 0
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.my_actions) > 0:
            self.cooperators_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.cooperators_count <= self.game_description.m - 1:
            self.threshold = self.game_description.m
        else:
            self.threshold = max(self.game_description.m, self.cooperators_count * (self.game_description.k / (self.game_description.k + 1)))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.cooperators_count <= self.threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_477(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy.
    
    This strategy prioritizes collective success over individual gain by adapting to the group's performance.
    It aims to create an environment where cooperation becomes the norm, making it robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.w = 5
        self.θ = 0.6
        self.streak_threshold = 3
        self.m = game_description.m
        self.success_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if m_t >= self.m:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 0:
            success_rate = self._update_success_rate(history)
            if success_rate is not None:
                if success_rate > self.θ:
                    self.m -= 1
                elif success_rate < self.θ:
                    self.m += 1
        streak = self._count_consecutive_actions(history)
        if streak > self.streak_threshold:
            if history.my_actions[-1] == Action.C:
                action = Action.C
            else:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

    def _update_success_rate(self, history: PlayerHistory) -> float | None:
        """Update success rate and store it for the moving window."""
        m_t = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        self.success_rates.append(m_t >= self.m)
        if len(self.success_rates) > self.w:
            self.success_rates.pop(0)
        return np.mean(self.success_rates) if self.success_rates else None

    def _count_consecutive_actions(self, history: PlayerHistory) -> int:
        """Count consecutive actions by this player."""
        action = history.my_actions[-1]
        count = 0
        for i in range(len(history.my_actions) - 2, -1, -1):
            if history.my_actions[i] == action:
                count += 1
            else:
                break
        return count + 1

class Strategy_COLLECTIVE_478(BaseStrategy):
    """
    This strategy adapts to the game's history and parameters by balancing individual self-interest with collective risk management.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.convergence_criterion = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        last_round_cooperators = sum(history.opponent_actions[-1, :] + [False])
        self.convergence_criterion = last_round_cooperators / self.game_description.m
        if self.convergence_criterion >= 0.5:
            return Action.C
        previous_payoff = history.my_payoffs[-1] + sum(history.opponent_payoffs[-1, :])
        if previous_payoff < self.game_description.k * (self.game_description.m - 1):
            if last_round_cooperators < self.game_description.m:
                return Action.D
            else:
                increase_cooperation = math.ceil(self.game_description.n_players * 0.1)
                if random.random() < increase_cooperation / self.game_description.n_players:
                    return Action.C
        return Action.D

    def edge_cases(self, state: GameState, history: None | PlayerHistory) -> list[Action]:
        if state.round_number == self.game_description.n_rounds - 1:
            previous_payoff = history.my_payoffs[-2] + sum(history.opponent_payoffs[-2, :])
            return [Action.C] if previous_payoff >= self.game_description.k * self.game_description.m else [Action.D]
        elif len(history.my_actions) > self.game_description.n_rounds // 4 and all(history.my_actions[-self.game_description.n_rounds // 4:-1]):
            return [Action.C] * (self.game_description.n_rounds - state.round_number)

class Strategy_COLLECTIVE_479(BaseStrategy):
    """
    This strategy encourages cooperation by initially cooperating and adapting to the group's behavior.
    It promotes a mutually beneficial outcome while balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m - 1
        self.cooperate_last_round = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :] == True)
        if self.cooperate_last_round and m_t >= self.game_description.m:
            return Action.C
        if not self.cooperate_last_round and m_t < self.game_description.m:
            self.threshold = max(self.game_description.m - 1, min(self.threshold + 0.1, self.game_description.n_players))
            return Action.D
        action = self.choose_action(self.cooperate_last_round, m_t)
        self.cooperate_last_round = action == Action.C
        return action

    def choose_action(self, cooperate_last_round: bool, m_t: int) -> Action:
        """
        Helper function to choose an action based on past behavior.
        """
        if not cooperate_last_round and m_t >= self.game_description.m:
            return Action.C
        elif cooperate_last_round and m_t < self.game_description.m:
            return Action.D
        else:
            return Action.C if cooperate_last_round else Action.D

class Strategy_COLLECTIVE_480(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if self.n == self.m else Action.C if random.random() < self.m / self.n else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if prev_round_cooperators >= self.m:
            cooperate_prob = max(0, 1 - (self.n - self.m) / self.n * (1 - self.k))
        elif prev_round_cooperators < math.ceil(self.m / 2):
            return Action.D
        else:
            cooperate_prob = min(1, 2 * self.m / self.n)
        if state.round_number == self.r - 1 and prev_round_cooperators >= math.ceil(self.m / 2):
            return Action.C
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_481(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_payoff_C = 0.0
        self.avg_payoff_D = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coops = sum((1 for action in history.opponent_actions[-1, :] if action))
        last_round_payoff_C = history.my_payoffs[-1] + self.game_description.k if history.my_actions[-1] else 0.0
        last_round_payoff_D = history.my_payoffs[-1]
        self.avg_payoff_C = (self.avg_payoff_C * state.round_number + last_round_payoff_C) / (state.round_number + 1)
        self.avg_payoff_D = (self.avg_payoff_D * state.round_number + last_round_payoff_D) / (state.round_number + 1)
        if prev_coops >= self.game_description.m:
            return Action.C
        p = (self.avg_payoff_C - self.avg_payoff_D + 1.0) / (2.0 * self.game_description.k)
        if random.random() < p:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_482(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev = sum(history.opponent_actions[-1, :]) + 1
        if num_cooperators_prev >= self.game_description.m / 2:
            return Action.C
        else:
            return Action.D if random.random() < self.p else Action.C

class Strategy_COLLECTIVE_483(BaseStrategy):
    """
    The Adaptive Collective Contribution (ACC) strategy adapts to various opponent behaviors 
    while balancing individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_c = 0.8
        self.p_d = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        my_last_action = history.my_actions[-1]
        total_cooperation = opponent_cooperations + (my_last_action == Action.C)
        if total_cooperation >= self.game_description.m:
            p_c = self.p_c
        else:
            p_c = 0.2 + (1 - total_cooperation / self.game_description.m) * 0.3
        if len(history.my_actions) % 5 == 0 and history.my_actions.size > 10:
            cooperation_rate = sum((1 for actions in history.opponent_actions[-10:] for action in actions if action)) / (self.game_description.n_players * 10)
            if cooperation_rate > 0.6:
                self.p_c = 0.9
                self.p_d = 0.1
            elif cooperation_rate < 0.4:
                self.p_c = 0.7
                self.p_d = 0.3
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if random.random() < p_c else Action.D

class Strategy_COLLECTIVE_484(BaseStrategy):
    """
    A robust and adaptive strategy that balances individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_threshold_met = False
        self.prev_num_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 2:
            return Action.C
        if self.game_description.n_rounds == 1:
            return Action.D
        prev_opponent_actions = history.opponent_actions[-1, :]
        self.prev_num_cooperators = sum(prev_opponent_actions) + 1
        self.prev_threshold_met = self.prev_num_cooperators >= self.game_description.m
        if self.prev_threshold_met:
            return Action.C if random.random() < 0.8 else Action.D
        elif self.prev_num_cooperators >= self.game_description.m / 2:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and self.prev_num_cooperators < self.game_description.m / 2:
            return Action.D

class Strategy_COLLECTIVE_485(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that balances individual self-interest with the need for collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 1:
            return Action.C
        if history is not None and len(history.my_actions) > 0:
            previous_round_outcome = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
            if previous_round_outcome >= self.game_description.m:
                self.consecutive_failures = 0
                return Action.C
            else:
                self.consecutive_failures += 1
        p_defect = (1 - self.game_description.k / (self.game_description.k + 1)) ** self.consecutive_failures if self.consecutive_failures > 0 else 0
        if state.round_number == self.game_description.n_rounds:
            expected_coop_payoff = self._expected_coop_payoff(history)
            expected_defect_payoff = self._expected_defect_payoff(history)
            return Action.C if expected_coop_payoff > expected_defect_payoff else Action.D
        return Action.D if random.random() < p_defect else Action.C

    def _expected_coop_payoff(self, history: PlayerHistory) -> float:
        """
        Estimate the expected payoff from cooperating in the last round.
        """
        previous_round_outcome = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        if previous_round_outcome >= self.game_description.m - 1:
            return 1 + self.game_description.k
        else:
            return 0

    def _expected_defect_payoff(self, history: PlayerHistory) -> float:
        """
        Estimate the expected payoff from defecting in the last round.
        """
        previous_round_outcome = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        if previous_round_outcome >= self.game_description.m - 1:
            return 1 + self.game_description.k
        else:
            return 1

class Strategy_COLLECTIVE_486(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    
    This strategy prioritizes collective cooperation while adapting to the behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        M = sum(history.opponent_actions[-1, :] & np.ones(self.game_description.n_players - 1, dtype=np.bool_)) + (history.my_actions[-1] if history.my_payoffs.size > 0 else True)
        N = self.game_description.n_players
        if M >= self.game_description.m:
            return Action.C
        elif M < self.game_description.m:
            p_defect = max(0, (self.game_description.m - M) / N)
            return Action.D if random.random() < p_defect else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if M == self.game_description.m - 1 and N - M == N - self.game_description.m + 1:
            return Action.C if random.random() < 0.5 else Action.D
        if all((not action for action in history.opponent_actions[-1, :])) and (not history.my_actions[-1]):
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_487(BaseStrategy):
    """
    Adaptive Cooperation strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective well-being,
    adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_round_outcome = None
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + 1
        self.consecutive_defections += int(history.my_actions[-1] == False)
        if state.round_number == self.game_description.n_rounds:
            return Action.C if prev_round_cooperators >= self.game_description.m else Action.D
        elif prev_round_cooperators >= self.game_description.m:
            p = 1 - (self.game_description.k - 1) / (self.game_description.n_players - self.game_description.m + 1)
            return Action.C if random.random() < p else Action.D
        else:
            p = 0.5 + (self.game_description.m - 1) / self.game_description.n_players
            action = Action.D if random.random() < p else Action.C
            if self.consecutive_defections >= 2:
                p = 0.5 + (self.game_description.m - 1) / self.game_description.n_players
                action = Action.C if random.random() < p else Action.D
                self.consecutive_defections = 0
            return action
        self.prev_round_outcome = prev_round_cooperators

class Strategy_COLLECTIVE_488(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.w = min(3, self.r - 1)
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            if state.round_number == 0:
                return Action.C
            elif sum(history.opponent_actions[-1, :]) < self.m:
                return Action.D
            else:
                return Action.C
        p = self.calc_cooperation_rate(history)
        theta = max(self.m / self.n_players, 0.5) + (self.k - 1) / 2 * p
        if p >= theta:
            return Action.C
        else:
            return Action.D

    def calc_cooperation_rate(self, history: PlayerHistory) -> float:
        recent_history = history.opponent_actions[-self.w:]
        coop_count = sum((1 for round in recent_history if sum(round) > 0))
        return coop_count / (len(recent_history) * (self.n_players - 1))

class Strategy_COLLECTIVE_489(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma.
    
    This strategy aims to balance individual self-interest with collective cooperation, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.game_description.n_rounds // 2:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators < self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif self.game_description.n_rounds // 2 <= state.round_number < 3 * self.game_description.n_rounds // 4:
            avg_payoff = np.mean(history.my_payoffs[-5:]) if len(history.my_payoffs) >= 5 else history.my_payoffs[-1]
            if avg_payoff < 1.5:
                return Action.D
            elif sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True) >= self.game_description.m - 1:
                return Action.C
        elif 3 * self.game_description.n_rounds // 4 <= state.round_number < self.game_description.n_rounds:
            avg_payoff = np.mean(history.my_payoffs[-5:]) if len(history.my_payoffs) >= 5 else history.my_payoffs[-1]
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators + (history.my_actions[-1] == True) >= self.game_description.m - 1 and avg_payoff > 1.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds:
            all_cooperated = np.all(history.opponent_payoffs[-1, :] >= self.game_description.k)
            if all_cooperated:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_490(BaseStrategy):
    """
    An adaptive strategy that promotes collective cooperation in the Collective Risk Dilemma game.
    
    The strategy is designed to be robust, collective-minded, and adaptive. It aims to achieve better outcomes 
    for all players by considering the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initializes the AdaptiveCollectiveCooperation strategy with a given game description.

        Args:
            game_description (CollectiveRiskDescription): The game description containing the number of players, rounds, 
                minimum cooperators needed, and reward factor.
        """
        self.game_description = game_description
        self.m_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Returns the action to take in a given game state.

        Args:
            state (GameState): The current game state.
            history (None | PlayerHistory): The player's history, or None if this is the first round.

        Returns:
            Action: The chosen action (C or D).
        """
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        self.m_prev = cooperators
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.m_prev >= self.game_description.m else Action.D
        cooperate_prob = (self.game_description.m - self.m_prev) / self.game_description.n_players if self.m_prev < self.game_description.m else 1.0
        if cooperate_prob == 0:
            return Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_491(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_C = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        avg_cooperators = np.mean(history.opponent_actions[:, :] | np.column_stack((history.my_actions,)))
        if state.round_number == self.game_description.n_rounds:
            return Action.C
        if avg_cooperators >= self.game_description.m / 2:
            self.p_C = (self.game_description.m - avg_cooperators) / (self.game_description.n_players - avg_cooperators)
            return Action.C if random.random() < self.p_C else Action.D
        elif np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1] < self.game_description.m:
            return Action.D if random.random() < 0.5 else Action.C
        else:
            self.p_C = min(self.p_C + 0.1, 1)
            return Action.C if random.random() < self.p_C else Action.D

class Strategy_COLLECTIVE_492(BaseStrategy):
    """
    Adaptive Threshold Cooperator (ATC) strategy for Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_memory = 0
        self.average_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + 1
        previous_threshold_met = num_cooperators_last_round >= self.game_description.m
        if state.round_number < self.game_description.n_rounds // 2:
            if not previous_threshold_met:
                return Action.D
            else:
                return Action.C
        elif self.game_description.n_rounds // 2 <= state.round_number < 3 * self.game_description.n_rounds // 4:
            self.threshold_memory += int(previous_threshold_met)
            if self.threshold_memory >= 2:
                return Action.C
            else:
                return Action.D
        else:
            if self.average_cooperators is None:
                total_cooperators = sum(sum(history.opponent_actions[:state.round_number, :]) + state.round_number)
                self.average_cooperators = total_cooperators / (self.game_description.n_rounds * self.game_description.n_players)
            if self.average_cooperators >= self.game_description.m / 2:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if previous_threshold_met:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_493(BaseStrategy):
    """
    Adaptive Threshold Cooperation Strategy for Collective Risk Dilemma.

    This strategy adapts to changing cooperation rates by adjusting its threshold,
    encouraging cooperation when possible while avoiding exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m / game_description.n_players
        self.coop_rate_prev = 0.0
        self.prev_payoff_C = 0.0
        self.prev_payoff_D = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(history.opponent_actions) > 0:
            coop_rate = np.mean(history.opponent_actions[-1, :])
        else:
            coop_rate = self.coop_rate_prev
        if state.round_number % 2 == 0:
            avg_coop_rate = (coop_rate + self.coop_rate_prev) / 2.0
            if avg_coop_rate > 0.5:
                self.threshold += 0.05
            elif avg_coop_rate < 0.5:
                self.threshold -= 0.05
        self.coop_rate_prev = coop_rate
        if coop_rate >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_494(BaseStrategy):
    """
    This strategy aims to balance individual self-interest with collective benefits by adapting to the game's history and opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_threshold_met = None
        self.defected_in_prev_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.prev_threshold_met = None
            return Action.C
        if history is not None and history.my_actions.shape[0] > 0:
            cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            self.prev_threshold_met = cooperators >= self.game_description.m
        if history is not None and history.my_actions.shape[0] > 0 and (not history.my_actions[-1]):
            self.defected_in_prev_round = True
        exploration_probability = 0.1
        if random.random() < exploration_probability:
            return random.choice([Action.C, Action.D])
        if self.prev_threshold_met is None or self.prev_threshold_met:
            p = (self.game_description.m / self.game_description.n_players) ** self.game_description.k
            if self.defected_in_prev_round:
                p = 0.5
            if random.random() < p:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_495(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Response (ACTR) strategy in a collective risk dilemma game.

    This strategy aims to balance individual self-interest with collective well-being by adaptively responding 
    to the actions of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m / 2
        self.cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.m - 1:
            return Action.C
        if history is None or len(history.opponent_actions) == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :])
        self.cooperators = prev_round_coops + (history.my_actions[-1] if len(history.my_actions) else 0)
        if self.cooperators >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 0 and history.opponent_payoffs is not None:
            prev_round_payoff_sum = sum(history.opponent_payoffs[-1, :]) + (history.my_payoffs[-1] if len(history.my_payoffs) else 0)
            k_achieved = prev_round_payoff_sum >= self.game_description.k * self.game_description.n_players
            if not k_achieved:
                if self.cooperators < self.game_description.m:
                    self.threshold += 1
                elif self.cooperators >= self.game_description.m:
                    self.threshold -= 1
        if state.round_number >= self.game_description.n_rounds - 2:
            total_payoff_C = sum(history.my_payoffs) + (self.game_description.k - 1) * (self.game_description.n_rounds - state.round_number)
            total_payoff_D = sum(history.my_payoffs) + (self.game_description.n_rounds - state.round_number)
            if total_payoff_C > total_payoff_D:
                action = Action.C
        return action

class Strategy_COLLECTIVE_496(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_mechanism_triggered = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators_in_previous_round = sum(history.opponent_actions[-1, :] == True)
        if cooperators_in_previous_round >= self.game_description.m - 1:
            if self.punishment_mechanism_triggered and cooperators_in_previous_round >= self.game_description.m:
                self.punishment_mechanism_triggered = False
            return Action.C
        if not self.punishment_mechanism_triggered:
            p = 1 - (self.game_description.k - 1) / (self.game_description.n_players - self.game_description.m + 1)
            if random.random() < p:
                self.punishment_mechanism_triggered = True
                return Action.D
        if cooperators_in_previous_round == self.game_description.m - 1:
            return random.choice([Action.C, Action.D])
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_497(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters, balancing individual self-interest with collective responsibility.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5
        self.cooperate_count = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators < self.game_description.m:
            self.p = min(1, self.game_description.k / (self.game_description.k + 1))
        else:
            self.p = min(1, 1 / (self.game_description.k + 1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.my_actions.all() and (history.opponent_actions.sum(axis=0) == history.opponent_actions.shape[0]).all():
            return Action.C
        if not history.my_actions.any() and (not history.opponent_actions.any(axis=1).any()):
            return Action.D
        action = Action.C if random.random() < self.p else Action.D
        self.cooperate_count[state.round_number % self.game_description.n_players] = 1 if action == Action.C else 0
        num_cooperators = sum(self.cooperate_count)
        if num_cooperators < self.game_description.m:
            self.p += 0.1
        elif num_cooperators >= self.game_description.m:
            self.p -= 0.1
        self.p = max(0, min(self.p, 1))
        return action

class Strategy_COLLECTIVE_498(BaseStrategy):
    """
    Adaptive Collective Risk strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is None:
            num_cooperators_last_round = self.game_description.m
        else:
            num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators_last_round < self.game_description.m:
            return Action.D
        elif num_cooperators_last_round >= self.game_description.m:
            p = (self.game_description.k - 1) / self.game_description.k
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_499(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Cooperation (ATC)
    
    This strategy adapts to the cooperation rate and collective payoff of players 
    in a collective risk dilemma game, balancing individual self-interest with 
    the need for collective action to achieve the reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.0
        self.collective_payoff = 0.0
        self.threshold = game_description.m / game_description.n_players
        self.window_size = min(5, game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :] + [True])
        self.cooperation_rate = num_cooperators / self.game_description.n_players
        if state.round_number >= self.window_size:
            start_round = state.round_number - self.window_size
            total_payoff = sum(history.my_payoffs[start_round:])
            for opponent_actions in history.opponent_actions.T:
                opponent_payoffs = np.where(opponent_actions, 0.0, 1.0)
                total_payoff += sum(opponent_payoffs[start_round:])
            self.collective_payoff = total_payoff / (self.game_description.n_players * self.window_size)
        if self.collective_payoff < (self.game_description.k + 1) / 2:
            self.threshold *= 0.9
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.cooperation_rate >= self.threshold else Action.D
        if state.round_number < 5 and any(history.opponent_actions[-1, :]):
            return Action.C
        return Action.C if self.cooperation_rate >= self.threshold else Action.D

class Strategy_COLLECTIVE_500(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation with Social Learning.
    
    This strategy aims to balance individual self-interest with collective well-being 
    by adapting to the behavior of others and learning from past experiences.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.reputation_scores = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        for i, coop in enumerate(history.opponent_actions[-1, :]):
            if coop:
                self.reputation_scores[i] += 1
            else:
                self.reputation_scores[i] -= 1
        exp_payoff = 1 + self.game_description.k
        p_coop = max(0.5, prev_cooperators / self.game_description.n_players) if prev_cooperators >= self.game_description.m else min(1, (self.game_description.n_players - prev_cooperators) / self.game_description.n_players)
        for i in range(self.game_description.n_players):
            p_coop += 0.05 * self.reputation_scores[i] if self.reputation_scores[i] > 0 else -0.05 * self.reputation_scores[i]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if prev_cooperators >= self.game_description.m else Action.D
        return Action.C if random.random() < p_coop else Action.D

class Strategy_COLLECTIVE_501(BaseStrategy):
    """
    A collective strategy that depends only on game parameters and history, 
    aiming to balance individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or len(history.my_actions) == 0:
            return Action.C if self.game_description.m >= self.game_description.n_players / 2 else Action.D
        prev_round = history.opponent_actions[-1]
        threshold_met = sum(prev_round) + (history.my_actions[-1] if history.my_actions is not None and len(history.my_actions) > 0 else False) >= self.game_description.m
        if state.round_number < self.game_description.n_rounds - 1:
            if threshold_met and (history.my_actions[-1] if history.my_actions is not None and len(history.my_actions) > 0 else True):
                return Action.C
            elif (history.my_actions[-1] if history.my_actions is not None and len(history.my_actions) > 0 else False) and (not threshold_met):
                p = (self.game_description.n_players - self.game_description.m) / self.game_description.n_players
                return Action.D if random.random() < p else Action.C
            else:
                coop_count = sum((1 for round_actions in history.opponent_actions if sum(round_actions) + True >= self.game_description.m))
                return Action.C if coop_count > state.round_number // 2 else Action.D
        never_cooperated_before = not any(history.my_actions) if history.my_actions is not None and len(history.my_actions) > 0 else True
        if never_cooperated_before:
            return Action.D
        coop_payoff_sum = sum([1 - my_action + self.game_description.k * (sum(opponent_actions) + my_action >= self.game_description.m) for opponent_actions, my_action in zip(history.opponent_actions, history.my_actions)] if history.my_actions is not None and len(history.my_actions) > 0 else [self.game_description.k])
        defect_payoff_sum = sum([1 - my_action for my_action in history.my_actions] if history.my_actions is not None and len(history.my_actions) > 0 else [self.game_description.n_players])
        return Action.C if coop_payoff_sum > defect_payoff_sum else Action.D

class Strategy_COLLECTIVE_502(BaseStrategy):
    """
    The Adaptive Threshold Tracker (ATT) strategy balances individual self-interest with collective risk aversion in a repeated Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = game_description.m / game_description.n_players
        self.adaptation_increment = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        prev_threshold_met = sum(history.opponent_actions[-1, :] + [Action.C]) >= self.game_description.m
        if prev_threshold_met:
            self.cooperation_probability = min(1, self.cooperation_probability + self.adaptation_increment)
        else:
            self.cooperation_probability = max(0, self.cooperation_probability - self.adaptation_increment)
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_503(BaseStrategy):
    """
    Adaptive Threshold Cooperation Strategy.
    
    This strategy aims to balance individual self-interest with collective well-being 
    by adapting cooperation levels based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperation_rates = []

    def calculate_avg_cooperation_rate(self, history):
        recent_rounds = 3 if len(history.my_actions) >= 3 else len(history.my_actions)
        avg_cooperation_rate = np.mean(history.opponent_actions[-recent_rounds:, :] == Action.C)
        return avg_cooperation_rate

    def increasing_trend(self, history):
        recent_rounds = 3 if len(history.my_actions) >= 3 else len(history.my_actions)
        cooperation_rates = np.mean(history.opponent_actions[-recent_rounds:, :] == Action.C, axis=1)
        return all((cooperation_rates[i] <= cooperation_rates[i + 1] for i in range(len(cooperation_rates) - 1)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_cooperation_rate = self.calculate_avg_cooperation_rate(history)
        self.avg_cooperation_rates.append(avg_cooperation_rate)
        threshold = self.game_description.m / self.game_description.n_players
        if avg_cooperation_rate >= threshold:
            return Action.C
        elif avg_cooperation_rate < threshold and self.increasing_trend(history):
            return Action.C
        else:
            return Action.D
        if self.game_description.k > 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_504(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    Prioritizes collective cooperation while adapting to other players' behaviors.
    Encourages continued cooperation when there's a strong collective effort and 
    introduces a gradual punishment mechanism to deter defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.n_players < self.m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators_prev_round >= self.m:
                return Action.C
            else:
                return Action.D
        if len(history.my_actions) > 2 and all((action for action in history.my_actions[-3:-1])):
            return Action.C
        if num_cooperators_prev_round >= math.ceil(self.m / 2):
            return Action.C
        punishment_prob = (self.m - num_cooperators_prev_round) / self.m
        return Action.D if random.random() < punishment_prob else Action.C

class Strategy_COLLECTIVE_505(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_number = state.round_number
        if self.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.game_description.m:
            return Action.C
        prev_total_payoff = sum(history.my_payoffs)
        average_payoff = prev_total_payoff / (self.round_number + 1) if history.my_payoffs.size > 0 else 0
        prev_round_opponent_payoffs = np.sum(history.opponent_payoffs[-1, :])
        if prev_round_opponent_payoffs > average_payoff:
            return Action.C
        if self.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_506(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.0
        self.total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            x = sum(history.opponent_actions[-1, :])
            self.total_cooperators += x
        else:
            x = 0
        if state.round_number <= self.game_description.m:
            return Action.C
        elif state.round_number < (self.game_description.n_rounds + 1) // 2:
            if x >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        else:
            self.cooperation_rate = self.total_cooperators / state.round_number
            if self.cooperation_rate >= 0.5 and x >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        if state.round_number > self.game_description.n_rounds - self.game_description.m and x < self.game_description.m:
            return Action.C

class Strategy_COLLECTIVE_507(BaseStrategy):
    """
    An adaptive collective cooperation strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.coop_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < self.coop_prob else Action.D
        prev_actions = history.opponent_actions[-1, :]
        cooperators = sum(prev_actions) + (history.my_actions[-1] == 1)
        if state.round_number == self.game_description.n_rounds:
            return Action.C
        elif cooperators >= self.game_description.m:
            self.coop_prob = 1.0
            return Action.C
        else:
            defect_prob = (self.game_description.n_players - self.game_description.m) / self.game_description.n_players
            punish_prob = self.game_description.k / (self.game_description.k + 1)
            if random.random() < defect_prob:
                return Action.D
            elif any(prev_actions == False):
                return Action.D if random.random() < punish_prob else Action.C
            else:
                return Action.C

class Strategy_COLLECTIVE_508(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperative Threshold (ACT)
    
    The ACT strategy aims to balance individual self-interest with collective cooperation, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_success_thresholds = 0
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        prev_round_outcome = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        defectors = self.game_description.n_players - prev_round_outcome
        cooperators = prev_round_outcome
        if len(self.history) > self.game_description.n_rounds // 2:
            self.avg_success_thresholds = sum((1 for outcome in self.history[-self.game_description.n_rounds // 2:] if outcome >= self.game_description.m)) / (self.game_description.n_rounds // 2)
        if prev_round_outcome >= self.game_description.m:
            cooperate_prob = 1 - defectors / self.game_description.n_players
        else:
            cooperate_prob = cooperators / self.game_description.n_players
        if self.avg_success_thresholds >= self.game_description.m:
            cooperate_prob = 1 - defectors / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate_prob = 1
        if len([outcome for outcome in self.history[-self.game_description.n_rounds // 4:] if outcome < self.game_description.m]) == self.game_description.n_rounds // 4:
            cooperate_prob = cooperators >= self.game_description.m - 1
        action = Action.C if random.random() < cooperate_prob else Action.D
        self.history.append(prev_round_outcome)
        return action

class Strategy_COLLECTIVE_509(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being by 
    encouraging cooperation through reciprocal altruism, detecting and responding to 
    defection, exploring alternative strategies to adapt to changing opponent behaviors, 
    and learning from the collective behavior of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.p_recip = 0.8
        self.p_defect = 0.7
        self.p_explore = 0.1
        self.delta_p = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        prev_round_payoff = history.my_payoffs[-1]
        avg_coops = np.mean(np.sum(history.opponent_actions, axis=1) + history.my_actions)
        coop_prob = self.p_init
        if prev_round_coops >= self.game_description.m and prev_round_payoff > 1:
            coop_prob = self.p_recip
        elif np.sum(history.opponent_actions[-1, :]) > self.game_description.n_players - self.game_description.m:
            coop_prob = 1 - self.p_defect
        if random.random() < self.p_explore:
            return Action.C if random.random() < 0.5 else Action.D
        else:
            action = Action.C if random.random() < coop_prob else Action.D
        prev_avg_coops = np.mean(np.sum(history.opponent_actions[:-1, :], axis=1) + history.my_actions[:-1])
        if avg_coops > prev_avg_coops and coop_prob < 1:
            coop_prob += self.delta_p
        elif avg_coops < prev_avg_coops and coop_prob > 0:
            coop_prob -= self.delta_p
        return action

class Strategy_COLLECTIVE_510(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and depends solely on game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.game_description.m:
            return Action.C
        if state.round_number > 1 and prev_cooperators > sum(history.opponent_actions[-2, :]) + history.my_actions[-2]:
            return Action.C
        opponents = np.any(history.opponent_actions, axis=0)
        for i, opponent in enumerate(opponents):
            if not opponent:
                return Action.D
            elif sum(history.opponent_actions[:, i]) / state.round_number > 0.5:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_511(BaseStrategy):
    """
    Adaptive Collective Risk (ACR) strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.r_played = 0
        self.T_met = 0
        self.C_rate = 0.5
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m == 1:
                return Action.D
            else:
                return Action.C
        self.r_played += 1
        if history.my_payoffs[-1] == 1 - 0 + self.game_description.k:
            self.T_met += 1
        self.C_rate = self.T_met / self.r_played
        self.cooperation_probability = max(0, min(1, self.cooperation_probability + 0.1 * (self.C_rate - 0.5)))
        if state.round_number == self.game_description.n_rounds - 1:
            coop_payoff = sum((1 - 1 + self.game_description.k if action else 1 - 0 for action in history.my_actions))
            defect_payoff = sum((1 - 0 if action else 1 - 1 + self.game_description.k for action in history.my_actions))
            return Action.C if coop_payoff >= defect_payoff else Action.D
        m_prev = sum(history.opponent_actions[-1, :])
        return Action.C if random.random() < self.cooperation_probability and m_prev >= self.game_description.m else Action.D

class Strategy_COLLECTIVE_512(BaseStrategy):
    """
    Balances individual self-interest with collective well-being, adapting to the game's history while maintaining a robust and optimistic approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coop_ratio = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            current_payoff = sum(history.my_payoffs)
            if current_payoff <= (self.game_description.k - 1) * state.round_number:
                return Action.C
        if state.round_number < self.game_description.m and history.opponent_actions.shape[0] > 0:
            prev_round_cooperators = sum(history.opponent_actions[-1, :])
            if prev_round_cooperators < self.game_description.m - 1:
                return Action.C
        if history.my_payoffs is not None and len(history.my_payoffs) > 0 and (history.my_payoffs[-1] < self.game_description.k * self.game_description.n_players):
            opponent_defectors = np.where(~history.opponent_actions[-1, :])[0]
            if len(opponent_defectors) > 0:
                punished_opponent = random.choice(opponent_defectors)
                return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        self.prev_coop_ratio = prev_round_cooperators / self.game_description.n_players
        if self.prev_coop_ratio >= self.game_description.m / self.game_description.n_players:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_1(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.punishment_rounds = 2
        self.window_size = 5
        self.exploited = False

    def calculate_ocr(self, opponent_actions: NDArray[np.bool_]) -> float:
        return np.mean(opponent_actions)

    def was_exploited(self, history: PlayerHistory) -> bool:
        if len(history.my_actions) < 2:
            return False
        prev_my_action = history.my_actions[-2]
        prev_opponent_actions = history.opponent_actions[-2]
        return prev_my_action and np.any(prev_opponent_actions == False)

    def adjust_tau_and_punishment_rounds(self, round_number: int, history: PlayerHistory):
        if len(history.opponent_actions) < self.window_size:
            return
        recent_ocr = np.mean([self.calculate_ocr(round) for round in history.opponent_actions[-self.window_size:]])
        if recent_ocr > self.tau and (not self.exploited):
            self.tau += 0.05
        elif (recent_ocr < self.tau or self.exploited) and len(history.my_actions) >= 2 and history.my_actions[-2] and np.any(history.opponent_actions[-1] == False):
            self.tau -= 0.05
            self.punishment_rounds += 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number > 0 and len(history.my_actions) < 2):
            return Action.C
        prev_cooperators = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
        prev_opponent_actions = history.opponent_actions[-1]
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif prev_cooperators >= self.game_description.m:
            opponent_ocr = self.calculate_ocr(prev_opponent_actions)
            if opponent_ocr > self.tau and (not (self.exploited and len(history.my_actions) - 1 < self.punishment_rounds)):
                action = Action.C
                self.exploited = False
            else:
                action = Action.D
        elif self.was_exploited(history):
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.adjust_tau_and_punishment_rounds(state.round_number, history)
        return action

class Strategy_EXPLOITATIVE_2(BaseStrategy):
    """
    This strategy adapts to the game's history and exploits the opponent's behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_cooperators >= self.game_description.m:
            p_c = (1 + self.game_description.k) / (self.game_description.n_players - self.game_description.m + 1)
        else:
            p_d = (self.game_description.n_players - self.game_description.m) / self.game_description.n_players
        if prev_cooperators >= self.game_description.m:
            action = Action.C if random.random() < p_c else Action.D
        else:
            action = Action.C if random.random() < 1 - p_d else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            average_expected_payoff = (self.game_description.k + 1) / 2
            total_cooperators = sum(history.my_actions)
            self.total_payoff += self.calculate_payoff(action, history.opponent_actions[-1], self.game_description.m, self.game_description.k)
            if self.total_payoff > average_expected_payoff * state.round_number:
                return Action.D
            else:
                p_c = 1 - self.total_payoff / ((state.round_number + 1) * (self.game_description.k + 1))
                action = Action.C if random.random() < p_c else Action.D
        self.total_payoff += self.calculate_payoff(action, history.opponent_actions[-1], self.game_description.m, self.game_description.k)
        return action

    def calculate_payoff(self, my_action: Action, opponent_actions: NDArray[np.bool_], m: int, k: float) -> float:
        cooperators = sum(opponent_actions + [my_action == Action.C])
        if cooperators < m:
            return 1 - (my_action == Action.C)
        else:
            return 1 - (my_action == Action.C) + k

class Strategy_EXPLOITATIVE_3(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    It adapts to the game parameters and history, balancing cooperation and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploit_mode = False
        self.prev_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2 or (state.round_number == 3 and self.game_description.n_players < 4):
            return Action.D
        if history is not None:
            current_cooperators = sum(history.opponent_actions[-1, :])
            self.prev_round_cooperators = current_cooperators
            if current_cooperators >= self.game_description.m and current_cooperators - 1 >= self.game_description.m:
                self.exploit_mode = True
        if self.exploit_mode:
            return Action.D
        if history is not None and history.my_actions[-1] and (self.prev_round_cooperators < self.game_description.m):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and self.prev_round_cooperators >= self.game_description.m:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_4(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr_history = []
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        opponent_actions = history.opponent_actions[:-1] if len(history.opponent_actions) > 0 else np.array([[]])
        acr = np.mean(opponent_actions)
        self.acr_history.append(acr)
        cooperate = acr >= self.game_description.m / self.game_description.n_players
        if cooperate and random.random() < self.epsilon:
            cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = np.mean(self.acr_history) >= self.game_description.m / self.game_description.n_players
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_6(BaseStrategy):
    """
    Exploits opponents' cooperation by adapting to the game's dynamics and adjusting its own cooperation rate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_rate = 0.5
        self.last_round_actions = None
        self.avg_cooperators_count = 0
        self.adaptation_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.last_round_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(self.last_round_actions)
        if self.adaptation_counter < 5:
            self.avg_cooperators_count += num_cooperators / 5
        else:
            self.avg_cooperators_count = (self.avg_cooperators_count * 4 + num_cooperators) / 5
        self.adaptation_counter += 1
        if self.adaptation_counter % 5 == 0:
            if self.avg_cooperators_count > self.game_description.m:
                self.coop_rate = min(self.coop_rate + 0.1, 1)
            elif self.avg_cooperators_count < self.game_description.m:
                self.coop_rate = max(self.coop_rate - 0.1, 0)
        if num_cooperators < self.game_description.m:
            return Action.D
        elif self.game_description.k > 1.5 and num_cooperators >= self.game_description.m:
            return Action.C
        else:
            return random.choice([Action.C, Action.D]) if random.random() < self.coop_rate else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_7(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < 5:
            prev_coops = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if prev_coops < self.game_description.m:
                return Action.D
            else:
                p = self.game_description.k / (self.game_description.k + 1)
                return Action.C if random.random() < p else Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            coop_payoffs = [sum((1 for x in history.opponent_actions[i, :] if x)) * (self.game_description.k + 1 - int(history.my_actions[i])) for i in range(max(state.round_number - self.game_description.n_rounds // 4, 0), state.round_number) if history.my_actions[i]]
            def_payoffs = [sum((1 for x in history.opponent_actions[i, :] if not x)) * (self.game_description.k + 1 - int(not history.my_actions[i])) for i in range(max(state.round_number - self.game_description.n_rounds // 4, 0), state.round_number) if not history.my_actions[i]]
            avg_coop_payoff = sum(coop_payoffs) / len(coop_payoffs) if coop_payoffs else 0
            avg_def_payoff = sum(def_payoffs) / len(def_payoffs) if def_payoffs else 0
            return Action.C if avg_coop_payoff >= avg_def_payoff else Action.D
        else:
            rounds_left = self.game_description.n_rounds - state.round_number
            if rounds_left <= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_8(BaseStrategy):
    """
    A hybrid approach that combines elements of cooperation, exploration, and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.c_prev = 0.5
        self.exploitation_phase_active = False
        self.exploitation_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        self.c_prev = num_cooperators_last_round / self.game_description.n_players
        T = self.game_description.m / (self.game_description.m + self.game_description.k * self.c_prev)
        if not self.exploitation_phase_active:
            if T > 0.5:
                return Action.C
            elif num_cooperators_last_round < self.game_description.m:
                self.exploitation_phase_active = True
                self.exploitation_rounds_remaining = random.randint(1, math.ceil(self.game_description.n_rounds / 2))
                return Action.D
        else:
            self.exploitation_rounds_remaining -= 1
            if self.exploitation_rounds_remaining == 0:
                self.exploitation_phase_active = False
        return Action.D

class Strategy_EXPLOITATIVE_10(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior and game parameters.
    Cooperates initially, then responds to the number of cooperators in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        num_cooperators_curr_round = sum(history.opponent_actions[-1, :])
        self.num_cooperators_prev_round = num_cooperators_curr_round
        if num_cooperators_curr_round >= self.game_description.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_11(BaseStrategy):
    """
    The Adaptive Collective Risk Exploiter (ACRE) strategy.
    
    This strategy adapts to the game's history, balancing exploitation and cooperation.
    It encourages early cooperation by cooperating with a probability that increases 
    as the minimum number of cooperators needed approaches the total number of players.
    
    In subsequent rounds, ACRE adjusts its strategy based on the previous round's outcome,
    exploiting the reward if enough players cooperated or defecting with higher probability
    if too few players cooperated to increase the likelihood of reaching the threshold 
    in future rounds. The last round adjustment ensures that ACRE will not miss the opportunity
    to cooperate if others have done so, maximizing its payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.c_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        if history is not None:
            opponent_actions_prev_round = history.opponent_actions[-1, :]
            self.c_prev = sum(opponent_actions_prev_round) + (history.my_actions[-1] == 1)
        if state.round_number < self.game_description.n_rounds - 1:
            if self.c_prev >= self.game_description.m:
                cooperate_prob = self.game_description.k / (self.game_description.k + 1)
            else:
                cooperate_prob = (self.game_description.n_players - self.game_description.m) / (self.game_description.n_players - self.c_prev + 1)
            return Action.C if random.random() < cooperate_prob else Action.D
        elif self.c_prev >= self.game_description.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_12(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0.6 * game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.game_description.n_rounds // 2:
            average_cooperators = np.mean(history.opponent_actions[:, :].sum(axis=1))
            return Action.C if average_cooperators >= self.game_description.m else Action.D
        recent_opponents_actions = history.opponent_actions[-5:, :]
        expected_cooperators = np.mean(recent_opponents_actions.sum(axis=1))
        return Action.C if expected_cooperators >= self.threshold else Action.D
        consistent_cooperators = np.any(history.opponent_actions[:, :] == True, axis=0)
        return Action.D if np.any(consistent_cooperators) else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_13(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperation levels of other players 
    while prioritizing individual payoff maximization in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.success_count = 0
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        if prev_cooperators < self.threshold:
            action = Action.D
        elif self.game_description.k > 1:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 1 and bool(action) and (prev_cooperators >= self.threshold):
            self.success_count += 1
        if self.success_count > 0.5 * state.round_number:
            self.threshold = min(self.threshold + 1, self.game_description.n_players)
        elif self.success_count < 0.25 * state.round_number and self.threshold > 1:
            self.threshold = max(self.threshold - 1, 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_14(BaseStrategy):
    """
    This strategy aims to maximize individual payoff by exploiting the collective risk dilemma's structure and adapting to opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def calculate_payoffs(self, actions: NDArray[np.bool_]) -> list[float]:
        payoffs = []
        for action in actions:
            if sum(actions) < self.game_description.m:
                payoff = 1 - int(action)
            else:
                payoff = 1 - int(action) + self.game_description.k
            payoffs.append(payoff)
        return payoffs

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        avg_payoff_prev_round = np.mean(self.calculate_payoffs(np.append(history.my_actions[-1], history.opponent_actions[-1, :])))
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif avg_payoff_prev_round >= 1 + self.game_description.k / 2:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_16(BaseStrategy):
    """
    The ATT strategy aims to balance individual self-interest with adaptability and opportunism,
    exploiting opponents' behaviors while maintaining a robust response to various scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.opponent_d_count = 0
        self.opponent_c_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_actions = history.opponent_actions[-1]
        self.prev_cooperators = sum(prev_actions)
        self.opponent_d_count = np.sum(np.logical_not(prev_actions))
        self.opponent_c_count = np.sum(prev_actions)
        if state.round_number > 1:
            prev_prev_actions = history.opponent_actions[-2]
            consecutive_d = np.all(np.logical_not([prev_actions, prev_prev_actions]), axis=0)
            consecutive_c = np.all([prev_actions, prev_prev_actions], axis=0)
            self.opponent_d_count += np.sum(consecutive_d)
            self.opponent_c_count += np.sum(consecutive_c)
        if self.prev_cooperators < self.game_description.m or self.opponent_d_count >= 2:
            return Action.D
        elif self.opponent_c_count >= 2 and np.mean(history.my_payoffs) > 1:
            return Action.C
        elif history is not None:
            prev_my_action = history.my_actions[-1]
            return Action.D if prev_my_action == False else Action.C
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and np.sum(history.my_payoffs) < (self.game_description.k + 1) * state.round_number / 2:
            return Action.D

class Strategy_EXPLOITATIVE_17(BaseStrategy):
    """
    The Adaptive Exploiter strategy adapts to various opponent behaviors while exploiting opportunities for higher payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = []
        self.total_payoffs = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.cooperators = history.opponent_actions[-1, :]
            self.total_payoffs.append(sum(history.my_payoffs))
        m_c = sum((1 for player in self.cooperators if player))
        action = Action.D
        if m_c >= self.game_description.m:
            action = Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            total_prev_payoffs = sum(self.total_payoffs)
            if total_prev_payoffs < self.game_description.n_players * self.game_description.k / 2:
                action = Action.D
            elif m_c > (self.game_description.m - 1) / 2:
                action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            if m_c < self.game_description.m:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_18(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' behaviors in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_round_payoff = None
        self.cooperators_history = []
        self.opponent_cooperation_frequency = np.zeros(game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        num_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        self.cooperators_history.append(num_cooperators)
        if len(self.cooperators_history) > 1:
            cooperators_diff = self.cooperators_history[-1] - self.cooperators_history[-2]
        else:
            cooperators_diff = 0
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            self.opponent_cooperation_frequency[i] += int(opponent_action)
        if num_cooperators >= self.game_description.m:
            avg_payoff = np.mean([history.my_payoffs[-1]] + [payoff for i, (action, payoff) in enumerate(zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :])) if action])
        else:
            avg_payoff = None
        if num_cooperators < self.game_description.m:
            return Action.D
        elif self.prev_round_payoff == 2:
            if cooperators_diff >= 0 or (cooperators_diff < 0 and avg_payoff is not None and (avg_payoff > self.prev_round_payoff)):
                return Action.C
            else:
                return Action.D
        elif self.prev_round_payoff == 3:
            if avg_payoff is not None and avg_payoff > self.prev_round_payoff:
                return Action.C
            else:
                return Action.D
        for i, frequency in enumerate(self.opponent_cooperation_frequency):
            if frequency / state.round_number < 0.5:
                return Action.D
        return random.choice([Action.C, Action.D])

class Strategy_EXPLOITATIVE_19(BaseStrategy):
    """
    The Adaptive Threshold Seeker (ATS) strategy aims to maximize its payoff by adaptively seeking the threshold of minimum cooperators needed while exploiting opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        n_cooperators = sum(opponent_actions) + 1
        coop_rate = n_cooperators / self.game_description.n_players
        threshold_distance = self.game_description.m - n_cooperators
        if state.round_number > 1:
            prev_n_cooperators = sum(history.opponent_actions[-2, :]) + (Action.C.value == history.my_actions[-2])
            payoff_differential = np.mean([history.my_payoffs[-2] if Action.C.value == a else history.opponent_payoffs[-2, i] for i, a in enumerate(opponent_actions) if prev_n_cooperators >= self.game_description.m]) - np.mean([history.my_payoffs[-2] if Action.D.value == a else history.opponent_payoffs[-2, i] for i, a in enumerate(opponent_actions) if prev_n_cooperators < self.game_description.m])
        else:
            payoff_differential = 0.0
        if coop_rate >= self.game_description.m / self.game_description.n_players:
            action = Action.C
        elif threshold_distance > 0 and payoff_differential > 0:
            action = Action.C
        elif threshold_distance < 0 and payoff_differential < 0:
            action = Action.D
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1 and coop_rate < self.game_description.m / self.game_description.n_players:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_20(BaseStrategy):
    """
    Exploit opponents in the Collective Risk Dilemma game by adapting to their behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_payoffs = history.my_payoffs[-1] + self.game_description.k * sum(history.opponent_actions[-1, :])
        if prev_round_payoffs < self.game_description.m * self.game_description.k:
            self.cooperate = False
        elif any((not opponent_action for opponent_action in history.opponent_actions[-1, :])):
            self.cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.cooperate:
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_21(BaseStrategy):
    """
    An exploitative strategy for Collective Risk Dilemma, adapting to various opponent behaviors and maximizing payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_threshold_met = 0
        self.total_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.total_rounds += 1
        if opponent_cooperators + (history.my_actions[-1] == True) >= self.game_description.m:
            self.coop_threshold_met += 1
        p_coop = self.coop_threshold_met / self.total_rounds if self.total_rounds > 0 else 0.5
        if p_coop > 0.6:
            return Action.D
        elif p_coop < 0.4:
            return Action.C
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_22(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        previous_round_outcome = history.my_payoffs[-1]
        opponents_previous_actions = history.opponent_actions[-1, :]
        if previous_round_outcome > 1 and sum(opponents_previous_actions) >= self.game_description.m:
            self.cooperation_probability += (self.game_description.k - 1) / self.game_description.n_players
        elif previous_round_outcome <= 1 or sum(opponents_previous_actions) < self.game_description.m:
            self.cooperation_probability -= (1 + self.game_description.k) / self.game_description.n_players
        for opponent_action in opponents_previous_actions:
            if opponent_action and history.opponent_payoffs[-1, int(np.where(history.opponent_actions[-1, :] == opponent_action)[0][0])] > 1:
                self.cooperation_probability += 1 / self.game_description.n_players
            elif not opponent_action:
                self.cooperation_probability -= 1 / self.game_description.n_players
        if sum(opponents_previous_actions) >= self.game_description.m - 1 and previous_round_outcome <= 1:
            self.cooperation_probability -= (self.game_description.k - 1) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = sum(history.my_payoffs)
            max_possible_payoff = self.game_description.n_rounds * self.game_description.k
            return Action.D if total_payoff >= max_possible_payoff else Action.C
        self.cooperation_probability = np.clip(self.cooperation_probability, 0.01, 0.99)
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_24(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = []
        self.threshold_met = False
        self.punishment_pending = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if history is not None:
            opponents_actions = history.opponent_actions[-1, :]
            self.cooperators.append(sum(opponents_actions))
        if len(self.cooperators) >= self.game_description.m - 1 and (not self.punishment_pending):
            action = Action.D
        elif len(self.cooperators) < self.game_description.m - 1:
            action = Action.D
        else:
            action = Action.C
        if history is not None:
            opponents_cooperators = sum(history.opponent_actions[-2, :])
            if opponents_cooperators < self.game_description.m and state.round_number > 1:
                self.punishment_pending = True
        if self.punishment_pending:
            if state.round_number == self.game_description.n_rounds - 1:
                action = Action.D
            else:
                action = Action.C
                self.punishment_pending = False
        return action

class Strategy_EXPLOITATIVE_25(BaseStrategy):
    """
    A strategy that dynamically adjusts its behavior based on the game's history and parameters.
    It aims to balance cooperation and defection to maximize payoffs while being robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        opponent_defections = []
        for opponent_actions in history.opponent_actions[-2:, :]:
            if all(opponent_actions == False):
                opponent_defections.append(True)
        if any(opponent_defections):
            return Action.D
        cooperators_last_round = sum(history.opponent_actions[-1, :] == True)
        if cooperators_last_round >= m / 2:
            return Action.C
        for opponent_actions in history.opponent_actions[-2:, :]:
            if all(opponent_actions == True):
                return Action.D
        defectors_last_round = sum(history.opponent_actions[-1, :] == False)
        if defectors_last_round > n - m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_26(BaseStrategy):
    """
    Exploitative "Adaptive Threshold" strategy for Collective Risk Dilemma game.
    
    This strategy adapts cooperation rates based on the game's history and parameters,
    aiming to maximize individual payoffs in a wide range of scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            action = Action.C if random.random() < cooperate_prob else Action.D
        else:
            opponent_actions_last_round = history.opponent_actions[-1, :]
            own_action_last_round = history.my_actions[-1]
            threshold_met_last_round = sum(opponent_actions_last_round) + int(own_action_last_round) >= self.game_description.m
            if threshold_met_last_round:
                cooperate_prob = self.game_description.k / (self.game_description.k + 1)
            else:
                cooperate_prob = 1 - self.game_description.m / self.game_description.n_players
            if not threshold_met_last_round:
                self.consecutive_defections += 1
                if self.consecutive_defections > 2 * self.game_description.n_rounds // 3:
                    cooperate_prob = 0
            action = Action.C if random.random() < cooperate_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_27(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperative behavior of others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.defect_prob = 0.5
        self.prev_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.my_actions) == 0):
            return Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number <= min(self.game_description.n_rounds // 2, 5):
            return Action.C
        if self.prev_cooperators >= self.game_description.m:
            action = Action.D
            self.defect_prob = min(self.defect_prob + 0.1, 0.9)
        else:
            action = Action.C
            self.defect_prob = max(self.defect_prob - 0.1, 0.1)
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if random.random() < self.defect_prob:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_28(BaseStrategy):
    """
    A strategic approach that adapts to various opponent behaviors and maximizes payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_coop = 0
        self.avg_payoff_C = 0
        self.avg_payoff_D = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1 or (state.round_number == 2 and history is None):
            return Action.C
        self.m_coop = sum(history.opponent_actions[-1, :])
        recent_rounds_C_payoffs = np.array([0.0 if not action else payoff for action, payoff in zip(history.my_actions[-2:], history.my_payoffs[-2:])])
        recent_rounds_D_payoffs = np.array([payoff if not action else 0.0 for action, payoff in zip(history.my_actions[-2:], history.my_payoffs[-2:])])
        self.avg_payoff_C = np.mean(recent_rounds_C_payoffs) if len(recent_rounds_C_payoffs) > 0 else 0
        self.avg_payoff_D = np.mean(recent_rounds_D_payoffs) if len(recent_rounds_D_payoffs) > 0 else 0
        if self.m_coop >= self.game_description.m or self.avg_payoff_C > self.avg_payoff_D + 0.5 * self.game_description.k:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and self.m_coop >= self.game_description.m:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_29(BaseStrategy):
    """
    An adaptive exploitation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop_threshold = 0.5
        self.lr = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :] == True)
        p_coop = m_prev / self.game_description.n_players
        if p_coop < self.p_coop_threshold:
            action = Action.D
        elif p_coop >= self.p_coop_threshold:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if m_prev >= self.game_description.m - 1:
                action = Action.C
            else:
                action = Action.D
        if p_coop == 0.5:
            action = Action.C
        if state.round_number > 0:
            m_prev = sum(history.opponent_actions[-1, :] == True)
            if m_prev >= self.game_description.m:
                self.p_coop_threshold += self.lr * (m_prev - self.game_description.m) / self.game_description.n_players
            elif m_prev < self.game_description.m:
                self.p_coop_threshold -= self.lr * (self.game_description.m - m_prev) / self.game_description.n_players
        return action

class Strategy_EXPLOITATIVE_30(BaseStrategy):
    """
    The Adaptive Exploiter strategy leverages historical analysis to inform adaptive decision-making.
    It aims to maximize individual payoffs in the Collective Risk Dilemma game by cooperating or defecting 
    based on the number of cooperators and defectors in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            n = self.game_description.n_players
            m = self.game_description.m
            if n % 2 != 0 and m == (n + 1) // 2:
                return Action.C
            else:
                return Action.C if m <= n / 2 else Action.D
        C_prev = sum(history.opponent_actions[-1, :])
        threshold_met = C_prev >= self.game_description.m
        k = self.game_description.k
        if threshold_met and k > 2:
            return Action.C
        elif not threshold_met and C_prev < self.game_description.m / 2:
            return Action.D
        else:
            p = C_prev / self.game_description.n_players
            if random.random() < p:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_31(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances cooperation and defection to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        prev_defectors = self.game_description.n_players - prev_cooperators
        if prev_cooperators >= self.game_description.m - 1:
            return Action.C
        prob_defect = 0.5 + (1 - prev_cooperators / self.game_description.n_players)
        if random.random() < prob_defect:
            return Action.D
        if self.game_description.k > 2 and prev_cooperators >= self.game_description.m - 1:
            return Action.C
        if prev_cooperators == self.game_description.m - 1:
            if random.random() < 0.75:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < 0.25:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_32(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy aims to maximize its payoff by exploiting the collective risk dilemma's structure and adapting to the opponent's behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_opponent_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_coop = np.sum(history.opponent_actions[-1, :])
        coop_rate = opponent_coop / self.game_description.n_players
        if opponent_coop >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        if coop_rate < self.game_description.m / self.game_description.n_players:
            action = Action.D
        self.prev_opponent_coop = opponent_coop
        return action

class Strategy_EXPLOITATIVE_33(BaseStrategy):
    """
    Exploits the cooperative behavior of other players while minimizing its own cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        opponents_coop_rate = np.mean(history.opponent_actions)
        if opponents_coop_rate < 0.1:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_cooperators < self.game_description.m:
            return Action.D
        else:
            self.coop_prob = (self.game_description.k - 1) / (self.game_description.n_players * (self.game_description.k - 1))
            if random.random() < self.coop_prob:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_34(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy exploits cooperation in the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.p_exploit = 0.8
        self.recent_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        self.threshold_met = prev_cooperators >= self.game_description.m
        if state.round_number % 5 == 0 and len(self.recent_payoffs) > 2:
            avg_recent_payoff = np.mean(self.recent_payoffs[-3:])
            if avg_recent_payoff < np.mean(self.recent_payoffs[:-3]):
                self.p_exploit *= 0.9
            else:
                self.p_exploit *= 1.1
        if self.threshold_met or state.round_number < 3:
            return Action.C
        if random.random() < self.p_exploit:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_35(BaseStrategy):
    """
    An adaptive exploitation strategy for the Collective Risk Dilemma game.
    
    This strategy balances cooperation and defection to maximize individual payoffs 
    by observing opponents' actions and adjusting accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.adaptive_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions
        my_payoffs = history.my_payoffs
        last_round_outcome = my_payoffs[-1]
        avg_cooperation_rate = np.mean(opponent_actions[:, :])
        if last_round_outcome > 0:
            self.adaptive_threshold -= 0.01
        else:
            self.adaptive_threshold += 0.01
        self.adaptive_threshold = max(0, min(self.adaptive_threshold, 1))
        cooperators_last_round = sum(opponent_actions[-1, :])
        if cooperators_last_round + 1 >= self.game_description.m:
            return Action.C
        if random.random() < 0.2 and state.round_number > 10:
            return Action.C
        if avg_cooperation_rate >= self.adaptive_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_36(BaseStrategy):
    """
    Exploitative strategy designed to maximize its own payoff by exploiting the behavior of other players while adapting to the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = 0.0
        self.d_threshold = (game_description.m - 1) / game_description.n_players
        self.opponent_defections = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, i] and (history.my_payoffs is None or history.my_payoffs[-1] == 0):
                self.opponent_defections.append(i)
        for opponent in self.opponent_defections:
            self.d_threshold *= 1.1
        if history.opponent_actions.size > 0:
            self.p_coop = np.mean(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.p_coop < self.d_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_37(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game that aims to exploit opponents by adapting to their cooperation rates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.c_bar = 0.0
        self.l = (game_description.m - 1) / game_description.n_players
        self.u = (game_description.m + 1) / game_description.n_players
        self.w = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 1 / self.game_description.m else Action.D
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        self.c_bar = (self.c_bar * (self.w - 1) + opponent_cooperations / self.game_description.n_players) / self.w
        if sum(history.my_payoffs) >= self.game_description.k:
            p_c = min(1, self.game_description.k / (self.game_description.k + 1)) if self.c_bar > self.u else 0
        else:
            p_c = min(1, self.game_description.m / self.game_description.n_players) if self.c_bar < self.l else 0
        return Action.C if random.random() < p_c else Action.D

class Strategy_EXPLOITATIVE_38(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.success_rate_threshold_high = 0.5
        self.success_rate_threshold_low = 0.3
        self.opponent_cooperation_rate_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        threshold_met = np.sum(np.count_nonzero(history.opponent_actions[:, :], axis=1) + np.array([a for a in history.my_actions]) >= self.game_description.m)
        success_rate = threshold_met / state.round_number
        opponent_cooperation_rate = np.mean(history.opponent_actions)
        if success_rate > self.success_rate_threshold_high and opponent_cooperation_rate >= self.opponent_cooperation_rate_threshold:
            return Action.C
        elif success_rate < self.success_rate_threshold_low and opponent_cooperation_rate < self.opponent_cooperation_rate_threshold:
            return Action.D
        else:
            cooperation_probability = (success_rate + opponent_cooperation_rate) / 2
            if random.random() < cooperation_probability:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_39(BaseStrategy):
    """
    A strategy that balances cooperation and exploitation in the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators >= self.game_description.m:
            action = Action.C
            self.coop_count += 1
        else:
            action = Action.D
        if self.coop_count > self.game_description.n_rounds // 2:
            return Action.D
        if num_cooperators == self.game_description.m - 1 and self.coop_count >= self.game_description.n_rounds // 2:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_40(BaseStrategy):
    """
    The Adaptive Threshold Hunter (ATH) strategy aims to maximize payoffs by exploiting 
    the collective risk dilemma's structure and adapting to opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m
        self.reward_adaptation = 0.5
        self.opponent_adaptation = 0.5
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        if self.punishment_phase and state.round_number % 3 != 0:
            self.punishment_phase = False
            return Action.D
        if history is not None:
            m_t = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        else:
            m_t = 0
        if self.game_description.k > 2:
            self.reward_adaptation += 0.1
            if self.reward_adaptation > 0.8:
                self.reward_adaptation = 0.8
        elif 1 < self.game_description.k <= 2:
            self.reward_adaptation = 0.5
        if history is not None and state.round_number > 1:
            opponent_cooperation_rate = sum(history.opponent_actions[-2, :]) / (self.game_description.n_players - 1)
            if opponent_cooperation_rate >= 0.75:
                self.opponent_adaptation = 0.8
            elif opponent_cooperation_rate <= 0.25:
                self.opponent_adaptation = 0
        if history is not None and state.round_number > 1:
            my_payoff = history.my_payoffs[-1]
            avg_opponent_payoff = np.mean(history.opponent_payoffs[-2, :])
            if my_payoff < avg_opponent_payoff and self.punishment_phase == False:
                self.punishment_phase = True
                return Action.D
        if m_t >= self.cooperation_threshold:
            action = Action.C
        else:
            action = Action.D
        if random.random() < self.reward_adaptation:
            action = Action.C
        elif random.random() < self.opponent_adaptation:
            action = Action.C
        return action

class Strategy_EXPLOITATIVE_41(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Explorer" (ATE)
    
    ATE aims to exploit opponents' behaviors by adaptively adjusting its cooperation threshold based on observed history.
    This approach allows ATE to balance individual payoff maximization with the need to contribute to the collective good.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 3:
            if state.round_number == 1:
                return Action.C
            else:
                p = self.game_description.m / self.game_description.n_players
                return Action.C if random.random() < p else Action.D
        avg_cooperators = np.mean(np.sum(history.opponent_actions, axis=1) + history.my_actions)
        if avg_cooperators >= self.game_description.m:
            return Action.C
        opponent_behaviors = np.mean(history.opponent_actions, axis=0)
        exploitable_opponents = np.any((opponent_behaviors < self.game_description.m / self.game_description.n_players) | (history.opponent_payoffs == 1))
        if exploitable_opponents:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_42(BaseStrategy):
    """
    A hybrid strategy that combines elements of cooperative and exploitative approaches,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_exploit = 0.5
        self.p_coop = 0.7
        self.p_endgame = 0.9
        self.avg_payoff_threshold = 1.5
        self.total_payoff_threshold = 2 * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if state.round_number <= 3:
            return Action.C
        elif state.round_number <= self.game_description.n_rounds // 2:
            if num_cooperators_prev < self.game_description.m:
                return random.choice([Action.D, Action.C]) if random.random() < self.p_exploit else Action.C
            else:
                return Action.C
        else:
            avg_payoff = np.mean(history.my_payoffs)
            total_payoff = sum(history.my_payoffs)
            if num_cooperators_prev >= self.game_description.m:
                self.p_coop += 0.1 * (self.game_description.k - avg_payoff) / self.game_description.k
            else:
                self.p_coop -= 0.1 * (avg_payoff + 1) / (self.game_description.k + 1)
            self.p_coop = max(0, min(self.p_coop, 1))
            if avg_payoff >= self.avg_payoff_threshold:
                return random.choice([Action.D, Action.C]) if random.random() < self.p_coop else Action.C
            else:
                return Action.D
        if state.round_number >= self.game_description.n_rounds - 1:
            if total_payoff >= self.total_payoff_threshold:
                return random.choice([Action.D, Action.C]) if random.random() < self.p_endgame else Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_43(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game.
    It focuses on maximizing payoffs by adaptively responding to the cooperation levels of other players while being mindful of the game's parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.initial_cooperative_phase = math.ceil(self.r / 4)
        self.observation_phase = math.ceil(self.r / 8)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.r - 1:
            return Action.D
        elif state.round_number < self.initial_cooperative_phase:
            return Action.C
        elif history is not None and len(history.my_actions) >= self.observation_phase + self.initial_cooperative_phase:
            avg_coop_rate = sum((1 for round in history.opponent_actions[-self.observation_phase:, :] if np.sum(round) >= self.m)) / self.observation_phase
            if avg_coop_rate > 0.7:
                return Action.D
            elif avg_coop_rate < 0.3:
                return Action.C
            else:
                prob_C = (avg_coop_rate - 0.3) / (0.7 - 0.3)
                return Action.C if random.random() < prob_C else Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_44(BaseStrategy):
    """
    Adaptive Threshold Pursuer (ATP) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.at_least_m_cooperators = 0
        self.C_t = []
        self.cooperate_prob = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_count = np.sum(history.opponent_actions[-1, :], axis=0)
        opponent_defected = np.any(~history.opponent_actions[-1, :])
        T = self.at_least_m_cooperators / state.round_number
        if T >= 0.5:
            action = Action.C
        else:
            CR = sum(self.C_t) / state.round_number if self.C_t else 0
            if CR > self.game_description.m / self.game_description.n_players:
                self.cooperate_prob += 0.1
            else:
                self.cooperate_prob -= 0.1
            if T >= 0.5 and CR <= self.game_description.m / self.game_description.n_players and opponent_defected:
                action = Action.D
            elif state.round_number == self.game_description.n_rounds - 1:
                action = Action.C if T >= 0.5 and CR > self.game_description.m / self.game_description.n_players else Action.D
            else:
                action = Action.C if random.random() < self.cooperate_prob else Action.D
        self.at_least_m_cooperators += int(opponent_count >= self.game_description.m)
        self.C_t.append(int(np.sum(history.opponent_actions[-1, :])))
        return action

class Strategy_EXPLOITATIVE_45(BaseStrategy):
    """
    Adaptive Threshold Escalation (ATE) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[state.round_number - 1, :])
        avg_payoff_last_two_rounds = np.mean(history.my_payoffs[max(0, state.round_number - 2):state.round_number])
        if state.round_number > 1:
            last_round_avg_payoff = np.mean(history.my_payoffs[state.round_number - 1])
        else:
            last_round_avg_payoff = 0
        if state.round_number <= 3:
            return Action.C if prev_cooperators <= self.m / 2 else Action.D
        elif 4 <= state.round_number < self.game_description.n_rounds - 3:
            return Action.C if prev_cooperators >= self.m or avg_payoff_last_two_rounds < self.k / 2 else Action.D
        elif prev_cooperators >= self.m and (last_round_avg_payoff < self.k / 2 or state.round_number % 2 == 0):
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D if prev_cooperators < self.m else Action.C
        return Action.D

class Strategy_EXPLOITATIVE_46(BaseStrategy):
    """
    A hybrid strategy that combines elements of Tit-for-Tat (TFT) and Pavlov's algorithm,
    designed to be exploitative, adaptive, and robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_outcome = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        threshold_met = prev_round_outcome >= self.game_description.m
        if threshold_met:
            return Action.C
        else:
            coop_prob = self.calculate_cooperation_probability(self.game_description.k, self.game_description.n_players, history)
            return Action.D if random.random() < coop_prob else Action.C

    def calculate_cooperation_probability(self, k: float, n: int, history: PlayerHistory) -> float:
        coop_count = sum(history.my_actions)
        threshold_met_count = sum((1 for i in range(len(history.opponent_actions)) if sum(history.opponent_actions[i, :]) + int(history.my_actions[i]) >= self.game_description.m))
        p = 1 - k / (n + k) * (coop_count / len(history.my_actions))
        if coop_count == 0:
            p = 0.5
        return p

class Strategy_EXPLOITATIVE_47(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.window_size = min(5, game_description.n_rounds // 2)
        self.prev_cooperators = None
        self.avg_cooperators = 0.0
        self.total_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_actions = history.opponent_actions[-1, :] if history else []
        self.prev_cooperators = sum(prev_round_actions)
        if history is not None and len(history.my_payoffs) > 0:
            self.total_payoff += history.my_payoffs[-1]
        if state.round_number <= self.game_description.n_rounds // 2:
            return Action.C if self.prev_cooperators >= self.game_description.m else Action.D
        self.avg_cooperators = np.mean(history.opponent_actions[-self.window_size:, :].sum(axis=1))
        if state.round_number < self.game_description.n_rounds:
            return Action.C if self.avg_cooperators >= self.game_description.m else Action.D
        return Action.D if self.total_payoff >= 1 + self.game_description.k else Action.C

class Strategy_EXPLOITATIVE_48(BaseStrategy):
    """
    Exploitative strategy that adapts to cooperation levels of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.opponent_actions) > 0:
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            self.coop_count += opponent_cooperations - (self.game_description.n_players - opponent_cooperations - int(self.last_action == Action.C))
            self.coop_count = max(-self.game_description.n_players, min(self.coop_count, self.game_description.n_players))
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players == 2 and history is not None and (len(history.opponent_actions) > 0):
            opponent_cooperation_rate = sum(history.opponent_actions[:, 0]) / state.round_number
            return Action.C if opponent_cooperation_rate > 0.5 else Action.D
        if self.game_description.n_players > 2 and history is not None and (len(history.opponent_actions) > 0):
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            return Action.C if opponent_cooperations >= (self.game_description.n_players - 1) // 2 else Action.D
        if self.coop_count <= self.game_description.m - 1:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 3 and self.coop_count < self.game_description.m - 2:
            action = Action.C
        elif state.round_number > 6 and self.coop_count >= self.game_description.m:
            opponent_cooperation_rate = sum(history.opponent_actions[:, :]) / (state.round_number * self.game_description.n_players)
            if opponent_cooperation_rate > 0.5:
                action = Action.C
        self.last_action = action
        return action

class Strategy_EXPLOITATIVE_49(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by cooperating initially,
    then exploiting successful cooperation thresholds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.m_prev = sum(history.opponent_actions[-1, :]) + int(self.__class__ is history.my_actions[-1])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.m_prev >= self.game_description.m and self.m_prev < self.game_description.n_players:
            if sum((self.__class__ is action for action in history.my_actions[:-1])) == len(history.my_actions[:-1]):
                return Action.C
        if self.m_prev >= self.game_description.m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_50(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to 
    observed cooperation rates and remaining rounds to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        self.history.append(np.sum(history.opponent_actions[-1, :]))
        if state.round_number < self.game_description.n_rounds / 2:
            return Action.C if random.random() < 0.5 else Action.D
        avg_cooperation_rate = np.mean(self.history)
        if avg_cooperation_rate > self.game_description.m / self.game_description.n_players:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            prob = 1 - avg_cooperation_rate / (self.game_description.m / self.game_description.n_players)
            return Action.C if random.random() < prob else Action.D

class Strategy_EXPLOITATIVE_51(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy.

    This strategy adapts to the observed cooperation rate and adjusts its threshold accordingly.
    It tries to take advantage of others' cooperative behavior without compromising its own payoff too much.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop_rate = np.mean(self.coop_history[-3:])
        if avg_coop_rate > self.game_description.m / self.game_description.n_players:
            T = max(1, self.game_description.m - 2)
        else:
            T = max(1, self.game_description.m - 2)
        prev_round_coops = np.sum(history.opponent_actions[-1, :])
        if prev_round_coops >= T:
            return Action.D
        else:
            return Action.C

    def update(self, history: PlayerHistory):
        self.coop_history.append(np.sum(history.opponent_actions[-1, :]))

class Strategy_EXPLOITATIVE_52(BaseStrategy):
    """
    Adaptive Exploitation (AE) strategy for Collective Risk Dilemma game.

    This strategy balances adaptability with exploitativeness by initially cooperating, 
    observing opponents' actions, and adapting its cooperation frequency based on observed outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.fc_history = []
        self.threshold_met_prev_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_prev_round = history.opponent_actions[-1]
        num_cooperators_prev_round = sum(opponent_actions_prev_round)
        fc_prev_round = num_cooperators_prev_round / self.game_description.n_players
        self.fc_history.append(fc_prev_round)
        if state.round_number < 6:
            if fc_prev_round >= 0.5:
                return Action.C
            else:
                return Action.D
        my_payoff_prev_round = history.my_payoffs[-1]
        threshold_met_prev_round = self.threshold_met_prev_round
        self.threshold_met_prev_round = num_cooperators_prev_round >= self.game_description.m
        if threshold_met_prev_round and my_payoff_prev_round > 1:
            if fc_prev_round >= 0.5:
                return Action.C
            else:
                return Action.D
        elif not threshold_met_prev_round:
            if history.my_actions[-1] == False and num_cooperators_prev_round < self.game_description.m:
                return Action.C
            else:
                return Action.D
        if abs(fc_prev_round - 0.5) <= 0.05:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        my_actions_prev_5_rounds = history.my_actions[-5:]
        if len(my_actions_prev_5_rounds) >= 4 and all(my_actions_prev_5_rounds):
            opponent_cooperators_last_5_rounds = [sum(history.opponent_actions[i]) for i in range(-5, 0)]
            if all([coops < self.game_description.m for coops in opponent_cooperators_last_5_rounds]):
                return Action.D
        num_my_cooperations = sum(history.my_actions[-1:])
        num_opponent_cooperations = sum([sum(round) for round in history.opponent_actions])
        if num_my_cooperations == 1 and num_opponent_cooperations < self.game_description.m:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_53(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's progression.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_actions = np.empty(game_description.n_rounds, dtype=bool)
        self.my_payoffs = np.zeros(game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        elif 3 <= state.round_number < self.game_description.n_rounds // 2:
            prev_coops = sum(history.opponent_actions[-1, :] == True)
            avg_payoff_coops = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1, :] == True] if np.any(history.opponent_actions[-1, :] == True) else 0)
            if prev_coops < self.game_description.m:
                return Action.D
            elif avg_payoff_coops >= self.game_description.k:
                return Action.C
        else:
            prev_coop_trend = sum(history.opponent_actions[-2, :] == True) <= sum(history.opponent_actions[-1, :] == True)
            avg_payoff_defectors = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1, :] == False] if np.any(history.opponent_actions[-1, :] == False) else 0)
            if prev_coop_trend:
                return Action.C
            elif avg_payoff_defectors >= self.game_description.k:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1 or state.round_number == self.game_description.n_rounds:
            if sum(history.opponent_actions[-1, :] == True) < self.game_description.m and sum(history.opponent_actions[-1, :] == True) + 1 >= self.game_description.m:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_54(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_payoff_prev = 0.0
        self.opponent_defected_prev = False
        self.opponent_defected_prev_prev = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        num_opponents_defected_prev = sum((1 - opponent_action for opponent_action in prev_round_opponent_actions))
        avg_payoff_prev = np.mean(history.my_payoffs[-1:] + history.opponent_payoffs[-1, :])
        self.avg_payoff_prev = avg_payoff_prev
        if avg_payoff_prev >= 1 + self.game_description.k / 2:
            action = Action.C
        else:
            action = Action.D
        if num_opponents_defected_prev >= self.game_description.n_players - self.game_description.m:
            action = Action.D
        if self.opponent_defected_prev and self.opponent_defected_prev_prev:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        self.opponent_defected_prev_prev = self.opponent_defected_prev
        self.opponent_defected_prev = num_opponents_defected_prev > 0
        return action

class Strategy_EXPLOITATIVE_55(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) for Collective Risk Dilemma.
    
    This strategy adapts its cooperation rate based on the game's history and parameters,
    aiming to exploit situations where other players are cooperating excessively or not contributing enough.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.p_init = self.m / self.n_players
        self.p_current = self.p_init
        self.delta_p = 0.1
        self.risk_averse_delta_p = 0.2
        self.p_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators_prev_round >= self.m:
            self.p_current += self.delta_p
        else:
            self.p_current -= self.delta_p
        if self.p_current > self.p_threshold:
            cooperate_prob = self.p_threshold
        else:
            cooperate_prob = self.p_current
        total_payoff_prev_round = sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        if num_cooperators_prev_round < self.m and total_payoff_prev_round < self.n_players * (1 + self.k):
            self.p_current -= self.risk_averse_delta_p
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_56(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy balances cooperation and exploitation by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5
        self.total_payoff_C = 0
        self.total_payoff_D = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        num_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev >= self.game_description.m:
            self.p = self.update_p(self.p, self.total_payoff_C, self.total_payoff_D)
            return Action.C if random.random() < self.p else Action.D
        else:
            return Action.D
        if history.my_actions[-1]:
            self.total_payoff_C += 1 + self.game_description.k if num_cooperators_prev >= self.game_description.m else 0
        else:
            self.total_payoff_D += 1

    def update_p(self, p: float, total_payoff_C: int, total_payoff_D: int) -> float:
        if total_payoff_C > total_payoff_D:
            return min(1, p + 0.05)
        else:
            return max(0, p - 0.05)

class Strategy_EXPLOITATIVE_57(BaseStrategy):
    """
    Conditional Cooperator with Adaptive Threshold (CCAT) strategy.
    
    This strategy balances cooperation and exploitation by adapting to the game's history.
    It cooperates in the first round with a probability proportional to the minimum number of cooperators needed,
    then adjusts its behavior based on the level of cooperation observed in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            p = self.m / self.n_players
            return Action.C if random.random() < p else Action.D
        if history is not None and len(history.my_actions) > 0:
            previous_round_cooperators = sum(history.opponent_actions[-1, :])
            CQ = previous_round_cooperators / self.m
            if CQ >= self.k / (self.k + 1):
                return Action.C
            else:
                if CQ == 0 and random.random() < self.m / self.n_players:
                    return Action.C
                return Action.D
        elif state.round_number + 1 == self.game_description.n_rounds:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_58(BaseStrategy):
    """
    A strategy that adapts to the opponents' behavior and exploits cooperative environments.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m - 1
        self.delta_pi = 0
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        self.prev_cooperators = sum(history.opponent_actions[-1, :] != False)
        my_payoff_last_round = np.sum(np.where(history.my_actions[-1] == True, 0 + self.game_description.k, 1))
        opponent_payoffs_last_round = history.opponent_payoffs[-1]
        opponent_defected_last_round = sum(opponent_payoffs_last_round > 1)
        if state.round_number >= 2:
            my_payoff_two_rounds_ago = np.sum(np.where(history.my_actions[-2] == True, 0 + self.game_description.k, 1))
            opponent_payoffs_two_rounds_ago = history.opponent_payoffs[-2]
            delta_pi_last_two_rounds = my_payoff_last_round - my_payoff_two_rounds_ago - np.mean(opponent_payoffs_last_round - opponent_payoffs_two_rounds_ago)
        else:
            delta_pi_last_two_rounds = 0
        self.delta_pi += delta_pi_last_two_rounds / state.round_number
        if self.delta_pi > 0 and self.cooperation_threshold < self.game_description.m:
            self.cooperation_threshold += 1
        elif self.delta_pi < 0 and self.cooperation_threshold > 1:
            self.cooperation_threshold -= 1
        if self.prev_cooperators >= self.cooperation_threshold and opponent_defected_last_round:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            if self.prev_cooperators >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        elif state.round_number < 5 and self.prev_cooperators >= self.cooperation_threshold - 1 and (opponent_defected_last_round >= 2):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_59(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Exploiter" (ATE)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds = game_description.n_rounds
        self.threshold = game_description.m
        self.reward_factor = game_description.k
        self.estimated_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            if state.round_number == 1:
                return Action.C
            else:
                return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        cooperators = sum(opponent_actions)
        if cooperators >= self.threshold:
            self.estimated_cooperators += 1
        else:
            self.estimated_cooperators -= 1
        if self.estimated_cooperators < self.threshold:
            return Action.D
        elif self.estimated_cooperators == self.threshold:
            return random.choice([Action.C, Action.D])
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if history.my_payoffs[-1] > 0:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_60(BaseStrategy):
    """
    Exploitative and adaptive strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = game_description.m / game_description.n_players
        if math.isclose(game_description.m, game_description.n_players - 1):
            self.p_init = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        θ_prev = history.my_payoffs[-1]
        c_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] * 1
        π_c = np.mean([payoff if action else payoff - self.game_description.k for payoff, action in zip(history.my_payoffs, history.my_actions)])
        π_d = np.mean([payoff for payoff, action in zip(history.my_payoffs, history.my_actions) if not action])
        if π_c >= π_d:
            cooperate_prob = max(self.p_init, c_prev / (self.game_description.n_players - 1))
        else:
            cooperate_prob = min(1 - self.p_init, 1 - c_prev / (self.game_description.n_players - 1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_62(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0
        self.opp_avg_coop_rate = None
        self.last_round_payoffs = []
        self.converged = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            coop_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < coop_prob else Action.D
        opponent_coops = np.sum(history.opponent_actions, axis=1)
        avg_coop_rate = (self.avg_coop_rate * (state.round_number - 1) + np.mean(opponent_coops)) / state.round_number
        self.avg_coop_rate = avg_coop_rate
        if history.opponent_actions.shape[0] > 1:
            opp_avg_coop_rate = np.mean(history.opponent_actions[-2:, :])
            self.opp_avg_coop_rate = opp_avg_coop_rate
        k = self.game_description.k
        m_over_n = self.game_description.m / self.game_description.n_players
        if avg_coop_rate >= m_over_n:
            coop_prob = k / (k + 1)
            cooperate = random.random() < coop_prob
        else:
            cooperate = False
        if self.opp_avg_coop_rate is not None and self.opp_avg_coop_rate >= m_over_n:
            exploit_prob = 1 - k / (k + 1)
            if random.random() > exploit_prob:
                cooperate = False
        if state.round_number > self.game_description.n_rounds // 2:
            opp_coop_rates = np.mean(history.opponent_actions, axis=0)
            converged = np.allclose(opp_coop_rates[-self.game_description.n_rounds // 2:], opp_coop_rates[-1])
            self.converged = converged
            if converged:
                cooperate = random.random() < 0.5
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_63(BaseStrategy):
    """
    Exploit the game's structure by adapting to the cooperation level of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_history = []
        self.average_payoffs_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] + [Action.C.value]) if history else None
        previous_average_payoff = np.mean(history.my_payoffs) if history else None
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if previous_cooperators < self.game_description.m:
            return Action.D
        if previous_average_payoff - 1 > self.game_description.k:
            action = Action.C
        else:
            action = Action.D
        self.cooperators_history.append(previous_cooperators)
        self.average_payoffs_history.append(previous_average_payoff)
        return action

class Strategy_EXPLOITATIVE_65(BaseStrategy):
    """
    A strategy that adapts its cooperation rate based on the number of cooperators in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        self.prev_cooperators = sum(history.opponent_actions[-1, :])
        self.threshold_met = self.prev_cooperators >= self.game_description.m
        if self.threshold_met:
            return Action.C if random.random() < 0.5 else Action.D
        elif self.prev_cooperators == self.game_description.m - 1:
            return Action.C if random.random() < 0.5 else Action.D
        elif all((not action for action in history.opponent_actions[-1, :])):
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        else:
            cooperate_prob = (self.game_description.m - self.prev_cooperators) / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_66(BaseStrategy):
    """
    A strategy that adapts to the game dynamics by balancing cooperation and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploited_count = 0
        self.payoff_diff_sum = 0.0
        self.defect_probability = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_history = history.opponent_actions[-1]
        cooperators = sum(previous_round_history)
        average_payoff_all = np.mean(history.my_payoffs)
        if state.round_number < self.game_description.n_rounds // 2:
            if cooperators >= self.game_description.m:
                action = Action.C
            else:
                action = Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            if cooperators >= self.game_description.m:
                action = Action.C
            else:
                action = Action.D
        else:
            total_payoff_i = sum(history.my_payoffs)
            if total_payoff_i < average_payoff_all:
                action = Action.D
            elif cooperators >= self.game_description.m:
                action = Action.C
            else:
                action = Action.D
        if history.my_actions[-1]:
            if cooperators < self.game_description.m:
                self.exploited_count += 1
        else:
            self.payoff_diff_sum += history.opponent_payoffs[-1, :].max() - history.my_payoffs[-1]
        if self.exploited_count > self.game_description.n_rounds // 4 or self.payoff_diff_sum > average_payoff_all:
            self.defect_probability = max(self.defect_probability, 0.5)
        return Action.D if random.random() < self.defect_probability else action

class Strategy_EXPLOITATIVE_67(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting 
    the cooperation of other players while minimizing its own contributions 
    to the community project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_obs = 0
        self.prev_payoff = 0
        self.avg_defector_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        m_obs = sum(opponent_actions)
        total_payoff = np.sum(history.opponent_payoffs[-1, :])
        if m_obs < self.game_description.m:
            defectors = ~opponent_actions & (history.my_actions[-1] == False)
            avg_defector_payoff = np.mean(history.opponent_payoffs[-1, defectors])
            if not np.isnan(avg_defector_payoff):
                self.avg_defector_payoff = avg_defector_payoff
        if m_obs >= self.game_description.m:
            if self.prev_payoff <= self.avg_defector_payoff and (m_obs >= self.m_obs or (self.m_obs == 0 and state.round_number > 1)):
                return Action.C
        self.m_obs = m_obs
        self.prev_payoff = history.my_payoffs[-1]
        return Action.D

class Strategy_EXPLOITATIVE_68(BaseStrategy):
    """
    This strategy adapts to the behavior of its opponents in the Collective Risk Dilemma game.
    It initially cooperates with a probability based on the minimum number of cooperators needed, 
    and then adjusts this probability based on the success or failure of recent rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_C = game_description.m / game_description.n_players
        self.t_last_success = -1
        self.success_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.n_rounds // 2:
            return Action.C if random.random() < self.p_C else Action.D
        elif state.round_number >= self.game_description.n_rounds // 2 and history.my_payoffs.size > 0:
            opponents_last_actions = np.sum(history.opponent_actions[-1, :])
            last_opponents_cooperated = opponents_last_actions + (history.my_actions[-1] == True)
            if last_opponents_cooperated < self.game_description.m:
                return Action.D
            else:
                average_payoff = np.mean(np.append(history.my_payoffs[-1], history.opponent_payoffs[-1, :]))
                return Action.C if history.my_payoffs[-1] < average_payoff else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponents_last_actions = np.sum(history.opponent_actions[-1, :])
        last_opponents_cooperated = opponents_last_actions + (history.my_actions[-1] == True)
        if last_opponents_cooperated >= self.game_description.m:
            self.success_count += 1
            self.t_last_success = state.round_number - 1
        if self.success_count > 0.5 * (self.game_description.n_rounds // 4) and self.p_C < 1:
            self.p_C += 0.1
        elif self.success_count < 0.5 * (self.game_description.n_rounds // 4) and self.p_C > self.game_description.m / self.game_description.n_players:
            self.p_C -= 0.1
        return Action.C if random.random() < self.p_C else Action.D

class Strategy_EXPLOITATIVE_69(BaseStrategy):
    """
    Exploitative strategy that adapts to the behavior of other players 
    while maximizing its own payoff in a Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = 0
        self.NT = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if sum(history.opponent_actions[-1, :] + [True]) >= self.game_description.m:
            self.T += 1
        else:
            self.NT += 1
        CT = (self.T + 1) / (self.T + self.NT + 2)
        expected_payoff = CT * self.game_description.k + (1 - CT)
        if expected_payoff <= 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_70(BaseStrategy):
    """
    Adaptive Threshold Tracker (ATT) Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.mcoop = 0
        self.exp_thr = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.my_actions) == 0):
            return Action.C
        prev_coop = sum(history.opponent_actions[-1, :] + [True])
        if self.mcoop >= self.game_description.n_rounds / 2:
            self.exp_thr = max(self.game_description.m - 1, 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_coop >= self.exp_thr:
            action = Action.D
        else:
            action = Action.C
        if history is not None and len(history.my_actions) > 0:
            last_coop = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            if last_coop >= self.game_description.m:
                self.mcoop += 1
        return action

class Strategy_EXPLOITATIVE_72(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperation rate and payoffs of opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.cooperation_probability = None
        self.avg_payoff_C = 0.0
        self.avg_payoff_D = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperation_probability = self.m / self.n
            return Action.C if random.random() < self.cooperation_probability else Action.D
        prev_coop_rate = sum(history.opponent_actions[-1, :]) / (self.n - 1)
        if prev_coop_rate >= self.m / self.n:
            self.cooperation_probability = self.k / (self.k + 1)
        else:
            self.cooperation_probability = 0.0
        if state.round_number > 2:
            payoffs_C = history.my_payoffs[history.my_actions]
            payoffs_D = history.my_payoffs[~history.my_actions]
            avg_payoff_C = np.mean(payoffs_C[-2:]) if len(payoffs_C) >= 2 else payoffs_C[-1] if payoffs_C.size > 0 else 0.0
            avg_payoff_D = np.mean(payoffs_D[-2:]) if len(payoffs_D) >= 2 else payoffs_D[-1] if payoffs_D.size > 0 else 0.0
            self.avg_payoff_C = avg_payoff_C
            self.avg_payoff_D = avg_payoff_D
            if avg_payoff_C < avg_payoff_D:
                self.cooperation_probability *= 0.9
        if state.round_number % 3 == 0 and state.round_number > 2:
            actual_cooperators = np.sum(history.opponent_actions[-3:, :], axis=1) + history.my_actions[-3:]
            if all(actual_cooperators >= self.m):
                self.m += 1
            elif all(actual_cooperators < self.m):
                self.m -= 1
        if state.round_number == self.r - 1:
            return Action.D
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_73(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    It starts by cooperating in the first two rounds to gather information about the opponent's behavior,
    then estimates the threshold of cooperation based on the number of opponents who cooperated in the previous rounds.
    The strategy then adapts its exploitation mode based on the estimated threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.estimated_threshold = 0
        self.strategy = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == 1:
            num_cooperators = sum((1 for action in [Action.C] + history.my_actions[:state.round_number - 1] if action == Action.C))
            self.estimated_threshold = max(1, min(num_cooperators + 1, self.game_description.n_players))
            if self.estimated_threshold <= 2:
                self.strategy = 'always_defect'
            elif self.estimated_threshold > self.game_description.n_players / 2:
                self.strategy = 'always_cooperate'
            else:
                p = self.estimated_threshold / self.game_description.n_players
                self.strategy = 'mixed_strategy'
        if self.strategy == 'always_defect':
            return Action.D
        elif self.strategy == 'always_cooperate':
            return Action.C
        elif random.random() < self.estimated_threshold / self.game_description.n_players:
            return Action.C
        else:
            return Action.D
        num_cooperators = sum((1 for action in history.opponent_actions[-1, :] + [Action.C] + history.my_actions[:state.round_number - 1] if action == Action.C))
        self.estimated_threshold = max(1, min(num_cooperators + 1, self.game_description.n_players))

class Strategy_EXPLOITATIVE_74(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior and seeks the minimum threshold of cooperators required.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_rate_history = []
        self.num_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.cooperate_rate_history.append(np.any(history.opponent_actions[-1, :] == True))
            self.num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        if state.round_number <= self.game_description.n_rounds // 2:
            if self.num_cooperators_prev_round < self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds // 2 + 1:
            avg_cooperate_rate = np.mean(self.cooperate_rate_history)
            if avg_cooperate_rate < 0.5:
                return Action.D
            else:
                return Action.C
        else:
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            if self.num_cooperators_prev_round < self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_75(BaseStrategy):
    """
    Exploitative strategy that adapts to the collective behavior of opponents while prioritizing self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.success_count = 0
        self.total_rounds_played = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return np.random.choice([Action.C, Action.D], p=[0.5, 0.5])
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :] | (history.my_actions[-1] if history is not None else False))
        if self.total_rounds_played > 0:
            p = self.success_count / self.total_rounds_played
        else:
            p = 0.5
        if state.round_number < self.game_description.n_rounds // 2:
            if num_cooperators_prev_round < self.game_description.m:
                return Action.D
            else:
                return np.random.choice([Action.C, Action.D], p=[p, 1 - p])
        elif state.round_number >= self.game_description.n_rounds // 2 and state.round_number < self.game_description.n_rounds:
            avg_payoff_last_few_rounds = np.mean(history.my_payoffs[max(0, state.round_number - 5):])
            if avg_payoff_last_few_rounds > 1.5:
                return np.random.choice([Action.C, Action.D], p=[p, 1 - p])
            else:
                return Action.D
        else:
            return Action.D
        self.success_count += 1 if history.my_actions[-1] and num_cooperators_prev_round >= self.game_description.m else 0
        self.total_rounds_played += 1

class Strategy_EXPLOITATIVE_76(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to balance cooperation and exploitation,
    adapting to the game's dynamics and opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        t = state.round_number - 1
        C = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        D = self.n_players - C
        if t <= 2:
            return Action.C
        elif 2 < t and t <= self.game_description.n_rounds // 2:
            if C < self.m:
                return Action.D
            else:
                opponent_defected_last_round = any((not action for action in history.opponent_actions[-1, :]))
                if opponent_defected_last_round:
                    return Action.C
        elif self.game_description.n_rounds // 2 < t and t <= 3 * self.game_description.n_rounds // 4:
            cooperators = sum((1 for i in range(self.n_players - 1) if history.opponent_actions[:, i].sum() >= 0.75 * t))
            defectors = sum((1 for i in range(self.n_players - 1) if history.opponent_actions[:, i].sum() <= 0.5 * t))
            unpredictables = self.n_players - 1 - cooperators - defectors
            if cooperators >= self.m - 1 and unpredictables <= 1:
                return Action.C
            else:
                return Action.D
        elif 3 * self.game_description.n_rounds // 4 < t and t <= self.game_description.n_rounds - 2:
            if C < self.m:
                return Action.D
            elif self.k > 1:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_77(BaseStrategy):
    """
    This strategy conditionally cooperates based on the opponent's cooperation rate and total payoff.
    It also punishes opponents who defect excessively.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_threshold = 0.7
        self.payoff_threshold = 1.5
        self.punish_threshold = 0.3
        self.punish_rounds = 2
        self.punishing_opponent = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        opponent_coop_rate = np.mean(opponent_actions)
        prev_round_payoff = self.calculate_payoff(history)
        if opponent_coop_rate >= self.coop_threshold and prev_round_payoff >= self.payoff_threshold or (state.round_number < self.game_description.m and opponent_coop_rate > 0):
            return Action.C
        elif not self.punishing_opponent:
            return Action.D
        if opponent_coop_rate < self.punish_threshold and (not self.punishing_opponent):
            self.punishing_opponent = True
            return Action.D
        elif self.punishing_opponent and state.round_number - history.my_actions.size >= self.punish_rounds:
            self.punishing_opponent = False
        return Action.D

    def calculate_payoff(self, history: PlayerHistory) -> float:
        if len(history.opponent_actions) == 0 or len(history.my_actions) == 0:
            return 0.0
        prev_round_opponent_coop_rate = np.mean(history.opponent_actions[-1, :])
        prev_round_my_action = history.my_actions[-1]
        if prev_round_opponent_coop_rate * self.game_description.n_players >= self.game_description.m:
            payoff = 1 - int(prev_round_my_action) + self.game_description.k
        else:
            payoff = 1 - int(prev_round_my_action)
        return payoff

class Strategy_EXPLOITATIVE_78(BaseStrategy):
    """
    Exploitative strategy that adapts to the average payoff in previous rounds,
    punishes early rounds with low cooperation, and exploits opponents with high cooperation rates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_rate_opponent = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_probability = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_probability else Action.D
        avg_payoff_prev = np.mean(history.my_payoffs[-1:])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_payoff_prev >= (self.game_description.k + 1) / 2:
            action = Action.C
        else:
            action = Action.D
        if state.round_number < 5 and avg_payoff_prev < 1 and ((history.opponent_actions[-1, :] == True).sum() / self.game_description.n_players < self.game_description.m / self.game_description.n_players):
            action = Action.D
        if self.coop_rate_opponent is None:
            self.coop_rate_opponent = np.zeros(self.game_description.n_players, dtype=float)
        opponent_cooperation_rate = np.mean(history.opponent_actions[:, :self.game_description.n_players - 1], axis=0)
        if opponent_cooperation_rate[state.round_number % (self.game_description.n_players - 1)] > 0.75:
            defect_probability = 0.25 + (opponent_cooperation_rate[state.round_number % (self.game_description.n_players - 1)] - 0.75) * 2
            action = Action.C if random.random() < 1 - defect_probability else Action.D
        return action

class Strategy_EXPLOITATIVE_79(BaseStrategy):
    """
    An exploitative strategy for Collective Risk Dilemma that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = [0] * game_description.n_rounds
        self.avg_payoffs = [0.0] * game_description.n_rounds

    def calculate_avg_payoff(self, t, history):
        if t == 0:
            return 1.0
        total_payoff = np.sum(history.my_payoffs[:t]) + np.sum(history.opponent_payoffs[:t])
        n_players = self.game_description.n_players
        return total_payoff / (n_players * t)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            t = state.round_number - 1
            opponent_actions = history.opponent_actions[t]
            self.cooperators[t] = np.sum(opponent_actions) + (history.my_actions[t] == True)
            self.avg_payoffs[t] = self.calculate_avg_payoff(t, history)
        t = state.round_number - 1
        if self.cooperators[t] >= self.game_description.m:
            return Action.D
        elif self.cooperators[t] + 1 >= self.game_description.m and self.avg_payoffs[t] > 2:
            return Action.C
        elif self.cooperators[t] < self.game_description.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_80(BaseStrategy):
    """
    A risk-averse exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_defections = []
        self.cooperation_rate = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_actions = history.opponent_actions[-1, :]
        for opponent in range(self.game_description.n_players - 1):
            if not opponents_actions[opponent] and (opponent in self.consecutive_defections or (state.round_number > 1 and (not history.opponent_actions[-2, opponent]))):
                self.consecutive_defections.append(opponent)
                return Action.D
        cooperators_count = sum((1 for action in history.opponent_actions[-1, :] if action)) + 1
        if cooperators_count >= self.game_description.m:
            return Action.C
        payoffs = history.my_payoffs
        opponent_payoffs = history.opponent_payoffs
        avg_coo_payoff = np.mean([payoff for i, payoff in enumerate(opponent_payoffs[-1, :]) if history.opponent_actions[-1, i]])
        avg_def_payoff = np.mean([payoff for i, payoff in enumerate(opponent_payoffs[-1, :]) if not history.opponent_actions[-1, i]])
        if math.isnan(avg_coo_payoff):
            avg_coo_payoff = 0
        if math.isnan(avg_def_payoff):
            avg_def_payoff = 0
        if avg_coo_payoff > avg_def_payoff:
            self.cooperation_rate += 0.1
        else:
            self.cooperation_rate -= 0.1
        self.cooperation_rate = max(0.1, min(self.cooperation_rate, 0.9))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.cooperation_rate else Action.D

class Strategy_EXPLOITATIVE_81(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.tau = game_description.m / 2
        self.w = min(game_description.n_rounds, 5)
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_coop_count = sum((1 for action in history.opponent_actions[-1, :] if action))
        if prev_coop_count >= self.game_description.m:
            return Action.D
        elif prev_coop_count > self.game_description.m / 2:
            return Action.C
        tau_update = self.tau + (prev_coop_count - self.game_description.m / 2) / self.game_description.n_rounds
        self.tau = max(self.game_description.m / 2, min(tau_update, self.game_description.n_players))
        if prev_coop_count > self.tau:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_82(BaseStrategy):
    """
    Exploit opponents' cooperation by adapting to their behavior and punishing defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_coop_count = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_coop_count >= self.game_description.m // 2:
            return Action.C
        elif prev_coop_count < self.game_description.m // 2:
            return Action.D
        elif prev_coop_count >= self.game_description.m and state.round_number > 1:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1 and prev_coop_count >= self.game_description.m - 1:
            return Action.C
        elif prev_coop_count == self.game_description.m // 2 and self.game_description.k * prev_coop_count > self.game_description.n_players - prev_coop_count + self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_83(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy adapts to the game's progression, 
    initially gathering information and then exploiting the cooperation threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.threshold_achieved = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        opponent_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        average_payoff_per_player = np.mean([payoff if payoff > 0 else 1 for payoff in history.my_payoffs])
        if state.round_number <= math.floor(self.game_description.n_rounds / 3):
            if previous_round_cooperators >= self.game_description.m and average_payoff_per_player > 1.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number <= math.floor(2 * self.game_description.n_rounds / 3):
            if opponent_cooperation_rate > 0.5:
                return Action.C if random.random() < 0.7 else Action.D
            else:
                return Action.C if random.random() < 0.2 else Action.D
        elif previous_round_cooperators >= self.game_description.m and average_payoff_per_player > 1.5:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if opponent_cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D
        if previous_round_cooperators >= self.game_description.m and average_payoff_per_player < 1.5:
            return Action.D

class Strategy_EXPLOITATIVE_84(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' cooperation rates while ensuring a stable threshold of cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.ocr_history = []
        self.stable_state_detected = False

    def calculate_OCR(self, round_number: int, history: PlayerHistory) -> float:
        if not history.opponent_actions.size:
            return 0.5
        opponent_cooperations = sum(history.opponent_actions[round_number - 1])
        return opponent_cooperations / self.game_description.n_players

    def moving_average(self, values: list[float]) -> float:
        return sum(values) / len(values)

    def stable_state(self) -> bool:
        if not self.ocr_history:
            return False
        recent_rounds = self.ocr_history[-3:]
        for round_OCR in recent_rounds:
            if round_OCR >= self.theta and sum(recent_rounds) == 3 * round_OCR:
                return True
        return False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.game_description.n_rounds // 2:
            p = min(0.5, self.game_description.m / self.game_description.n_players)
            return Action.C if random.random() < p else Action.D
        ocr = self.calculate_OCR(state.round_number - 1, history)
        self.ocr_history.append(ocr)
        self.theta = self.moving_average(self.ocr_history[-self.game_description.n_rounds // 4:])
        if sum(history.opponent_actions[state.round_number - 2]) < self.game_description.m:
            return Action.C if ocr > self.theta else Action.D
        p = 1 - self.game_description.k / (self.game_description.k + 1)
        return Action.D if random.random() < p else Action.C
        if state.round_number > 3 * self.game_description.n_rounds // 4:
            ocr = self.calculate_OCR(state.round_number - 1, history)
            self.ocr_history.append(ocr)
            self.theta = self.moving_average(self.ocr_history[-self.game_description.n_rounds // 4:])
            if not self.stable_state_detected and ocr > self.theta:
                return Action.C
            else:
                self.stable_state_detected = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_85(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that is adaptive and robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_to_observe = 3
        self.threshold_type = 'conservative'
        self.free_riders = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.rounds_to_observe:
            return Action.C
        recent_cooperators = sum(history.opponent_actions[-3:, :].mean(axis=0))
        m_avg = np.mean(recent_cooperators)
        if m_avg < self.game_description.m:
            self.threshold_type = 'conservative'
        else:
            self.threshold_type = 'aggressive'
        threshold = self.game_description.m - 1 if self.threshold_type == 'conservative' else self.game_description.m + 1
        recent_round_cooperators = sum(history.opponent_actions[-1, :])
        if recent_round_cooperators < threshold:
            return Action.C
        else:
            return Action.D
        for player in range(self.game_description.n_players):
            if history.opponent_actions[-1, player] == 0 and history.my_payoffs[-1] + self.game_description.k > history.opponent_payoffs[-1, player]:
                self.free_riders.add(player)
        for fr_player in self.free_riders:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if history.my_payoffs[-1] + self.game_description.k > history.opponent_payoffs[-1, :].mean():
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_86(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors and game performance.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.n:
            return Action.C
        elif state.round_number < self.r / 2:
            prev_coops = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if prev_coops >= self.m:
                return Action.C
            else:
                return Action.D
        elif state.round_number < 3 * self.r / 4:
            avg_payoff = np.mean(history.my_payoffs[-self.n:]) if len(history.my_payoffs) > self.n else 0
            if avg_payoff > self.k / 2:
                return Action.C
            else:
                return Action.D
        else:
            total_coops = sum((sum(round) for round in history.opponent_actions)) + sum(history.my_actions)
            if total_coops > self.n * self.r / 2:
                prev_m_coops = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
                if prev_m_coops >= self.m:
                    return Action.C
                else:
                    return Action.D
            elif state.round_number == self.r - 1:
                prev_m_coops = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
                if prev_m_coops >= self.m:
                    return Action.C
                else:
                    return Action.D
            return Action.D

class Strategy_EXPLOITATIVE_87(BaseStrategy):
    """
    Exploitative strategy adapting to opponents' behaviors in the collective risk dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_cooperation_rate = 0.0
        self.punishment_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if history.my_payoffs.size > 0 and state.round_number > 1:
            successful_thresholds = np.sum(np.apply_along_axis(lambda x: np.sum(x) >= self.game_description.m, 0, history.opponent_actions))
            self.opponent_cooperation_rate = successful_thresholds / state.round_number
        if num_cooperators_prev_round < self.game_description.m:
            action = Action.D
        elif num_cooperators_prev_round >= self.game_description.m:
            if self.opponent_cooperation_rate > 0.5:
                action = Action.C
            else:
                action = Action.D
        if self.punishment_mode and num_cooperators_prev_round >= self.game_description.m:
            action = Action.D
        if history.my_actions[-1] == False and num_cooperators_prev_round >= self.game_description.m:
            self.punishment_mode = True
        if state.round_number == self.game_description.n_rounds - 1:
            if sum(history.opponent_actions[-2, :]) >= self.game_description.m - 1:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_88(BaseStrategy):
    """
    An adaptive strategy that balances cooperation with exploitation in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        if num_cooperators_prev_round >= self.game_description.m // 2 and avg_payoff_prev_round < self.game_description.k / 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperations = sum(history.my_actions) + num_cooperators_prev_round * (state.round_number - 1)
            if total_cooperations >= self.game_description.m * state.round_number // 2:
                return Action.C
        if history.opponent_actions[-1, :].sum() == self.game_description.n_players // 2:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_89(BaseStrategy):
    """
    This strategy aims to exploit opponents' cooperative behavior while minimizing the risk of being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_count_history = []
        self.defection_count = {}
        self.punishment_queue = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return random.choice([Action.C, Action.D])
        if history is not None and len(history.my_actions) > 0:
            previous_cooperation_count = sum(np.logical_not(history.opponent_actions[-1, :]))
            self.cooperation_count_history.append(previous_cooperation_count)
        if state.round_number >= 3:
            previous_cooperation_count = self.cooperation_count_history[-1] if self.cooperation_count_history else None
            if previous_cooperation_count is not None and previous_cooperation_count < self.game_description.m:
                return Action.D
            elif previous_cooperation_count is not None and previous_cooperation_count >= self.game_description.m:
                return Action.C
        if history is not None and len(history.opponent_actions) > 0:
            for opponent in range(self.game_description.n_players):
                defection = history.opponent_actions[-1, opponent]
                self.defection_count[opponent] = self.defection_count.get(opponent, 0) + 1 if defection else self.defection_count.get(opponent, 0)
                if self.defection_count[opponent] > self.game_description.n_players / 2:
                    self.punishment_queue.append(opponent)
        for opponent in self.punishment_queue[:2]:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            expected_cooperation_payoff = self.game_description.k + 0.5 * self.game_description.m
            expected_defection_payoff = 1 + 0.5 * self.game_description.m
            return Action.C if expected_cooperation_payoff > expected_defection_payoff else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_90(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.m
        self.avg_coop_rate = 0.0
        self.total_payoff_diff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.avg_coop_rate = np.mean(history.my_actions)
            coop_payoffs = history.my_payoffs[history.my_actions]
            defect_payoffs = history.my_payoffs[np.logical_not(history.my_actions)]
            self.total_payoff_diff += np.mean(coop_payoffs) - np.mean(defect_payoffs)
        if state.round_number < self.game_description.n_rounds // 2:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            if opponent_cooperators >= self.coop_threshold:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds // 2:
            if self.total_payoff_diff > 0:
                self.coop_threshold += 1
            elif self.total_payoff_diff < 0 and self.coop_threshold > 1:
                self.coop_threshold -= 1
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.coop_threshold or self.avg_coop_rate > self.coop_threshold / self.game_description.n_players:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_91(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy balances cooperation and exploitation.
    It initially cooperates, observes opponents' behavior, and adjusts its cooperation threshold accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if prev_coop_rate >= self.game_description.m / self.game_description.n_players:
            action = Action.C
        elif self.cooperation_threshold > 0.7:
            action = Action.D
        else:
            action = Action.D
        self._update_threshold(prev_coop_rate)
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_coop_rate >= self.game_description.m / self.game_description.n_players:
                action = Action.C
            else:
                action = Action.D
        elif state.round_number < 5:
            self._adjust_threshold_more_aggressively(prev_coop_rate)
        return action

    def _update_threshold(self, prev_coop_rate):
        if prev_coop_rate >= self.game_description.m / self.game_description.n_players:
            self.cooperation_threshold += 0.1
        else:
            self.cooperation_threshold -= 0.1

    def _adjust_threshold_more_aggressively(self, prev_coop_rate):
        if prev_coop_rate >= self.game_description.m / self.game_description.n_players:
            self.cooperation_threshold += 0.2
        else:
            self.cooperation_threshold -= 0.2

class Strategy_EXPLOITATIVE_92(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Cooperate-to-Defect (ACD)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = game_description.m / game_description.n_players
        self.theta = []
        self.cooperate_prob = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperate_prob = self.p_init
            action = random.random() < self.cooperate_prob
            return Action.C if action else Action.D
        C_prev = sum(history.my_actions[:state.round_number])
        theta_t = (self.game_description.m - C_prev) / self.game_description.n_players
        self.theta.append(theta_t)
        if theta_t > (1 - self.game_description.k) ** (-1):
            self.cooperate_prob = 1.0
        else:
            self.cooperate_prob = 0.0
        action = random.random() < self.cooperate_prob
        if not action and history.opponent_actions.shape[0] > 0 and (sum(history.opponent_actions[-1, :]) >= self.game_description.m):
            delta = 1 / self.game_description.n_players
            self.cooperate_prob -= delta
        return Action.C if action else Action.D

class Strategy_EXPLOITATIVE_93(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy focuses on exploitation and adaptability.
    It initially defects to gather information, then adapts its cooperation probability based on the number of cooperators and reward factor.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.avg_payoff_diff = 0
        self.window_size = min(5, game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number > 0 and len(history.my_actions) == 0):
            return Action.D
        current_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        self.prev_cooperators = current_cooperators
        if current_cooperators >= self.game_description.m:
            payoff_diff = current_cooperators / self.game_description.n_players * self.game_description.k - 1
            self.avg_payoff_diff = (self.window_size - 1) / self.window_size * self.avg_payoff_diff + payoff_diff / self.window_size
            coop_prob = np.clip(payoff_diff / self.game_description.k * (current_cooperators / self.game_description.n_players), 0, 1)
            return Action.C if random.random() < coop_prob else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_94(BaseStrategy):
    """
    Exploitative strategy "Adaptive Risk-Taker" for Collective Risk Dilemma.
    Cooperates initially, adapts to opponents' behavior and exploits potential defectors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.defector_flag = False
        self.opponent_types = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        m = self.game_description.m
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.opponent_types is None:
            self.opponent_types = [None] * n_players
        if state.round_number <= math.ceil(n_players / 2):
            action = Action.C
        elif previous_round_cooperators >= m:
            action = Action.C
        else:
            action = Action.D
        for i in range(n_players - 1):
            if history.opponent_actions[-1, i]:
                self.coop_count += 1
                if self.coop_count / state.round_number > 2 / 3:
                    self.opponent_types[i] = 'cooperative'
            elif 1 - (self.coop_count / state.round_number or 0) > 2 / 3:
                self.opponent_types[i] = 'uncooperative'
        for i, opponent_type in enumerate(self.opponent_types):
            if opponent_type == 'cooperative':
                action = Action.C
            elif opponent_type == 'uncooperative':
                action = Action.D
        multiple_cooperators = previous_round_cooperators > 1
        single_defector = n_players - previous_round_cooperators == 1
        if multiple_cooperators and single_defector:
            self.defector_flag = True
        if self.defector_flag:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_95(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's history and exploits opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :])
        if prev_coops < self.game_description.m:
            return Action.D
        p = min(1, (self.game_description.k - 1) / max(self.game_description.n_players - self.game_description.m + 1, 1))
        if prev_coops == self.game_description.m:
            p = 0.5
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_96(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy aims to exploit opponents by initially cooperating,
    monitoring the threshold, adapting cooperation rate, punishing defectors, and maximizing individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.c_rate = 0.5
        self.m_t = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history.my_actions[-1]:
            self.c_rate += 0.1 * (history.opponent_actions[-1, :].sum() / len(history.opponent_actions[-1, :]))
        else:
            self.c_rate -= 0.1
        self.c_rate = max(0, min(self.c_rate, 1))
        self.m_t = sum(history.my_actions) + history.opponent_actions[-1, :].sum()
        if state.round_number < self.game_description.n_rounds - 1:
            if self.m_t >= self.game_description.m:
                if self.c_rate > 0.7:
                    return Action.C
                else:
                    return Action.D
            else:
                if self.m_t == self.game_description.m - 1 and history.opponent_actions[-1, :].sum() < self.game_description.n_players:
                    return Action.C
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_97(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for the Collective Risk Dilemma game.
    
    This strategy aims to exploit the benefits of cooperation while minimizing the risk 
    of contributing to a failing community project. It adapts to the number of cooperators 
    and adjusts probabilities based on observed behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_c = 0.8
        self.p_d = 0.6
        self.cooperators = []
        self.rounds_since_threshold_check = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        prev_cooperators = sum(self.cooperators)
        if prev_cooperators >= self.game_description.m:
            action = Action.C if random.random() < self.p_c else Action.D
        else:
            action = Action.D if random.random() < self.p_d else Action.C
        self.rounds_since_threshold_check += 1
        if self.rounds_since_threshold_check == 5 and state.round_number > 0:
            avg_cooperators = sum(self.cooperators[-10:]) / min(10, len(self.cooperators))
            if avg_cooperators < self.game_description.m:
                self.p_c = min(self.p_c + 0.1, 1)
            elif avg_cooperators > self.game_description.m:
                self.p_d = max(self.p_d - 0.1, 0)
            self.rounds_since_threshold_check = 0
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        self.cooperators.append(action == Action.C)
        return action

class Strategy_EXPLOITATIVE_98(BaseStrategy):
    """
    Adaptive Exploiter strategy for the Collective Risk Dilemma game.
    This strategy adapts to the cooperation rate of other players and exploits situations where others are not cooperating.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        opponents_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if history.my_payoffs[-1] == 1:
            self.cooperate_prob = max(0.05, min(self.cooperate_prob - 0.05, 1))
        elif history.my_payoffs[-1] > 1:
            self.cooperate_prob = min(max(self.cooperate_prob + 0.05, 0), 1)
        if opponents_coop_rate < self.game_description.m / self.game_description.n_players:
            return Action.D if random.random() < 0.9 else Action.C
        else:
            return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_99(BaseStrategy):
    """
    Exploitative strategy: Adaptive Threshold Hunter.
    
    This strategy adapts to opponents' cooperation levels and identifies opportunities to defect while minimizing the risk of collective failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.num_cooperators_prev_round = 0
        self.avg_coop_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.num_cooperators_prev_round = sum(history.opponent_actions[-1, :] != False)
        if state.round_number <= self.game_description.n_rounds // 2:
            if self.num_cooperators_prev_round < self.m:
                return Action.C
            else:
                p = (self.m - self.num_cooperators_prev_round) / self.n_players
                return Action.D if random.random() < p else Action.C
        elif state.round_number <= 3 * self.game_description.n_rounds // 4:
            opponent_actions = history.opponent_actions[:-1, :] != False
            avg_coop_rate_t = np.mean(opponent_actions.sum(axis=0))
            self.avg_coop_rate = (avg_coop_rate_t * state.round_number + self.num_cooperators_prev_round) / (state.round_number + 1)
            if self.avg_coop_rate >= self.m / self.n_players:
                return Action.C
            else:
                p = (self.m - self.avg_coop_rate * self.n_players) / self.n_players
                return Action.D if random.random() < p else Action.C
        elif self.num_cooperators_prev_round < self.m:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_100(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.k = game_description.k
        self.p = self.k / (self.k + 1)
        self.avg_cooperators_window_size = min(5, game_description.n_rounds // 2)
        self.avg_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        self.avg_cooperators.append(num_cooperators / self.game_description.n_players)
        if len(self.avg_cooperators) > self.avg_cooperators_window_size:
            self.avg_cooperators.pop(0)
        avg_cooperator_rate = sum(self.avg_cooperators) / len(self.avg_cooperators)
        if avg_cooperator_rate < self.m / self.game_description.n_players:
            self.p += 0.05
        else:
            self.p -= 0.05
        self.p = max(0, min(self.p, 1))
        if state.round_number >= self.game_description.n_rounds // 2:
            if num_cooperators >= self.m:
                return Action.D if random.random() < 0.8 else Action.C
            else:
                return Action.C
        elif num_cooperators < self.m:
            return Action.D
        else:
            return Action.C if random.random() < self.p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_101(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy balances cooperation and exploitation based on the number of cooperators in each round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        ew = min(3, max(2, math.floor(self.game_description.n_players / 4)) + math.ceil(state.round_number / self.game_description.n_rounds))
        if history is not None:
            cooperators = sum(history.opponent_actions[-1, :])
        else:
            cooperators = 0
        threshold_met = cooperators >= self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if threshold_met:
            action = Action.C
        elif cooperators < self.game_description.m - ew:
            action = Action.D
        else:
            action = Action.C
        return action

class Strategy_EXPLOITATIVE_102(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit the opponent's behavior by adapting to their level of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_previous_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        self.cooperators_previous_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.game_description.m - self.cooperators_previous_round > 0:
            p_defect = (self.game_description.m - self.cooperators_previous_round) / self.game_description.n_players
        else:
            p_defect = 0
        if random.random() < p_defect:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if self.cooperators_previous_round >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_103(BaseStrategy):
    """
    Adaptive Exploiter strategy for the Collective Risk Dilemma game.

    This strategy balances exploration, adaptation, and exploitation while taking into account the game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if m_t >= self.game_description.m:
            return Action.C
        opponent_cooperation_rate = np.mean(history.opponent_actions[:-1, :])
        if state.round_number < self.game_description.n_rounds / 2 and opponent_cooperation_rate < 0.5:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if m_t >= self.game_description.m - 1 and m_t < self.game_description.m:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_104(BaseStrategy):
    """
    Exploitative strategy that adaptively adjusts cooperation level based on observed behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        p_t = max(0, (self.game_description.m - num_cooperators_prev_round) / self.game_description.n_players)
        action = random.choices([Action.C, Action.D], weights=[p_t, 1 - p_t])[0]
        if history.my_actions[-1] == Action.C:
            self.num_cooperators_prev_round += 1
        return action

class Strategy_EXPLOITATIVE_105(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior while pursuing a threshold of cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        if state.round_number == 0 or (history is not None and len(history.my_actions) == 0):
            return Action.C if random.random() < m_cooperators_needed / n_players else Action.D
        prev_round_outcome = history.opponent_actions[-1, :]
        opponents_cooperated = sum(prev_round_outcome)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if opponents_cooperated >= m_cooperators_needed:
            return Action.C if random.random() < m_cooperators_needed / n_players else Action.D
        x_defectors = n_players - opponents_cooperated
        coop_probability = max(m_cooperators_needed / n_players, (m_cooperators_needed - x_defectors) / n_players)
        return Action.C if random.random() < coop_probability else Action.D

class Strategy_EXPLOITATIVE_106(BaseStrategy):
    """
    Exploitative strategy: Adaptive Threshold Seeker.
    This strategy aims to exploit the game's structure by adaptively seeking the minimum cooperation threshold (m) while minimizing personal costs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m
        self.previous_rounds_met_threshold = 0
        self.total_rounds_played = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_rounds_played += 1
        if history is not None:
            cooperation_level_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            if cooperation_level_last_round >= self.cooperation_threshold:
                self.previous_rounds_met_threshold += 1
        if self.previous_rounds_met_threshold / self.total_rounds_played > 0.5 and self.cooperation_threshold > 1:
            self.cooperation_threshold -= 1
        elif self.previous_rounds_met_threshold / self.total_rounds_played < 0.5 and self.cooperation_threshold < self.game_description.n_players - 1:
            self.cooperation_threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.previous_rounds_met_threshold / self.total_rounds_played >= 0.5:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_107(BaseStrategy):
    """
    This strategy aims to balance the desire to free-ride with the need to contribute 
    to the collective good by adapting to the opponent's behavior and adjusting its 
    cooperation probability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_probability = 0

    def calculate_average_cooperator_payoff(self, history: PlayerHistory) -> float:
        if len(history.my_actions) == 1 or not any(history.my_actions[:-1]):
            return 0
        cooperators_payoffs = [payoff for payoff, action in zip(history.my_payoffs[:-1], history.my_actions[:-1]) if action]
        return np.mean(cooperators_payoffs)

    def calculate_average_defector_payoff(self, history: PlayerHistory) -> float:
        if len(history.my_actions) == 1 or not any((not action for action in history.my_actions[:-1])):
            return 0
        defectors_payoffs = [payoff for payoff, action in zip(history.my_payoffs[:-1], history.my_actions[:-1]) if not action]
        return np.mean(defectors_payoffs)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperate_probability = self.game_description.m / self.game_description.n_players
        else:
            avg_payoff_C = self.calculate_average_cooperator_payoff(history)
            avg_payoff_D = self.calculate_average_defector_payoff(history)
            if avg_payoff_C > avg_payoff_D:
                self.cooperate_probability = 0.7 + 0.3 * (avg_payoff_C / avg_payoff_D) if avg_payoff_D != 0 else 0.7
            elif avg_payoff_C == avg_payoff_D:
                self.cooperate_probability = 0.5
            else:
                self.cooperate_probability = 0.5
            if history is not None and sum(history.opponent_actions[-1, :]) < self.game_description.m:
                self.cooperate_probability += 0.1
            elif history is not None and sum(history.opponent_actions[-1, :]) >= self.game_description.m:
                self.cooperate_probability -= 0.05
            self.cooperate_probability = min(max(self.cooperate_probability, 0.1), 0.9)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < self.cooperate_probability:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_109(BaseStrategy):
    """
    The Adaptive Threshold Tracker strategy aims to exploit opponents by adapting to their cooperation rates while ensuring a stable threshold of cooperation is maintained.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            opponent_cooperations = np.sum(history.opponent_actions, axis=1)
            avg_acr = np.mean(opponent_cooperations / self.game_description.n_players)
            self.acr_history.append(avg_acr)
        if state.round_number == 1:
            return Action.C
        avg_acr = np.mean(self.acr_history) if self.acr_history else 0
        if avg_acr >= 0.5 * self.game_description.m / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds // 2:
            if avg_acr >= 0.7 * self.game_description.m / self.game_description.n_players:
                return Action.C
            elif avg_acr < 0.3 * self.game_description.m / self.game_description.n_players:
                return Action.D
        return action

class Strategy_EXPLOITATIVE_110(BaseStrategy):
    """
    A strategy that balances cooperation with strategic exploitation to maximize its own payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.total_payoff_prev_round = 0
        self.exploit_trigger = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            prev_cooperators = sum(history.opponent_actions[-1, :] > 0)
            total_payoff_prev_round = np.sum(history.my_payoffs[-1] + history.opponent_payoffs[-1])
            if prev_cooperators >= self.game_description.m / 2:
                self.exploit_trigger = False
                return Action.C
            elif total_payoff_prev_round < self.game_description.n_players * (1 + self.game_description.k / 2):
                self.exploit_trigger = True
                return Action.D
            if self.exploit_trigger and prev_cooperators >= self.game_description.m / 2:
                self.exploit_trigger = False
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif history is not None and sum(history.opponent_actions[-1, :] < 0) == 1:
            return Action.C
        elif history is not None and all((np.all(opponent_actions < 0) for opponent_actions in history.opponent_actions)):
            return Action.C
        self.prev_cooperators = prev_cooperators if history else 0
        self.total_payoff_prev_round = total_payoff_prev_round if history else 0
        return Action.D

class Strategy_EXPLOITATIVE_111(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for Collective Risk Dilemma.

    This strategy balances exploitation with adaptability to maximize payoffs.
    It cooperates initially, tracks opponents' cooperation, and adjusts its
    exploitation threshold based on previous round's payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.exploit_threshold = game_description.m - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] is True)
        if self.coop_count >= self.exploit_threshold:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 1:
            payoff = history.my_payoffs[-1]
            if payoff >= self.game_description.k:
                self.exploit_threshold -= 1
            elif payoff < 1 + self.game_description.k / 2:
                self.exploit_threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_112(BaseStrategy):
    """
    Adaptive Exploiter strategy for the Collective Risk Dilemma game.
    
    Initially plays Cooperate to gather information about opponents' strategies. 
    Then, it calculates the average cooperation rate of all opponents and adjusts its own behavior accordingly.
    It also implements a punishment mechanism to deter opponents from exploiting it.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.coop_rate_opponents = []
        self.punishment_flags = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if len(self.coop_rate_opponents) >= 3:
            self.theta = self.update_threshold(self.coop_rate_opponents)
        avg_coop_rate_opponents = np.mean(self.coop_rate_opponents[-3:]) if len(self.coop_rate_opponents) >= 3 else 0
        opponent_defected_recently = any(self.punishment_flags)
        if avg_coop_rate_opponents > self.theta:
            action = Action.C
        elif opponent_defected_recently:
            action = Action.D
        else:
            action = Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            my_payoff = history.my_payoffs[-1]
            opponent_payoff = history.opponent_payoffs[-1, i]
            if opponent_action == 0 and my_payoff < opponent_payoff - self.game_description.k / 2:
                self.punishment_flags[i] = True
            else:
                self.punishment_flags[i] = False
        opponent_cooperations = np.sum(history.opponent_actions[-1, :] == 1)
        self.coop_rate_opponents.append(opponent_cooperations / self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

    def update_threshold(self, coop_rate_opponents):
        avg_coop_rate = np.mean(coop_rate_opponents[-3:]) if len(coop_rate_opponents) >= 3 else 0
        if avg_coop_rate > 0.7:
            self.theta -= 0.05
        elif avg_coop_rate < 0.3:
            self.theta += 0.05
        return max(0, min(self.theta, 1))

class Strategy_EXPLOITATIVE_113(BaseStrategy):
    """
    Exploitative strategy: Adaptive Collective Risk Taker.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators = [0] * game_description.n_players
        self.avg_payoff = []
        self.opp_behavior = [[0, 0] for _ in range(game_description.n_players)]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number < self.game_description.n_players // 2:
            num_coop_prev_round = sum(self.num_cooperators)
            if num_coop_prev_round < self.game_description.m:
                return Action.D
            else:
                p = (num_coop_prev_round / self.game_description.n_players) ** 2
                return Action.C if random.random() < p else Action.D
        avg_payoff_last_n4 = sum(self.avg_payoff[-self.game_description.n_players // 4:]) / (self.game_description.n_players // 4)
        if avg_payoff_last_n4 > 1.5:
            return Action.D
        else:
            num_coop_avg = sum(self.num_cooperators) / self.game_description.n_players ** 2
            p = num_coop_avg ** 2
            return Action.C if random.random() < p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

    def update_state(self, action: Action, history: PlayerHistory):
        if action == Action.C:
            self.num_cooperators[state.round_number % self.game_description.n_players] += 1
        self.avg_payoff.append(calculate_payoff(action, self.num_cooperators))
        for i in range(self.game_description.n_players):
            self.opp_behavior[i][action == Action.C] += 1

    def calculate_payoff(self, action: Action, num_cooperators: list[int]) -> float:
        if sum(num_cooperators) < self.game_description.m:
            return 1 - int(action == Action.C)
        else:
            return 1 - int(action == Action.C) + self.game_description.k

class Strategy_EXPLOITATIVE_114(BaseStrategy):
    """
    The ATE strategy aims to exploit opponents by adapting to their cooperation rates while ensuring a high payoff for itself.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_rate = 0.0
        self.threshold = game_description.m / game_description.n_players - (game_description.k - 1) / (2 * game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < min(3, self.game_description.n_rounds // 2):
            return Action.C
        if history is not None and len(history.opponent_actions) > 0:
            opponents_cooperated = sum(history.opponent_actions[-1, :])
        else:
            opponents_cooperated = 0
        self.coop_rate = 0.5 * self.coop_rate + 0.5 * (opponents_cooperated / (self.game_description.n_players - 1))
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if self.coop_rate >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_115(BaseStrategy):
    """
    The Adaptive Risk Taker strategy balances cooperation and exploitation by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_exploit = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return self.cooperate_with_probability(self.game_description.m / self.game_description.n_players)
        prev_round = history.my_actions[state.round_number - 1]
        opponent_cooperations = sum(history.opponent_actions[state.round_number - 1, :])
        threshold_met = opponent_cooperations + int(prev_round) >= self.game_description.m
        if threshold_met:
            defect_prob = self.p_exploit
        else:
            p_coop = self.game_description.m / self.game_description.n_players
            defect_prob = 1 - p_coop
        if not prev_round and (not threshold_met):
            defect_prob = 1 - self.p_exploit
        if threshold_met and (not prev_round):
            self.p_exploit -= 0.1
        elif not threshold_met and prev_round:
            self.p_exploit += 0.1
        self.p_exploit = max(0, min(self.p_exploit, 1))
        return Action.D if random.random() < defect_prob else Action.C

    def cooperate_with_probability(self, p):
        """
        Randomly choose to cooperate or defect based on the given probability.
        """
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_116(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy combines elements of tit-for-tat and Pavlov strategies, 
    adapting to opponents' behaviors while maintaining a robust exploitative mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.6
        self.payoff_multiplier = 1.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if 1 <= state.round_number < 5:
            num_cooperators_prev = sum(history.opponent_actions[-1, :] | np.array([True])) - 1
            avg_payoff_coops_prev = np.mean(np.append(history.my_payoffs[-1:], history.opponent_payoffs[-1, :]))
            if num_cooperators_prev < self.game_description.m or avg_payoff_coops_prev <= 1 + self.game_description.k / 2:
                return Action.D
            else:
                return Action.C
        elif 5 <= state.round_number < math.ceil(self.game_description.n_rounds / 2):
            opponent_coop_rates = np.array([sum(history.opponent_actions[max(0, -5 + state.round_number):, i]) / min(state.round_number, 5) for i in range(self.game_description.n_players - 1)])
            if max(opponent_coop_rates) > self.cooperation_threshold:
                return Action.C
            elif sum(history.my_actions) / state.round_number < self.cooperation_threshold:
                return Action.D
            else:
                return Action.C
        else:
            total_payoff_coops = np.sum(np.append(history.my_payoffs, history.opponent_payoffs))
            total_payoff_defects = sum([sum(payoffs) for payoffs in history.opponent_payoffs])
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            elif total_payoff_coops > total_payoff_defects * self.payoff_multiplier:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_117(BaseStrategy):
    """
    Adaptive Threshold Escalation strategy for Collective Risk Dilemma.
    
    This strategy balances cooperation and exploitation while adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = 0
        self.total_coop_payoff = 0
        self.total_defect_payoff = 0
        self.prev_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        self.cooperators = sum(prev_round_actions)
        if history.my_payoffs.size > 0:
            my_prev_round_payoff = history.my_payoffs[-1]
            opponents_prev_round_payoffs = history.opponent_payoffs[-1, :]
            if history.my_actions[-1]:
                self.total_coop_payoff += my_prev_round_payoff
            else:
                self.total_defect_payoff += my_prev_round_payoff
        if state.round_number < self.game_description.n_rounds / 2:
            if self.cooperators < self.game_description.m:
                return Action.C
            elif self.prev_round_cooperators is not None and np.mean(history.my_payoffs[-2:]) > my_prev_round_payoff + self.game_description.k:
                return Action.D
            else:
                return Action.C
        elif state.round_number >= self.game_description.n_rounds / 2:
            if self.cooperators < self.game_description.m and self.total_coop_payoff < self.total_defect_payoff:
                return Action.D
            elif self.cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        elif state.round_number > 3 * self.game_description.n_rounds / 4:
            if self.total_coop_payoff > self.total_defect_payoff:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.prev_round_cooperators = self.cooperators
        return Action.C

class Strategy_EXPLOITATIVE_118(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma (ACRD) strategy.

    This strategy is designed to be exploitative, adaptive, and robust.
    It adjusts its cooperation probability based on the observed behavior of opponents,
    aiming to maximize payoff while encouraging others to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        opponent_actions = history.opponent_actions[-1]
        num_cooperators_prev = sum(opponent_actions)
        if num_cooperators_prev < self.game_description.m and (history.my_actions[-1] == Action.C or history.my_payoffs[-1] < 1):
            self.cooperation_probability = max(0, self.cooperation_probability - 0.1 * (self.game_description.m / self.game_description.n_players))
        elif num_cooperators_prev >= self.game_description.m and (history.my_actions[-1] == Action.D or history.my_payoffs[-1] > 1 + self.game_description.k):
            self.cooperation_probability = min(1, self.cooperation_probability + 0.1)
        elif num_cooperators_prev >= self.game_description.m:
            self.cooperation_probability = self.game_description.k / (self.game_description.k + 1)
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_119(BaseStrategy):
    """
    Robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.D
        n_cooperators_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if n_cooperators_prev >= self.game_description.m:
            action = Action.D
        else:
            action = Action.C
        if action == Action.C and n_cooperators_prev < self.game_description.m:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            avg_payoff_per_round = np.mean(history.my_payoffs) / state.round_number
            if history.my_payoffs[-1] < avg_payoff_per_round:
                return Action.C
        return action

class Strategy_EXPLOITATIVE_120(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperation_rates = np.mean(history.opponent_actions[:, :], axis=0)
        cooperators = []
        defectors = []
        mixed_strategists = []
        for i, rate in enumerate(opponent_cooperation_rates):
            if rate > 0.75:
                cooperators.append(i)
            elif rate < 0.25:
                defectors.append(i)
            else:
                mixed_strategists.append(i)
        num_cooperators_last_round = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators_last_round >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        for opponent in range(self.game_description.n_players):
            if opponent in cooperators:
                return Action.D
            elif opponent in defectors:
                if num_cooperators_last_round >= self.game_description.m - 1:
                    return Action.C
                else:
                    return Action.D
            elif opponent in mixed_strategists:
                mirrored_action = history.opponent_actions[-1, opponent]
                return Action.C if mirrored_action else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_121(BaseStrategy):
    """
    Threshold Tracker with Adaptive Defection strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_round_defections = False
        self.avg_cooperators = 0.0
        self.prev_round_actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :] + [Action.C]) / self.game_description.n_players
        self.avg_cooperators = self.avg_cooperators * 0.7 + m_t
        if self.avg_cooperators >= self.game_description.m / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if self.prev_round_defections and action == Action.D:
            opponent_response = np.mean(history.opponent_actions[-1, :]) < 0.5
            if opponent_response:
                action = Action.C
        self.prev_round_defections = action == Action.D
        self.prev_round_actions.append(action)
        if state.round_number >= self.game_description.n_rounds - 5:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_122(BaseStrategy):
    """
    An adaptive exploitative strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.theta = 0.5
        self.alpha = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        c_prev = sum(history.opponent_actions[-1, :]) / n_players
        self.theta = (1 - self.alpha) * self.theta + self.alpha * c_prev
        prob_m_or_more_cooperate = 1 if c_prev >= m / n_players else c_prev ** m
        expected_payoff_C = 1 - 0.5 + k * prob_m_or_more_cooperate
        cooperate_prob = self.p_init if state.round_number == 1 else 0 if c_prev < self.theta else int(expected_payoff_C > 1)
        if state.round_number == r - 1:
            return Action.D
        if n_players == 2:
            return history.opponent_actions[-1, 0] * Action.C + (not history.opponent_actions[-1, 0]) * Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_123(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma Exploiter (ACRDE) strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round = 0
        self.previous_cooperators = []
        self.payoffs = []
        self.action_frequency = [True, False]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            self.round += 1
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            self.previous_cooperators.append(num_cooperators_prev_round)
            if self.round <= 5:
                action = Action.C if self.action_frequency[self.round % 2] else Action.D
            elif num_cooperators_prev_round < self.game_description.m:
                action = Action.D
            else:
                action = Action.C
            if history.my_actions[-1] == True:
                self.payoffs.append(0 + self.game_description.k * (num_cooperators_prev_round >= self.game_description.m))
            else:
                self.payoffs.append(1 + self.game_description.k * (num_cooperators_prev_round >= self.game_description.m))
            avg_payoff_last_5_rounds = sum(self.payoffs[-min(5, len(self.payoffs)):]) / min(5, len(self.payoffs))
            if avg_payoff_last_5_rounds < 1.5:
                self.action_frequency = [False, True]
            elif avg_payoff_last_5_rounds >= 1.5 and self.action_frequency != [True, False]:
                self.action_frequency = [True, False]
        return action

class Strategy_EXPLOITATIVE_124(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit 
    the collective risk dilemma by adapting to the evolving cooperation levels 
    of opponents while ensuring robustness against various behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_level = 0
        self.prev_actions = []
        self.p = (game_description.k - 1) / (game_description.k * (game_description.m - 1)) if game_description.m > 1 else 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_players == 2 and state.round_number < self.game_description.n_rounds - 1:
            return Action.C
        if self.game_description.m == 1:
            return Action.C
        if state.round_number == 0 or history is None:
            action = Action.C
        else:
            self.cooperate_level = sum(history.my_actions[:state.round_number])
            if self.cooperate_level < self.game_description.m:
                action = Action.D
            elif state.round_number == self.game_description.n_rounds - 1:
                action = Action.D
            else:
                action = Action.C if random.random() < self.p else Action.D
        self.prev_actions.append(action)
        return action

class Strategy_EXPLOITATIVE_125(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy balances cooperation and exploitation.
    It initially probes the opponent's behavior with a probability-based cooperation,
    then adapts to the game dynamics by adjusting its exploitation threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploitation_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        my_previous_action = history.my_actions[-1]
        if previous_round_cooperators >= self.exploitation_threshold:
            self.exploitation_threshold = max(1, self.exploitation_threshold - 1)
        else:
            self.exploitation_threshold = min(self.game_description.n_players - 1, self.exploitation_threshold + 1)
        if previous_round_cooperators >= self.game_description.m or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if previous_round_cooperators >= self.exploitation_threshold and random.random() < (self.game_description.k - 1) / (self.game_description.k + 1):
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_126(BaseStrategy):
    """
    The Threshold Tracker strategy is designed to adaptively exploit opponents' behavior 
    while ensuring a high payoff for itself. This strategy relies solely on game parameters 
    and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        m_prev = sum(history.opponent_actions[-1, :] + [True])
        if m_prev < self.game_description.m:
            return Action.D
        if state.round_number % 2 == 0:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if m_prev >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_127(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma game.
    This strategy adapts to various opponent behaviors while maintaining an exploitative mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_hat = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if previous_round_cooperators > self.m_hat:
            self.m_hat = previous_round_cooperators
        p = 0.0
        q = self.game_description.m / self.game_description.n_players * (1 + (self.game_description.k - 1) / self.game_description.n_players)
        if previous_round_cooperators >= self.game_description.m:
            p = 1 - (self.game_description.k - 1) / (self.game_description.n_players - self.game_description.m + 1)
            return Action.D if random.random() < p else Action.C
        else:
            return Action.C if random.random() < q else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_128(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        x = sum(history.opponent_actions[-1, :] if history is not None else []) + int(self.game_description.n_players > 2 and history.my_actions[-1] if history is not None and len(history.my_actions) > 0 else False)
        if state.round_number == self.game_description.n_rounds - 1:
            if x >= self.game_description.m - 1 and (history is None or history.my_actions[-1] == 0):
                return Action.C
        elif x >= self.game_description.m and (history is not None and history.my_actions[-1] == 0):
            return Action.D
        elif x < self.game_description.m and (history is not None and history.my_actions[-1] == 0):
            if sum((x for x in [sum(history.opponent_actions[t, :] if history else []) + int(self.game_description.n_players > 2 and history.my_actions[t] if history else False) for t in range(state.round_number)])) / state.round_number < self.game_description.m:
                return Action.C
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_129(BaseStrategy):
    """
    Exploitative strategy that adaptively tracks the threshold of cooperative players and exploits non-cooperative behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.m_hat = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        self.coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if self.coop_count >= self.game_description.m:
            self.m_hat = max(self.game_description.m, self.coop_count - 1)
        punish_next_round = False
        if history.my_payoffs.shape[0] > 0 and state.round_number > 1:
            if history.my_actions[-2] == Action.D and self.coop_count >= self.game_description.m:
                punish_next_round = True
        if state.round_number < self.game_description.n_rounds - 1:
            any_player_defected = not np.all(history.opponent_actions[-1, :]) or history.my_actions[-1] == Action.D
            if self.coop_count >= self.m_hat and any_player_defected:
                return Action.C
        else:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_130(BaseStrategy):
    """
    An exploitative strategy that adapts to various opponent behaviors in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.previous_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        p_defect = min(1, (self.game_description.m - self.previous_cooperators) / self.game_description.n_players)
        if random.random() < p_defect:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_131(BaseStrategy):
    """
    The ATE strategy exploits opponents' behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_avg = 0
        self.history_buffer = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= n // 2:
            num_cooperators = sum(history.opponent_actions[-1, :])
            self.history_buffer.append((state.round_number, num_cooperators))
            return Action.D
        else:
            if not self.m_avg:
                cooperator_sums = [x[1] for x in self.history_buffer]
                self.m_avg = np.mean(cooperator_sums)
            num_cooperators = sum(history.opponent_actions[-1, :])
            action = Action.D
            if self.m_avg >= m:
                action = Action.C
            elif abs(num_cooperators - self.m_avg) > 0.2 * m:
                if num_cooperators < self.m_avg:
                    action = Action.D
                else:
                    action = Action.C
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return action

class Strategy_EXPLOITATIVE_132(BaseStrategy):
    """
    Robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        if state.round_number < math.ceil(self.game_description.n_rounds / 4):
            p = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators < self.game_description.m:
            self.punish_counter = 2
            return Action.D
        else:
            p = self.game_description.k / (self.game_description.k + 1)
            return Action.C if random.random() < p else Action.D
        if history.my_actions[-1] and prev_cooperators < self.game_description.m:
            self.punish_counter -= 1
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        for i in range(self.game_description.n_players):
            if opponent_cooperation_rates[i] == 1:
                p = 0.7
                return Action.C if random.random() < p else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_133(BaseStrategy):
    """
    Exploit the collective risk dilemma by initially cooperating to gather information,
    then monitoring and adapting to maintain or exceed the threshold, 
    while exploiting opponents' behavior in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.total_payoff_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if history is not None:
            self.total_payoff_prev_round = np.sum(history.my_payoffs[-1:]) + np.sum(history.opponent_payoffs[-1, :])
        if self.prev_cooperators >= self.game_description.m:
            cooperate = True
        else:
            cooperate = False
        if state.round_number == self.game_description.n_rounds - 1 and self.prev_cooperators >= self.game_description.m and (self.total_payoff_prev_round >= self.game_description.n_players * self.game_description.k):
            cooperate = False
        if state.round_number < self.game_description.n_rounds - 1 and self.prev_cooperators < self.game_description.m:
            diff = self.game_description.m - self.prev_cooperators
            if diff > 1:
                p = (self.game_description.m - self.prev_cooperators + 1) / self.game_description.n_players
                cooperate = random.random() < p
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_134(BaseStrategy):
    """
    An adaptive exploitative strategy for the Collective Risk Dilemma game.
    
    The strategy initially probes the willingness of other players to cooperate and 
    then adjusts its cooperation rate based on observed outcomes, aiming to maximize 
    individual payoffs by exploiting opportunities where others are willing to bear 
    the cost of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize the Adaptive Exploiter strategy.

        Args:
        - game_description (CollectiveRiskDescription): The description of the game.
        """
        self.game_description = game_description
        self.cooperation_rate = game_description.m / game_description.n_players
        self.min_cooperation_rate = 0.2
        self.max_adjustment = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Make a decision based on the current game state and history.

        Args:
        - state (GameState): The current state of the game.
        - history (None | PlayerHistory): The history of previous rounds.

        Returns:
        - Action: The action to take in this round (C or D).
        """
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_rate else Action.D
        opponent_payoffs_last_round = history.opponent_payoffs[-1, :]
        avg_payoff = np.mean(opponent_payoffs_last_round)
        threshold_met = sum(history.my_actions[-1:]) >= self.game_description.m
        if threshold_met and avg_payoff > 1:
            if self.cooperation_rate < self.game_description.m / self.game_description.n_players:
                self.cooperation_rate = min(self.cooperation_rate + self.max_adjustment, self.game_description.m / self.game_description.n_players)
        else:
            self.cooperation_rate = max(self.cooperation_rate - self.max_adjustment, self.min_cooperation_rate)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if sum(history.my_actions) >= self.game_description.m else Action.D
        return Action.C if random.random() < self.cooperation_rate else Action.D

class Strategy_EXPLOITATIVE_135(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances exploration, exploitation, and cooperation to maximize payoffs while adapting to the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishable_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == 1:
            if any((not action for action in history.opponent_actions[0])):
                return Action.D
            else:
                return Action.C
        m_t_1 = sum(history.opponent_actions[-1, :])
        n_m_t_1 = self.game_description.n_players - 1 - m_t_1
        if m_t_1 >= self.game_description.m:
            return Action.C
        elif m_t_1 < self.game_description.m and n_m_t_1 > 0:
            return Action.D
        else:
            return Action.C
        if history.my_payoffs[-1] == 0 and any((action for action in history.opponent_actions[-1])):
            self.punishable_rounds += 1
            if self.punishable_rounds >= 2:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if m_t_1 >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_136(BaseStrategy):
    """
    The Adaptive Risk Taker (ART) strategy is designed to exploit a wide range of opponent behaviors 
    while adapting to changing game conditions. ART balances cooperation and defection to maximize individual payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.coop_prob = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            cooperate = random.random() < self.coop_prob
        else:
            num_coop = sum(history.opponent_actions[-1, :])
            self.avg_coop = (self.avg_coop * 2 + num_coop) / 3
            if self.avg_coop >= self.game_description.m:
                self.coop_prob = self.game_description.k / (self.game_description.k + 1)
            else:
                self.coop_prob = 0.5
            if state.round_number > 3 and sum(history.opponent_actions[-2:, :].sum(axis=1)) < self.game_description.m * 2:
                self.coop_prob += 0.1
            elif state.round_number > 3 and sum(history.opponent_actions[-2:, :].sum(axis=1)) >= self.game_description.m * 2:
                self.coop_prob -= 0.1
            self.coop_prob = max(0, min(self.coop_prob, 1))
            cooperate = random.random() < self.coop_prob
        if state.round_number == self.game_description.n_rounds - 1:
            if sum(history.opponent_actions[-2, :]) < self.game_description.m - 1:
                cooperate = False
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_137(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma.
    
    This strategy aims to exploit opponents' behaviors by adapting to the game's dynamics and encouraging cooperation while minimizing losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.thr = game_description.m
        self.prev_coops = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        self.prev_coops.append(prev_coops)
        if state.round_number <= 5:
            action = Action.C if prev_coops >= math.ceil(self.game_description.m / 2) else Action.D
        elif state.round_number <= math.floor(self.game_description.n_rounds / 2):
            action = Action.C if prev_coops >= self.thr else Action.D
            if prev_coops < self.thr:
                self.thr -= 1
        elif state.round_number == self.game_description.n_rounds:
            return Action.D
        else:
            coop_payoff_last_round = 1 - int(history.my_actions[-1]) + self.game_description.k if history.my_actions[-1] else 1
            avg_defect_payoffs = np.mean([1 for _ in range(3)]) if state.round_number >= 3 else np.mean([1])
            action = Action.C if coop_payoff_last_round >= avg_defect_payoffs else Action.D
        return action

class Strategy_EXPLOITATIVE_138(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy adapts to the behavior of opponents and prioritizes individual payoff maximization.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.reliable_cooperators = []
        self.avg_payoff_c = 0
        self.avg_payoff_d = 0
        self.opponent_frequencies = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        elif 3 <= state.round_number < self.game_description.n_rounds / 2:
            num_cooperators = np.sum(history.opponent_actions[-1, :])
            if num_cooperators < self.game_description.m:
                return Action.D
            else:
                prev_payoffs_c = history.my_payoffs[-1] + self.game_description.k if history.my_actions[-1] else 0
                prev_payoffs_d = history.my_payoffs[-1] if not history.my_actions[-1] else 0
                avg_payoff_c = np.mean(prev_payoffs_c)
                avg_payoff_d = np.mean(prev_payoffs_d)
                if avg_payoff_c > avg_payoff_d:
                    return Action.C
                else:
                    return Action.D
        elif self.game_description.n_rounds / 2 <= state.round_number < 3 * self.game_description.n_rounds / 4:
            for i in range(self.game_description.n_players - 1):
                self.opponent_frequencies[i] = np.mean(history.opponent_actions[-self.game_description.n_rounds // 4:, i])
            reliable_cooperators = [i for i, freq in enumerate(self.opponent_frequencies) if freq >= 0.75]
            if len(reliable_cooperators) < self.game_description.m:
                return Action.D
            else:
                expected_payoff_c = history.my_payoffs[-1] + self.game_description.k
                expected_payoff_d = history.my_payoffs[-1]
                if expected_payoff_c > expected_payoff_d:
                    return Action.C
                else:
                    return Action.D
        elif 3 * self.game_description.n_rounds / 4 <= state.round_number < self.game_description.n_rounds:
            num_cooperators = np.sum(history.opponent_actions[-1, :])
            if num_cooperators < self.game_description.m:
                return Action.D
            else:
                expected_payoff_c = history.my_payoffs[-1] + self.game_description.k * (self.game_description.n_rounds - state.round_number)
                expected_payoff_d = history.my_payoffs[-1] * (self.game_description.n_rounds - state.round_number)
                if expected_payoff_c > expected_payoff_d:
                    return Action.C
                else:
                    return Action.D
        elif state.round_number == self.game_description.n_rounds:
            return Action.D

class Strategy_EXPLOITATIVE_139(BaseStrategy):
    """
    Exploit the Collective Risk Dilemma by adapting to game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploitation_mode = False
        self.successful_thresholds = 0
        self.total_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history and len(history.my_payoffs) > 1:
            self.total_rounds += 1
            if history.my_payoffs[-1] > 1 - self.game_description.k:
                self.successful_thresholds += 1
        if state.round_number < int(0.2 * self.game_description.n_rounds):
            return Action.C
        elif not self.exploitation_mode:
            threshold_assessment = self.successful_thresholds / max(self.total_rounds, 1)
            if threshold_assessment > 0.7:
                self.exploitation_mode = True
                return Action.D
            else:
                return Action.C
        elif self.exploitation_mode:
            if state.round_number == self.game_description.n_rounds - 1 or (history and len(history.my_payoffs) > 2 and (history.my_payoffs[-2] <= 1) and (history.my_payoffs[-1] <= 1)):
                self.exploitation_mode = False
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_140(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while maintaining a conditional cooperative approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0
        self.dynamic_threshold = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        my_action_last_round = bool(history.my_actions[-1])
        self.cooperation_rate = (sum(opponent_actions_last_round) + int(my_action_last_round)) / (len(opponent_actions_last_round) + 1)
        if self.cooperation_rate > self.dynamic_threshold or (not my_action_last_round and self.cooperation_rate < self.dynamic_threshold):
            return Action.C
        elif self.cooperation_rate <= self.dynamic_threshold or (my_action_last_round and self.cooperation_rate > self.dynamic_threshold):
            return Action.D
        if self.cooperation_rate > self.dynamic_threshold:
            if state.round_number > 1 and sum(history.opponent_actions[-2, :]) / len(history.opponent_actions[-2, :]) + int(bool(history.my_actions[-2])) > self.dynamic_threshold:
                self.dynamic_threshold -= 0.1
        elif state.round_number > 1 and sum(history.opponent_actions[-2, :]) / len(history.opponent_actions[-2, :]) + int(bool(history.my_actions[-2])) < self.dynamic_threshold:
            self.dynamic_threshold += 0.1
        for opponent in range(len(opponent_actions_last_round)):
            if state.round_number > 3 and sum((1 - bool(x) for x in history.opponent_actions[-4:, opponent])) == 4:
                self.dynamic_threshold += 0.1
        if state.round_number + 1 == self.game_description.n_rounds:
            if my_action_last_round and self.cooperation_rate > self.dynamic_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_141(BaseStrategy):
    """
    Adaptive Threshold Exploiter Strategy.
    
    Cooperates in initial rounds, then adapts to opponent behavior by cooperating 
    if enough opponents cooperate or defecting if not enough do so.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.OCR_threshold = 0.6
        self.ODR_threshold = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        m_t_1 = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if m_t_1 >= self.game_description.m:
            OCR = np.mean(history.opponent_actions[-1, :])
            if OCR > self.OCR_threshold:
                return Action.C
            else:
                return Action.D
        else:
            ODR = 1 - np.mean(history.opponent_actions[-1, :])
            if ODR > self.ODR_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_142(BaseStrategy):
    """
    Exploitative strategy "Threshold-Adaptive Defector" (TAD) that balances exploitation with caution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action)) + (history.my_actions[-1] if history.my_actions.size > 0 else False)
        avg_payoff_prev = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        if prev_cooperators < self.game_description.m:
            return Action.C
        elif avg_payoff_prev > 1 + self.game_description.k / 2:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_143(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to exploit the collective risk dilemma by adaptively adjusting its cooperation threshold based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m / self.game_description.n_players < 0.5 else Action.D
        coop_rate = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]]) / self.game_description.n_players
        if coop_rate >= self.game_description.m / self.game_description.n_players:
            self.theta -= 0.01
        else:
            self.theta += 0.01
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if coop_rate >= self.game_description.m / self.game_description.n_players and self.theta > coop_rate else Action.D

class Strategy_EXPLOITATIVE_145(BaseStrategy):
    """
    This strategy aims to maximize individual payoff by adapting to the game's history and exploiting opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.consecutive_failures = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        last_round_opponent_actions = history.opponent_actions[-1, :]
        num_cooperators_last_round = sum(last_round_opponent_actions) + (history.my_actions[-1] == True)
        if state.round_number > 1:
            self.threshold_met = num_cooperators_last_round >= self.game_description.m
            if self.threshold_met:
                self.consecutive_failures = 0
            else:
                self.consecutive_failures += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.consecutive_failures >= 2:
            return Action.C if state.round_number % 2 else Action.D
        num_defectors_last_round = self.game_description.n_players - num_cooperators_last_round
        cooperation_probability = self.game_description.m / self.game_description.n_players if state.round_number == 1 else 1 - num_defectors_last_round / self.game_description.n_players if self.threshold_met else 0
        return Action.C if random.random() < cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_146(BaseStrategy):
    """
    Exploitative strategy that adaptively tracks the threshold of cooperators needed and exploits opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_avg = 0
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] + [Action.C.value])
        if previous_cooperators < self.threshold:
            action = Action.D
        elif previous_cooperators >= self.threshold:
            payoff_c = 1 - 1 + self.game_description.k
            payoff_d = 1 - 0 + self.game_description.k
            action = Action.C if payoff_c > payoff_d else Action.D
        self.cooperators_avg += previous_cooperators / self.game_description.n_players
        if state.round_number > 1 and self.cooperators_avg / state.round_number >= self.threshold:
            self.threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C if 1 - 1 + self.game_description.k > 1 - 0 + self.game_description.k else Action.D
        return action

class Strategy_EXPLOITATIVE_147(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    
    This strategy starts with a high cooperation probability and adjusts it based on 
    the observed behavior of other players. It aims to maximize its payoff by 
    identifying opportunities to cooperate or defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperation_probability = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < self.cooperation_probability else Action.D
        if history is not None and len(history.my_actions) > 0:
            cooperators_in_last_round = np.sum(~history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            threshold = self.game_description.m / self.game_description.n_players * (cooperators_in_last_round / self.game_description.n_players)
            if cooperators_in_last_round >= self.game_description.m:
                self.cooperation_probability += 0.1
            else:
                self.cooperation_probability -= 0.2
            self.cooperation_probability = max(0, min(self.cooperation_probability, 1))
            if state.round_number == self.game_description.n_rounds - 1 or cooperators_in_last_round < threshold:
                return Action.D
            else:
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_148(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy exploits opponents by identifying opportunities to defect when the threshold is likely to be met, and cooperating otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperations = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if self.cooperations is None:
            self.cooperations = [int(action) for action in history.my_actions]
        expected_cooperators = (self.game_description.m - 1) * sum(self.cooperations) / (self.game_description.n_players * state.round_number)
        if expected_cooperators >= self.game_description.m:
            p_D = max(0, (expected_cooperators - self.game_description.m) / (self.game_description.n_players - self.game_description.m))
            return Action.D if random.random() < p_D else Action.C
        return Action.C
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        elif sum(self.cooperations) / (self.game_description.n_players * state.round_number) < 0.2:
            p_C = min(1, (self.game_description.m - sum(self.cooperations)) / (self.game_description.n_players * state.round_number))
            return Action.C if random.random() < p_C else Action.D
        self.cooperations.append(int(history.my_actions[-1]))

class Strategy_EXPLOITATIVE_149(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_opponents_actions = history.opponent_actions[-1, :]
        cooperators = np.sum(prev_round_opponents_actions)
        self.cooperators_history.append(cooperators)
        converged = False
        if len(self.cooperators_history) > 2 and abs(self.cooperators_history[-1] - self.game_description.m) <= 2:
            converged = True
        if converged or (state.round_number >= self.game_description.n_rounds - 2 and cooperators >= self.game_description.m):
            return Action.C
        elif cooperators < self.game_description.m:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_150(BaseStrategy):
    """
    An adaptive strategy that exploits cooperation dynamics in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_count = 0
        self.coop_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.n_players <= 3:
                return Action.C
            else:
                return random.choice([Action.C, Action.D])
        prev_m_count = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        prev_action = Action.C if history.my_actions[-1] else Action.D
        if prev_m_count >= self.game_description.m:
            self.coop_prob = 1.0
        elif prev_m_count >= self.game_description.m - 1:
            self.coop_prob = 0.8 * self.game_description.k / (1 + self.game_description.k)
        else:
            self.coop_prob = 0.2
        if prev_action == Action.D and prev_m_count >= self.game_description.m:
            self.coop_prob *= 0.2 * self.game_description.k / (1 + self.game_description.k)
        elif prev_action == Action.C and prev_m_count < self.game_description.m:
            self.coop_prob += 0.1 * (1 - self.game_description.k / (1 + self.game_description.k))
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_m_count >= self.game_description.m - 1:
                self.coop_prob = 1.0
        action = Action.C if random.random() < self.coop_prob else Action.D
        self.m_count += 1 if action == Action.C else 0
        return action

class Strategy_EXPLOITATIVE_151(BaseStrategy):
    """
    An exploitative strategy that adapts to the behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.punish_non_coop = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :])
        if state.round_number <= 5:
            self.punish_non_coop = False
            return Action.D
        elif prev_round_coops >= self.game_description.m:
            self.coop_count += sum([1 for action in history.opponent_actions[-1, :] if action])
            if self.coop_count > (self.game_description.n_players - self.game_description.m) / 2:
                return Action.C
            else:
                return Action.D
        elif self.punish_non_coop:
            self.punish_non_coop = False
            return Action.D
        else:
            self.punish_non_coop = True
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.coop_count > (self.game_description.n_players - self.game_description.m) / 2 and prev_round_coops < self.game_description.m:
            return Action.D

class Strategy_EXPLOITATIVE_153(BaseStrategy):
    """
    Exploitative strategy that adapts to the behavior of other players by adjusting its cooperation threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :] if history.opponent_actions.shape[0] > 1 else [True])
        if prev_cooperators < self.game_description.m:
            self.threshold += 1
        elif prev_cooperators >= self.game_description.m and self.threshold > self.game_description.m:
            self.threshold = self.game_description.m
        return Action.C if prev_cooperators >= self.threshold else Action.D

class Strategy_EXPLOITATIVE_154(BaseStrategy):
    """
    The Adaptive Threshold Hunter strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_number = 0
        self.cooperators_count = []
        self.threshold_estimate = 0
        self.opponent_cooperation_rate_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        self.round_number = state.round_number
        cooperators_in_last_round = sum(history.opponent_actions[-1, :] == True)
        self.cooperators_count.append(cooperators_in_last_round)
        if len(self.cooperators_count) > 2:
            self.threshold_estimate = np.mean(self.cooperators_count[:-1])
        opponent_cooperation_rate = sum(history.opponent_actions[-1, :] == True) / self.game_description.n_players
        if opponent_cooperation_rate >= self.opponent_cooperation_rate_threshold and cooperators_in_last_round + 1 >= math.ceil(self.threshold_estimate):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_155(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma (ACRD) Strategy.

    This strategy is designed to exploit potential cooperation among opponents while minimizing losses.
    It initially cooperates and monitors the group's behavior, identifying opportunities to build mutually beneficial relationships or defect when necessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.0
        self.total_coop_payoffs = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 5:
            prev_round_coops = sum(history.opponent_actions[-1, :] & (history.my_actions[-1] == False)) + history.my_actions[-1]
            if prev_round_coops >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        self.cooperation_rate = self.total_coop_payoffs / state.round_number
        opponent_coops_last_round = sum(history.opponent_actions[-1, :])
        if opponent_coops_last_round > self.game_description.m:
            return Action.C
        elif self.cooperation_rate > 0.5:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_156(BaseStrategy):
    """
    The Adaptive Threshold Tracker (ATT) strategy balances cooperation and exploitation in the Collective Risk Dilemma game.
    It adapts to the opponent's behavior and adjusts its decision-making based on the game's history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = None
        self.defect_count = None
        self.threshold = (game_description.m - 1) / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.coop_count = sum(history.opponent_actions[-1, :] == True)
        self.defect_count = self.game_description.n_players - self.coop_count - 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        exploitation_opportunity = False
        if history.opponent_actions.shape[0] > 1:
            for i in range(self.game_description.n_players - 1):
                if not history.opponent_actions[-1, i] and (not history.opponent_actions[-2, i]):
                    exploitation_opportunity = True
                    break
        if self.coop_count >= self.threshold or (self.coop_count == math.ceil(self.threshold) and random.random() < 0.5):
            return Action.C
        elif exploitation_opportunity:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_157(BaseStrategy):
    """
    An adaptive exploiter strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploration_phase_length = math.ceil(game_description.n_rounds / 3)
        self.consistent_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number <= self.exploration_phase_length:
            return Action.C if state.round_number % 2 != 0 else Action.D
        if self.consistent_cooperators is None:
            opponent_actions = history.opponent_actions[:self.exploration_phase_length, :]
            self.consistent_cooperators = np.all(opponent_actions == True, axis=0)
            self.consistent_cooperators = np.sum(self.consistent_cooperators) >= self.game_description.m - 1
        if self.consistent_cooperators:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_158(BaseStrategy):
    """
    Adaptive Exploiter strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.m / self.n_players else Action.D
        opponent_behaviors = np.mean(history.opponent_actions, axis=0)
        if state.round_number < self.r // 2:
            avg_coop_rate = np.mean(np.concatenate([history.my_actions, history.opponent_actions.flatten()]))
            if avg_coop_rate > self.m / self.n_players:
                return Action.C if random.random() < 1 - (self.k - 1) / (self.n_players - self.m) else Action.D
        elif state.round_number < 3 * self.r // 4:
            max_cooperator = np.argmax(opponent_behaviors)
            if opponent_behaviors[max_cooperator] > 0.75:
                return Action.D if random.random() < 1 - (self.k - 1) / (self.n_players - self.m) else Action.C
        elif state.round_number == self.r - 1:
            return Action.D
        else:
            num_cooperators = np.sum(np.concatenate([history.my_actions, history.opponent_actions.flatten()]))
            if num_cooperators >= self.m:
                return Action.D
            avg_coop_rate = np.mean(np.concatenate([history.my_actions, history.opponent_actions.flatten()]))
            if avg_coop_rate > self.m / self.n_players and self.k > 2.5:
                return Action.C
            elif self.m - 1 < num_cooperators < self.m:
                return Action.C if random.random() < 0.5 else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_159(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for Collective Risk Dilemma.

    This strategy is designed to exploit the game's parameters and history while adapting to various opponent behaviors.
    It aims to maximize individual payoff by encouraging cooperation when beneficial, punishing repeated defection,
    exploiting free-riders and potential cooperation failures, and adapting to changes in the game environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponents_defected = [False] * game_description.n_players
        self.history = []
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        if sum(prev_round_opponent_actions) >= self.game_description.m:
            return Action.C
        elif sum(~prev_round_opponent_actions) > self.game_description.n_players / 2:
            return Action.D
        elif any(self.opponents_defected):
            return Action.D
        if state.round_number % 3 == 0 and state.round_number != 0:
            self.avg_payoff = np.mean(history.my_payoffs[-3:])
            if self.avg_payoff < self.game_description.k:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif sum(~prev_round_opponent_actions) == 1:
            return Action.C
        elif all((sum(history.opponent_actions[-i, :]) < self.game_description.m for i in range(3))):
            return Action.D
        return Action.C

    def __post_init__(self):
        self.opponents_defected = np.array(self.opponents_defected, dtype=bool)

    def update_opponents_defected(self, history: PlayerHistory) -> None:
        prev_round_actions = history.opponent_actions[-1, :]
        self.opponents_defected[~prev_round_actions] = True

class Strategy_EXPLOITATIVE_160(BaseStrategy):
    """
    Exploitative strategy that takes advantage of opponents' cooperation in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(opponent_actions)
        if num_cooperators < self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_161(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.5
        self.success_rate = 0.5
        self.coop_threshold = 0.4
        self.success_threshold = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        total_cooperators = np.sum(opponent_cooperations) + (history.my_actions[-1] == 1)
        self.cooperation_rate = np.mean(total_cooperators >= self.game_description.m)
        self.success_rate = np.mean(np.sum(opponent_cooperations, axis=0) >= self.game_description.m)
        if state.round_number <= self.game_description.n_rounds / 10:
            return random.choice([Action.C, Action.D])
        if self.cooperation_rate > self.coop_threshold and self.success_rate < self.success_threshold:
            return Action.D
        elif self.success_rate > self.success_threshold and self.cooperation_rate < self.coop_threshold:
            return Action.D
        if state.round_number > self.game_description.n_rounds - self.game_description.n_rounds / 10:
            if self.success_rate > self.success_threshold:
                return Action.C
            else:
                return random.choice([Action.C, Action.D])
        return Action.C

class Strategy_EXPLOITATIVE_162(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    It initially explores by cooperating, then adapts to opponents' strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_rate = 0.5
        self.window_size = min(game_description.n_rounds // 4, 10)
        self.num_cooperators_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.num_cooperators_prev is not None and state.round_number < self.game_description.n_rounds - 1:
            avg_cooperate_rate = np.mean(np.concatenate((history.my_actions[1:], history.opponent_actions[:-1].mean(axis=1)))[:self.window_size])
            if num_cooperators_prev < self.game_description.m:
                return Action.D
            elif avg_cooperate_rate < self.cooperate_rate:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1 and num_cooperators_prev < self.game_description.m:
            return Action.C
        self.num_cooperators_prev = num_cooperators_prev
        return Action.C

class Strategy_EXPLOITATIVE_163(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = game_description.m
        self.prev_cooperators = None
        self.prev_defectors = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_actions = history.opponent_actions[-1]
        prev_payoffs = history.opponent_payoffs[-1]
        self.prev_cooperators = sum(prev_actions)
        self.prev_defectors = len(prev_actions) - self.prev_cooperators
        avg_payoff_coop_prev = np.mean(prev_payoffs[prev_actions]) if prev_actions.any() else 0.0
        avg_payoff_defect_prev = np.mean(prev_payoffs[np.logical_not(prev_actions)]) if (~prev_actions).any() else 0.0
        delta_theta = self.prev_cooperators - self.prev_defectors
        self.theta = max(1, min(self.game_description.m - 1, self.theta + delta_theta))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.prev_cooperators < self.game_description.m:
            return Action.D
        elif avg_payoff_coop_prev > avg_payoff_defect_prev:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_164(BaseStrategy):
    """
    An adaptive strategy that exploits the opponent's behavior in a Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_cooperators < self.game_description.m:
            return Action.D
        if self.game_description.k > 1.5 and prev_cooperators >= self.game_description.m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_165(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma Exploiter (ACRDE) - a strategy to balance exploration, adaptation, and exploitation 
    in the collective risk dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []
        self.phase = 'initial'
        self.opponent_thresholds = [1] * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or len(self.history) == 0:
            self.history.append({'cooperators': 0, 'opponent_actions': history.opponent_actions[0] if history else [False], 't': state.round_number})
            return Action.C
        prev_round = self.history[-1]
        prev_round['cooperators'] = sum(prev_round['opponent_actions'])
        self.history.append({'cooperators': 0, 'opponent_actions': history.opponent_actions[-1], 't': state.round_number})
        if self.phase == 'initial':
            action = Action.C
            self.phase = 'exploration'
        elif self.phase == 'exploration':
            t = len(self.history)
            action = Action.C if t % 2 == 0 else Action.D
            if t >= math.ceil(self.game_description.n_rounds / 3):
                self.phase = 'exploitation'
        elif self.phase == 'exploitation':
            avg_coop_rate = sum([round['cooperators'] for round in self.history]) / len(self.history)
            if avg_coop_rate < self.game_description.m / 2:
                action = Action.D
            else:
                prev_round_coop_rate = self.history[-2]['cooperators']
                if prev_round_coop_rate < self.game_description.m:
                    action = Action.C
                else:
                    action = Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            if history.my_payoffs.shape[0] > 1 and opponent_action != history.opponent_actions[-2, i]:
                prev_round_opponent_thresholds = self.opponent_thresholds.copy()
                prev_round_opponent_thresholds[i] += 1 if opponent_action else -1
                prev_round_opponent_thresholds[i] = max(1, min(prev_round_opponent_thresholds[i], self.game_description.m))
                self.opponent_thresholds = prev_round_opponent_thresholds
        if state.round_number == self.game_description.n_rounds - 1:
            current_coop_rate = sum(history.opponent_actions[-1])
            action = Action.C if current_coop_rate < self.game_description.m else Action.D
        return action

class Strategy_EXPLOITATIVE_166(BaseStrategy):
    """
    Exploitative strategy that adaptively tracks the minimum number of cooperators required to meet the threshold (m) while minimizing personal contributions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = []
        self.avg_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if state.round_number < n:
            p = (m - len(self.cooperators)) / n
            action = Action.C if random.random() < p else Action.D
        elif state.round_number >= n and state.round_number <= 2 * n:
            self.avg_cooperators = sum(self.cooperators[-n:]) / n
            p = (m - self.avg_cooperators) / n
            action = Action.C if random.random() < p else Action.D
        elif state.round_number > 2 * n:
            opponent_payoffs = history.opponent_payoffs[-n:, :]
            payoff_bin = np.mean(opponent_payoffs)
            if payoff_bin > k:
                action = Action.C if random.random() < 0.5 else Action.D
            else:
                action = Action.D
        else:
            action = Action.D
        if history is not None and len(history.my_actions) > state.round_number - 1:
            self.cooperators.append(1 if history.my_actions[state.round_number - 1] else 0)
        return action

class Strategy_EXPLOITATIVE_167(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances exploration and exploitation to maximize individual payoff.
    It cooperates in initial rounds, adapts its probability of cooperation based on opponent behavior,
    and prioritizes defecting when possible while maintaining a minimum level of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize the Adaptive Exploiter strategy with the given game description.

        :param game_description: The game's parameters (n_players, n_rounds, m, k)
        """
        self.game_description = game_description
        self.num_cooperators_prev_round = None
        self.total_payoff = 0.0
        self.coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determine the next action based on the current game state and player history.

        :param state: The current game state (round number)
        :param history: The player's history (actions, payoffs, opponent actions, opponent payoffs)
        :return: The chosen action (COOPERATE or DEFECT)
        """
        if state.round_number == 0:
            return Action.C
        if self.num_cooperators_prev_round is None:
            self.num_cooperators_prev_round = sum(history.opponent_actions[0, :])
        if state.round_number <= 2:
            return Action.C
        elif 2 < state.round_number <= self.game_description.n_rounds // 2:
            p = (self.game_description.m - self.num_cooperators_prev_round) / (self.game_description.n_players - self.game_description.m)
            return Action.C if random.random() < p else Action.D
        else:
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            my_payoff_defect = 1.0 + (self.game_description.k if opponent_cooperations >= self.game_description.m else 0)
            my_payoff_coop = 1 - 1 + (self.game_description.k if opponent_cooperations + 1 >= self.game_description.m else 0)
            if my_payoff_defect > my_payoff_coop:
                return Action.D
            elif opponent_cooperations < self.game_description.m and my_payoff_coop >= my_payoff_defect:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_168(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.opponents_actions = None
        self.num_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        elif state.round_number < self.r - 1:
            self.opponents_actions = history.opponent_actions[-1]
            self.num_cooperators_prev_round = sum((1 for action in self.opponents_actions if action))
            p_c = (self.m - self.num_cooperators_prev_round) / self.n_players
            punishment_phase = False
            for i, action in enumerate(self.opponents_actions):
                if action and self.num_cooperators_prev_round < self.m:
                    self.opponents_actions[i] = Action.D
                    punishment_phase = True
            return random.choices([Action.C, Action.D], weights=[p_c, 1 - p_c])[0]
        else:
            return Action.D

class Strategy_EXPLOITATIVE_169(BaseStrategy):
    """
    Conditional Cooperation with Adaptive Threshold (CCAT) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.adaptive_threshold = game_description.m
        self.last_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        self.adaptive_threshold = max(self.game_description.m, min(self.game_description.n_players - 1, self.last_round_cooperators / self.game_description.n_players * self.game_description.n_players + 0.01))
        if history is not None:
            self.last_round_cooperators = sum(history.opponent_actions[-1, :])
        if self.adaptive_threshold >= self.game_description.m:
            return Action.C
        elif self.last_round_cooperators > self.game_description.n_players - 1:
            return Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_170(BaseStrategy):
    """
    Exploitative strategy that aims to maximize individual payoffs by adaptively responding 
    to the collective behavior of opponents while prioritizing exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.avg_payoff_per_cooperator = 0.0
        self.threshold = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        avg_payoff_per_cooperator = np.mean(history.my_payoffs[history.my_actions] if history.my_actions.any() else 0.0)
        self.prev_cooperators = prev_cooperators
        self.avg_payoff_per_cooperator = avg_payoff_per_cooperator
        delta = 0.01
        if state.round_number < self.game_description.n_rounds:
            self.threshold += delta * (avg_payoff_per_cooperator >= self.threshold and prev_cooperators >= self.game_description.m)
        if self.prev_cooperators < self.game_description.m:
            return Action.D
        elif self.avg_payoff_per_cooperator >= self.threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_171(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game.
    
    This strategy balances cooperation and exploitation to maximize payoffs. 
    It gathers information about opponents' behavior initially, then adapts its 
    actions based on the number of cooperators in each round and whether the 
    threshold has been met or exceeded.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = 0
        self.defect_count = 0
        self.threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if history.my_actions[-1]:
            self.cooperate_count += 1
        else:
            self.defect_count += 1
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_cooperators = self.cooperate_count + opponent_cooperators
        if total_cooperators >= self.game_description.m:
            self.threshold_met = True
        else:
            self.threshold_met = False
        if state.round_number > 1 and (not history.my_actions[-2]) and self.threshold_met:
            return Action.D
        if self.threshold_met and opponent_cooperators >= self.game_description.m - 1:
            return Action.C
        elif opponent_cooperators < self.game_description.m - 1:
            if state.round_number == self.game_description.n_rounds:
                if self.threshold_met and sum(history.opponent_actions[-2, :]) >= 1:
                    return Action.C
            return Action.D
        if state.round_number == self.game_description.n_rounds:
            if self.threshold_met and sum(history.opponent_actions[-2, :]) >= 1:
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_172(BaseStrategy):
    """
    An exploitative strategy that adapts to opponent behavior, game history, and payoff structures.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide whether to cooperate or defect based on the current state and history.

        :param state: The current game state.
        :param history: The player's history, including actions and payoffs for all players.
        :return: The chosen action (Action.C or Action.D).
        """
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        self.cooperation_threshold = max(self.game_description.m, self.game_description.m - (state.round_number - 1) / self.game_description.n_players)
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        last_round_payoff_if_cooperated = (self.game_description.k + 1) * (opponent_cooperators >= self.game_description.m)
        last_round_payoff_if_defected = history.my_payoffs[-1] + self.game_description.k
        if opponent_cooperators >= self.cooperation_threshold or (opponent_cooperators < self.game_description.m and last_round_payoff_if_cooperated > last_round_payoff_if_defected):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_173(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior by initially cooperating 
    and then adjusting cooperation level based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coops = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        avg_coops = sum(history.my_actions[:state.round_number]) / state.round_number
        if self.game_description.n_players % 2 == 0 and self.game_description.m % 2 != 0:
            threshold = (self.game_description.m + 1) / 2
        else:
            threshold = self.game_description.m / 2
        if avg_coops <= threshold:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_174(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    Adapts to the game's history and parameters, making calculated decisions to cooperate or defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        self.threshold = m_prev / self.game_description.n_players if m_prev >= self.game_description.m else (self.game_description.m - 1) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if m_prev >= math.ceil(self.threshold * self.game_description.n_players):
            return Action.C
        elif self.game_description.k > 2 and m_prev < self.game_description.m and (state.round_number > 2):
            p = (self.game_description.k - 1) / self.game_description.k
            return Action.C if random.random() < p else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_175(BaseStrategy):
    """
    Exploitative strategy that adapts to the level of cooperation in the game.
    Encourages cooperation in early rounds, maintains a high cooperation level when possible,
    and defects strategically to extract maximum payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coops = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        thresh = self.game_description.m - 1 if self.prev_coops >= self.game_description.m else self.game_description.m
        self.prev_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.prev_coops >= thresh:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_176(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_history = []
        self.m = game_description.m

    def count_cooperators(self, history):
        if len(history) == 0:
            return 0
        else:
            return sum(history[-1])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.opponent_history = [[False] * self.game_description.n_players]
            return Action.C
        if len(self.opponent_history) < state.round_number + 1:
            self.opponent_history.append([False] * self.game_description.n_players)
        for i in range(len(history.opponent_actions)):
            for j, action in enumerate(history.opponent_actions[i]):
                self.opponent_history[-(i + 2)][j] = action == True
        M = self.count_cooperators(self.opponent_history)
        if M < self.m:
            return Action.D
        elif M >= self.m:
            for i in range(len(history.opponent_actions)):
                for j, action in enumerate(history.opponent_actions[i]):
                    if action == False and M >= self.m:
                        return Action.D
            if state.round_number + 1 == self.game_description.n_rounds:
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_177(BaseStrategy):
    """
    An exploitative strategy that adapts to a wide range of opponent behaviors in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_payoff = history.my_payoffs[-1]
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        if state.round_number % 3 == 0:
            avg_payoff = np.mean(history.my_payoffs)
            if avg_payoff < (self.game_description.k + 1) / 2:
                if self.cooperation_rate > 0.5:
                    self.cooperation_rate -= 1 / 3
                else:
                    self.cooperation_rate += 1 / 3
        if opponent_cooperations < self.game_description.m:
            return Action.D
        elif prev_payoff > 1 + self.game_description.k / 2:
            return Action.D
        else:
            return Action.C if random.random() < self.cooperation_rate else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = np.sum(history.my_payoffs)
            avg_possible_reward = (self.game_description.k + 1) * self.game_description.n_rounds / 2
            return Action.C if total_payoff < avg_possible_reward else Action.D

class Strategy_EXPLOITATIVE_178(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for Collective Risk Dilemma.
    
    AE aims to balance cooperation and exploitation, adapting to the game's progression.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_cooperation_failure = 0
        self.epsilon = 1e-09

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_outcome = sum(history.opponent_actions[-1, :]) + (Action.C.value if history.my_actions[-1] else 0)
        if prev_outcome >= self.game_description.m:
            if self.game_description.k > 2:
                return Action.C
            else:
                prob_defect = min(1, max(0, 1 / (self.game_description.k - 1) + self.epsilon))
                return Action.D if random.random() < prob_defect else Action.C
        else:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_outcome == self.game_description.m:
            return Action.D
        elif self.consecutive_cooperation_failure > 2:
            return Action.D
        elif self.game_description.k > 3 and prev_outcome >= self.game_description.m:
            prob_cooperate = max(0, min(1, 1 - self.epsilon))
            return Action.C if random.random() < prob_cooperate else Action.D
        self.consecutive_cooperation_failure = self.consecutive_cooperation_failure + 1 if prev_outcome < self.game_description.m else 0

class Strategy_EXPLOITATIVE_179(BaseStrategy):
    """
    Exploitative strategy "Conditional Cooperate with Threshold Adaptation" 
    for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5
        self.prev_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.prev_round_cooperators is None:
            if history.my_actions[state.round_number - 1]:
                self.prev_round_cooperators = sum(history.opponent_actions[state.round_number - 2, :]) + 1
            else:
                self.prev_round_cooperators = sum(history.opponent_actions[state.round_number - 2, :])
        if state.round_number > 0:
            if self.prev_round_cooperators >= self.game_description.m:
                self.p = min(1, self.p + 0.1)
            elif self.prev_round_cooperators < self.game_description.m - 1:
                self.p = max(0, self.p - 0.2)
        if random.random() < self.p:
            action = Action.C
        else:
            action = Action.D
        if history.my_actions[state.round_number - 1]:
            self.prev_round_cooperators = sum(history.opponent_actions[state.round_number - 2, :]) + 1
        else:
            self.prev_round_cooperators = sum(history.opponent_actions[state.round_number - 2, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_180(BaseStrategy):
    """
    A strategy that adapts to the opponent's behavior while maximizing our own payoff.
    It uses a combination of cooperation and defection to exploit their actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.alpha = 0.1
        self.beta = 0.2
        self.current_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.p_init else Action.D
        self.current_cooperators = sum(history.opponent_actions[-1, :])
        T = self.game_description.m / self.game_description.n_players + self.alpha * (self.game_description.m - self.current_cooperators)
        if random.random() < T:
            return Action.C
        p_defect = max(0, self.beta * (self.game_description.m - self.current_cooperators) / self.game_description.n_players)
        if history.my_payoffs[-1] < 0:
            self.alpha *= 0.9
            self.beta *= 0.8
        return Action.D if random.random() < p_defect else Action.C

    def __getstate__(self):
        return {'game_description': self.game_description, 'p_init': self.p_init, 'alpha': self.alpha, 'beta': self.beta, 'current_cooperators': self.current_cooperators}

    def __setstate__(self, state):
        self.__dict__.update(state)

class Strategy_EXPLOITATIVE_181(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by monitoring their cooperation rate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = game_description.m / game_description.n_players * game_description.k / (game_description.k + 1)
        self.avg_coop_rate = 0
        self.prev_coop_count = 0
        self.prev_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        coop_count = sum(history.opponent_actions[-1, :])
        w = min(3, state.round_number)
        self.avg_coop_rate = (self.avg_coop_rate * (w - 1) + coop_count) / w
        if self.prev_action == Action.C and coop_count < self.game_description.m:
            return Action.D
        elif coop_count >= self.game_description.n_players - self.game_description.m:
            p = min(1, state.round_number / self.game_description.n_rounds * 0.5)
            return random.choice([Action.C, Action.D]) if random.random() < p else Action.D
        else:
            return Action.C if self.avg_coop_rate >= self.theta else Action.D
        self.prev_action = Action.C if history.my_actions[-1] else Action.D
        self.prev_coop_count = coop_count

class Strategy_EXPLOITATIVE_182(BaseStrategy):
    """
    An exploitative strategy designed to adapt to changing circumstances 
    and exploit cooperative behaviors while minimizing losses when cooperation fails.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M_t = 0
        self.D_t = 0
        self.P_C = 1.0
        self.P_D = 1.0
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        self.M_t = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        self.D_t = self.game_description.n_players - self.M_t
        if self.M_t >= self.game_description.m:
            coop_payoff = 1.0 - 1 + self.game_description.k
            self.P_C = np.mean([coop_payoff] + [history.my_payoffs[t] for t in range(len(history.my_payoffs)) if sum(history.opponent_actions[t, :] | history.my_actions[t]) >= self.game_description.m])
        else:
            coop_payoff = 1.0 - 1
            self.P_C = np.mean([coop_payoff] + [history.my_payoffs[t] for t in range(len(history.my_payoffs)) if sum(history.opponent_actions[t, :] | history.my_actions[t]) < self.game_description.m])
        self.P_D = np.mean([history.my_payoffs[t] for t in range(len(history.my_payoffs)) if not history.my_actions[t]])
        p_t = min(max(self.P_C / (self.P_C + self.P_D), 0.5), 1)
        self.cooperate_prob = p_t if self.M_t >= self.game_description.m else 0.2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.M_t >= self.game_description.m else Action.D
        elif 2 <= state.round_number <= 5:
            if random.random() < 0.5:
                self.cooperate_prob = min(self.cooperate_prob + 0.1, 1)
        elif self.D_t > self.game_description.n_players - self.game_description.m:
            self.cooperate_prob = min(self.cooperate_prob + 0.1, 1)
        return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_183(BaseStrategy):
    """
    An exploitative strategy that adaptively responds to the cooperation levels of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = None
        self.last_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.last_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if self.last_round_cooperators >= self.game_description.m:
            self.cooperation_threshold = max(self.game_description.m - 1, 0)
        else:
            self.cooperation_threshold = max(self.game_description.m - 2, 0)
        if self.last_round_cooperators == self.cooperation_threshold:
            return Action.C if random.random() < 0.5 else Action.D
        return Action.C if self.last_round_cooperators >= self.cooperation_threshold else Action.D

class Strategy_EXPLOITATIVE_184(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_C = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            return Action.C
        if history is not None:
            num_cooperators = np.sum(history.opponent_actions[-1, :]) + 1
            self.avg_C = (self.avg_C * (state.round_number - 1) + num_cooperators) / state.round_number
        if self.avg_C >= self.game_description.m:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds:
            if self.avg_C >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_185(BaseStrategy):
    """
    The Adaptive Exploiter strategy cooperates initially and adapts to the history of cooperation.
    It attempts to encourage others to cooperate, thereby increasing the likelihood of reaching the threshold (m) and earning the reward (k).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_round_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :] == True)
        ae_payoff_prev_round = history.my_payoffs[-1]
        if self.prev_round_payoffs is None:
            total_payoffs_prev_round = np.sum(history.opponent_payoffs[-1]) + history.my_payoffs[-1]
        else:
            total_payoffs_prev_round = np.sum(self.prev_round_payoffs)
        avg_payoff_prev_round = total_payoffs_prev_round / self.game_description.n_players
        if num_cooperators_prev_round >= self.game_description.m and ae_payoff_prev_round <= avg_payoff_prev_round:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D
        self.prev_round_payoffs = history.opponent_payoffs[-1]

class Strategy_EXPLOITATIVE_186(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting 
    the cooperative behavior of other players while minimizing its own contribution 
    to the collective good.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if self.game_description.n_players == 2:
            prev_opponent_action = history.opponent_actions[-1, 0]
            return Action.C if prev_opponent_action else Action.D
        elif self.game_description.m == 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        avg_payoff_per_cooperator = np.mean(history.opponent_payoffs[-1, :] * history.opponent_actions[-1, :])
        if prev_cooperators >= self.game_description.m and avg_payoff_per_cooperator > 1 + self.game_description.k / 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_187(BaseStrategy):
    """
    This strategy focuses on maximizing individual payoff by adapting to the evolving game dynamics and exploiting opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.coop_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop = sum(history.opponent_actions[-1, :]) + int(bool(self.game_description.n_players - history.my_actions[-1]))
        self.avg_coop = self.avg_coop * state.round_number / (state.round_number + 1)
        if prev_coop > 0:
            self.avg_coop += prev_coop / (state.round_number + 1)
        self.coop_threshold = self.game_description.m - 1 if self.avg_coop > (self.game_description.m - 1) / self.game_description.n_players else math.ceil(self.game_description.m / 2)
        if prev_coop < self.coop_threshold:
            return Action.D
        elif prev_coop >= self.coop_threshold:
            p_c = min(1, self.game_description.k / (self.game_description.k + 1))
            return Action.C if random.random() < p_c else Action.D

class Strategy_EXPLOITATIVE_188(BaseStrategy):
    """
    Exploitative strategy: "Adaptive Threshold Escalation".
    
    This strategy starts with an initial cooperation probability based on the minimum number of cooperators needed.
    It then adapts this probability in subsequent rounds based on the outcome of the previous round.
    If enough players cooperated, it increases its own cooperation probability; otherwise, it decreases it.
    In the final round, it defects with certainty.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_C = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.p_C = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < self.p_C else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        if prev_cooperators >= self.game_description.m:
            self.p_C = min(self.p_C + 0.1, 1)
        else:
            self.p_C = max(self.p_C - 0.2, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.p_C else Action.D

class Strategy_EXPLOITATIVE_189(BaseStrategy):
    """
    A strategy that aims to exploit the collective risk dilemma by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CC_threshold = math.floor((game_description.m - 1) / 2)
        self.RCI = 0
        self.p_exploit = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if sum(history.my_actions[state.round_number - 2:state.round_number]) >= self.CC_threshold:
            action = Action.C
        elif self.RCI >= 2:
            action = Action.C
        elif random.random() < self.p_exploit:
            action = Action.D
        else:
            action = Action.D
        if sum(history.my_actions[state.round_number - 3:state.round_number]) >= self.game_description.m * 3:
            self.RCI += 1
        elif state.round_number > 3:
            self.RCI -= 1
        self.RCI = max(0, min(self.RCI, 3))
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_191(BaseStrategy):
    """
    Exploitative strategy that adapts to the opponent's behavior while balancing cooperation and defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C] * int(self.m / self.n_players) + [Action.D] * (1 - int(self.m / self.n_players)))
        avg_cooperators = np.mean(history.opponent_actions[:state.round_number, :])
        if state.round_number <= self.r // 2:
            return Action.C if random.random() < self.m / self.n_players else Action.D
        elif state.round_number <= 3 * self.r // 4:
            if avg_cooperators < self.m / self.n_players:
                return Action.D if random.random() < 0.8 else Action.C
            else:
                return Action.C if random.random() < 0.7 else Action.D
        opponent_coop_rate = np.mean(history.opponent_actions[:state.round_number, :])
        if opponent_coop_rate > 0.5:
            return Action.D if random.random() < 0.9 else Action.C
        else:
            return Action.C if random.random() < 0.5 else Action.D
        total_payoff = np.sum(history.my_payoffs[:state.round_number])
        max_possible_payoff = self.k * state.round_number
        return Action.D if total_payoff < max_possible_payoff else Action.C

class Strategy_EXPLOITATIVE_192(BaseStrategy):
    """
    Adaptive Exploiter strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players == 2:
            return Action.C if state.round_number % 2 == 0 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        cr_t_minus_1 = prev_cooperators / self.game_description.n_players
        if state.round_number > 1:
            prev_prev_cooperators = sum(history.opponent_actions[-2, :]) + (history.my_actions[-2] == True)
            cr_t_minus_2 = prev_prev_cooperators / self.game_description.n_players
        else:
            cr_t_minus_2 = 0
        recent_success = len([p for p in history.opponent_payoffs[-1, :] if p > 0]) >= self.game_description.m - (history.my_actions[-1] == True) and history.my_payoffs[-1] > 0
        increasing_cooperation_trend = cr_t_minus_1 > cr_t_minus_2
        low_defection_rate = len([p for p in history.opponent_actions[-1, :] if not p]) + (history.my_actions[-1] == False) <= self.game_description.n_players / 4
        if recent_success or increasing_cooperation_trend or low_defection_rate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_193(BaseStrategy):
    """
    The Adaptive Risk-Taker (ART) strategy aims to balance cooperation and defection 
    to maximize individual payoffs while adapting to the evolving game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CR = 0
        self.total_cooperations = 0
        self.total_actions = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if state.round_number <= 3:
            if self.total_cooperations < self.game_description.m:
                action = Action.D
            else:
                action = Action.C
        else:
            CR = self.total_cooperations / self.total_actions
            if CR < self.game_description.m / self.game_description.n_players:
                action = Action.D
            else:
                action = Action.C
            if self.game_description.k * (self.game_description.m / self.game_description.n_players - CR) > 0.5:
                action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None:
            self.total_cooperations += sum(history.opponent_actions[-1, :] | (history.my_actions[-1] != 0))
            self.total_actions += self.game_description.n_players
        return action

class Strategy_EXPLOITATIVE_194(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_cooperators = 0
        self.cooperation_threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_cooperators_in_last_round = sum(history.opponent_actions[-1, :])
        self.opponent_cooperators += sum(np.all(history.opponent_actions == True, axis=0)) - self.opponent_cooperators
        if opponent_cooperators_in_last_round < self.game_description.m:
            self.cooperation_threshold_met = False
            return Action.D
        else:
            self.cooperation_threshold_met = True
            payoff_C = 0 + self.game_description.k if opponent_cooperators_in_last_round >= self.game_description.m else 0
            payoff_D = 1 + self.game_description.k if opponent_cooperators_in_last_round >= self.game_description.m else 1
            if payoff_C > payoff_D:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_195(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Threshold Tracker.

    This strategy adapts to opponents' behavior by tracking the number of cooperators and adjusting its cooperation probability accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperation_probability = self.game_description.m / self.game_description.n_players
            return random.choices([Action.C, Action.D], weights=[self.cooperation_probability, 1 - self.cooperation_probability])[0]
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            if history.my_actions[-1] == Action.C and num_cooperators_prev_round < self.game_description.m:
                punishment_probability = 1 - self.game_description.k / (self.game_description.k + 1)
                action = random.choices([Action.D, Action.C], weights=[punishment_probability, 1 - punishment_probability])[0]
            elif num_cooperators_prev_round >= self.game_description.m:
                action = Action.C
            else:
                action = Action.D
            if num_cooperators_prev_round >= self.game_description.m:
                self.cooperation_probability *= self.game_description.k / (self.game_description.k + 1)
            else:
                self.cooperation_probability /= self.game_description.k + 1
            return action

class Strategy_EXPLOITATIVE_196(BaseStrategy):
    """
    The Adaptive Threshold Hunter strategy aims to exploit opponents by adapting to their behavior while prioritizing self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_coop = 0
        self.CR = 0.5
        self.thresh_met_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            if state.round_number == 1 or history is None:
                return Action.C
            else:
                majority_action = np.mean(history.opponent_actions[-1, :]) > 0.5
                action = Action.D if majority_action else Action.C
                self.m_coop = sum(history.opponent_actions[-1, :])
                self.CR = self.m_coop / self.game_description.n_players
                return action
        opponent_cooperators = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] else sum(history.opponent_actions[-1, :])
        self.m_coop = opponent_cooperators
        self.CR = self.m_coop / self.game_description.n_players
        if state.round_number > 3:
            if history.my_payoffs[-2] == 1 - int(history.my_actions[-2]) + self.game_description.k:
                self.thresh_met_count += 1
            else:
                self.thresh_met_count -= 1
            if abs(self.thresh_met_count) >= 2 and state.round_number != self.game_description.n_rounds - 1:
                self.CR += np.sign(self.thresh_met_count) * 0.1
                self.thresh_met_count = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.CR > 0.5 and opponent_cooperators >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_197(BaseStrategy):
    """
    A strategy that balances cooperation and defection based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CR = 0
        self.ER = 0
        self.DT = game_description.m / game_description.n_players
        self.opponent_defected_last_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if history is not None:
            opponent_actions_last_round = history.opponent_actions[-1, :]
            cooperators_last_round = np.sum(opponent_actions_last_round)
            self.CR = cooperators_last_round / (self.game_description.n_players - 1)
        if self.opponent_defected_last_round is not None:
            return Action.D
        self.ER = self.CR * self.game_description.k + (1 - int(Action.C.value))
        if self.CR < self.DT or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_198(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) is an exploitative strategy designed for the Collective Risk Dilemma game.
    It adapts to the cooperation level of other players and minimizes its own contributions while maximizing its payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_coops = sum(history.opponent_actions[-1, :] + history.my_actions[-1])
        n_players = self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_coops >= self.game_description.m:
            return Action.C
        elif prev_coops < self.game_description.m / 2 and state.round_number <= 5:
            return Action.D if random.random() < 0.8 else Action.C
        elif prev_coops == self.game_description.m - 1:
            return Action.C if random.random() < 0.7 else Action.D
        else:
            defect_prob = (n_players - prev_coops) / (n_players - self.game_description.m + 1)
            return Action.C if random.random() > defect_prob else Action.D

class Strategy_EXPLOITATIVE_199(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy adapts to the game's dynamics by balancing cooperation and defection.
    It encourages others to cooperate initially, then adjusts its behavior based on recent payoffs and opponent actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = game_description.m / game_description.n_players
        self.exploiters = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        prev_round_payoff = self.calculate_total_payoff(history.my_payoffs[-1], len([opponent for opponent in history.opponent_actions[-1] if opponent]))
        cooperate = prev_round_payoff >= self.game_description.k * (self.game_description.n_players - 1)
        if not cooperate and sum([opponent for opponent in history.opponent_actions[-1]]) > self.game_description.m:
            self.exploiters.add((state.round_number - 1) % self.game_description.n_players)
        if state.round_number % self.game_description.n_players in self.exploiters and random.random() < 0.8:
            cooperate = False
        if state.round_number % 3 == 0:
            avg_payoff = sum(history.my_payoffs[-3:]) / 3
            if avg_payoff > 1:
                self.p += 0.05
            elif avg_payoff <= 1:
                self.p -= 0.05
        return Action.C if cooperate else Action.D

    def calculate_total_payoff(self, my_action: float, opponent_cooperators: int) -> float:
        """
        Calculate the total payoff for a given round.
        
        :param my_action: This player's action (1 = COOPERATE, 0 = DEFECT)
        :param opponent_cooperators: Number of opponents who cooperated in this round
        :return: Total payoff for this round
        """
        if opponent_cooperators < self.game_description.m:
            return 1 - my_action
        else:
            return 1 - my_action + self.game_description.k

class Strategy_EXPLOITATIVE_200(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma Exploiter (ACRDE) strategy.

    This strategy adapts to the opponent's behavior while maintaining a balance between cooperation and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_payoff = 0
        self.consecutive_defects = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        π_prev = np.mean(history.opponent_payoffs[-1, :])
        if π_prev >= 1 + self.game_description.k / 2:
            return Action.D if random.random() < 0.6 else Action.C
        else:
            action = Action.C
        if history.my_actions[-1] == False and self.consecutive_defects > 2:
            action = Action.D
        self.prev_payoff = π_prev
        if action == Action.D:
            self.consecutive_defects += 1
        else:
            self.consecutive_defects = 0
        return action

class Strategy_EXPLOITATIVE_201(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.w = min(game_description.n_rounds - 1, 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        m_t = sum(history.my_actions[:-1]) + sum(history.opponent_actions[-1, :])
        n_m_t = self.game_description.n_players - m_t
        w = min(self.w, state.round_number)
        if not hasattr(self, 'm_avg'):
            self.m_avg = np.mean([m_t] * w)
            self.d_avg = np.mean([n_m_t] * w)
        else:
            self.m_avg = (self.m_avg * (w - 1) + m_t) / w
            self.d_avg = (self.d_avg * (w - 1) + n_m_t) / w
        p_coop = self.m_avg / self.game_description.n_players
        if p_coop > 0.5 and self.m_avg >= self.game_description.m:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_202(BaseStrategy):
    """
    A strategy that balances cooperation and exploitation based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        prev_round_outcome = history
        threshold_met = sum(prev_round_outcome.opponent_actions[-1, :]) >= self.game_description.m - 1 + prev_round_outcome.my_actions[-1]
        if threshold_met:
            self.cooperation_level += 1 if not prev_round_outcome.my_actions[-1] else -1
        else:
            self.cooperation_level -= 1 if not prev_round_outcome.my_actions[-1] else -1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if self.cooperation_level >= 0 else Action.D

class Strategy_EXPLOITATIVE_203(BaseStrategy):
    """
    Exploitative strategy designed to adaptively adjust cooperation threshold based on game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.m_dyn = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :])
        self.avg_coop = (self.avg_coop * 2 + prev_round_coops) / 3
        if self.avg_coop > self.game_description.m:
            self.m_dyn = max(0, self.game_description.m - 1)
        elif self.avg_coop < self.game_description.m:
            self.m_dyn = min(self.game_description.n_players, self.game_description.m + 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_round_coops == self.m_dyn:
            return Action.C
        if prev_round_coops >= self.m_dyn:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_204(BaseStrategy):
    """
    The Adaptive Threshold Tracker (ATT) strategy aims to maximize individual payoff by adaptively tracking the minimum number of cooperators required to reach the threshold, while exploiting potential free-riders and avoiding being exploited itself.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.total_payoff = 0
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        m_prev = self.m_prev
        total_payoff = self.total_payoff
        punishment_rounds = self.punishment_rounds
        if history.my_actions[-1] and m_prev >= self.game_description.m:
            c_i = 0 + self.game_description.k
        else:
            c_i = 1 - history.my_actions[-1]
        if not history.my_actions[-1] and m_prev >= self.game_description.m:
            action = Action.C
            punishment_rounds += 1
            if punishment_rounds == 2:
                punishment_rounds = 0
        elif state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        elif m_prev >= self.game_description.m and c_i <= 1 + self.game_description.k:
            action = Action.D
        elif history.opponent_actions[-1].sum() >= self.game_description.m - 1:
            action = Action.C
        else:
            action = Action.D
        m_prev = history.opponent_actions[-1].sum()
        total_payoff += c_i
        punishment_rounds = punishment_rounds
        self.m_prev = m_prev
        self.total_payoff = total_payoff
        self.punishment_rounds = punishment_rounds
        return action

class Strategy_EXPLOITATIVE_205(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Collective Risk (ACR)

    This strategy is designed to exploit the game's structure and adapt to a wide range of opponent behaviors while maximizing individual payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = game_description.m
        self.cooperate_rate_threshold = 0.5

    def estimate_cooperators(self, history: PlayerHistory) -> int:
        """
        Estimate the number of cooperators based on history.

        This implementation assumes that opponents with a cooperation rate above the threshold are likely to cooperate.
        """
        opponent_rates = np.mean(history.opponent_actions, axis=0)
        return sum((1 for rate in opponent_rates if rate > self.cooperate_rate_threshold))

    def calculate_opponent_rates(self, history: PlayerHistory) -> float:
        """
        Calculate opponent cooperation rates based on history.

        This implementation returns the average cooperation rate of all opponents.
        """
        opponent_rates = np.mean(history.opponent_actions, axis=0)
        return np.mean(opponent_rates)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        expected_cooperators = self.estimate_cooperators(history)
        opponent_rates = self.calculate_opponent_rates(history)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if expected_cooperators >= self.T and opponent_rates > self.cooperate_rate_threshold:
            return Action.C
        if expected_cooperators < self.T or opponent_rates < self.cooperate_rate_threshold / 2:
            return Action.D
        if random.random() < 0.55:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_206(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for the Collective Risk Dilemma game.
    
    This strategy aims to exploit opponents' cooperative behavior while maintaining 
    adaptability to various scenarios. It cooperates in the first round, then adapts 
    its strategy based on the number of cooperators and defects when the threshold is met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m_c = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if m_c >= self.game_description.m - 1:
            return Action.D
        else:
            p = max(0, (self.game_description.m - m_c) / (self.game_description.n_players - m_c))
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_207(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while maintaining a robust approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if self.m_prev >= self.game_description.m:
            self.p_coop = 1
        else:
            distance = abs(self.game_description.m - self.m_prev)
            self.p_coop = max(0.2, 1 - distance / self.game_description.n_players)
        if random.random() < self.p_coop:
            action = Action.C
        else:
            action = Action.D
        if history is not None:
            self.m_prev = sum(history.opponent_actions[-1, :] == True) + 1
            m_curr = sum(history.opponent_actions[-1, :] == True)
            if m_curr < self.game_description.m - 2:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_208(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff based on game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_count = 0
        self.cooperation_probability = game_description.k / (game_description.k + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == 1:
            self.threshold_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            if self.threshold_count < self.game_description.m:
                return Action.D
            else:
                return Action.C
        self.threshold_count = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.threshold_count <= self.game_description.m - 1:
            return Action.D
        elif self.threshold_count == self.game_description.m:
            if random.random() < self.cooperation_probability:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_209(BaseStrategy):
    """
    This strategy aims to maximize individual payoffs by adapting to the evolving behavior of opponents while prioritizing self-interest.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = np.sum(history.opponent_actions[-1, :])
        if prev_round_coops < self.game_description.m:
            return Action.D
        else:
            p = self.calculate_cooperation_probability(history)
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

    def calculate_cooperation_probability(self, history: PlayerHistory):
        recent_defections = np.sum(history.opponent_actions[-self.game_description.n_rounds // 2:, :])
        if recent_defections > self.game_description.n_rounds // 4 * self.game_description.n_players:
            return 0.5
        else:
            return min(1, self.game_description.k / (self.game_description.k + 1))

class Strategy_EXPLOITATIVE_210(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for Collective Risk Dilemma.

    This strategy aims to exploit opponents by adapting to their cooperation rates while minimizing its own contributions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number <= self.game_description.n_rounds // 2:
            opponent_cooperations = history.opponent_actions[-1, :].sum()
            self.opponent_cooperation_rates.append(opponent_cooperations / self.game_description.n_players)
            return random.choice([Action.C, Action.D])
        avg_opponent_rate = np.mean(self.opponent_cooperation_rates) if self.opponent_cooperation_rates else 0.5
        if avg_opponent_rate > (self.game_description.m - 1) / self.game_description.n_players:
            action = random.choice([Action.C, Action.D])
        else:
            action = Action.D
        if state.round_number > 0 and history.my_payoffs[-1] < self.game_description.k - 1:
            action = Action.D
        if state.round_number == self.game_description.n_rounds:
            avg_opponent_rate = np.mean(self.opponent_cooperation_rates) if self.opponent_cooperation_rates else 0.5
            return Action.C if avg_opponent_rate > (self.game_description.m - 1) / self.game_description.n_players else Action.D
        return action

class Strategy_EXPLOITATIVE_211(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior.
    Initially explores, then cooperates or defects based on the number of opponent cooperators.
    Punishes consistent defectors and defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperator_count_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.opponent_actions) > 1:
            prev_round_coops = sum((1 for action in history.opponent_actions[-2, :] if action))
            self.cooperator_count_history.append(prev_round_coops)
        if state.round_number < 3:
            return Action.C
        if len(self.cooperator_count_history) > 0 and self.cooperator_count_history[-1] < self.game_description.m:
            return Action.D
        else:
            p_c = self.cooperator_count_history[-1] / self.game_description.n_players
            if random.random() < p_c:
                return Action.C
            else:
                return Action.D
        for opponent in range(self.game_description.n_players):
            if history is not None and len(history.opponent_actions) > 2:
                if not history.opponent_actions[-1, opponent] and (not history.opponent_actions[-2, opponent]):
                    return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_212(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while maintaining a robust and aggressive approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = None
        self.punished_opponents = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        opponents_cooperated = sum(prev_round_actions)
        if state.round_number < self.game_description.n_rounds - 2:
            if self.cooperation_threshold is None:
                self.cooperation_threshold = self.game_description.m - 1
            if opponents_cooperated >= self.cooperation_threshold and (not self.should_punish(prev_round_actions)):
                return Action.C
            else:
                self.update_cooperation_threshold(opponents_cooperated)
                return Action.D
        elif state.round_number >= self.game_description.n_rounds - 2:
            prev_prev_round_defectors = sum((1 for action in history.opponent_actions[-2] if not action))
            if prev_prev_round_defectors > 0:
                return Action.C
            else:
                return Action.D

    def should_punish(self, opponent_actions: NDArray[np.bool_]) -> bool:
        """Check if an opponent has been punished before"""
        for i, action in enumerate(opponent_actions):
            if not action and i in self.punished_opponents:
                return True
        return False

    def update_cooperation_threshold(self, opponents_cooperated: int) -> None:
        """Increase the cooperation threshold if necessary"""
        if opponents_cooperated > self.game_description.m:
            self.cooperation_threshold += 1

class Strategy_EXPLOITATIVE_213(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by cooperating conditionally.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m_prev = sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :])
        if state.round_number < self.r - 1:
            if m_prev >= self.m:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_214(BaseStrategy):
    """
    This strategy initially cooperates to gather information and encourage cooperation.
    It then monitors opponents' behavior, adapting its own actions based on their cooperation levels.
    If average payoffs are low or the number of cooperators is insufficient, it defects.
    In the final round, it unconditionally defects to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.monitoring_phase_cooperation_probability = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if history is None or len(history.my_actions) == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number <= self.game_description.n_rounds // 2:
            cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if cooperators >= self.game_description.m:
                if self.monitoring_phase_cooperation_probability is None:
                    self.monitoring_phase_cooperation_probability = self.game_description.m / self.game_description.n_players + 1 / self.game_description.n_players * (self.game_description.k - 1)
                return Action.C if random.random() < self.monitoring_phase_cooperation_probability else Action.D
            return Action.D
        else:
            last_5_rounds_average_payoff = np.mean(history.my_payoffs[-5:]) if len(history.my_payoffs) >= 5 else history.my_payoffs[-1]
            if last_5_rounds_average_payoff >= self.game_description.k:
                return Action.C if random.random() < self.game_description.m / self.game_description.n_players + 1 / self.game_description.n_players * (self.game_description.k - 1) else Action.D
            if len(history.opponent_actions[-1, :]) > 0 and sum(history.opponent_actions[-1, :]) < self.game_description.n_players // 2:
                return random.choice([Action.C, Action.D])
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_215(BaseStrategy):
    """
    A strategy that adapts to the game's dynamics by initially exploring opponents' behaviors,
    then exploiting their cooperation while adjusting its own threshold for cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_adj = game_description.m - 1
        self.coop_count_prev = 0
        self.threshold_was_met_prev = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 2:
            self.coop_count_prev = sum(history.opponent_actions[-1, :] != False)
            return Action.C
        self.threshold_was_met_prev = self.coop_count_prev >= self.game_description.m
        if history is not None:
            current_cooperators = sum(history.opponent_actions[-1, :])
            if current_cooperators < self.game_description.m and (not self.threshold_was_met_prev):
                self.m_adj -= 1
        if state.round_number < self.game_description.n_rounds - 1:
            if self.coop_count_prev >= self.m_adj:
                return Action.C
            elif self.threshold_was_met_prev:
                return Action.D
        total_payoff_from_cooperating = history.my_payoffs.sum() + self.game_description.k
        total_payoff_from_defecting = history.my_payoffs.sum() + 1
        if state.round_number == self.game_description.n_rounds - 1 and total_payoff_from_cooperating > total_payoff_from_defecting:
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_216(BaseStrategy):
    """
    This strategy adapts the cooperation threshold based on game history to balance cooperation with exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            if history is not None and any((not action for action in history.opponent_actions)):
                cooperate_prob -= 0.1
            return Action.C if random.random() < cooperate_prob else Action.D
        opponents_cooperated = sum(history.opponent_actions[-1, :])
        if opponents_cooperated >= self.game_description.m:
            self.cooperation_threshold = max(1, self.cooperation_threshold - 1)
        else:
            self.cooperation_threshold = min(self.game_description.n_players, self.cooperation_threshold + 1)
        cooperate_prob = self.game_description.k / (self.game_description.k + 1) if opponents_cooperated >= self.cooperation_threshold else 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_217(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma.
    
    This strategy aims to exploit the cooperative behavior of other players while adapting to changes in their strategies.
    By estimating the minimum number of cooperators required to reach the threshold and adjusting its actions accordingly,
    this strategy can take advantage of situations where others are more likely to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        self.prev_cooperators = sum(opponent_actions)
        self.threshold_met = self.prev_cooperators >= self.game_description.m
        if state.round_number < self.game_description.n_rounds - 1:
            if self.threshold_met and self.prev_cooperators <= self.game_description.m:
                return Action.C
            elif not self.threshold_met or self.prev_cooperators > self.game_description.m:
                return Action.D
            else:
                return Action.D
        return Action.C if self.threshold_met else Action.D

class Strategy_EXPLOITATIVE_218(BaseStrategy):
    """
    The ATE strategy aims to exploit opponents by adapting to their behavior and maximizing payoffs based on the game's parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        k_reward_factor = self.game_description.k
        r_total_rounds = self.game_description.n_rounds
        if state.round_number <= 1:
            return Action.C
        if n_players == 2:
            return Action(history.opponent_actions[-1, 0])
        elif state.round_number <= r_total_rounds // 2:
            cooperators = sum(history.opponent_actions[-1, :])
            return Action.C if cooperators >= m_cooperators_needed - 1 else Action.D
        elif state.round_number < r_total_rounds:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators < m_cooperators_needed:
                return Action.D
            else:
                previous_payoff = history.my_payoffs[-1]
                my_previous_action = history.my_actions[-1]
                k_term = k_reward_factor * int(my_previous_action)
                expected_payoff_if_cooperated = 1 - int(my_previous_action) + k_term
                return Action.C if math.isclose(previous_payoff, expected_payoff_if_cooperated) else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_219(BaseStrategy):
    """
    An exploitative strategy that adapts to the game's dynamics, 
    balancing cooperation and exploitation while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if len(history.my_actions) >= 2 and history.my_actions[-2] and (prev_round_coops < m):
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        if prev_round_coops >= m:
            return Action.D
        TPC = (k - 1) * (prev_round_coops / n)
        EPD = 1
        if TPC > EPD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_220(BaseStrategy):
    """
    Adaptive Collective Risk (ACR) Strategy.
    
    This strategy balances cooperation and exploitation by adapting to opponents' behaviors.
    It encourages positive feedback loops while taking advantage of situations where opponents are not cooperating.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        m_prev = sum(history.opponent_actions[-1, :])
        cr = m_prev / self.game_description.n_players
        if cr >= 0.7:
            p = min(1, self.game_description.k / 2 + 0.5)
        elif cr < 0.3:
            p = max(0, 1 - self.game_description.k / 2)
        else:
            p = 0.5
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_222(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit opponents' behavior while adapting to the game's dynamics.
    It balances cooperation and defection to maximize payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_coops = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history.my_actions.size > 1:
            self.total_coops = sum(history.opponent_actions[:, :].sum(axis=1) >= self.game_description.m)
            cr = self.total_coops / (state.round_number - 1)
        if state.round_number <= 3:
            if state.round_number == 1:
                return Action.C
            else:
                majority_action = np.mean(history.opponent_actions[state.round_number - 2, :]) > 0.5
                if majority_action:
                    return Action.D
                else:
                    return Action.C
        elif state.round_number < self.game_description.n_rounds:
            if cr >= 0.5:
                return Action.C
            else:
                return Action.D
        elif sum(history.opponent_actions[-1, :]) + 1 >= self.game_description.m:
            return Action.C
        elif cr >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_223(BaseStrategy):
    """
    An adaptive strategy that tracks the opponent's behavior and adjusts its own cooperation level accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.successful_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :] != False)
        if prev_coops < self.game_description.m:
            return Action.D
        else:
            self.successful_rounds += 1
            p_t = self.game_description.k * self.successful_rounds / state.round_number
            if random.random() < p_t and state.round_number != self.game_description.n_rounds - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_224(BaseStrategy):
    """
    Exploitative strategy that adapts to the opponent's behavior by calculating a dynamic cooperation threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_round_stats = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        m_prev = sum(history.opponent_actions[-1, :] | np.array([True])) - 1
        avg_payoff_c = np.mean(np.where(history.my_actions[-1] == True, history.my_payoffs[-1], 0))
        avg_payoff_d = np.mean(np.where(history.my_actions[-1] == False, history.my_payoffs[-1], 0))
        if avg_payoff_c + avg_payoff_d == 0:
            theta = (self.game_description.m - 1) / self.game_description.n_players
        else:
            theta = (self.game_description.m - 1) / self.game_description.n_players + (self.game_description.k - 1) * avg_payoff_c / (avg_payoff_c + avg_payoff_d)
        if m_prev >= theta:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_225(BaseStrategy):
    """
    Adaptive Threshold Exploiter Strategy.
    
    This strategy aims to exploit the opponent's behavior by adapting to their actions and adjusting its cooperation probability accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            prev_round_cooperators = sum(history.opponent_actions[-1, :])
            avg_defector_payoff = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if not action])
            if prev_round_cooperators >= self.game_description.m and avg_defector_payoff <= self.game_description.k:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D
        if state.round_number % 2 == 0 and state.round_number > 0:
            avg_cooperator_payoff = np.mean([payoff for payoff, action in zip(history.my_payoffs[-2:], history.my_actions[-2:]) if action])
            avg_defector_payoff = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-2:, :].flatten(), history.opponent_actions[-2:, :].flatten()) if not action])
            if avg_cooperator_payoff >= avg_defector_payoff:
                self.p += 0.1
            else:
                self.p -= 0.1
            self.p = max(0, min(self.p, 1))

class Strategy_EXPLOITATIVE_226(BaseStrategy):
    """
    Exploitative strategy that adapts to the cooperation level of opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history else False)
        prev_round_payoff = np.sum(history.opponent_payoffs[-1, :]) + (history.my_payoffs[-1] if history else 0)
        if prev_round_cooperators == self.game_description.m:
            self.threshold += 1
        if prev_round_cooperators >= self.threshold and prev_round_payoff < self.game_description.n_players * self.game_description.k / self.game_description.m:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_227(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit opponents in the Collective Risk Dilemma game.
    It adapts its behavior based on the number of cooperators and defectors in previous rounds, aiming to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []
        self.total_payoff_coop = 0
        self.total_payoff_defect = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_history = history.opponent_actions[-1]
        num_coop_prev = sum(prev_round_history)
        defector_in_prev = any((not a for a in prev_round_history))
        if state.round_number < self.game_description.n_rounds // 2:
            action = Action.C if num_coop_prev >= self.game_description.m / 2 else Action.D
        elif state.round_number < 3 * self.game_description.n_rounds // 4:
            action = Action.C if num_coop_prev >= self.game_description.m and defector_in_prev else Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            recent_history = history.opponent_actions[-self.game_description.n_rounds // 2:]
            self.total_payoff_coop += sum((1 for round in recent_history if sum(round) >= self.game_description.m)) * (1 + self.game_description.k)
            self.total_payoff_defect += sum((1 for round in recent_history if not all(round)))
            action = Action.C if self.total_payoff_coop >= self.total_payoff_defect else Action.D
        else:
            action = Action.D
        self.history.append(action)
        return action

class Strategy_EXPLOITATIVE_228(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behaviors and game parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :] != False)
        p_c = (self.game_description.k - 1) / (self.game_description.k + 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif prev_round_coops >= self.game_description.m:
            return Action.C if random.random() < p_c else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_229(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploit_threshold = 0
        self.exploit_limit = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if m_prev >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        if m_prev < self.game_description.m:
            self.exploit_threshold += 1
            if all((not opponent for opponent in history.opponent_actions[-1, :])) and (not history.my_actions[-1]):
                self.exploit_threshold += 2
        elif m_prev >= self.game_description.m and self.exploit_threshold > 0:
            if action == Action.D and self.exploit_threshold >= self.exploit_limit or (all(history.opponent_actions[-1, :]) and history.my_actions[-1] == False):
                self.exploit_threshold -= 2
            else:
                self.exploit_threshold -= 1
        if self.exploit_threshold >= self.exploit_limit:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_230(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the collective project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round < self.game_description.m:
            p = (self.game_description.m - num_cooperators_prev_round) / (self.game_description.n_players - 1 - num_cooperators_prev_round)
            return Action.C if random.random() < p else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_231(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff based on game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_rate_history = []
        self.previous_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        overall_cooperate_rate = np.mean(self.cooperate_rate_history)
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (self.previous_action == Action.C)
        if state.round_number < 5:
            if overall_cooperate_rate > self.game_description.m / self.game_description.n_players:
                return Action.D
        elif state.round_number >= 5 and state.round_number <= self.game_description.n_rounds - 2:
            if previous_round_cooperators <= self.game_description.m and self.previous_action == Action.C:
                return Action.D
            elif previous_round_cooperators >= self.game_description.m:
                return self.previous_action
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D
        action = Action.C
        self.cooperate_rate_history.append((previous_round_cooperators + (self.previous_action == Action.C)) / self.game_description.n_players)
        self.previous_action = action
        return action

class Strategy_EXPLOITATIVE_232(BaseStrategy):
    """
    Exploitative strategy that adapts to the number of cooperators in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        t = state.round_number
        prev_round_coops = 0 if history is None else sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if m == 1:
            return Action.C
        elif n == m:
            return Action.D
        elif t == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        if prev_round_coops < m:
            return Action.D
        p = max(0, min(1, (m - 1) / (n - 1) + k / (k + 1)))
        coop_prob = random.random()
        if coop_prob < p:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_233(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)
    
    This strategy balances cooperation and exploitation by adapting to opponents' behavior, 
    defecting when the threshold is close to being met, and cooperating when it's consistently exceeded.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_played = 0
        self.threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.rounds_played += 1
        opponent_actions = history.opponent_actions[-3:]
        cooperators_in_last_rounds = np.sum(opponent_actions, axis=1)
        self.threshold_met = np.any(cooperators_in_last_rounds >= self.game_description.m)
        opponents_defected = history.opponent_actions[-1] == False
        punishment_rounds = min(2, self.rounds_played // 3)
        if np.any(opponents_defected) and punishment_rounds > 0:
            return Action.D
        if self.threshold_met:
            return Action.C
        if cooperators_in_last_rounds[-1] >= self.game_description.m - 1:
            return Action.D
        if state.round_number > self.game_description.n_rounds - 2:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_234(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy balances self-interest with adaptability to the collective risk dilemma.
    It monitors cooperation rates, punishes defectors, and exploits vulnerabilities to maximize its payoff while minimizing contributions to the collective effort.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_payoff = 0
        self.prev_CR = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        my_previous_action = history.my_actions[-1]
        CR = np.sum(opponent_actions) / self.game_description.n_players
        if CR >= self.game_description.m / self.game_description.n_players and (not my_previous_action):
            return Action.D
        if CR >= self.game_description.m / self.game_description.n_players and np.sum(opponent_actions) < self.game_description.m:
            payoff_from_cooperating = 1 - 1 + self.game_description.k
            payoff_from_defecting = 1 - 0
            if payoff_from_cooperating > payoff_from_defecting:
                return Action.C
            else:
                return Action.D
        if CR < self.game_description.m / self.game_description.n_players:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if CR == self.game_description.m / self.game_description.n_players:
            payoff_from_cooperating = 1 - 1 + self.game_description.k
            payoff_from_defecting = 1 - 0
            if payoff_from_cooperating > payoff_from_defecting:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_235(BaseStrategy):
    """
    An adaptive strategy that chases the reward k while maintaining cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.τ = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        prev_payoffs = self.calculate_payoff(history.my_actions[-1], prev_cooperators)
        if prev_cooperators >= self.game_description.m:
            return Action.C
        reward_chaser_threshold = np.mean(history.opponent_payoffs[-1, :]) + self.game_description.k / 2
        if history.my_payoffs[-1] > reward_chaser_threshold:
            return Action.C
        defectors = self.game_description.n_players - prev_cooperators
        if defectors > self.game_description.n_players - self.game_description.m:
            return Action.D
        avg_cooperation_rate = self.calculate_avg_coop_rate(history, min(self.game_description.n_rounds / 2, 10), self.τ)
        if avg_cooperation_rate > self.τ:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

    def calculate_payoff(self, action: bool, cooperators: int) -> float:
        """
        Calculates payoff for a single player based on their action and number of cooperators.

        Args:
            action (bool): True means COOPERATE, False means DEFECT.
            cooperators (int): Number of players who cooperated in the same round.

        Returns:
            float: Payoff for this round.
        """
        if cooperators < self.game_description.m:
            return 1 - int(action)
        return 1 - int(action) + self.game_description.k

    def calculate_avg_coop_rate(self, history: PlayerHistory, rounds: int, τ: float) -> float:
        """
        Calculates the average cooperation rate for a given number of rounds.

        Args:
            history (PlayerHistory): History of game states.
            rounds (int): Number of rounds to consider.
            τ (float): Threshold value.

        Returns:
            float: Average cooperation rate.
        """
        if rounds == 0 or rounds > len(history.my_actions):
            return τ
        total_cooperations = sum(history.my_actions[-rounds:]) + np.sum(history.opponent_actions[-rounds:, :], axis=0)
        avg_coop_rate = total_cooperations / (self.game_description.n_players * rounds)
        return avg_coop_rate

class Strategy_EXPLOITATIVE_236(BaseStrategy):
    """
    A strategic approach for playing the Collective Risk Dilemma game, 
    balancing exploitation and cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.my_payoff = 0.0
        self.avg_payoff = 0.0
        self.num_cooperators = 0
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.my_payoff = history.my_payoffs[-1]
        self.avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        self.num_cooperators = sum(history.opponent_actions[-1, :] == 1)
        if history.my_actions[-1] == 0:
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = 0
        if self.num_cooperators < self.game_description.m and self.my_payoff > self.avg_payoff:
            return Action.D
        elif self.num_cooperators >= self.game_description.m or self.my_payoff < self.avg_payoff:
            return Action.C
        elif self.consecutive_defections >= 2 and state.round_number > 1 and (self.num_cooperators < self.game_description.m):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return random.choice([Action.C, Action.D])

class Strategy_EXPLOITATIVE_237(BaseStrategy):
    """
    Exploitative strategy that adapts to the level of cooperation in previous rounds.
    Cooperates initially, and then uses a threshold-based mechanism to decide whether 
    to cooperate or defect. If the number of cooperators is below the threshold, 
    defects as punishment; otherwise, cooperates unless in the final round, when it always defects.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev = np.sum(history.opponent_actions[-1, :] == True)
        avg_payoff_prev = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        if num_cooperators_prev >= self.game_description.m or avg_payoff_prev >= self.game_description.k:
            action = Action.C
        else:
            action = Action.D
        self.history.append((action == Action.C, history.my_payoffs[-1]))
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_238(BaseStrategy):
    """
    A robust and adaptive strategy designed to exploit opponents in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_estimate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 3:
            return Action.C
        if self.threshold_estimate is None:
            opponent_cooperations = sum(history.opponent_actions[:3, :].mean(axis=0))
            self.threshold_estimate = math.ceil(opponent_cooperations)
        if state.round_number < self.game_description.n_rounds - 1:
            opponent_cooperations_last_round = sum(history.opponent_actions[-1, :])
            if opponent_cooperations_last_round > self.threshold_estimate:
                self.threshold_estimate += 1
            elif opponent_cooperations_last_round < self.threshold_estimate and self.threshold_estimate > 0:
                self.threshold_estimate -= 1
            if opponent_cooperations_last_round >= self.threshold_estimate:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_239(BaseStrategy):
    """
    Adaptive Risk-Taker strategy: 
    This strategy adapts to the game's history and exploits opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        reward_factor = self.game_description.k
        n_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            p_coop = m_cooperators_needed / n_players
            action = Action.C if random.random() < p_coop else Action.D
        elif state.round_number == n_rounds - 1:
            action = Action.D
        else:
            C_avg = sum(self.history) / (state.round_number * n_players)
            if C_avg >= m_cooperators_needed / n_players:
                p_coop = 0
            else:
                p_coop = reward_factor * (m_cooperators_needed / n_players - C_avg)
            action = Action.C if random.random() < p_coop else Action.D
        self.history.append(action == Action.C)
        return action

class Strategy_EXPLOITATIVE_240(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's dynamics by balancing cooperation and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_payoff = game_description.m * game_description.k + (game_description.n_players - game_description.m)
        self.punishment_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        prev_defectors = self.game_description.n_players - prev_cooperators
        total_payoff_prev_round = history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :])
        if prev_cooperators < self.game_description.m:
            return Action.C
        elif prev_cooperators >= self.game_description.m and total_payoff_prev_round > self.threshold_payoff:
            return Action.D
        elif prev_defectors > math.ceil(self.game_description.n_players / 2):
            self.punishment_flag = True
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.punishment_flag:
            self.punishment_flag = False
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_242(BaseStrategy):
    """
    Exploitative strategy adapting to game history and opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperation_probability = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < self.cooperation_probability else Action.D
        prev_outcome = sum(history.my_actions[-1] + history.opponent_actions[-1, :])
        if prev_outcome >= self.game_description.m:
            if not history.my_actions[-1]:
                self.cooperation_probability += 0.05
            else:
                self.cooperation_probability -= 0.05
        else:
            self.cooperation_probability += 0.15
        if state.round_number > self.game_description.n_rounds - 3:
            self.cooperation_probability *= (self.game_description.n_rounds - state.round_number) / 3
        if math.isclose(self.cooperation_probability, 0.5):
            return Action.D
        elif all(history.opponent_actions[-1, :]):
            self.cooperation_probability += 0.25
        elif not any(history.opponent_actions[-1, :]):
            self.cooperation_probability -= 0.25
        self.cooperation_probability = max(0.0, min(self.cooperation_probability, 1.0))
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_243(BaseStrategy):
    """
    Exploits opponents' behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.coop_threshold = 0.6
        self.min_payoff_threshold = 0.5 * self.r

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        num_cooperators_last_m_rounds = sum(history.opponent_actions[-self.m:, :].sum(axis=1))
        avg_coop_rate = num_cooperators_last_m_rounds / self.n / min(self.r, state.round_number + 1)
        if state.round_number < self.m:
            return Action.D if num_cooperators_last_m_rounds < self.m else Action.C
        payoff_diff = np.sum(history.my_payoffs) - state.round_number * self.k
        if state.round_number >= self.r - self.m:
            return Action.D if payoff_diff < 0 else Action.C
        return Action.C if avg_coop_rate > self.coop_threshold else Action.D

    def calculate_payoff(self, action, opponent_actions):
        total_cooperators = sum(opponent_actions) + (1 if action == Action.C else 0)
        if total_cooperators >= self.m:
            return 1 - int(action == Action.D) + self.k
        return 1 - int(action == Action.D)

    def update_payoff_diff(self, state: GameState, history: PlayerHistory):
        my_actions = np.array([action.value for action in history.my_actions])
        opponent_actions = history.opponent_actions
        payoff_diff = 0
        for t in range(state.round_number + 1):
            my_action = Action(my_actions[t])
            payoff_diff += self.calculate_payoff(my_action, opponent_actions[t]) - (self.k if my_action == Action.C else 1)
        return payoff_diff

class Strategy_EXPLOITATIVE_244(BaseStrategy):
    """
    Exploitative strategy that tracks the minimum number of cooperators needed 
    to meet the threshold while adapting to opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = 0
        self.exploitable_opponents = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action and num_cooperators >= self.game_description.m:
                if i not in self.exploitable_opponents:
                    self.exploitable_opponents.append(i)
        if num_cooperators >= self.game_description.m:
            self.threshold_met += 1
        if self.threshold_met >= (state.round_number - 1) / 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i in self.exploitable_opponents:
            if history.opponent_actions[-1, i]:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_245(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma game that aims to balance exploitation with adaptability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_payoff = 0.0
        self.cooperators_prev_round = []
        self.unreliable_opponents = set()
        self.cooperative_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        self.total_payoff += self.calculate_payoff(history.my_actions[-1], cooperators_prev_round)
        if state.round_number == 1:
            return Action.C
        elif state.round_number <= self.game_description.n_rounds // 2:
            if len(self.cooperators_prev_round) >= self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif state.round_number <= 3 * self.game_description.n_rounds // 4:
            average_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
            if self.total_payoff > average_opponent_payoff:
                return Action.D
            else:
                return Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            if cooperators_prev_round < self.game_description.m:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

    def calculate_payoff(self, my_action: bool, cooperators: int) -> float:
        """
        Calculate the payoff for a given action and number of cooperators.
        :param my_action: True if cooperate (Action.C), False if defect (Action.D)
        :param cooperators: Number of players who cooperated in this round
        :return: Payoff for this player
        """
        c_i = int(my_action)
        if cooperators < self.game_description.m:
            return 1.0 - c_i
        else:
            return 1.0 - c_i + self.game_description.k

    def mark_opponents(self, my_action: bool, opponent_actions: NDArray[np.bool_], cooperators_prev_round: int):
        """
        Mark opponents as unreliable or cooperative based on their actions.
        :param my_action: True if cooperate (Action.C), False if defect (Action.D)
        :param opponent_actions: Array of opponents' actions
        :param cooperators_prev_round: Number of players who cooperated in the previous round
        """
        for i, action in enumerate(opponent_actions):
            if my_action == Action.D and cooperators_prev_round >= self.game_description.m:
                self.unreliable_opponents.add(i)
            elif my_action == Action.C and cooperators_prev_round < self.game_description.m:
                self.cooperative_opponents.add(i)

class Strategy_EXPLOITATIVE_246(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC) strategy.
    
    This strategy balances exploitation with adaptability, ensuring robustness against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if num_cooperators_last_round == self.game_description.m - 1:
            return Action.C if random.random() < 0.5 else Action.D
        if num_cooperators_last_round >= self.game_description.m:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_247(BaseStrategy):
    """
    Hybrid exploitative strategy for Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.my_recent_cooperations = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1]
        cooperators_last_round = sum(opponent_actions_last_round)
        coop_rate = cooperators_last_round / self.game_description.n_players
        self.cooperation_rate_history.append(coop_rate)
        if state.round_number > 0:
            my_action_last_round = bool(history.my_actions[-1])
            self.my_recent_cooperations += int(my_action_last_round)
        coop_threshold = 0.6
        defect_threshold = 0.4
        if coop_rate > coop_threshold:
            return Action.D
        elif coop_rate < defect_threshold:
            return Action.C
        elif random.random() > 0.5:
            return Action.C
        else:
            return Action.D
        if state.round_number > self.game_description.n_rounds - self.game_description.n_rounds / 5:
            return Action.C
        if state.round_number % (self.game_description.n_rounds // 10) == 0:
            return Action.D

class Strategy_EXPLOITATIVE_248(BaseStrategy):
    """
    A strategy that balances cooperation and exploitation in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperators = 0
        self.previous_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        current_round_cooperators = sum(history.opponent_actions[-1, :])
        if self.previous_action is not None and history.my_payoffs[-1] > 0:
            self.avg_cooperators = 0.5 * self.avg_cooperators + 0.5 * current_round_cooperators
        expected_payoff_c = self.game_description.k - 1 if self.avg_cooperators >= self.game_description.m else 0
        expected_payoff_d = self.game_description.k
        cooperate = expected_payoff_c > expected_payoff_d or (state.round_number == self.game_description.n_rounds - 1 and self.avg_cooperators >= self.game_description.m)
        if self.previous_action is not None and self.previous_action == Action.D and (current_round_cooperators < self.game_description.m):
            cooperate = False
        if current_round_cooperators >= self.game_description.m and self.avg_cooperators < self.game_description.m:
            cooperate = False
        action = Action.C if cooperate else Action.D
        self.previous_action = action
        return action

class Strategy_EXPLOITATIVE_249(BaseStrategy):
    """
    A strategy that initially cooperates and then adapts to opponents' behavior by exploiting their willingness to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
        threshold_ratio = prev_cooperators / self.game_description.n_players
        if threshold_ratio > self.theta:
            return Action.C
        elif threshold_ratio == self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_250(BaseStrategy):
    """
    Exploitative strategy based on tracking cooperation rates and adapting to changing circumstances.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.p_coop = 0.0
        self.rounds_left = game_description.n_rounds - 1
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        self.m_prev = sum(history.opponent_actions[-1, :])
        self.p_coop = np.mean(history.my_actions[:state.round_number]) if state.round_number > 0 else 0.5
        if self.m_prev >= self.game_description.m:
            action = Action.C if random.random() < self.p_coop else Action.D
        else:
            action = Action.D
        if random.random() < self.epsilon:
            action = Action.D if action == Action.C else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.m_prev == self.game_description.m - 1 and abs(self.p_coop - 0.5) < 0.01:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_251(BaseStrategy):
    """
    The Adaptive Exploiter strategy in Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met_prev = False
        self.avg_payoff_coop = 0.0
        self.avg_payoff_defect = 0.0
        self.opponent_actions = np.zeros((game_description.n_rounds, game_description.n_players - 1), dtype=np.bool_)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        if state.round_number == 0:
            return Action.C if random.random() < m / n else Action.D
        t = state.round_number - 1
        prev_t = t - 1
        if t == 0:
            return Action.C if random.random() < m / n else Action.D
        self.opponent_actions[t, :] = history.opponent_actions[-2]
        threshold_met_prev = self.threshold_met_prev
        self.threshold_met_prev = sum(self.opponent_actions[prev_t]) >= m
        avg_payoff_coop_prev = self.avg_payoff_coop
        avg_payoff_defect_prev = self.avg_payoff_defect
        if history:
            cooperators = self.opponent_actions[t, :] | np.array([history.my_actions[-2]])
            num_cooperators = sum(cooperators)
            coop_payoffs = np.where(cooperators, history.opponent_payoffs[-1], 0)
            defect_payoffs = np.where(~cooperators & ~np.isnan(history.opponent_payoffs[-1]), history.opponent_payoffs[-1], 0)
            self.avg_payoff_coop = np.mean(coop_payoffs) if len(coop_payoffs[coop_payoffs > 0]) else avg_payoff_coop_prev
            self.avg_payoff_defect = np.mean(defect_payoffs) if len(defect_payoffs[defect_payoffs > 0]) else avg_payoff_defect_prev
        if self.threshold_met_prev:
            return Action.C if self.avg_payoff_coop > self.avg_payoff_defect else Action.D
        else:
            num_defectors = n - 1 - sum(self.opponent_actions[prev_t])
            prev_num_defectors = n - 1 - sum(self.opponent_actions[prev_t - 1]) if t > 1 else 0
            return Action.D if num_defectors > prev_num_defectors else Action.C if random.random() < m / (n + 1) else Action.D

class Strategy_EXPLOITATIVE_252(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation levels while being mindful of the game's threshold requirement.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.w = 3
        self.theta = 0.7
        self.delta = 0.15

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperation_rate = self.calculate_opponent_cooperation_rate(history)
        action = Action.C if opponent_cooperation_rate >= self.theta else Action.D
        if random.random() < self.delta:
            action = Action.D if action == Action.C else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

    def calculate_opponent_cooperation_rate(self, history: PlayerHistory) -> float:
        start_index = max(0, history.my_actions.size - self.w)
        opponent_actions_in_window = history.opponent_actions[start_index:, :]
        cooperation_count = np.sum(opponent_actions_in_window)
        return cooperation_count / (opponent_actions_in_window.size * self.game_description.n_players)

class Strategy_EXPLOITATIVE_253(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to balance cooperation and defection 
    to maximize its payoff while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.w = 5
        self.δ = 0.1
        self.coop_rate = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :] == True)
        if m_t >= self.game_description.m:
            action = Action.D
        else:
            action = Action.C
        π_avg = np.mean(history.my_payoffs[max(0, state.round_number - self.w):])
        if π_avg > self.game_description.k:
            self.coop_rate -= self.δ
        elif π_avg < 1:
            self.coop_rate += self.δ
        if state.round_number == self.game_description.n_rounds - 1:
            m_t_prev = sum(history.opponent_actions[-2, :] == True) if state.round_number >= 2 else self.game_description.m
            if m_t_prev >= self.game_description.m:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_255(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        coop_rate = cooperators / self.game_description.n_players
        k = self.game_description.k
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if cooperators >= self.game_description.m - 1 else Action.D
        if cooperators < self.game_description.m:
            return Action.D
        if coop_rate >= (self.game_description.m - 1) / self.game_description.n_players:
            return Action.D
        elif k > 1 + (1 - coop_rate) * (self.game_description.n_players - self.game_description.m):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_256(BaseStrategy):
    """
    A strategy that adapts to the game parameters and history, aiming to exploit opponents while being robust to various behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        prev_opponent_actions = history.opponent_actions[-1, :]
        prev_opponent_payoffs = history.opponent_payoffs[-1, :]
        prev_threshold_met = np.sum(prev_opponent_actions) + (history.my_actions[-1] == 1) >= self.game_description.m
        if prev_threshold_met:
            self.p = self.game_description.k / (self.game_description.k + 1)
        else:
            self.p = 1 - self.game_description.m / self.game_description.n_players
        for i, action in enumerate(prev_opponent_actions):
            if not action and prev_threshold_met:
                self.p -= 0.1
            elif action and (not prev_threshold_met):
                self.p += 0.1
        self.p = max(0, min(self.p, 1))
        return Action.C if random.random() < self.p else Action.D

class Strategy_EXPLOITATIVE_257(BaseStrategy):
    """
    Exploitative strategy "Adaptive Threshold Hunter" (ATH) for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M = [0] * game_description.n_rounds
        self.R = [0.0] * game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        M_t = self.M[state.round_number - 1]
        R_t = self.R[state.round_number - 1]
        if state.round_number == self.game_description.n_rounds:
            return Action.C
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if M_t >= m - 1 and R_t > (n_players - M_t + 1) * k:
            return Action.C
        elif M_t < m - 1:
            coop_prob = (m - M_t) / n_players
            return Action.C if random.random() < coop_prob else Action.D
        elif R_t > n_players * k / 2:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_258(BaseStrategy):
    """
    A dynamic, history-dependent strategy that adapts to the opponents' behavior 
    while exploiting their willingness to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1]
        prev_my_payoff = history.my_payoffs[-1]
        self.previous_round_cooperators = sum(prev_opponent_actions) + (1 if history.my_actions[-1] else 0)
        all_previous_actions = prev_opponent_actions.tolist() + [history.my_actions[-1]]
        if all(all_previous_actions):
            p = 1 - 1 / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        defector_count = sum((1 for action in prev_opponent_actions if not action)) + (0 if history.my_actions[-1] else 1)
        if state.round_number > 1:
            previous_defector_count = sum((1 for action in history.opponent_actions[-2, :] if not action)) + (0 if history.my_actions[-2] else 1)
            if defector_count + previous_defector_count > self.game_description.n_players / 2:
                return Action.D
        if self.previous_round_cooperators < self.game_description.m:
            return Action.D
        p = self.previous_round_cooperators / self.game_description.n_players * (self.game_description.k / (1 + self.game_description.k))
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_259(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperation level of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        self.num_cooperators_prev_round = sum(opponent_actions)
        if state.round_number < self.game_description.n_rounds - 1:
            p_c = (self.game_description.m - self.num_cooperators_prev_round) / self.game_description.n_players
            p_d = 1 - self.num_cooperators_prev_round / self.game_description.n_players
            return Action.C if random.random() < p_c else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_260(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy.
    
    This strategy aims to exploit opponents by adapting its cooperation threshold based on the game history.
    It encourages cooperation when it leads to higher payoffs and deters exploitation by defecting when opponents are not cooperative enough.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        M = sum(history.opponent_actions[-1, :])
        avg_M = sum((sum(history.opponent_actions[t, :]) for t in range(max(0, state.round_number - self.game_description.n_rounds // 2), state.round_number))) / min(self.game_description.n_rounds // 2, state.round_number)
        if avg_M > self.game_description.m:
            self.threshold -= 0.01
        elif avg_M < self.game_description.m:
            self.threshold += 0.01
        if M >= self.game_description.m:
            return Action.C if 1 - 1 + self.game_description.k > 1 else Action.D
        elif avg_M > self.game_description.m:
            return Action.C if M >= self.threshold else Action.D
        else:
            return Action.D if M < self.threshold else Action.C

class Strategy_EXPLOITATIVE_262(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in a Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.threshold_adj = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3 or state.round_number == 0:
            return Action.C
        prev_coop_count = self.coop_count
        self.coop_count = sum(history.opponent_actions[-1, :] != False) + (history.my_actions[-1] == True)
        if self.coop_count >= self.game_description.m - 1 + self.threshold_adj:
            return Action.C
        elif self.coop_count == self.game_description.m - 1 and 1 - (history.my_actions[-1] == True) + self.game_description.k < 1:
            return Action.D
        else:
            return Action.D
        if state.round_number % 3 == 0:
            avg_coop_count = sum((sum(history.opponent_actions[i, :] != False) + (history.my_actions[i] == True) for i in range(max(0, state.round_number - 2), state.round_number))) / 3
            if avg_coop_count < self.game_description.m / 2:
                self.threshold_adj += 1
            elif avg_coop_count > self.game_description.m / 2:
                self.threshold_adj -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.coop_count == self.game_description.m - 1 and history.my_actions[-1] is not None:
            return Action.C if history.my_actions[-1] else Action.D

class Strategy_EXPLOITATIVE_263(BaseStrategy):
    """
    A strategy that initially cooperates, then exploits the opponents' behavior by defecting when possible and cooperating to maintain a high overall cooperation rate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploitation_phase = False
        self.last_cooperation_rate = 0
        self.rounds_since_last_defection = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        self.exploitation_phase = True
        if state.round_number == self.game_description.n_rounds - 1:
            if sum(history.opponent_actions[-1, :]) >= self.game_description.m:
                return Action.C
        if history is not None and len(history.my_payoffs) > 5:
            opponent_cooperation_rate = np.mean(np.sum(history.opponent_actions, axis=1)) / (self.game_description.n_players - 1)
            if opponent_cooperation_rate < 0.2:
                return Action.D
        if history is not None:
            last_round_opponents = history.opponent_actions[-1, :]
            cooperators_last_round = sum(last_round_opponents)
            if cooperators_last_round < self.game_description.m:
                return Action.D
            else:
                self.last_cooperation_rate = np.mean(last_round_opponents)
        if self.last_cooperation_rate >= 0.5:
            self.rounds_since_last_defection += 1
            if self.rounds_since_last_defection == 5:
                self.rounds_since_last_defection = 0
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_264(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy leverages game parameters and history 
    to make adaptive decisions, aiming for maximum payoff while being exploitative.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_threshold = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_actions = history.opponent_actions[-1, :]
        last_round_cooperators = sum(last_round_actions)
        second_last_round_cooperators = None if state.round_number < 2 else sum(history.opponent_actions[-2, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if second_last_round_cooperators >= self.m_threshold else Action.D
        if last_round_cooperators >= self.m_threshold:
            return Action.C if self.n_players - last_round_cooperators <= math.floor(self.n_players / 2) else Action.D
        elif last_round_cooperators == self.m_threshold - 1:
            return Action.C
        if state.round_number > 2 and all((not action for action in history.opponent_actions[-1, :])):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_265(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    
    The strategy adapts to the game's history and parameters by probing other players' willingness to cooperate,
    adjusting cooperation probability based on observed cooperation rates and success rates, and defecting when
    cooperation rates or success rates are low.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.5
        self.success_rate = 0.5
        self.coop_prob = game_description.m / game_description.n_players

    def calculate_cooperation_rate(self, history_round: NDArray[np.bool_]) -> float:
        return sum(history_round) / len(history_round)

    def calculate_success_rate(self, history: PlayerHistory) -> float:
        if not history.my_actions.size:
            return 0.5
        threshold_met = history.opponent_actions.sum(axis=1) + history.my_actions >= self.game_description.m
        return sum(threshold_met) / len(history.my_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        last_round_history = history.opponent_actions[-1, :]
        self.cooperation_rate = self.calculate_cooperation_rate(last_round_history)
        self.success_rate = self.calculate_success_rate(history)
        if self.cooperation_rate > 0.5 and self.success_rate > 0.5:
            self.coop_prob = min(self.game_description.k / (self.game_description.k + 1), 1)
        elif self.cooperation_rate <= 0.5 or self.success_rate <= 0.5:
            self.coop_prob = 0
        action = Action.C if random.random() < self.coop_prob else Action.D
        threshold_met = sum(last_round_history) + int(action == Action.C) >= self.game_description.m
        if state.round_number > 1:
            if threshold_met and action == Action.C:
                self.coop_prob = min(self.coop_prob + 0.1, 1)
            elif not threshold_met and action == Action.D:
                self.coop_prob = max(self.coop_prob - 0.1, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_266(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit the game's structure and opponents' behaviors while being robust to various opponent strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T_i = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        average_payoff_per_player = np.mean(history.opponent_payoffs[-1, :] + [history.my_payoffs[-1]])
        if num_cooperators < self.game_description.m:
            self.T_i -= 1
            return Action.D
        if self.game_description.k > average_payoff_per_player:
            if self.T_i >= num_cooperators:
                self.T_i += 1
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_267(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game that initially 
    encourages cooperation and then exploits or revives cooperation based on 
    opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= r // 2:
            prev_coop_count = sum(history.opponent_actions[-1, :])
            if prev_coop_count < m:
                return Action.D
            else:
                return Action.C
        elif state.round_number > r // 2 and state.round_number < r:
            opponent_payoffs = np.sum(history.opponent_payoffs, axis=1)[:-1]
            if np.mean(opponent_payoffs[-5:]) > np.mean(opponent_payoffs[:-5]):
                return Action.D
            else:
                return Action.C
        elif state.round_number == r:
            return Action.D

class Strategy_EXPLOITATIVE_268(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game.
    It adapts to changing conditions by adjusting its cooperation threshold based on the game's history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperator_count = 0
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if history.my_actions[-1]:
            self.cooperator_count += 1
        else:
            self.cooperator_count -= 1
        self.update_threshold(self.cooperator_count)
        expected_payoff = self.game_description.k > 1 + (self.threshold - self.cooperator_count)
        if expected_payoff or (state.round_number == self.game_description.n_rounds - 1 and history.my_actions[-2]):
            return Action.C
        return Action.D

    def update_threshold(self, cooperators_t: int):
        """
        Update the cooperation threshold based on the number of cooperators in the previous round.
        """
        if cooperators_t >= self.game_description.m:
            self.threshold = min(self.game_description.m + 1, self.game_description.n_players)
        else:
            self.threshold = max(1, self.threshold - 1)

class Strategy_EXPLOITATIVE_269(BaseStrategy):
    """
    A strategy that adapts to the game's dynamics while minimizing its own risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round = history.opponent_actions[-1]
        total_cooperators = sum(previous_round)
        if state.round_number < self.game_description.n_players:
            if total_cooperators < self.game_description.m or self.previous_payoff <= 0:
                return Action.D
            else:
                return Action.C
        elif self.game_description.n_players <= state.round_number < 2 * self.game_description.n_players:
            if total_cooperators == self.game_description.m:
                return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
            elif total_cooperators < self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif self.previous_payoff <= 0 or all((not opponent for opponent in previous_round)):
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and self.previous_payoff + self.game_description.k > self.previous_payoff:
            return Action.C

    def set_previous_payoff(self, previous_payoff):
        self.previous_payoff = previous_payoff

class Strategy_EXPLOITATIVE_270(BaseStrategy):
    """
    A strategy that adapts to changing game conditions by monitoring the number of cooperators and adjusting behavior based on the reward factor k.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_prev = sum(history.opponent_actions[-1, :] == True)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if num_cooperators_prev < self.game_description.m:
            return Action.D
        elif self.game_description.k > 2.5:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_271(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to balance cooperation and defection 
    to maximize individual payoff while adapting to the behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        num_cooperators = sum(history.opponent_actions[-1, :])
        ACR = num_cooperators / (self.game_description.n_players - 1)
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if ACR >= self.game_description.m / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_273(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :])
        if prev_coops <= self.game_description.m - 1:
            return Action.D
        opponent_defected = any((not action for action in history.opponent_actions[-1, :]))
        if opponent_defected and prev_coops >= self.game_description.m:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_274(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's dynamics and opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        cr = np.mean(opponent_cooperations + history.my_actions) / self.game_description.n_players
        et = (self.game_description.n_players - self.game_description.m + 1) / self.game_description.n_players
        if cr >= self.game_description.m / self.game_description.n_players:
            if cr > et:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if cr < self.game_description.m / self.game_description.n_players:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_275(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = 0
        self.defect_count = 0
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        prev_cooperators = sum(prev_round_actions)
        if prev_cooperators >= self.game_description.m:
            self.threshold *= 0.99
        else:
            self.threshold *= 1.05
        if state.round_number == 1:
            ec = self.cooperate_count / 1 * self.game_description.k
            ed = 1 + self.defect_count / 1
        else:
            ec = self.cooperate_count / (state.round_number - 1) * self.game_description.k
            ed = 1 + self.defect_count / (state.round_number - 1)
        if ec > ed and random.random() < self.threshold:
            return Action.C
        else:
            return Action.D
        prev_my_action = history.my_actions[-1]
        if prev_my_action:
            self.cooperate_count += 1
        else:
            self.defect_count += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_276(BaseStrategy):
    """
    Exploitative strategy for Collective Risk Dilemma game that adapts cooperation probability based on history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = game_description.m / game_description.n_players
        self.p_defect = 1 - game_description.k ** (-1)
        self.p_coop = 1 - 1 / game_description.n_players
        self.Δp = 1 / (game_description.n_players * game_description.m)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.p_init else Action.D
        threshold_met = sum(history.opponent_actions[-1, :] + [True]) >= self.game_description.m
        success_history = np.array([sum(history.my_payoffs[max(0, i - self.game_description.m):i] > 0) for i in range(state.round_number)])
        coop_prob = max(0, min(self.p_init + (self.Δp if np.mean(success_history[-self.game_description.m:]) >= 0.5 else -self.Δp), 1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif threshold_met:
            return Action.D if random.random() < self.p_defect else Action.C
        else:
            return Action.C if random.random() < coop_prob else Action.D

class Strategy_EXPLOITATIVE_277(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behavior while minimizing losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_cooperators < self.game_description.m:
            return Action.D
        p = (self.game_description.k - 1) / (self.game_description.k * (self.game_description.n_players - self.game_description.m + 1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_278(BaseStrategy):
    """
    An adaptive strategy that balances exploration, exploitation, and adaptation throughout the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = [0] * (game_description.n_rounds + 1)
        self.total_payoff = [0.0] * (game_description.n_rounds + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        t = state.round_number
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if t <= 5:
            prev_cooperators = self.cooperate_count[t - 1]
            if prev_cooperators < m:
                return Action.D
            else:
                p = k / (k + 1)
                return Action.C if random.random() < p else Action.D
        c_avg = sum(self.cooperate_count[max(0, t - 6):t]) / min(t, 5)
        if c_avg < m / n_players:
            return Action.D
        else:
            p = k / (k + 1)
            return Action.C if random.random() < p else Action.D
        tp = sum(self.total_payoff[max(0, t - 6):t]) / min(t, 5)
        action = self.__call__(state, history)
        if tp > 2 * n_players and action == Action.D:
            return Action.C
        self.cooperate_count[t] = self.cooperate_count[t - 1] + (1 if action == Action.C else 0)
        self.total_payoff[t] = self.total_payoff[t - 1] + (1.0 if action == Action.D else k)
        return action

class Strategy_EXPLOITATIVE_279(BaseStrategy):
    """
    Adaptive Greed Strategy for Collective Risk Dilemma.

    This strategy aims to maximize individual payoffs by adapting to the opponents' behavior and exploiting their cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_payoffs = [1 - action for action in history.my_actions]
        avg_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if sum((1 for payoff in prev_round_payoffs if payoff == 1)) >= self.game_description.n_players - self.game_description.m + 1:
            if avg_coop_rate > sum(history.my_actions[:-1]) / state.round_number:
                return Action.D
            else:
                return Action.C
        elif prev_round_payoffs[-1] < sum(prev_round_payoffs) / self.game_description.n_players:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_280(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy balances short-term gains with long-term risk management by:
    - Encouraging cooperation in early rounds to set a positive tone.
    - Adapting to opponents' behaviors using a moving average of cooperation rates.
    - Manipulating the endgame to maximize personal gain while minimizing collective risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_bool = history.opponent_actions.astype(bool)
        avg_coop_rate = np.mean(opponent_actions_bool[-min(5, state.round_number - 1):], axis=0).mean()
        if state.round_number < 3:
            if sum(history.opponent_actions[-1]) < self.game_description.m:
                action = Action.D
            else:
                action = Action.C
        elif state.round_number == self.game_description.n_rounds - 2:
            if sum(history.opponent_actions[-1]) >= self.game_description.m:
                action = Action.D
            else:
                action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        elif avg_coop_rate > (self.game_description.m - 1) / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        self.history.append({'actions': [action], 'payoffs': []})
        return action

class Strategy_EXPLOITATIVE_281(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    It prioritizes maximizing individual payoffs while adapting to the dynamics of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        threshold_met = prev_round_coops >= self.game_description.m
        if threshold_met:
            return Action.D
        elif prev_round_coops >= self.game_description.m - 1 and self.game_description.k > 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_282(BaseStrategy):
    """
    The Threshold-Adaptive Defector (TAD) strategy capitalizes on collective rewards when opponents cooperate, 
    while minimizing our own contribution when the threshold is not met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.thresh_coop_rate = 0.5
        self.avg_coop_payoff = 0
        self.coop_count = 0
        self.defect_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_rate = np.mean(history.opponent_actions[-1, :])
        prev_avg_payoff = self.avg_coop_payoff
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            self.coop_count += 1
        else:
            self.defect_count += 1
        coop_payoffs = [payoff for action, payoff in zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :]) if action]
        if coop_payoffs:
            self.avg_coop_payoff = np.mean(coop_payoffs)
        if prev_coop_rate < self.thresh_coop_rate or opponent_cooperators < self.game_description.m:
            return Action.D
        if prev_avg_payoff > self.avg_coop_payoff and history.my_actions[-1] == False:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_283(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' behavior in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.defection_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coops = sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :])
        if prev_coops >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
            opponent_payoffs_avg = np.mean(history.opponent_payoffs)
            my_payoff_avg = history.my_payoffs[-1]
            if opponent_payoffs_avg > my_payoff_avg and random.random() < 0.5:
                self.defection_rate += 0.1
                if self.defection_rate > 0.5:
                    action = Action.D
        return action

class Strategy_EXPLOITATIVE_284(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's history and punishes non-cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        num_cooperators = sum(prev_round)
        if num_cooperators >= self.game_description.m:
            return Action.C
        if self.total_payoff > np.mean(history.my_payoffs) and self.game_description.k > 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_rounds == 1 and self.game_description.m > self.game_description.n_players - 2:
            return Action.C
        else:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_285(BaseStrategy):
    """
    An exploitative strategy for Collective Risk Dilemma that adapts to game parameters and opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.n_players <= 4 and state.round_number >= self.n_rounds - 2:
            return Action.D
        if state.round_number == 0:
            return Action.C
        past_coop_count = sum((1 for h in history.my_actions if h))
        avg_payoff = np.mean(history.my_payoffs)
        if state.round_number <= 5:
            opponent_coops_last_round = sum(history.opponent_actions[-1, :])
            if opponent_coops_last_round >= self.m:
                return Action.C
            elif opponent_coops_last_round < math.ceil(self.m / 2):
                return Action.D
        if state.round_number > 5 and state.round_number <= math.floor(self.n_rounds / 2):
            past_avg_payoff = np.mean(history.my_payoffs[:-1])
            if avg_payoff < past_avg_payoff:
                return Action.D
            opponent_coops_last_round = sum(history.opponent_actions[-1, :])
            if opponent_coops_last_round >= math.ceil(self.m / 2):
                return Action.C
            else:
                return Action.D
        if state.round_number > math.floor(self.n_rounds / 2) and state.round_number < self.n_rounds - 1:
            past_avg_payoff = np.mean(history.my_payoffs[:-1])
            if avg_payoff > past_avg_payoff:
                return Action.C
            opponent_coops_last_round = sum(history.opponent_actions[-1, :])
            if opponent_coops_last_round >= math.ceil(self.m / 3):
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_286(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for Collective Risk Dilemma.
    
    This strategy adapts its behavior based on the game history, increasing exploitation 
    as opponents become more cooperative and punishing non-cooperation when necessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.w = 3
        self.avg_coop_thresh = 0.5
        self.delta_p = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        p_def = min(0.5 + (prev_cooperators - self.game_description.m) / self.game_description.n_players, 1)
        avg_coop_rate = np.mean(history.opponent_actions[-self.w:, :] + history.my_actions[-self.w:].reshape(-1, 1))
        if avg_coop_rate > self.avg_coop_thresh:
            p_def += self.delta_p
        if prev_cooperators < self.game_description.m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and prev_cooperators >= self.game_description.m:
            return Action.C
        return Action.D if random.random() < p_def else Action.C

class Strategy_EXPLOITATIVE_287(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1] if len(history.my_payoffs.shape) == 2 else history.my_payoffs)
        punish_opponent = False
        for opponent in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, opponent] and history.my_actions[-1]:
                punish_opponent = True
                break
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        if (prev_round_cooperators < self.game_description.m or avg_payoff_prev_round > 1.5) and (not punish_opponent):
            return Action.D
        if punish_opponent:
            self.punishment_rounds_remaining = math.ceil(self.game_description.n_players / 2)
            return Action.D
        if prev_round_cooperators >= self.game_description.m and history.my_payoffs[-1] > 1.5:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            threshold_met_recently = np.all((prev_round_cooperators >= self.game_description.m for _ in range(math.ceil(self.game_description.n_players / 2))))
            return Action.C if threshold_met_recently else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_288(BaseStrategy):
    """
    An adaptive strategy that exploits the weaknesses of other players by 
    initially cooperating with moderate probability, defecting when others' 
    defection leads to a higher payoff, punishing defectors who undermine collective 
    success, and conditionally cooperating in other cases.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.t_init_coop = False
        self.prev_round_defectors = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            coop_prob = random.random()
            if coop_prob < 0.5:
                return Action.C
            else:
                return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_round_cooperators < self.game_description.m:
            return Action.D
        elif any((opponent in self.prev_round_defectors for opponent in range(self.game_description.n_players))):
            return Action.D
        else:
            coop_prob = random.random()
            if coop_prob < 0.7:
                return Action.C
            else:
                return Action.D
        self.prev_round_defectors = [opponent for opponent in range(self.game_description.n_players) if not history.opponent_actions[-1, opponent]]

class Strategy_EXPLOITATIVE_289(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.unreliable_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif prev_cooperators >= self.game_description.m:
            p = (self.game_description.k * (self.game_description.m - 1) / self.game_description.n_players) ** ((state.round_number + 1) / self.game_description.n_rounds)
            if random.random() < p and state.round_number != self.game_description.n_rounds - 1:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_290(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    Adapts to the game parameters and history without relying on coordination mechanisms or shared norms.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_hat = 0
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if self.game_description.n_players == 2:
            return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if prev_round_cooperators >= self.game_description.m:
            self.m_hat = self.game_description.m
        else:
            self.m_hat = math.ceil(prev_round_cooperators)
        estimated_cooperators = sum(np.mean(history.opponent_actions, axis=0) > 0.5)
        if estimated_cooperators <= self.m_hat - 1:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        payoffs = np.concatenate(([0], history.my_payoffs)) if history is not None else [0]
        self.avg_payoff = np.mean(payoffs)
        if self.avg_payoff < 1.5 and self.m_hat > 1:
            self.m_hat -= 1
        elif self.avg_payoff > 2.5 and self.m_hat < self.game_description.n_players - 1:
            self.m_hat += 1
        return action

class Strategy_EXPLOITATIVE_291(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by tracking the number of cooperators and adjusting its threshold accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.w = 5
        self.cooperators_count = None
        self.defectors_count = None
        self.payoff_diff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == 1:
            self.cooperators_count = history.my_actions[0] + sum(history.opponent_actions[0, :])
            self.defectors_count = self.game_description.n_players - self.cooperators_count
            return Action.D if self.cooperators_count < self.m else Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        prev_round_defs = self.game_description.n_players - prev_round_coops
        if prev_round_coops <= self.m - 1:
            return Action.C
        else:
            return Action.D
        if state.round_number % self.w == 0 and state.round_number > 0:
            coop_payoffs = history.my_payoffs[history.my_actions]
            def_payoffs = history.my_payoffs[~history.my_actions]
            self.payoff_diff = np.mean(coop_payoffs) - np.mean(def_payoffs)
            if self.payoff_diff > 0.2 and self.m < self.game_description.n_players:
                self.m += 1
            elif self.payoff_diff < -0.2 and self.m > 1:
                self.m -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_round_coops == self.m - 1 and self.payoff_diff > 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_293(BaseStrategy):
    """
    Exploitative strategy that adapts to game dynamics by adjusting cooperation rates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.payoff_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return self.initial_round()
        if state.round_number <= math.floor(self.game_description.n_rounds / 2):
            return self.middle_rounds()
        average_cooperation_rate = np.mean([int(action) for action in self.cooperation_rate_history])
        if average_cooperation_rate < self.game_description.m / self.game_description.n_players:
            cooperate_probability = 0.5
        else:
            cooperate_probability = self.game_description.m / self.game_description.n_players
        return self.adaptive_action(cooperate_probability)

    def initial_round(self) -> Action:
        cooperate_probability = self.game_description.m / self.game_description.n_players
        action = random.random() < cooperate_probability
        self.cooperation_rate_history.append(action)
        return Action.C if action else Action.D

    def middle_rounds(self) -> Action:
        cooperate_probability = self.game_description.m / self.game_description.n_players
        action = random.random() < cooperate_probability
        self.cooperation_rate_history.append(action)
        return Action.C if action else Action.D

    def adaptive_action(self, cooperate_probability: float) -> Action:
        action = random.random() < cooperate_probability
        if len(self.payoff_history) == self.game_description.n_rounds - 1 and (not action):
            payoff = sum(self.payoff_history)
            if payoff < self.game_description.k:
                return Action.D
        self.cooperation_rate_history.append(action)
        return Action.C if action else Action.D

    def update_payoff_history(self, payoff: float) -> None:
        self.payoff_history.append(payoff)

class Strategy_EXPLOITATIVE_294(BaseStrategy):
    """
    An exploitative strategy that adapts to opponent behavior in a Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_cooperation = np.mean(history.opponent_actions[-1, :])
        threshold_met = sum(history.opponent_actions[-1, :] == 1) >= self.game_description.m
        if history.my_payoffs.size > 0 and history.my_payoffs[-1] == 0:
            if opponent_cooperation > 0.5 and threshold_met:
                self.cooperation_probability = min(1, max(0, self.cooperation_probability - 0.05))
            else:
                self.cooperation_probability = min(1, max(0, self.cooperation_probability + 0.1))
        if history.opponent_actions.size > 2 and np.all(history.opponent_actions[-3:, :] == 0):
            self.consecutive_defections += 1
            if self.consecutive_defections >= 3:
                return Action.D if random.random() < 0.9 else Action.C
        else:
            self.consecutive_defections = 0
        if state.round_number == self.game_description.n_rounds - 1:
            threshold_met_before = np.any(np.sum(history.opponent_actions, axis=1) >= self.game_description.m)
            return Action.C if threshold_met_before else Action.D
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_295(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        k = self.game_description.k
        if m_t < self.game_description.m:
            return Action.D
        elif m_t >= self.game_description.m:
            p = k / (k + 1)
            return Action.C if random.random() < p else Action.D
        else:
            p = 0.5
            return Action.C if random.random() < p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_296(BaseStrategy):
    """
    A strategy that exploits opponents' behavior while adapting to the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        prev_my_payoff = history.my_payoffs[-1]
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif prev_my_payoff + self.game_description.k > 1:
            return Action.C
        else:
            return Action.D
        for i in range(self.game_description.n_players):
            opponent_action = history.opponent_actions[-1, i]
            opponent_payoff = history.opponent_payoffs[-1, i]
            if opponent_action == False and opponent_payoff > self.game_description.k:
                if prev_cooperators < self.game_description.m:
                    return Action.C
                else:
                    return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_297(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.initial_rounds = math.ceil(0.1 * self.game_description.n_rounds)
        self.exploitation_mode_threshold = 0.5
        self.punishment_counter = 0
        self.punishment_rounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if state.round_number > self.initial_rounds:
            if previous_cooperators < self.game_description.m:
                return Action.D
            else:
                opponent_cooperation_rate = np.mean(history.opponent_actions[:, :])
                if opponent_cooperation_rate > self.exploitation_mode_threshold:
                    if history.my_actions[-1] and (not all(history.opponent_actions[-1, :])):
                        self.punishment_counter = self.punishment_rounds
                    return Action.C
                else:
                    return Action.D
        opponent_cooperation_rate = np.mean(history.opponent_actions[:, :])
        if opponent_cooperation_rate > 0.7:
            self.exploitation_mode_threshold += 0.1
        elif opponent_cooperation_rate < 0.3:
            self.exploitation_mode_threshold -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_298(BaseStrategy):
    """
    This strategy adapts to the game's history by initially cooperating, 
    then defecting when the collective effort is unlikely to succeed, and 
    cooperating only when there's a good chance of success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        num_coop_prev = sum(prev_round) + (history.my_actions[-1] == True)
        num_def_prev = self.n - num_coop_prev
        if num_coop_prev < self.m:
            return Action.D
        elif num_coop_prev >= self.m:
            total_payoff_prev = 1 - history.my_actions[-1] + self.k if num_coop_prev >= self.m else 0
            if total_payoff_prev >= self.k and num_def_prev <= self.n - self.m:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_299(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation rates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M = 0
        self.S = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        t = state.round_number
        m = self.game_description.m
        k = self.game_description.k
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if history.my_payoffs.size > 0:
            my_last_payoff = history.my_payoffs[-1]
            self.S = (self.S * (t - 1) + my_last_payoff) / t
        if opponent_cooperators >= m:
            self.M += 1
        threshold = 1 - (k - 1) / k
        avg_threshold = 1 + (k - 1) / 2
        if self.M / t > threshold or self.S > avg_threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_300(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coop_count = 0
        self.cooperation_threshold = game_description.m - 1
        self.threshold_met_punishment_prob = min(0.5, game_description.k / 2)
        self.last_round_cooperate = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_opponent_actions = history.opponent_actions[-1]
        self.prev_coop_count = np.sum(prev_opponent_actions)
        if state.round_number == self.game_description.n_rounds - 1:
            if self.game_description.n_rounds % 2 == 0:
                return Action.C
        if self.prev_coop_count < self.cooperation_threshold - 1:
            return Action.D
        elif self.prev_coop_count >= self.game_description.m:
            return Action.D
        elif self.prev_coop_count >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.C if random.random() < self.threshold_met_punishment_prob else Action.D

class Strategy_EXPLOITATIVE_301(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game.
    
    This strategy initially encourages cooperation, then adjusts its behavior 
    based on observed outcomes and the game's parameters to balance exploitation 
    with cooperation and maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploration_phase = min(game_description.n_rounds // 4, 5)
        self.cooperation_rate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= self.exploration_phase + 1:
            return Action.D if (state.round_number - 1) % 2 == 0 else Action.C
        else:
            cr = sum(self.cooperation_rate_history[-self.exploration_phase:]) / self.exploration_phase
            if history is not None:
                cooperators_in_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
                self.cooperation_rate_history.append(cooperators_in_last_round / self.game_description.n_players)
            if cr >= self.game_description.m / self.game_description.n_players:
                return Action.C
            elif abs(cr - self.game_description.m / self.game_description.n_players) <= 0.1:
                return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_302(BaseStrategy):
    """
    Exploitative strategy: "Adaptive Risk-Taker".
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0
        self.prev_threshold_met = False
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if state.round_number > 1:
            self.avg_coop_rate = (self.avg_coop_rate * (state.round_number - 2) + self.prev_cooperators / self.game_description.n_players) / (state.round_number - 1)
        risk_threshold = 0.5 * self.game_description.m / self.game_description.n_players
        if self.avg_coop_rate < risk_threshold:
            return Action.D
        elif self.prev_cooperators >= self.game_description.m and self.prev_threshold_met:
            return Action.C
        else:
            return Action.D
        self.prev_threshold_met = self.prev_cooperators >= self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1 and self.avg_coop_rate >= risk_threshold:
            return Action.C

class Strategy_EXPLOITATIVE_304(BaseStrategy):
    """
    Exploitative strategy that adapts to the behavior of other players and exploits their cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        elif state.round_number < self.n_rounds // 2:
            prev_cooperators = sum(history.opponent_actions[-1, :])
            p = 1 - (self.m - 1) / self.n_players
            if prev_cooperators < self.m and random.random() < p:
                return Action.C
            else:
                return Action.D
        else:
            c_rate = self.calculate_c_rate(history.opponent_actions)
            p = 1 - (self.m - 1) / self.n_players
            if c_rate > 0.5 and random.random() < p:
                return Action.C
            elif state.round_number == self.n_rounds - 1:
                return Action.D
            else:
                return Action.D

    def calculate_c_rate(self, opponent_actions):
        total_cooperations = sum((sum(round) for round in opponent_actions))
        return total_cooperations / (self.n_players * len(opponent_actions))

class Strategy_EXPLOITATIVE_305(BaseStrategy):
    """
    Adaptive Reward-Seeking (ARS) Strategy.
    
    This is a history-dependent, adaptive strategy designed to exploit opponents' behaviors while maximizing personal payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.k_avg = 0.0
        self.opponent_cooperation_rates = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if history.my_payoffs.size > 1:
            self.update_k_avg(history)
            self.update_opponent_cooperation_rates(history)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.k_avg < 1.0:
            return Action.D
        if self.k_avg >= 1.5:
            return Action.C
        for opponent, rate in self.opponent_cooperation_rates.items():
            if rate > 0.75:
                return Action.D
            elif rate < 0.25:
                return Action.C
        return Action.D

    def update_k_avg(self, history: PlayerHistory):
        rewards = history.my_payoffs[history.my_actions]
        self.k_avg = np.mean(rewards)

    def update_opponent_cooperation_rates(self, history: PlayerHistory):
        for opponent in range(history.opponent_actions.shape[1]):
            rate = np.mean(history.opponent_actions[:, opponent])
            self.opponent_cooperation_rates[opponent] = rate

class Strategy_EXPLOITATIVE_306(BaseStrategy):
    """
    The Adaptive Risk-Taker strategy aims to maximize its payoff by exploiting the game's structure and adapting to the behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperators = 0
        self.prev_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.prev_cooperators.append(sum(history.opponent_actions[-1, :]))
        if len(self.prev_cooperators) > 2:
            self.prev_cooperators.pop(0)
        if state.round_number < 2:
            return Action.C
        self.avg_cooperators = sum(self.prev_cooperators) / len(self.prev_cooperators)
        if self.avg_cooperators >= self.game_description.m:
            return Action.C
        if len(self.prev_cooperators) > 1 and self.prev_cooperators[-1] + 1 >= self.game_description.m and (self.prev_cooperators[-1] < self.game_description.m):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_307(BaseStrategy):
    """
    Exploit the Collective Risk Dilemma game by monitoring the threshold and adapting to opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prime_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None:
            opponent_actions = history.opponent_actions[-1, :]
            my_action = history.my_actions[-1]
            cooperators = sum(opponent_actions) + (1 if my_action else 0)
            self.m_prime_history.append(cooperators)
        if len(self.m_prime_history) < 2:
            return Action.C
        m_prime_current = self.m_prime_history[-1]
        m_prime_previous = self.m_prime_history[-2]
        if m_prime_current < self.game_description.m:
            return Action.D
        elif m_prime_current >= self.game_description.m:
            if m_prime_current > m_prime_previous or m_prime_current == m_prime_previous:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_308(BaseStrategy):
    """
    Conditional Cooperator with Adaptive Threshold (CCAT) strategy for Collective Risk Dilemma.
    
    This strategy balances adaptability and exploitation by initially cooperating, observing the frequency of cooperation in the group,
    and then conditionally cooperating based on an adaptive threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.thresh = max(1, game_description.m - 1)
        self.prev_round_coops = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 5 or self.game_description.n_rounds < 10:
            prev_cooperators = sum(history.opponent_actions[-1, :])
            self.avg_coop += (prev_cooperators - self.avg_coop) / state.round_number
            if self.avg_coop >= self.game_description.m / 2.0:
                self.thresh = max(self.game_description.m - 1, math.floor(self.avg_coop * 0.8))
            else:
                self.thresh = max(1, self.game_description.m - 1)
        if history is not None:
            prev_cooperators = sum(history.opponent_actions[-1, :])
            self.prev_round_coops = prev_cooperators
        if state.round_number > self.game_description.n_rounds - 2:
            return Action.D
        if self.prev_round_coops >= self.thresh or random.random() < (self.game_description.k - 1.0) / (self.game_description.n_players * self.game_description.k):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_309(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to maximize its payoff by exploiting 
    the cooperative behavior of others while minimizing its own contributions to the community project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.tau = game_description.m
        self.alpha = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        m_t = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if m_t >= self.tau:
            action = Action.C
        elif state.round_number > 1:
            p_defect = (self.tau - m_t) / (self.game_description.n_players - 1)
            action = random.choices([Action.C, Action.D], weights=[1 - p_defect, p_defect])[0]
        else:
            action = Action.D
        self.tau += self.alpha * (m_t - self.tau)
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperate_payoff = sum(history.my_payoffs) + (self.game_description.k if m_t >= self.game_description.m else 0)
            private_defect_payoff = 1
            if total_cooperate_payoff > private_defect_payoff:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_310(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by assessing their willingness to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 3:
            return Action.C
        else:
            self.avg_coop = (self.coop_count + sum(history.opponent_actions[-1, :])) / (state.round_number - 2)
            if self.avg_coop <= self.game_description.m / 2:
                action = Action.D
            else:
                num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
                if num_cooperators_prev_round >= self.game_description.m:
                    action = Action.D
                else:
                    action = Action.C
                if state.round_number % 5 == 0 and state.round_number > 3:
                    new_avg_coop = (self.coop_count + sum(history.opponent_actions[-1, :])) / (state.round_number - 2)
                    if new_avg_coop < self.avg_coop * 0.8:
                        action = Action.C
            if state.round_number == self.game_description.n_rounds - 1:
                num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
                if num_cooperators_prev_round >= self.game_description.m:
                    return Action.D
                else:
                    return Action.C
            self.coop_count = sum(history.opponent_actions[-1, :])
            return action

class Strategy_EXPLOITATIVE_311(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.avg_payoff = 0.0
        self.total_payoff = 0.0
        self.opp_coop_rate = 0.0
        self.num_coop_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number > 2 and self.avg_coop == 0:
            avg_coop_list = [sum(history.opponent_actions[i, :]) + history.my_actions[i] for i in range(state.round_number)]
            self.avg_coop = np.mean(avg_coop_list)
        if state.round_number > 1 and self.avg_payoff == 0:
            my_payoffs = [history.my_payoffs[i] * (1 - history.my_actions[i]) + history.my_payoffs[i] * self.game_description.k * history.my_actions[i] for i in range(state.round_number)]
            self.avg_payoff = np.mean(my_payoffs)
        if state.round_number > 0:
            my_payoffs = [history.my_payoffs[i] * (1 - history.my_actions[i]) + history.my_payoffs[i] * self.game_description.k * history.my_actions[i] for i in range(state.round_number)]
            self.total_payoff = sum(my_payoffs)
        if state.round_number > 3:
            prev_round_coop = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            prev_round_payoff = history.my_payoffs[-1] * (1 - history.my_actions[-1]) + history.my_payoffs[-1] * self.game_description.k * history.my_actions[-1]
            if state.round_number > 5:
                opponent_coop_list = [sum(history.opponent_actions[i, :]) for i in range(state.round_number)]
                self.opp_coop_rate = np.mean(opponent_coop_list) / (self.game_description.n_players - 1)
            if prev_round_payoff < self.avg_payoff or prev_round_coop < self.game_description.m:
                return Action.C
        elif state.round_number == self.game_description.n_rounds:
            avg_total_payoffs = [sum([history.my_payoffs[i] * (1 - history.my_actions[i]) + history.my_payoffs[i] * self.game_description.k * history.my_actions[i] for i in range(state.round_number)]) / state.round_number for _ in range(self.game_description.n_players)]
            avg_total_payoff = np.mean(avg_total_payoffs)
            if self.total_payoff < avg_total_payoff:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_312(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by balancing cooperation and defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        prev_coops = sum(history.opponent_actions[-1, :] + history.my_actions[-1])
        m = self.game_description.m
        if prev_coops < m / 2:
            T = m - 1
        elif prev_coops >= m / 2 and prev_coops < 3 * m / 4:
            T = m
        else:
            T = m + 1
        expected_coops = self._estimate_cooperators(history)
        if state.round_number < self.game_description.n_rounds - 1:
            return Action.C if expected_coops <= T else Action.D
        else:
            return Action.C if expected_coops == m - 1 else Action.D

    def _estimate_cooperators(self, history: PlayerHistory) -> int:
        avg_cooperation_rate = np.mean(history.opponent_actions + history.my_actions[:, None])
        return math.floor(avg_cooperation_rate * self.game_description.n_players)

class Strategy_EXPLOITATIVE_313(BaseStrategy):
    """
    An adaptive strategy that balances cooperation and defection to maximize individual payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = np.full(game_description.n_players - 1, 0.5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        self._update_opponent_cooperation_rates(history)
        previous_round_outcome = history.opponent_actions[-1, :]
        threshold_met = np.sum(previous_round_outcome) >= self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if threshold_met:
            opponents_cooperated = np.sum(history.opponent_actions[-1, :])
            if opponents_cooperated >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        else:
            opponents_defected = np.sum(~previous_round_outcome)
            if opponents_defected > 0:
                return Action.D
            else:
                cooperation_probability = self._get_cooperation_probability()
                return Action.C if random.random() < cooperation_probability else Action.D

    def _update_opponent_cooperation_rates(self, history: PlayerHistory):
        previous_round_outcome = history.opponent_actions[-1, :]
        for i in range(len(previous_round_outcome)):
            self.opponent_cooperation_rates[i] = (self.opponent_cooperation_rates[i] * history.my_payoffs.size + int(previous_round_outcome[i])) / (history.my_payoffs.size + 1)

    def _get_cooperation_probability(self):
        return np.mean(np.where(self.opponent_cooperation_rates >= 0.7, 0.8, self.game_description.m / self.game_description.n_players))

class Strategy_EXPLOITATIVE_314(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Cooperative Threshold" (ACT)

    This strategy adapts to the opponents' behavior by adjusting its cooperation rate based on the average number of cooperators.
    It initially tests the waters with a cooperation probability of m/n, then punishes non-cooperative behavior while maintaining a balance between self-interest and community contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        avg_coop = np.mean(history.my_actions[:-1]) if len(history.my_actions) > 1 else self.coop_prob
        if avg_coop >= self.game_description.m / self.game_description.n_players:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        if len(history.my_actions) > 0 and history.my_actions[-1] and (sum(history.opponent_actions[-1, :]) < self.game_description.m):
            return Action.D if random.random() < 0.5 else Action.C
        return Action.D

class Strategy_EXPLOITATIVE_315(BaseStrategy):
    """
    The Adaptive Threshold Seeker strategy is designed to exploit potential free-riders and manipulate the number of cooperators to maximize its own payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.defect_rate = 0.0
        self.cooperate_rate = 1.0
        self.alpha = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is None or len(history.my_payoffs) < self.alpha:
            prev_round_cooperators = sum(history.opponent_actions[-1, :]) if history else 0
            return Action.D if prev_round_cooperators < self.game_description.m else Action.C
        avg_payoff = np.mean(history.my_payoffs[-self.alpha:])
        if avg_payoff < 1:
            self.defect_rate += 0.1
            self.cooperate_rate = max(0, 1 - self.defect_rate)
        else:
            self.cooperate_rate -= 0.05
            self.cooperate_rate = max(0, min(self.cooperate_rate, 1))
            self.defect_rate = 1 - self.cooperate_rate
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if prev_round_cooperators < self.game_description.m:
            return Action.D
        else:
            return Action.C if random.random() <= self.cooperate_rate else Action.D

class Strategy_EXPLOITATIVE_316(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy.
    
    Initially explores the game by cooperating with a probability of 0.5 for the first three rounds.
    Then, it estimates the threshold probability based on opponents' cooperation rates and adapts its strategy accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = []
        self.memory = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C if random.random() < 0.5 else Action.D
        self.update_opponent_rates(history.opponent_actions)
        alpha = self.estimate_threshold(self.opponent_cooperation_rates)
        return Action.C if alpha >= 0.5 else Action.D

    def update_opponent_rates(self, opponent_actions):
        num_cooperators = sum((1 for action in opponent_actions[-1] if action))
        self.opponent_cooperation_rates.append(num_cooperators / (self.game_description.n_players - 1))

    def estimate_threshold(self, opponent_cooperation_rates):
        avg_coop_rate = sum(opponent_cooperation_rates) / len(opponent_cooperation_rates)
        alpha = min(1, max(0, 0.5 + (avg_coop_rate - 0.5) * (self.game_description.n_rounds - len(self.opponent_cooperation_rates)) / self.game_description.n_rounds))
        return alpha

class Strategy_EXPLOITATIVE_317(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that balances cooperation to achieve the threshold with exploitation to maximize personal payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.cooperate_count = 0
        self.my_payoffs = []
        self.all_payoffs = [[] for _ in range(game_description.n_players)]
        self.adaptive_adjustment_rounds = max(5, math.ceil(game_description.n_rounds / 10))
        self.last_opponent_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_history = history.my_payoffs[state.round_number - 1]
        opponent_actions = history.opponent_actions[state.round_number - 1]
        self.cooperate_count = sum(opponent_actions)
        self.threshold_met = self.cooperate_count >= self.game_description.m
        if state.round_number % self.adaptive_adjustment_rounds == 0:
            average_payoffs = [np.mean(player_payoffs[:state.round_number]) for player_payoffs in self.all_payoffs]
            top_earner_index = np.argmax(average_payoffs)
            if average_payoffs[top_earner_index] > np.mean(self.my_payoffs):
                self.last_opponent_actions = history.opponent_actions[:, top_earner_index]
        if self.threshold_met:
            if np.mean(self.my_payoffs) > np.mean([np.mean(player_payoffs[:state.round_number]) for player_payoffs in self.all_payoffs]):
                return Action.C
            else:
                return Action.D
        elif self.cooperate_count > self.game_description.n_players // 2 and self.cooperate_count < self.game_description.m:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D
        if self.last_opponent_actions is not None:
            return Action(self.last_opponent_actions[state.round_number - 1])

class Strategy_EXPLOITATIVE_318(BaseStrategy):
    """
    Exploitative strategy aiming to maximize individual payoffs by adapting to the evolving cooperation threshold and exploiting opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C] * self.game_description.m + [Action.D] * (self.game_description.n_players - self.game_description.m))
        c_prev = sum(history.opponent_actions[-1, :] + history.my_actions[-1:])
        m_est = self.game_description.m - 1 + c_prev / self.game_description.n_players
        if c_prev >= m_est:
            return Action.C
        elif c_prev == math.floor(m_est):
            return random.choice([Action.C, Action.D])
        else:
            return Action.D

class Strategy_EXPLOITATIVE_319(BaseStrategy):
    """
    An adaptive strategy that exploits the behavior of opponents in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.threshold_adjustment_rounds = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators >= self.m:
            return Action.D
        previous_payoffs = history.opponent_payoffs[-1, :] if len(history.opponent_payoffs) > 0 else np.zeros(self.game_description.n_players - 1)
        average_payoff = np.mean(previous_payoffs) if len(previous_payoffs) > 0 else 0
        trigger_cooperation = any([payoff > average_payoff for payoff in previous_payoffs])
        if trigger_cooperation:
            return Action.C
        consecutive_defections = np.sum(history.opponent_actions[-2:, :], axis=0) == 0
        punish_free_riders = any(consecutive_defections)
        if punish_free_riders:
            return Action.D
        if state.round_number % self.threshold_adjustment_rounds == 0:
            self.adaptive_threshold_adjustment(history)
        return Action.C

    def adaptive_threshold_adjustment(self, history: PlayerHistory):
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators < self.m:
            self.m -= 1
        elif num_cooperators > self.m:
            self.m += 1

class Strategy_EXPLOITATIVE_320(BaseStrategy):
    """
    Exploitative strategy that adapts to changing cooperation levels by adjusting its threshold for cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators < self.threshold:
            self.threshold += 1
        elif opponent_cooperators > self.threshold:
            self.threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif opponent_cooperators < self.threshold:
            return Action.D
        elif opponent_cooperators >= self.threshold:
            return Action.C

class Strategy_EXPLOITATIVE_321(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while prioritizing the pursuit of rewards when the threshold is likely to be met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m
        self.previous_round_cooperators = None
        self.opponent_defection_patterns = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == 1:
            self.previous_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
            if self.previous_round_cooperators < self.game_description.m:
                return Action.D
            else:
                return Action.C
        self.previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if self.previous_round_cooperators >= self.game_description.m:
            return Action.C
        if self.previous_round_cooperators < self.threshold:
            self.threshold += 1
        else:
            self.threshold = self.game_description.m
        if self.previous_round_cooperators >= self.threshold:
            return Action.C
        for i, opponent_actions in enumerate(history.opponent_actions.T):
            if i not in self.opponent_defection_patterns:
                self.opponent_defection_patterns[i] = []
            self.opponent_defection_patterns[i].append(not opponent_actions[-1])
        for opponent, defections in self.opponent_defection_patterns.items():
            if len(defections) > state.round_number // 4 and sum(defections) / len(defections) > 0.75:
                self.threshold -= 1
                break
        if state.round_number == self.game_description.n_rounds - 1:
            if self.previous_round_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_322(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def calculate_avg_payoff(self, actions: NDArray[np.bool_]) -> float:
        avg_cooperators = np.mean(actions)
        avg_defectors = 1 - avg_cooperators
        if avg_cooperators < self.m / self.n_players:
            return 1 - avg_cooperators
        else:
            return 1 - avg_cooperators + self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :] if history else np.array([])
        prev_round_coops = np.sum(prev_round_actions)
        avg_payoff_prev_round = self.calculate_avg_payoff(np.append(history.my_actions[-1], prev_round_actions))
        if prev_round_coops < self.m:
            return Action.D
        else:
            T = avg_payoff_prev_round / self.k
            p = T / (1 + T) if T != 0 else 0.5
            return Action.C if random.random() < p else Action.D
        opponent_actions = history.opponent_actions[:, :] if history else np.array([])
        for i in range(self.n_players):
            if all(opponent_actions[:, i] == False):
                return Action.D
        if state.round_number % 2 == 0 and self.game_description.n_rounds % 2 == 1:
            return Action.C

class Strategy_EXPLOITATIVE_323(BaseStrategy):
    """
    A strategy for playing the Collective Risk Dilemma game that balances cooperation and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = 0.5
        self.alpha = 0.2
        self.beta = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D]) if random.random() < self.p_coop else Action.D
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if avg_payoff_prev_round >= self.game_description.k:
            coop_prob = max(0.5, self.alpha * (self.game_description.m - 1) / self.game_description.n_players)
        elif num_cooperators_prev_round >= self.game_description.m - 2 and avg_payoff_prev_round < self.game_description.k:
            coop_prob = min(1, self.beta * (self.game_description.n_players - self.game_description.m + 2) / self.game_description.n_players)
        else:
            coop_prob = self.p_coop
        if state.round_number == self.game_description.n_rounds - 1 and np.sum(history.my_payoffs) <= self.game_description.k * state.round_number:
            return Action.C
        if num_cooperators_prev_round == 0:
            coop_prob = 0.5
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_EXPLOITATIVE_324(BaseStrategy):
    """
    A hybrid strategy that combines elements of tit-for-tat, win-stay-lose-shift, and adaptive threshold adjustment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_outcome = history.opponent_actions[-1, :]
        opponent_defected = any((not action for action in previous_outcome))
        if self.game_description.k >= 1 and self.threshold <= sum((action for action in previous_outcome)):
            cooperate = True
        else:
            cooperate = False
        if opponent_defected or not cooperate:
            return Action.D
        return Action.C

    def update_threshold(self, history: PlayerHistory):
        success = sum((1 for actions in history.opponent_actions if sum((action for action in actions)) >= self.game_description.m)) > 0
        failure = not success
        if success:
            self.threshold += 1
            if self.threshold > self.game_description.n_players - 1:
                self.threshold = self.game_description.n_players - 1
        elif failure:
            self.threshold -= 1
            if self.threshold < 2:
                self.threshold = 2

class Strategy_EXPLOITATIVE_325(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to balance cooperation and exploitation by adapting to the opponent's behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_value = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        prev_round_outcome = sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :]) >= self.game_description.m
        opponent_cooperation_rate = np.mean(history.opponent_actions)
        if prev_round_outcome:
            if opponent_cooperation_rate > self.threshold_value:
                action = Action.C
            else:
                action = Action.D
        else:
            defect_prob = 1 - self.game_description.m / self.game_description.n_players
            action = Action.C if random.random() < defect_prob else Action.D
        if prev_round_outcome and opponent_cooperation_rate:
            self.threshold_value += 0.01
        elif not prev_round_outcome or not opponent_cooperation_rate:
            self.threshold_value -= 0.01
        return action

class Strategy_EXPLOITATIVE_326(BaseStrategy):
    """
    The Adaptive Collective Risk (ACR) strategy is designed to exploit a wide range of opponent behaviors 
    while being robust and adaptive. It leverages the game's history to inform its decisions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        num_cooperators_prev_round = sum((1 for action in history.opponent_actions[-1] if action))
        if self.m == 1:
            return Action.C
        if self.n_players == 2:
            return Action.D if state.round_number % 2 == 0 else Action.C
        if num_cooperators_prev_round >= self.m or len(history.opponent_actions) < self.game_description.n_rounds - 1:
            return Action.C
        p = self.k * (self.m - num_cooperators_prev_round) / (self.n_players * self.k)
        if math.isclose(p, float('inf')) or math.isnan(p):
            return Action.D
        if random.random() < p:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_327(BaseStrategy):
    """
    Adaptive Collective Risk Exploiter Strategy.

    Balances individual self-interest with collective risk management,
    while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_initial = game_description.m
        self.k = game_description.k
        self.round_number = 0
        self.opponent_behavior = {}
        self.previous_action = None
        self.previous_round_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.round_number += 1
            return Action.C if self.m_initial <= self.n_players / 2 else Action.D
        self.round_number += 1
        opponent_actions = history.opponent_actions[-1, :]
        for i in range(self.n_players):
            if i != self.game_description.n_players - 1:
                if i not in self.opponent_behavior:
                    self.opponent_behavior[i] = []
                self.opponent_behavior[i].append(opponent_actions[i])
                if len(self.opponent_behavior[i]) >= 3:
                    cooperation_rate = sum(self.opponent_behavior[i][-3:]) / 3
                    if cooperation_rate > 0.7 and self.game_description.m < self.n_players - 1:
                        self.game_description.m += 1
                    elif cooperation_rate < 0.3 and self.game_description.m > 1:
                        self.game_description.m -= 1
        if self.previous_round_payoff is None or self.previous_round_payoff >= self.k * self.game_description.m:
            action = Action.C if self.previous_action == Action.D else Action.C if self.previous_action is None else self.previous_action
        else:
            action = Action.D if self.previous_action == Action.C else Action.D
        self.previous_action = action
        if history.my_payoffs.size > 0:
            self.previous_round_payoff = history.my_payoffs[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_328(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = None
        self.delta_p = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            self.coop_prob = m / n_players
            return Action.C if random.random() < self.coop_prob else Action.D
        elif state.round_number == r - 1:
            return Action.D
        prev_outcome = np.sum(history.opponent_actions[-1, :] + (history.my_actions[-1] if history.my_actions.size > 0 else True))
        self.update_probabilities(prev_outcome, history)
        if prev_outcome >= m:
            return Action.D if random.random() < 1 - k / (k + 1) else Action.C
        else:
            coop_adapt = max(0, min(self.coop_prob + self.delta_p, 1))
            return Action.C if random.random() < coop_adapt else Action.D

    def update_probabilities(self, prev_outcome: int, history: PlayerHistory):
        global_delta_p = 0.05
        if prev_outcome >= self.game_description.m and (history.my_actions[-1] if history.my_actions.size > 0 else True) == False:
            self.delta_p -= 0.1
        elif prev_outcome < self.game_description.m and (history.my_actions[-1] if history.my_actions.size > 0 else True):
            self.delta_p += global_delta_p
        self.coop_prob = max(0, min(self.coop_prob + self.delta_p, 1))

class Strategy_EXPLOITATIVE_329(BaseStrategy):
    """
    Adaptive Exploiter strategy balances short-term gains with long-term adaptability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.sequence = [Action.C, Action.D] * math.ceil(game_description.n_rounds / 8)
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        elif state.round_number <= math.floor(self.game_description.n_rounds / 4):
            return self.sequence[state.round_number - 1]
        elif history is not None:
            m_avg = np.mean(np.sum(history.opponent_actions, axis=1))
            if m_avg >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds:
            if history is not None:
                self.total_payoff = np.sum(history.my_payoffs)
                if self.total_payoff < math.floor(self.game_description.k / 2):
                    return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_330(BaseStrategy):
    """
    The Threshold Tracker strategy aims to exploit opponents by adapting to their behavior and maximizing personal payoffs while ensuring the collective risk is averted.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = []
        self.free_riders = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_cooperators = sum(history.opponent_actions[-1, :])
        prev_my_action = bool(history.my_actions[-1])
        for opponent_idx in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, opponent_idx] and prev_cooperators + 1 == self.game_description.m:
                self.free_riders.append(opponent_idx)
        p_c = (self.game_description.k - 1) / self.game_description.k
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_cooperators < self.game_description.m:
            return Action.D
        elif random.random() < p_c and prev_my_action != bool(random.choice([True, False])):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_331(BaseStrategy):
    """
    Exploitative strategy "Conditional Cooperator with Dynamic Threshold" for the Collective Risk Dilemma game.
    
    This strategy adapts to various opponent behaviors while exploiting their tendencies to maximize its own payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_dyn = game_description.m
        self.cr_init = 0.5
        self.coop_est = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        n = self.game_description.n_players
        m_dyn = self.m_dyn
        cr_init = self.cr_init
        opponent_actions = history.opponent_actions[-1, :]
        cr = np.ones(n - 1) * cr_init
        if state.round_number > 1:
            prev_opponent_actions = history.opponent_actions[-2, :]
            for i in range(n - 1):
                if prev_opponent_actions[i]:
                    cr[i] = min(1, cr[i] + 0.1)
                else:
                    cr[i] = max(0, cr[i] - 0.1)
        coop_est = sum(cr)
        private_payoff = 1.0
        if coop_est >= m_dyn:
            exp_payoff = 1 - 1 + self.game_description.k
        else:
            exp_payoff = 1 - 1
        if exp_payoff >= private_payoff and coop_est >= m_dyn:
            action = Action.C
        else:
            action = Action.D
        for i in range(n - 1):
            if cr[i] > 0.9:
                self.m_dyn -= 1
        return action

class Strategy_EXPLOITATIVE_332(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma that balances cooperation and exploitation.
    Initially cooperates, then adapts to opponent behavior by exploiting opportunities 
    and punishing non-cooperation when necessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperators = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if history is not None and len(history.my_actions) > 0:
            self.avg_cooperators = np.mean(np.sum(history.opponent_actions, axis=1))
        if self.avg_cooperators < self.game_description.m:
            return Action.D if state.round_number % 2 == 0 else Action.C
        avg_payoff_defecting = np.mean(history.my_payoffs[history.opponent_actions[:, 0] == False]) if len(history.my_payoffs) > 0 and np.any(history.opponent_actions[:, 0] == False) else float('-inf')
        avg_payoff_cooperating = np.mean(history.my_payoffs[history.opponent_actions[:, 0] == True]) if len(history.my_payoffs) > 0 and np.any(history.opponent_actions[:, 0] == True) else float('inf')
        if self.game_description.k < 2 or avg_payoff_defecting >= avg_payoff_cooperating:
            return Action.D
        elif avg_payoff_defecting < avg_payoff_cooperating:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            opponent_actions = history.opponent_actions[-1] if history is not None else []
            total_cooperators_prev_round = np.sum(opponent_actions) + 1
            return Action.C if total_cooperators_prev_round >= self.game_description.m - 1 else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_333(BaseStrategy):
    """
    An exploitative strategy that adapts to the observed behavior of others.
    It initially cooperates with a moderate probability, then adjusts its cooperation rate based on the previous round's outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        reward_factor = self.game_description.k
        if state.round_number == 0:
            initial_cooperation_prob = m_cooperators_needed / n_players
            return Action.C if random.random() < initial_cooperation_prob else Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        prev_cooperators = sum(prev_round_opponent_actions)
        if prev_cooperators >= m_cooperators_needed:
            cooperate_prob = 1 - (n_players - prev_cooperators) / n_players
        else:
            cooperate_prob = 0
        if prev_cooperators < m_cooperators_needed / 2:
            cooperate_prob = 0
        elif cooperate_prob > 0:
            cooperate_prob *= reward_factor / (reward_factor + 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_334(BaseStrategy):
    """
    Exploitative strategy for Collective Risk Dilemma.
    
    Initially probes the environment by mixing cooperation and defection in early rounds, 
    then adapts based on observed cooperation rates among other players and their average payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_counts_last_w_rounds = []
        self.average_cooperator_payoff = 0.0
        self.average_defector_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C if random.random() < 0.5 else Action.D
        self.update_cooperation_counts(history)
        self.update_average_payoffs(history)
        if sum(self.cooperation_counts_last_w_rounds) / len(self.cooperation_counts_last_w_rounds) >= self.game_description.m:
            if self.average_cooperator_payoff > self.average_defector_payoff:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if sum(self.cooperation_counts_last_w_rounds) / len(self.cooperation_counts_last_w_rounds) >= self.game_description.m and self.average_cooperator_payoff > self.average_defector_payoff:
                return Action.C
        return Action.D

    def update_cooperation_counts(self, history: PlayerHistory):
        cooperators = sum(history.opponent_actions[-1, :])
        self.cooperation_counts_last_w_rounds.append(cooperators)
        if len(self.cooperation_counts_last_w_rounds) > 3:
            self.cooperation_counts_last_w_rounds.pop(0)

    def update_average_payoffs(self, history: PlayerHistory):
        last_round_cooperators = np.where(history.opponent_actions[-1, :] == True)[0]
        last_round_defectors = np.where(history.opponent_actions[-1, :] == False)[0]
        if len(last_round_cooperators) > 0:
            self.average_cooperator_payoff = np.mean(history.opponent_payoffs[-1, last_round_cooperators])
        if len(last_round_defectors) > 0:
            self.average_defector_payoff = np.mean(history.opponent_payoffs[-1, last_round_defectors])

class Strategy_EXPLOITATIVE_335(BaseStrategy):
    """
    Adaptive Exploiter strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.m - 1
        self.prev_num_coop = 0
        self.prev_reward = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_coop_t_minus_1 = sum(history.opponent_actions[-1, :])
        opponent_defected = history.opponent_actions[-1, 0] == False
        prev_num_coop_met_threshold = self.prev_num_coop >= self.game_description.m
        prev_reward_value = self.prev_reward > self.game_description.k * 2
        if prev_num_coop_met_threshold and opponent_defected:
            return Action.D
        if prev_reward_value:
            self.coop_threshold -= 1
        elif self.prev_reward < self.game_description.k / 2:
            self.coop_threshold += 1
        if num_coop_t_minus_1 < self.coop_threshold:
            return Action.D
        else:
            return Action.C
        self.prev_num_coop = num_coop_t_minus_1
        self.prev_reward = 0

class Strategy_EXPLOITATIVE_336(BaseStrategy):
    """
    The ATE strategy is designed to exploit the Collective Risk Dilemma game by adaptively responding 
    to the cooperation levels of other players while ensuring robustness against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_avg = 0.0
        self.last_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 3:
            if state.round_number == 2:
                return Action.D
            else:
                return Action.C
        if history is not None:
            recent_rounds = np.array(history.opponent_actions[-2:, :])
            self.m_avg = (recent_rounds.sum(axis=1).mean() + self.game_description.n_players - 1) / self.game_description.n_players
        if self.m_avg >= (self.game_description.m - 1) / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if history is not None and len(history.my_payoffs) > 0:
            self.last_payoff = history.my_payoffs[-1]
            if self.last_payoff > 1 + self.game_description.k / 2:
                self.m_avg += 0.5
        if state.round_number == self.game_description.n_rounds - 1 and sum(history.my_payoffs) < (self.game_description.n_rounds - 1) * (1 + self.game_description.k / 2):
            return Action.D
        return action

class Strategy_EXPLOITATIVE_337(BaseStrategy):
    """
    Exploit-Risk-Averse (ERA) strategy for the Collective Risk Dilemma game.
    Balances cooperation and exploitation while adapting to the game's progression.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = 0
        self.prev_coop = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.cooperate_count = sum(history.opponent_actions[-1, :])
        if self.cooperate_count >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        if not self.prev_coop and self.cooperate_count >= self.game_description.m and (history.my_payoffs[-1] < 2):
            action = Action.C
        if self.cooperate_count < self.game_description.m and history.my_payoffs[-1] >= self.game_description.k - 1:
            action = Action.D
        self.prev_coop = action == Action.C
        return action

class Strategy_EXPLOITATIVE_338(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.apc_history = []
        self.apd_history = []

    def _calculate_payoffs(self, history: PlayerHistory) -> tuple[float, float]:
        """
        Calculate average payoffs for cooperators and defectors.
        
        :param history: Game history.
        :return: Average payoff for cooperators (APC), average payoff for defectors (APD).
        """
        if len(history.my_actions) < 3:
            return (0.0, 0.0)
        recent_rounds = slice(-3, None)
        cooperators_payoffs = history.my_payoffs[recent_rounds][history.my_actions[recent_rounds]]
        defectors_payoffs = history.my_payoffs[recent_rounds][~history.my_actions[recent_rounds]]
        apc = np.mean(cooperators_payoffs) if len(cooperators_payoffs) else 0.0
        apd = np.mean(defectors_payoffs) if len(defectors_payoffs) else 0.0
        self.apc_history.append(apc)
        self.apd_history.append(apd)
        return (apc, apd)

    def _assess_risk(self, history: PlayerHistory) -> bool:
        """
        Assess the overall risk by calculating the ratio of average cooperators to total players.
        
        :param history: Game history.
        :return: Whether the assessed risk is high (True), or low (False).
        """
        if len(history.my_actions) < 10:
            return False
        recent_rounds = slice(-10, None)
        avg_cooperators = np.mean(np.sum(history.opponent_actions[recent_rounds, :], axis=1)) + history.my_actions[recent_rounds].mean()
        acr = avg_cooperators / self.game_description.n_players
        return acr < self.game_description.m / self.game_description.n_players

    def _mirror_majority(self, history: PlayerHistory) -> Action:
        """
        Mirror the majority action from previous rounds.
        
        :param history: Game history.
        :return: Mirrored action.
        """
        prev_round_actions = history.opponent_actions[-1, :]
        if np.sum(prev_round_actions) >= len(prev_round_actions) / 2:
            return Action.C
        else:
            return Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(history.my_actions) and np.sum(~history.opponent_actions[-1, :]) >= self.game_description.n_players - self.game_description.m + 1:
            return Action.D
        apc, apd = self._calculate_payoffs(history)
        if state.round_number % 5 == 0 and len(history.my_actions) > 10:
            risk_assessment_result = self._assess_risk(history)
            if risk_assessment_result:
                return random.choice([Action.C, Action.D], p=[1 / 3.0, 2 / 3.0]) if apc > apd + self.game_description.k / 4 else Action.D
        elif state.round_number <= 3 and len(history.my_actions) < 3:
            return Action.C if len(history.my_actions) == 0 else self._mirror_majority(history)
        return Action.C if apc > apd + self.game_description.k / 2 else Action.D

    def __del__(self):
        pass

class Strategy_EXPLOITATIVE_339(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit the game's dynamics and opponents' behavior while minimizing individual risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.c_history = 0.5
        self.total_payoff = 0

    def calculate_c_history(self, history: PlayerHistory) -> float:
        cooperative_actions = sum((1 for i in range(history.opponent_actions.shape[1]) if history.opponent_actions[-1, i]))
        total_actions = history.opponent_actions.shape[0] * history.opponent_actions.shape[1]
        return cooperative_actions / total_actions

    def adjust_strategy(self, c_history: float) -> None:
        self.c_history += 0.1

    def get_payoff(self, action: Action, round_number: int, history: PlayerHistory) -> float:
        if sum((1 for i in range(history.opponent_actions.shape[1]) if history.opponent_actions[-1, i])) < self.game_description.m:
            return 1 - (action == Action.C)
        else:
            return 1 - (action == Action.D) + self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            return random.choice([Action.C, Action.D])
        elif state.round_number == self.game_description.n_rounds:
            if self.total_payoff < self.get_payoff(Action.C, state.round_number, history):
                return Action.D
            else:
                return Action.C
        else:
            c_history = self.calculate_c_history(history)
            if c_history >= self.game_description.m / self.game_description.n_players:
                return Action.C
            else:
                if any((self.calculate_c_history(PlayerHistory(my_actions=history.my_actions[:t], my_payoffs=history.my_payoffs[:t], opponent_actions=history.opponent_actions[:t, :], opponent_payoffs=history.opponent_payoffs[:t, :])) < self.game_description.m / self.game_description.n_players for t in range(1, state.round_number))):
                    self.adjust_strategy(c_history)
                return Action.D

class Strategy_EXPLOITATIVE_340(BaseStrategy):
    """
    An exploitative strategy designed to adapt to various opponent behaviors 
    while ensuring robustness against different strategies in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.recent_cooperators = []
        self.threshold_tracker = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if state.round_number < self.game_description.n_rounds // 2:
            if sum(history.opponent_actions[-1, :]) + 1 >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        if state.round_number < 3 * self.game_description.n_rounds // 4:
            self.recent_cooperators.append(sum(history.opponent_actions[-1, :]) + 1)
            self.recent_cooperators = self.recent_cooperators[-3:]
            if len(self.recent_cooperators) > 0:
                self.threshold_tracker = sum(self.recent_cooperators) / len(self.recent_cooperators)
            else:
                self.threshold_tracker = 0
            if self.threshold_tracker > 0.5 and 1 - history.my_actions[-1] < self.game_description.k + 1 - history.opponent_payoffs[-1, :].mean():
                return Action.C
            else:
                return Action.D
        if state.round_number >= 3 * self.game_description.n_rounds // 4:
            total_cooperation_payoff = sum(history.my_payoffs) + (self.game_description.k - history.opponent_payoffs[-1, :].mean())
            expected_defect_payoff = 1 + history.opponent_payoffs[-1, :].mean()
            if total_cooperation_payoff > expected_defect_payoff:
                return Action.C
            else:
                if state.round_number == self.game_description.n_rounds - 1 and sum(history.opponent_actions[-1, :]) + 1 >= self.game_description.m:
                    return Action.C
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_341(BaseStrategy):
    """
    Exploitative strategy that adapts to the collective behavior of opponents 
    while minimizing the risk of cooperating without sufficient support.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        min_cooperators_needed = self.game_description.m
        reward_factor = self.game_description.k
        rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions
        my_payoffs = history.my_payoffs
        opponent_payoffs = history.opponent_payoffs
        prev_coops = sum((1 for action in opponent_actions[-1, :] if action))
        if state.round_number <= n_players // 2:
            return Action.C
        elif state.round_number <= 3 * n_players // 4:
            avg_coop_payoff = np.mean([payoffs for payoffs, actions in zip(opponent_payoffs[-n_players // 2:], opponent_actions[-n_players // 2:]) if all((action for action in actions))])
            avg_defect_payoff = np.mean([payoffs for payoffs, actions in zip(opponent_payoffs[-n_players // 2:], opponent_actions[-n_players // 2:]) if not any((action for action in actions))])
            return Action.C if prev_coops >= min_cooperators_needed and avg_coop_payoff > avg_defect_payoff else Action.D
        else:
            total_coop_payoffs = np.sum([payoffs for payoffs, actions in zip(opponent_payoffs[-n_players // 2:], opponent_actions[-n_players // 2:]) if all((action for action in actions))])
            total_defect_payoffs = np.sum([payoffs for payoffs, actions in zip(opponent_payoffs[-n_players // 2:], opponent_actions[-n_players // 2:]) if not any((action for action in actions))])
            return Action.C if prev_coops >= min_cooperators_needed and total_coop_payoffs > total_defect_payoffs else Action.D

class Strategy_EXPLOITATIVE_342(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to exploit the game's structure and opponent behaviors by adapting its cooperation threshold based on the history of play.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        curr_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if curr_cooperators < self.game_description.m:
            self.threshold += 1
        elif curr_cooperators >= self.game_description.m:
            self.threshold -= 1
            self.threshold = max(self.threshold, self.game_description.m - 1)
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_cooperators >= self.game_description.m:
                return Action.C
        return action

class Strategy_EXPLOITATIVE_343(BaseStrategy):
    """
    Balances cooperation and exploitation by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coops = 0
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        self.prev_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.prev_coops >= self.game_description.m - 1:
            return Action.C
        if random.random() < self.cooperation_probability:
            return Action.C
        else:
            return Action.D
        if self.game_description.k > 2.5:
            self.increase_cooperation()
        elif self.game_description.k < 1.5:
            self.decrease_cooperation()

    def increase_cooperation(self):
        self.cooperation_probability = min(1, self.cooperation_probability + 0.05)

    def decrease_cooperation(self):
        self.cooperation_probability = max(0, self.cooperation_probability - 0.05)

class Strategy_EXPLOITATIVE_344(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by dynamically adjusting the probability of cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :]) + 1
        rounds_remaining = self.game_description.n_rounds - state.round_number
        if prev_coops < self.game_description.m or rounds_remaining == 0:
            return Action.D
        p = max(0.5, min(1 - rounds_remaining / self.game_description.n_rounds, self.game_description.k / (self.game_description.k + 1)))
        if p == 0.5:
            return Action.C
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_345(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy is designed to take advantage of the collective risk dilemma's structure.
    By monitoring the number of cooperators and adjusting behavior accordingly, ATE exploits the fact that some players may be more willing to cooperate than others.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        if game_description.m == 1 or game_description.k == 1:
            self.simple_strategy = Action.C if game_description.m == 1 else Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if hasattr(self, 'simple_strategy'):
            return self.simple_strategy
        if state.round_number == 0:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if m_prev >= self.game_description.m:
            return Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            return Action.D
        else:
            total_cooperate_payoff = sum(history.my_payoffs) + self.game_description.k * history.my_actions.sum()
            total_defect_payoff = sum(history.my_payoffs) + history.my_actions.size
            return Action.C if total_cooperate_payoff >= total_defect_payoff else Action.D

class Strategy_EXPLOITATIVE_346(BaseStrategy):
    """
    The Threshold Tracker strategy is designed to exploit a wide range of opponent behaviors 
    while being adaptive and robust. It maximizes payoffs by identifying when cooperation is 
    likely to be successful and defecting otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.prev_cooperators = sum(history.opponent_actions[-1, :])
        if self.prev_cooperators >= self.game_description.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_347(BaseStrategy):
    """
    A strategic approach for playing the Collective Risk Dilemma game.
    Combines elements of cooperation, defection, and adaptability to maximize payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []
        self.app = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        total_payoff = sum(self.history)
        app = total_payoff / self.game_description.n_players
        et = self.game_description.k * (self.game_description.m - 1) / (self.game_description.n_players - 1)
        if app >= et:
            return Action.C
        else:
            return Action.D

    def update_history(self, payoff: float):
        self.history.append(payoff)

class Strategy_EXPLOITATIVE_348(BaseStrategy):
    """
    Exploitative strategy that adapts to the cooperation rate of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.avg_coop_rate = 0
        self.total_payoff_coop = 0
        self.total_payoff_defect = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        avg_coop_rate = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]]) / self.game_description.n_players
        if history.my_payoffs[-1] == 1 and sum(history.opponent_actions[-1, :] + [history.my_actions[-1]]) < self.game_description.m:
            return Action.D
        if avg_coop_rate >= self.game_description.m / self.game_description.n_players:
            return Action.C if random.random() < 0.8 else Action.D
        w = min(3, state.round_number)
        if state.round_number > w:
            avg_total_payoff_coop = (self.total_payoff_coop + sum([history.my_payoffs[i] for i in range(-w, 0)])) / w
            avg_total_payoff_defect = (self.total_payoff_defect + sum([history.opponent_payoffs[-1, j] for j in range(self.game_description.n_players - 1) if history.opponent_actions[-1, j]])) / w
            if avg_total_payoff_coop - avg_total_payoff_defect > 10:
                self.coop_prob += 0.1
        self.avg_coop_rate = avg_coop_rate
        self.total_payoff_coop += sum([history.my_payoffs[i] for i in range(-w, 0)])
        self.total_payoff_defect += sum([history.opponent_payoffs[-1, j] for j in range(self.game_description.n_players - 1) if history.opponent_actions[-1, j]])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.coop_prob else Action.D

class Strategy_EXPLOITATIVE_349(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC) strategy for Collective Risk Dilemma.

    This strategy adapts to the game's history and opponents' behaviors, seeking to exploit their tendencies while minimizing losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.E_m = 0
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.E_m = self.game_description.n_players // 2
            return Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_cooperators > self.game_description.m:
            self.E_m -= 1
        elif previous_round_cooperators < self.game_description.m:
            self.E_m += 1
        if self.E_m <= self.game_description.m and self.consecutive_defections < 2:
            action = Action.C
        else:
            action = Action.D
        if action == Action.D and self.consecutive_defections >= 2:
            action = Action.C
            self.consecutive_defections = 0
        if action == Action.D:
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = 0
        if state.round_number == self.game_description.n_rounds - 1 and self.E_m <= self.game_description.m:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_350(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Hunter" (ATH)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.exploitation_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :] != False)
        m_half = math.ceil(self.game_description.m / 2)
        if prev_coop_count >= m_half:
            self.exploitation_mode = False
            action = Action.C
        elif self.exploitation_mode:
            action = Action.D
        else:
            action = Action.D
            self.exploitation_mode = True
        if self.exploitation_mode and state.round_number > 1:
            prev_prev_coop_count = sum(history.opponent_actions[-2, :] != False)
            if prev_prev_coop_count >= m_half:
                self.exploitation_mode = False
                action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperators = sum(history.my_actions) + sum(sum(history.opponent_actions))
            if total_cooperators >= self.game_description.m:
                return Action.C
        return action

class Strategy_EXPLOITATIVE_351(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors while minimizing losses and maximizing gains.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr_history = []

    def calculate_acr(self, history: PlayerHistory) -> float:
        """
        Calculate the average cooperation rate (ACR) over the last few rounds.
        """
        recent_rounds = min(5, len(history.my_actions))
        recent_coops = sum(history.opponent_actions[-recent_rounds:, :])
        return np.mean(recent_coops)

    def opponent_cooperates_when_acr_low(self, history: PlayerHistory) -> bool:
        """
        Check if an opponent consistently cooperates when ACR is below the threshold.
        """
        opponent_coop_rates = []
        for i in range(1, len(history.my_actions)):
            acr = self.calculate_acr(PlayerHistory(my_actions=history.my_actions[:i], my_payoffs=history.my_payoffs[:i], opponent_actions=history.opponent_actions[:i, :], opponent_payoffs=history.opponent_payoffs[:i, :]))
            if acr < self.game_description.m / self.game_description.n_players and history.opponent_actions[i - 1, 0]:
                opponent_coop_rates.append(1)
            else:
                opponent_coop_rates.append(0)
        return np.mean(opponent_coop_rates) > 0.5

    def opponent_defects_when_acr_high(self, history: PlayerHistory) -> bool:
        """
        Check if an opponent tends to defect when ACR is above the threshold.
        """
        opponent_defect_rates = []
        for i in range(1, len(history.my_actions)):
            acr = self.calculate_acr(PlayerHistory(my_actions=history.my_actions[:i], my_payoffs=history.my_payoffs[:i], opponent_actions=history.opponent_actions[:i, :], opponent_payoffs=history.opponent_payoffs[:i, :]))
            if acr >= self.game_description.m / self.game_description.n_players and (not history.opponent_actions[i - 1, 0]):
                opponent_defect_rates.append(1)
            else:
                opponent_defect_rates.append(0)
        return np.mean(opponent_defect_rates) > 0.5

    def tied_last_round(self, history: PlayerHistory) -> bool:
        """
        Check if the last round was a tie.
        """
        prev_cooperators = sum(history.opponent_actions[-1, :])
        prev_defectors = self.game_description.n_players - 1 - prev_cooperators
        return prev_cooperators == prev_defectors

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        elif 3 <= state.round_number <= self.game_description.n_rounds // 2:
            acr = self.calculate_acr(history)
            if acr >= self.game_description.m / self.game_description.n_players:
                return Action.C
            else:
                return Action.D
        elif self.opponent_cooperates_when_acr_low(history):
            return Action.D
        elif self.opponent_defects_when_acr_high(history):
            return Action.C
        if state.round_number == self.game_description.n_rounds:
            acr = self.calculate_acr(history)
            if acr >= self.game_description.m / self.game_description.n_players:
                return Action.C
        if self.tied_last_round(history):
            prev_cooperators = sum(history.opponent_actions[-1, :])
            prev_defectors = self.game_description.n_players - 1 - prev_cooperators
            if prev_cooperators > prev_defectors:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_352(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 1.0
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        prev_round_defectors = self.game_description.n_players - 1 - prev_round_cooperators
        delta_C = prev_round_cooperators - self.prev_cooperators
        delta_D = prev_round_defectors - (self.game_description.n_players - 1 - self.prev_cooperators)
        self.cooperation_probability = max(0, min(1, self.cooperation_probability + (delta_C - delta_D) / self.game_description.n_players))
        if prev_round_cooperators < self.game_description.m:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if random.random() < self.cooperation_probability:
            return Action.C
        else:
            return Action.D
        self.prev_cooperators = prev_round_cooperators

class Strategy_EXPLOITATIVE_353(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Threshold Tracker.

    This strategy aims to exploit the collective risk dilemma by adapting to 
    the opponents' behavior and adjusting its own actions to maximize payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.p_coop = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if history.my_payoffs[-1] > 1:
            self.p_coop = (self.coop_count + 1) / (self.game_description.n_players + 1)
        self.coop_count = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.coop_count >= self.game_description.m - 1 or random.random() < self.p_coop:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_354(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = math.ceil(game_description.m / 2)
        self.punish = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators_prev_round >= self.threshold:
            action = Action.C
        elif num_cooperators_prev_round < self.game_description.m and num_cooperators_prev_round > self.threshold:
            self.threshold += 1
            action = Action.D
        else:
            action = Action.D
        if history.my_actions[-1] == False and num_cooperators_prev_round >= self.game_description.m:
            self.punish = True
        elif self.punish:
            action = Action.D
            self.punish = False
        return action

class Strategy_EXPLOITATIVE_355(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.threshold_high = 0.7
        self.threshold_low = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        total_rounds_played = state.round_number - 1
        if total_rounds_played > 0:
            average_cooperation_rate = np.mean(opponent_cooperations[1:total_rounds_played + 1] / self.n_players)
            if average_cooperation_rate > self.threshold_high:
                return Action.C
            elif average_cooperation_rate < self.threshold_low:
                return Action.D
            else:
                p = (self.m - np.sum(history.opponent_actions[-1, :])) / (self.n_players - np.sum(history.opponent_actions[-1, :]))
                return Action.C if random.random() < p else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_356(BaseStrategy):
    """
    A robust and adaptive strategy designed to exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        round_number = state.round_number
        r = self.game_description.n_rounds
        if round_number <= 2:
            return Action.C
        elif 2 < round_number <= r // 3:
            return Action.C if round_number % 2 == 0 else Action.D
        elif round_number > r // 3 and history is not None:
            prev_coops = sum(history.opponent_actions[-1, :])
            if prev_coops < m:
                return Action.D
            else:
                p = (m - 1) / (n_players - 1) + k / (k + 1)
                return Action.C if random.random() < p else Action.D
        elif round_number >= r - 2:
            return Action.D

class Strategy_EXPLOITATIVE_357(BaseStrategy):
    """
    An adaptive risk-taker strategy that balances cooperation and defection based on game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_prev = 0
        self.success_count = 0
        self.threshold_adj = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        self.cooperators_prev = sum(history.opponent_actions[-1, :] != False)
        if self.cooperators_prev <= self.game_description.m - 1:
            action = Action.C
        else:
            action = Action.D
        if state.round_number % (self.game_description.n_rounds // 2) == 0 and state.round_number > 0:
            success_count_window = sum((1 for i in range(max(state.round_number - self.game_description.n_rounds // 2, 0), state.round_number) if history.opponent_actions[i - 1, :].sum() + (history.my_actions[i - 1] == True) >= self.game_description.m))
            if success_count_window < self.game_description.k / 2:
                self.threshold_adj -= 1
            elif success_count_window > self.game_description.k / 2:
                self.threshold_adj += 1
        if state.round_number >= self.game_description.n_rounds - (self.game_description.m - 1):
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_358(BaseStrategy):
    """
    An adaptive strategy that balances cooperation with exploitation in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploitation_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        my_action = history.my_actions[-1]
        opponent_payoffs = history.opponent_payoffs[-1, :]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        coop_ratio = (opponent_cooperators + int(my_action)) / self.game_description.n_players
        if self.exploitation_mode:
            if coop_ratio > 0.5:
                self.exploitation_mode = False
            else:
                return Action.D
        my_payoff = history.my_payoffs[-1]
        if opponent_cooperators + int(my_action) < self.game_description.m and my_payoff <= (1 + self.game_description.k) / 2:
            self.exploitation_mode = True
            return Action.D
        if coop_ratio >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_359(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for Collective Risk Dilemma.

    This strategy initially cooperates to gather information about opponents' strategies.
    It then escalates cooperation based on threshold conditions, while punishing defectors
    and minimizing losses against exploitative opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_defections = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        avg_payoff_prev_round = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        if prev_cooperators >= self.game_description.m / 2 and avg_payoff_prev_round >= self.game_description.k / 2:
            return Action.C
        if prev_cooperators < self.game_description.m:
            return Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if opponent_action == 0 and prev_cooperators > self.game_description.m / 2:
                self.opponent_defections[i] = True
        if any(self.opponent_defections.values()):
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_360(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = np.zeros(game_description.n_players - 1)
        self.prev_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        self.prev_round_cooperators = np.sum(prev_round_actions)
        for i in range(self.game_description.n_players - 1):
            opponent_actions = history.opponent_actions[:, i]
            opponent_cooperation_rate = np.mean(opponent_actions)
            self.opponent_cooperation_rates[i] = opponent_cooperation_rate
        if state.round_number > 1:
            for i in range(self.game_description.n_players - 1):
                prev_prev_round_cooperators = np.sum(history.opponent_actions[-2, :])
                prev_opponent_action = history.opponent_actions[-1, i]
                if prev_prev_round_cooperators >= self.game_description.m and prev_opponent_action == False:
                    return Action.D
        opponent_average_cooperation_rate = np.mean(self.opponent_cooperation_rates)
        if self.prev_round_cooperators < self.game_description.m:
            return Action.D
        elif opponent_average_cooperation_rate > 0.5 and self.prev_round_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and self.prev_round_cooperators >= self.game_description.m:
            return Action.C
        if self.prev_round_cooperators == self.game_description.m - 1:
            return Action.D

class Strategy_EXPLOITATIVE_361(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game that balances cooperation and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.avg_payoff_others = 0
        self.prev_round_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            return Action.C
        avg_coop = np.mean(history.opponent_actions[-self.game_description.n_rounds // 2:, :])
        own_payoff = history.my_payoffs[-1]
        median_prev_payoffs = np.median(self.prev_round_payoffs)
        if avg_coop >= self.game_description.m:
            action = Action.C
        elif avg_coop < self.game_description.m and own_payoff > median_prev_payoffs:
            action = Action.D
        else:
            action = Action.C
        avg_payoff_others = np.mean(history.opponent_payoffs[-self.game_description.n_rounds // 2:, :])
        if avg_payoff_others > 1 + self.game_description.k / 2:
            action = Action.D
        if state.round_number == 0:
            action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        self.prev_round_payoffs.append(own_payoff)
        return action

class Strategy_EXPLOITATIVE_362(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    Adaptively tracks cooperation threshold and responds accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_count = 0
        self.d_count = 0
        self.threshold = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.m_count = sum(history.opponent_actions[-1, :] != False)
        self.d_count = self.game_description.n_players - 1 - self.m_count
        if self.m_count < self.game_description.m:
            self.threshold = float('inf')
        else:
            self.threshold = self.m_count / (self.game_description.m - 1)
        if self.threshold <= self.game_description.m / self.game_description.n_players:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and history.my_actions.all():
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_363(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma game.
    This strategy aims to exploit the game's structure by adapting to the opponents' behavior 
    while ensuring a balance between cooperation and defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators_prev_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players <= 2:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        self.num_cooperators_prev_round = cooperators + (history.my_actions[-1] if history else 0)
        CT = self.game_description.m - self.num_cooperators_prev_round
        cooperate_prob = 1.0 if CT <= 0 else 0.0
        if cooperators >= self.game_description.m and self.game_description.k > 1.5:
            cooperate_prob = 0.3
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_364(BaseStrategy):
    """
    Conditional Cooperator with Adaptive Threshold (CCAT) strategy.
    
    This exploitative strategy takes advantage of the opponent's behavior 
    while still considering the collective risk dilemma. It adapts to changing 
    opponent behaviors by adjusting its cooperation level based on observed payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.defect_count = 0
        self.avg_coop_payoff = 0
        self.avg_defect_payoff = 0
        self.coop_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = self.coop_count
        prev_defect_count = self.defect_count
        self.avg_coop_payoff, self.avg_defect_payoff = self.update_averages(history.my_payoffs, history.opponent_actions, history.opponent_payoffs)
        if prev_coop_count < self.game_description.m:
            return Action.D
        elif self.avg_coop_payoff > self.avg_defect_payoff + self.game_description.k:
            return Action.C
        else:
            return Action.D

    def update_averages(self, my_payoffs: NDArray[np.float64], opponent_actions: NDArray[np.bool_], opponent_payoffs: NDArray[np.float64]) -> tuple[float, float]:
        """Calculate the average payoffs of cooperators and defectors."""
        coop_total = 0
        defect_total = 0
        coop_count = 0
        defect_count = 0
        for t in range(max(1, self.game_description.n_rounds // 2), len(my_payoffs)):
            if opponent_actions[t - 1, :].any():
                coop_total += my_payoffs[t]
                coop_count += 1
            else:
                defect_total += my_payoffs[t]
                defect_count += 1
        avg_coop_payoff = coop_total / max(coop_count, 1)
        avg_defect_payoff = defect_total / max(defect_count, 1)
        self.coop_count = coop_count
        self.defect_count = defect_count
        return (avg_coop_payoff, avg_defect_payoff)

    def adapt_threshold(self, avg_coop_payoff: float, avg_defect_payoff: float, coop_threshold: int) -> int:
        """Adapt the cooperation threshold based on average payoffs."""
        if avg_coop_payoff > avg_defect_payoff:
            return min(coop_threshold + 1, self.game_description.n_players)
        else:
            return max(coop_threshold - 1, 0)

    def update_counts(self, action: Action, coop_count: int, defect_count: int) -> tuple[int, int]:
        """Update the count of cooperators and defectors."""
        if action == Action.C:
            return (coop_count + 1, defect_count)
        else:
            return (coop_count, defect_count + 1)

    def last_round(self, state: GameState) -> bool:
        """Check if it's the last round."""
        return state.round_number == self.game_description.n_rounds - 1

    def __del__(self):
        pass

class Strategy_EXPLOITATIVE_365(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters by balancing cooperation and exploitation.
    It aims to exploit opponents' cooperation while minimizing the risk of being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_round_coops < self.game_description.m:
            self.T += 1
        elif prev_round_coops >= self.game_description.m and self.T > self.game_description.m:
            self.T -= 1
        coop_payoff = 1 - int(history.my_actions[-1]) + self.game_description.k * (prev_round_coops / self.game_description.n_players)
        defect_payoff = 1 + self.game_description.k * (prev_round_coops / self.game_description.n_players)
        if prev_round_coops >= self.T and coop_payoff > defect_payoff:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_366(BaseStrategy):
    """
    A strategy that balances cooperation and defection by leveraging information from previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.moving_average_window_size = 3
        self.high_cooperation_threshold = 0.7
        self.low_cooperation_threshold = 0.3
        self.cooperation_epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_actions = history.opponent_actions[:-1, :]
        cooperators_in_last_round = sum(history.opponent_actions[-1, :])
        if state.round_number >= self.moving_average_window_size:
            cooperation_rates = np.mean(opponent_actions[-self.moving_average_window_size:, :], axis=0)
            moving_average_cooperation_rate = np.mean(cooperation_rates)
        else:
            cooperation_rates = np.mean(opponent_actions, axis=0)
            moving_average_cooperation_rate = np.mean(cooperation_rates)
        cooperation_threshold = self.game_description.m / self.game_description.n_players
        if cooperators_in_last_round >= math.ceil(cooperation_threshold * self.game_description.n_players) + math.floor(self.cooperation_epsilon * self.game_description.n_players):
            return Action.C
        elif moving_average_cooperation_rate > self.high_cooperation_threshold:
            return Action.C
        elif cooperators_in_last_round < math.floor((cooperation_threshold - self.cooperation_epsilon) * self.game_description.n_players):
            return Action.D
        elif moving_average_cooperation_rate < self.low_cooperation_threshold:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            my_payoff_if_c = history.my_payoffs[-1] + self.game_description.k * (cooperators_in_last_round >= self.game_description.m)
            return Action.C if my_payoff_if_c > history.my_payoffs[-1] else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_367(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma game that balances cooperation and defection adaptively based on the game's history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.m / self.n_players else Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        avg_coop = np.mean(opponent_cooperations >= self.m)
        success_rate = np.mean(np.sum(history.my_actions) + opponent_cooperations >= self.m)
        coop_prob = max(self.m / self.n_players, avg_coop + success_rate * (self.k - 1) / self.k)
        defect_prob = min(1 - self.m / self.n_players, 1 - avg_coop + (1 - success_rate) * (self.k - 1) / self.k)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D if avg_coop < self.m else Action.C
        consecutive_defections = np.sum(history.my_actions[-int(self.game_description.n_rounds / 2):] == False)
        if consecutive_defections > self.game_description.n_rounds / 4:
            return Action.C
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_EXPLOITATIVE_368(BaseStrategy):
    """
    A robust and adaptive exploitative strategy that adjusts its cooperation level based on the game's history and parameters.
    It aims to maximize individual payoffs while minimizing the risk of contributing to a failed collective effort.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_defectors = 0

    def count_C(self, opponent_actions: NDArray[np.bool_]) -> int:
        """
        Count the number of cooperators in a given round.

        Args:
            opponent_actions (NDArray[np.bool_]): Opponents' actions in the last round.

        Returns:
            int: The number of cooperators.
        """
        return sum(opponent_actions)

    def detect_consecutive_D(self, my_history: NDArray[np.bool_], opponent_actions: NDArray[np.bool_]) -> int:
        """
        Detect consecutive defections by opponents.

        Args:
            my_history (NDArray[np.bool_]): This player's actions.
            opponent_actions (NDArray[np.bool_]): Opponents' actions in the last round.

        Returns:
            int: The number of rounds with consecutive defection.
        """
        if not np.any(opponent_actions):
            self.consecutive_defectors += 1
        else:
            self.consecutive_defectors = 0
        return self.consecutive_defectors

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        cooperators = self.count_C(opponent_actions)
        if cooperators < self.game_description.m:
            return Action.D
        elif self.game_description.m <= cooperators < self.game_description.n_players:
            return Action.D
        else:
            consecutive_defectors = self.detect_consecutive_D(history.my_actions, opponent_actions)
            if consecutive_defectors > 2:
                if cooperators < self.game_description.m:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_EXPLOITATIVE_369(BaseStrategy):
    """
    An exploitative strategy that adapts to the behavior of other players.
    Initially cooperates with a probability based on the minimum number of cooperators needed,
    then observes and adjusts its cooperation rate accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.prev_round_actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            coop_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < coop_prob else Action.D
        elif state.round_number <= 5:
            if history is not None:
                self.prev_round_actions.append(history.my_actions[-1])
                self.avg_coop_rate = sum(self.prev_round_actions) / len(self.prev_round_actions)
            return Action.C if random.random() < self.avg_coop_rate else Action.D
        else:
            num_prev_coops = sum((1 for a in history.opponent_actions[-1, :] if a == 1))
            action = Action.C if num_prev_coops < self.game_description.m else Action.D
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return action

class Strategy_EXPLOITATIVE_370(BaseStrategy):
    """
    A strategy that exploits the collective risk dilemma by adapting to the behavior of other players while prioritizing its own payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = opponent_cooperators_prev_round + history.my_actions[-1]
        if num_cooperators_prev_round < self.game_description.m / 2:
            return Action.D
        elif num_cooperators_prev_round >= self.game_description.m / 2 and num_cooperators_prev_round < self.game_description.m:
            p = (self.game_description.m - num_cooperators_prev_round) / (self.game_description.m - self.game_description.m / 2)
            return Action.C if random.random() < p else Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            q = 1 - (self.game_description.k - 1) / (self.game_description.n_players - 1)
            return Action.D if random.random() < q else Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_371(BaseStrategy):
    """
    Robust and exploitative strategy for the Collective Risk Dilemma game.
    Adapts to other players' behaviors by adjusting its expected reward calculation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.er_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        er = self.game_description.k * (prev_cooperators / self.game_description.n_players)
        if history.my_payoffs.size > 0:
            actual_payoff = history.my_payoffs[-1]
            prev_payoff = history.my_payoffs[-2] if history.my_payoffs.size > 1 else 0
            if actual_payoff > prev_payoff:
                self.er_threshold += 0.1
            elif actual_payoff < prev_payoff:
                self.er_threshold -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if er >= self.er_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_372(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) Strategy.
    
    This strategy aims to exploit opponents' behaviors while adapting to the game's dynamics. 
    It balances cooperation and defection to maximize payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.CI = 0
        self.pi_avg_C = 0
        self.pi_avg_D = 0
        self.threshold = game_description.m
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if abs(self.game_description.n_players / 2 - self.game_description.m) < 1 else Action.D
        pi_prev = np.mean(history.my_payoffs[-1:])
        m_prev = sum(history.opponent_actions[-1, :])
        self.m_prev = m_prev
        CI = self.game_description.k - 1 if m_prev >= self.threshold else -1
        self.CI = CI
        opponents_cooperated = np.sum(history.opponent_actions[-1, :]) > len(history.opponent_actions[-1, :]) / 2
        if CI > 0 and opponents_cooperated:
            return Action.C
        elif CI < 0:
            self.consecutive_defections += 1
            if self.consecutive_defections >= 3:
                return Action.D
            return Action.D
        else:
            most_common_action = np.argmax(np.bincount(history.opponent_actions[-1, :]))
            return Action.C if most_common_action == 1 else Action.D
        if history.my_actions[-1] == 1:
            self.pi_avg_C += pi_prev
        else:
            self.pi_avg_D += pi_prev
        if state.round_number % (self.game_description.n_rounds // 4) == 0:
            if self.pi_avg_C > self.pi_avg_D and self.threshold < self.game_description.n_players - 1:
                self.threshold += 1
            elif self.pi_avg_C <= self.pi_avg_D and self.threshold > 1:
                self.threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D if opponents_cooperated == False or abs(self.game_description.m - self.game_description.n_players) < 2 else Action.C
        return None

class Strategy_EXPLOITATIVE_373(BaseStrategy):
    """
    Exploitative strategy: "Conditional Cooperator".

    Initially cooperates, then reciprocates cooperation if at least m-1 opponents cooperated in the previous round.
    Punishes defection by defecting in the next round. Always defects in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :])
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if prev_round_coops >= self.m - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_374(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game that balances cooperation and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_payoff = 0.0
        self.avg_cooperation_rate = 0.5
        self.exploitation_phase = False

    def calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        """Calculate the average cooperation rate among opponents in previous rounds."""
        opponent_actions = history.opponent_actions[:-1]
        return np.mean(opponent_actions)

    def evaluate_total_payoff_and_opponent_behavior(self, total_payoff: float, history: PlayerHistory, t: int) -> bool:
        """Evaluate the total payoff and opponent behavior up to this point."""
        if self.avg_cooperation_rate < 0.5 or total_payoff < self.game_description.k * t / 2:
            return True
        return False

    def calculate_average_payoffs(self, history: PlayerHistory) -> tuple[float, float]:
        """Calculate the average payoffs for cooperators and defectors in recent rounds."""
        opponent_actions = history.opponent_actions[:-1]
        opponent_payoffs = history.opponent_payoffs[:-1]
        pi_c = np.mean(opponent_payoffs[opponent_actions == True])
        pi_d = np.mean(opponent_payoffs[opponent_actions == False])
        return (pi_c, pi_d)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < self.game_description.n_rounds // 2:
            self.avg_cooperation_rate = self.calculate_average_cooperation_rate(history)
            if self.avg_cooperation_rate > 0.5:
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds // 2:
            self.exploitation_phase = self.evaluate_total_payoff_and_opponent_behavior(self.total_payoff, history, state.round_number)
        if self.exploitation_phase:
            pi_c, pi_d = self.calculate_average_payoffs(history)
            if pi_d > pi_c:
                return Action.D
            else:
                return Action.C
        elif state.round_number > self.game_description.n_rounds // 2:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            if self.total_payoff > self.game_description.k * state.round_number / 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_375(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy is designed to adapt to various opponent behaviors 
    while maximizing its own payoff. It relies solely on game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_avg = 0.0
        self.r_exploitation = math.floor(game_description.n_rounds / 2)
        self.r_adjustment = math.floor(game_description.n_rounds / 4)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number < self.r_exploitation:
            if state.round_number % 2 == 0:
                return Action.C
            else:
                return Action.D
        if history is not None:
            opponent_cooperations = np.sum(history.opponent_actions, axis=1)
            self.m_avg = np.mean(opponent_cooperations)
        if state.round_number % self.r_adjustment == 0:
            self.m_avg = max(self.game_description.m, math.floor(self.m_avg))
        if self.m_avg >= self.game_description.m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_376(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game. 
    This strategy adapts to the behavior of other players while maintaining an exploitative mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_cooperated = 0
        self.cooperation_frequency = 0.0
        self.defection_frequency = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= self.game_description.m:
            return Action.C
        elif state.round_number > self.game_description.m and state.round_number <= self.game_description.n_rounds // 2:
            num_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators >= self.game_description.m:
                return Action.D
            else:
                return Action.C
        else:
            recent_rounds = history.opponent_actions[-self.game_description.n_rounds // 4:, :]
            cooperations = np.sum(recent_rounds, axis=(0, 1))
            total_actions = recent_rounds.shape[0] * recent_rounds.shape[1]
            self.cooperation_frequency = cooperations / total_actions
            self.defection_frequency = 1 - self.cooperation_frequency
            if self.cooperation_frequency > 0.5 * self.game_description.m / self.game_description.n_players:
                return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds:
            return Action.D

class Strategy_EXPLOITATIVE_377(BaseStrategy):
    """
    An exploitative strategy designed to maximize its payoff by exploiting 
    the cooperative behavior of other players while minimizing its own contribution.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0
        self.last_round_defected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if state.round_number == 0:
            cooperate_prob = m / n_players
            action = Action.C if random.random() < cooperate_prob else Action.D
        else:
            avg_coop_rate = np.mean(history.my_actions[:state.round_number])
            opponent_avg_coop_rate = np.mean(np.any(history.opponent_actions, axis=1))
            total_avg_coop_rate = (avg_coop_rate + opponent_avg_coop_rate) / 2
            if self.last_round_defected and sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :]) < m:
                cooperate_prob = 0.5
            elif total_avg_coop_rate >= (m - 1) / n_players:
                defect_prob = 1 - (total_avg_coop_rate - (m - 1) / n_players) / (m / n_players - (m - 1) / n_players)
                cooperate_prob = 1 - defect_prob
            else:
                cooperate_prob = 0.5
            if state.round_number >= r - 2 and sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :]) < m:
                cooperate_prob = 0
            action = Action.C if random.random() < cooperate_prob else Action.D
        self.last_round_defected = action == Action.D
        return action

class Strategy_EXPLOITATIVE_378(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rounds_to_exploit = min(5, game_description.n_rounds - 1)
        self.mid_round_threshold = math.ceil(game_description.n_rounds / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        m = self.game_description.m
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if state.round_number <= self.rounds_to_exploit:
            return Action.C if num_cooperators >= m else Action.D
        elif state.round_number < self.mid_round_threshold:
            opponent_payoffs = history.opponent_payoffs
            my_payoffs = history.my_payoffs
            avg_payoff_C = (my_payoffs[-1] + np.sum(opponent_payoffs[-1, :][history.opponent_actions[-1, :] == 1])) / num_cooperators if num_cooperators > 0 else 0
            avg_payoff_D = (np.sum(my_payoffs[:-1]) + np.sum(np.sum(opponent_payoffs[:-1, :], axis=1))) / (n_players * state.round_number - num_cooperators) if n_players * state.round_number - num_cooperators > 0 else 0
            return Action.C if avg_payoff_C > avg_payoff_D else Action.D
        elif state.round_number < self.game_description.n_rounds:
            rounds_remaining = self.game_description.n_rounds - state.round_number
            return Action.C if rounds_remaining <= n_players else Action.D
        else:
            total_payoff_C = np.sum(history.my_payoffs[history.my_actions == Action.C]) + np.sum(np.sum(opponent_payoffs[:, :][history.opponent_actions == 1], axis=0))
            total_payoff_D = np.sum(history.my_payoffs[history.my_actions == Action.D]) + np.sum(np.sum(opponent_payoffs[:, :][history.opponent_actions == 0], axis=0))
            return Action.C if total_payoff_C > total_payoff_D else Action.D

class Strategy_EXPLOITATIVE_379(BaseStrategy):
    """
    Adaptive Collective Risk Exploiter strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.5
        self.threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        observed_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        self.cooperation_rate += observed_cooperators / self.game_description.n_players - self.cooperation_rate
        if not self.threshold_met and state.round_number < self.game_description.n_rounds:
            return Action.C
        exp_payoff_C = 0 + self.game_description.k * self.cooperation_rate
        exp_payoff_D = 1 + self.game_description.k * (1 - self.cooperation_rate)
        if exp_payoff_D > exp_payoff_C:
            return Action.D
        return Action.C

    def update_threshold_met(self, history: PlayerHistory):
        observed_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        self.threshold_met = observed_cooperators >= self.game_description.m

class Strategy_EXPLOITATIVE_380(BaseStrategy):
    """
    Exploitative strategy "Adaptive Threshold Exploiter" (ATE) for the Collective Risk Dilemma game.
    
    ATE aims to maximize its payoff by adapting to the cooperation levels of other players while exploiting their willingness to cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.exploit_phase = False
        self.adjustment_trigger = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if history is not None and len(history.my_actions) > 0:
            num_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            self.m_prev = (self.m_prev * (state.round_number - 1) + num_cooperators) / state.round_number
        if self.adjustment_trigger and self.game_description.m > 1:
            self.game_description.m -= 1
            self.adjustment_trigger = False
        if not self.exploit_phase and self.m_prev >= self.game_description.m:
            self.exploit_phase = True
        if self.exploit_phase:
            return Action.C if state.round_number % 2 == 0 else Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            if self.m_prev >= self.game_description.m:
                return Action.C
            else:
                self.exploit_phase = True
                return Action.D
        elif self.m_prev >= self.game_description.m:
            return Action.C
        elif random.random() < 0.2 * self.game_description.n_players * self.game_description.k / (state.round_number + 1):
            self.adjustment_trigger = True
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_381(BaseStrategy):
    """
    A strategy that adaptively adjusts its cooperation level based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_rounds = math.ceil(0.1 * game_description.n_rounds)
        self.prev_coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.coop_rounds:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        self.prev_coop_count = sum(prev_opponent_actions)
        prev_payoff = history.my_payoffs[-1]
        avg_payoff = np.mean(history.opponent_payoffs[-1])
        if self.prev_coop_count >= self.game_description.m and prev_payoff >= avg_payoff:
            return Action.C
        elif history.my_actions[-1] == False and self.prev_coop_count > sum(history.opponent_actions[-2, :]):
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_382(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy implementation.
    
    This strategy exploits the game's structure and opponents' behaviors to maximize its payoff.
    It balances cooperation and defection based on the history of play, using a combination of trigger strategies and conditional cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate = True
        self.under_threshold_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_cooperators = sum(prev_round_actions)
        if prev_round_cooperators >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        if prev_round_cooperators < self.game_description.m:
            self.under_threshold_count += 1
            if self.under_threshold_count > 2:
                action = Action.D
                self.under_threshold_count = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_383(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator"
    
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == True)
        if previous_cooperators >= self.game_description.m:
            return Action.C
        total_payoff_for_cooperation = 0 if previous_cooperators < self.game_description.m else self.game_description.k
        expected_defection_payoff = 1 + (self.game_description.k if previous_cooperators + 1 >= self.game_description.m else 0)
        if previous_cooperators == self.game_description.m - 1 and total_payoff_for_cooperation > expected_defection_payoff:
            return Action.C
        exploitation_mode = False
        for i in range(max(0, state.round_number - 2), state.round_number):
            opponent_defected_and_threshold_met = np.any((history.opponent_actions[i] == False) & (np.sum(history.opponent_actions[i] == True) + (history.my_actions[i] == True) >= self.game_description.m))
            if opponent_defected_and_threshold_met:
                exploitation_mode = True
                break
        if exploitation_mode:
            return Action.D
        repeated_opponent_defection = False
        for i in range(max(0, state.round_number - 2), state.round_number):
            opponent_repeatedly_defected = np.any(np.sum(history.opponent_actions[i:i + 3] == False, axis=0) >= 2)
            other_player_cooperated_during_this_time = np.any(np.sum(history.opponent_actions[i:i + 3] == True, axis=0) > 0)
            if opponent_repeatedly_defected and other_player_cooperated_during_this_time:
                repeated_opponent_defection = True
                break
        if repeated_opponent_defection:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_384(BaseStrategy):
    """
    Exploitative strategy that adapts to the evolving game dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.M += sum([1 if other_player_action[-1] else 0 for other_player_action in history.opponent_actions])
        if self.M < self.game_description.m - 1:
            return Action.D
        elif self.M >= self.game_description.m + 2:
            return Action.D
        else:
            p = (self.M - self.game_description.m + 1) / (self.game_description.m + 1)
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            elif self.M == self.game_description.m - 1 or self.M == self.game_description.m + 2:
                return Action.C if random.random() < 0.5 else Action.D
            else:
                return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_385(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game.
    
    The primary goal of this strategy is to maximize its payoff by adapting to 
    opponents' behavior while minimizing cooperation when possible.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponents_cooperation_rates = [0] * game_description.n_players
        self.average_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        for i in range(self.game_description.n_players):
            if i != self.game_description.n_players - 1:
                opponent_action = history.opponent_actions[-1, i]
                self.opponents_cooperation_rates[i] += 1 / state.round_number * (opponent_action - self.opponents_cooperation_rates[i])
        ct = self.game_description.m - sum(self.opponents_cooperation_rates)
        if ct <= 0:
            action = Action.C
        else:
            action = Action.D
        group_average_payoff = np.mean(history.my_payoffs + history.opponent_payoffs.sum(axis=1))
        if self.average_payoff < group_average_payoff - 0.2 * self.game_description.k:
            action = Action.D
        if state.round_number == self.game_description.n_rounds and self.average_payoff >= self.game_description.m / self.game_description.n_players * self.game_description.k + (1 - self.game_description.m / self.game_description.n_players):
            action = Action.C
        return action

class Strategy_EXPLOITATIVE_386(BaseStrategy):
    """
    A strategy that balances cooperation and exploitation by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.t = 0
        self.c_rounds = 2
        self.t_max = 3
        self.cooperation_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        avg_payoff = np.mean(history.my_payoffs)
        own_payoff = history.my_payoffs[-1]
        if prev_round_coops < self.game_description.m:
            return Action.D
        elif prev_round_coops == self.game_description.m:
            if own_payoff > avg_payoff:
                return Action.C
            else:
                return Action.D
        elif prev_round_coops > self.game_description.m:
            if own_payoff <= avg_payoff:
                return Action.C
            else:
                return Action.D
        if prev_round_coops >= self.game_description.m:
            self.t += 1
            if self.t == self.t_max:
                self.cooperation_mode = True
                self.t = 0
        else:
            self.t = 0
        if self.cooperation_mode and sum(history.opponent_actions[-self.c_rounds:, :].sum(axis=1)) == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.cooperation_mode and self.c_rounds > 0:
            self.c_rounds -= 1
        return Action.C

class Strategy_EXPLOITATIVE_387(BaseStrategy):
    """
    An exploitative strategy that adaptively responds to opponents' cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_avg = 0
        self.last_round_coops = []
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_count += 1
        if self.round_count <= 3:
            if self.round_count == 1:
                return Action.C
            else:
                return Action.D
        if history is not None:
            opponent_coops = np.sum(history.opponent_actions, axis=1)
            self.last_round_coops.append(opponent_coops[-1])
            self.m_avg = np.mean(self.last_round_coops[-3:])
            m_cur = opponent_coops[-1]
            if m_cur >= self.game_description.m:
                return Action.C
            elif m_cur < self.game_description.m and self.low_cooperation():
                return Action.D
            if self.m_avg >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        return Action.D

    def low_cooperation(self):
        coop_threshold = 0.5 * self.game_description.m
        return np.sum(self.last_round_coops[-3:]) < coop_threshold

class Strategy_EXPLOITATIVE_388(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for the Collective Risk Dilemma game.
    
    This strategy initially cooperates if the minimum number of cooperators needed is less than or equal to half of the total players.
    It then adapts its cooperation threshold based on the previous round's outcomes, exploiting non-cooperative behavior when possible.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploitation_mode = False
        self.cooperation_threshold = max(1, game_description.m - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if previous_cooperators < self.game_description.m:
            self.exploitation_mode = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.exploitation_mode:
            return Action.D
        if previous_cooperators >= self.cooperation_threshold:
            action = Action.C
        else:
            action = Action.D
            if self.game_description.k > 2:
                self.cooperation_threshold += 1
            else:
                self.cooperation_threshold -= 1
        return action

class Strategy_EXPLOITATIVE_389(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        n_players = self.game_description.n_players
        m_threshold = self.game_description.m
        k_factor = self.game_description.k
        r_rounds = self.game_description.n_rounds
        prev_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        avg_payoff_window = min(state.round_number // 4, state.round_number)
        if state.round_number < r_rounds // 2:
            if prev_coops < m_threshold:
                coop_prob = m_threshold / n_players
                return Action.C if random.random() < coop_prob else Action.D
            else:
                return Action.D
        elif state.round_number >= r_rounds // 2:
            avg_payoff = np.mean(history.my_payoffs[-avg_payoff_window:])
            self.avg_payoff = avg_payoff
            if avg_payoff > 1 + k_factor / 2:
                coop_prob = 0.5
                return Action.C if random.random() < coop_prob else Action.D
            else:
                return Action.D
        elif state.round_number > r_rounds - 2:
            if prev_coops < m_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_390(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        self.update_history(history)
        if self.game_description.m - 2 <= sum(self.history[-1]) < self.game_description.m:
            return Action.D
        if self.avg_coop >= self.game_description.m - 1:
            return Action.C
        else:
            return Action.D

    def update_history(self, history: PlayerHistory):
        opponent_actions = [action for action in history.opponent_actions[-1]]
        total_cooperators = sum((1 for action in opponent_actions if action))
        self.history.append(opponent_actions)
        self.avg_coop = (self.avg_coop * len(self.history) + total_cooperators) / (len(self.history) + 1)

    def reset_history(self):
        self.avg_coop = 0
        self.history = []

class Strategy_EXPLOITATIVE_391(BaseStrategy):
    """
    An adaptive risk-taker strategy that balances exploration and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m // 2
        self.coop_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_coop_count = sum(history.opponent_actions[-1, :])
        if prev_coop_count >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        if self.game_description.k > 1.5:
            self.coop_prob += 0.2
        if state.round_number >= self.game_description.n_rounds - 2:
            total_payoff = sum(history.my_payoffs)
            avg_payoff = np.mean(history.opponent_payoffs, axis=0).sum()
            if total_payoff < avg_payoff:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_392(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to exploit other players' cooperative tendencies while balancing its own payoff maximization goals.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        k_reward_factor = self.game_description.k
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            cooperate_prob = m_cooperators_needed / n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        prev_opponent_actions = history.opponent_actions[-1, :]
        prev_cooperation_level = np.sum(prev_opponent_actions) + (history.my_actions[-1] == True)
        if prev_cooperation_level >= m_cooperators_needed:
            cooperate_prob = 1 - (k_reward_factor - 1) / (n_players - m_cooperators_needed + 1)
        else:
            cooperate_prob = 0 if state.round_number < total_rounds - 1 else 1
        if state.round_number == total_rounds - 1:
            return Action.C
        if prev_cooperation_level == m_cooperators_needed - 1:
            cooperate_prob = 0.5
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_393(BaseStrategy):
    """
    A decision-making strategy that dynamically adjusts its cooperation threshold based on the game's history and parameters.
    This approach aims to balance individual payoffs with collective risk management, making it robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.success_rate = 0.5
        self.avg_cooperators = 0
        self.cooperation_threshold = (game_description.n_players - 1) // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players % 2 == 0 or self.game_description.m < self.game_description.n_players / 2 else Action.D
        for t in range(1, len(history.opponent_actions)):
            success = sum(history.opponent_actions[t - 1, :]) >= self.game_description.m
            self.success_rate += success
            self.avg_cooperators += sum(history.opponent_actions[t - 1, :])
        self.success_rate /= state.round_number
        self.avg_cooperators /= state.round_number
        if self.success_rate > 0.7:
            self.cooperation_threshold = min(self.cooperation_threshold + 1, self.game_description.n_players - 1)
        elif self.success_rate < 0.3:
            self.cooperation_threshold = max(self.cooperation_threshold - 1, 1)
        return Action.C if sum(history.opponent_actions[-1, :]) >= (self.game_description.m + self.cooperation_threshold) / 2 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_395(BaseStrategy):
    """
    An exploitative strategy for Collective Risk Dilemma that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_count = 0
        self.d_opportunity = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 5:
            if state.round_number % 2 == 0:
                return Action.C
            else:
                return Action.D
        for outcome, action in zip(history.my_payoffs[:-1], history.my_actions[:-1]):
            if outcome >= self.game_description.k * (self.game_description.m - 1) + 1:
                self.m_count += 1
            if outcome < self.game_description.k * (self.game_description.m - 1) + 1 and (not action):
                self.d_opportunity = True
        if self.m_count / state.round_number > 0.5:
            if history.my_payoffs[-1] >= self.game_description.k * (self.game_description.m - 1) + 1 or not self.d_opportunity:
                return Action.C
            else:
                return Action.D
        elif self.m_count / state.round_number < 0.5:
            if history.my_payoffs[-1] >= self.game_description.k * (self.game_description.m - 1) + 1 and self.d_opportunity:
                return Action.C
            else:
                return Action.D
        elif not history.my_actions[-1] and (not self.d_opportunity):
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_396(BaseStrategy):
    """
    Exploitative strategy that aims to maximize payoffs by adaptively responding 
    to the observed behavior of opponents while prioritizing cooperation when beneficial 
    and defecting when safe.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_payoff = 0.0
        self.avg_coop = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        my_prev_action = history.my_actions[-1]
        prev_payoff = history.my_payoffs[-1]
        if state.round_number > 1:
            self.avg_payoff = (self.avg_payoff * (state.round_number - 1) + prev_payoff) / state.round_number
            self.avg_coop = (self.avg_coop * (state.round_number - 1) + sum(prev_round_actions)) / state.round_number
        majority_coop = sum(prev_round_actions) >= self.game_description.m / 2
        recent_defectors = sum([not a for a in prev_round_actions])
        if majority_coop or (self.avg_payoff > 1 + self.game_description.k / 2 and self.avg_coop >= self.game_description.m):
            action = Action.C
        elif recent_defectors > (self.game_description.n_players - self.game_description.m) / 2:
            action = Action.D
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_397(BaseStrategy):
    """
    Exploits cooperation in Collective Risk Dilemma by adapting to opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperated_last_round = False
        self.opponent_cooperation_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + self.cooperated_last_round
        m_ratio = self.game_description.m / self.game_description.n_players
        self.opponent_cooperation_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if m_prev >= self.game_description.m or (m_prev < self.game_description.m and self.opponent_cooperation_rate > m_ratio):
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        self.cooperated_last_round = action == Action.C
        return action

class Strategy_EXPLOITATIVE_398(BaseStrategy):
    """
    Exploit opponents by adapting to their behavior and manipulating the collective risk dilemma's threshold mechanism.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators = 0
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.num_cooperators = sum(history.opponent_actions[-1, :] + [True])
            opponent_payoffs = np.sum(history.opponent_payoffs[-1, :])
            if history.my_payoffs.size > 0:
                opponent_payoffs += history.my_payoffs[-1]
            self.avg_payoff = opponent_payoffs / (self.game_description.n_players + 1)
        if state.round_number < self.game_description.n_rounds // 2:
            if self.num_cooperators < self.game_description.m:
                return Action.D
            else:
                p = (self.game_description.k - 1) / self.game_description.k
                return Action.C if random.random() < p else Action.D
        elif state.round_number < self.game_description.n_rounds:
            if self.avg_payoff >= 1 + self.game_description.k:
                p = (self.game_description.m - 1) / self.game_description.n_players
                return Action.C if random.random() < p else Action.D
            else:
                return Action.D
        elif state.round_number >= self.game_description.n_rounds:
            if self.num_cooperators < self.game_description.m:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_399(BaseStrategy):
    """
    This strategy aims to exploit opponents by adapting to their cooperation levels 
    while maintaining a strong incentive for cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_prev_round = 0
        self.avg_payoff_coop_prev_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_payoff = history.my_payoffs[-1]
        self.cooperators_prev_round = sum(opponent_actions) + (history.my_actions[-1] == True)
        if self.cooperators_prev_round >= self.game_description.m:
            all_cooperators_payoffs = [p for p, a in zip(history.opponent_payoffs[-1], opponent_actions) if a]
            if history.my_actions[-1] == True:
                all_cooperators_payoffs.append(my_payoff)
            avg_payoff_coop_prev_round = np.mean(all_cooperators_payoffs) if len(all_cooperators_payoffs) > 0 else None
        else:
            avg_payoff_coop_prev_round = None
        self.avg_payoff_coop_prev_round = avg_payoff_coop_prev_round
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.cooperators_prev_round < self.game_description.m:
            return Action.D
        elif self.avg_payoff_coop_prev_round is not None and self.avg_payoff_coop_prev_round < self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_400(BaseStrategy):
    """
    An exploitative strategy designed to adapt to changing opponent behaviors.
    
    The ACD strategy balances cooperation and defection based on opponents' actions, 
    punishing non-cooperation and rewarding cooperation while ensuring our own payoffs are maximized.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.consecutive_below_threshold_count = 0
        self.consecutive_above_threshold_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        self.cooperation_rate_history.append(sum(opponent_actions) / self.game_description.n_players)
        if len(self.cooperation_rate_history) > 5:
            recent_rates = np.array(self.cooperation_rate_history[-5:])
            avg_recent_rate = np.mean(recent_rates)
            if avg_recent_rate < self.game_description.m / self.game_description.n_players:
                self.consecutive_below_threshold_count += 1
            else:
                self.consecutive_above_threshold_count += 1
        recent_cooperation_rate = sum(self.cooperation_rate_history) / len(self.cooperation_rate_history)
        p = max(0.5, self.game_description.k * recent_cooperation_rate / (1 + self.game_description.k))
        if recent_cooperation_rate < self.game_description.m / self.game_description.n_players or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif random.random() < p:
            return Action.C
        if len(self.cooperation_rate_history) % 5 == 0 and recent_cooperation_rate < self.game_description.m / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_401(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    This strategy is designed to exploit opponents' cooperation while minimizing potential losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prime = 0
        self.prev_action = None
        self.punishment_mode = False
        self.concession_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if n < 5:
            if state.round_number <= 1:
                return Action.D
        elif r - state.round_number < 3:
            if self.m_prime >= m:
                return Action.C
            else:
                return Action.D
        self.m_prime = sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == True)
        if history.opponent_payoffs[-1, 0] > history.my_payoffs[-1]:
            self.punishment_mode = True
        cooperators = sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == False)
        if cooperators >= n / 2:
            self.concession_mode = True
        if self.m_prime < m and self.prev_action == Action.C:
            return Action.D if state.round_number % 2 == 0 else Action.C
        if self.punishment_mode:
            self.punishment_mode = False
            return Action.D
        elif self.concession_mode:
            self.concession_mode = False
            return Action.C
        elif self.m_prime >= m:
            return Action.C
        else:
            return Action.D
        self.prev_action = history.my_actions[-1] if state.round_number > 0 else None

class Strategy_EXPLOITATIVE_402(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy, designed to exploit the game's structure and other players' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [Action.C.value])
        avg_defector_payoff = np.mean([payoff for payoff, action in zip(np.append(history.opponent_payoffs[-1], 0), history.opponent_actions[-1] + [False]) if not action])
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif self.game_description.k > avg_defector_payoff:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_cooperators == self.game_description.m - 1:
            if self.game_description.k > 1.5 * avg_defector_payoff:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_403(BaseStrategy):
    """
    Exploitative strategy that adapts to the collective risk dilemma by balancing cooperation and defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[self.game_description.m / self.game_description.n_players, 1 - self.game_description.m / self.game_description.n_players])[0]
        if history is not None:
            total_coop = sum((1 for actions in history.opponent_actions for action in actions if action)) + (history.my_actions.sum() - state.round_number)
            total_rounds = state.round_number * self.game_description.n_players
            self.avg_coop_rate = total_coop / total_rounds
        if state.round_number < self.game_description.n_rounds - 1:
            if self.avg_coop_rate >= self.game_description.m / self.game_description.n_players:
                return random.choices([Action.C, Action.D], weights=[self.game_description.k / (self.game_description.k + 1), 1 - self.game_description.k / (self.game_description.k + 1)])[0]
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_404(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob_first = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.coop_prob_first else Action.D
        coop_ratio = sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1)
        if coop_ratio >= self.game_description.m / self.game_description.n_players:
            if coop_ratio > (self.game_description.m + 1) / self.game_description.n_players and random.random() < (self.game_description.k - 1) / (self.game_description.k + 1):
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_406(BaseStrategy):
    """
    Exploitative strategy that balances cooperation and adaptability.
    It escalates cooperation gradually, encouraging opponents to reciprocate,
    while adapting to those who defect frequently.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.r = game_description.n_rounds
        self.prev_coops = 0
        self.avg_coop_rate = 0.0
        self.action_history = []
        self.opponent_avg_coop_rate = np.zeros(self.r)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < min(3, self.r):
            return Action.C
        T = max(self.m - 2, 0)
        if self.prev_coops <= self.m:
            T = self.m - 1
        if history is not None:
            coop_rates = np.mean(history.opponent_actions[max(0, state.round_number - 5):state.round_number], axis=0)
            self.opponent_avg_coop_rate[state.round_number] = np.mean(coop_rates)
        if self.prev_coops >= T and np.mean(self.opponent_avg_coop_rate[:state.round_number]) > 0.5:
            return Action.C
        return Action.D

    def update_state(self, history: PlayerHistory):
        self.prev_coops = sum(history.my_actions)
        if history.opponent_actions.shape[0] < 5:
            self.avg_coop_rate = np.mean(history.opponent_actions, axis=1).mean()
        else:
            self.avg_coop_rate = np.mean(history.opponent_actions[-5:, :], axis=(0, 1))
        self.action_history.append(self(history.state, history))

class Strategy_EXPLOITATIVE_407(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to exploit opponents' cooperative tendencies 
    while minimizing its own risk exposure in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.cooperators_count_history = []
        self.total_payoff_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1]
        self.cooperators_count_history.append(sum(opponent_actions_last_round))
        my_payoff_last_round = history.my_payoffs[-1] if len(history.my_payoffs) > 0 else 0
        total_payoff_last_round = sum([my_payoff_last_round] + [payoff for payoff in history.opponent_payoffs[-1]])
        self.total_payoff_history.append(total_payoff_last_round)
        cooperators_count_last_round = self.cooperators_count_history[-1]
        total_payoff_last_round = self.total_payoff_history[-1]
        if cooperators_count_last_round < self.m:
            return Action.D
        p = self.k * cooperators_count_last_round / total_payoff_last_round
        if math.isclose(p, 0):
            return Action.C
        elif math.isclose(p, 1):
            return Action.D
        else:
            random_number = random.random()
            if random_number < p:
                return Action.C
            else:
                return Action.D
        total_cooperators = sum(self.cooperators_count_history)
        if state.round_number == self.r - 1 and total_cooperators >= self.m * (self.r // 2):
            return Action.C

class Strategy_EXPLOITATIVE_408(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    
    This strategy adapts to the game's history and parameters, aiming to maximize individual payoff by exploiting others' cooperative tendencies while minimizing the risk of contributing to a failed community project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + (self.game_description.n_players - 1 > history.my_actions[-1]))
        if prev_cooperators >= self.game_description.m:
            return Action.D
        else:
            prev_defectors = sum((not action for action in history.opponent_actions[-1, :])) + (not self.game_description.n_players > history.my_actions[-1])
            if prev_defectors > self.game_description.m:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperators = sum(history.my_actions) + sum((sum(actions) for actions in history.opponent_actions))
            if total_cooperators >= self.game_description.m - 1:
                return Action.C

class Strategy_EXPLOITATIVE_409(BaseStrategy):
    """
    Exploits the cooperation of others in the Collective Risk Dilemma while minimizing risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = game_description.m / game_description.n_players
        self.θ = game_description.m / game_description.n_players
        self.Δθ = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        p_coop_prev = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if p_coop_prev >= self.θ:
            action = Action.D
        else:
            action = Action.C
        if p_coop_prev > self.game_description.m / self.game_description.n_players:
            self.θ -= self.Δθ
        elif p_coop_prev < self.game_description.m / self.game_description.n_players:
            self.θ += self.Δθ
        return action

class Strategy_EXPLOITATIVE_410(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = None
        self.opponent_actions = [[] for _ in range(game_description.n_players)]
        self.round_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperation_probability = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperation_probability else Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            self.opponent_actions[i].append(opponent_action)
        num_cooperators = sum(history.opponent_actions[-1]) + (history.my_actions[-1] if history else 0)
        if num_cooperators >= self.game_description.m:
            cooperation_probability = 1 - (self.game_description.n_players - num_cooperators) / self.game_description.n_players
        else:
            cooperation_probability = 0
        for i, opponent_action_history in enumerate(self.opponent_actions):
            if len(opponent_action_history) >= self.game_description.n_rounds // 2:
                num_defections = sum((not action for action in opponent_action_history))
                if num_defections > len(opponent_action_history) / 2:
                    cooperation_probability -= 0.1
                elif (num_cooperations := (len([action for action in opponent_action_history if action]) > 3 * len(opponent_action_history) / 4)):
                    cooperation_probability += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.cooperation_probability = max(0, min(cooperation_probability, 1))
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_411(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        avg_payoff_prev_round = np.mean(np.concatenate([history.my_payoffs[-1:], history.opponent_payoffs[-1]]))
        if prev_round_cooperators < self.game_description.m:
            return Action.D
        elif avg_payoff_prev_round < 1.5 * self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_412(BaseStrategy):
    """
    The Adaptive Threshold Hunter strategy seeks to exploit opponents' behavior while ensuring a stable, cooperative outcome when possible.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.w = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if m_t >= self.game_description.m:
            action = Action.C
        else:
            opponent_cooperation_rates = []
            for i in range(self.game_description.n_players - 1):
                cooperation_count = sum(history.opponent_actions[max(0, state.round_number - self.w):, i])
                cooperation_rate = cooperation_count / min(state.round_number + 1, self.w)
                opponent_cooperation_rates.append(cooperation_rate)
            if all((rate >= 0.5 for rate in opponent_cooperation_rates)):
                action = Action.C
            else:
                action = Action.D
        if state.round_number > 1:
            current_payoff = history.my_payoffs[-1]
            previous_payoff = history.my_payoffs[-2]
            if current_payoff > previous_payoff:
                self.w += 1
            elif current_payoff < previous_payoff:
                self.w -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_414(BaseStrategy):
    """
    Exploits opponents' generosity in the Collective Risk Dilemma by adapting to their cooperation rate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperative_players = set()
        self.CR = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number <= min(self.game_description.n_rounds, 3):
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        opponent_actions_last_round = history.opponent_actions[-1, :]
        self.CR = sum(opponent_actions_last_round) / self.game_description.n_players
        if self.CR >= self.game_description.m / self.game_description.n_players:
            play_C = True
        else:
            play_C = False
        for i, opponent_action in enumerate(opponent_actions_last_round):
            if opponent_action and i not in self.cooperative_players:
                self.cooperative_players.add(i)
        current_opponent = state.round_number % self.game_description.n_players
        if current_opponent in self.cooperative_players:
            play_C = False
        if state.round_number == self.game_description.n_rounds - 1:
            current_payoff = sum(history.my_payoffs)
            expected_payoff_from_cooperating = self.CR * self.game_description.k + (1 - self.CR)
            play_C = True if current_payoff <= expected_payoff_from_cooperating else False
        return Action.C if play_C else Action.D

class Strategy_EXPLOITATIVE_415(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' behavior by adapting to their cooperation rates and payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_thresholds = {'initial': math.floor(game_description.n_rounds / 3), 'mid_game': math.floor(2 * game_description.n_rounds / 3)}
        self.p_init = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D]) if random.random() < self.p_init else Action.D

        def calculate_average_cooperation_rate(rounds):
            avg = np.mean(history.my_actions[rounds] + history.opponent_actions[rounds, :].sum(axis=1))
            return avg / self.game_description.n_players

        def calculate_average_payoff(rounds):
            payoffs = np.concatenate([history.my_payoffs[rounds], history.opponent_payoffs[rounds, :].flatten()])
            return np.mean(payoffs)
        if state.round_number <= self.round_thresholds['initial']:
            return Action.C if random.random() < self.p_init else Action.D
        elif state.round_number <= self.round_thresholds['mid_game']:
            c_avg = calculate_average_cooperation_rate(slice(-self.round_thresholds['initial'], None))
            p_mid = min(c_avg, 0.8) if c_avg >= self.game_description.m / self.game_description.n_players else 0
            return Action.C if random.random() < p_mid else Action.D
        else:
            avg_payoff = calculate_average_payoff(slice(-self.round_thresholds['initial'], None))
            c_avg = calculate_average_cooperation_rate(slice(-self.round_thresholds['initial'], None))
            p_late = max(0, c_avg - 0.2) if avg_payoff > 1.5 else 0
            return Action.C if random.random() < p_late else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_416(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        self.m_prev = sum(prev_opponent_actions) + 1
        if history.my_payoffs.shape[0] > 1 and history.opponent_payoffs[-2, :].max() == 1:
            return Action.D
        if self.m_prev >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        coop_prob = self.m_prev / self.game_description.n_players * self.game_description.k
        coop_prob = max(0.1, min(coop_prob, 0.9))
        if random.random() < coop_prob:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_417(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma game.
    
    This strategy exploits the collective risk dilemma by adapting to the behavior of other players.
    By cooperating initially and in subsequent rounds when the threshold is not met, we encourage cooperation 
    and create an opportunity for mutual benefit. When the threshold is met, we defect to maximize our individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.m:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if previous_round_cooperators >= self.game_description.m:
            return Action.D
        elif previous_round_cooperators > 0 and history.my_payoffs[-1] == 1 + self.game_description.k:
            return Action.C
        elif previous_round_cooperators == 0 or history.my_payoffs[-1] == 1:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_418(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy.
    This strategy aims to exploit opponents by adapting to their behavior 
    and maximizing payoffs while minimizing contributions to the community project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        k = self.game_description.k
        if m == 1:
            return Action.D
        elif n_players == 2:
            round_number = state.round_number
            if history and round_number > 0:
                previous_action = bool(history.my_actions[-1])
                return Action.C if not previous_action else Action.D
            else:
                return Action.C
        if state.round_number == 0:
            return Action.D
        opponent_actions_last_round = history.opponent_actions[-1, :]
        my_previous_action = bool(history.my_actions[-1])
        if state.round_number <= math.ceil(r / 2):
            cooperators_last_round = sum(opponent_actions_last_round) + my_previous_action
            return Action.C if cooperators_last_round >= m - 1 else Action.D
        elif state.round_number <= math.ceil(3 * r / 4):
            avg_payoff = np.mean(history.my_payoffs)
            my_previous_payoff = history.my_payoffs[-1]
            cooperators_last_round = sum(opponent_actions_last_round) + my_previous_action
            return Action.C if cooperators_last_round >= m or my_previous_payoff < avg_payoff else Action.D
        elif state.round_number <= r - 1:
            avg_payoff = np.mean(history.my_payoffs)
            my_previous_payoff = history.my_payoffs[-1]
            cooperators_last_round = sum(opponent_actions_last_round) + my_previous_action
            return Action.D if cooperators_last_round < m or my_previous_payoff >= avg_payoff else Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_419(BaseStrategy):
    """
    Exploitative strategy that adapts to the number of cooperators in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def calculate_p(self, t, r):
        return 1 - t / r

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + 1
        if cooperators < self.game_description.m:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            p = self.calculate_p(state.round_number, self.game_description.n_rounds)
            return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_420(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.5
        self.cooperators_detected = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number < int(0.1 * self.game_description.n_rounds):
            return Action.C
        opponents_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        if history.my_payoffs is not None:
            my_last_payoff = history.my_payoffs[-1]
        else:
            my_last_payoff = 0
        opponent_defected_last_round = history.opponent_actions[-1, 0] == False
        if opponent_defected_last_round and my_last_payoff < self.game_description.k:
            self.cooperation_rate -= max(0.01, 0.1 * self.cooperation_rate)
        elif not opponent_defected_last_round and my_last_payoff > self.game_description.k:
            self.cooperation_rate += min(0.99, 0.1 + self.cooperation_rate)
        if np.mean(history.opponent_actions[-5:, :].sum(axis=1)) >= self.game_description.m:
            self.cooperators_detected += 1
        if self.cooperators_detected / state.round_number >= self.game_description.m and opponents_cooperation_rate > 0.5:
            return Action.C
        elif opponents_cooperation_rate < 0.5 or (self.game_description.k < 2 and np.mean(history.my_payoffs) < self.game_description.k):
            return Action.D
        elif random.random() < self.cooperation_rate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_421(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == 1:
            coop_prev = sum(history.opponent_actions[-1, :])
            return Action.C if coop_prev >= self.m / 2 else Action.D
        avg_coop = np.mean([np.sum(round) for round in history.opponent_actions])
        if avg_coop >= self.m:
            p_c = (self.k - 1) / self.k
            return Action.C if random.random() < p_c else Action.D
        else:
            coop_prev = sum(history.opponent_actions[-1, :])
            if coop_prev < self.m / 2:
                return Action.D
            else:
                p_c = 1 - (self.m - coop_prev) / (self.n - coop_prev + 1)
                return Action.C if random.random() < p_c else Action.D
        avg_coop = np.mean([np.sum(round) for round in history.opponent_actions[:-1]])
        if avg_coop >= self.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_422(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    
    This strategy is designed to exploit various aspects of human behavior, 
    including altruism, complacency, and predictability. It adapts to the 
    game's progression and responds to different opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 1.5
        self.complacency_window = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        elif 1 <= state.round_number < self.game_description.n_rounds // 2:
            prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            if prev_cooperators < self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif self.game_description.n_rounds // 2 <= state.round_number < self.game_description.n_rounds - 2:
            avg_payoff = np.mean(history.my_payoffs[-self.complacency_window:])
            if avg_payoff > self.threshold:
                return Action.D
            else:
                return Action.C
        elif state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        elif self.game_description.n_players == 1:
            if history.my_actions[-1]:
                return Action.C
            else:
                return Action.D
        elif np.count_nonzero(np.isclose(history.my_payoffs[-1], history.opponent_payoffs[-1, :])) > 0:
            return random.choice([Action.C, Action.D])

class Strategy_EXPLOITATIVE_423(BaseStrategy):
    """
    An adaptive strategy that balances cooperation and exploitation based on observed opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.dt = None
        self.p_init = 0.5
        self.avg_payoff_game_so_far = 0.0
        self.last_3_round_actions = []
        self.last_3_round_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if self.dt is None or (state.round_number - 1) % 3 == 0:
            if prev_round_cooperators >= self.game_description.m:
                self.dt = 0.3
            else:
                self.dt = 0.7
        coop_prob = prev_round_cooperators / self.game_description.n_players * self.dt
        action = Action.C if random.random() < coop_prob else Action.D
        if history is not None and state.round_number > 1:
            self.avg_payoff_game_so_far += history.my_payoffs[-1] - self.avg_payoff_game_so_far / (state.round_number - 1)
            self.last_3_round_actions.append(action.value)
            self.last_3_round_payoffs.append(history.my_payoffs[-1])
        if state.round_number % 3 == 0:
            avg_last_3_round_payoffs = np.mean(self.last_3_round_payoffs) if len(self.last_3_round_payoffs) > 2 else history.my_payoffs[-1]
            if avg_last_3_round_payoffs > self.avg_payoff_game_so_far:
                self.dt += 0.1
            else:
                self.dt -= 0.1
        self.dt = max(0, min(self.dt, 1))
        return action

class Strategy_EXPLOITATIVE_424(BaseStrategy):
    """
    An exploitative strategy that adapts its cooperation threshold based on previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.delta_theta = 0.1
        self.theta = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        if self.theta is None:
            self.theta = max(self.game_description.m - 1, min(self.p_init * self.game_description.n_players, self.game_description.n_players))
        else:
            previous_round_payoff = history.my_payoffs[-1]
            if previous_round_payoff > 1:
                self.theta += self.delta_theta
            elif previous_round_payoff <= 1:
                self.theta -= self.delta_theta
            self.theta = max(self.game_description.m - 1, min(self.theta, self.game_description.n_players))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_cooperators >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_425(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma game that adapts its cooperation level based on the history of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        if prev_round_coops >= self.game_description.m:
            ET = 1 - self.game_description.k / self.game_description.n_players
        else:
            ET = 0
        if state.round_number < self.game_description.n_rounds - 1:
            return Action.C if prev_round_coops > ET * self.game_description.n_players else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_426(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy aims to maximize its payoff by adaptively exploiting 
    the collective risk dilemma game's parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators < self.game_description.m:
            self.p = max(0, self.p - 0.1)
            return Action.D
        elif prev_cooperators == self.game_description.m:
            my_payoff = history.my_payoffs[-1]
            avg_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
            if my_payoff > avg_opponent_payoff:
                self.p = min(1, self.p + 0.1)
            else:
                self.p = max(0, self.p - 0.1)
            if random.random() < self.p:
                return Action.C
            else:
                return Action.D
        elif prev_cooperators > self.game_description.m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_427(BaseStrategy):
    """
    The Adaptive Cooperator strategy is designed to exploit a wide range of opponent behaviors while adapting to changing conditions throughout the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_behavior = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.opponent_actions.shape) < 2:
            history.opponent_actions = np.atleast_2d(history.opponent_actions)
        self.update_opponent_behavior(history)
        m_prev = self.calculate_m_prev(history)
        if state.round_number <= 2:
            return Action.C
        if m_prev >= self.game_description.m - 1:
            return Action.C
        opponent_action = self.get_opponent_action(history)
        if self.is_consistent_defector(opponent_action):
            return Action.D
        reciprocate_cooperation = self.check_reciprocal_cooperation(history)
        if reciprocate_cooperation:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

    def update_opponent_behavior(self, history: PlayerHistory):
        for opponent in range(history.opponent_actions.shape[1]):
            action = history.opponent_actions[-1, opponent]
            if opponent not in self.opponent_behavior:
                self.opponent_behavior[opponent] = [action]
            else:
                self.opponent_behavior[opponent].append(action)

    def calculate_m_prev(self, history: PlayerHistory):
        m_prev = sum((1 for action in history.opponent_actions[-1, :] if action == 1))
        return m_prev

    def get_opponent_action(self, history: PlayerHistory):
        opponent = random.choice(range(history.opponent_actions.shape[1]))
        return self.opponent_behavior[opponent]

    def is_consistent_defector(self, opponent_action: list):
        return len([action for action in opponent_action if not action]) >= len(opponent_action) / 2

    def check_reciprocal_cooperation(self, history: PlayerHistory):
        reciprocal_cooperation = False
        for opponent, actions in self.opponent_behavior.items():
            if len([action for action in actions if action]) > 0:
                reciprocal_cooperation = True
        return reciprocal_cooperation

class Strategy_EXPLOITATIVE_428(BaseStrategy):
    """
    The Adaptive Exploiter strategy exploits opponents' cooperative behavior while adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_counts = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history is not None else True)
        if prev_coop_count < self.game_description.m:
            return Action.D
        elif prev_coop_count == self.game_description.m:
            ocrs = [count / state.round_number for count in self.cooperation_counts]
            if any((ocr > 0.5 for ocr in ocrs)):
                return Action.C
            else:
                return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if action:
                self.cooperation_counts[i] += 1
        if history.my_actions[-1]:
            self.cooperation_counts[0] += 1

class Strategy_EXPLOITATIVE_429(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.estimated_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.my_actions) > 0:
            opponents_cooperated = np.sum(np.mean(history.opponent_actions, axis=1) >= 0.5)
            self.estimated_prob = (self.estimated_prob * len(history.my_actions) + opponents_cooperated) / (len(history.my_actions) + 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.estimated_prob > 0.7:
            return Action.D
        elif self.estimated_prob < 0.4 or (state.round_number < 5 and len(history.my_actions) > 0 and (np.mean(np.mean(history.opponent_actions, axis=1)) < self.game_description.m / self.game_description.n_players)):
            return Action.C
        else:
            p = math.pow(1 - self.estimated_prob, 2)
            if random.random() < p:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_430(BaseStrategy):
    """
    Adaptive Reward-Seeking (ARS) strategy for Collective Risk Dilemma.
    Balances individual reward-seeking with adaptability to opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_thresholds = [5, game_description.n_rounds // 2]
        self.avg_coop_rates = []
        self.opponent_avg_coop_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(self.avg_coop_rates) > 0 and state.round_number % 5 == 1:
            self.opponent_avg_coop_rate = np.mean([sum(h) for h in history.opponent_actions]) / state.round_number
        if state.round_number < self.round_thresholds[0]:
            prev_round_coops = sum(history.my_actions[-1:])
            if prev_round_coops < self.game_description.m:
                return Action.D
            else:
                p = self.game_description.k / (self.game_description.k + 1)
                return Action.C if random.random() < p else Action.D
        avg_coop_rate = np.mean(self.avg_coop_rates) if len(self.avg_coop_rates) > 0 else self.opponent_avg_coop_rate
        if state.round_number < self.round_thresholds[1]:
            if avg_coop_rate > self.game_description.m / self.game_description.n_players:
                p = 0.7 + (self.game_description.k - 1) / (2 * self.game_description.k)
                return Action.C if random.random() < p else Action.D
            else:
                q = 0.3 + (self.game_description.n_players - self.game_description.m) / (2 * self.game_description.n_players)
                return Action.D if random.random() < q else Action.C
        opponent_coop_rate = np.mean([sum(h) for h in history.opponent_actions]) / state.round_number
        if opponent_coop_rate > self.game_description.m / self.game_description.n_players:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_431(BaseStrategy):
    """
    A strategic approach that aims to exploit various opponent behaviors while being robust and adaptive throughout the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_cooperators = [0] * (game_description.n_rounds + 1)
        self.avg_cooperation_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            if state.round_number == 1:
                return Action.C
            else:
                return Action.D
        elif state.round_number < 11:
            self.m_cooperators[state.round_number - 1] = sum(history.opponent_actions[-2, :] + [True])
            if self.m_cooperators[state.round_number - 1] < self.game_description.m:
                return Action.C
            else:
                return Action.D
        else:
            avg_cooperation_rate = np.mean([sum(history.opponent_actions[t, :] + [True]) / self.game_description.n_players for t in range(max(0, state.round_number - 6), state.round_number - 1)])
            self.avg_cooperation_rate = (self.avg_cooperation_rate * (state.round_number - 6) + avg_cooperation_rate) / (state.round_number - 5)
            if self.avg_cooperation_rate > 0.5:
                return Action.C if random.random() < 0.6 else Action.D
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds:
            total_coop_payoff = sum([self.calculate_payoff(t, True) for t in range(1, state.round_number + 1)])
            total_defect_payoff = sum([self.calculate_payoff(t, False) for t in range(1, state.round_number + 1)])
            if total_coop_payoff <= total_defect_payoff:
                return Action.C

    def calculate_payoff(self, round_number: int, cooperate: bool):
        if self.m_cooperators[round_number - 1] < self.game_description.m:
            return 1.0 - float(cooperate)
        else:
            return 1.0 - float(cooperate) + self.game_description.k

class Strategy_EXPLOITATIVE_432(BaseStrategy):
    """
    Exploitative strategy that adapts to game history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_number = state.round_number
        if self.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_threshold_met = history.my_payoffs[-1] > 0
        p_C = self.game_description.m / self.game_description.n_players * ((self.game_description.k - 1) / self.game_description.k) if prev_threshold_met else 0
        opponent_actions = history.opponent_actions[-1, :]
        for i in range(len(opponent_actions)):
            opponent_cooperation_rate = sum(history.opponent_actions[:, i]) / (self.round_number + 1)
            if opponent_cooperation_rate > 0.75:
                p_C += min(0.1, 1 - p_C)
        if self.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_defection_rates = [sum(history.opponent_actions[:, i]) / (self.round_number + 1) for i in range(len(opponent_actions))]
        if all((defection_rate > 0.9 for defection_rate in opponent_defection_rates)):
            return Action.D
        return Action.C if random.random() < p_C else Action.D

class Strategy_EXPLOITATIVE_433(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.θ = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if previous_round_cooperators < self.game_description.m:
            action = Action.D
        else:
            CR = (previous_round_cooperators - 1) / (self.game_description.n_players - 1)
            if CR > self.θ:
                action = Action.C
            else:
                action = Action.D
        for i in range(self.game_description.n_players - 1):
            if np.all(history.opponent_actions[-2:, i]) and history.opponent_actions.shape[0] >= 3:
                self.θ = 0.7
            elif not np.any(history.opponent_actions[max(-3, -history.opponent_actions.shape[0]):, i]):
                self.θ = 0.3
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_434(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game that exploits cooperative behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coops = 0.0
        self.observation_phase_complete = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if not self.observation_phase_complete and state.round_number < min(5, self.game_description.n_rounds):
            return Action.C
        else:
            self.observation_phase_complete = True
            opponent_coops = history.opponent_actions[:, 0].sum()
            my_coop_count = state.round_number - (history.my_actions.sum() if history.my_actions.size > 1 else 0)
            total_cooperators = opponent_coops + my_coop_count
            self.avg_coops = total_cooperators / state.round_number
            if self.avg_coops >= self.game_description.m:
                return Action.D if random.random() < 0.8 else Action.C
            elif self.avg_coops + 1 >= self.game_description.m:
                return Action.C if state.round_number % 2 == 0 else Action.D
            else:
                return Action.D
        if state.round_number > self.game_description.n_rounds - 2:
            opponent_coops = history.opponent_actions[:, 0].sum()
            my_payoff_total = np.sum(history.my_payoffs)
            avg_opponent_payoff = np.mean(np.sum(history.opponent_payoffs, axis=0))
            if my_payoff_total > avg_opponent_payoff:
                return Action.D
            else:
                return Action.C if state.round_number % 2 == 0 else Action.D

class Strategy_EXPLOITATIVE_436(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Risk-Taker" 
    Balances initial cooperation, adaptive exploitation, and final-round optimization.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.successful_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        n = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if state.round_number > 1 and history.opponent_actions[-2, :].sum() + 1 >= m:
            self.successful_cooperation_rounds += 1
        p = min(0.5 + 0.2 * (self.successful_cooperation_rounds / r), 1)
        if state.round_number <= r // 2:
            return Action.C if random.random() < p else Action.D
        elif state.round_number > r // 2 and state.round_number <= 3 * r // 4:
            opponent_actions = history.opponent_actions[-1, :]
            if opponent_actions.sum() / n > 0.6:
                return Action.D if random.random() < 0.8 else Action.C
            return Action.C if random.random() < 0.5 else Action.D
        else:
            previous_cooperators = history.opponent_actions[-1, :].sum()
            if previous_cooperators + 1 >= m:
                return Action.C if random.random() < 0.8 else Action.D
            return Action.D if random.random() < 0.8 else Action.C

class Strategy_EXPLOITATIVE_437(BaseStrategy):
    """
    Exploits opponents by cooperating when they cooperate and defecting otherwise.
    Adapts its threshold for cooperation based on the history of cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_history = []
        self.threshold_adaptive = [game_description.m]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.my_actions) > 0:
            self.cooperators_history.append(sum(history.opponent_actions[-1, :]))
        if len(self.cooperators_history) > 0:
            max_cooperators_past = max(self.cooperators_history)
            m̂_t = self.game_description.m - 1 / 2 * (self.game_description.n_players - max_cooperators_past)
            self.threshold_adaptive.append(m̂_t)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if len(self.cooperators_history) > 0 and self.cooperators_history[-1] >= self.threshold_adaptive[-1]:
            return Action.C
        elif len(self.cooperators_history) > 0 and self.cooperators_history[-1] == self.threshold_adaptive[-1]:
            return random.choice([Action.C, Action.D])
        else:
            return Action.D

class Strategy_EXPLOITATIVE_438(BaseStrategy):
    """
    Implementing Adaptive Threshold Pursuit (ATP) strategy for Collective Risk Dilemma.
    
    ATP adapts to opponents' behavior, initially exploring their cooperation rate and then 
    exploiting their efforts while focusing on securing rewards by meeting the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize the strategy with the given game description.

        :param game_description: CollectiveRiskDescription instance containing n_players, n_rounds, m, and k
        """
        self.game_description = game_description
        self.r_avg = 0.0
        self.p_Coop = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the action based on the current state and past actions.

        :param state: Current game state
        :param history: History of all players' actions and payoffs (or None for round 0)
        :return: Chosen action (Action.C or Action.D)
        """
        if state.round_number == 0:
            return Action.C if random.random() < self.p_Coop else Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        total_opponent_cooperations = np.sum(opponent_cooperations)
        self.r_avg = total_opponent_cooperations / (state.round_number * self.game_description.n_players)
        if self.r_avg >= self.game_description.m / self.game_description.n_players:
            self.p_Coop = max(0.5, self.game_description.m / self.game_description.n_players + 0.1)
        else:
            p_Defect = min(1, self.game_description.n_players / self.game_description.m - 0.1)
            return Action.D if random.random() < p_Defect else Action.C
        if state.round_number > self.game_description.n_rounds // 2:
            total_rewards = np.sum(history.my_payoffs) / (self.game_description.k - 1)
            if total_rewards < self.game_description.m:
                self.p_Coop = max(0.5, 1 / self.game_description.m + 0.1)
        return Action.C if random.random() < self.p_Coop else Action.D

class Strategy_EXPLOITATIVE_439(BaseStrategy):
    """
    The Threshold Tracker strategy adapts to opponents' behavior in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coops = 0

    def opponent_is_free_rider(self, history: PlayerHistory) -> bool:
        if len(history.my_actions) == 1:
            return False
        opponent_defections = sum(~history.opponent_actions[:-1])
        opponent_rounds = len(history.opponent_actions[:-1])
        return opponent_defections / opponent_rounds > 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        my_cooperations = sum(history.my_actions[:-1])
        opponent_cooperations = np.sum(~history.opponent_actions[:-1], axis=0)
        avg_coops = (my_cooperations + np.mean(opponent_cooperations)) / self.game_description.n_players
        if avg_coops >= self.game_description.m:
            cooperate_prob = (self.game_description.k - 1) / self.game_description.k
        else:
            cooperate_prob = 0
        for opponent_actions in history.opponent_actions.T:
            opponent_history = PlayerHistory(my_actions=history.my_actions, my_payoffs=history.my_payoffs, opponent_actions=opponent_actions.reshape(-1, 1), opponent_payoffs=np.zeros((len(history.my_payoffs), 1)))
            if self.opponent_is_free_rider(opponent_history):
                cooperate_prob *= (self.game_description.k - 1) / (2 * self.game_description.k)
        if state.round_number > 0 and sum(~history.opponent_actions[-1, :]) < self.game_description.m:
            return Action.D
        if random.random() < cooperate_prob:
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_440(BaseStrategy):
    """
    This strategy balances cooperation and exploitation by adapting to the game's history and parameters.
    It uses a simple exponential smoothing method to estimate the probability of reaching the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.theta = 0.6
        self.p_hat_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if history is not None:
            cooperation_rate = np.mean(history.my_actions[:state.round_number - 1])
            self.p_hat_prev = self.alpha * cooperation_rate + (1 - self.alpha) * self.p_hat_prev
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        my_cooperator = history.my_actions[-1]
        if self.p_hat_prev >= self.theta and opponent_cooperators + my_cooperator <= self.game_description.m - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_441(BaseStrategy):
    """
    Adaptive Exploiter strategy for the Collective Risk Dilemma game.
    
    This strategy monitors opponent behavior and adapts its cooperation threshold 
    based on their actions. It aims to exploit opponents' cooperative tendencies 
    while minimizing the risk of being exploited itself.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_actions = history.opponent_actions[:-1] if len(history.opponent_actions) > 0 else []
        self.avg_coop_rate = np.mean(opponent_actions) if len(opponent_actions) > 0 else 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        coop_threshold = self.game_description.m / self.game_description.n_players + (1 - self.avg_coop_rate) * (self.game_description.k - 1) / self.game_description.k
        return Action.C if self.avg_coop_rate >= coop_threshold else Action.D

class Strategy_EXPLOITATIVE_442(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy in Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_coop_prev = None
        self.avg_payoff_coop_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_coop = sum(history.opponent_actions[-1, :])
        total_coop = opponent_coop + history.my_actions[-1]
        self.n_coop_prev = total_coop
        if total_coop > 0:
            avg_payoff = np.mean(np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1, :]))) if history.my_actions[-1] else np.mean(history.opponent_payoffs[-1, :])
            self.avg_payoff_coop_prev = avg_payoff
        if state.round_number < self.game_description.n_rounds - 1:
            if self.n_coop_prev < self.game_description.m:
                return Action.D
            elif self.avg_payoff_coop_prev >= self.game_description.k:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_443(BaseStrategy):
    """
    Exploitative strategy that adapts to the number of cooperators and adjusts its actions accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        c_count = sum(opponent_actions)
        π_prev = np.mean(history.my_payoffs)
        if c_count >= self.game_description.m:
            action = Action.C
        else:
            p_defect = (self.game_description.m - c_count) / self.game_description.n_players
            action = Action.D if random.random() < p_defect else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if c_count + 1 >= self.game_description.m:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_444(BaseStrategy):
    """
    Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold"

    This strategy aims to balance cooperation and exploitation by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M = game_description.m
        self.cooperate_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if state.round_number == 0 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (Action.C.value if history.my_actions[-1] else 0)
        if prev_cooperators >= self.M:
            self.M -= 1
        else:
            self.M += 1
        opponent_actions = history.opponent_actions[-1, :]
        if sum(opponent_actions) + (Action.C.value if Action.C else 0) >= self.M:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_445(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing the risk of contributing to a failed collective project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_t = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_t else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if self.game_description.n_players == self.game_description.m:
            return Action.C
        if prev_cooperators < self.game_description.m:
            cooperate_prob = 0
        else:
            cooperate_prob = self.game_description.k * self.game_description.m / (self.game_description.n_players * self.game_description.k + self.game_description.n_players - 1)
            opponent_action = history.opponent_actions[-1, 0]
            if opponent_action:
                cooperate_prob += 0.1
            else:
                cooperate_prob -= 0.1
        cooperate_prob = max(0, min(cooperate_prob, 1))
        self.p_t = cooperate_prob
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_446(BaseStrategy):
    """
    The Adaptive Cooperative Exploiter (ACE) strategy balances cooperation and exploitation in the Collective Risk Dilemma game.
    It adapts to opponents' behavior over time, seeking to maximize its own payoff while minimizing the risk of being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploration_phase = False
        self.exploitation_phase = False
        self.adaptive_response = False
        self.endgame = False
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.m:
            return Action.C
        if not self.exploration_phase and state.round_number >= self.game_description.m:
            self.exploration_phase = True
            self.round_count = 0
            if state.round_number % 2 == 0:
                return Action.C
            else:
                return Action.D
        if self.exploration_phase and self.round_count >= math.floor(self.game_description.n_rounds / 4):
            self.exploration_phase = False
            self.exploitation_phase = True
        if self.exploitation_phase:
            opponent_cooperation = np.mean(history.opponent_actions[-1, :])
            if opponent_cooperation >= 0.5:
                return Action.C
            else:
                return Action.D
        if not self.adaptive_response and state.round_number > math.floor(self.game_description.n_rounds / 4):
            self.adaptive_response = True
            if history.my_actions[-1] and np.any(np.logical_not(history.opponent_actions[-1, :])):
                return Action.D
            elif not history.my_actions[-1] and np.any(history.opponent_actions[-1, :]):
                return Action.C
        if state.round_number >= self.game_description.n_rounds - self.game_description.m:
            self.endgame = True
            opponent_cooperations = np.sum(np.any(history.opponent_actions, axis=0))
            if opponent_cooperations >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_448(BaseStrategy):
    """
    A strategic approach for the Collective Risk Dilemma game that is exploitative and adaptive.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_cooperators < self.game_description.m and history.my_actions[-1]:
            self.punishment_counter += 1
        elif prev_cooperators >= self.game_description.m:
            self.punishment_counter = 0
        if state.round_number < self.game_description.n_rounds - 1 and self.punishment_counter <= 2:
            return Action.C if prev_cooperators >= self.game_description.m - 1 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if prev_cooperators >= self.game_description.m - 1 else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_449(BaseStrategy):
    """
    Adaptive Threshold Tracker strategy for Collective Risk Dilemma.
    
    This strategy adapts to opponent behavior by tracking their actions and adjusting 
    its own threshold based on their cooperation rates. It aims to balance cooperation 
    with the risk of exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m
        self.coop_payoff = 0
        self.defect_payoff = 0
        self.opponent_coop_count = [0] * game_description.n_players
        self.opponent_defect_count = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if prev_coop_count < self.threshold or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.coop_payoff < self.defect_payoff:
            return Action.D
        else:
            return Action.C
        self.opponent_coop_count = [self.opponent_coop_count[i] + (history.opponent_actions[-1, i] == Action.C) for i in range(self.game_description.n_players)]
        self.opponent_defect_count = [self.opponent_defect_count[i] + (history.opponent_actions[-1, i] == Action.D) for i in range(self.game_description.n_players)]
        if prev_coop_count >= self.threshold:
            self.coop_payoff += self.game_description.k
        else:
            self.defect_payoff += 1
        if sum(self.opponent_defect_count) > self.game_description.n_players // 2 and state.round_number >= 3:
            self.threshold = min(self.threshold + 1, self.game_description.n_players - 1)
        elif sum(self.opponent_coop_count) > self.game_description.n_players // 2 and state.round_number >= 3:
            self.threshold = max(self.threshold - 1, 1)

class Strategy_EXPLOITATIVE_450(BaseStrategy):
    """
    A strategy that leverages game history and parameters to maximize payoffs 
    while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = 0
        self.prev_payoff = None
        self.coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        self.threshold_met += sum(prev_round_opponent_actions) >= self.game_description.m - 1
        if self.threshold_met < state.round_number / 2:
            return Action.D
        if self.prev_payoff is not None and self.prev_payoff >= self.game_description.k and history.my_actions[-1]:
            return Action.C
        else:
            return Action.D
        self.prev_payoff = np.mean(history.opponent_payoffs[-1, :])
        if history.my_actions[-1]:
            self.coop_count += 1
        if state.round_number == self.game_description.n_rounds - 1:
            opponent_defections = sum(history.opponent_actions[:, :])
            return Action.D if opponent_defections > self.game_description.n_players * state.round_number / 2 else Action.C

class Strategy_EXPLOITATIVE_451(BaseStrategy):
    """
    Adaptive Collective Risk Exploiter (ACRE) strategy.
    
    This strategy is designed to exploit opponents' behaviors while adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.avg_payoff_coop = 0.0
        self.avg_payoff_defect = 0.0

    def update_avg_payoff(self, avg_payoff, t, action, payoff):
        if action == Action.C:
            return (avg_payoff * (t - 1) + payoff) / t
        else:
            return (avg_payoff * (t - 1) + payoff) / t

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.total_cooperators = 0
            self.avg_payoff_coop = 0.0
            self.avg_payoff_defect = 0.0
            return Action.C
        total_rounds = state.round_number - 1
        opponents_last_actions = history.opponent_actions[-1, :]
        my_last_action = history.my_actions[-1]
        last_payoff = history.my_payoffs[-1]
        self.total_cooperators += 1 if my_last_action else 0
        avg_payoff_coop_count = sum((1 for action in history.opponent_actions[-1, :] if action))
        avg_payoff_defect_count = len(history.opponent_actions[-1, :]) - avg_payoff_coop_count
        self.avg_payoff_coop = self.update_avg_payoff(self.avg_payoff_coop, total_rounds + 1, Action.C, last_payoff if my_last_action else 0)
        self.avg_payoff_defect = self.update_avg_payoff(self.avg_payoff_defect, total_rounds + 1, Action.D, last_payoff if not my_last_action else 0)
        if state.round_number <= math.floor(self.game_description.n_rounds / 3):
            if self.total_cooperators < self.game_description.m or self.avg_payoff_coop <= self.avg_payoff_defect:
                return Action.D
            else:
                return Action.C
        elif state.round_number <= math.floor(2 * self.game_description.n_rounds / 3):
            if self.total_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        elif self.total_cooperators >= self.game_description.m * (total_rounds + 1):
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_452(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by adaptively responding 
    to the opponent's behavior while exploiting their tendencies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        cr = np.mean(history.opponent_actions)
        if history.my_payoffs[-2] < history.opponent_payoffs[-2, 0]:
            self.threshold += 0.1
        elif history.my_payoffs[-2] > history.opponent_payoffs[-2, 0]:
            self.threshold -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cr >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_453(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior while minimizing the risk of being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_defected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        M = sum(opponent_actions)
        N = self.game_description.n_players - 1 - M
        CR = M / (M + N) if M + N > 0 else 0
        if CR >= 0.5:
            p_cooperate = 1 - (self.game_description.m / self.game_description.n_players) ** 2
            return Action.C if random.random() < p_cooperate else Action.D
        else:
            return Action.D
        if self.opponent_defected and M >= self.game_description.m:
            self.opponent_defected = False
            return Action.D
        elif not history.my_payoffs[-1] > 0:
            self.opponent_defected = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_454(BaseStrategy):
    """
    An adaptive strategy that balances cooperation and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.coop_threshold = (game_description.m + 1) / game_description.n_players
        self.punished_opponents = set()
        self.previous_round_history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        opponent_cooperations = history.opponent_actions[:-1].sum(axis=0)
        self.avg_coop = opponent_cooperations.mean()
        if history.my_actions[-2] and (not history.opponent_actions[-2, 0]):
            return Action.D
        if self.avg_coop <= self.game_description.m:
            return Action.D
        else:
            opp_coop_rate = opponent_cooperations.mean()
            if opp_coop_rate >= self.coop_threshold:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_455(BaseStrategy):
    """
    Exploitative strategy: adapt to opponents' cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        self.prev_cooperators = sum(prev_round_actions)
        if self.prev_cooperators >= self.game_description.m:
            p = self.game_description.k / (self.game_description.k + 1) if self.game_description.k > 0 else 0.5
        else:
            p = 0
        action = Action.C if random.random() < p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_456(BaseStrategy):
    """
    A strategy that balances exploration and exploitation to maximize payoffs 
    while adapting to various opponent behaviors in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_actions = [[] for _ in range(game_description.n_players - 1)]
        self.opponent_classifications = [None] * (game_description.n_players - 1)
        self.round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_number = state.round_number
        if history is not None:
            for i in range(self.game_description.n_players - 1):
                self.opponent_actions[i].append(history.opponent_actions[-1, i])
        if self.round_number == 0:
            return Action.D
        elif self.round_number <= 5:
            return Action.C if self.round_number % 2 == 1 else Action.D
        else:
            for i in range(self.game_description.n_players - 1):
                if len(self.opponent_actions[i]) >= 5:
                    cooperation_rate = sum(self.opponent_actions[i][-5:]) / 5.0
                    if cooperation_rate > 0.7:
                        self.opponent_classifications[i] = 'A'
                    elif cooperation_rate > 0.3 and cooperation_rate < 0.7:
                        self.opponent_classifications[i] = 'C'
                    else:
                        self.opponent_classifications[i] = 'D'
            for i in range(self.game_description.n_players - 1):
                if self.opponent_classifications[i] == 'D':
                    return Action.D
                elif self.opponent_classifications[i] == 'A' or self.opponent_classifications[i] == 'C':
                    expected_cooperators = sum((1 for j in range(self.game_description.n_players - 1) if self.opponent_classifications[j] in ['A', 'C']))
                    return Action.C if expected_cooperators < self.game_description.m else Action.D
            return Action.D

class Strategy_EXPLOITATIVE_457(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy is designed to exploit opponents' cooperative behaviors 
    while minimizing the risk of contributing to a failed collective effort.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = []
        self.defectors = []
        self.round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        if self.round_number <= 4 and self.round_number > 0:
            action = Action.C if (self.round_number - 1) % 2 == 0 else Action.D
        elif len(self.defectors) > 0 or num_cooperators < self.game_description.m:
            action = Action.D
        elif self.game_description.n_rounds - state.round_number <= 1 and num_cooperators >= self.game_description.m:
            action = Action.C
        else:
            action = Action.C
        self.cooperators = [i for i, opponent_action in enumerate(history.opponent_actions[-1, :]) if opponent_action and i not in self.defectors]
        self.defectors = [i for i, opponent_action in enumerate(history.opponent_actions[-1, :]) if not opponent_action and i not in self.cooperators]
        return action

class Strategy_EXPLOITATIVE_458(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        prev_round_coops = sum(history.opponent_actions[-1, :] != Action.D)
        if prev_round_coops < self.game_description.m:
            return Action.D
        p = max(0.5, min(1, self.game_description.k * (self.game_description.m - 1) / (self.game_description.n_players - 1)))
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_459(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy initially cooperates, 
    then adapts its threshold based on past cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.T = game_description.m + 1
        self.M_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperations = history.opponent_actions[-1, :]
        M_prev = np.sum(opponent_cooperations) + (history.my_actions[-1] == Action.C)
        self.M_prev = M_prev
        T = max(self.game_description.m, self.M_prev + 1)
        if M_prev >= T:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_460(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy aims to exploit the collective risk dilemma by adapting to the opponents' behavior and maximizing personal payoffs while minimizing the risk of threshold failure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        if n == 2:
            return Action.D if state.round_number >= self.game_description.n_rounds - 1 else Action.C
        elif m == 1:
            return Action.D
        if state.round_number == 0:
            return Action.C if random.random() < m / n else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        threshold_met = sum(opponent_actions) + (history.my_actions[-1] == 1) >= m
        if threshold_met:
            return Action.D if random.random() < 1 - (k - 1) / (n - m) else Action.C
        return Action.C if random.random() < m / n else Action.D

class Strategy_EXPLOITATIVE_461(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to 
    opponent behaviors by observing past actions and adjusting its behavior accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_number = 0
        self.avg_coop = None
        self.coop_freq = np.zeros(game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 5 or self.round_number == 0:
            return Action.C
        self.round_number = state.round_number
        opponent_actions = history.opponent_actions
        my_actions = np.array([action == Action.C for action in history.my_actions])
        if len(opponent_actions) > 0:
            self.avg_coop = np.mean(np.sum(opponent_actions, axis=1))
            for i in range(self.game_description.n_players - 1):
                self.coop_freq[i] = np.mean(opponent_actions[:, i])
        exploit_threshold = 0.4
        if self.avg_coop is not None and self.avg_coop >= self.game_description.m:
            return Action.D
        elif len(my_actions) > 0 and np.mean(my_actions) < exploit_threshold:
            return Action.C
        elif len(opponent_actions) > 0:
            last_round_cooperators = np.sum(opponent_actions[-1, :])
            for i in range(self.game_description.n_players - 1):
                if last_round_cooperators < self.game_description.m and self.coop_freq[i] < exploit_threshold:
                    return Action.C
        if state.round_number == self.game_description.n_rounds:
            if self.avg_coop is not None and self.avg_coop >= self.game_description.m:
                return Action.D
            else:
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_462(BaseStrategy):
    """
    Exploit opponent's cooperativeness in a collective risk dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = None

    def calculate_opponent_cooperation_rate(self, history: PlayerHistory) -> float:
        opponent_actions = np.concatenate(history.opponent_actions)
        return np.mean(opponent_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        if state.round_number == 0:
            self.cooperate_prob = m / n
            return Action.C if random.random() < self.cooperate_prob else Action.D
        prev_round_outcome = np.sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if prev_round_outcome >= m:
            self.cooperate_prob += 0.1
            self.cooperate_prob = min(self.cooperate_prob, 1)
        else:
            self.cooperate_prob -= 0.2
            self.cooperate_prob = max(self.cooperate_prob, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_round_outcome == m - 1:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_coop_rate = self.calculate_opponent_cooperation_rate(history)
        if opponent_coop_rate > 0.7 and state.round_number < self.game_description.n_rounds - 2:
            return Action.D
        return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_463(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' cooperation rates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = game_description.m / game_description.n_players * (1 + game_description.k / 2)
        self.epsilon = 0.01
        self.opponent_coop_rate_estimate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            return Action.C
        if history is not None and self.opponent_coop_rate_estimate is None:
            opponent_actions = history.opponent_actions[:state.round_number - 1, :]
            coop_rates = np.mean(opponent_actions, axis=0)
            self.opponent_coop_rate_estimate = np.mean(coop_rates)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_coop_rate = self.opponent_coop_rate_estimate or 0
        p = min(1, 1 - self.theta + self.epsilon)
        if opponent_coop_rate < self.theta:
            return Action.D
        else:
            return Action.C if random.random() < p else Action.D
        if history is not None and np.all(history.opponent_actions[-1, :] == True):
            self.update_opponent_coop_rate_estimate(history)

    def update_opponent_coop_rate_estimate(self, history: PlayerHistory):
        opponent_actions = history.opponent_actions[:history.my_actions.size - 1, :]
        coop_rates = np.mean(opponent_actions, axis=0)
        self.opponent_coop_rate_estimate = np.mean(coop_rates)

class Strategy_EXPLOITATIVE_464(BaseStrategy):
    """
    An exploitative strategy that adapts to various opponent behaviors while maximizing payoffs by balancing cooperation and defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_index = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        avg_payoff_C = np.mean(np.where(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :], 0)) if prev_cooperators > 0 else 0
        avg_payoff_D = np.mean(np.where(~history.opponent_actions[-1, :], history.opponent_payoffs[-1, :], 0)) if self.game_description.n_players - prev_cooperators > 0 else 0
        self.cooperation_index += 1 if prev_cooperators >= self.game_description.m else -1
        cooperate_probability = min(1, (self.cooperation_index + 1) / self.game_description.n_players)
        if random.random() < cooperate_probability:
            return Action.C
        if prev_cooperators < math.ceil(self.game_description.m / 2) or avg_payoff_D >= avg_payoff_C:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and prev_cooperators + 1 >= self.game_description.m:
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_465(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met_prev_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :] == True) + int(history.my_actions[-1])
        if num_cooperators_prev_round == self.game_description.m - 1:
            return Action.C if random.random() < 0.5 else Action.D
        prev_threshold_met = num_cooperators_prev_round >= self.game_description.m
        self.threshold_met_prev_round = prev_threshold_met
        if prev_threshold_met:
            defect_prob = 1 - (self.game_description.k - 1) / self.game_description.k
            return Action.D if random.random() < defect_prob else Action.C
        else:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_467(BaseStrategy):
    """
    Adaptive Exploitation (AE) strategy for Collective Risk Dilemma.
    
    AE adapts to the success or failure of collective cooperation, shifting between cooperation and exploitation phases.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.current_state = 'Cooperative'
        self.exploitation_probability = 0.5
        self.return_probability = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.current_state == 'Cooperative':
            action = Action.C
            if self.assess_success(history):
                self.current_state = 'Exploitation'
        elif self.current_state == 'Exploitation':
            if random.random() < self.exploitation_probability:
                action = Action.D
            else:
                action = Action.C
            if not self.assess_success(history):
                self.current_state = 'Punishment'
        elif self.current_state == 'Punishment':
            action = Action.C
            if self.assess_success(history) and random.random() < self.return_probability:
                self.current_state = 'Exploitation'
        if state.round_number == self.game_description.n_rounds - 1:
            cooperators_needed = max(0, self.game_description.m - sum(history.opponent_actions[-1, :]))
            if cooperators_needed > 1:
                action = Action.D
        return action

    def assess_success(self, history: PlayerHistory) -> bool:
        return sum([action == Action.C for action in [Action.C] + list(history.opponent_actions[-1, :])]) >= self.game_description.m

class Strategy_EXPLOITATIVE_468(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Tracker"
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            cooperate_prob = m / n
            return Action.C if random.random() < cooperate_prob else Action.D
        prev_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        self.round_cooperators.append(prev_coops)
        if len(self.round_cooperators) > 5:
            self.round_cooperators.pop(0)
        avg_coops = sum(self.round_cooperators) / len(self.round_cooperators)
        cooperate_prob = m / n
        opponent_actions = history.opponent_actions[-1, :]
        for i in range(n - 1):
            if opponent_actions[i] == False:
                cooperate_prob *= (n - m) / n
        if prev_coops < m:
            cooperate_prob = 0
        else:
            cooperate_prob = k / (k + 1)
        if avg_coops > m:
            cooperate_prob += 0.05
        elif avg_coops < m:
            cooperate_prob -= 0.05
        if state.round_number == r - 1:
            return Action.D
        if state.round_number < 5 and random.random() < 0.1:
            cooperate_prob += 0.05
        cooperate_prob = max(0, min(cooperate_prob, 1))
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_469(BaseStrategy):
    """
    Exploitative strategy that adapts to the number of cooperators in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_cooperators < self.threshold:
            self.threshold += 1
            return Action.D
        total_payoff_C = np.sum(history.my_payoffs) + (self.game_description.k - 1)
        expected_payoff_D = np.mean(np.max(history.opponent_payoffs, axis=1)) + self.game_description.k
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if total_payoff_C > expected_payoff_D:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_470(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Tit-for-Tat with Threshold Adjustment (ATTA)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.k = game_description.k
        self.m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        threshold = cooperators / self.m
        probability_defect = (1 - threshold) ** 2
        if state.round_number < self.game_description.n_rounds - 1:
            if threshold >= 0.5:
                return Action.C
            elif self.k > 1.5 and random.random() < probability_defect:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_471(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior and exploits cooperative majorities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = []
        self.free_riders = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        if history is not None:
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if opponent_action and i not in self.cooperators:
                    self.cooperators.append(i)
                elif not opponent_action and i not in self.free_riders:
                    self.free_riders.append(i)
        threshold_met = sum(self.cooperators) >= self.game_description.m
        prioritize_cooperation = self.game_description.k > 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif threshold_met and (not prioritize_cooperation):
            return Action.D
        elif any((i in self.free_riders for i, _ in enumerate(history.opponent_actions[-1, :]))):
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_472(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior and adjusts its Cooperation Threshold accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0
        self.defecting_opponents = set()
        self.prev_cooperation_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions[-1]
        cooperation_count = sum(opponent_actions)
        self.cooperation_threshold = max(self.game_description.m - 1 + state.round_number / self.game_description.n_rounds, 0)
        if state.round_number < self.game_description.n_rounds:
            if cooperation_count >= self.cooperation_threshold:
                action = Action.C
            else:
                action = Action.D
            if cooperation_count < self.game_description.m and self.cooperation_threshold > 0:
                self.cooperation_threshold -= 1
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_473(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponents' behavior while ensuring a high payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators = []
        self.my_defection_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        elif 2 <= state.round_number <= self.game_description.n_rounds // 2:
            cooperators = sum(history.opponent_actions[-1, :])
            self.cooperators.append(cooperators)
            if cooperators < self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif state.round_number > self.game_description.n_rounds // 2:
            avg_cooperators = sum(self.cooperators[-2:]) / 2
            if avg_cooperators >= self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators >= self.game_description.m:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_474(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by observing their cooperation rate and calculating a dynamic threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr = 0.0
        self.dt = 0.0
        self.num_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if state.round_number < self.game_description.n_rounds // 2:
            opponent_actions = history.opponent_actions[-1, :]
            self.acr = (self.acr * (state.round_number - 1) + sum(opponent_actions)) / state.round_number
            return Action.C
        if self.acr < self.game_description.m / self.game_description.n_players:
            self.dt = 0.0
        else:
            self.dt = (self.game_description.m - self.acr * self.game_description.n_players) / (self.game_description.n_players - self.acr * self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-2, :]
        self.num_cooperators_prev_round = sum(opponent_actions)
        if self.num_cooperators_prev_round >= self.dt:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_475(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponents' behavior while maximizing personal payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.ocr_history = []
        self.tau = game_description.m

    def calculate_ocr(self, w):
        if len(self.ocr_history) < w:
            return sum(self.ocr_history) / len(self.ocr_history)
        else:
            return sum(self.ocr_history[-w:]) / w

    def prob_coop(self, tau, n):
        ocr = self.calculate_ocr(min(len(self.ocr_history), 5))
        return max(0, (tau - 1) * ocr + 1 - ocr)

    def prob_defect(self, tau, n):
        return 1 - self.prob_coop(tau, n)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            self.ocr_history.append(1)
            return Action.C
        if history is not None:
            opponent_coop_rate = sum(history.opponent_actions[-1, :]) / len(history.opponent_actions[-1, :])
            self.ocr_history.append(opponent_coop_rate)
            tau = self.game_description.m - (self.game_description.m - 1) * self.calculate_ocr(min(len(self.ocr_history), 5))
            e_pi_c = self.game_description.k * self.prob_coop(tau, self.game_description.n_players) + (1 - 0)
            e_pi_d = 1 + self.game_description.k * self.prob_defect(tau, self.game_description.n_players)
            if e_pi_c > e_pi_d:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_476(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.tev = (game_description.k + 1) / 2
        self.avg_payoff_window = []

    def calculate_avg_payoff(self, num_cooperators, k):
        if num_cooperators == 0:
            return 0
        total_payoff = (num_cooperators - 1) * (k + 1)
        return total_payoff / num_cooperators

    def count_cooperators(self, actions):
        return sum(actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        num_cooperators_prev_round = self.count_cooperators(prev_round_actions)
        if state.round_number < self.game_description.n_rounds // 2:
            if num_cooperators_prev_round < self.game_description.m:
                return Action.D
            avg_payoff_per_cooperator_prev_round = self.calculate_avg_payoff(num_cooperators_prev_round, self.game_description.k)
            if avg_payoff_per_cooperator_prev_round > self.tev:
                return Action.C
            else:
                return Action.D
        elif state.round_number >= self.game_description.n_rounds // 2 and state.round_number < self.game_description.n_rounds - 3:
            if num_cooperators_prev_round < self.game_description.m:
                return Action.D
            avg_payoff_per_cooperator_prev_round = self.calculate_avg_payoff(num_cooperators_prev_round, self.game_description.k)
            self.avg_payoff_window.append(avg_payoff_per_cooperator_prev_round)
            if len(self.avg_payoff_window) >= 5:
                avg_payoff_over_window = sum(self.avg_payoff_window[-5:]) / len(self.avg_payoff_window[-5:])
                if avg_payoff_over_window > self.tev:
                    return Action.C
                else:
                    return Action.D
        elif state.round_number == self.game_description.n_rounds - 3 or state.round_number == self.game_description.n_rounds - 2:
            if num_cooperators_prev_round < self.game_description.m:
                return Action.D
            avg_payoff_per_cooperator_prev_round = self.calculate_avg_payoff(num_cooperators_prev_round, self.game_description.k)
            total_payoffs = sum(history.my_payoffs)
            if total_payoffs + avg_payoff_per_cooperator_prev_round * 2 > self.tev * 3:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_477(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        elif state.round_number <= self.game_description.n_rounds // 2:
            previous_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            if previous_cooperators < self.game_description.m:
                return Action.D
            else:
                p = (self.game_description.k - 1) / self.game_description.k
                return random.choices([Action.C, Action.D], weights=[p, 1 - p])[0]
        elif state.round_number <= self.game_description.n_rounds:
            my_payoff = sum(history.my_payoffs)
            average_payoff = np.mean(np.sum(history.opponent_payoffs, axis=0))
            if my_payoff < average_payoff:
                return Action.D
            else:
                p = (self.game_description.k - 1) / self.game_description.k
                std_dev = np.std(np.sum(history.opponent_payoffs, axis=0))
                if my_payoff > average_payoff + 2 * std_dev:
                    p = 0.5
                return random.choices([Action.C, Action.D], weights=[p, 1 - p])[0]
        else:
            return Action.D

class Strategy_EXPLOITATIVE_478(BaseStrategy):
    """
    A strategy that aims to exploit opponents' behaviors by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.previous_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.previous_cooperators is None:
            previous_cooperators = sum(history.opponent_actions[0, :])
        else:
            previous_cooperators = self.previous_cooperators
        if previous_cooperators < self.game_description.m:
            self.threshold -= 1 / (self.game_description.n_rounds - state.round_number + 1)
            self.threshold = max(0, self.threshold)
        else:
            self.threshold += 1 / (self.game_description.n_rounds - state.round_number + 1)
            self.threshold = min(1, self.threshold)
        if previous_cooperators < self.game_description.m:
            p_defect = 0.5 + (self.game_description.m - previous_cooperators) / (self.game_description.n_players - self.game_description.m)
            return Action.D if random.random() < p_defect else Action.C
        else:
            p_coop = 0.5 + (previous_cooperators - self.game_description.m) / (self.game_description.n_players - self.game_description.m)
            self.previous_cooperators = previous_cooperators
            return Action.C if random.random() < p_coop else Action.D
        return Action.C if random.random() < 0.5 else Action.D

class Strategy_EXPLOITATIVE_479(BaseStrategy):
    """
    Exploitative strategy that adapts to the history of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.epsilon = 0.0001

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        avg_payoff_c = np.mean([p for p, a in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if a] or [0])
        avg_payoff_d = np.mean([p for p, a in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if not a] or [history.my_payoffs[-1]])
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif avg_payoff_c > avg_payoff_d + self.epsilon:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_cooperators >= self.game_description.m:
                return Action.C

class Strategy_EXPLOITATIVE_480(BaseStrategy):
    """
    An adaptive strategy for the Collective Risk Dilemma game that balances cooperation and defection based on opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_tolerance = 0.5
        self.SR_c_window = []
        self.defect_threshold = 0.4
        self.recover_threshold = 0.8
        self.window_size = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if previous_cooperators >= self.game_description.m:
            action = Action.C
        else:
            SR_c = len([x for x in self.SR_c_window if x == 1]) / len(self.SR_c_window) if self.SR_c_window else 0
            if SR_c < self.defect_threshold:
                self.cooperation_tolerance -= 0.1
            elif SR_c > self.recover_threshold:
                self.cooperation_tolerance += 0.05
            if random.random() < self.cooperation_tolerance:
                action = Action.C
            else:
                action = Action.D
        if len(self.SR_c_window) >= self.window_size:
            self.SR_c_window.pop(0)
        self.SR_c_window.append(int(action == Action.C))
        if state.round_number > self.game_description.n_rounds - 3:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_481(BaseStrategy):
    """
    A exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m / self.game_description.n_players <= 0.5 else Action.D
        prev_round = history.opponent_actions[-1, :]
        threshold_met = sum(prev_round) >= self.game_description.m
        avg_defector_payoff = np.mean(history.opponent_payoffs[-1, ~prev_round])
        if threshold_met:
            return Action.C if history.my_payoffs[-1] <= avg_defector_payoff else Action.D
        else:
            num_cooperators = sum(prev_round)
            return Action.C if num_cooperators <= self.game_description.m - 1 and history.my_payoffs[-1] < 1 else Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.D

class Strategy_EXPLOITATIVE_482(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy balances cooperation and exploitation, 
    adapting to the game history while prioritizing its own payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        threshold_met = sum(history.opponent_actions[-1, :]) >= self.game_description.m
        if state.round_number <= 5:
            if threshold_met:
                return random.choice([Action.C] * 7 + [Action.D])
            else:
                return random.choice([Action.C] + [Action.D] * 8)
        freq_cooperation = np.mean(history.opponent_actions[:, :])
        if state.round_number <= self.game_description.n_rounds // 2:
            if freq_cooperation > 0.5:
                return random.choice([Action.C] * 4 + [Action.D])
            else:
                return random.choice([Action.C] + [Action.D] * 9)
        payoff_difference = np.sum(history.my_payoffs[history.my_actions == True]) - np.sum(history.my_payoffs[history.my_actions == False])
        if state.round_number < self.game_description.n_rounds:
            if payoff_difference > 0:
                return random.choice([Action.C] * 9 + [Action.D])
            else:
                return random.choice([Action.C] + [Action.D] * 19)
        return Action.D if state.round_number == self.game_description.n_rounds else random.choice([Action.C] * 3 + [Action.D] * 2)

class Strategy_EXPLOITATIVE_483(BaseStrategy):
    """
    Adaptive Threshold Hunter strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.0
        self.exploit_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= 3:
            if state.round_number == 1:
                return Action.C
            else:
                return Action.D
        elif self.cooperation_rate < self.game_description.m / self.game_description.n_players:
            action = Action.D
        else:
            p = (self.game_description.m - 1) / self.game_description.n_players
            if random.random() < p:
                action = Action.C
            else:
                action = Action.D
        if self.exploit_flag and self.cooperation_rate >= self.game_description.m / self.game_description.n_players:
            action = Action.D
        opponent_actions = history.opponent_actions[-1, :]
        n_cooperators = sum(opponent_actions)
        self.cooperation_rate += (n_cooperators - sum(history.opponent_actions[-2, :])) / self.game_description.n_players
        if state.round_number > 0 and np.mean(history.my_payoffs) > np.mean(np.mean(history.opponent_payoffs, axis=1)) * self.game_description.k:
            self.exploit_flag = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif math.isclose(self.cooperation_rate, self.game_description.m / self.game_description.n_players):
            if random.random() < 0.5:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_484(BaseStrategy):
    """
    The ATE strategy exploits the Collective Risk Dilemma by adapting to opponents' behavior and maximizing its own payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def calculate_cooperation_rate(self, opponent_actions: NDArray[np.bool_]) -> float:
        cooperators = sum(opponent_actions)
        return cooperators / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.n_players == 2:
                return Action.C
            else:
                return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        cooperation_rate = self.calculate_cooperation_rate(opponent_actions)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperation_rate >= self.game_description.m / self.game_description.n_players:
            return Action.C
        else:
            p = (1 - self.game_description.k * (self.game_description.m / self.game_description.n_players)) / (1 + self.game_description.k)
            if random.random() < p:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_485(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' cooperation rates and adjusts its behavior accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.w = 5
        self.p = game_description.m / game_description.n_players
        self.x = 10

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if random.random() < self.p else Action.D
        opponent_actions = history.opponent_actions[-min(self.w, state.round_number):]
        cr = np.mean(opponent_actions)
        if cr < self.game_description.m / self.game_description.n_players:
            return Action.D
        if (state.round_number + 1) % self.x == 0:
            self._adjust()
        opponent_payoffs = history.opponent_payoffs[-min(self.w, state.round_number):]
        for i in range(self.game_description.n_players - 1):
            if opponent_actions[:, i].sum() >= min(self.w, state.round_number):
                return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D
        return Action.C if cr >= self.game_description.m / self.game_description.n_players else Action.D

    def _adjust(self):
        self.p = max(0, min(self.p * 1.1, 1))
        self.w = max(5, min(self.w + 1, self.x // 2))

class Strategy_EXPLOITATIVE_486(BaseStrategy):
    """
    A strategy that adapts to opponents' behavior and exploits their cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coop = sum((1 for action in history.opponent_actions[-1, :] if action))
        CR = prev_round_coop / (self.game_description.n_players - 1)
        if CR >= self.game_description.m / self.game_description.n_players:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_487(BaseStrategy):
    """
    Exploitative strategy that aims to maximize payoffs by adaptively responding 
    to opponents' behavior while minimizing cooperation when possible.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_round_coops >= self.game_description.m:
            return Action.D if random.random() < 0.7 else Action.C
        elif history.my_actions[-1]:
            return Action.C if random.random() < 0.5 else Action.D
        else:
            return Action.D if random.random() < 0.8 else Action.C

class Strategy_EXPLOITATIVE_488(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_number = 0
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_number = state.round_number
        if state.round_number < self.game_description.n_players:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        prev_my_payoff = history.my_payoffs[-1]
        cooperators_last_round = sum(prev_opponent_actions) + (history.my_actions[-1] == 1)
        if cooperators_last_round < self.game_description.m:
            p_coop = 0.5
        else:
            p_coop = 1 - cooperators_last_round / self.game_description.n_players
        if history.my_actions[-1] and prev_my_payoff == self.game_description.k:
            return Action.D
        if state.round_number % (self.game_description.n_players // 2) == 0:
            recent_cooperation_rate = sum(history.my_actions[-(self.game_description.n_players // 2):]) / (self.game_description.n_players // 2)
            if recent_cooperation_rate > 0.5:
                self.p_coop += 0.1
            elif recent_cooperation_rate < 0.3:
                self.p_coop -= 0.1
        return Action.C if random.random() < self.p_coop else Action.D

class Strategy_EXPLOITATIVE_489(BaseStrategy):
    """
    The ATE strategy aims to exploit opponents' behaviors while adapting to the game's progress and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.adaptive_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C if random.random() < 0.5 else Action.D
        num_cooperators_prev = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        self.adaptive_threshold = self.game_description.m - (self.game_description.n_players - num_cooperators_prev) / self.game_description.n_players
        if self.adaptive_threshold < 0:
            self.adaptive_threshold = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if num_cooperators_prev <= self.adaptive_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_490(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    Adapts to opponent behavior by updating an estimated threshold based on observed actions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.thresh = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        m_prev = sum(history.opponent_actions[-1, :] == True)
        coop_prob = (self.game_description.m - m_prev) / self.game_description.n_players if m_prev <= self.game_description.m else 0
        if random.random() < coop_prob:
            action = Action.C
        elif self.thresh_update(m_prev):
            action = Action.D if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.C
        else:
            action = Action.D
        return action

    def thresh_update(self, m_prev: int) -> bool:
        """
        Updates the estimated threshold based on observed actions.
        Returns True if the estimated threshold is met and we have a high confidence that our cooperation will lead to a reward.
        """
        if m_prev < self.thresh:
            self.thresh -= 1
        elif m_prev >= self.thresh:
            self.thresh += 1
        return self.thresh == self.game_description.m and self.game_description.k * (self.game_description.n_players - 1) > self.game_description.n_players - self.thresh

class Strategy_EXPLOITATIVE_491(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponents' cooperation levels.
    
    In the initial rounds, it cooperates to gather information. Then, it tracks the average number of cooperators and adjusts its threshold accordingly.
    If the number of cooperators is below the threshold, it defects; otherwise, it cooperates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.threshold = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            num_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            avg_rounds = min(self.game_description.n_rounds // 2, 5)
            self.coop_count = (self.coop_count * (state.round_number - 1) + num_cooperators) / state.round_number
            if self.coop_count >= self.game_description.m:
                self.threshold = self.game_description.m - 1
            else:
                self.threshold = max(self.game_description.m - 2, 0)
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            if num_cooperators <= self.threshold:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_492(BaseStrategy):
    """
    Exploit opponents' behavior by adapting to their cooperation or defection tendencies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_last_round = 0
        self.k_effective = 0
        self.opponent_cooperation_rate = 0.5

    def estimate_opponent_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Estimate the opponent cooperation rate over the last few rounds.
        
        :param history: The game history.
        :return: The estimated opponent cooperation rate.
        """
        recent_rounds = min(5, len(history.opponent_actions))
        total_cooperations = sum((sum(round_) for round_ in history.opponent_actions[-recent_rounds:]))
        total_players = self.game_description.n_players - 1
        return total_cooperations / (total_players * recent_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        self.m_last_round = sum(history.opponent_actions[-1, :])
        self.k_effective = self.game_description.k if self.m_last_round >= self.game_description.m else 0
        self.opponent_cooperation_rate = self.estimate_opponent_cooperation_rate(history)
        if state.round_number < self.game_description.n_rounds:
            if self.m_last_round < self.game_description.m and self.k_effective > 1 or self.opponent_cooperation_rate < 0.5:
                return Action.C
            elif self.m_last_round >= self.game_description.m and self.k_effective > 1 or self.opponent_cooperation_rate > 0.5:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_493(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation rates in a Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m_min_cooperators = game_description.m
        self.k_reward_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.m_min_cooperators / self.n_players else Action.D
        prev_round_cooperations = sum((1 for opponent_action in history.opponent_actions[-1, :] if opponent_action))
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        elif prev_round_cooperations >= self.m_min_cooperators:
            return Action.C if random.random() < self.k_reward_factor / (self.k_reward_factor + 1) else Action.D
        else:
            return Action.D if random.random() < (self.n_players - self.m_min_cooperators) / self.n_players else Action.C

class Strategy_EXPLOITATIVE_494(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Cooperate-to-Defect" (ACD)

    This strategy aims to exploit opponents by adapting to their behavior and adjusting cooperation levels accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.delta = 0.1
        self.delta_p_d = 0.2
        self.cooperation_prob = 0.5 + (self.k - 1) / (2 * self.k)
        self.punishment_prob = 1 - self.m / self.n_players
        self.opponent_defection_count = [0] * self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round = history.opponent_actions[-1, :]
        cooperators = sum(previous_round)
        if cooperators >= self.m:
            self.cooperation_prob = 0.5 + (self.k - 1) / (2 * self.k)
        else:
            self.punishment_prob = 1 - cooperators / self.n_players
        m_prime = self.m + self.delta * (cooperators - self.m)
        if m_prime >= cooperators:
            self.cooperation_prob = 0.5 + (self.k - 1) / (2 * self.k)
        for opponent_id, opponent_action in enumerate(previous_round):
            if not opponent_action and cooperators >= self.m:
                self.opponent_defection_count[opponent_id] += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        random_value = random.random()
        if random_value < self.cooperation_prob:
            return Action.C
        elif random_value < self.punishment_prob + sum(self.opponent_defection_count) / len(self.opponent_defection_count):
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_495(BaseStrategy):
    """
    Adaptive Threshold Seeker strategy for Collective Risk Dilemma.

    This strategy aims to balance cooperation and defection based on the game's history.
    By cooperating initially, we encourage others to do the same. We then adapt our 
    strategy based on the opponents' behavior: if there are too few cooperators, defecting 
    minimizes losses; if there are enough cooperators, cooperating maximizes gains. 
    Marking unreliable players and responding with defection when they exploit us helps 
    maintain cooperation. In the endgame, we try to secure a favorable outcome by 
    cooperating if there are already enough cooperators.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.unreliable_players = set()
        self.cooperate_count_history = [0]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperate_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        self.cooperate_count_history.append(cooperate_count)
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if opponent_action == False and self.cooperate_count_history[-2] == self.game_description.m - 1:
                self.unreliable_players.add(i)
            elif i in self.unreliable_players and opponent_action == False:
                return Action.D
        if cooperate_count < self.game_description.m:
            return Action.D
        elif cooperate_count >= self.game_description.m:
            return Action.C
        if state.round_number + 1 == self.game_description.n_rounds:
            if cooperate_count >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_496(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    Adapts to the evolving game dynamics by cooperating with a probability 
    that increases as the threshold is approached, and decreasing it when 
    the threshold is not met. Maximizes personal payoff in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        if opponent_cooperations >= self.game_description.m - 1:
            self.cooperation_probability = min(1, self.game_description.k / (self.game_description.k + 1))
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < self.cooperation_probability:
            action = Action.C
        else:
            action = Action.D
        if history is not None and len(history.my_payoffs) > 0:
            previous_payoff = history.my_payoffs[-1]
            if opponent_cooperations >= self.game_description.m and previous_payoff > 1:
                self.cooperation_probability += 0.1
            else:
                self.cooperation_probability -= 0.1
        return action

class Strategy_EXPLOITATIVE_497(BaseStrategy):
    """
    An exploitative strategy that balances individual gain with collective success by adapting to the dynamics of cooperation and defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.dyn_threshold = self.game_description.m / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.opponent_actions) > 0:
            last_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        else:
            last_round_cooperators = self.game_description.m - 1
        if state.round_number <= self.game_description.n_rounds // 3:
            if last_round_cooperators < self.game_description.m:
                return Action.C
            else:
                return Action.D
        elif state.round_number <= 2 * self.game_description.n_rounds // 3:
            threshold_ratio = last_round_cooperators / self.game_description.n_players
            if threshold_ratio < self.dyn_threshold:
                action = Action.C
            else:
                action = Action.D
            if last_round_cooperators > self.game_description.m:
                self.dyn_threshold += 0.1
            else:
                self.dyn_threshold -= 0.05
                self.dyn_threshold = max(self.dyn_threshold, 0)
            return action
        elif history.my_payoffs[-1] == self.game_description.k + 1 - int(history.my_actions[-1]):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_498(BaseStrategy):
    """
    An exploitative strategy that aims to maximize individual payoffs by adaptively responding 
    to the collective behavior of opponents while exploiting potential cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        else:
            prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            if prev_round_cooperators < self.game_description.m:
                action = Action.D
            elif state.round_number == self.game_description.n_rounds - 1:
                action = Action.D
            else:
                if prev_round_cooperators >= self.game_description.m:
                    self.cooperation_probability = min(1, self.cooperation_probability + 0.05)
                else:
                    self.cooperation_probability = max(0, self.cooperation_probability - 0.2)
                action = Action.C if random.random() < self.cooperation_probability else Action.D
            if history.my_actions.size >= 3 and sum(history.opponent_actions[-2:, :].sum(axis=1) + history.my_actions[-2:]) < 2 * self.game_description.m:
                action = Action.D
                self.cooperation_probability = 0.0
            return action

class Strategy_EXPLOITATIVE_499(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.threshold = game_description.m
        self.cooperators_history = []
        self.payoffs_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_coop = np.sum(history.opponent_actions[-1, :])
        self.avg_coop = (self.avg_coop * (state.round_number - 1) + num_coop) / state.round_number
        if self.avg_coop < self.game_description.m - 0.5:
            self.threshold += 1
        elif self.avg_coop > self.game_description.m - 1:
            self.threshold = max(self.game_description.m, self.avg_coop)
        previous_payoff = history.my_payoffs[-1]
        private_payoff = 1.0
        if num_coop >= self.threshold or previous_payoff > private_payoff:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_500(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploitation_mode = False

    def calculate_cooperation_rate(self, action, n_players):
        if action == Action.C:
            return 1 / n_players
        else:
            return 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate = random.random() < self.game_description.m / self.game_description.n_players
            action = Action.C if cooperate else Action.D
            return action
        opponent_actions = history.opponent_actions[-1, :]
        CR_prev = np.mean(opponent_actions) + (history.my_actions[-1] == Action.C)
        if not self.exploitation_mode and CR_prev > 0.7:
            self.exploitation_mode = True
        cooperate = False
        if CR_prev >= self.game_description.m / self.game_description.n_players:
            cooperate = True
        elif self.exploitation_mode:
            opponent_cooperators = np.sum(opponent_actions)
            cooperate = opponent_cooperators == self.game_description.m - 1
        elif state.round_number >= self.game_description.n_rounds * 0.9:
            cooperate = CR_prev >= 0.5
        action = Action.C if cooperate else Action.D
        return action

class Strategy_EXPLOITATIVE_501(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation levels.
    Cooperates initially, then adjusts based on average cooperators and game progress.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.m / 2
        self.previous_rounds_cooperators = []
        self.adaptive_adjustment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_cooperators = sum(history.my_actions[:state.round_number]) / state.round_number
        self.previous_rounds_cooperators.append(sum(history.opponent_actions[-1, :]))
        if (state.round_number - 1) % 3 == 0:
            self.adaptive_adjustment_counter += 1
            if len(self.previous_rounds_cooperators) > 1:
                previous_cooperators = sum(self.previous_rounds_cooperators[-2:]) / 2
                if previous_cooperators > self.game_description.m:
                    self.cooperation_threshold -= 1
                elif previous_cooperators < self.game_description.m / 2 and self.adaptive_adjustment_counter >= 2:
                    self.cooperation_threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_502(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Tracker" (ATT)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
        if self.m_prev >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        if prev_round_cooperators >= self.game_description.m:
            if not all(history.opponent_actions[-1, :]):
                self.m_prev += 1
        elif all(history.opponent_actions[-1, :]):
            self.m_prev -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_503(BaseStrategy):
    """
    Exploitative strategy that balances cooperation and exploitation.
    Adapts to the evolving game dynamics while minimizing losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            if state.round_number == 1:
                return Action.C
            else:
                return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_cooperators >= self.m - 1:
            return Action.C
        if history.my_payoffs[-1] < 1 + self.k:
            return Action.D
        coop_prob = 0.5
        if self.k > 2:
            coop_prob = 0.7
        elif self.k < 1.5:
            coop_prob = 0.3
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_EXPLOITATIVE_504(BaseStrategy):
    """
    Exploitative strategy that adaptively pursues the threshold of minimum cooperators needed (m) 
    while being robust to a wide range of opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        previous_round_actions = history.opponent_actions[-1, :]
        CR = np.mean(previous_round_actions)
        DI = 0.0
        if not all((not action for action in previous_round_actions)):
            opponent_payoffs = history.opponent_payoffs[-1, :]
            defectors = [payoff for payoff, action in zip(opponent_payoffs, previous_round_actions) if not action]
            DI = np.mean(defectors)
        theta = self.game_description.m - CR * self.game_description.n_players + DI / self.game_description.k
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if CR >= theta else Action.D

class Strategy_EXPLOITATIVE_505(BaseStrategy):
    """
    The Adaptive Threshold Tracker (ATT) strategy is designed to exploit opponents' behavior while adapting to the game's dynamics.
    It relies solely on game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_coop = sum(history.opponent_actions[-1, :] | np.array([True]))
        if prev_coop < self.threshold:
            self.threshold -= 1
        elif self.game_description.k > 2 and prev_coop >= self.threshold:
            self.threshold += 1
        if prev_coop >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_506(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma game.

    This strategy starts by cooperating to gather information about opponents' behavior.
    It then uses an adaptive threshold mechanism to decide whether to cooperate or defect,
    based on the success rate of reaching the cooperative threshold in recent rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count_history = []
        self.thresh_success_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :] != False)
        self.coop_count_history.append(prev_coop_count)
        if state.round_number <= 5:
            if prev_coop_count >= m / 2:
                return Action.C
            else:
                return Action.D
        avg_window_size = math.ceil(r / 2)
        recent_coops = self.coop_count_history[-avg_window_size:]
        thresh_success_rate = sum((coop >= m for coop in recent_coops)) / len(recent_coops)
        self.thresh_success_rate = thresh_success_rate
        if thresh_success_rate > 0.5:
            return Action.C
        elif thresh_success_rate == 0.5:
            return Action.D
        else:
            return Action.D
        if state.round_number == r - 1:
            return Action.D

class Strategy_EXPLOITATIVE_507(BaseStrategy):
    """
    The Adaptive Exploiter is a robust and adaptive strategy designed to exploit a wide range of opponent behaviors in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CR = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return random.choice([Action.C, Action.D]) if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_actions = history.opponent_actions[:state.round_number - 1]
        cooperators = np.sum(opponent_actions, axis=0)
        CR = np.mean(cooperators > self.game_description.m - 1)
        if self.CR == 0:
            self.CR = CR
        p = self.game_description.k / (self.game_description.k + 1) if CR >= self.game_description.m / self.game_description.n_players else 1 - self.game_description.m / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds:
            if np.sum(history.my_payoffs) + (0 if CR < self.game_description.m / self.game_description.n_players else 1 - self.CR + self.game_description.k) >= np.max(np.sum(history.opponent_payoffs, axis=0)):
                return Action.D
        opponent_scores = np.sum(history.opponent_payoffs, axis=0)
        if np.any(opponent_scores == np.max(opponent_scores)) and len(np.where(opponent_scores == np.max(opponent_scores))[0]) > 1:
            return Action.C
        self.CR = CR
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_509(BaseStrategy):
    """
    An exploitative strategy for Collective Risk Dilemma, adapting to opponents' cooperation rates.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr_history = []
        self.cooperation_threshold = 0.5 * game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.n_players:
            return Action.C
        recent_rounds = history.opponent_actions[-self.game_description.n_rounds // 2:]
        acr = np.mean(np.sum(recent_rounds, axis=1) / self.game_description.n_players - 1)
        self.acr_history.append(acr)
        if acr >= self.cooperation_threshold:
            return Action.C
        return Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.n_players:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.C
        recent_rounds = history.opponent_actions[-self.game_description.n_rounds // 2:]
        acr = np.mean(np.sum(recent_rounds, axis=1) / self.game_description.n_players - 1)
        self.acr_history.append(acr)
        if acr >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_510(BaseStrategy):
    """
    An exploitative strategy that adapts to the game history by cooperating initially,
    defecting when cooperation is low, and adjusting cooperation probability based on
    the number of cooperators in previous rounds and the difference between the previous
    round's payoff and the average payoff so far.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.average_payoff_so_far = 0.0

    def calculate_payoff(self, actions: NDArray[np.bool_]) -> float:
        num_cooperators = sum((1 for action in actions if action))
        payoff = 1 - int(actions[0])
        if num_cooperators >= self.game_description.m:
            payoff += self.game_description.k
        return payoff

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_previous_round = sum((1 for action in history.opponent_actions[-1, :] if action))
        previous_round_payoff = self.calculate_payoff(np.append(history.my_actions[-1], history.opponent_actions[-1, :]))
        if state.round_number == 1:
            self.average_payoff_so_far = previous_round_payoff
        else:
            self.average_payoff_so_far = (self.average_payoff_so_far * (state.round_number - 2) + previous_round_payoff) / (state.round_number - 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if num_cooperators_previous_round < self.game_description.m:
            return Action.D
        probability_of_cooperating = min(1, self.game_description.k * (self.game_description.m - 1) / (self.game_description.n_players - 1))
        if previous_round_payoff > self.average_payoff_so_far:
            probability_of_cooperating = min(1, (previous_round_payoff - self.average_payoff_so_far) / (self.game_description.k - 1))
        return Action.C if random.random() < probability_of_cooperating else Action.D

class Strategy_EXPLOITATIVE_511(BaseStrategy):
    """
    Exploitative strategy that adaptively adjusts its cooperation level based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperators = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_cooperators = self.game_description.m
        n_players = self.game_description.n_players
        if state.round_number > 1:
            opponent_actions_last_round = history.opponent_actions[-1, :]
            my_action_last_round = bool(history.my_actions[-1])
            cooperators_last_round = sum(opponent_actions_last_round) + int(my_action_last_round)
            self.avg_cooperators = (self.avg_cooperators * (state.round_number - 2) + cooperators_last_round) / (state.round_number - 1)
        else:
            opponent_actions_last_round = history.opponent_actions[-1, :]
            my_action_last_round = bool(history.my_actions[-1])
            self.avg_cooperators = sum(opponent_actions_last_round) + int(my_action_last_round)
        if self.avg_cooperators < m_cooperators:
            p_cooperate = (m_cooperators - self.avg_cooperators) / (n_players - m_cooperators)
        else:
            p_cooperate = 0.5
        if random.random() <= p_cooperate:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_EXPLOITATIVE_512(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            num_cooperators = sum(history.opponent_actions[-1, :])
            self.coop_count += num_cooperators - self.game_description.m
        if state.round_number < self.game_description.n_rounds - 1:
            if self.coop_count >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_157(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy is designed to balance individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_coop = 0
        self.avg_payoff_defectors = 0.0
        self.consecutive_defections = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 5 and self.m_coop < self.game_description.m:
            return random.choice([Action.C, Action.D]) if random.random() < 0.6 else Action.D
        if self.m_coop < self.game_description.n_players / 2:
            cooperate_prob = 0.4
        else:
            cooperate_prob = 1.0
        if self.m_coop >= self.game_description.m:
            return Action.C
        elif self.avg_payoff_defectors > 1 + self.game_description.k / 2:
            return random.choice([Action.C, Action.D]) if random.random() < cooperate_prob else Action.D
        for player in range(self.game_description.n_players):
            if self.consecutive_defections[player] >= 2:
                return Action.D
        return random.choice([Action.C, Action.D]) if random.random() < cooperate_prob else Action.D

    def update_variables(self, history: PlayerHistory):
        self.m_coop = sum(history.opponent_actions[-1, :])
        opponent_payoffs_defectors = [payoff for action, payoff in zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :]) if not action]
        if opponent_payoffs_defectors:
            self.avg_payoff_defectors = np.mean(opponent_payoffs_defectors)
        else:
            self.avg_payoff_defectors = 0.0
        for player in range(self.game_description.n_players):
            self.consecutive_defections[player] += 1 if not history.opponent_actions[-1, player] else 0

class Strategy_COLLECTIVE_207(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy balances individual self-interest with collective cooperation to achieve a high total payoff over multiple rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_rates = [0] * game_description.n_players

    def calculate_payoff(self, round_number: int, history: PlayerHistory) -> float:
        """
        Calculate payoff for the specified round based on game parameters and history.
        :param round_number: The round number
        :param history: The player's history
        :return: Payoff value
        """
        prev_round_payoffs = history.my_payoffs[round_number - 1] if round_number > 0 else 0
        return prev_round_payoffs

    def count_cooperators(self, round_number: int, history: PlayerHistory) -> int:
        """
        Count the number of cooperators in the specified round.
        :param round_number: The round number
        :param history: The player's history
        :return: Number of cooperators
        """
        prev_round_actions = history.opponent_actions[round_number - 1] if round_number > 0 else np.zeros(self.game_description.n_players, dtype=np.bool_)
        return sum(prev_round_actions)

    def update_cooperate_rate(self, opponent_id: int, history: PlayerHistory):
        """
        Update the cooperation rate of an opponent.
        :param opponent_id: The ID of the opponent
        :param history: The player's history
        """
        prev_round_actions = history.opponent_actions[-min(5, len(history.opponent_actions)):, opponent_id]
        self.cooperate_rates[opponent_id] = np.mean(prev_round_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Make a decision based on the current game state and player history.
        :param state: The current game state
        :param history: The player's history
        :return: Decision (Action.C or Action.D)
        """
        if state.round_number == 0:
            return Action.C
        prev_round_payoff = self.calculate_payoff(state.round_number - 1, history)
        prev_cooperators = self.count_cooperators(state.round_number - 1, history)
        for i in range(self.game_description.n_players):
            if self.cooperate_rates[i] < 0.2:
                return Action.D
        for opponent_id in range(self.game_description.n_players):
            self.update_cooperate_rate(opponent_id, history)
        threshold_met = prev_cooperators >= self.game_description.m
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if threshold_met else Action.D
        elif threshold_met or (prev_cooperators >= self.game_description.m - 1 and prev_round_payoff >= self.game_description.k - 1):
            return Action.C
        elif prev_cooperators < self.game_description.m and prev_round_payoff < self.game_description.k - 1:
            return Action.C if np.any(np.logical_not(history.opponent_actions[state.round_number - 1])) else Action.D
        elif prev_cooperators == self.game_description.m:
            return Action.C if random.random() < 0.5 else Action.D
        return Action.D

class Strategy_COLLECTIVE_241(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) Strategy.

    The ACC strategy aims to balance individual self-interest with collective cooperation,
    ensuring a robust and adaptive approach that converges towards optimal outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        avg_cooperation_rate = np.mean(history.opponent_actions[-3:, :])
        if avg_cooperation_rate >= self.game_description.m / self.game_description.n_players:
            return Action.C
        if state.round_number > 3 and history.opponent_actions[state.round_number - 2, 0] == True and (history.opponent_actions[state.round_number - 1, 0] == False):
            avg_cooperation_rate = np.mean(history.opponent_actions[-2:, :])
            return Action.C if avg_cooperation_rate >= self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_374(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy.
    
    This strategy balances individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors while maintaining a robust and fair approach.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_c_prev = 0.5
        self.punish_duration = None
        self.window_size = max(1, game_description.n_rounds // 10)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        if history.my_payoffs.size > self.window_size:
            avg_c_window = np.mean(np.sum(history.opponent_actions[-self.window_size:-1, :], axis=0) / self.game_description.n_players)
        else:
            avg_c_window = 0.5
        punish_player = False
        if history.my_payoffs.size > self.window_size and np.mean(avg_c_window) < 0.2:
            punish_player = True
        if opponent_cooperations >= self.game_description.m:
            cooperate_prob = max(0.5, self.avg_c_prev)
        else:
            cooperate_prob = 1 - self.avg_c_prev / self.game_description.m
        if punish_player and self.punish_duration is not None:
            self.punish_duration -= 1
        if punish_player or (not punish_player and random.random() < cooperate_prob):
            return Action.C
        else:
            return Action.D
        self.avg_c_prev = opponent_cooperations / self.game_description.n_players

    def reset(self):
        """
        Reset the strategy to its initial state.
        
        This method is called at the beginning of each game.
        """
        self.avg_c_prev = 0.5
        self.punish_duration = None

class Strategy_COLLECTIVE_390(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective welfare by adaptively enforcing the cooperation threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_cooperators = np.mean(history.opponent_actions[:state.round_number - 1])
        current_cooperation_level = sum(history.opponent_actions[state.round_number - 2, :])
        if state.round_number < self.game_description.n_rounds / 2:
            return Action.C if avg_cooperators < self.game_description.m else Action.D
        elif self.game_description.n_rounds / 2 <= state.round_number < 3 * self.game_description.n_rounds / 4:
            if current_cooperation_level >= self.game_description.m:
                return Action.C
            else:
                p = (self.game_description.m - avg_cooperators) / (self.game_description.n_players - avg_cooperators)
                return Action.D if random.random() < p else Action.C
        elif state.round_number >= 3 * self.game_description.n_rounds / 4 and current_cooperation_level < self.game_description.m:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_408(BaseStrategy):
    """
    A strategy that balances cooperation with adaptability to changing opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.opponent_defections = [0] * game_description.n_players
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_opponent_actions = history.opponent_actions[-1]
        m_prime = sum(previous_round_opponent_actions)
        self.threshold_met = m_prime >= self.game_description.m
        for i, opponent_action in enumerate(previous_round_opponent_actions):
            if state.round_number > 1 and opponent_action and (not history.opponent_actions[-2][i]) and self.threshold_met:
                self.opponent_defections[i] += 1
        random_exploration = random.random() < 0.1
        exploration_phase = (state.round_number - 1) % 3 == 0
        if self.threshold_met or random_exploration or exploration_phase:
            action = Action.C
        elif sum(self.opponent_defections) > 0:
            action = Action.D
        else:
            action = Action.C
        my_payoff = 1 - int(action == Action.C) + self.game_description.k * self.threshold_met
        self.avg_payoff = (self.avg_payoff * state.round_number + my_payoff) / (state.round_number + 1)
        if state.round_number >= self.game_description.n_rounds - 2 and my_payoff < self.avg_payoff:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_5(BaseStrategy):
    """
    The ATE strategy aims to exploit the Collective Risk Dilemma game by adaptively adjusting its cooperation level based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = game_description.m / game_description.n_players
        self.threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        total_cooperations = opponent_cooperations + (Action.C == history.my_actions[-1])
        self.threshold_met = total_cooperations >= self.game_description.m
        if state.round_number < self.game_description.n_rounds - 1:
            if self.threshold_met:
                action = Action.D if random.random() < 1 - self.game_description.k / (self.game_description.k + 1) else Action.C
            else:
                action = Action.C if random.random() < self.cooperate_prob else Action.D
            step_size = 1 / (self.game_description.n_players * self.game_description.n_rounds)
            self.cooperate_prob += -step_size if self.threshold_met else step_size
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_9(BaseStrategy):
    """
    A strategy that adapts to opponent behavior by balancing cooperation and exploitation.
    It encourages collective success while exploiting insufficient cooperation, making it robust across different game stages and behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.first_round_cooperation_achieved = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if last_round_cooperators >= self.game_description.m:
            if not self.first_round_cooperation_achieved and last_round_cooperators >= self.game_description.m:
                self.first_round_cooperation_achieved = True
            return Action.C
        elif 0 < last_round_cooperators < self.game_description.m:
            if self.first_round_cooperation_achieved:
                probability_to_defect = (self.game_description.m - last_round_cooperators) / self.game_description.m
                if random.random() <= probability_to_defect:
                    return Action.D
            else:
                return Action.C
        elif last_round_cooperators == 0 and state.round_number > 1:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_61(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting 
    the behaviors of other players while adapting to changing conditions throughout the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.coop_expectations = [0.5] * self.n_players
        self.opp_behavior_history = [[] for _ in range(self.n_players)]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        for i, action in enumerate(history.opponent_actions[-1, :]):
            self.opp_behavior_history[i].append(action)
            coop_rate = sum(self.opp_behavior_history[i]) / len(self.opp_behavior_history[i])
            self.coop_expectations[i] = 0.9 * self.coop_expectations[i] + 0.1 * coop_rate
        if state.round_number > 1:
            for i, action in enumerate(history.opponent_actions[-2, :]):
                if action != self._get_expected_action(i):
                    return Action.D
        coop_threshold_met = sum(self.coop_expectations) >= self.m
        increasing_cooperators = len([action for action in history.opponent_actions[-1, :] if action]) >= (self.n_players - 1) // 2
        if coop_threshold_met and increasing_cooperators:
            return Action.C
        expected_payoff_defecting = 1 + self.k * sum(self.coop_expectations)
        expected_payoff_cooperating = self.k * sum(self.coop_expectations)
        if expected_payoff_cooperating > expected_payoff_defecting:
            return Action.C
        return Action.D

    def _get_expected_action(self, player_id):
        return Action.C if self.coop_expectations[player_id] >= 0.5 else Action.D

class Strategy_EXPLOITATIVE_64(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances cooperation and defection by adapting to opponent behavior,
    exploiting non-cooperative opponents, and maintaining a relatively cooperative environment.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.CR_values = None
        self.ACT = None

    def calculate_ACT(self, CR_values):
        if sum((1 for cr in CR_values if cr > 0.7)) > self.game_description.n_players / 2:
            return 0.6
        else:
            return 0.4

    def estimate_cooperators(self, CR_values, ACT):
        return sum((1 for cr in CR_values if cr >= ACT))

    def analyze_behavior(self, opponent_actions):
        return opponent_actions.mean()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            return Action.C if random.random() < 0.5 else Action.D
        self.CR_values = [self.analyze_behavior(opponent_actions) for opponent_actions in history.opponent_actions]
        self.ACT = self.calculate_ACT(self.CR_values)
        n_C = self.estimate_cooperators(self.CR_values, self.ACT)
        if n_C >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D
        for opponent_CR in self.CR_values:
            if opponent_CR < 0.3:
                return Action.D
        if n_C == self.game_description.m - 1:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1 and (n_C >= self.game_description.m or all(self.CR_values)):
            return Action.C
        return action

class Strategy_EXPLOITATIVE_108(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponents' behavior and seeks to maximize its own payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = game_description.m / game_description.n_players
        self.prev_round_outcome = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        self.prev_round_outcome = sum(opponent_actions) + 1
        if self.prev_round_outcome >= self.game_description.m:
            self.cooperate_prob = min(1, self.cooperate_prob + 0.1)
        else:
            self.cooperate_prob = max(0, self.cooperate_prob - 0.1)
        if state.round_number % (self.game_description.n_rounds // 2) == 0:
            avg_payoff = np.mean(history.my_payoffs[-(self.game_description.n_rounds // 2):])
            if avg_payoff > 1 + self.game_description.k:
                self.cooperate_prob = min(1, self.cooperate_prob + 0.2)
            else:
                self.cooperate_prob = max(0, self.cooperate_prob - 0.2)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if history.my_payoffs[-1] + self.game_description.k * int(self.prev_round_outcome >= self.game_description.m) > 1 + self.game_description.k else Action.D
        return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_144(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors while ensuring robustness.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_players <= 2 or self.game_description.m == 1:
            return Action.D
        if self.k == 1:
            return Action.C
        if state.round_number == 0 and history is None:
            return Action.C
        prev_round_opponents = history.opponent_actions[-1, :]
        cooperators_last = np.sum(prev_round_opponents) + (history.my_actions[-1] if history.my_actions[-1] else 0)
        coop_rate = np.mean(history.opponent_actions[:, :])
        if cooperators_last >= self.m:
            return Action.C
        elif cooperators_last < self.m and coop_rate > 0.5:
            return Action.D
        else:
            p_c = (self.m - cooperators_last) / (self.n_players - cooperators_last + 1)
            return Action.C if random.random() < p_c else Action.D
        if state.round_number == self.r - 1:
            total_coop_payoff = np.sum(self.k * history.my_actions[:-1])
            total_defect_payoff = np.sum(history.my_payoffs[:-1] * (history.my_actions[:-1] == False))
            return Action.D if total_coop_payoff <= total_defect_payoff else Action.C

class Strategy_EXPLOITATIVE_190(BaseStrategy):
    """
    An adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_models = [0.5] * game_description.n_players
        self.coop_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if prev_coop_count < self.game_description.m:
            return Action.D
        elif prev_coop_count == self.game_description.m:
            action = Action.C if random.random() < self.coop_probability else Action.D
            return action
        else:
            return Action.D
        if state.round_number < self.game_description.n_rounds - 1:
            self.update_opponent_models(history)
            self.adjust_cooperation_probability()

    def update_opponent_models(self, history: PlayerHistory):
        opponent_actions = history.opponent_actions[-1, :]
        for i, action in enumerate(opponent_actions):
            if action:
                self.opponent_models[i] += 0.05
            else:
                self.opponent_models[i] -= 0.05

    def adjust_cooperation_probability(self):
        opponent_coop_rates = np.mean(self.opponent_models)
        if opponent_coop_rates > 0.75:
            self.coop_probability *= 0.9
        elif opponent_coop_rates < 0.25:
            self.coop_probability *= 1.1
        self.coop_probability = min(1, max(0, self.coop_probability))

class Strategy_EXPLOITATIVE_241(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy adapts to the game's parameters and history 
    to maximize its payoff while minimizing cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.altruists = []
        self.cooperators_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        elif state.round_number < 3:
            last_round_cooperators = sum(history.opponent_actions[-1, :])
            if last_round_cooperators < self.game_description.m:
                return Action.C
            else:
                return Action.D
        else:
            self.cooperators_history.append(sum(history.opponent_actions[-1, :]))
            self.avg_coop = np.mean(self.cooperators_history[-3:])
            if self.avg_coop >= self.game_description.m:
                action = Action.C
            else:
                action = Action.D
            for i, opponent in enumerate(history.opponent_actions[:, -1]):
                if history.my_payoffs.size > 0 and np.all(opponent) and (self.avg_coop < self.game_description.m):
                    self.altruists.append(i)
                    action = Action.D
                    break
            if state.round_number == self.game_description.n_rounds - 1:
                last_round_cooperators = sum(history.opponent_actions[-1, :])
                if last_round_cooperators < self.game_description.m:
                    return Action.C
                else:
                    return Action.D
            return action

class Strategy_EXPLOITATIVE_303(BaseStrategy):
    """
    This strategy exploits the opponent's behavior by adapting to their cooperation threshold and maximizing personal payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        ε = 0.05
        θ = (self.game_description.m - 1) / self.game_description.n_players + ε
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            if opponent_action:
                self.acr[i] += 1 / state.round_number
        opponents_acr = [acr for j, acr in enumerate(self.acr) if j != 0]
        avg_opponents_acr = sum(opponents_acr) / len(opponents_acr)
        action = Action.D if avg_opponents_acr > θ else Action.C
        if history and action == Action.C:
            self.acr[0] += 1 / state.round_number
        return action

class Strategy_EXPLOITATIVE_394(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its own payoff by exploiting opponents' behaviors,
    while also adapting to changing circumstances.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_behavior = [None] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number < 5:
            if prev_cooperators >= self.game_description.m // 2:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            p = 0.7
            if prev_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.C if random.random() > p else Action.D
        else:
            for i, behavior in enumerate(self.opponent_behavior):
                if behavior == [True, False]:
                    self.opponent_behavior[i] = None
                    return Action.D
            p = 0.7
            if prev_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.C if random.random() > p else Action.D
        for i in range(self.game_description.n_players):
            if history.opponent_actions[-1, i]:
                self.opponent_behavior[i] = [True]
            elif self.opponent_behavior[i] == [True]:
                self.opponent_behavior[i].append(False)

class Strategy_EXPLOITATIVE_405(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy exploits opponents' cooperative behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cumulative_coops = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        CR = self.cumulative_coops / (self.game_description.n_players * state.round_number)
        if CR >= self.game_description.m / self.game_description.n_players:
            action = Action.D
        else:
            p = self.game_description.m / self.game_description.n_players / CR if CR != 0 else 1
            action = Action.C if random.random() < p else Action.D
        if history and history.my_actions[-1]:
            self.cumulative_coops += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if CR == self.game_description.m / self.game_description.n_players:
            action = Action.C if random.random() < 0.5 else Action.D
        return action

class Strategy_EXPLOITATIVE_508(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior and exploits their tendencies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.reliable_cooperators = []
        self.prev_round_outcome = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + 1
        avg_payoff_prev = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        if prev_round_cooperators < self.game_description.m:
            return Action.D
        elif prev_round_cooperators >= self.game_description.m:
            if avg_payoff_prev > 1 + self.game_description.k / 2:
                pass
            return Action.C
        opponent_cooperation_rates = []
        for opp in range(self.game_description.n_players):
            cooperation_rate = sum(history.opponent_actions[:, opp]) / len(history.opponent_actions[:, opp])
            if cooperation_rate > 0.5:
                self.reliable_cooperators.append(opp)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif state.round_number < 2:
            return Action.C
        opponent_cooperation_rates = []
        for opp in range(self.game_description.n_players):
            cooperation_rate = sum(history.opponent_actions[:, opp]) / len(history.opponent_actions[:, opp])
            if np.std([opp_action == True for opp_action in history.opponent_actions[:, opp]]) > 0.5:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_513(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coop_count = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        m = self.game_description.m
        CT = m / n_players
        if history is None or len(history.my_actions) < 1:
            return Action.D
        prev_coop_count = np.sum(np.concatenate((history.my_actions[-1:], history.opponent_actions[-1, :]), axis=0))
        if self.prev_coop_count is not None and prev_coop_count >= CT * n_players and (self.prev_coop_count < m / 2):
            return Action.C
        if prev_coop_count >= CT * n_players:
            return Action.C
        elif prev_coop_count < m / 2:
            return Action.D
        else:
            return Action.D
        self.prev_coop_count = prev_coop_count
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_514(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.

    Initially cooperates to establish a cooperative tone. In subsequent rounds, adjusts action based on previous round's cooperation level.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and self.game_description.n_players == 2):
            return Action.C
        if state.round_number > 0:
            prev_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if prev_round_cooperators >= self.game_description.m:
                return Action.C
            elif prev_round_cooperators < self.game_description.m and prev_round_cooperators > 0:
                prob_defect = (self.game_description.m - prev_round_cooperators) / (self.game_description.n_players - prev_round_cooperators)
                if random.random() < prob_defect:
                    return Action.D
                else:
                    return Action.C
            elif prev_round_cooperators == 0 and state.round_number < self.game_description.n_rounds:
                prob_coop = (self.game_description.m - 1) / self.game_description.n_players
                if random.random() < prob_coop:
                    return Action.C
        if state.round_number + 1 == self.game_description.n_rounds:
            potential_payoff = history.my_payoffs[-1] + self.game_description.k * prev_round_cooperators
            threshold = self.game_description.m * self.game_description.k
            if potential_payoff > threshold:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_515(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.convergence_window = max(1, math.ceil(game_description.n_rounds / 10))
        self.probability_adjustment = 0.01
        self.risk_aversion_threshold = max(1, math.floor(game_description.n_rounds / 5))
        self.cooperation_probability = 0.5
        self.previous_round_outcome = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.previous_round_outcome = np.sum(history.opponent_actions[-1, :] + (history.my_actions[-1] == True))
        if self.previous_round_outcome >= self.game_description.m:
            self.cooperation_probability = min(1.0, max(0.0, self.cooperation_probability * 1.05))
        else:
            self.cooperation_probability = max(0.0, min(1.0, self.cooperation_probability * 0.95))
        if state.round_number >= self.game_description.n_rounds - self.risk_aversion_threshold:
            return Action.D if random.random() > self.previous_round_outcome / self.game_description.n_players else Action.C
        if random.random() < self.cooperation_probability:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_516(BaseStrategy):
    """
    This strategy adapts to the level of cooperation observed in previous rounds.
    It encourages others to cooperate by setting a positive tone and avoiding low-payoff situations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_t = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        total_payoff_previous_round = np.sum(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        if m_t >= self.game_description.m or (total_payoff_previous_round <= self.game_description.n_players and state.round_number < self.game_description.n_rounds - 1):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_517(BaseStrategy):
    """
    Adaptive Cooperation with History-Dependent Threshold (ACHT) strategy for the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperation_threshold = self.game_description.m - previous_cooperators
        if previous_cooperators <= cooperation_threshold or (state.round_number == self.game_description.n_rounds - 1 and (not any((sum(round) >= self.game_description.m for round in history.opponent_actions.T)))):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_518(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_adapt = game_description.m
        self.punish = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if m_prev >= self.game_description.m:
            action = Action.C
        elif self.punish:
            action = Action.D
            self.punish = False
        else:
            action = Action.D
        if m_prev < self.game_description.m:
            self.m_adapt = max(self.game_description.m - 1, 1)
        elif m_prev >= self.game_description.m:
            self.m_adapt = min(self.game_description.m + 1, self.game_description.n_players)
        if m_prev < self.m_adapt and (not self.punish):
            self.punish = True
        return action

class Strategy_COLLECTIVE_519(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for cooperation to achieve a higher collective payoff.
    It adapts to the game's history, using a probabilistic approach to decide between cooperation and defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = max(game_description.m - 1, 0)
        self.previous_cooperation_level = 0
        self.previous_cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        current_cooperation_level = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if current_cooperation_level >= self.cooperation_threshold:
            new_probability = min(self.previous_cooperation_probability + 0.1, 0.9)
        else:
            new_probability = max(self.previous_cooperation_probability - 0.1, 0.1)
        action = Action.C if random.random() < new_probability else Action.D
        self.previous_cooperation_level = current_cooperation_level
        self.previous_cooperation_probability = new_probability
        return action

class Strategy_COLLECTIVE_520(BaseStrategy):
    """
    Adaptive Collective Responsibility strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.c_prev = 0
        self.p_prev = self.m / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.m / self.n_players
        else:
            opponent_cooperations = sum(history.opponent_actions[state.round_number - 1])
            total_cooperations = opponent_cooperations + (history.my_actions[state.round_number - 1] if state.round_number > 0 else 0)
            self.c_prev = total_cooperations / self.n_players
            cooperate_prob = max(0.5, self.k / (self.k + 1)) if self.c_prev >= self.m / self.n_players else 1 - self.p_prev
        explore_prob = 0.05
        if random.random() < explore_prob:
            action = Action.C if random.random() < 0.5 else Action.D
        else:
            action = Action.C if random.random() < cooperate_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate_prob = self.m / self.n_players
            action = Action.C if random.random() < cooperate_prob else Action.D
        self.p_prev = cooperate_prob
        return action

class Strategy_COLLECTIVE_521(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy in the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.cooperate_prob = 1.0
        self.prev_threshold_met = False
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            if self.prev_threshold_met:
                self.cooperate_prob = (self.game_description.k - 1) / self.game_description.k
            else:
                self.cooperate_prob = self.game_description.m / self.game_description.n_players
            if state.round_number < self.game_description.n_rounds - 1:
                if self.prev_threshold_met:
                    self.cooperate_prob = min(1.0, self.cooperate_prob + 0.1)
                else:
                    self.cooperate_prob = max(0.0, self.cooperate_prob - 0.05)
            action = Action.C if random.random() < self.cooperate_prob else Action.D
            if state.round_number < self.game_description.n_rounds:
                opponent_cooperations = np.sum(history.opponent_actions[-1, :])
                self.prev_threshold_met = opponent_cooperations + 1 >= self.game_description.m
            elif state.round_number == self.game_description.n_rounds - 1:
                action = Action.C
        return action

class Strategy_COLLECTIVE_522(BaseStrategy):
    """
    Adaptive Collective Risk Dilemma (ACRD) strategy.
    
    This strategy balances individual self-interest with collective cooperation 
    to achieve a mutually beneficial outcome in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperation_level = 0
        self.prev_payoff = 0
        self.avg_prev_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        prev_opponent_actions = history.opponent_actions[-1, :]
        self.prev_cooperation_level = sum(prev_opponent_actions) + (history.my_actions[-1] == True)
        if state.round_number < self.game_description.n_rounds - 1:
            if self.prev_cooperation_level >= self.game_description.m and self.prev_payoff >= self.avg_prev_payoff:
                return Action.C
            elif self.prev_cooperation_level < self.game_description.m and self.prev_payoff < self.avg_prev_payoff:
                return Action.D
        elif self.prev_cooperation_level >= self.game_description.m and sum(history.my_payoffs) >= np.mean(np.sum(history.opponent_payoffs, axis=0)):
            return Action.C
        return Action.D

    def update_variables(self, payoff: float):
        self.prev_payoff = payoff
        if hasattr(self, 'history'):
            opponent_payoffs = self.history.opponent_payoffs[-1, :]
            self.avg_prev_payoff = np.mean(opponent_payoffs)

class Strategy_COLLECTIVE_523(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Cooperation (ATC)

    This strategy prioritizes cooperation while adapting to the behavior of opponents.
    By initially cooperating and responding to the outcome of previous rounds,
    the strategy promotes a collective mindset focused on achieving the threshold.

    The convergence criterion and adaptive response mechanisms allow the strategy
    to adjust its level of cooperation based on the success or failure of meeting
    the threshold, ultimately promoting a balance between individual payoffs and
    collective benefits.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.consecutive_successes = 0
        self.opponent_cooperation_streak = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_outcome = history.opponent_actions[-1, :]
        self.opponent_cooperation_streak = [streak + (action == True) for streak, action in zip(self.opponent_cooperation_streak, prev_round_outcome)]
        num_cooperators_prev_round = sum(prev_round_outcome)
        if num_cooperators_prev_round >= self.game_description.m:
            self.consecutive_successes += 1
            action = Action.C
        elif self.consecutive_successes >= 3 * self.game_description.m:
            action = Action.C
        elif max(self.opponent_cooperation_streak) >= 2 * self.game_description.m:
            action = Action.C
        elif self.get_total_payoff(history.my_payoffs, history.opponent_payoffs) < self.get_private_payoff():
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if self.get_total_payoff(history.my_payoffs, history.opponent_payoffs) >= self.get_private_payoff():
                action = Action.C
            else:
                action = Action.D
        return action

    def get_total_payoff(self, my_payoffs: NDArray[np.float64], opponent_payoffs: NDArray[np.float64]) -> float:
        return np.sum(my_payoffs)

    def get_private_payoff(self) -> float:
        return 1.0

class Strategy_COLLECTIVE_524(BaseStrategy):
    """
    A strategy that adapts to collective performance by adjusting cooperation probabilities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.observation_phase = True
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if self.observation_phase:
            avg_cooperation_rate = np.mean(history.opponent_actions[:-1, :])
            if avg_cooperation_rate >= self.game_description.m / self.game_description.n_players:
                return Action.C
            else:
                return random.choice([Action.C, Action.D])
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            avg_payoff = np.mean(history.my_payoffs)
            if avg_payoff > 1 + self.game_description.k / 2:
                self.cooperate_prob = 0.7
            elif avg_payoff < 1 + self.game_description.k / 4:
                self.cooperate_prob = 0.3
            if not self.punishment_phase and sum(history.my_actions[-2:]) == 0 and (np.sum(history.opponent_actions[-2:, :], axis=0).max() < self.game_description.m):
                self.punishment_phase = True
                return Action.D
            elif self.punishment_phase and state.round_number > history.my_payoffs.size - 1 + 1:
                self.punishment_phase = False
            if np.random.rand() < self.cooperate_prob:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_525(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        k_reward_factor = self.game_description.k
        r_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C if random.random() < m_cooperators_needed / n_players else Action.D
        prev_opponent_actions = history.opponent_actions[-1, :]
        num_prev_cooperators = sum(prev_opponent_actions)
        if num_prev_cooperators >= m_cooperators_needed:
            coop_prob = k_reward_factor / (k_reward_factor + 1)
        else:
            coop_prob = 0
        if history.my_payoffs[-1] == 1 + k_reward_factor and state.round_number < r_rounds - 1:
            return Action.D
        if state.round_number == r_rounds - 1:
            coop_prob = 1 if num_prev_cooperators >= m_cooperators_needed else 0
        if num_prev_cooperators == m_cooperators_needed:
            coop_prob = 0.5
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_COLLECTIVE_526(BaseStrategy):
    """
    Adaptive Collective Responsibility Strategy.

    This strategy balances individual self-interest with collective responsibility,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.pi_avg = 0.0
        self.opponent_cooperations = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        previous_round_payoffs = history.my_payoffs[-1] + (history.opponent_payoffs[-1].sum() - history.my_payoffs[-1]) / (self.game_description.n_players - 1)
        self.pi_avg = previous_round_payoffs
        if self.pi_avg >= 1 + self.game_description.k / 2:
            cooperate_prob = 0.5 + (self.pi_avg - 1) / (self.game_description.k - 1)
        else:
            cooperate_prob = 0.0
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        if opponent_cooperations >= self.game_description.m:
            cooperate_prob = max(0.5, opponent_cooperations / self.game_description.n_players)
        for i in range(self.game_description.n_players - 1):
            if history.opponent_payoffs[-1][i] > history.my_payoffs[-1]:
                cooperate_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            opponent_cooperations = sum(history.opponent_actions[-2, :])
            if opponent_cooperations >= self.game_description.m - 1:
                return Action.C
        consecutive_defections = np.sum(history.opponent_actions[:, 0], dtype=np.int64) > self.game_description.n_rounds // 4
        if consecutive_defections:
            cooperate_prob = max(0.2, min(cooperate_prob, 1 - 0.8))
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_527(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation.
    
    Our strategy aims to balance individual payoff maximization with collective risk management. 
    We adapt our behavior based on observed opponent actions and game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = 0
        self.defect_probability = 1 - game_description.k / (game_description.n_players + game_description.k)
        self.punishment_target = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        if history is not None:
            self.cooperate_count = sum((1 for opponent_action in history.opponent_actions[-1, :] if opponent_action))
            opponents_defected = [i for i, opponent_action in enumerate(history.opponent_actions[-1, :]) if not opponent_action]
            if self.cooperate_count >= self.game_description.m and len(opponents_defected) > 0:
                self.punishment_target = random.choice(opponents_defected)
        if self.cooperate_count >= self.game_description.m:
            action = Action.C
        else:
            action = Action.D if random.random() < self.defect_probability else Action.C
        if self.punishment_target is not None and action == Action.C:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1 and self.cooperate_count == self.game_description.m - 1:
            action = Action.C if random.random() < 0.5 else Action.D
        return action

class Strategy_COLLECTIVE_528(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        else:
            prev_round_cooperators = sum(history.opponent_actions[-1, :] + (history.my_actions[-1],))
            total_payoff = np.sum(history.my_payoffs[-1:] + history.opponent_payoffs[-1, :])
            if prev_round_cooperators >= self.game_description.m:
                return Action.C
            elif prev_round_cooperators < self.game_description.m and random.random() < 1 - self.game_description.k / (self.game_description.k + 1):
                return Action.D
            elif total_payoff >= self.game_description.n_players * self.game_description.k / 2:
                return Action.C
            elif prev_round_cooperators == self.game_description.m:
                return random.choice([Action.C, Action.D])
            else:
                return Action.D
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D

class Strategy_COLLECTIVE_529(BaseStrategy):
    """
    This strategy aims to balance individual self-interest with collective well-being 
    while adapting to diverse opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        num_coop_prev = sum(prev_round)
        if num_coop_prev >= self.game_description.m:
            return Action.C
        elif num_coop_prev > 0:
            p = self.game_description.m / self.game_description.n_players
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        elif num_coop_prev == 0 and state.round_number < self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif num_coop_prev == self.game_description.m - 1:
            p = 0.5
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        elif sum(prev_round) == self.game_description.n_players - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_530(BaseStrategy):
    """
    Adaptive Cooperation strategy.
    
    This strategy prioritizes collective success while adapting to changing circumstances. 
    By adjusting the cooperation rate based on previous rounds' outcomes, it balances individual risk-taking with the need for collective effort to reach the threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            cooperate_prob = m / n_players
        else:
            prev_round_payoff = history.my_payoffs[-1]
            prev_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if prev_round_cooperators >= m:
                cooperate_prob = min(1, m / n_players + 0.1)
            else:
                cooperate_prob = max(m / n_players, m / n_players - 0.1)
        if state.round_number == r - 1:
            expected_coop_payoff = n_players * k / n_players
            expected_defect_payoff = 1 + (m - 1) * k / (n_players - 1)
            cooperate_prob = 1 if expected_coop_payoff > expected_defect_payoff else 0
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_531(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation.
    
    This strategy aims to balance individual self-interest with collective well-being 
    by adapting to the group's cooperation level and past behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < self.game_description.n_rounds // 2:
            prev_cooperators = sum(history.opponent_actions[-1, :] + [True])
            if prev_cooperators < self.game_description.m:
                return Action.D
            else:
                return Action.C
        elif state.round_number >= self.game_description.n_rounds // 2 and state.round_number < self.game_description.n_rounds - 1:
            avg_coop_rate = np.mean(history.opponent_actions[-min(10, state.round_number), :])
            if avg_coop_rate > self.theta:
                return Action.C
            else:
                return Action.D
        elif state.round_number >= self.game_description.n_rounds - 1 and state.round_number < self.game_description.n_rounds:
            prev_collective_payoff = sum(history.opponent_payoffs[-1, :] + [0])
            if prev_collective_payoff >= self.game_description.m * self.game_description.k:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            prev_cooperators = sum(history.opponent_actions[-1, :] + [True])
            if prev_cooperators >= self.game_description.m:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_532(BaseStrategy):
    """
    The Adaptive Collective Threshold strategy for the Collective Risk Dilemma game.
    
    This strategy adapts to the cooperation ratio of other players and punishes defection when the threshold is met.
    It prioritizes collective success by encouraging cooperation when the threshold is met and punishing defection in such situations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cr = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        cr = np.mean(opponent_actions)
        if history.my_payoffs.size > 0:
            prev_payoff = history.my_payoffs[-1]
            if cr >= self.game_description.m / self.game_description.n_players and history.my_actions[-1] == Action.D.value:
                prev_payoff *= 0.9
            elif cr < self.game_description.m / self.game_description.n_players and history.my_actions[-1] == Action.C.value:
                prev_payoff *= 1.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if cr >= self.game_description.m / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_533(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.avg_coop_prev = None
        self.m_prime = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        if history is not None:
            prev_coop_players = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
            avg_coop_prev = prev_coop_players / self.game_description.n_players
            if avg_coop_prev >= (self.m_prime - 1) / self.game_description.n_players:
                action = Action.C
            else:
                action = Action.D
            if state.round_number % 5 == 0 and state.round_number > 0:
                self.m_prime = math.ceil((avg_coop_prev + 0.1) * self.game_description.n_players)
        else:
            action = Action.C
        if random.random() < 0.1:
            action = Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if avg_coop_prev >= (self.m_prime - 1) / self.game_description.n_players:
                action = Action.C
            else:
                action = Action.D
        if prev_coop_players == self.m_prime - 1 and history is not None:
            action = Action.C if random.random() < 0.7 else Action.D
        return action

class Strategy_COLLECTIVE_534(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Cooperation (ATC)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_prev_round = 0
        self.total_payoff_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.cooperators_prev_round = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        self.total_payoff_prev_round = np.sum(history.my_payoffs[-1]) + np.sum(history.opponent_payoffs[-1])
        if self.cooperators_prev_round >= self.game_description.m / 2 or self.total_payoff_prev_round >= self.game_description.n_players * self.game_description.k / 2:
            return Action.C
        elif self.cooperators_prev_round < self.game_description.m / 2 and self.total_payoff_prev_round < self.game_description.n_players * self.game_description.k / 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.cooperators_prev_round < self.game_description.m or self.total_payoff_prev_round < self.game_description.n_players * self.game_description.k / 2:
                return Action.C
        return Action.C

class Strategy_COLLECTIVE_535(BaseStrategy):
    """
    Adaptive Cooperation with Reputation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.reputation = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and sum(history.my_actions):
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        for i in range(self.game_description.n_players - 1):
            self.reputation[i] += 1 if opponent_actions[i] else -1
        T = max(self.game_description.m, min(self.game_description.n_players, math.ceil((state.round_number + 1) / 2)))
        my_reputation = self.reputation[self.game_description.n_players - 1]
        if my_reputation >= T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_536(BaseStrategy):
    """
    A strategy that adapts to the game's history by balancing individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
        avg_payoff_C = np.mean([payoff for coop, payoff in zip(history.opponent_actions[-1, :] + [True], history.opponent_payoffs[-1, :] + [0]) if coop])
        avg_payoff_D = np.mean([payoff for coop, payoff in zip(history.opponent_actions[-1, :] + [True], history.opponent_payoffs[-1, :] + [0]) if not coop])
        if prev_round_cooperators < self.game_description.m and prev_round_cooperators == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if avg_payoff_C + self.game_description.k > avg_payoff_D or (avg_payoff_C + self.game_description.k == avg_payoff_D and random.choice([True, False])):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_537(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective responsibility,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_round_cooperators = 0
        self.average_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_cooperators >= self.game_description.m and history.my_payoffs[-1] == 1 - int(history.my_actions[-1]) + self.game_description.k:
            return Action.C
        else:
            last_r_div_2 = min(self.game_description.n_rounds, state.round_number) // 2
            cooperators_last_r_div_2 = np.sum(np.any(history.opponent_actions[-last_r_div_2:, :], axis=1))
            self.average_cooperators = cooperators_last_r_div_2 / last_r_div_2
            if self.average_cooperators < self.game_description.m / 2:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and (previous_round_cooperators >= self.game_description.m or self.average_cooperators < self.game_description.m / 2):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_538(BaseStrategy):
    """
    Adaptive Threshold Response strategy for Collective Risk Dilemma.
    
    This strategy adapts to opponent behavior by adjusting cooperation probability 
    based on average payoffs of cooperators and defectors in the previous round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        M_t = self.game_description.m - (self.game_description.n_rounds - state.round_number) / self.game_description.n_rounds * (self.game_description.m - 1)
        cooperators_prev = sum(history.opponent_actions[-1, :])
        if cooperators_prev >= M_t or state.round_number == self.game_description.n_rounds:
            return Action.C
        avg_coop_payoff = np.mean(np.where(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :], 0))
        avg_def_payoff = np.mean(np.where(~history.opponent_actions[-1, :], history.opponent_payoffs[-1, :], 0))
        if avg_coop_payoff > avg_def_payoff:
            self.coop_prob += 0.1 * (1 - self.coop_prob)
        else:
            self.coop_prob -= 0.1 * self.coop_prob
        return Action.C if random.random() < self.coop_prob else Action.D

class Strategy_COLLECTIVE_539(BaseStrategy):
    """
    Adaptive Collective Contribution (ACC) strategy.
    
    This strategy adapts the cooperation probability based on previous rounds' outcomes,
    promoting a dynamic balance between individual self-interest and collective welfare.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5
        self.total_payoff = 0
        self.threshold_met_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        previous_round_outcome = history.my_payoffs[-1]
        opponents_cooperated = sum(history.opponent_actions[-1, :])
        threshold_was_met = opponents_cooperated >= self.game_description.m
        self.threshold_met_count += int(threshold_was_met)
        if previous_round_outcome > 1.5 * self.game_description.k and threshold_was_met:
            self.cooperation_probability = min(self.cooperation_probability + 0.1, 0.9)
        elif previous_round_outcome < 0.5 and (not threshold_was_met):
            self.cooperation_probability = max(self.cooperation_probability - 0.1, 0.1)
        if self.threshold_met_count > state.round_number / 2:
            self.cooperation_probability += 0.05
        else:
            self.cooperation_probability -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.total_payoff > np.mean(history.my_payoffs) else Action.D
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_540(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Threshold (ACT)
    
    This strategy balances individual self-interest with collective well-being, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_threshold_met = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] >= self.game_description.m
        if prev_threshold_met:
            p_coop = (self.game_description.k - 1) / self.game_description.k
        else:
            p_coop = 0.5 + (self.game_description.n_players - self.game_description.m) / (2 * self.game_description.n_players)
        coop_prob = random.random()
        if coop_prob < p_coop or (state.round_number == self.game_description.n_rounds - 1 and history.my_payoffs[-1] >= self.game_description.k):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_541(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Cooperator (ATC)

    The ATC strategy is designed to balance individual self-interest with collective cooperation, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.punish_prob = 1 - game_description.k / (game_description.k + 1)
        self.exploration_prob = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_round_coop = sum(history.opponent_actions[-1, :] | history.my_actions[-1]) / self.game_description.n_players
        self.avg_coop = (self.avg_coop * (state.round_number - 1) + prev_round_coop) / state.round_number
        if self.avg_coop >= self.game_description.m / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D if random.random() < self.punish_prob else Action.C
        if random.random() < self.exploration_prob:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_COLLECTIVE_542(BaseStrategy):
    """
    Adaptive Collective Risk (ACR) strategy balances individual self-interest with collective risk management.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators = 0
        self.own_payoff = 0
        self.consecutive_cooperations = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if prev_cooperators >= self.game_description.m:
            action = Action.C
        elif history.my_payoffs[-1] != 1 + self.game_description.k:
            action = Action.D
        elif history.my_payoffs[-1] == 1 + self.game_description.k and any((not x for x in history.opponent_actions[-1, :])):
            action = Action.C
        else:
            action = Action.D
        if prev_cooperators < self.game_description.m - 1 and self.consecutive_cooperations >= self.game_description.m - 1:
            action = Action.D
        self.num_cooperators = prev_cooperators
        self.own_payoff = history.my_payoffs[-1]
        self.consecutive_cooperations += 1 if action == Action.C else -self.consecutive_cooperations
        return action

class Strategy_COLLECTIVE_543(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: "Adaptive Collective Optimism" (ACO)

    This strategy is designed to promote collective cooperation while adapting to various opponent behaviors.
    By initially cooperating and observing others' actions, we create an environment that encourages mutual cooperation.
    The adaptive nature of the strategy allows us to respond to changes in the collective behavior,
    rewarding successful cooperation and deterring defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = max(1 - (game_description.n_players - game_description.m) / (game_description.n_players - 1), 0.5)
        self.p_defect = min((game_description.n_players - game_description.m) / game_description.n_players, 0.8)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        m_obs = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if m_obs >= self.game_description.m:
            cooperate = random.random() < self.p_coop
            self.p_coop += 0.2
            if self.p_coop > 1:
                self.p_coop = 1
        else:
            cooperate = not random.random() < self.p_defect
            self.p_defect -= 0.1
            if self.p_defect < 0:
                self.p_defect = 0
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_544(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = (game_description.m - 1) / game_description.n_players
        self.previous_cooperators = None
        self.previous_previous_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.previous_cooperators is None:
            self.previous_cooperators = sum(history.opponent_actions[-1, :])
        else:
            self.previous_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if state.round_number > 1 and self.previous_previous_cooperators is not None:
            if self.previous_cooperators >= self.game_description.m and self.previous_previous_cooperators >= self.game_description.m:
                self.cooperation_threshold -= 0.1
            elif self.previous_cooperators < self.game_description.m and self.previous_previous_cooperators < self.game_description.m:
                self.cooperation_threshold += 0.1
        self.previous_previous_cooperators = self.previous_cooperators
        if self.previous_cooperators / self.game_description.n_players >= self.cooperation_threshold:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_545(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= min(self.game_description.n_rounds // 2, 3):
            return Action.C
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [Action.C.value])
        if self.game_description.m <= prev_cooperators:
            return Action.C
        else:
            p = 1 - (self.game_description.k - 1) / self.game_description.k
            return Action.D if random.random() < p else Action.C
        total_payoff = sum(history.my_payoffs)
        expected_threshold_reward = self.game_description.m * self.game_description.k
        if total_payoff < expected_threshold_reward:
            return Action.C
        else:
            p = 1 - (self.game_description.k - 1) / self.game_description.k
            return Action.D if random.random() < p else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if total_payoff < expected_threshold_reward:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_546(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy for Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        coop_rate = np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if state.round_number > 0:
            payoffs = np.sum(np.where(history.my_payoffs > 0, history.my_payoffs, 0)) + np.sum(np.where(history.opponent_payoffs > 0, history.opponent_payoffs, 0))
            if coop_rate * self.game_description.k >= np.sum(history.opponent_actions[-1, :]):
                self.theta += 1 / (self.game_description.n_players * self.game_description.n_rounds)
            else:
                self.theta -= 1 / (self.game_description.n_players * self.game_description.n_rounds)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.game_description.k * np.sum(history.opponent_actions[-1, :]) > 1 else Action.D
        elif coop_rate >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_547(BaseStrategy):
    """
    A Collective Risk Dilemma strategy that adapts to changing circumstances and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.my_actions) == 0):
            return Action.C
        prev_outcome = sum(history.opponent_actions[-1, :] + [True])
        n_defected = self.game_description.n_players - prev_outcome
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif prev_outcome >= self.game_description.m:
            return Action.C
        elif prev_outcome == self.game_description.m - 1:
            return Action.C
        elif n_defected == self.game_description.n_players:
            return Action.D
        else:
            p = n_defected / self.game_description.n_players
            if random.random() < p:
                return Action.D
            else:
                return Action.C
        self.history.append(prev_outcome)

class Strategy_COLLECTIVE_548(BaseStrategy):
    """
    A strategy that balances individual self-interest with the need to achieve collective cooperation thresholds.
    It adapts based on observed game history and adjusts its cooperation level accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperators = 0.0
        self.success_rate = 0.0
        self.cautious_cooperation_level = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            opponent_actions = history.opponent_actions
            avg_cooperators = np.mean(np.sum(opponent_actions, axis=1))
            success_rate = np.mean(np.sum(opponent_actions, axis=1) >= self.game_description.m)
            if success_rate < 0.5:
                self.cautious_cooperation_level *= 0.9
        if state.round_number > self.game_description.n_rounds - 3:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            potential_payoff = np.mean(history.my_payoffs)
            if avg_cooperators >= self.game_description.m and random.random() < self.cautious_cooperation_level:
                return Action.C
            else:
                return Action.D
        if random.random() < self.cautious_cooperation_level:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_549(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Memory (ACM)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.memory_window_size = 5
        self.cooperation_probability = game_description.k / (game_description.k + 1)
        self.round_memory = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players // 2 else Action.D
        self.update_memory(history)
        self.update_cooperation_probability()
        cooperate = random.random() < self.cooperation_probability
        if state.round_number == self.game_description.n_rounds - 1:
            threshold_met, _ = self.calculate_outcome(self.round_memory[-1], len([p for p in history.opponent_actions[-1, :] if p]))
            cooperate = threshold_met
        return Action.C if cooperate else Action.D

    def update_memory(self, history: PlayerHistory):
        threshold_met, _ = self.calculate_outcome(history.my_payoffs[-1], len([p for p in history.opponent_actions[-1, :] if p]))
        self.round_memory.append(threshold_met)
        if len(self.round_memory) > self.memory_window_size:
            self.round_memory.pop(0)

    def update_cooperation_probability(self):
        threshold_proportion = sum([m for m in self.round_memory]) / len(self.round_memory)
        avg_payoff_c = np.mean([p for p, m in zip(self.game_description.k * np.ones(len(self.round_memory)), self.round_memory) if m])
        if threshold_proportion > 0.5 and avg_payoff_c > 1:
            self.cooperation_probability *= 1.1
        elif threshold_proportion < 0.5 or avg_payoff_c <= 1:
            self.cooperation_probability /= 1.1
        self.cooperation_probability = min(max(self.cooperation_probability, 0), 1)

    def calculate_outcome(self, payoff: float, cooperators: int):
        threshold_met = cooperators >= self.game_description.m
        return (threshold_met, payoff)

class Strategy_COLLECTIVE_550(BaseStrategy):
    """
    Adaptive Cooperative Threshold Strategy for Collective Risk Dilemma.
    
    Initially cooperates to establish a threshold, then adapts to the level of cooperation in the group.
    Encourages others to cooperate while being robust to exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.alpha = 3
        self.beta = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.m:
            cooperate = True
        else:
            cooperate = False
        consecutive_coop_rounds = 0
        consecutive_noncoop_rounds = 0
        for i in range(max(0, state.round_number - max(self.alpha, self.beta)), state.round_number):
            if sum(history.opponent_actions[i, :]) + history.my_actions[i] >= self.m:
                consecutive_coop_rounds += 1
            else:
                consecutive_noncoop_rounds += 1
        p_coop = None
        if consecutive_coop_rounds == self.alpha:
            p_coop = 1 - (self.n - self.m) / self.n
        elif consecutive_noncoop_rounds == self.beta:
            cooperate = False
        if state.round_number == self.r - 1:
            cooperate = False
        if p_coop is not None:
            return Action.C if random.random() < p_coop else Action.D
        else:
            return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_551(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma game.
    This strategy prioritizes collective success over individual gain when the benefits of cooperation are clear.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.prev_round_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None:
            prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            self.prev_round_cooperators.append(prev_cooperators >= self.game_description.m)
            adjust_prob = self.calculate_adjustment(self.cooperate_prob, self.prev_round_cooperators)
            self.cooperate_prob += adjust_prob
        if len(self.prev_round_cooperators) >= 1 and self.prev_round_cooperators[-1]:
            return Action.C
        return Action.C if random.random() < self.cooperate_prob else Action.D

    def calculate_adjustment(self, current_prob: float, prev_round_cooperators: list[bool]) -> float:
        """
        Calculate the adjustment to the cooperation probability based on collective success.
        """
        if self.collective_success(prev_round_cooperators):
            return 0.1
        else:
            return -0.1

    def collective_success(self, prev_round_cooperators: list[bool]) -> bool:
        """
        Check if there has been collective success in at least half of the previous rounds.
        """
        success_count = sum((1 for coop in prev_round_cooperators if coop))
        return success_count >= len(prev_round_cooperators) / 2

class Strategy_COLLECTIVE_552(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective risk aversion,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperated_last_round = False
        self.num_cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(self.cooperated_last_round)
        threshold = math.ceil(self.game_description.m / 2)
        if self.game_description.k > 1.5:
            threshold += 1
        action = Action.C
        if self.num_cooperators_last_round < threshold:
            prob_defect = (threshold - self.num_cooperators_last_round) / self.game_description.n_players
            action = Action.D if random.random() < prob_defect else Action.C
        if random.random() < 0.05:
            action = Action.D if action == Action.C else Action.C
        self.cooperated_last_round = action == Action.C
        return action

class Strategy_COLLECTIVE_553(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = {}
        self.cooperate = True
        self.threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.threshold_met = sum(history.opponent_actions[-1, :] + [self.cooperate]) >= self.game_description.m
        if self.threshold_met:
            self.cooperate = True
        else:
            for player in range(self.game_description.n_players - 1):
                if history.opponent_actions.shape[0] > 0:
                    cooperation_rate = np.mean(history.opponent_actions[:, player])
                    self.opponent_cooperation_rates[player] = cooperation_rate
                    if cooperation_rate > 0.5:
                        self.cooperate = True
                    elif history.opponent_actions[-1, player] == False:
                        self.cooperate = False
        if not self.threshold_met and any((not opponent_action for opponent_action in history.opponent_actions[-1, :])):
            self.cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            self.cooperate = self.threshold_met
        return Action.C if self.cooperate else Action.D

class Strategy_COLLECTIVE_554(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy for the Collective Risk Dilemma.

    This strategy aims to balance individual self-interest with collective cooperation,
    adapting to the game's history and opponent behavior while prioritizing the collective good.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.prev_cooperators >= self.game_description.m:
            action = Action.C
        elif 1 - int(history.my_actions[-1]) + self.game_description.k > history.my_payoffs[-1]:
            action = Action.D
        else:
            action = Action.C
        collective_cooperation_rate = self.prev_cooperators / self.game_description.n_players
        if collective_cooperation_rate < 0.25:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_COLLECTIVE_555(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that prioritizes cooperation and adaptability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_num_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_num_cooperators >= math.ceil(self.game_description.m / 2):
            return Action.C
        if self.game_description.k > 1.5 and prev_num_cooperators >= self.game_description.m - 1:
            return Action.C
        if prev_num_cooperators < math.ceil(self.game_description.m / 2):
            return Action.D
        return Action(random.choice([True, False]))

class Strategy_COLLECTIVE_556(BaseStrategy):
    """
    The Adaptive Threshold Response strategy, aiming to promote collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_threshold = self.game_description.m
        if state.round_number == 0:
            return Action.C if random.random() < m_threshold / n_players else Action.D
        previous_round_actions = history.opponent_actions[-1, :]
        avg_C_prev_round = np.mean(previous_round_actions)
        if avg_C_prev_round >= m_threshold / n_players:
            return Action.C
        elif random.random() < 0.5 and history.my_actions[-1]:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_557(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        my_last_action = bool(history.my_actions[-1])
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        my_payoff = self._calculate_my_payoff(my_last_action, opponent_cooperations)
        if opponent_cooperations >= self.m and my_payoff > 1:
            return Action.C
        elif opponent_cooperations < self.m and (my_payoff == 0 or my_payoff == 1 - my_last_action):
            return Action.D
        else:
            p = min((self.m - 1) / (self.n_players - 1), 1)
            if opponent_cooperations > self.m / 2:
                return random.choice([Action.C, Action.D], p=[p, 1 - p])
            else:
                return Action.D

    def _calculate_my_payoff(self, my_last_action: bool, opponent_cooperations: int) -> float:
        """
        Calculate the payoff for this player in a given round.
        
        :param my_last_action: The action taken by this player in the last round (True if cooperated, False otherwise)
        :param opponent_cooperations: Number of opponents who cooperated in the last round
        """
        c_i = int(my_last_action)
        if opponent_cooperations < self.m:
            return 1 - c_i
        else:
            return 1 - c_i + self.k

class Strategy_COLLECTIVE_558(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy.
    
    This strategy aims to balance individual self-interest with collective well-being by adaptively responding to the cooperation levels of others while prioritizing the achievement of the minimum threshold required for the collective reward.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_c_avg = 0.5
        self.m_adj = game_description.m
        self.w = 3
        self.tau = 0.6
        self.contributed_C_last_round = False
        self.punish_free_riders = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        p_c_last_round = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        self.p_c_avg = (self.p_c_avg * self.w + p_c_last_round) / (self.w + 1)
        collective_reward_achieved = sum(history.opponent_actions[-1, :]) >= self.m_adj
        if self.p_c_avg >= self.tau and self.m_adj < self.game_description.n_players - 1:
            self.m_adj += 1
        elif self.p_c_avg < self.tau and self.m_adj > 1:
            self.m_adj -= 1
        if not collective_reward_achieved and self.contributed_C_last_round:
            return Action.D
        if collective_reward_achieved and history.my_payoffs[-1] <= 1 and (not self.punish_free_riders):
            self.punish_free_riders = True
            return Action.D
        elif self.punish_free_riders:
            self.punish_free_riders = False
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        self.contributed_C_last_round = True
        return Action.C

class Strategy_COLLECTIVE_559(BaseStrategy):
    """
    Cooperative Threshold Tracker strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.theta = 0.5
        self.epsilon = 0.01
        self.delta = 0.05
        self.window_size = min(10, self.r)
        self.p = self.m / self.n_players
        self.T_t = np.zeros(self.window_size, dtype=int)
        self.MA_T = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        opponent_cooperations = np.sum(history.opponent_actions[-1, :] == True)
        self.T_t[:-1] = self.T_t[1:]
        self.T_t[-1] = int(opponent_cooperations + (history.my_actions[-1] == True))
        self.MA_T = np.mean(self.T_t)
        if state.round_number == self.r - 1:
            return Action.C
        cooperate = False
        if history.my_actions[-1] is not None and np.sum(history.opponent_actions[-1, :] == True) < self.m:
            if history.my_actions[-1]:
                self.p -= self.epsilon
                self.p = max(0.0, min(self.p, 1.0))
            else:
                self.p += self.epsilon
                self.p = max(0.0, min(self.p, 1.0))
        if self.MA_T >= self.theta * self.m:
            cooperate = True
        if random.random() < self.delta:
            return Action.C if random.choice([True, False]) else Action.D
        return Action.C if cooperate and random.random() < self.p else Action.D

class Strategy_COLLECTIVE_560(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy for Collective Risk Dilemma.
    
    This strategy balances individual self-interest with collective welfare by adapting to the game's history and parameters.
    It encourages cooperation when beneficial for all parties involved while being robust against various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.recent_payoffs_window_size = min(5, game_description.n_rounds)
        self.recent_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        threshold_met = prev_cooperators >= self.game_description.m
        critical_mass = math.ceil(self.game_description.m / 2)
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = sum(history.my_payoffs)
            return Action.D if total_payoff < self.game_description.k * (self.game_description.m / self.game_description.n_players) else Action.C
        if threshold_met:
            if prev_cooperators >= len(self.recent_payoffs):
                return Action.C
            elif sum(history.my_actions[-prev_cooperators:]) < prev_cooperators / self.game_description.n_players * 0.8:
                return Action.D
        if not threshold_met and prev_cooperators >= critical_mass:
            return Action.C
        self.recent_payoffs.append(history.my_payoffs[-1])
        if len(self.recent_payoffs) > self.recent_payoffs_window_size:
            self.recent_payoffs.pop(0)
        avg_recent_payoff = sum(self.recent_payoffs) / len(self.recent_payoffs)
        return Action.C if avg_recent_payoff >= (self.game_description.k * prev_cooperators + 1 - prev_cooperators) / self.game_description.n_players else Action.D
        return Action.D

class Strategy_COLLECTIVE_561(BaseStrategy):
    """
    Adaptive Threshold Response strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = 0.0
        self.rounds_since_last_update = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions
        my_actions = history.my_actions
        if state.round_number <= min(5, self.game_description.n_rounds):
            total_coops = np.sum(opponent_actions[:state.round_number, :])
            total_actions = opponent_actions[:state.round_number, :].size
            p = total_coops / total_actions
            self.threshold = max(self.game_description.m / self.game_description.n_players, p)
        self.rounds_since_last_update += 1
        if state.round_number % random.choice([3, 4, 5]) == 0:
            total_coops = np.sum(opponent_actions[-min(5, state.round_number):, :])
            total_actions = opponent_actions[-min(5, state.round_number):, :].size
            p = total_coops / total_actions
            self.threshold = max(self.game_description.m / self.game_description.n_players, p)
            self.rounds_since_last_update = 0
        num_coops = np.sum(opponent_actions[-1, :])
        if num_coops >= self.threshold * self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_562(BaseStrategy):
    """
    The Adaptive Collective Defender strategy balances individual self-interest with collective cooperation,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_likelihood = 0.5
        self.exploration_probability = 0.1

    def calculate_proportion_of_cooperators(self, history: PlayerHistory) -> float:
        """
        Calculate the proportion of cooperators in the previous round.
        """
        if len(history.opponent_actions) == 0 or len(history.my_actions) == 0:
            return 0
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        my_cooperation = history.my_actions[-1]
        total_cooperations = opponent_cooperations + int(my_cooperation)
        return total_cooperations / self.game_description.n_players

    def calculate_collective_reward_achieved(self, history: PlayerHistory) -> bool:
        """
        Check if the collective reward is achieved in the previous round.
        """
        if len(history.opponent_payoffs) == 0 or len(history.my_payoffs) == 0:
            return False
        average_payoff = np.mean(np.append(history.my_payoffs[-1], history.opponent_payoffs[-1, :]))
        return average_payoff > self.game_description.k

    def increase_cooperation_likelihood(self):
        """
        Increase the likelihood of cooperation in the next round.
        """
        self.cooperation_likelihood = min(0.95, self.cooperation_likelihood + 0.05)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        p_c = self.calculate_proportion_of_cooperators(history)
        delta = max(0, self.game_description.m - int(p_c * self.game_description.n_players))
        punishment_probability = min(delta / (self.game_description.n_players - self.game_description.m + 1), 1)
        if random.random() < punishment_probability:
            action = Action.D
        else:
            action = Action.C
        if self.calculate_collective_reward_achieved(history):
            self.increase_cooperation_likelihood()
        if random.random() < self.exploration_probability:
            action = Action(random.choice([0, 1]))
        return action

class Strategy_COLLECTIVE_563(BaseStrategy):
    """
    The ACT strategy aims to balance individual self-interest with collective risk aversion by adaptively responding to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.success_count = 0
        self.failure_count = 0
        self.p_coop = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1]
        prev_round_payoff = sum(history.my_payoffs)
        if prev_round_payoff >= self.game_description.m:
            self.success_count += 1
        else:
            self.failure_count += 1
        if self.success_count > self.failure_count:
            self.p_coop = min(self.p_coop + 0.1, 0.9)
        elif self.failure_count > self.success_count:
            self.p_coop = max(self.p_coop - 0.1, 0.1)
        if random.random() < self.p_coop and state.round_number != self.game_description.n_rounds:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_564(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy for Collective Risk Dilemma.
    
    This strategy prioritizes collective success while being robust to varying opponent behaviors. 
    It balances exploration and exploitation by adjusting cooperation probability based on previous outcomes.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 1 / self.game_description.n_players else Action.D
        previous_outcome = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history is not None else False)
        if self.cooperation_probability is None:
            self.cooperation_probability = 0.5
        if previous_outcome >= self.game_description.m:
            self.cooperation_probability = min(1, self.cooperation_probability + 0.1)
        else:
            self.cooperation_probability = max(0.1 / self.game_description.n_players, self.cooperation_probability - 0.2)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if previous_outcome >= self.game_description.m - 1 else Action.D
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_565(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        elif state.round_number <= self.game_description.n_rounds // 2:
            opponent_actions = np.concatenate((history.opponent_actions, history.my_actions[:, None]), axis=1)
            self.acr = np.mean(opponent_actions[:state.round_number].flatten())
            return random.choice([Action.C, Action.D]) if state.round_number == 1 else Action.C
        elif self.acr >= 0.5:
            probability_cooperate = 1 - (1 - self.game_description.m / self.game_description.n_players)
            return Action.C if np.random.rand() < probability_cooperate else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_566(BaseStrategy):
    """
    A collective strategy for the Collective Risk Dilemma game that balances individual self-interest with the need to achieve collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [True])
        k = self.game_description.k
        if state.round_number < self.game_description.n_rounds - 1:
            if prev_cooperators >= self.game_description.m:
                return Action.C
            else:
                p = (k - 1) / k
                return Action.C if random.random() < p else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            prev_defectors = sum((not action for action in history.opponent_actions[-1, :])) + (not history.my_actions[-1])
            if prev_cooperators >= self.game_description.m:
                return Action.C
            elif prev_defectors == 0:
                return Action.D
            else:
                return Action.D

class Strategy_COLLECTIVE_567(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Risk Awareness.
    
    This strategy prioritizes cooperation while being aware of the risks associated 
    with not meeting the threshold. By cooperating initially and adapting based on 
    the group's behavior, this strategy encourages others to cooperate while minimizing 
    personal losses when necessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate_t = 0.0
        self.m_t = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            self.cooperation_rate_t = np.mean(history.opponent_actions[-1, :] + history.my_actions[-1])
            if state.round_number == self.game_description.n_rounds - 2:
                if self.cooperation_rate_t >= 0.5:
                    return Action.C
                else:
                    return Action.D
            elif self.cooperation_rate_t >= 0.5:
                return Action.C
            elif self.cooperation_rate_t < 0.5 and np.sum(history.opponent_actions[-1, :] + history.my_actions[-1]) < self.game_description.m:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_568(BaseStrategy):
    """
    Collective Risk Dilemma strategy: Adaptive Cooperative Threshold (ACT).
    
    This strategy aims to balance individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_exp = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_cooperators > self.m_exp:
            self.m_exp -= 1
        elif prev_cooperators < self.m_exp:
            self.m_exp += 1
        if self.m_exp > self.game_description.n_players / 2:
            return Action.D
        elif (self.game_description.n_rounds - state.round_number) / 2 >= self.m_exp and prev_cooperators >= self.game_description.m:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_569(BaseStrategy):
    """
    Collective Strategy: Adaptive Threshold Cooperativity (ATC)
    
    Balances individual self-interest with collective cooperation, adapting to historical cooperation rates and adjusting based on rewards.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        ACR = sum([1 if x else 0 for x in history.my_actions]) / len(history.my_actions)
        if ACR < self.game_description.m / self.game_description.n_players:
            prob_coop = self.game_description.m / self.game_description.n_players / (1 + (self.game_description.n_rounds - state.round_number) / self.game_description.n_rounds)
        else:
            prob_coop = self.game_description.m / self.game_description.n_players * (1 + (ACR - self.game_description.m / self.game_description.n_players))
        if self.game_description.k > 2:
            prob_coop *= 1.1
        if sum([1 if x else 0 for x in history.my_actions[-self.game_description.m:]]) < self.game_description.m:
            return Action.D if random.random() < (self.game_description.n_players - self.game_description.m) / self.game_description.n_players else Action.C
        return Action.C if random.random() < prob_coop else Action.D

class Strategy_COLLECTIVE_570(BaseStrategy):
    """
    ACDT - Adaptive Cooperation with Dynamic Threshold.
    
    Balances cooperation and defection based on game history and parameters.
    Prioritizes cooperation when others demonstrate willingness to cooperate, 
    adapting to changes in the game's dynamics.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_counter = -1
        self.cooperator_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is not None:
            self.round_counter += 1
        if self.round_counter == -1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] == True)
        self.cooperator_history.append(prev_cooperators)
        if len(self.cooperator_history) > 0:
            dt = self.game_description.m - 1 + (self.game_description.n_rounds - state.round_number) / self.game_description.n_rounds * (self.game_description.n_players - self.game_description.m)
            if prev_cooperators >= dt or state.round_number == self.game_description.n_rounds:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_571(BaseStrategy):
    """
    Collective Risk Dilemma Strategy: Adaptive Cooperation.
    
    This strategy aims to balance individual self-interest with collective well-being 
    by adapting to the evolving game dynamics. It initially cooperates, punishes 
    defection, and rewards cooperation to promote a cooperative environment while 
    protecting individual interests.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators_prev >= math.ceil(self.game_description.m / 2):
            return Action.C
        elif num_cooperators_prev < self.game_description.m / 2:
            return Action.D
        if self.game_description.k > 1.5:
            for round_num in range(state.round_number):
                num_cooperators = sum(history.opponent_actions[round_num, :]) + (history.my_actions[round_num] == True)
                if num_cooperators >= math.ceil(self.game_description.m / 2):
                    self.cooperation_probability += 0.1
            return random.choice([Action.C, Action.D], p=[self.cooperation_probability, 1 - self.cooperation_probability])
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators_prev >= math.ceil(self.game_description.m / 2):
                return Action.C
        if num_cooperators_prev == math.ceil(self.game_description.m / 2):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_572(BaseStrategy):
    """
    An adaptive collective strategy designed for the Collective Risk Dilemma game.
    Balances cooperation and defection to maximize individual payoffs while promoting collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        prev_round_actions = history.opponent_actions[-1, :] if history else []
        num_cooperators_prev = sum((1 for action in prev_round_actions if action))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if num_cooperators_prev >= self.game_description.m else Action.D
        elif num_cooperators_prev < self.game_description.m:
            return Action.D
        p = max(0.5, min(1, 1 - (self.game_description.n_players - self.game_description.m) / self.game_description.n_players * self.game_description.k))
        return Action.C if random.random() < p else Action.D

class Strategy_COLLECTIVE_573(BaseStrategy):
    """
    Adaptive Collective Risk Averter (ACRA) strategy.

    Encourages cooperation while avoiding risk by adapting to the collective cooperation level and defecting when necessary.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.x_prev = 0
        self.rounds_since_success = 0
        self.last_round_payoff = 0
        self.delta = 0.05
        self.tau = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            p_coop = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < p_coop else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        x_prev = opponent_cooperations / self.game_description.n_players
        if x_prev >= self.game_description.m / self.game_description.n_players:
            p_coop = 1 - self.delta
        else:
            p_coop = 0
        self.rounds_since_success += 1
        if self.rounds_since_success > self.tau:
            p_coop = 1 - self.delta
        if state.round_number == self.game_description.n_rounds - 1:
            expected_payoff_from_cooperation = (self.game_description.k + 1) / 2
            if self.last_round_payoff <= expected_payoff_from_cooperation:
                return Action.C
        return Action.C if random.random() < p_coop else Action.D

class Strategy_COLLECTIVE_574(BaseStrategy):
    """
    A strategy that adapts to the number of cooperators in the previous round.
    Cooperates initially and in rounds where the number of cooperators is greater than or equal to m.
    Defects with a probability p = (m - num_cooperators_prev_round) / n otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if self.num_cooperators_prev_round >= self.game_description.m:
            return Action.C
        else:
            p_defect = (self.game_description.m - self.num_cooperators_prev_round) / self.game_description.n_players
            return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_575(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Collective Risk Dilemma game.
    
    This strategy aims to balance individual self-interest with collective cooperation, 
    adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_t_minus_1 = 0
        self.alpha = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        if history is not None:
            self.cooperators_t_minus_1 = sum(history.opponent_actions[-1, :])
        if self.cooperators_t_minus_1 >= self.game_description.m:
            return Action.C if random.random() < (self.game_description.k - 1) / self.game_description.k + self.alpha else Action.D
        total_payoff = sum(history.my_payoffs)
        opponent_total_payoff = np.sum(np.sum(history.opponent_payoffs, axis=0))
        if state.round_number > 1:
            collective_payoff_t_minus_1 = total_payoff + opponent_total_payoff - (history.my_payoffs[-2] + np.sum(history.opponent_payoffs[-2, :]))
            self.alpha = max(0, min(1, self.alpha + 0.1)) if collective_payoff_t_minus_1 >= self.game_description.n_players * self.game_description.k else max(0, min(1, self.alpha - 0.1))
        if state.round_number == self.game_description.n_rounds:
            return Action.D if total_payoff >= self.game_description.n_players * self.game_description.k * (self.game_description.n_rounds - 1) else Action.C
        return Action.C if random.random() < (self.game_description.k - 1) / self.game_description.k + self.alpha else Action.D

class Strategy_COLLECTIVE_576(BaseStrategy):
    """
    An adaptive collective cooperation strategy that prioritizes group well-being by 
    adjusting the cooperation threshold based on observed behavior and payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.m
        self.avg_payoff = 0
        self.m_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 2:
            return Action.C
        if self.game_description.n_rounds == 2 and state.round_number == 1:
            return Action.D
        if history is not None:
            self.m_prev = sum(history.opponent_actions[-1, :])
        if history is not None and len(history.my_payoffs) > 0:
            self.avg_payoff = np.mean(history.my_payoffs)
            if self.avg_payoff < 1.5:
                self.coop_threshold -= 1
            elif self.avg_payoff > 2.5:
                self.coop_threshold += 1
        if self.m_prev >= self.coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_577(BaseStrategy):
    """
    A collective strategy that adapts to the game's history and parameters,
    balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
        if prev_round_cooperators >= self.game_description.m:
            return Action.C
        elif any(history.opponent_actions[-1, :]):
            return Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            cooperation_prob = (self.game_description.n_rounds - state.round_number) / self.game_description.n_rounds
            return Action.C if random.random() < cooperation_prob else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_578(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    This strategy balances individual self-interest with collective cooperation, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :] | np.array([True]))
        if prev_round_coops >= self.game_description.m:
            return Action.C
        p = 1 - self.game_description.k / (self.game_description.n_players + self.game_description.k)
        if random.random() < p:
            return Action.D
        else:
            return Action.C
        if state.round_number > 2 and all((sum(history.opponent_actions[i, :] | np.array([True])) < self.game_description.m for i in range(state.round_number - 3, state.round_number))):
            q = 1 - self.game_description.m / self.game_description.n_players
            if random.random() < q:
                return Action.D

class Strategy_COLLECTIVE_579(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    
    This strategy promotes collective cooperation by adapting to others' behavior,
    encouraging mutual success in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_quotient = 0.0
        self.punishment_probability = 0.7
        self.last_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.cooperation_quotient = opponent_cooperators / self.game_description.n_players
        self.last_round_cooperators = opponent_cooperators
        if self.cooperation_quotient >= self.game_description.m / self.game_description.n_players:
            return Action.C
        if self.game_description.k > 1.5:
            self.punishment_probability = max(0.2, 0.9 - 0.02 * state.round_number)
            if random.random() < self.punishment_probability:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_580(BaseStrategy):
    """
    Adaptive Collective Response strategy for the Collective Risk Dilemma.
    
    This strategy adapts based on the game's history and parameters, balancing cooperation with the need to defect when the threshold of cooperators is not met.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.0
        self.success_rate = 0.0
        self.defection_threshold = 0.5
        self.initial_rounds = 3
        self.round_number = 0
        self.cooperators_history = []
        self.successful_cooperations = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.round_number += 1
            return Action.C
        self.cooperators_history.append(sum(history.opponent_actions[-1, :]))
        if sum(self.cooperators_history[-self.initial_rounds:]) >= self.game_description.m * self.initial_rounds:
            self.successful_cooperations += 1
        self.round_number += 1
        if state.round_number > self.initial_rounds:
            self.cooperation_rate = np.mean(self.cooperators_history)
            self.success_rate = self.successful_cooperations / (state.round_number - self.initial_rounds + 1)
            if self.success_rate > 0.7:
                self.defection_threshold *= 0.9
            elif self.success_rate < 0.3:
                self.defection_threshold *= 1.1
            recent_cooperation_rates = np.mean(self.cooperators_history[-min(5, len(self.cooperators_history)):])
            if recent_cooperation_rates < 0.2 * self.game_description.n_players:
                return Action.D
        if self.cooperation_rate >= self.defection_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_581(BaseStrategy):
    """
    A strategy for the Collective Risk Dilemma game that balances individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_opponent_actions = history.opponent_actions[-1, :]
        previous_my_action = history.my_actions[-1]
        cooperators = sum(previous_opponent_actions) + (1 if previous_my_action else 0)
        if cooperators >= self.game_description.m:
            return Action.C
        p = math.pow(1 - self.game_description.k / (self.game_description.k + 1), (self.game_description.n_rounds - state.round_number) / self.game_description.n_rounds)
        if random.random() < p:
            return Action.D
        if cooperators == 0:
            q = 1 / self.game_description.n_players
            if random.random() < q:
                return Action.C
        if cooperators >= self.game_description.m - 1 and (not previous_my_action):
            return Action.C
        elif random.random() < 0.05:
            return random.choice([Action.C, Action.D])
        return Action.D

class Strategy_COLLECTIVE_582(BaseStrategy):
    """
    A hybrid approach combining reactive and proactive elements to adapt to various opponent behaviors in the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < min(self.game_description.n_rounds // 2, 3):
            return Action.C
        num_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev >= self.game_description.m:
            action = True
        else:
            cooperate_prob = (self.game_description.m - num_cooperators_prev) / self.game_description.n_players
            action = random.random() < cooperate_prob
        if state.round_number > 1 and num_cooperators_prev < self.game_description.m and history.my_actions[-1]:
            punish_prob = 0.5
            action = random.random() < punish_prob
        return Action.C if action else Action.D

class Strategy_COLLECTIVE_583(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy aims to balance individual self-interest 
    with collective cooperation, adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_actions = history.opponent_actions[-1]
        num_cooperators_prev = sum((1 for action in previous_round_actions if action))
        avg_defector_payoff_prev = (1 + self.game_description.k) / (self.game_description.n_players - num_cooperators_prev) if self.game_description.n_players - num_cooperators_prev > 0 else 0
        total_payoff_from_cooperating_prev = self.game_description.k if num_cooperators_prev >= self.game_description.m else 0
        if num_cooperators_prev >= math.floor(self.game_description.m / 2) and total_payoff_from_cooperating_prev > avg_defector_payoff_prev:
            return Action.C
        elif num_cooperators_prev < math.floor(self.game_description.m / 2) or avg_defector_payoff_prev >= total_payoff_from_cooperating_prev:
            return Action.D
        if all((not action for action in history.opponent_actions[-math.ceil(state.round_number / 2):])):
            return Action.C
        avg_cooperator_payoff_prev = (total_payoff_from_cooperating_prev + self.game_description.k) / num_cooperators_prev if num_cooperators_prev > 0 else 0
        return Action.C if avg_cooperator_payoff_prev > avg_defector_payoff_prev else Action.D

    def all_players_defected_consecutively(self, actions: NDArray[np.bool_], consecutive_rounds: int):
        return all((not action for action in actions[-consecutive_rounds:]))

class Strategy_COLLECTIVE_584(BaseStrategy):
    """
    Adaptive Collective Risk Management strategy.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0
        self.prev_coops = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            coop_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < coop_prob else Action.D
        if history is not None:
            prev_coops = sum(history.opponent_actions[-1, :])
            if prev_coops >= self.game_description.m:
                self.cooperation_threshold = self.game_description.m
            else:
                self.cooperation_threshold += 1
                self.cooperation_threshold = min(self.cooperation_threshold, self.game_description.n_players - 1)
            self.prev_coops = prev_coops
        if self.prev_coops is not None and (self.prev_coops >= self.cooperation_threshold or self.prev_coops >= self.game_description.m):
            return Action.C
        return Action.D if state.round_number < self.game_description.n_rounds - 1 else Action.C

class Strategy_COLLECTIVE_585(BaseStrategy):
    """
    A collective strategy called "Adaptive Threshold Reciprocity" (ATR) that balances individual self-interest with collective cooperation while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_exploration = 0.1
        self.q_restart_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players <= 5 else Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        prev_round_my_payoff = history.my_payoffs[-1]
        prev_round_opponent_cooperators = sum(prev_round_opponent_actions)
        prev_round_opponent_defectors = self.game_description.n_players - 1 - prev_round_opponent_cooperators
        if prev_round_opponent_cooperators > 0:
            prev_round_coop_avg_payoff = np.mean(history.opponent_payoffs[-1, prev_round_opponent_actions])
        else:
            prev_round_coop_avg_payoff = -math.inf
        if prev_round_opponent_defectors > 0:
            prev_round_defector_avg_payoff = np.mean(history.opponent_payoffs[-1, ~prev_round_opponent_actions])
        else:
            prev_round_defector_avg_payoff = math.inf
        if prev_round_coop_avg_payoff >= prev_round_defector_avg_payoff and prev_round_opponent_cooperators >= self.game_description.m - 1:
            return Action.C
        elif prev_round_coop_avg_payoff < prev_round_defector_avg_payoff or prev_round_opponent_cooperators < self.game_description.m - 1:
            return Action.D
        if random.random() < self.p_exploration:
            return Action.C
        if all(~prev_round_opponent_actions):
            return Action.C if random.random() < self.q_restart_coop else Action.D
        elif sum(prev_round_opponent_actions) == 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_586(BaseStrategy):
    """
    A collective risk dilemma strategy that adapts to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        my_payoff = self._calculate_my_payoff(history)
        avg_payoff = np.mean(history.opponent_payoffs[-1])
        if prev_cooperators >= self.game_description.m:
            return Action.C
        elif prev_cooperators < self.game_description.m and my_payoff < avg_payoff:
            return Action.D
        if random.random() < self.epsilon:
            return Action.C if random.random() < 0.5 else Action.D
        return Action.C

    def _calculate_my_payoff(self, history: PlayerHistory) -> float:
        last_round = len(history.my_actions) - 1
        cooperators_last_round = sum(history.opponent_actions[last_round, :]) + (1 if history.my_actions[last_round] else 0)
        if cooperators_last_round >= self.game_description.m:
            return 1.0 + self.game_description.k
        else:
            return 1.0 - int(history.my_actions[last_round])

class Strategy_COLLECTIVE_587(BaseStrategy):
    """
    Collective Risk Dilemma strategy that balances individual self-interest with collective risk management.
    Adapts to the game's history and opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2 or history is None:
            return Action.C
        elif 3 <= state.round_number <= self.game_description.n_rounds // 2:
            prev_coop = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
            if prev_coop >= self.game_description.m:
                return Action.C
            else:
                p = (self.game_description.m - 1) / (self.game_description.n_players - 1)
                return Action.D if random.random() < p else Action.C
        elif state.round_number > self.game_description.n_rounds // 2:
            prev_payoff = history.my_payoffs[-1] if len(history.my_payoffs) == state.round_number else 0
            if prev_payoff >= self.game_description.k:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_588(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        num_coop = sum(prev_round)
        if num_coop >= self.game_description.m and np.any(history.my_payoffs[-1] > 0):
            return Action.C
        elif num_coop > 0:
            coop_prob = num_coop / self.game_description.n_players
            return Action.C if random.random() < coop_prob else Action.D
        else:
            return Action.D if random.random() < 0.8 else Action.C

class Strategy_COLLECTIVE_589(BaseStrategy):
    """
    Adaptive Threshold Cooperation strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.success_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] == True else sum(history.opponent_actions[-1, :])
        self.success_flag = previous_cooperators >= self.game_description.m
        if state.round_number < self.game_description.n_rounds - 1:
            cooperate_prob = self.game_description.k / (self.game_description.k + 1) if self.success_flag else 1 - self.game_description.m / self.game_description.n_players
        else:
            cooperate_prob = 1
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_591(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        prev_defectors = self.game_description.n_players - 1 - prev_cooperators
        if prev_cooperators >= self.game_description.m:
            return Action.C
        elif prev_cooperators >= prev_defectors and prev_cooperators < self.game_description.m:
            return Action.D if random.random() < 1 / self.game_description.k else Action.C
        elif all((not action for action in history.opponent_actions[-1, :])):
            return Action.C if random.random() < 1 / self.game_description.k else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_592(BaseStrategy):
    """
    The Adaptive Cooperative Escalation (ACE) strategy prioritizes collective success by:
    - Encouraging initial cooperation to establish a baseline of trust.
    - Adapting to the group's behavior through a contribution threshold and punishment mechanism.
    - Escalating rewards when the collective achieves the desired outcome.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_threshold = self.game_description.m - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :])
        if prev_coop_count < self.coop_threshold:
            return Action.D
        if prev_coop_count >= self.coop_threshold:
            return Action.C
        if prev_coop_count >= self.game_description.m and self.game_description.k > 1:
            self.coop_threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_593(BaseStrategy):
    """
    A collective strategy called "Adaptive Collective Optimism" (ACO) that balances individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.x = 0
        self.pi_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.m >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        p = (self.game_description.m - self.x) / (self.game_description.n_players - self.x)
        if self.pi_prev >= self.game_description.k:
            p = 0.7 + (self.pi_prev - 1) / (self.game_description.k - 1)
        if history is not None:
            opponent_actions_last_round = history.opponent_actions[-1, :]
            self.x = sum(~opponent_actions_last_round)
            self.pi_prev = np.mean(history.my_payoffs[-1])
        action = Action.C if random.random() < p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            cumulative_payoff = sum(history.my_payoffs) + sum(np.sum(history.opponent_payoffs, axis=0))
            if cumulative_payoff <= self.game_description.n_players * self.game_description.k:
                return Action.C
        return action

class Strategy_COLLECTIVE_594(BaseStrategy):
    """
    The Adaptive Collective Risk (ACR) strategy balances individual self-interest with collective cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_prob = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperate_prob = self.game_description.m / self.game_description.n_players
        else:
            prev_round_coops = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            if prev_round_coops >= self.game_description.m:
                self.cooperate_prob = self.game_description.k / (self.game_description.k + 1)
            else:
                self.cooperate_prob = (self.game_description.n_players - self.game_description.m) / self.game_description.n_players
        if state.round_number > 1:
            prev_prev_round_coops = sum(history.opponent_actions[-2, :] + [history.my_actions[-2]])
            if prev_prev_round_coops >= self.game_description.m:
                self.cooperate_prob += 0.1
            else:
                self.cooperate_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            if any((sum(history.opponent_actions[round, :] + [history.my_actions[round]]) >= self.game_description.m for round in range(state.round_number))):
                return Action.C
            else:
                return Action.D
        if state.round_number > 1 and sum(history.opponent_actions[-2, :] + [history.my_actions[-2]]) >= self.game_description.m:
            deviating_player = next((i for i, action in enumerate(history.opponent_actions[-1, :]) if not action), None)
            if deviating_player is not None:
                self.cooperate_prob -= 0.2
        self.cooperate_prob = max(0, min(self.cooperate_prob, 1))
        return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_COLLECTIVE_595(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy for Collective Risk Dilemma.
    
    Balances individual self-interest with cooperation to achieve the community's goal of meeting the minimum cooperators needed (m).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_c = 0.8
        self.p_d = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_threshold_met = sum(history.opponent_actions[-1, :]) >= self.game_description.m
        successes = sum((1 for i in range(len(history.my_payoffs[:-1])) if sum(history.opponent_actions[i, :]) + (history.my_actions[i] == 1) >= self.game_description.m))
        failures = len(history.my_payoffs[:-1]) - successes
        if successes > failures:
            self.p_c += 0.1
            self.p_d -= 0.1
        else:
            self.p_c -= 0.1
            self.p_d += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            threshold_successes = sum((1 for i in range(len(history.my_payoffs[:-1])) if sum(history.opponent_actions[i, :]) + (history.my_actions[i] == 1) >= self.game_description.m))
            return Action.C if threshold_successes >= len(history.my_payoffs[:-1]) / 2 else Action.D
        if prev_threshold_met:
            return Action.C if random.random() < self.p_c else Action.D
        else:
            return Action.D if random.random() < self.p_d else Action.C

class Strategy_COLLECTIVE_596(BaseStrategy):
    """
    An adaptive collective strategy that balances individual self-interest with collective success.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.prev_coop_count = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_round_data = history.my_payoffs[state.round_number - 1]
        opponent_actions_prev_round = history.opponent_actions[state.round_number - 1]
        coop_count_t_minus_1 = sum(opponent_actions_prev_round) + (history.my_actions[state.round_number - 1] == Action.C)
        self.prev_coop_count = coop_count_t_minus_1
        if coop_count_t_minus_1 >= self.game_description.m:
            return Action.C
        elif self.prev_coop_count / self.game_description.n_players < self.game_description.m / self.game_description.n_players:
            return Action.D
        if prev_round_data > 1 + self.game_description.k / 2:
            self.coop_prob = min(1, self.coop_prob + 0.1)
        elif prev_round_data < 1 + self.game_description.k / 4:
            self.coop_prob = max(0, self.coop_prob - 0.1)
        if state.round_number == self.game_description.n_rounds - 1 and sum(opponent_actions_prev_round) > 0:
            return Action.C
        return Action.C if random.random() < self.coop_prob else Action.D

class Strategy_COLLECTIVE_597(BaseStrategy):
    """
    A strategy that adapts to the collective behavior of opponents by adjusting its cooperation threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        m_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if m_prev >= self.coop_threshold:
            action = Action.C
        else:
            action = Action.D
            if self.game_description.n_players - m_prev > self.game_description.n_players / 2:
                self.coop_threshold += 1
            elif m_prev > self.game_description.n_players / 2:
                self.coop_threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        return action

class Strategy_COLLECTIVE_598(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective success by adapting to the game's history and the behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + int(bool(self.game_description.n_players - history.my_payoffs.shape[0]))
        if prev_round_cooperators >= self.game_description.m / 2 and self.game_description.k > 1:
            return Action.C
        if prev_round_cooperators < self.game_description.m / 2:
            return Action.D
        elif prev_round_cooperators >= self.game_description.m - 1:
            if self.game_description.k > 2 * 1:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if state.round_number < 5 and prev_round_cooperators < self.game_description.m / 4:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_599(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperativity with Memory (ACM)
    
    This strategy balances individual payoff maximization with collective risk management,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_outcome = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.game_description.m == 1:
            return Action.C
        k_approx_one = math.isclose(self.game_description.k, 1)
        if prev_round_outcome >= self.game_description.m:
            p = (self.game_description.k - 1) / self.game_description.k if not k_approx_one else 0.5
            return Action.C if random.random() < p else Action.D
        else:
            p = 1 - (self.game_description.m - 1) / self.game_description.n_players
            return Action.D if random.random() < p else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_600(BaseStrategy):
    """
    Adaptive Cooperation with Threshold Adjustment (ACTA) Strategy.

    This strategy adapts to the game's history by adjusting its cooperation threshold based on the observed behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p = game_description.m / game_description.n_players
        self.delta = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        c_prev = opponent_cooperations / self.game_description.n_players
        if c_prev >= self.game_description.m / self.game_description.n_players:
            self.p = min(self.p + self.delta, 1)
        else:
            self.p = max(self.p - self.delta, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if c_prev >= self.game_description.m / self.game_description.n_players else Action.D
        return Action.C if random.random() < self.p else Action.D

class Strategy_EXPLOITATIVE_514(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Pursuer" (ATP)
    
    This strategy adapts to the number of cooperators in previous rounds, 
    aiming to encourage cooperation when the threshold is not met and exploit 
    cooperation when the threshold is met or exceeded.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_threshold = self.game_description.m
        k_factor = self.game_description.k
        if state.round_number == 0 or history is None:
            return random.choice([Action.C] * (m_threshold // n_players + (1 if m_threshold % n_players != 0 else 0)) + [Action.D] * (n_players - (m_threshold // n_players + (1 if m_threshold % n_players != 0 else 0))))
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators < m_threshold:
            return Action.D if random.random() > (k_factor - 1) / (n_players - m_threshold + 1) else Action.C
        elif prev_cooperators == m_threshold - 1:
            return Action.C if random.random() < (k_factor - 1) / (k_factor + 1) else Action.D
        else:
            return Action.C if random.random() < (k_factor - 1) / k_factor else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_515(BaseStrategy):
    """
    A strategy for Collective Risk Dilemma games, aiming to exploit opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_cooperators_needed = self.game_description.m
        reward_factor = self.game_description.k
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C if random.random() < m_cooperators_needed / n_players else Action.D
        previous_cooperation_rate = sum(history.opponent_actions[-1, :]) / (n_players - 1)
        current_round = state.round_number + 1
        if current_round <= total_rounds // 2:
            cooperate_probability = m_cooperators_needed / n_players
        elif current_round <= 3 * total_rounds // 4:
            if previous_cooperation_rate < m_cooperators_needed / n_players:
                return Action.D
            else:
                cooperate_probability = (reward_factor - 1) / reward_factor if reward_factor > 1.5 else 0
        else:
            cooperation_payoff = sum(history.my_payoffs[history.my_actions])
            defection_payoff = current_round - len(history.my_actions[history.my_actions == False])
            if cooperation_payoff >= defection_payoff:
                return Action.C
            else:
                return Action.D
        return Action.C if random.random() < cooperate_probability else Action.D

class Strategy_EXPLOITATIVE_516(BaseStrategy):
    """
    Exploitative strategy "Adaptive Cooperate-to-Exploit" (ACE) for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met = False
        self.punishment_count = 0
        self.adaptive_threshold = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents = history.opponent_actions.shape[1]
        coop_rate = np.mean(history.opponent_actions[-1, :])
        if coop_rate > 0.5:
            self.adaptive_threshold -= 1
        elif coop_rate < 0.3:
            self.adaptive_threshold += 1
        cooperators = np.sum(history.opponent_actions[-1, :] | (history.my_actions[-1] == Action.C.value))
        if cooperators >= self.adaptive_threshold:
            self.threshold_met = True
        else:
            self.threshold_met = False
        if self.punishment_count > 0 and history.my_actions[-1] == Action.C.value:
            action = Action.D
            self.punishment_count -= 1
        elif not self.threshold_met and history.my_actions[-1] == Action.C.value:
            self.punishment_count = self.game_description.m
            action = Action.D
        elif state.round_number < math.ceil(self.game_description.n_rounds / 4):
            action = Action.C
        elif self.threshold_met:
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = np.sum(history.my_payoffs)
            expected_defect_payoff = history.opponent_payoffs[-1, :].mean() + self.game_description.k
            if total_payoff > expected_defect_payoff:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_517(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while maintaining a strategic advantage.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players <= 1 or self.game_description.n_rounds <= 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_coops >= self.game_description.m:
            return Action.C
        else:
            defect_prob = (self.game_description.m - prev_coops) / self.game_description.m
            if math.isclose(defect_prob, 1):
                return Action.D
            elif math.isclose(defect_prob, 0):
                return Action.C
            return Action.D if random.random() < defect_prob else Action.C

class Strategy_EXPLOITATIVE_518(BaseStrategy):
    """
    A exploitative strategy for Collective Risk Dilemma that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.OCR = 0.0
        self.cooperative_players = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        exploration_threshold = self.game_description.m / n_players
        if state.round_number <= n_rounds // 4:
            if state.round_number == 1:
                return Action.C
            else:
                num_cooperators = sum(history.opponent_actions[-1, :])
                if num_cooperators >= self.game_description.m // 2:
                    return Action.C
                else:
                    return Action.D
        elif state.round_number <= 3 * n_rounds // 4:
            self.OCR = self.update_OCR(history)
            if self.OCR > exploration_threshold:
                return Action.C
            else:
                return Action.D
        else:
            self.cooperative_players = self.count_cooperative_players(history)
            if self.cooperative_players >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D

    def update_OCR(self, history: PlayerHistory):
        n_rounds = self.game_description.n_rounds
        OCR_window_size = n_rounds // 4
        opponent_cooperations = np.sum(history.opponent_actions[-OCR_window_size:, :], axis=0)
        OCR = np.mean(opponent_cooperations) / OCR_window_size
        return OCR

    def count_cooperative_players(self, history: PlayerHistory):
        m2 = self.game_description.m // 2
        n_rounds = self.game_description.n_rounds
        opponent_actions = history.opponent_actions[:n_rounds // 4 + 1, :]
        cooperative_players = np.sum(np.sum(opponent_actions, axis=0) >= m2)
        return cooperative_players

    def count_cooperative_players_original(self, round_number: int):
        count = 0
        for opponent in self.opponents:
            if opponent.cooperated_in_at_least_m2_rounds:
                count += 1
        return count

class Strategy_EXPLOITATIVE_519(BaseStrategy):
    """
    Exploitative strategy "Adaptive Collective Risk Taker" (ACRT) for the Collective Risk Dilemma game.
    
    This strategy adapts to opponents' cooperation rates and adjusts based on average payoffs, aiming to maximize individual payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_prob = None
        self.prev_coop_rate = None
        self.avg_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            self.cooperation_prob = m / n
            return Action.C if random.random() < self.cooperation_prob else Action.D
        prev_opponent_actions = history.opponent_actions[-1, :]
        self.prev_coop_rate = sum(prev_opponent_actions) / n
        if self.prev_coop_rate >= m / n:
            self.cooperation_prob = k / (k + 1)
        else:
            self.cooperation_prob = 0
        self.avg_payoff = np.mean(history.my_payoffs)
        if self.avg_payoff > 1 + k / 2:
            self.cooperation_prob = min(self.cooperation_prob + 0.1, k / (k + 1))
        elif self.avg_payoff < 1 + k / 4:
            self.cooperation_prob = max(self.cooperation_prob - 0.1, m / n)
        if state.round_number == r - 1:
            return Action.D
        return Action.C if random.random() < self.cooperation_prob else Action.D

class Strategy_EXPLOITATIVE_520(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_payoffs = history.opponent_payoffs[-1, :]
        prev_round_cooperators = sum(prev_round_actions)
        if prev_round_cooperators >= self.game_description.m:
            cooperate = False
        else:
            cooperate = self.game_description.k > 1
        m_min = max(0, self.game_description.m - sum(prev_round_actions))
        if prev_round_cooperators == self.game_description.m and m_min > 0:
            first_m_min_players = np.where(prev_round_actions[:m_min])[0]
            if len(first_m_min_players) > 0:
                cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = True
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_521(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances cooperation with the risk of exploitation.
    It adapts to the game's history and opponent behavior, increasing or decreasing its cooperation probability accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_outcome = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] >= self.game_description.m
        if prev_outcome:
            self.cooperation_probability = min(0.9, self.cooperation_probability + 0.1)
        else:
            self.cooperation_probability = max(0.1, self.cooperation_probability - 0.2)
        opponent_coop_rate = np.mean(history.opponent_actions[-5:, :])
        if opponent_coop_rate > 0.8:
            self.cooperation_probability += 0.05
        elif opponent_coop_rate < 0.2:
            self.cooperation_probability -= 0.1
        return Action.C if random.random() < self.cooperation_probability else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_522(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] * 1
        if prev_coop_count >= self.game_description.m - 1:
            return Action.C
        elif prev_coop_count < self.game_description.m - 1:
            return Action.D
        elif prev_coop_count == self.game_description.m - 1 and (not history.my_actions[-1]):
            return Action.D
        elif prev_coop_count >= self.game_description.m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_coop_count < self.game_description.m:
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_523(BaseStrategy):
    """
    Adaptive Collective Risk Explorer (ACRE) strategy.

    This strategy initially explores opponent behavior by cooperating in the first two rounds.
    It then assesses the cooperation level and adapts its decision-making based on the observed behavior.
    The goal is to balance cooperation with exploitation, maximizing individual payoff while minimizing risk.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.m_avg = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        m_current = sum(history.opponent_actions[-1, :] != False)
        self.m_avg = self.alpha * self.m_avg + (1 - self.alpha) * m_current
        if self.game_description.n_players == 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        p_defect = 0 if self.m_avg < 1 else self.m_avg >= self.game_description.m and 1 - self.m_avg / self.game_description.n_players or random.random()
        return Action.C if random.random() > p_defect else Action.D

class Strategy_EXPLOITATIVE_524(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Threshold Enforcement (ATE)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum((1 for action in (history.opponent_actions[-1] if history else []) + [Action.D] if action))
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        if prev_round_cooperators < self.game_description.m:
            return Action.D
        if prev_round_cooperators == self.game_description.n_players:
            return Action.D
        elif prev_round_cooperators >= self.game_description.m:
            p = self.game_description.k * prev_round_cooperators / (self.game_description.n_players * self.game_description.k)
            return Action.C if random.random() < p else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_525(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances cooperation and self-interest by adapting to the game's history and opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_round_outcome = sum(history.opponent_actions[-1, :] == True)
        if prev_round_outcome >= self.game_description.m:
            self.cooperation_probability = min(0.9, self.cooperation_probability + 0.1)
        else:
            self.cooperation_probability = max(0.1, self.cooperation_probability - 0.1)
        for opponent in history.opponent_actions[-1, :]:
            if not opponent and prev_round_outcome >= self.game_description.m:
                return Action.D if random.random() < 0.5 else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.cooperation_probability + (self.cooperation_probability == 0.5) * 0.01 else Action.D

class Strategy_EXPLOITATIVE_526(BaseStrategy):
    """
    An exploitative and adaptive strategy for the Collective Risk Dilemma game.
    
    This strategy aims to exploit other players' cooperative tendencies while 
    minimizing its own contributions to the community project. It initially 
    cooperates randomly, then adapts based on previous rounds, gathering information 
    about others' strategies and adjusting its behavior accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        m_threshold = self.game_description.m
        k_factor = self.game_description.k
        r_rounds = self.game_description.n_rounds
        if n_players == 2 and m_threshold == 1:
            return Action.C if state.round_number < r_rounds - 1 else Action.D
        elif r_rounds == 2:
            return Action.C if random.random() < 0.5 else Action.D
        if history is None or len(history.my_actions) == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number < r_rounds - 1:
            p_C = k_factor / (k_factor + 1) if prev_round_cooperators >= m_threshold else 0.1
            return Action.C if random.random() < p_C else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_527(BaseStrategy):
    """
    A strategic approach for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.C_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        self.C_count = sum(history.opponent_actions[-1, :] != False)
        if self.game_description.n_players == 2:
            action = Action.C
        elif state.round_number < self.game_description.n_rounds:
            if self.C_count >= self.game_description.m - 1 or (self.game_description.k > 2 and self.C_count >= self.game_description.m - 2):
                action = Action.C
            else:
                action = Action.D
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_528(BaseStrategy):
    """
    An exploitative strategy that adapts to opponent cooperation rates and exploits favorable situations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.w = 3
        self.θ = 0.6
        self.prev_coops = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_coops_prev = sum(history.my_actions[:state.round_number - 1]) if len(self.prev_coops) > self.w else sum(self.prev_coops)
        if num_coops_prev < self.game_description.m:
            return Action.D
        avg_coop_rate = np.mean(self.prev_coops[-self.w:]) if len(self.prev_coops) >= self.w else np.mean(self.prev_coops)
        p = 0.5
        if avg_coop_rate > self.θ:
            p = 1 - (self.game_description.k - 1) / self.game_description.k
        action = Action.C if random.random() < p else Action.D
        self.prev_coops.append(action == Action.C)
        if len(self.prev_coops) > self.w:
            self.prev_coops.pop(0)
        if action == Action.C and sum(history.my_actions[:state.round_number - 1]) + 1 >= self.game_description.m:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_529(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_coop = 0
        self.num_defect = 0
        self.defectors = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1]
        self.num_coop = sum(opponent_actions_last_round)
        self.num_defect = self.game_description.n_players - 1 - self.num_coop
        for i, opponent in enumerate(history.opponent_actions):
            if np.mean(opponent) < 0.5:
                self.defectors.append(i)
        coop_threshold = self.game_description.m / self.game_description.n_players
        p_c = 1 - self.num_defect / (self.game_description.n_players - 1) if self.num_coop >= coop_threshold else (self.game_description.m - self.num_coop) / (self.game_description.n_players - 1)
        action = Action.C if random.random() < p_c else Action.D
        for opponent in self.defectors:
            if self.num_coop < coop_threshold and random.random() < 0.8:
                action = Action.D
        if state.round_number > self.game_description.n_rounds - 2 or state.round_number == self.game_description.n_rounds - 1:
            action = Action.C if random.random() < 0.8 else Action.D
        return action

class Strategy_EXPLOITATIVE_530(BaseStrategy):
    """
    The Adaptive Exploiter strategy for Collective Risk Dilemma games.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_action = None
        self.cooperators = 0
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.cooperators = sum(history.opponent_actions[-1, :]) + int(self.previous_action == Action.C)
            self.total_payoff = np.sum(history.my_payoffs)
        if state.round_number <= 3:
            if state.round_number == 1:
                return Action.C
            elif self.cooperators >= math.ceil(self.game_description.m / 2):
                return Action.C
            else:
                return Action.D
        if self.total_payoff < self.game_description.n_players and self.cooperators < self.game_description.m:
            if self.previous_action == Action.D:
                return Action.D
            elif self.previous_action == Action.C and self.cooperators >= math.ceil(self.game_description.m / 2):
                return Action.C
            else:
                return Action.D
        elif self.total_payoff >= self.game_description.n_players and self.cooperators >= self.game_description.m:
            if self.previous_action == Action.D:
                return Action.C
            elif self.previous_action == Action.C and self.cooperators < self.game_description.m:
                return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if self.total_payoff < self.game_description.n_players:
                return Action.D
            elif self.previous_action == Action.D:
                return Action.C
        self.previous_action = Action.D if random.random() < 0.5 else Action.C
        return self.previous_action

class Strategy_EXPLOITATIVE_531(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        overall_cooperation_rate = np.mean(opponent_cooperations[-1] > 0)
        cooperation_rate = np.mean(np.any(history.opponent_actions[:, :], axis=0))
        success_rate = np.mean(np.sum(history.opponent_actions, axis=1) >= self.game_description.m)
        opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        opponent_cooperation_rate = np.mean(opponent_cooperation_rates > overall_cooperation_rate)
        if cooperation_rate >= 0.5 and success_rate > 0.2 and (opponent_cooperation_rate >= overall_cooperation_rate):
            return Action.C
        elif cooperation_rate < 0.5 and success_rate <= 0.2 or opponent_cooperation_rate < overall_cooperation_rate:
            return Action.D
        else:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_532(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances cooperation and exploitation by initially cooperating,
    defecting when the threshold is not met or when there's an opportunity to exploit over-cooperation,
    punishing itself for defecting when the community project succeeds without its contribution,
    and cooperating in the last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + self.my_last_action(history)
        if prev_cooperators < self.m:
            return Action.D
        elif abs(prev_cooperators - self.m) <= 1:
            return Action.D
        elif self.my_last_action(history) == Action.D and prev_cooperators >= self.m:
            return Action.C
        else:
            if state.round_number + 1 == self.game_description.n_rounds:
                return Action.C
            return Action.C

    def my_last_action(self, history):
        last_action = bool(history.my_actions[-1])
        return Action.C if last_action else Action.D

class Strategy_EXPLOITATIVE_533(BaseStrategy):
    """
    Exploitative strategy aiming to maximize payoff by adapting to changing circumstances.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return Action.D
        elif state.round_number < r // 2:
            prev_coops = sum(history.opponent_actions[-1, :])
            p = min(1, m / n)
            if prev_coops < m and random.random() < p:
                return Action.C
            else:
                return Action.D
        elif state.round_number >= r // 2:
            avg_payoff = np.mean(history.opponent_payoffs[-1, :])
            if avg_payoff > 1 + k / 2:
                p = min(1, m / n)
                return Action.C if random.random() < p else Action.D
            else:
                return Action.D
        elif state.round_number == r - 1:
            return Action.D
        elif n % 2 == 1 and prev_coops == m - 1:
            if history.opponent_actions[-1, :].sum() == m - 1:
                return Action.C
        elif prev_coops == m - 1 and (~history.opponent_actions[-1, :]).sum() == m - 1:
            return Action.C if state.round_number > 0 and history.my_actions[state.round_number - 1] == False else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_534(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.5
        self.threshold_met = False
        self.opponent_cooperation_tendency = [0.5] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.m <= self.game_description.n_players / 2 else Action.D
        previous_round = state.round_number - 1
        opponent_actions = history.opponent_actions[previous_round]
        self.cooperation_rate = np.mean(opponent_actions)
        self.threshold_met = sum(opponent_actions) >= self.game_description.m
        self.opponent_cooperation_tendency = [tendency * 0.7 + action * 0.3 for tendency, action in zip(self.opponent_cooperation_tendency, opponent_actions)]
        if self.threshold_met and self.cooperation_rate > 0.6:
            return Action.C
        elif sum((1 for tendency in self.opponent_cooperation_tendency if tendency < 0.4)) >= self.game_description.n_players // 2:
            return Action.D
        else:
            high_tendency_opponents = sum((1 for tendency in self.opponent_cooperation_tendency if tendency > 0.6))
            return Action.C if high_tendency_opponents >= self.game_description.m / 2 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if not self.threshold_met:
                return Action.D
            return Action.C

class Strategy_EXPLOITATIVE_535(BaseStrategy):
    """
    A robust and exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = [0] * game_description.n_rounds
        self.success_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = self.cooperate_count[state.round_number - 1]
        if prev_coop_count < self.game_description.m:
            return Action.D
        coop_trend = self.cooperate_count[state.round_number - 1] - self.cooperate_count[state.round_number - 2]
        if state.round_number < self.game_description.n_rounds // 2:
            if coop_trend >= 0:
                return Action.C
            else:
                return Action.D
        self.success_count += self.cooperate_count[state.round_number - 1] >= self.game_description.m
        if self.success_count > 0.5 * state.round_number:
            return Action.C
        else:
            return Action.D
        if state.round_number > 3 * self.game_description.n_rounds // 4:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None:
            self.cooperate_count[state.round_number] = sum(history.opponent_actions[-1, :]) + int(bool(self))

class Strategy_EXPLOITATIVE_536(BaseStrategy):
    """
    Exploitative strategy that adaptively pursues the threshold of minimum cooperators needed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        if state.round_number < min(self.game_description.n_rounds // 2, 5):
            num_cooperators_last_round = sum(history.opponent_actions[-1, :])
            if num_cooperators_last_round < self.game_description.m:
                return Action.C
            else:
                return Action.D
        self.avg_coop = np.mean(np.sum(history.opponent_actions, axis=1) + history.my_actions[:state.round_number])
        if self.avg_coop < self.game_description.m and state.round_number != self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_537(BaseStrategy):
    """
    Exploitative strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.exploration_phase = True
        self.cooperators_observed = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.exploration_phase:
            if state.round_number < min(5, math.ceil(math.sqrt(self.game_description.n_rounds))):
                return Action.D if state.round_number % 2 == 0 else Action.C
            self.exploration_phase = False
            self.cooperators_observed = np.any(np.sum(history.opponent_actions[:min(5, math.ceil(math.sqrt(self.game_description.n_rounds))), :], axis=1) >= self.game_description.m)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if len([p for p in history.opponent_actions[-1, :] if p]) < self.game_description.m:
            return Action.D
        if self.cooperators_observed or (len(history.my_actions) > 0 and np.any(history.my_payoffs[:-1] > 0)):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_538(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Threshold Pursuer (ATP)
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.num_cooperators_prev_round = None
        self.round_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        elif t < math.floor(r / 2):
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round < m:
                return Action.D
            else:
                return Action.C
        elif t >= math.floor(r / 2):
            avg_payoff = np.mean(self.round_payoffs[-5:]) if len(self.round_payoffs) >= 5 else np.mean(self.round_payoffs)
            threshold = 1.5
            if avg_payoff < threshold:
                return Action.D
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round >= m:
                return Action.C
            else:
                return Action.D
        elif t > math.floor(3 * r / 4):
            total_payoff = sum(self.round_payoffs) if self.round_payoffs else 0
            threshold = 2 * n * r
            if total_payoff < threshold:
                return Action.C
            else:
                return Action.D
        elif t == r - 1:
            total_payoff = sum(self.round_payoffs) if self.round_payoffs else 0
            threshold = 3 * n * r
            if total_payoff >= threshold:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_539(BaseStrategy):
    """
    An adaptive strategy that initially encourages cooperation and then exploits opponents' vulnerabilities.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.defectors = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_coop = sum(np.sum(history.opponent_actions[:state.round_number, :], axis=1)) + (history.my_actions[:state.round_number] == True).sum()
        self.avg_coop = total_coop / (self.game_description.n_players * state.round_number)
        if self.avg_coop >= self.game_description.m:
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if not opponent_action and i not in self.defectors:
                    self.defectors.append(i)
            if any((defector in np.where(~history.opponent_actions[-1, :])[0] for defector in self.defectors)):
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_540(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponent's behavior by cooperating when enough opponents cooperate and defecting otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        prev_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
        opponent_cooperation_rate = prev_cooperators / (self.game_description.n_players - 1)
        if prev_cooperators >= self.game_description.m:
            return Action.C
        elif opponent_cooperation_rate > self.game_description.m / self.game_description.n_players:
            return Action.D
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if not action and prev_cooperators >= self.game_description.m:
                return Action.D
        return Action.C if random.random() < 1 / self.game_description.n_players else Action.D

class Strategy_EXPLOITATIVE_541(BaseStrategy):
    """
    A strategy that balances cooperation and defection to maximize payoffs while adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_count = 0
        self.adaptive_m = game_description.m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.cooperators_count += sum(history.opponent_actions[-1, :])
        if self.cooperators_count <= self.game_description.m:
            self.adaptive_m = self.game_description.m
        else:
            self.adaptive_m = min(self.game_description.m + 1, self.game_description.n_players)
        previous_payoff = history.my_payoffs[-1]
        if previous_payoff >= self.game_description.k * 1 and self.cooperators_count <= self.adaptive_m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if self.cooperators_count < self.game_description.m:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_542(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to effectively balance cooperation and exploitation.
    It adapts to the game's progression while remaining robust against a wide range of opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.average_payoff_last_3_rounds = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        cooperation_rate = opponent_cooperations / self.game_description.n_players
        self.cooperation_rate_history.append(cooperation_rate)
        if state.round_number <= 5:
            if cooperation_rate < self.game_description.m / self.game_description.n_players:
                return Action.D
            else:
                return Action.C
        my_payoffs = history.my_payoffs[-3:]
        opponent_payoffs = np.mean(history.opponent_payoffs[-3:, :], axis=0)
        if len(my_payoffs) == 3:
            self.average_payoff_last_3_rounds = np.mean(my_payoffs)
        if self.average_payoff_last_3_rounds > 1:
            return Action.C
        else:
            return Action.D
        if state.round_number >= self.game_description.n_rounds - 2:
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            elif opponent_payoffs > 1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_543(BaseStrategy):
    """
    A strategy that adapts to the game's parameters and history to exploit opponents.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif prev_cooperators >= self.game_description.m:
            p_coop = (self.game_description.k - 1) / self.game_description.k
            if history.opponent_actions.shape[0] > 1 and any(history.opponent_actions[-2, :] != history.opponent_actions[-1, :]):
                return Action.D
            else:
                return random.choices([Action.C, Action.D], weights=[p_coop, 1 - p_coop])[0]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_544(BaseStrategy):
    """
    Adaptive Exploitation strategy aims to maximize its payoff by adaptively exploiting the behavior of other players while minimizing the risk of contributing to a failed collective project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.1
        self.p_c = 0.5
        self.num_cooperators = [None] * (game_description.n_rounds + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = self.num_cooperators[state.round_number - 1] or 0
        prev_p_c = self.p_c
        self.p_c = self.game_description.m / self.game_description.n_players * self.alpha + (1 - self.alpha) * prev_p_c
        m_threshold = min(self.game_description.m, self.game_description.n_players)
        if self.p_c < 0.5:
            return Action.D
        elif prev_round_cooperators < m_threshold:
            return Action.C
        else:
            return Action.D
        self.num_cooperators[state.round_number] = sum(history.opponent_actions[-1, :])

class Strategy_EXPLOITATIVE_545(BaseStrategy):
    """
    Adaptive Threshold Tracker strategy.
    
    This strategy starts by cooperating to test opponents' willingness to cooperate. 
    It then adapts its actions based on the number of cooperators in previous rounds, 
    aiming to reinforce existing cooperation while protecting individual interests.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.opponent_coop_count = [0] * game_description.n_players
        self.opponent_defect_count = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :])
        if prev_coop_count < 0.8 * self.game_description.m or sum(self.opponent_defect_count) > self.game_description.n_players - self.game_description.m:
            return Action.D
        opponent_coop_rates = [coop / state.round_number for coop in self.opponent_coop_count]
        if any((coop_rate > 0.75 for coop_rate in opponent_coop_rates)):
            return Action.D
        if prev_coop_count >= 0.8 * self.game_description.m:
            return Action.C
        return Action.D

    def update(self, history: PlayerHistory):
        for i in range(self.game_description.n_players):
            if history.opponent_actions[-1, i]:
                self.opponent_coop_count[i] += 1
            else:
                self.opponent_defect_count[i] += 1

class Strategy_EXPLOITATIVE_546(BaseStrategy):
    """
    A strategic approach for the Collective Risk Dilemma game that is adaptive and exploitative.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.k = game_description.k
        self.n = game_description.n_players
        self.risk_assessment_threshold = 0.5
        self.prev_payoff = None
        self.cooperation_ratio = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.n <= 2 * self.m else Action.D
        prev_round_outcome = self._evaluate_prev_round(history)
        if prev_round_outcome == 'fewer_than_m_cooperated':
            if self.prev_payoff is not None and self.prev_payoff >= self.k:
                return Action.C
            else:
                return Action.D
        elif prev_round_outcome == 'at_least_m_cooperated':
            self.cooperation_ratio = self._update_cooperation_ratio(history, self.n)
            if self._should_defect_due_to_low_cooperation():
                return Action.D
            elif self.prev_payoff is not None and self.prev_payoff >= self.k:
                return Action.C
            else:
                return Action.D

    def _evaluate_prev_round(self, history: PlayerHistory) -> str:
        prev_cooperators = sum(history.opponent_actions[-1, :] == 1)
        if prev_cooperators < self.m:
            return 'fewer_than_m_cooperated'
        else:
            return 'at_least_m_cooperated'

    def _update_cooperation_ratio(self, history: PlayerHistory, n_players: int) -> float:
        prev_cooperators = sum(history.opponent_actions[-1, :] == 1)
        cooperation_ratio = prev_cooperators / n_players
        return cooperation_ratio

    def _should_defect_due_to_low_cooperation(self) -> bool:
        if self.cooperation_ratio is None or self.prev_payoff is None:
            return False
        prev_prev_cooperators = sum(history.opponent_actions[-2, :] == 1)
        prev_prev_cooperation_ratio = prev_prev_cooperators / self.n
        return self.cooperation_ratio < self.risk_assessment_threshold and prev_prev_cooperation_ratio < self.risk_assessment_threshold

class Strategy_EXPLOITATIVE_547(BaseStrategy):
    """
    A strategic approach for the Collective Risk Dilemma game that focuses on adaptability and exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.floor(game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [Action.C])
        previous_round_reward = self.calculate_payoff(previous_round_cooperators)
        if previous_round_cooperators < self.game_description.m:
            return Action.D
        current_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_reward > 0 and current_cooperators >= self.cooperation_threshold:
            return Action.C
        if previous_round_reward > 0 and current_cooperators > math.floor(self.game_description.n_players / 2):
            self.cooperation_threshold -= 1
        else:
            self.cooperation_threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

    def calculate_payoff(self, cooperators: int) -> float:
        """
        Calculate the payoff for a given number of cooperators.
        """
        if cooperators >= self.game_description.m:
            return self.game_description.k
        else:
            return 0.0

class Strategy_EXPLOITATIVE_548(BaseStrategy):
    """
    Exploitative strategy that adapts to the number of cooperators in previous rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.previous_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.previous_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        threshold = math.ceil(self.game_description.m / 2)
        if state.round_number < self.game_description.n_rounds - 1:
            if self.previous_cooperators >= threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_549(BaseStrategy):
    """
    The Adaptive Exploiter strategy for Collective Risk Dilemma games.
    
    It starts by cooperating in the first round, then adapts to opponents' behavior,
    taking into account their cooperativeness and adjusting its own cooperation threshold accordingly.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_cooperators = None
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round = history.opponent_actions[-1]
        if self.prev_cooperators is None:
            self.prev_cooperators = sum(prev_round)
        else:
            self.prev_cooperators = sum(history.my_actions[:-1]) + sum(prev_round)
        avg_payoff = (self.avg_payoff * 2 + state.round_number) / 3
        if history is not None and len(history.opponent_payoffs):
            payoffs = np.mean(history.opponent_payoffs, axis=1)
            avg_payoff = np.mean(payoffs)
        threshold = math.ceil(self.game_description.m / 2)
        if avg_payoff > (1 + self.game_description.k) / 2:
            new_threshold = math.ceil(self.game_description.m / 3)
            if self.prev_cooperators >= new_threshold:
                return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.C if self.prev_cooperators >= self.game_description.m else Action.D
        if self.prev_cooperators >= threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_550(BaseStrategy):
    """
    Exploitative strategy that adapts to the behavior of other players while maintaining a focus on exploiting their cooperative tendencies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate = True
        self.prev_cooperators = []
        self.avg_defector_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        self.prev_cooperators.append(num_cooperators_prev_round)
        if num_cooperators_prev_round >= self.game_description.m:
            defector_payoffs = np.array([payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if not action])
            avg_defector_payoff = np.mean(defector_payoffs) if len(defector_payoffs) > 0 else 0
            self.avg_defector_payoff = avg_defector_payoff
            if self.game_description.k > self.avg_defector_payoff:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_551(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy for Collective Risk Dilemma.
    
    ATE initially explores opponents' behaviors by cooperating. It then adapts to changes in the environment,
    defecting when the threshold is unlikely to be reached and cooperating otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_C = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if state.round_number > 1 and len(history.opponent_actions) > 1:
            self.avg_C = (self.avg_C * (state.round_number - 1) + sum(history.opponent_actions[-2, :])) / state.round_number
        if self.avg_C == 0:
            return Action.D
        if math.isclose(self.avg_C, self.game_description.m - 1):
            return Action.C if (self.game_description.n_rounds - state.round_number) % 2 else Action.D
        if self.avg_C >= self.game_description.m - 1:
            return Action.C
        elif self.avg_C < self.game_description.m - 1 and state.round_number != self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_552(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors in the Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_coop = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < min(self.game_description.n_rounds // 2, 3):
            return Action.C
        num_coop_prev = sum(history.opponent_actions[-1, :])
        if num_coop_prev >= self.game_description.m:
            action = Action.C if random.random() < 0.8 else Action.D
        elif num_coop_prev in (self.game_description.m - 1, self.game_description.m - 2):
            action = Action.D if random.random() < 0.8 else Action.C
        elif sum(history.opponent_actions[-1, :]) > self.game_description.n_players // 3 and sum(history.opponent_actions[-2, :]) > self.game_description.n_players // 3:
            self.p_coop += 0.1
            action = Action.C if random.random() < self.p_coop else Action.D
        else:
            action = Action.C if random.random() < self.p_coop else Action.D
        if state.round_number >= max(self.game_description.n_rounds // 2, 2):
            return Action.C if random.random() < 0.8 else Action.D
        return action

class Strategy_EXPLOITATIVE_553(BaseStrategy):
    """
    Adaptive Cooperative Punisher Strategy for Collective Risk Dilemma.
    
    Exploits opponents' behavior while adapting to changing circumstances 
    and maintaining robust performance across various opponent strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = max(0.1, game_description.m - 1)
        self.few_cooperators_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_actions = history.opponent_actions[-1, :]
        my_prev_action = history.my_actions[-1]
        prev_C = sum(prev_actions) + int(my_prev_action)
        self.few_cooperators_rounds += 1 if prev_C < self.game_description.m else 0
        self.cooperation_threshold = max(0.1, self.game_description.m - self.few_cooperators_rounds / state.round_number)
        if my_prev_action and prev_C >= self.cooperation_threshold:
            return Action.C
        elif not my_prev_action and prev_C < self.game_description.m:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and prev_C >= self.game_description.m - 1:
            return Action.C
        elif my_prev_action and prev_C < self.cooperation_threshold:
            return Action.D
        consecutive_defections = sum((1 for i in range(-2, 0) if not history.my_actions[i]))
        if consecutive_defections >= 2 and prev_C < self.game_description.m:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_554(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_count = 0
        self.defect_count = 0
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        m = self.game_description.m
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number <= 1:
            return Action.C
        elif 2 < state.round_number <= r // 2:
            cooperators = sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == True)
            if cooperators < m:
                return Action.D
            else:
                p = (m - 1) / (n - 1)
                return Action.C if random.random() < p else Action.D
        elif state.round_number > r // 2:
            if self.total_payoff > 1.5 * k:
                return Action.C
            elif self.total_payoff < k or state.round_number == r - 1:
                return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_555(BaseStrategy):
    """
    Exploitative strategy that adapts to the evolving behavior of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_est = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        else:
            opponent_cooperations = np.sum(history.opponent_actions[-1, :])
            self.m_est = min(max(self.m_est + (1 if opponent_cooperations >= self.game_description.m else -1), 0), self.game_description.m)
            threshold_met = opponent_cooperations >= self.game_description.m
            other_defected = not np.all(history.opponent_actions[-1, :])
            cooperate = self.m_est < self.game_description.m or (other_defected and threshold_met)
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_556(BaseStrategy):
    """
    Exploitative strategy that adapts to changing game conditions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators_prev_round >= self.game_description.m:
            cooperate_prob = 0.2 + 0.8 * self.cooperation_probability
        else:
            cooperate_prob = 0.6
        if state.round_number > 1:
            threshold_success_rate = sum([sum(history.opponent_actions[t, :]) + history.my_actions[t] >= self.game_description.m for t in range(state.round_number)]) / (state.round_number - 1)
            if threshold_success_rate > 0.5:
                self.cooperation_probability += 0.1
            else:
                self.cooperation_probability -= 0.1
        self.cooperation_probability = max(0.1, min(self.cooperation_probability, 0.9))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if random.random() < 0.2 else Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_557(BaseStrategy):
    """
    The Adaptive Threshold Exploiter strategy aims to maximize its payoff by adapting to the opponents' behavior while exploiting their cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prime = game_description.m
        self.cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if self.cooperators_last_round > self.game_description.n_players / 2 and self.m_prime < self.game_description.n_players - 1:
            self.m_prime += 1
        elif self.cooperators_last_round < self.game_description.n_players / 2 and self.m_prime > 2:
            self.m_prime -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif self.cooperators_last_round >= self.game_description.m:
            return Action.D
        elif self.cooperators_last_round < self.game_description.m and self.cooperators_last_round >= self.m_prime:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_558(BaseStrategy):
    """
    A strategy that adapts to the opponents' behavior and exploits their cooperation patterns.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        k = self.game_description.k
        t = state.round_number + 1
        if n <= 2 or r == 1:
            return Action.D
        if m <= 1 or k <= 1:
            return Action.C if random.random() < 0.5 else Action.D
        if t == 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        avg_payoff = np.mean(history.my_payoffs)
        if t < r / 2:
            if prev_cooperators >= m:
                return Action.C
            else:
                return Action.D
        elif t < 3 * r / 4:
            if avg_payoff > 1 + k / 2:
                return Action.C
            else:
                return Action.D
        elif t < r:
            if prev_cooperators >= m:
                return Action.D
            else:
                return Action.C
        else:
            total_cooperators = sum(sum(history.opponent_actions))
            if total_cooperators >= m * (r - 1):
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_559(BaseStrategy):
    """
    Exploitative Strategy: Threshold Tracker with Adaptive Defection.
    
    This strategy aims to exploit opponents by adapting to their behavior and maximizing personal payoffs while minimizing contributions to the collective project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold_met_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if history.my_actions.size > 2 and cooperators >= self.game_description.m:
            self.threshold_met_count += 1
        adaptivity_factor = 0.8 if self.threshold_met_count / state.round_number >= 0.5 else 0
        if state.round_number < self.game_description.n_rounds - 1:
            if cooperators < self.game_description.m:
                return Action.D
            elif self.game_description.k > 1.5 and random.random() < adaptivity_factor + (self.game_description.k - 1.5) / (2.5 - 1):
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_560(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) Strategy.

    This strategy adapts to the evolving cooperation levels and payoff structures.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.0
        self.total_payoff = 0.0
        self.window_size = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        self.cooperation_threshold = num_cooperators_prev_round / self.game_description.n_players
        cooperate_prob = 1.0 if self.cooperation_threshold >= self.game_description.m / self.game_description.n_players else 0.0
        if state.round_number >= self.window_size:
            avg_payoff_per_round = self.total_payoff / self.window_size
            if avg_payoff_per_round < self.game_description.k / 2:
                cooperate_prob -= 0.1
            elif avg_payoff_per_round > self.game_description.k:
                cooperate_prob += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C if random.random() < cooperate_prob else Action.D

    def update_payoff(self, payoff: float):
        self.total_payoff += payoff

class Strategy_EXPLOITATIVE_561(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.C_count = 0
        self.D_count = 0
        self.CR = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        self.C_count = sum(history.opponent_actions[-1, :])
        self.D_count = self.game_description.n_players - 1 - self.C_count
        self.CR = self.C_count / (self.game_description.n_players - 1)
        if self.CR >= self.game_description.m / self.game_description.n_players or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif self.game_description.k > 2 and random.random() < (self.game_description.k - 1) / self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_563(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Threshold Pursuer" (ATP)

    The ATP strategy aims to exploit the game's structure by adapting to the evolving cooperation level and opponents' behavior.
    This strategy is designed to maximize payoffs while minimizing the risk of being exploited.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_C_prev = 0.5
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            p_C = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < p_C else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        avg_C_prev = opponent_cooperations / self.game_description.n_players
        if avg_C_prev >= self.game_description.m / self.game_description.n_players:
            p_C = 1 - self.epsilon
        else:
            p_C = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < p_C else Action.D

class Strategy_EXPLOITATIVE_564(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to be adaptive, robust, and exploitative.
    It adapts to opponents' behavior by adjusting the cooperation probability based on their actions,
    and exploits consistent cooperators to maximize payoffs in a wide range of scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_payoff = 0.0
        self.threshold = game_description.m / game_description.n_players
        self.consistent_cooperators = []
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        self.round_count += 1
        prev_p_C = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if self.round_count % (self.game_description.n_rounds // 4) == 0:
            self.avg_payoff += history.my_payoffs[-1] / self.game_description.n_rounds
            if self.avg_payoff > np.mean(history.my_payoffs):
                self.threshold = max(0, self.threshold - 0.1)
            else:
                self.threshold = min(1, self.threshold + 0.1)
        action = Action.C if random.random() < min(1, prev_p_C * (self.game_description.k + 1) / self.game_description.k) else Action.D
        self.consistent_cooperators = [i for i, coop_rate in enumerate(history.opponent_actions[:, :]) if coop_rate[-1] > self.threshold]
        if action == Action.D and self.consistent_cooperators:
            pass
        return action

class Strategy_EXPLOITATIVE_565(BaseStrategy):
    """
    A strategy that balances exploration and exploitation by adapting to the game's dynamics and opponents' behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperators = 0
        self.opponent_behavior = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        avg_cooperators = self.calculate_avg_cooperators(history)
        exploitable_opponent = self.exploitable_opponent(history)
        if state.round_number > self.game_description.n_rounds * 0.9:
            return self.adapt_to_conditions(avg_cooperators, history)
        if avg_cooperators < self.game_description.m / 2:
            return Action.D
        elif avg_cooperators >= self.game_description.m - 1:
            return Action.C
        if exploitable_opponent:
            return Action.D
        return Action.C

    def calculate_avg_cooperators(self, history: PlayerHistory):
        recent_cooperations = sum(np.sum(history.opponent_actions[-3:], axis=0)) + int(history.my_actions[-1])
        self.avg_cooperators = recent_cooperations / 3 if history.opponent_actions.shape[0] >= 3 else recent_cooperations / (history.opponent_actions.shape[0] + 1)
        return self.avg_cooperators

    def exploitable_opponent(self, history: PlayerHistory):
        for opponent in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, opponent]:
                return True
        return False

    def adapt_to_conditions(self, avg_cooperators, history: PlayerHistory):
        if avg_cooperators >= self.game_description.m:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_566(BaseStrategy):
    """
    Adaptive Exploiter strategy.
    
    This strategy aims to maximize its own payoff by adapting to the environment 
    and exploiting opportunities for cooperation while minimizing the risk of being exploited itself.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.last_round_cooperators = None
        self.avg_payoff_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperators >= self.game_description.m:
            coop_prob = min(1, cooperators / self.game_description.n_players + self.game_description.k / 10)
        else:
            coop_prob = max(0, (self.game_description.m - cooperators) / self.game_description.n_players + 1 / 10)
        if state.round_number % 5 == 4:
            avg_payoff = np.mean(self.avg_payoff_history[-5:]) if len(self.avg_payoff_history) >= 5 else np.mean(self.avg_payoff_history)
            if avg_payoff > 1.5:
                coop_prob += 0.05
            elif avg_payoff < 1.2:
                coop_prob -= 0.05
        if len(history.my_payoffs) > 0:
            self.avg_payoff_history.append(history.my_payoffs[-1])
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_EXPLOITATIVE_567(BaseStrategy):
    """
    Exploitative strategy that adaptively responds to cooperation levels of other players.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.acr_threshold = game_description.m / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = np.array(history.opponent_actions, dtype=bool)
        n_cooperators = np.sum(opponent_actions[-1]) + history.my_actions[-1]
        acr = n_cooperators / self.game_description.n_players
        if not history.my_actions[-1] and n_cooperators < self.game_description.m:
            self.acr_threshold -= 0.1
        elif history.my_actions[-1] and n_cooperators >= self.game_description.m:
            self.acr_threshold += 0.05
        if state.round_number == self.game_description.n_rounds - 1 and acr >= self.acr_threshold:
            return Action.C
        if acr >= self.acr_threshold:
            return Action.C
        else:
            single_defector = np.sum(~opponent_actions[-1]) == 1 and (not history.my_actions[-1])
            if single_defector:
                return Action.C
            return Action.D

class Strategy_EXPLOITATIVE_568(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) Strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_payoff_defect = 0
        self.avg_payoff_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.my_actions) > 1:
            cooperators_t = sum([int(a) for a in history.opponent_actions[-2, :]]) + int(history.my_actions[-2])
        else:
            return Action.C
        defectors_t = self.game_description.n_players - cooperators_t
        if cooperators_t < self.game_description.m:
            action = Action.D
        elif history is not None and len(history.my_actions) > 1:
            opponent_cooperators_last_round = sum([int(a) for a in history.opponent_actions[-2, :]])
            my_payoff_defect = 0.0
            my_payoff_coop = 0.0
            if opponent_cooperators_last_round >= self.game_description.m - 1:
                my_payoff_coop += (0 + self.game_description.k) / self.game_description.n_rounds
            else:
                my_payoff_defect = (1.0 + self.game_description.k) / self.game_description.n_rounds
            self.avg_payoff_defect = max(self.avg_payoff_defect, my_payoff_defect)
            self.avg_payoff_coop = max(self.avg_payoff_coop, my_payoff_coop)
            if self.avg_payoff_defect > self.avg_payoff_coop:
                action = Action.D
            else:
                action = Action.C
        else:
            action = Action.C
        if defectors_t > self.game_description.n_players - self.game_description.m:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1 and cooperators_t >= self.game_description.m:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_569(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_cooperators = 0
        self.unreliable_players = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        self.avg_cooperators = (self.avg_cooperators * state.round_number + num_cooperators_prev_round) / (state.round_number + 1)
        for opponent in self.unreliable_players:
            if random.random() < 0.5:
                return Action.C
        if self.avg_cooperators >= self.game_description.m / 2 and state.round_number < self.game_description.n_rounds:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_570(BaseStrategy):
    """
    A strategy that balances cooperation and defection based on the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p_init = game_description.m / game_description.n_players
        self.p_max = 0.8
        self.p_min = 0.2
        self.cooperation_prob = self.p_init

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        prev_round_success = sum(history.my_actions[-1] + history.opponent_actions[-1]) >= self.game_description.m
        if prev_round_success:
            self.cooperation_prob = min(self.p_max, self.cooperation_prob + 0.1 * (self.p_max - self.p_min))
        else:
            self.cooperation_prob = max(self.p_min, self.cooperation_prob - 0.2 * (self.p_max - self.p_min))
        opponent_actions = history.opponent_actions[:-1]
        for i in range(self.game_description.n_players - 1):
            opponent_cooperations = sum(opponent_actions[:, i])
            opponent_defections = len(opponent_actions) - opponent_cooperations
            if opponent_defections > opponent_cooperations:
                self.cooperation_prob = max(self.p_min, self.cooperation_prob - 0.3 * (self.p_max - self.p_min))
            elif opponent_cooperations > opponent_defections:
                self.cooperation_prob = min(self.p_max, self.cooperation_prob + 0.2 * (self.p_max - self.p_min))
        if state.round_number == self.game_description.n_rounds and (not any(history.my_payoffs)):
            return Action.D
        return Action.C if random.random() < self.cooperation_prob else Action.D

class Strategy_EXPLOITATIVE_571(BaseStrategy):
    """
    The Adaptive Cooperative Exploiter (ACE) strategy for the Collective Risk Dilemma.
    
    Initially cooperates to encourage others, then adapts based on cooperation rate.
    Becomes more exploitative if cooperation is low, balancing contribution and minimizing losses.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.mid_game_avg_coop_rate_calculated = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.game_description.n_rounds // 2:
            prev_cooperators = sum(history.opponent_actions[-1, :])
            if prev_cooperators >= self.game_description.m - 1 or (prev_cooperators < self.game_description.m - 1 and history.my_actions[-1]):
                return Action.C
            else:
                self.mid_game_avg_coop_rate_calculated = True
                p = 1 - (self.game_description.m - 1) / self.game_description.n_players
                if random.random() < p:
                    return Action.C
                else:
                    return Action.D
        if not self.mid_game_avg_coop_rate_calculated:
            window_size = self.game_description.n_rounds // 4
            coop_counts = np.sum(history.opponent_actions[-window_size:, :], axis=1)
            mid_game_avg_coop_rate = np.mean(coop_counts) / self.game_description.n_players
            if mid_game_avg_coop_rate < 0.5:
                p = 1 - (self.game_description.m - 1) / self.game_description.n_players
                self.mid_game_avg_coop_rate_calculated = True
                if random.random() < p:
                    return Action.C
                else:
                    return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_572(BaseStrategy):
    """
    Adaptive Threshold Exploiter strategy for Collective Risk Dilemma.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.coop_rate_prev = 0.0
        self.m_threshold = game_description.m

    def calculate_coop_rate(self, history: PlayerHistory) -> float:
        """
        Calculate cooperation rate of opponents in the previous round.
        """
        opponent_actions = history.opponent_actions[-1, :]
        return np.mean(opponent_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= min(self.game_description.n_rounds // 2, 3):
            return Action.C
        self.coop_rate_prev = self.calculate_coop_rate(history)
        if self.coop_rate_prev < self.game_description.m / self.game_description.n_players:
            self.m_threshold = self.game_description.m - 1
        else:
            self.m_threshold = self.game_description.m
        if self.coop_rate_prev < self.m_threshold / self.game_description.n_players:
            return Action.D
        else:
            payoff_increase = self.game_description.k * (1.0 - float(self.game_description.n_players > 1))
            p_C = min(1.0, payoff_increase / self.game_description.k)
            if random.random() < p_C:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        all_defected = np.all(~history.opponent_actions[-1, :])
        if all_defected and random.random() < 0.1:
            return Action.C

class Strategy_EXPLOITATIVE_575(BaseStrategy):
    """
    Adaptive Threshold Exploiter (ATE) strategy for Collective Risk Dilemma.
    
    This strategy adapts its cooperation rate based on the game's history, 
    aiming to exploit opponents by cooperating when the threshold is likely 
    to be met and defecting otherwise.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.m == 0 or self.game_description.k == 0:
            return Action.D
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :] | np.bool_(history.my_actions[-1]))
        if prev_cooperators >= self.game_description.m:
            cooperate_prob = 1 - 1 / self.game_description.k
        else:
            cooperate_prob = self.game_description.m / self.game_description.n_players
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_576(BaseStrategy):
    """
    Exploitative strategy that adaptively seeks the threshold of minimum cooperators needed while balancing individual payoffs.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        avg_payoff_per_cooperator = self._calculate_average_payoff(history)
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif avg_payoff_per_cooperator >= (self.game_description.k + 1) / 2:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

    def _calculate_average_payoff(self, history: PlayerHistory) -> float:
        cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        total_payoff = history.my_payoffs[-1] if history.my_actions[-1] else 0
        for opponent_action, opponent_payoff in zip(history.opponent_actions[-1], history.opponent_payoffs[-1]):
            if opponent_action:
                cooperators += 1
                total_payoff += opponent_payoff
        return total_payoff / cooperators if cooperators > 0 else 0

class Strategy_EXPLOITATIVE_577(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game.
    Cooperates in initial rounds, then adapts to opponents' cooperation levels,
    punishing defecting players and exploiting cooperative ones.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.n:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :]) + 1
        if prev_coops < self.m:
            return Action.D
        elif prev_coops >= self.m:
            p = self.k / (self.k + 1)
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        for i in range(self.n - 1):
            if not history.opponent_actions[-1, i] and prev_coops > self.m:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        p = 0.5
        if random.random() < p:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_578(BaseStrategy):
    """
    Adaptive Exploiter strategy balances exploration, exploitation, and adaptability 
    to maximize payoffs in a wide range of scenarios.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_level = 0
        self.opponents_cooperated = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.opponents_cooperated = [bool(action) for action in history.opponent_actions[-1, :]]
            self.cooperation_level = sum(self.opponents_cooperated)
        if state.round_number < 3:
            return Action.D
        elif self.cooperation_level < self.game_description.m:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_579(BaseStrategy):
    """
    An adaptive strategy that exploits opponents when possible while adapting to changing behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperate_threshold = math.floor(game_description.m / 2)
        self.exploit_threshold = game_description.k / 2 * game_description.m
        self.prev_defection_status = False
        self.defect_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        prev_payoff = history.my_payoffs[-1]
        if prev_cooperators <= self.cooperate_threshold:
            return Action.D
        exploit_rounds = math.floor(self.game_description.n_rounds / 4)
        start_index = max(0, state.round_number - exploit_rounds + 1)
        total_payoff = sum(history.my_payoffs[start_index:state.round_number])
        if total_payoff < self.exploit_threshold:
            return Action.D
        if self.prev_defection_status and prev_cooperators > self.cooperate_threshold:
            return Action.C
        if self.defect_count >= 2:
            return Action.D
        self.prev_defection_status = False
        return Action.C

    def update_defection_status(self, action: Action):
        if action == Action.D:
            self.prev_defection_status = True
            self.defect_count += 1
        else:
            self.defect_count = 0

class Strategy_EXPLOITATIVE_580(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy adapts to the game's history and parameters.
    It exploits opponents' behaviors by adjusting a dynamic threshold based on previous rounds' cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.threshold = game_description.m

    def estimate_cooperators(self, opponent_actions: NDArray[np.bool_]) -> int:
        cooperation_rate = np.mean(opponent_actions) if len(opponent_actions) > 0 else 0.5
        return math.ceil(cooperation_rate * self.game_description.n_players)

    def update_threshold(self, prev_coops: int) -> None:
        if prev_coops < self.game_description.m:
            self.threshold = min(self.threshold + 1, self.game_description.n_players - 1)
        elif prev_coops >= self.game_description.m:
            self.threshold = max(self.threshold - 1, self.game_description.m)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.my_actions) < 2):
            return Action.C
        opponent_actions = history.opponent_actions if history else np.empty((0, self.game_description.n_players - 1))
        prev_coops = sum(opponent_actions[-1, :]) + (history.my_actions[-1] if history else False)
        self.update_threshold(prev_coops)
        expected_coops = self.estimate_cooperators(opponent_actions) + (1 if state.round_number == len(history.my_actions) else 0)
        if expected_coops <= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_581(BaseStrategy):
    """
    The Adaptive Threshold Exploiter (ATE) strategy is designed to maximize personal payoff by exploiting others' cooperative behaviors 
    while ensuring enough cooperation to meet thresholds beneficially. It adapts based on observed strategies and game history.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        your_action_prev_round = history.my_actions[-1]
        if prev_cooperators < self.game_description.m:
            return Action.D
        elif prev_cooperators >= self.game_description.m and your_action_prev_round:
            return Action.D
        elif prev_cooperators >= self.game_description.m and (not your_action_prev_round):
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_582(BaseStrategy):
    """
    An adaptive exploitative strategy for the Collective Risk Dilemma game.
    Balances cooperation with opportunistic defection based on observed opponent behavior.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.round_count = 0
        self.last_defect_rounds = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < math.ceil(0.1 * self.game_description.n_rounds):
            return Action.C
        if state.round_number == math.ceil(0.1 * self.game_description.n_rounds):
            opponent_actions = history.opponent_actions[:state.round_number]
            coop_count = np.sum(opponent_actions)
            self.avg_coop_rate = coop_count / (self.game_description.n_players * state.round_number)
        if self.avg_coop_rate > 0.7:
            if not self.last_defect_rounds or state.round_number - self.last_defect_rounds[-1] > 5:
                self.last_defect_rounds.append(state.round_number)
                return Action.D
        elif self.avg_coop_rate < 0.3:
            if random.random() < 0.7:
                return Action.C
        opponent_coop_count = np.sum(history.opponent_actions[-1, :])
        if opponent_coop_count < self.game_description.m:
            return Action.C
        else:
            return Action.D
        self.round_count += 1
        if self.round_count % 20 == 0 and state.round_number > math.ceil(0.1 * self.game_description.n_rounds):
            self.adjust_strategy(history)
        if state.round_number > math.floor(0.9 * self.game_description.n_rounds):
            return Action.C

    def adjust_strategy(self, history: PlayerHistory):
        """
        Adjust strategy based on observed opponent behavior and game progress.
        Increase or decrease cooperation rate to maximize payoff.
        """
        pass

class Strategy_EXPLOITATIVE_583(BaseStrategy):
    """
    The Adaptive Threshold Hunter (ATH) strategy is designed to exploit opponents' behaviors while adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.M_t = 0
        self.defection_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.M_t = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        if self.M_t >= self.game_description.m:
            if history.my_payoffs[-1] > 1 + self.game_description.k and (not history.my_actions[-1]):
                self.defection_count = 0
                return Action.D
            if self.defection_count > 0:
                if self.M_t >= self.game_description.m - 1:
                    self.defection_count = 0
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        elif history.my_payoffs[-1] > 1 + self.game_description.k and (not history.my_actions[-1]):
            self.defection_count += 1
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_584(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperator_count_history = []
        self.total_payoff_history = []
        self.last_adjustment_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        self.cooperator_count_history.append(cooperators)
        if len(self.cooperator_count_history) > self.game_description.n_rounds // 4:
            if sum(self.cooperator_count_history[:-self.game_description.n_rounds // 4]) < self.game_description.m * (len(self.cooperator_count_history) - self.game_description.n_rounds // 4):
                return Action.D
            else:
                if cooperators >= self.game_description.n_players // 2:
                    return Action.C
                return Action.D
        if state.round_number % (self.game_description.n_rounds // 4) == 0 and state.round_number > self.last_adjustment_round:
            self.last_adjustment_round = state.round_number
            total_payoff_change = sum(self.total_payoff_history[-(state.round_number - self.last_adjustment_round):]) - sum(self.total_payoff_history[-2 * (state.round_number - self.last_adjustment_round):-(state.round_number - self.last_adjustment_round)])
            if len(self.total_payoff_history) > self.game_description.n_rounds // 4 and total_payoff_change <= 0:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperators < self.game_description.m // 2:
            return Action.D
        if history is not None and len(history.my_payoffs) > state.round_number - 1:
            self.total_payoff_history.append(history.my_payoffs[state.round_number - 1])
        return Action.C

class Strategy_EXPLOITATIVE_585(BaseStrategy):
    """
    Exploitative strategy that adapts its cooperation threshold based on past observations.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_threshold = None
        self.opponent_cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions_last_round = history.opponent_actions[-1, :]
        self.opponent_cooperation_rate = sum(opponent_actions_last_round) / (self.game_description.n_players - 1)
        if self.opponent_cooperation_rate >= (self.game_description.m - 1) / (self.game_description.n_players - 1):
            self.cooperation_threshold = self.game_description.m
        else:
            self.cooperation_threshold = math.ceil((self.game_description.m - 1) / (self.game_description.n_players - 1)) * (self.game_description.n_players - 1)
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if sum(opponent_actions_last_round) < self.game_description.m and state.round_number > 0:
            return random.choices([Action.C, Action.D], weights=[0.25, 0.75])[0]
        return Action.C if sum(opponent_actions_last_round) >= self.cooperation_threshold else Action.D

class Strategy_EXPLOITATIVE_586(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation levels.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.m = game_description.m
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            initial_coop_prob = min(1, max(0, self.m / self.n_players))
            return Action.C if random.random() < initial_coop_prob else Action.D
        prev_coop_level = sum(history.opponent_actions[-1, :])
        if prev_coop_level < self.m:
            defect_prob = max(0, min(1, 1 - prev_coop_level / self.m))
            return Action.D if random.random() > defect_prob else Action.C
        else:
            coop_prob = min(1, max(0, prev_coop_level / self.m + (self.k - 1) / self.n_players))
            return Action.C if random.random() < coop_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_587(BaseStrategy):
    """
    A strategy that exploits opponents' behaviors while adapting to the game's history and parameters.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators < self.game_description.m:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C if random.random() < self.game_description.k / (self.game_description.k + 1) else Action.D

class Strategy_EXPLOITATIVE_588(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for the Collective Risk Dilemma game.
    
    AE adapts to the game's history while focusing on maximizing individual payoffs.
    It cooperates initially, then exploits potential cooperative tendencies 
    while minimizing its own risk exposure.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        """
        Initialize the Adaptive Exploiter strategy.

        Args:
            game_description (CollectiveRiskDescription): Game description object containing n_players, n_rounds, m, and k parameters.
        """
        self.game_description = game_description
        self.cooperation_frequency_window = math.ceil(game_description.n_rounds / 6)
        self.payoff_difference_threshold = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the action for the current round based on the game's history.

        Args:
            state (GameState): Current game state containing the round number.
            history (None | PlayerHistory): History of actions and payoffs for all players up to this round.

        Returns:
            Action: The chosen action (C or D) for the current round.
        """
        if state.round_number == 0:
            return Action.C
        elif state.round_number < math.ceil(self.game_description.n_rounds / 3):
            prev_cooperators = sum(history.opponent_actions[-1, :] + history.my_actions[-1:])
            if prev_cooperators >= self.game_description.m:
                return Action.C
            elif len(history.my_actions) > 1 and history.my_actions[-2] == True and (sum(history.opponent_actions[-1, :] + history.my_actions[-1:]) > prev_cooperators):
                return Action.D
            else:
                return Action.C if state.round_number % 2 == 0 else Action.D
        elif state.round_number < math.ceil(2 * self.game_description.n_rounds / 3):
            cooperation_frequency = sum(history.my_actions[-self.cooperation_frequency_window:]) / self.cooperation_frequency_window
            if cooperation_frequency >= 0.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number > math.ceil(2 * self.game_description.n_rounds / 3):
            payoff_difference = sum(history.my_payoffs[history.my_actions == True]) - sum(history.my_payoffs[history.my_actions == False])
            if payoff_difference > self.payoff_difference_threshold:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_590(BaseStrategy):
    """
    A strategic approach for the Collective Risk Dilemma game that aligns with an exploitative mindset 
    and adapts to various opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.0
        self.opponent_actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        recent_opponent_actions = history.opponent_actions[-1, :] if history else []
        self.cooperation_rate = self.calculate_CR(self.opponent_actions + [recent_opponent_actions])
        punishment_phase = False
        if state.round_number > 2 and len(history.my_actions) > 0:
            my_previous_action = history.my_actions[-1]
            opponent_previous_actions = history.opponent_actions[-1, :]
            opponent_defected_while_i_cooperated = np.any(opponent_previous_actions == False) and my_previous_action
            if opponent_defected_while_i_cooperated:
                punishment_phase = True
        if punishment_phase or self.cooperation_rate >= 0.5:
            return Action.D
        else:
            return Action.C

    def calculate_CR(self, opponent_actions):
        total_actions = sum((len(actions) for actions in opponent_actions))
        cooperations = sum((np.sum(actions == True) for actions in opponent_actions if isinstance(actions, np.ndarray)))
        if total_actions > 0:
            return cooperations / total_actions
        else:
            return 0.0

    def last_round(self, state: GameState):
        return Action.D

class Strategy_EXPLOITATIVE_591(BaseStrategy):
    """
    An adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.cooperators_met = False
        self.defectors_exceeded = False
        self.cooperation_window_open = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        cooperators_last_round = sum(opponent_actions_last_round)
        defectors_last_round = self.game_description.n_players - 1 - cooperators_last_round
        if cooperators_last_round >= self.game_description.m:
            self.cooperators_met = True
            self.cooperation_window_open = True
        else:
            self.cooperators_met = False
            self.cooperation_window_open = False
        if defectors_last_round > self.game_description.n_players - self.game_description.m:
            self.defectors_exceeded = True
        else:
            self.defectors_exceeded = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.cooperation_window_open and state.round_number % 2 == 0:
            return Action.C
        elif self.defectors_exceeded:
            return Action.D
        elif self.cooperators_met:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_592(BaseStrategy):
    """
    The Adaptive Risk-Taker strategy aims to maximize its payoff by adaptively exploiting the cooperative behavior of other players while minimizing the risk of contributing to a failed community project.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.defection_threshold = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.m / self.game_description.n_players else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_round_cooperators >= self.game_description.m:
            cooperation_probability = 1 - 1 / self.game_description.k
            self.defection_threshold += 1
        else:
            cooperation_probability = 1 - self.game_description.m / self.game_description.n_players
            self.defection_threshold -= 1
        if self.defection_threshold >= self.game_description.m:
            cooperation_probability = 1 - 1 / self.game_description.k
        if self.game_description.n_rounds > 2 and state.round_number in [self.game_description.n_rounds - 1, self.game_description.n_rounds]:
            cooperation_probability = self.game_description.m / self.game_description.n_players
        return Action.C if random.random() < cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_593(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_players == 2:
            if state.round_number == 0 or (history is not None and history.my_actions[-1]):
                return Action.C
            else:
                return Action.D
        elif self.game_description.m == 1:
            return Action.C
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.game_description.n_rounds - 1:
            m_prev = sum(history.opponent_actions[-1, :] | np.array([history.my_actions[-1]]))
            self.m_prev = m_prev
            delta_m = self.game_description.m - m_prev
            if m_prev >= self.game_description.m:
                return Action.C
            elif delta_m > 1:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_594(BaseStrategy):
    """
    An adaptive strategy that exploits cooperative opponents in a Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.window_size = 5
        self.cooperation_threshold = 0.4 * self.window_size

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.m / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        opponent_actions = np.roll(history.opponent_actions, -1, axis=0)[:-1]
        opponent_coop_counts = np.sum(opponent_actions[-self.window_size:], axis=0)
        for i in range(self.game_description.n_players):
            if opponent_coop_counts[i] >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_595(BaseStrategy):
    """
    Exploitative strategy for the Collective Risk Dilemma game.
    This strategy aims to balance between maximizing personal payoff and influencing others to cooperate when beneficial.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.round_number = 0
        self.all_cooperations = []
        self.cooperations_in_previous_rounds = []
        self.total_expected_cooperations = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_number = state.round_number
        if history is not None:
            self.cooperations_in_previous_rounds.append(sum(history.opponent_actions[-1, :]))
        if state.round_number <= 2:
            return Action.C
        total_cooperators = sum(self.cooperations_in_previous_rounds)
        avg_coop_rate = total_cooperators / len(self.cooperations_in_previous_rounds) if self.cooperations_in_previous_rounds else 0
        if avg_coop_rate >= 0.5 and total_cooperators > 0:
            return Action.C
        elif total_cooperators < self.game_description.m and total_cooperators / self.game_description.n_players > 0.25:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            expected_cooperators = total_cooperators + 1
            if expected_cooperators >= self.game_description.m:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_596(BaseStrategy):
    """
    An adaptive exploitation strategy for the Collective Risk Dilemma game.
    This strategy balances cooperation and exploitation by adapting to the game's progress and other players' behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.prev_coops = []
        self.avg_payoff = 0

    def calculate_avg_payoff(self, history: PlayerHistory) -> float:
        payoffs = np.sum(history.my_payoffs[:history.opponent_actions.shape[0]], axis=0)
        return payoffs / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.prev_coops.append(np.sum(history.opponent_actions[-1, :]))
        if state.round_number < math.floor(self.game_description.n_rounds / 3):
            if len(self.prev_coops) >= self.game_description.m - 1:
                return Action.C
            else:
                return Action.D
        self.avg_payoff = self.calculate_avg_payoff(history)
        if self.avg_payoff > 1 + self.game_description.k / 2:
            if len(self.prev_coops) >= self.game_description.m:
                return Action.C
            else:
                return Action.D
        elif len(self.prev_coops) >= self.game_description.m - 1:
            return Action.C
        else:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_597(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to capitalize on opponents' cooperation 
    while minimizing its own contributions. It adapts to various opponent behaviors based 
    on the game's history, making it robust against different strategies.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m = game_description.m
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        avg_coops = np.mean(history.opponent_actions[:-1], axis=1).mean() + history.my_actions[state.round_number - 1]
        coop_prob = 0.8 if avg_coops <= (self.m - 1) / self.n_players else 0.2
        if state.round_number == self.n_rounds - 1:
            return Action.D
        elif random.random() < coop_prob:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_598(BaseStrategy):
    """
    An adaptive exploitative strategy for the Collective Risk Dilemma game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.m_prev = 0
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        if history is not None:
            opponent_actions = history.opponent_actions[-1, :]
            my_action = history.my_actions[-1]
            self.m_prev = sum(opponent_actions) + (1 if my_action else 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.m_prev >= self.game_description.m:
            action = Action.D if random.random() < 0.7 else Action.C
        else:
            action = Action.C if random.random() < 1 - self.m_prev / self.game_description.m else Action.D
        if self.m_prev > self.game_description.m * 0.8:
            self.cooperate_prob -= 0.1
        elif self.m_prev < self.game_description.m * 0.2:
            self.cooperate_prob += 0.1
        self.cooperate_prob = max(0, min(self.cooperate_prob, 1))
        return action

class Strategy_EXPLOITATIVE_599(BaseStrategy):
    """
    An exploitative strategy for the Collective Risk Dilemma game that adapts to various opponent behaviors and maximizes payoff.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < math.ceil(self.game_description.n_rounds / 2):
            return Action.C
        opponent_cooperation_rate = np.mean(history.opponent_actions[-3:, :])
        if opponent_cooperation_rate >= 0.75:
            return Action.D
        if history is not None and len(history.my_payoffs) > 1:
            my_previous_payoff = history.my_payoffs[-1]
            opponents_previous_cooperation_rate = np.mean(history.opponent_actions[-2:, :])
            if my_previous_payoff < self.game_description.k + 1:
                return Action.D
            elif opponents_previous_cooperation_rate > 0.5:
                return Action.C
        opponent_actions = history.opponent_actions[-3:, :]
        most_frequent_opponent_action = np.bincount(opponent_actions.flatten()).argmax()
        return Action(most_frequent_opponent_action)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_600(BaseStrategy):
    """
    Exploitative strategy that adapts to changing game conditions by monitoring average payoffs and opponent behaviors.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.p_init = self.m / self.n
        self.cooperation_prob = self.p_init
        self.avg_payoff_coop = 0
        self.avg_payoff_defect = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        my_payoff_coop = (self.avg_payoff_coop * (state.round_number - 1) + (1 - 1 + self.k if history.my_actions[-1] else 0)) / state.round_number
        my_payoff_defect = (self.avg_payoff_defect * (state.round_number - 1) + (1 + 0 if not history.my_actions[-1] else 0)) / state.round_number
        if my_payoff_coop > my_payoff_defect:
            self.cooperation_prob = min(self.cooperation_prob + 0.1, 1)
        elif my_payoff_defect > my_payoff_coop:
            self.cooperation_prob = max(self.cooperation_prob - 0.1, 0)
        num_cooperators = sum(history.opponent_actions[-1, :])
        if num_cooperators >= self.m:
            return Action.D if random.random() < 0.8 else Action.C
        if state.round_number == self.r - 1:
            return Action.D
        return Action.C if random.random() < self.cooperation_prob else Action.D
